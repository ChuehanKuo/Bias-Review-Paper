{
  "ft_screening_criteria": {
    "approach_indicators": [
      "bias assessment",
      "bias detection",
      "bias evaluation",
      "bias audit",
      "fairness assessment",
      "fairness evaluation",
      "fairness audit",
      "bias measurement",
      "bias quantification",
      "bias analysis",
      "measuring bias",
      "detecting bias",
      "evaluating bias",
      "fairness metric",
      "fairness measure",
      "bias metric",
      "demographic parity",
      "equalized odds",
      "equal opportunity",
      "disparate impact",
      "predictive parity",
      "calibration across",
      "subgroup analysis",
      "disaggregated",
      "stratified performance",
      "bias mitigation",
      "bias reduction",
      "bias correction",
      "debiasing",
      "debias",
      "fairness-aware",
      "fair machine learning",
      "fair classification",
      "adversarial debiasing",
      "reweighting",
      "reweighing",
      "resampling",
      "data augmentation for fairness",
      "fairness constraint",
      "fairness regularization",
      "threshold adjustment",
      "calibration",
      "post-processing",
      "pre-processing",
      "in-processing",
      "counterfactual fairness",
      "causal fairness",
      "fairness framework",
      "bias framework",
      "equity framework",
      "ai fairness 360",
      "aequitas",
      "fairlearn",
      "bias toolkit",
      "fairness toolkit",
      "model card",
      "datasheet",
      "review of bias",
      "survey of fairness",
      "review of fairness",
      "bias mitigation strategies",
      "approaches to fairness",
      "methods for bias",
      "techniques for fairness"
    ],
    "ai_terms": [
      "machine learning",
      "deep learning",
      "artificial intelligence",
      "neural network",
      "algorithm",
      "predictive model",
      "classifier",
      "natural language processing",
      "computer vision",
      "random forest",
      "logistic regression",
      "xgboost",
      "convolutional",
      "transformer",
      "large language model",
      "llm",
      "decision support",
      "risk prediction",
      "federated learning",
      "reinforcement learning",
      "foundation model",
      "chatgpt",
      "gpt"
    ],
    "health_terms": [
      "health",
      "clinical",
      "medical",
      "patient",
      "hospital",
      "disease",
      "diagnosis",
      "treatment",
      "care",
      "radiology",
      "dermatology",
      "cardiology",
      "oncology",
      "ophthalmology",
      "psychiatry",
      "ehr",
      "electronic health",
      "biomedical",
      "mortality",
      "readmission",
      "sepsis",
      "icu",
      "emergency",
      "chest x-ray",
      "cancer",
      "diabetes",
      "cardiovascular",
      "public health",
      "healthcare",
      "medicine"
    ],
    "bias_title_terms": [
      "bias",
      "fairness",
      "fair ",
      "equitable",
      "equity",
      "disparity",
      "disparities",
      "debiasing",
      "debias",
      "underdiagnos",
      "inequit",
      "discrimination"
    ],
    "logic": "Include if full text has: (1) AI/ML terms, AND (2) health terms, AND meets one of: (a) approach_count >= 2 AND bias in title, (b) approach_count >= 3, (c) bias in title AND approach_count >= 1. Approach indicators count occurrences of 50+ bias assessment/mitigation terms."
  },
  "phase1_pmcid_lookup": {
    "total_papers": 1146,
    "pmcids_found": 792,
    "no_pmcid": 354
  },
  "phase2_fulltext_screening": {
    "papers_with_fulltext": 792,
    "ft_included": 424,
    "ft_excluded": 356,
    "ft_fetch_failed": 12,
    "exclusion_reasons": {
      "Excluded: insufficient approach content (0 indicators)": 319,
      "No AI/ML component in full text": 17,
      "Not health-related in full text": 1,
      "Excluded: insufficient approach content (1 indicators)": 14,
      "Excluded: insufficient approach content (2 indicators)": 5
    }
  },
  "final_counts": {
    "two_stage_screened": 424,
    "one_stage_only_no_fulltext": 366,
    "ft_excluded": 356
  },
  "ft_included": [
    {
      "pmid": "20879389",
      "title": "Standing on the shoulders of giants: improving medical image segmentation via bias correction.",
      "abstract": "We propose a simple strategy to improve automatic medical image segmentation. The key idea is that without deep understanding of a segmentation method, we can still improve its performance by directly calibrating its results with respect to manual segmentation. We formulate the calibration process as a bias correction problem, which is addressed by machine learning using training data. We apply this methodology on three segmentation problems/methods and show significant improvements for all of them.",
      "journal": "Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "year": "2010",
      "doi": "10.1007/978-3-642-15711-0_14",
      "authors": "Wang Hongzhi et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artifacts; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/20879389/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC3095022",
      "ft_text_length": 504,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC3095022)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "24525488",
      "title": "Evaluating treatment effectiveness under model misspecification: A comparison of targeted maximum likelihood estimation with bias-corrected matching.",
      "abstract": "Statistical approaches for estimating treatment effectiveness commonly model the endpoint, or the propensity score, using parametric regressions such as generalised linear models. Misspecification of these models can lead to biased parameter estimates. We compare two approaches that combine the propensity score and the endpoint regression, and can make weaker modelling assumptions, by using machine learning approaches to estimate the regression function and the propensity score. Targeted maximum likelihood estimation is a double-robust method designed to reduce bias in the estimate of the parameter of interest. Bias-corrected matching reduces bias due to covariate imbalance between matched pairs by using regression predictions. We illustrate the methods in an evaluation of different types of hip prosthesis on the health-related quality of life of patients with osteoarthritis. We undertake a simulation study, grounded in the case study, to compare the relative bias, efficiency and confidence interval coverage of the methods. We consider data generating processes with non-linear functional form relationships, normal and non-normal endpoints. We find that across the circumstances considered, bias-corrected matching generally reported less bias, but higher variance than targeted maximum likelihood estimation. When either targeted maximum likelihood estimation or bias-corrected matching incorporated machine learning, bias was much reduced, compared to using misspecified parametric models.",
      "journal": "Statistical methods in medical research",
      "year": "2016",
      "doi": "10.1177/0962280214521341",
      "authors": "Kreif No\u00e9mi et al.",
      "keywords": "bias-corrected matching; double robustness; machine learning; model misspecification; targeted maximum likelihood estimation; treatment effectiveness",
      "mesh_terms": "Aged; Bias; Computer Simulation; Confidence Intervals; Data Interpretation, Statistical; Hip Prosthesis; Humans; Likelihood Functions; Machine Learning; Male; Models, Statistical; Osteoarthritis; Quality of Life; Treatment Outcome",
      "pub_types": "Comparative Study; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/24525488/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC5051604",
      "ft_text_length": 50131,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC5051604)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "25086004",
      "title": "Bias correction for selecting the minimal-error classifier from many machine learning models.",
      "abstract": "MOTIVATION: Supervised machine learning is commonly applied in genomic research to construct a classifier from the training data that is generalizable to predict independent testing data. When test datasets are not available, cross-validation is commonly used to estimate the error rate. Many machine learning methods are available, and it is well known that no universally best method exists in general. It has been a common practice to apply many machine learning methods and report the method that produces the smallest cross-validation error rate. Theoretically, such a procedure produces a selection bias. Consequently, many clinical studies with moderate sample sizes (e.g. n = 30-60) risk reporting a falsely small cross-validation error rate that could not be validated later in independent cohorts. RESULTS: In this article, we illustrated the probabilistic framework of the problem and explored the statistical and asymptotic properties. We proposed a new bias correction method based on learning curve fitting by inverse power law (IPL) and compared it with three existing methods: nested cross-validation, weighted mean correction and Tibshirani-Tibshirani procedure. All methods were compared in simulation datasets, five moderate size real datasets and two large breast cancer datasets. The result showed that IPL outperforms the other methods in bias correction with smaller variance, and it has an additional advantage to extrapolate error estimates for larger sample sizes, a practical feature to recommend whether more samples should be recruited to improve the classifier and accuracy. An R package 'MLbias' and all source files are publicly available. AVAILABILITY AND IMPLEMENTATION: tsenglab.biostat.pitt.edu/software.htm. CONTACT: ctseng@pitt.edu SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
      "journal": "Bioinformatics (Oxford, England)",
      "year": "2014",
      "doi": "10.1093/bioinformatics/btu520",
      "authors": "Ding Ying et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Breast Neoplasms; Data Interpretation, Statistical; Female; Gene Expression Profiling; Genomics; Humans; Models, Theoretical; Sample Size",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/25086004/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC4221122",
      "ft_text_length": 1867,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC4221122)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "27006645",
      "title": "Multiple-bias analysis as a technique to address systematic error in measures of abortion-related mortality.",
      "abstract": "BACKGROUND: The UN Millennium Development Goals (MDGs) and Sustainable Development Goals (SDGs) have brought heightened global attention to the measurement of maternal mortality. It is imperative that new and novel approaches be used to measure maternal mortality and to better understand existing data. In this paper we present one approach: an epidemiologic framework for identifying the identification and quantification of systematic error (multiple-bias analysis), outline the necessary steps for investigators interested in conducting multiple-bias analyses in their own data, and suggest approaches for reporting such analyses in the literature. METHODS: To conceptualize the systematic error present in studies of abortion-related deaths, we propose a bias framework. We posit that selection bias and misclassification are present in both verbal autopsy studies and facility-based studies. The multiple-bias analysis framework provides a relatively simple, quantitative strategy for assessing systematic error and resulting bias in any epidemiologic study. RESULTS: In our worked example of multiple-bias analysis on a study reporting 20.6 % of maternal deaths to be abortion related, after adjustment for selection bias, misclassification, and random error, the median increased, on average, to 0.308, approximately 20 % greater than the reported proportion of abortion-related deaths. CONCLUSIONS: Reporting results of multiple-bias analyses in estimates of abortion-related mortality, predictors of unsafe abortion, and other reproductive health questions that suffer from similar biases would not only improve reporting practices in the field, but might also provide a more accurate understanding of the range of potential impact of policies and programs that target the underlying causes of unsafe abortion and abortion-related mortality.",
      "journal": "Population health metrics",
      "year": "2016",
      "doi": "10.1186/s12963-016-0075-3",
      "authors": "Gerdts Caitlin et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/27006645/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC4802921",
      "ft_text_length": 27994,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC4802921)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "27943018",
      "title": "Item bias detection in the Hospital Anxiety and Depression Scale using structural equation modeling: comparison with other item bias detection methods.",
      "abstract": "PURPOSE: Comparison of patient-reported outcomes may be invalidated by the occurrence of item bias, also known as differential item functioning. We show two ways of using structural equation modeling (SEM) to detect item bias: (1) multigroup SEM, which enables the detection of both uniform and nonuniform bias, and (2) multidimensional SEM, which enables the investigation of item bias with respect to several variables simultaneously. METHOD: Gender- and age-related bias in the items of the Hospital Anxiety and Depression Scale (HADS; Zigmond and Snaith in Acta Psychiatr Scand 67:361-370, 1983) from a sample of 1068 patients was investigated using the multigroup SEM approach and the multidimensional SEM approach. Results were compared to the results of the ordinal logistic regression, item response theory, and contingency tables methods reported by Cameron et al. (Qual Life Res 23:2883-2888, 2014). RESULTS: Both SEM approaches identified two items with gender-related bias and two items with age-related bias in the Anxiety subscale, and four items with age-related bias in the Depression subscale. Results from the SEM approaches generally agreed with the results of Cameron et al., although the SEM approaches identified more items as biased. CONCLUSION: SEM provides a flexible tool for the investigation of item bias in health-related questionnaires. Multidimensional SEM has practical and statistical advantages over multigroup SEM, and over other item bias detection methods, as it enables item bias detection with respect to multiple variables, of various measurement levels, and with more statistical power, ultimately providing more valid comparisons of patients' well-being in both research and clinical practice.",
      "journal": "Quality of life research : an international journal of quality of life aspects of treatment, care and rehabilitation",
      "year": "2017",
      "doi": "10.1007/s11136-016-1469-1",
      "authors": "Verdam Mathilde G E et al.",
      "keywords": "Depression Scale; Differential item functioning; Hospital Anxiety; Item bias; Structural equation modeling",
      "mesh_terms": "Adult; Age Factors; Aged; Anxiety; Bias; Depression; Female; Humans; Logistic Models; Male; Middle Aged; Patient Reported Outcome Measures; Quality of Life; Sex Factors; Surveys and Questionnaires",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/27943018/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC5420371",
      "ft_text_length": 46668,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC5420371)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "29312464",
      "title": "Correcting Classifiers for Sample Selection Bias in Two-Phase Case-Control Studies.",
      "abstract": "Epidemiological studies often utilize stratified data in which rare outcomes or exposures are artificially enriched. This design can increase precision in association tests but distorts predictions when applying classifiers on nonstratified data. Several methods correct for this so-called sample selection bias, but their performance remains unclear especially for machine learning classifiers. With an emphasis on two-phase case-control studies, we aim to assess which corrections to perform in which setting and to obtain methods suitable for machine learning techniques, especially the random forest. We propose two new resampling-based methods to resemble the original data and covariance structure: stochastic inverse-probability oversampling and parametric inverse-probability bagging. We compare all techniques for the random forest and other classifiers, both theoretically and on simulated and real data. Empirical results show that the random forest profits from only the parametric inverse-probability bagging proposed by us. For other classifiers, correction is mostly advantageous, and methods perform uniformly. We discuss consequences of inappropriate distribution assumptions and reason for different behaviors between the random forest and other classifiers. In conclusion, we provide guidance for choosing correction methods when training classifiers on biased samples. For random forests, our method outperforms state-of-the-art procedures if distribution assumptions are roughly fulfilled. We provide our implementation in the R package sambia.",
      "journal": "Computational and mathematical methods in medicine",
      "year": "2017",
      "doi": "10.1155/2017/7847531",
      "authors": "Krautenbacher Norbert et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Case-Control Studies; Computer Simulation; Hepatitis; Humans; Machine Learning; Patient Selection; Research Design; Selection Bias; Software",
      "pub_types": "Journal Article; Randomized Controlled Trial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29312464/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC5632994",
      "ft_text_length": 51396,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC5632994)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "30288421",
      "title": "Efficient and Fair Heart Allocation Policies for Transplantation.",
      "abstract": "Background: The optimal allocation of limited donated hearts to patients on the waiting list is one of the top priorities in heart transplantation management. We developed a simulation model of the US waiting list for heart transplantation to investigate the potential impacts of allocation policies on several outcomes such as pre- and posttransplant mortality. Methods: We used data from the United Network for Organ Sharing (UNOS) and the Scientific Registry of Transplant Recipient (SRTR) to simulate the heart allocation system. The model is validated by comparing the outcomes of the simulation with historical data. We also adapted fairness schemes studied in welfare economics to provide a framework to assess the fairness of allocation policies for transplantation. We considered three allocation policies, each a modification to the current UNOS allocation policy, and analyzed their performance via simulation. The first policy broadens the geographical allocation zones, the second modifies the health status order for receiving hearts, and the third prioritizes patients according to their waiting time. Results: Our results showed that the allocation policy similar to the current UNOS practice except that it aggregates the three immediate geographical allocation zones, improves the health outcomes, and is \"closer\" to an optimal fair policy compared to all other policies considered in this study. Specifically, this policy could have saved 319 total deaths (out of 3738 deaths) during the 2006 to 2014 time horizon, in average. This policy slightly differs from the current UNOS allocation policy and allows for easy implementation. Conclusion: We developed a model to compare the outcomes of heart allocation policies. Combining the three immediate geographical zones in the current allocation algorithm could potentially reduce mortality rate and is closer to an optimal fair policy.",
      "journal": "MDM policy & practice",
      "year": "2017",
      "doi": "10.1177/2381468317709475",
      "authors": "Hasankhani Farhad et al.",
      "keywords": "allocation policy; fairness; heart failure; simulation; survival; transplantation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30288421/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC6125046",
      "ft_text_length": 40567,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC6125046)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "31138828",
      "title": "Rapid discrimination of multiple myeloma patients by artificial neural networks coupled with mass spectrometry of peripheral blood plasma.",
      "abstract": "Multiple myeloma (MM) is a highly heterogeneous disease of malignant plasma cells. Diagnosis and monitoring of MM patients is based on bone marrow biopsies and detection of abnormal immunoglobulin in serum and/or urine. However, biopsies have a single-site bias; thus, new diagnostic tests and early detection strategies are needed. Matrix-Assisted Laser Desorption/Ionization Time-of Flight Mass Spectrometry (MALDI-TOF MS) is a powerful method that found its applications in clinical diagnostics. Artificial intelligence approaches, such as Artificial Neural Networks (ANNs), can handle non-linear data and provide prediction and classification of variables in multidimensional datasets. In this study, we used MALDI-TOF MS to acquire low mass profiles of peripheral blood plasma obtained from MM patients and healthy donors. Informative patterns in mass spectra served as inputs for ANN that specifically predicted MM samples with high sensitivity (100%), specificity (95%) and accuracy (98%). Thus, mass spectrometry coupled with ANN can provide a minimally invasive approach for MM diagnostics.",
      "journal": "Scientific reports",
      "year": "2019",
      "doi": "10.1038/s41598-019-44215-1",
      "authors": "Deulofeu Meritxell et al.",
      "keywords": "",
      "mesh_terms": "Aged; Aged, 80 and over; Artificial Intelligence; Bone Marrow; Case-Control Studies; Datasets as Topic; Female; Humans; Immunoglobulins; Male; Metabolic Networks and Pathways; Metabolome; Middle Aged; Multiple Myeloma; Neural Networks, Computer; Principal Component Analysis; Spectrometry, Mass, Matrix-Assisted Laser Desorption-Ionization",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31138828/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC6538619",
      "ft_text_length": 17865,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC6538619)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "31795063",
      "title": "Bias-adjustment in neuroimaging-based brain age frameworks: A robust scheme.",
      "abstract": "The level of prediction error in the brain age estimation frameworks is associated with the authenticity of statistical inference on the basis of regression models. In this paper, we present an efficacious and plain bias-adjustment scheme using chronological age as a covariate through the training set for downgrading the prediction bias in a Brain-age estimation framework. We applied proposed bias-adjustment scheme coupled by a machine learning-based brain age framework on a large set of metabolic brain features acquired from 675 cognitively unimpaired adults through fluorodeoxyglucose positron emission tomography data as the training set to build a robust Brain-age estimation framework. Then, we tested the reliability of proposed bias-adjustment scheme on 75 cognitively unimpaired adults, 561 mild cognitive impairment patients as well as 362 Alzheimer's disease patients as independent test sets. Using the proposed method, we gained a strong R2 of 0.81 between the chronological age and brain estimated age, as well as an excellent mean absolute error of 2.66 years on 75 cognitively unimpaired adults as an independent set; whereas an R2 of 0.24 and a mean absolute error of 4.71 years was achieved without bias-adjustment. The simulation results demonstrated that the proposed bias-adjustment scheme has a strong capability to diminish prediction error in brain age estimation frameworks for clinical settings.",
      "journal": "NeuroImage. Clinical",
      "year": "2019",
      "doi": "10.1016/j.nicl.2019.102063",
      "authors": "Beheshti Iman et al.",
      "keywords": "Bias-adjustment; Brain age; Brain metabolism; Estimation; Pet",
      "mesh_terms": "Aged; Aged, 80 and over; Brain; Female; Humans; Image Processing, Computer-Assisted; Machine Learning; Male; Middle Aged; Neuroimaging; Positron-Emission Tomography",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31795063/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC6861562",
      "ft_text_length": 25157,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC6861562)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "32188481",
      "title": "A geographic identifier assignment algorithm with Bayesian variable selection to identify neighborhood factors associated with emergency department visit disparities for asthma.",
      "abstract": "BACKGROUND: Ecologic health studies often rely on outcomes from health service utilization data that are limited by relatively coarse spatial resolutions and missing geographic information, particularly neighborhood level identifiers. When fine-scale geographic data are missing, the ramifications and strategies for addressing them are not well researched or developed. This study illustrates a novel spatio-temporal framework that combines a geographic identifier assignment (i.e., geographic imputation) algorithm with predictive Bayesian variable selection to identify neighborhood factors associated with disparities in emergency department (ED) visits for asthma. METHODS: ED visit records with missing fine-scale spatial identifiers (~\u200920%) were geocoded using information from known, coarser, misaligned spatial units using an innovative geographic identifier assignment algorithm. We then employed systematic variable selection in a spatio-temporal Bayesian hierarchical model (BHM) predictive framework within the NIMBLE package in R. Our novel methodology is illustrated in an ecologic case study aimed at identifying neighborhood-level predictors of asthma ED visits in South Carolina, United States, from 1999 to 2015. The health outcome was annual ED visit counts in small areas (i.e., census tracts) with primary diagnoses of asthma (ICD9 codes 493.XX) among children ages 5 to 19\u00a0years. RESULTS: We maintained 96% of ED visit records for this analysis. When the algorithm used areal proportions as probabilities for assignment, which addressed differential missingness of census tract identifiers in rural areas, variable selection consistently identified significant neighborhood-level predictors of asthma ED visit risk including pharmacy proximity, average household size, and carbon monoxide interactions. Contrasted with common solutions of removing geographically incomplete records or scaling up analyses, our methodology identified critical differences in parameters estimated, predictors selected, and inferences. We posit that the differences were attributable to improved data resolution, resulting in greater power and less bias. Importantly, without this methodology, we would have inaccurately identified predictors of risk for asthma ED visits, particularly in rural areas. CONCLUSIONS: Our approach innovatively addressed several issues in ecologic health studies, including missing small-area geographic information, multiple correlated neighborhood covariates, and multiscale unmeasured confounding factors. Our methodology could be widely applied to other small-area studies, useful to a range of researchers throughout the world.",
      "journal": "International journal of health geographics",
      "year": "2020",
      "doi": "10.1186/s12942-020-00203-7",
      "authors": "Bozigar Matthew et al.",
      "keywords": "Air pollution; Bayesian spatio-temporal modeling; Geographic imputation; Hospitalization record data; Respiratory diseases; Rural health; SEA-AIR Study; Social determinants of health; Urban health",
      "mesh_terms": "Adolescent; Algorithms; Asthma; Bayes Theorem; Child; Child, Preschool; Emergency Service, Hospital; Geographic Information Systems; Geography; Health Status Disparities; Humans; Residence Characteristics; South Carolina; Young Adult",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32188481/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7081565",
      "ft_text_length": 52041,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC7081565)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "32607482",
      "title": "Stigma, biomarkers, and algorithmic bias: recommendations for precision behavioral health with artificial intelligence.",
      "abstract": "Effective implementation of artificial intelligence in behavioral healthcare delivery depends on overcoming challenges that are pronounced in this domain. Self and social stigma contribute to under-reported symptoms, and under-coding worsens ascertainment. Health disparities contribute to algorithmic bias. Lack of reliable biological and clinical markers hinders model development, and model explainability challenges impede trust among users. In this perspective, we describe these challenges and discuss design and implementation recommendations to overcome them in intelligent systems for behavioral and mental health.",
      "journal": "JAMIA open",
      "year": "2020",
      "doi": "10.1093/jamiaopen/ooz054",
      "authors": "Walsh Colin G et al.",
      "keywords": "artificial intelligence; behavioral health; ethics; health disparities, algorithms,\u00a0mental health; precision medicine; predictive modeling",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32607482/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7309258",
      "ft_text_length": 25039,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC7309258)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "32671340",
      "title": "Towards accurate and unbiased imaging-based differentiation of Parkinson's disease, progressive supranuclear palsy and corticobasal syndrome.",
      "abstract": "The early and accurate differential diagnosis of parkinsonian disorders is still a significant challenge for clinicians. In recent years, a number of studies have used magnetic resonance imaging data combined with machine learning and statistical classifiers to successfully differentiate between different forms of Parkinsonism. However, several questions and methodological issues remain, to minimize bias and artefact-driven classification. In this study, we compared different approaches for feature selection, as well as different magnetic resonance imaging modalities, with well-matched patient groups and tightly controlling for data quality issues related to patient motion. Our sample was drawn from a cohort of 69 healthy controls, and patients with idiopathic Parkinson's disease (n\u2009=\u200935), progressive supranuclear palsy Richardson's syndrome (n\u2009=\u200952) and corticobasal syndrome (n\u2009=\u200936). Participants underwent standardized T1-weighted and diffusion-weighted magnetic resonance imaging. Strict data quality control and group matching reduced the control and patient numbers to 43, 32, 33 and 26, respectively. We compared two different methods for feature selection and dimensionality reduction: whole-brain principal components analysis, and an anatomical region-of-interest based approach. In both cases, support vector machines were used to construct a statistical model for pairwise classification of healthy controls and patients. The accuracy of each model was estimated using a leave-two-out cross-validation approach, as well as an independent validation using a different set of subjects. Our cross-validation results suggest that using principal components analysis for feature extraction provides higher classification accuracies when compared to a region-of-interest based approach. However, the differences between the two feature extraction methods were significantly reduced when an independent sample was used for validation, suggesting that the principal components analysis approach may be more vulnerable to overfitting with cross-validation. Both T1-weighted and diffusion magnetic resonance imaging data could be used to successfully differentiate between subject groups, with neither modality outperforming the other across all pairwise comparisons in the cross-validation analysis. However, features obtained from diffusion magnetic resonance imaging data resulted in significantly higher classification accuracies when an independent validation cohort was used. Overall, our results support the use of statistical classification approaches for differential diagnosis of parkinsonian disorders. However, classification accuracy can be affected by group size, age, sex and movement artefacts. With appropriate controls and out-of-sample cross validation, diagnostic biomarker evaluation including magnetic resonance imaging based classifiers may be an important adjunct to clinical evaluation.",
      "journal": "Brain communications",
      "year": "2020",
      "doi": "10.1093/braincomms/fcaa051",
      "authors": "Correia Marta M et al.",
      "keywords": "Parkinson\u2019s disease; corticobasal degeneration syndrome; magnetic resonance imaging; progressive supranuclear palsy; support vector machine",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32671340/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7325838",
      "ft_text_length": 50864,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC7325838)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "32805004",
      "title": "Bias at warp speed: how AI may contribute to the disparities gap in the time of COVID-19.",
      "abstract": "The COVID-19 pandemic is presenting a disproportionate impact on minorities in terms of infection rate, hospitalizations, and mortality. Many believe artificial intelligence (AI) is a solution to guide clinical decision-making for this novel disease, resulting in the rapid dissemination of underdeveloped and potentially biased models, which may exacerbate the disparities gap. We believe there is an urgent need to enforce the systematic use of reporting standards and develop regulatory frameworks for a shared COVID-19 data source to address the challenges of bias in AI during this pandemic. There is hope that AI can help guide treatment decisions within this crisis; yet given the pervasiveness of biases, a failure to proactively develop comprehensive mitigation strategies during the COVID-19 pandemic risks exacerbating existing health disparities.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2021",
      "doi": "10.1093/jamia/ocaa210",
      "authors": "R\u00f6\u00f6sli Eliane et al.",
      "keywords": "COVID; artificial intelligence; bias; disparities, reporting standards\u00a0",
      "mesh_terms": "Artificial Intelligence; Bias; COVID-19; Clinical Decision-Making; Health Status Disparities; Healthcare Disparities; Humans; Information Storage and Retrieval; Minority Groups; Resource Allocation; United States",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32805004/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7454645",
      "ft_text_length": 8726,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC7454645)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "32838979",
      "title": "Bias and ethical considerations in machine learning and the automation of perioperative risk assessment.",
      "abstract": "",
      "journal": "British journal of anaesthesia",
      "year": "2020",
      "doi": "10.1016/j.bja.2020.07.040",
      "authors": "O'Reilly-Shah Vikas N et al.",
      "keywords": "artificial intelligence; gender bias; healthcare inequality; machine learning; perioperative medicine; racial bias; risk prediction",
      "mesh_terms": "Anesthesiology; Automation; Bias; Humans; Machine Learning; Perioperative Care; Risk Assessment",
      "pub_types": "Editorial; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32838979/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7442146",
      "ft_text_length": 18499,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC7442146)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "33090117",
      "title": "A Racially Unbiased, Machine Learning Approach to Prediction of Mortality: Algorithm Development Study.",
      "abstract": "BACKGROUND: Racial disparities in health care are well documented in the United States. As machine learning methods become more common in health care settings, it is important to ensure that these methods do not contribute to racial disparities through biased predictions or differential accuracy across racial groups. OBJECTIVE: The goal of the research was to assess a machine learning algorithm intentionally developed to minimize bias in in-hospital mortality predictions between white and nonwhite patient groups. METHODS: Bias was minimized through preprocessing of algorithm training data. We performed a retrospective analysis of electronic health record data from patients admitted to the intensive care unit (ICU) at a large academic health center between 2001 and 2012, drawing data from the Medical Information Mart for Intensive Care-III database. Patients were included if they had at least 10 hours of available measurements after ICU admission, had at least one of every measurement used for model prediction, and had recorded race/ethnicity data. Bias was assessed through the equal opportunity difference. Model performance in terms of bias and accuracy was compared with the Modified Early Warning Score (MEWS), the Simplified Acute Physiology Score II (SAPS II), and the Acute Physiologic Assessment and Chronic Health Evaluation (APACHE). RESULTS: The machine learning algorithm was found to be more accurate than all comparators, with a higher sensitivity, specificity, and area under the receiver operating characteristic. The machine learning algorithm was found to be unbiased (equal opportunity difference 0.016, P=.20). APACHE was also found to be unbiased (equal opportunity difference 0.019, P=.11), while SAPS II and MEWS were found to have significant bias (equal opportunity difference 0.038, P=.006 and equal opportunity difference 0.074, P<.001, respectively). CONCLUSIONS: This study indicates there may be significant racial bias in commonly used severity scoring systems and that machine learning algorithms may reduce bias while improving on the accuracy of these methods.",
      "journal": "JMIR public health and surveillance",
      "year": "2020",
      "doi": "10.2196/22400",
      "authors": "Allen Angier et al.",
      "keywords": "health disparities; machine learning; mortality; prediction; racial disparities",
      "mesh_terms": "APACHE; Adult; Aged; Algorithms; Cohort Studies; Early Warning Score; Electronic Health Records; Female; Forecasting; Hospital Mortality; Humans; Machine Learning; Male; Middle Aged; Retrospective Studies; Simplified Acute Physiology Score",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33090117/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7644374",
      "ft_text_length": 24561,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC7644374)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "33220494",
      "title": "An empirical characterization of fair machine learning for clinical risk prediction.",
      "abstract": "The use of machine learning to guide clinical decision making has the potential to worsen existing health disparities. Several recent works frame the problem as that of algorithmic fairness, a framework that has attracted considerable attention and criticism. However, the appropriateness of this framework is unclear due to both ethical as well as technical considerations, the latter of which include trade-offs between measures of fairness and model performance that are not well-understood for predictive models of clinical outcomes. To inform the ongoing debate, we conduct an empirical study to characterize the impact of penalizing group fairness violations on an array of measures of model performance and group fairness. We repeat the analysis across multiple observational healthcare databases, clinical outcomes, and sensitive attributes. We find that procedures that penalize differences between the distributions of predictions across groups induce nearly-universal degradation of multiple performance metrics within groups. On examining the secondary impact of these procedures, we observe heterogeneity of the effect of these procedures on measures of fairness in calibration and ranking across experimental conditions. Beyond the reported trade-offs, we emphasize that analyses of algorithmic fairness in healthcare lack the contextual grounding and causal awareness necessary to reason about the mechanisms that lead to health disparities, as well as about the potential of algorithmic fairness methods to counteract those mechanisms. In light of these limitations, we encourage researchers building predictive models for clinical use to step outside the algorithmic fairness frame and engage critically with the broader sociotechnical context surrounding the use of machine learning in healthcare.",
      "journal": "Journal of biomedical informatics",
      "year": "2021",
      "doi": "10.1016/j.jbi.2020.103621",
      "authors": "Pfohl Stephen R et al.",
      "keywords": "Algorithmic fairness; Bias; Clinical risk prediction",
      "mesh_terms": "Delivery of Health Care; Empirical Research; Machine Learning",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33220494/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7871979",
      "ft_text_length": 1815,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC7871979)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "33236066",
      "title": "Addressing bias in prediction models by improving subpopulation calibration.",
      "abstract": "OBJECTIVE: To illustrate the problem of subpopulation miscalibration, to adapt an algorithm for recalibration of the predictions, and to validate its performance. MATERIALS AND METHODS: In this retrospective cohort study, we evaluated the calibration of predictions based on the Pooled Cohort Equations (PCE) and the fracture risk assessment tool (FRAX) in the overall population and in subpopulations defined by the intersection of age, sex, ethnicity, socioeconomic status, and immigration history. We next applied the recalibration algorithm and assessed the change in calibration metrics, including calibration-in-the-large. RESULTS: 1\u00a0021\u00a0041 patients were included in the PCE population, and 1\u00a0116\u00a0324 patients were included in the FRAX population. Baseline overall model calibration of the 2 tested models was good, but calibration in a substantial portion of the subpopulations was poor. After applying the algorithm, subpopulation calibration statistics were greatly improved, with the variance of the calibration-in-the-large values across all subpopulations reduced by 98.8% and 94.3% in the PCE and FRAX models, respectively. DISCUSSION: Prediction models in medicine are increasingly common. Calibration, the agreement between predicted and observed risks, is commonly poor for subpopulations that were underrepresented in the development set of the models, resulting in bias and reduced performance for these subpopulations. In this work, we empirically evaluated an adapted version of the fairness algorithm designed by Hebert-Johnson et al. (2017) and demonstrated its use in improving subpopulation miscalibration. CONCLUSION: A postprocessing and model-independent fairness algorithm for recalibration of predictive models greatly decreases the bias of subpopulation miscalibration and thus increases fairness and equality.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2021",
      "doi": "10.1093/jamia/ocaa283",
      "authors": "Barda Noam et al.",
      "keywords": "Predictive\u00a0; models, algorithmic fairness, calibration, model bias, cardiovascular disease, osteoporosis",
      "mesh_terms": "Adult; Aged; Algorithms; Bias; Female; Humans; Male; Middle Aged; Models, Statistical; Multivariate Analysis; Prognosis; Proportional Hazards Models; Retrospective Studies; Risk Assessment",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33236066/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7936516",
      "ft_text_length": 1845,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC7936516)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "33459685",
      "title": "COVID-19 diagnosis from chest X-ray images using transfer learning: Enhanced performance by debiasing dataloader.",
      "abstract": "BACKGROUND: Chest X-ray imaging has been proved as a powerful diagnostic method to detect and diagnose COVID-19 cases due to its easy accessibility, lower cost and rapid imaging time. OBJECTIVE: This study aims to improve efficacy of screening COVID-19 infected patients using chest X-ray images with the help of a developed deep convolutional neural network model (CNN) entitled nCoV-NET. METHODS: To train and to evaluate the performance of the developed model, three datasets were collected from resources of \"ChestX-ray14\", \"COVID-19 image data collection\", and \"Chest X-ray collection from Indiana University,\" respectively. Overall, 299 COVID-19 pneumonia cases and 1,522 non-COVID 19 cases are involved in this study. To overcome the probable bias due to the unbalanced cases in two classes of the datasets, ResNet, DenseNet, and VGG architectures were re-trained in the fine-tuning stage of the process to distinguish COVID-19 classes using a transfer learning method. Lastly, the optimized final nCoV-NET model was applied to the testing dataset to verify the performance of the proposed model. RESULTS: Although the performance parameters of all re-trained architectures were determined close to each other, the final nCOV-NET model optimized by using DenseNet-161 architecture in the transfer learning stage exhibits the highest performance for classification of COVID-19 cases with the accuracy of 97.1 %. The Activation Mapping method was used to create activation maps that highlights the crucial areas of the radiograph to improve causality and intelligibility. CONCLUSION: This study demonstrated that the proposed CNN model called nCoV-NET can be utilized for reliably detecting COVID-19 cases using chest X-ray images to accelerate the triaging and save critical time for disease control as well as assisting the radiologist to validate their initial diagnosis.",
      "journal": "Journal of X-ray science and technology",
      "year": "2021",
      "doi": "10.3233/XST-200757",
      "authors": "Polat \u00c7a\u011f\u00edn et al.",
      "keywords": "Coronavirus (COVID-19) infection; chest X-ray images; convolutional neural network (CNN); deep learning; transfer learning",
      "mesh_terms": "Algorithms; COVID-19; Deep Learning; Early Diagnosis; Humans; Neural Networks, Computer; Pneumonia; Radiography, Thoracic; Reproducibility of Results; SARS-CoV-2; Tomography, X-Ray Computed",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33459685/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7990426",
      "ft_text_length": 38551,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC7990426)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "33501466",
      "title": "Identifying bias in models that detect vocal fold paralysis from audio recordings using explainable machine learning and clinician ratings.",
      "abstract": "INTRODUCTION: Detecting voice disorders from voice recordings could allow for frequent, remote, and low-cost screening before costly clinical visits and a more invasive laryngoscopy examination. Our goals were to detect unilateral vocal fold paralysis (UVFP) from voice recordings using machine learning, to identify which acoustic variables were important for prediction to increase trust, and to determine model performance relative to clinician performance. METHODS: Patients with confirmed UVFP through endoscopic examination (N=77) and controls with normal voices matched for age and sex (N=77) were included. Voice samples were elicited by reading the Rainbow Passage and sustaining phonation of the vowel \"a\". Four machine learning models of differing complexity were used. SHapley Additive explanations (SHAP) was used to identify important features. RESULTS: The highest median bootstrapped ROC AUC score was 0.87 and beat clinician's performance (range: 0.74 - 0.81) based on the recordings. Recording durations were different between UVFP recordings and controls due to how that data was originally processed when storing, which we can show can classify both groups. And counterintuitively, many UVFP recordings had higher intensity than controls, when UVFP patients tend to have weaker voices, revealing a dataset-specific bias which we mitigate in an additional analysis. CONCLUSION: We demonstrate that recording biases in audio duration and intensity created dataset-specific differences between patients and controls, which models used to improve classification. Furthermore, clinician's ratings provide further evidence that patients were over-projecting their voices and being recorded at a higher amplitude signal than controls. Interestingly, after matching audio duration and removing variables associated with intensity in order to mitigate the biases, the models were able to achieve a similar high performance. We provide a set of recommendations to avoid bias when building and evaluating machine learning models for screening in laryngology.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2024",
      "doi": "10.1101/2020.11.23.20235945",
      "authors": "Low Daniel M et al.",
      "keywords": "acoustic analysis; bias; explainability; interpretability; machine learning; speech; vocal fold paralysis; voice",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33501466/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7836138",
      "ft_text_length": 50403,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC7836138)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "33568741",
      "title": "Systematic auditing is essential to debiasing machine learning in biology.",
      "abstract": "Biases in data used to train machine learning (ML) models can inflate their prediction performance and confound our understanding of how and what they learn. Although biases are common in biological data, systematic auditing of ML models to identify and eliminate these biases is not a common practice when applying ML in the life sciences. Here we devise a systematic, principled, and general approach to audit ML models in the life sciences. We use this auditing framework to examine biases in three ML applications of therapeutic interest and identify unrecognized biases that hinder the ML process and result in substantially reduced model performance on new datasets. Ultimately, we show that ML models tend to learn primarily from data biases when there is insufficient signal in the data to learn from. We provide detailed protocols, guidelines, and examples of code to enable tailoring of the auditing framework to other biomedical applications.",
      "journal": "Communications biology",
      "year": "2021",
      "doi": "10.1038/s42003-021-01674-5",
      "authors": "Eid Fatma-Elzahraa et al.",
      "keywords": "",
      "mesh_terms": "Animals; Bias; Data Mining; Databases, Protein; Histocompatibility Antigens; Humans; Machine Learning; Pharmaceutical Preparations; Protein Binding; Protein Interaction Maps; Proteins; Proteome; Proteomics; Reproducibility of Results",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33568741/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7876113",
      "ft_text_length": 42362,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC7876113)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "33713239",
      "title": "Towards a pragmatist dealing with algorithmic bias in medical machine learning.",
      "abstract": "Machine Learning (ML) is on the rise in medicine, promising improved diagnostic, therapeutic and prognostic clinical tools. While these technological innovations are bound to transform health care, they also bring new ethical concerns to the forefront. One particularly elusive challenge regards discriminatory algorithmic judgements based on biases inherent in the training data. A common line of reasoning distinguishes between justified differential treatments that mirror true disparities between socially salient groups, and unjustified biases which do not, leading to misdiagnosis and erroneous treatment. In the curation of training data this strategy runs into severe problems though, since distinguishing between the two can be next to impossible. We thus plead for a pragmatist dealing with algorithmic bias in healthcare environments. By recurring to a recent reformulation of William James's pragmatist understanding of truth, we recommend that, instead of aiming at a supposedly objective truth, outcome-based therapeutic usefulness should serve as the guiding principle for assessing ML applications in medicine.",
      "journal": "Medicine, health care, and philosophy",
      "year": "2021",
      "doi": "10.1007/s11019-021-10008-5",
      "authors": "Starke Georg et al.",
      "keywords": "Algorithmic bias; Artificial intelligence; Fairness; Machine learning; Philosophy of Science; Pragmatism",
      "mesh_terms": "Bias; Delivery of Health Care; Education, Medical; Humans; Machine Learning; Morals",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33713239/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7955212",
      "ft_text_length": 33907,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC7955212)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "33856478",
      "title": "Comparison of Methods to Reduce Bias From Clinical Prediction Models of Postpartum Depression.",
      "abstract": "IMPORTANCE: The lack of standards in methods to reduce bias for clinical algorithms presents various challenges in providing reliable predictions and in addressing health disparities. OBJECTIVE: To evaluate approaches for reducing bias in machine learning models using a real-world clinical scenario. DESIGN, SETTING, AND PARTICIPANTS: Health data for this cohort study were obtained from the IBM MarketScan Medicaid Database. Eligibility criteria were as follows: (1) Female individuals aged 12 to 55 years with a live birth record identified by delivery-related codes from January 1, 2014, through December 31, 2018; (2) greater than 80% enrollment through pregnancy to 60 days post partum; and (3) evidence of coverage for depression screening and mental health services. Statistical analysis was performed in 2020. EXPOSURES: Binarized race (Black individuals and White individuals). MAIN OUTCOMES AND MEASURES: Machine learning models (logistic regression [LR], random forest, and extreme gradient boosting) were trained for 2 binary outcomes: postpartum depression (PPD) and postpartum mental health service utilization. Risk-adjusted generalized linear models were used for each outcome to assess potential disparity in the cohort associated with binarized race (Black or White). Methods for reducing bias, including reweighing, Prejudice Remover, and removing race from the models, were examined by analyzing changes in fairness metrics compared with the base models. Baseline characteristics of female individuals at the top-predicted risk decile were compared for systematic differences. Fairness metrics of disparate impact (DI, 1 indicates fairness) and equal opportunity difference (EOD, 0 indicates fairness). RESULTS: Among 573\u202f634 female individuals initially examined for this study, 314\u202f903 were White (54.9%), 217\u202f899 were Black (38.0%), and the mean (SD) age was 26.1 (5.5) years. The risk-adjusted odds ratio comparing White participants with Black participants was 2.06 (95% CI, 2.02-2.10) for clinically recognized PPD and 1.37 (95% CI, 1.33-1.40) for postpartum mental health service utilization. Taking the LR model for PPD prediction as an example, reweighing reduced bias as measured by improved DI and EOD metrics from 0.31 and -0.19 to 0.79 and 0.02, respectively. Removing race from the models had inferior performance for reducing bias compared with the other methods (PPD: DI\u2009=\u20090.61; EOD\u2009=\u2009-0.05; mental health service utilization: DI\u2009=\u20090.63; EOD\u2009=\u2009-0.04). CONCLUSIONS AND RELEVANCE: Clinical prediction models trained on potentially biased data may produce unfair outcomes on the basis of the chosen metrics. This study's results suggest that the performance varied depending on the model, outcome label, and method for reducing bias. This approach toward evaluating algorithmic bias can be used as an example for the growing number of researchers who wish to examine and address bias in their data and models.",
      "journal": "JAMA network open",
      "year": "2021",
      "doi": "10.1001/jamanetworkopen.2021.3909",
      "authors": "Park Yoonyoung et al.",
      "keywords": "",
      "mesh_terms": "Adolescent; Adult; Algorithms; Cohort Studies; Depression, Postpartum; Female; Humans; Middle Aged; Models, Statistical; Odds Ratio; Patient-Specific Modeling; Postpartum Period; Pregnancy; Prognosis; Retrospective Studies; Risk Assessment; Risk Factors; United States; Young Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33856478/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8050742",
      "ft_text_length": 29225,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8050742)",
      "ft_reason": "Included: bias central + approach content (11 indicators)"
    },
    {
      "pmid": "33981989",
      "title": "Addressing Fairness, Bias, and Appropriate Use of Artificial Intelligence and Machine Learning in Global Health.",
      "abstract": "In Low- and Middle- Income Countries (LMICs), machine learning (ML) and artificial intelligence (AI) offer attractive solutions to address the shortage of health care resources and improve the capacity of the local health care infrastructure. However, AI and ML should also be used cautiously, due to potential issues of fairness and algorithmic bias that may arise if not applied properly. Furthermore, populations in LMICs can be particularly vulnerable to bias and fairness in AI algorithms, due to a lack of technical capacity, existing social bias against minority groups, and a lack of legal protections. In order to address the need for better guidance within the context of global health, we describe three basic criteria (Appropriateness, Fairness, and Bias) that can be used to help evaluate the use of machine learning and AI systems: 1) APPROPRIATENESS is the process of deciding how the algorithm should be used in the local context, and properly matching the machine learning model to the target population; 2) BIAS is a systematic tendency in a model to favor one demographic group vs another, which can be mitigated but can lead to unfairness; and 3) FAIRNESS involves examining the impact on various demographic groups and choosing one of several mathematical definitions of group fairness that will adequately satisfy the desired set of legal, cultural, and ethical requirements. Finally, we illustrate how these principles can be applied using a case study of machine learning applied to the diagnosis and screening of pulmonary disease in Pune, India. We hope that these methods and principles can help guide researchers and organizations working in global health who are considering the use of machine learning and artificial intelligence.",
      "journal": "Frontiers in artificial intelligence",
      "year": "2020",
      "doi": "10.3389/frai.2020.561802",
      "authors": "Fletcher Richard Rib\u00f3n et al.",
      "keywords": "appropriate use; artificial intelligence; bias; ethics; fairness; global health; machine learning; medicine",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33981989/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8107824",
      "ft_text_length": 65620,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8107824)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "34012993",
      "title": "A Joint Fairness Model with Applications to Risk Predictions for Under-represented Populations.",
      "abstract": "In data collection for predictive modeling, under-representation of certain groups, based on gender, race/ethnicity, or age, may yield less-accurate predictions for these groups. Recently, this issue of fairness in predictions has attracted significant attention, as data-driven models are increasingly utilized to perform crucial decision-making tasks. Existing methods to achieve fairness in the machine learning literature typically build a single prediction model in a manner that encourages fair prediction performance for all groups. These approaches have two major limitations: i) fairness is often achieved by compromising accuracy for some groups; ii) the underlying relationship between dependent and independent variables may not be the same across groups. We propose a Joint Fairness Model (JFM) approach for logistic regression models for binary outcomes that estimates group-specific classifiers using a joint modeling objective function that incorporates fairness criteria for prediction. We introduce an Accelerated Smoothing Proximal Gradient Algorithm to solve the convex objective function, and present the key asymptotic properties of the JFM estimates. Through simulations, we demonstrate the efficacy of the JFM in achieving good prediction performance and across-group parity, in comparison with the single fairness model, group-separate model, and group-ignorant model, especially when the minority group's sample size is small. Finally, we demonstrate the utility of the JFM method in a real-world example to obtain fair risk predictions for under-represented older patients diagnosed with coronavirus disease 2019 (COVID-19).",
      "journal": "ArXiv",
      "year": "2021",
      "doi": "",
      "authors": "Do Hyungrok et al.",
      "keywords": "algorithmic bias; algorithmic fairness; joint estimation; under-represented population",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34012993/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8132236",
      "ft_text_length": 42119,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8132236)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "34083673",
      "title": "Automatic and unbiased segmentation and quantification of myofibers in skeletal muscle.",
      "abstract": "Skeletal muscle has the remarkable ability to regenerate. However, with age and disease muscle strength and function decline. Myofiber size, which is affected by injury and disease, is a critical measurement to assess muscle health. Here, we test and apply Cellpose, a recently developed deep learning algorithm, to automatically segment myofibers within murine skeletal muscle. We first show that tissue fixation is necessary to preserve cellular structures such as primary cilia, small cellular antennae, and adipocyte lipid droplets. However, fixation generates heterogeneous myofiber labeling, which impedes intensity-based segmentation. We demonstrate that Cellpose efficiently delineates thousands of individual myofibers outlined by a variety of markers, even within fixed tissue with highly uneven myofiber staining. We created a novel ImageJ plugin (LabelsToRois) that allows processing\u00a0of multiple Cellpose segmentation images in batch. The plugin also contains a semi-automatic erosion function to correct for the area bias introduced by the different stainings, thereby\u00a0identifying myofibers as accurately as human experts. We successfully applied our segmentation pipeline to uncover myofiber regeneration differences between two different muscle injury models, cardiotoxin and glycerol. Thus, Cellpose combined with LabelsToRois allows for fast, unbiased, and reproducible myofiber quantification for a variety of staining and fixation conditions.",
      "journal": "Scientific reports",
      "year": "2021",
      "doi": "10.1038/s41598-021-91191-6",
      "authors": "Waisman Ariel et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Animals; Computational Biology; Histocytochemistry; Image Processing, Computer-Assisted; Mice; Microscopy; Muscle Fibers, Skeletal; Muscle, Skeletal; Software",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34083673/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8175575",
      "ft_text_length": 46364,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8175575)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "34132919",
      "title": "Letter to the editor: \"Not all biases are bad: equitable and inequitable biases in machine learning and radiology\".",
      "abstract": "Artificial intelligence algorithms are booming in medicine, and the question of biases induced or perpetuated by these tools is a very important topic. There is a greater risk of these biases in radiology, which is now the primary diagnostic tool in modern treatment. Some authors have recently proposed an analysis framework for social inequalities and the biases at risk of being introduced into future algorithms. In our paper, we comment on the different strategies for resolving these biases. We warn that there is an even greater risk in mixing the notion of equity, the definition of which is socio-political, into the design stages of these algorithms. We believe that rather than being beneficial, this could in fact harm the main purpose of these artificial intelligence tools, which is the care of the patient.",
      "journal": "Insights into imaging",
      "year": "2021",
      "doi": "10.1186/s13244-021-01022-5",
      "authors": "Iannessi Antoine et al.",
      "keywords": "Artificial intelligence; Bias; Inequity",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34132919/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8208365",
      "ft_text_length": 7601,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8208365)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "34222857",
      "title": "Gender Bias in the News: A Scalable Topic Modelling and Visualization Framework.",
      "abstract": "We present a topic modelling and data visualization methodology to examine gender-based disparities in news articles by topic. Existing research in topic modelling is largely focused on the text mining of closed corpora, i.e., those that include a fixed collection of composite texts. We showcase a methodology to discover topics via Latent Dirichlet Allocation, which can reliably produce human-interpretable topics over an open news corpus that continually grows with time. Our system generates topics, or distributions of keywords, for news articles on a monthly basis, to consistently detect key events and trends aligned with events in the real world. Findings from 2 years worth of news articles in mainstream English-language Canadian media indicate that certain topics feature either women or men more prominently and exhibit different types of language. Perhaps unsurprisingly, topics such as lifestyle, entertainment, and healthcare tend to be prominent in articles that quote more women than men. Topics such as sports, politics, and business are characteristic of articles that quote more men than women. The data shows a self-reinforcing gendered division of duties and representation in society. Quoting female sources more frequently in a caregiving role and quoting male sources more frequently in political and business roles enshrines women's status as caregivers and men's status as leaders and breadwinners. Our results can help journalists and policy makers better understand the unequal gender representation of those quoted in the news and facilitate news organizations' efforts to achieve gender parity in their sources. The proposed methodology is robust, reproducible, and scalable to very large corpora, and can be used for similar studies involving unsupervised topic modelling and language analyses.",
      "journal": "Frontiers in artificial intelligence",
      "year": "2021",
      "doi": "10.3389/frai.2021.664737",
      "authors": "Rao Prashanth et al.",
      "keywords": "corpus linguistics; gender bias; machine learning; natural language processing; news media; topic modelling",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34222857/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8242240",
      "ft_text_length": 66280,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8242240)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "34278590",
      "title": "Optimized bias and signal inference in diffusion-weighted image analysis (OBSIDIAN).",
      "abstract": "PURPOSE: Correction of Rician signal bias in magnitude MR images. METHODS: A model-based, iterative fitting procedure is used to simultaneously estimate true signal and underlying Gaussian noise with standard deviation \u03c3g on a pixel-by-pixel basis in magnitude MR images. A precomputed function that relates absolute residuals between measured signals and model fit to \u03c3g is used to iteratively estimate \u03c3g . The feasibility of the method is evaluated and compared to maximum likelihood estimation (MLE) for diffusion signal decay simulations and diffusion-weighted images of the prostate considering 21 linearly spaced b-values from 0 to 3000\u00a0s/mm2 . A multidirectional analysis was performed with publically available brain data. RESULTS: Model simulations show that the Rician bias correction algorithm is fast, with an accuracy and precision that is on par to model-based MLE and direct fitting in the case of pure Gaussian noise. Increased accuracy in parameter prediction in a low signal-to-noise ratio (SNR) scenario is ideally achieved by using a composite of multiple signal decays from neighboring voxels as input for the algorithm. For patient data, good agreement with high SNR reference data of diffusion in prostate is achieved. CONCLUSIONS: OBSIDIAN is a novel, alternative, simple to implement approach for rapid Rician bias correction applicable in any case where differences between true signal decay and underlying model function can be considered negligible in comparison to noise. The proposed composite fitting approach permits accurate parameter estimation even in typical clinical scenarios with low SNR, which significantly simplifies comparison of complex diffusion parameters among studies.",
      "journal": "Magnetic resonance in medicine",
      "year": "2021",
      "doi": "10.1002/mrm.28773",
      "authors": "Kuczera Stefan et al.",
      "keywords": "Rician bias correction; diffusion MRI; noise; prostate",
      "mesh_terms": "Algorithms; Brain; Diffusion; Diffusion Magnetic Resonance Imaging; Humans; Image Processing, Computer-Assisted; Normal Distribution; Signal-To-Noise Ratio",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34278590/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9009782",
      "ft_text_length": 1727,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9009782)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "34285218",
      "title": "The impact of site-specific digital histology signatures on deep learning model accuracy and bias.",
      "abstract": "The Cancer Genome Atlas (TCGA) is one of the largest biorepositories of digital histology. Deep learning (DL) models have been trained on TCGA to predict numerous features directly from histology, including survival, gene expression patterns, and driver mutations. However, we demonstrate that these features vary substantially across tissue submitting sites in TCGA for over 3,000 patients with six cancer subtypes. Additionally, we show that histologic image differences between submitting sites can easily be identified with DL. Site detection remains possible despite commonly used color normalization and augmentation methods, and we quantify the image characteristics constituting this site-specific digital histology signature. We demonstrate that these site-specific signatures lead to biased accuracy for prediction of features including survival, genomic mutations, and tumor stage. Furthermore, ethnicity can also be inferred from site-specific signatures, which must be accounted for to ensure equitable application of DL. These site-specific signatures can lead to overoptimistic estimates of model performance, and we propose a quadratic programming method that abrogates this bias by ensuring models are not trained and validated on samples from the same site.",
      "journal": "Nature communications",
      "year": "2021",
      "doi": "10.1038/s41467-021-24698-1",
      "authors": "Howard Frederick M et al.",
      "keywords": "",
      "mesh_terms": "Biomarkers, Tumor; DNA Mutational Analysis; Data Accuracy; Deep Learning; Gene Expression Profiling; Humans; Image Processing, Computer-Assisted; Mutation; Neoplasm Staging; Neoplasms; Risk Assessment; Specimen Handling",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34285218/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8292530",
      "ft_text_length": 62926,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8292530)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "34338646",
      "title": "Enabling Wearable Pulse Transit Time-Based Blood Pressure Estimation for Medically Underserved Areas and Health Equity: Comprehensive Evaluation Study.",
      "abstract": "BACKGROUND: Noninvasive and cuffless approaches to monitor blood pressure (BP), in light of their convenience and accuracy, have paved the way toward remote screening and management of hypertension. However, existing noninvasive methodologies, which operate on mechanical, electrical, and optical sensing modalities, have not been thoroughly evaluated in demographically and racially diverse populations. Thus, the potential accuracy of these technologies in populations where they could have the greatest impact has not been sufficiently addressed. This presents challenges in clinical translation due to concerns about perpetuating existing health disparities. OBJECTIVE: In this paper, we aim to present findings on the feasibility of a cuffless, wrist-worn, pulse transit time (PTT)-based device for monitoring BP in a diverse population. METHODS: We recruited a diverse population through a collaborative effort with a nonprofit organization working with medically underserved areas in Georgia. We used our custom, multimodal, wrist-worn device to measure the PTT through seismocardiography, as the proximal timing reference, and photoplethysmography, as the distal timing reference. In addition, we created a novel data-driven beat-selection algorithm to reduce noise and improve the robustness of the method. We compared the wearable PTT measurements with those from a finger-cuff continuous BP device over the course of several perturbations used to modulate BP. RESULTS: Our PTT-based wrist-worn device accurately monitored diastolic blood pressure (DBP) and mean arterial pressure (MAP) in a diverse population (N=44 participants) with a mean absolute difference of 2.90 mm Hg and 3.39 mm Hg for DBP and MAP, respectively, after calibration. Meanwhile, the mean absolute difference of our systolic BP estimation was 5.36 mm Hg, a grade B classification based on the Institute for Electronics and Electrical Engineers standard. We have further demonstrated the ability of our device to capture the commonly observed demographic differences in underlying arterial stiffness. CONCLUSIONS: Accurate DBP and MAP estimation, along with grade B systolic BP estimation, using a convenient wearable device can empower users and facilitate remote BP monitoring in medically underserved areas, thus providing widespread hypertension screening and management for health equity.",
      "journal": "JMIR mHealth and uHealth",
      "year": "2021",
      "doi": "10.2196/27466",
      "authors": "Ganti Venu et al.",
      "keywords": "cuffless blood pressure; health equity; mobile phone; noninvasive blood pressure estimation; pulse transit time; wearable sensing",
      "mesh_terms": "Blood Pressure; Health Equity; Humans; Medically Underserved Area; Pulse Wave Analysis; Wearable Electronic Devices",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34338646/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8369375",
      "ft_text_length": 51761,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8369375)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "34383925",
      "title": "Bias and fairness assessment of a natural language processing opioid misuse classifier: detection and mitigation of electronic health record data disadvantages across racial subgroups.",
      "abstract": "OBJECTIVES: To assess fairness and bias of a previously validated machine learning opioid misuse classifier. MATERIALS & METHODS: Two experiments were conducted with the classifier's original (n\u2009=\u20091000) and external validation (n\u2009=\u200953 974) datasets from 2 health systems. Bias was assessed via testing for differences in type II error rates across racial/ethnic subgroups (Black, Hispanic/Latinx, White, Other) using bootstrapped 95% confidence intervals. A local surrogate model was estimated to interpret the classifier's predictions by race and averaged globally from the datasets. Subgroup analyses and post-hoc recalibrations were conducted to attempt to mitigate biased metrics. RESULTS: We identified bias in the false negative rate (FNR = 0.32) of the Black subgroup compared to the FNR (0.17) of the White subgroup. Top features included \"heroin\" and \"substance abuse\" across subgroups. Post-hoc recalibrations eliminated bias in FNR with minimal changes in other subgroup error metrics. The Black FNR subgroup had higher risk scores for readmission and mortality than the White FNR subgroup, and a higher mortality risk score than the Black true positive subgroup (P\u2009<\u2009.05). DISCUSSION: The Black FNR subgroup had the greatest severity of disease and risk for poor outcomes. Similar features were present between subgroups for predicting opioid misuse, but inequities were present. Post-hoc mitigation techniques mitigated bias in type II error rate without creating substantial type I error rates. From model design through deployment, bias and data disadvantages should be systematically addressed. CONCLUSION: Standardized, transparent bias assessments are needed to improve trustworthiness in clinical machine learning models.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2021",
      "doi": "10.1093/jamia/ocab148",
      "authors": "Thompson Hale M et al.",
      "keywords": "bias and fairness; interpretability; machine learning; natural language processing; opioid use disorder; structural racism",
      "mesh_terms": "Electronic Health Records; Hispanic or Latino; Humans; Machine Learning; Natural Language Processing; Opioid-Related Disorders",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34383925/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8510285",
      "ft_text_length": 30835,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8510285)",
      "ft_reason": "Included: bias central + approach content (10 indicators)"
    },
    {
      "pmid": "34441187",
      "title": "Toward Learning Trustworthily from Data Combining Privacy, Fairness, and Explainability: An Application to Face Recognition.",
      "abstract": "In many decision-making scenarios, ranging from recreational activities to healthcare and policing, the use of artificial intelligence coupled with the ability to learn from historical data is becoming ubiquitous. This widespread adoption of automated systems is accompanied by the increasing concerns regarding their ethical implications. Fundamental rights, such as the ones that require the preservation of privacy, do not discriminate based on sensible attributes (e.g., gender, ethnicity, political/sexual orientation), or require one to provide an explanation for a decision, are daily undermined by the use of increasingly complex and less understandable yet more accurate learning algorithms. For this purpose, in this work, we work toward the development of systems able to ensure trustworthiness by delivering privacy, fairness, and explainability by design. In particular, we show that it is possible to simultaneously learn from data while preserving the privacy of the individuals thanks to the use of Homomorphic Encryption, ensuring fairness by learning a fair representation from the data, and ensuring explainable decisions with local and global explanations without compromising the accuracy of the final models. We test our approach on a widespread but still controversial application, namely face recognition, using the recent FairFace dataset to prove the validity of our approach.",
      "journal": "Entropy (Basel, Switzerland)",
      "year": "2021",
      "doi": "10.3390/e23081047",
      "authors": "Franco Danilo et al.",
      "keywords": "Algorithmic Fairness; Homomorphic Encryption; attention maps; deep neural networks; dimensionality reduction; explainable artificial intelligence; learning fair representation; privacy-preserving machine learning; trustworthy artificial intelligence",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34441187/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8393832",
      "ft_text_length": 57386,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8393832)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "34451100",
      "title": "Explainable Artificial Intelligence for Bias Detection in COVID CT-Scan Classifiers.",
      "abstract": "PROBLEM: An application of Explainable Artificial Intelligence Methods for COVID CT-Scan classifiers is presented. MOTIVATION: It is possible that classifiers are using spurious artifacts in dataset images to achieve high performances, and such explainable techniques can help identify this issue. AIM: For this purpose, several approaches were used in tandem, in order to create a complete overview of the classificatios. METHODOLOGY: The techniques used included GradCAM, LIME, RISE, Squaregrid, and direct Gradient approaches (Vanilla, Smooth, Integrated). MAIN RESULTS: Among the deep neural networks architectures evaluated for this image classification task, VGG16 was shown to be most affected by biases towards spurious artifacts, while DenseNet was notably more robust against them. Further impacts: Results further show that small differences in validation accuracies can cause drastic changes in explanation heatmaps for DenseNet architectures, indicating that small changes in validation accuracy may have large impacts on the biases learned by the networks. Notably, it is important to notice that the strong performance metrics achieved by all these networks (Accuracy, F1 score, AUC all in the 80 to 90% range) could give users the erroneous impression that there is no bias. However, the analysis of the explanation heatmaps highlights the bias.",
      "journal": "Sensors (Basel, Switzerland)",
      "year": "2021",
      "doi": "10.3390/s21165657",
      "authors": "Palatnik de Sousa Iam et al.",
      "keywords": "Computerized Tomography; Covid 19; Explainable AI; computer vision; image classification; medical imaging",
      "mesh_terms": "Artificial Intelligence; Bias; COVID-19; Humans; SARS-CoV-2; Tomography, X-Ray Computed",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34451100/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8402377",
      "ft_text_length": 29832,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8402377)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "34553213",
      "title": "Bias-invariant RNA-sequencing metadata annotation.",
      "abstract": "BACKGROUND: Recent technological advances have resulted in an unprecedented increase in publicly available biomedical data, yet the reuse of the data is often precluded by experimental bias and a lack of annotation depth and consistency. Missing annotations makes it impossible for researchers to find datasets specific to their needs. FINDINGS: Here, we investigate RNA-sequencing metadata prediction based on gene expression values. We present a deep-learning-based domain adaptation algorithm for the automatic annotation of RNA-sequencing metadata. We show, in multiple experiments, that our model is better at integrating heterogeneous training data compared with existing linear regression-based approaches, resulting in improved tissue type classification. By using a model architecture similar to Siamese networks, the algorithm can learn biases from datasets with few samples. CONCLUSION: Using our novel domain adaptation approach, we achieved metadata annotation accuracies up to 15.7% better than a previously published method. Using the best model, we provide a list of >10,000 novel tissue and sex label annotations for 8,495 unique SRA samples. Our approach has the potential to revive idle datasets by automated annotation making them more searchable.",
      "journal": "GigaScience",
      "year": "2021",
      "doi": "10.1093/gigascience/giab064",
      "authors": "Wartmann Hannes et al.",
      "keywords": "RNA-seq metadata; automated annotation; bias invariance; deep learning; computational biology; bioinformatics; data reusability; domain adaptation; machine learning",
      "mesh_terms": "Algorithms; Bias; Metadata; Molecular Sequence Annotation; RNA; Sequence Analysis, RNA",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34553213/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8559615",
      "ft_text_length": 65521,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8559615)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "34568771",
      "title": "Quantifying representativeness in randomized clinical trials using machine learning fairness metrics.",
      "abstract": "OBJECTIVE: We help identify subpopulations underrepresented in randomized clinical trials (RCTs) cohorts with respect to national, community-based or health system target populations by formulating population representativeness of RCTs as a machine learning (ML) fairness problem, deriving new representation metrics, and deploying them in easy-to-understand interactive visualization tools. MATERIALS AND METHODS: We represent RCT cohort enrollment as random binary classification fairness problems, and then show how ML fairness metrics based on enrollment fraction can be efficiently calculated using easily computed rates of subpopulations in RCT cohorts and target populations. We propose standardized versions of these metrics and deploy them in an interactive tool to analyze 3 RCTs with respect to type 2 diabetes and hypertension target populations in the National Health and Nutrition Examination Survey. RESULTS: We demonstrate how the proposed metrics and associated statistics enable users to rapidly examine representativeness of all subpopulations in the RCT defined by a set of categorical traits (eg, gender, race, ethnicity, smoking status, and blood pressure) with respect to target populations. DISCUSSION: The normalized metrics provide an intuitive standardized scale for evaluating representation across subgroups, which may have vastly different enrollment fractions and rates in RCT study cohorts. The metrics are beneficial complements to other approaches (eg, enrollment fractions) used to identify generalizability and health equity of RCTs. CONCLUSION: By quantifying the gaps between RCT and target populations, the proposed methods can support generalizability evaluation of existing RCT cohorts. The interactive visualization tool can be readily applied to identified underrepresented subgroups with respect to any desired source or target populations.",
      "journal": "JAMIA open",
      "year": "2021",
      "doi": "10.1093/jamiaopen/ooab077",
      "authors": "Qi Miao et al.",
      "keywords": "health equity; machine learning; population representativeness; randomized clinical trials; subgroup",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34568771/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8460438",
      "ft_text_length": 37901,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8460438)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "34573790",
      "title": "The Problem of Fairness in Synthetic Healthcare Data.",
      "abstract": "Access to healthcare data such as electronic health records (EHR) is often restricted by laws established to protect patient privacy. These restrictions hinder the reproducibility of existing results based on private healthcare data and also limit new research. Synthetically-generated healthcare data solve this problem by preserving privacy and enabling researchers and policymakers to drive decisions and methods based on realistic data. Healthcare data can include information about multiple in- and out- patient visits of patients, making it a time-series dataset which is often influenced by protected attributes like age, gender, race etc. The COVID-19 pandemic has exacerbated health inequities, with certain subgroups experiencing poorer outcomes and less access to healthcare. To combat these inequities, synthetic data must \"fairly\" represent diverse minority subgroups such that the conclusions drawn on synthetic data are correct and the results can be generalized to real data. In this article, we develop two fairness metrics for synthetic data, and analyze all subgroups defined by protected attributes to analyze the bias in three published synthetic research datasets. These covariate-level disparity metrics revealed that synthetic data may not be representative at the univariate and multivariate subgroup-levels and thus, fairness should be addressed when developing data generation methods. We discuss the need for measuring fairness in synthetic healthcare data to enable the development of robust machine learning models to create more equitable synthetic healthcare datasets.",
      "journal": "Entropy (Basel, Switzerland)",
      "year": "2021",
      "doi": "10.3390/e23091165",
      "authors": "Bhanot Karan et al.",
      "keywords": "covariate; disparate impact; fairness; health inequities; healthcare; synthetic data; temporal; time-series",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34573790/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8468495",
      "ft_text_length": 61998,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8468495)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "34637384",
      "title": "Mix-and-Interpolate: A Training Strategy to Deal With Source-Biased Medical Data.",
      "abstract": "Till March 31st, 2021, the coronavirus disease 2019 (COVID-19) had reportedly infected more than 127 million people and caused over 2.5 million deaths worldwide. Timely diagnosis of COVID-19 is crucial for management of individual patients as well as containment of the highly contagious disease. Having realized the clinical value of non-contrast chest computed tomography (CT) for diagnosis of COVID-19, deep learning (DL) based automated methods have been proposed to aid the radiologists in reading the huge quantities of CT exams as a result of the pandemic. In this work, we address an overlooked problem for training deep convolutional neural networks for COVID-19 classification using real-world multi-source data, namely, the data source bias problem. The data source bias problem refers to the situation in which certain sources of data comprise only a single class of data, and training with such source-biased data may make the DL models learn to distinguish data sources instead of COVID-19. To overcome this problem, we propose MIx-aNd-Interpolate (MINI), a conceptually simple, easy-to-implement, efficient yet effective training strategy. The proposed MINI approach generates volumes of the absent class by combining the samples collected from different hospitals, which enlarges the sample space of the original source-biased dataset. Experimental results on a large collection of real patient data (1,221 COVID-19 and 1,520 negative CT images, and the latter consisting of 786 community acquired pneumonia and 734 non-pneumonia) from eight hospitals and health institutions show that: 1) MINI can improve COVID-19 classification performance upon the baseline (which does not deal with the source bias), and 2) MINI is superior to competing methods in terms of the extent of improvement.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2022",
      "doi": "10.1109/JBHI.2021.3119325",
      "authors": "Li Yuexiang et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; COVID-19; Deep Learning; Humans; Pandemics; SARS-CoV-2",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34637384/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8908883",
      "ft_text_length": 99565,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8908883)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "34679619",
      "title": "Detection Accuracy and Latency of Colorectal Lesions with Computer-Aided Detection System Based on Low-Bias Evaluation.",
      "abstract": "We developed a computer-aided detection (CADe) system to detect and localize colorectal lesions by modifying You-Only-Look-Once version 3 (YOLO v3) and evaluated its performance in two different settings. The test dataset was obtained from 20 randomly selected patients who underwent endoscopic resection for 69 colorectal lesions at the Jikei University Hospital between June 2017 and February 2018. First, we evaluated the diagnostic performances using still images randomly and automatically extracted from video recordings of the entire endoscopic procedure at intervals of 5 s, without eliminating poor quality images. Second, the latency of lesion detection by the CADe system from the initial appearance of lesions was investigated by reviewing the videos. A total of 6531 images, including 662 images with a lesion, were studied in the image-based analysis. The AUC, sensitivity, specificity, positive predictive value, negative predictive value, and accuracy were 0.983, 94.6%, 95.2%, 68.8%, 99.4%, and 95.1%, respectively. The median time for detecting colorectal lesions measured in the lesion-based analysis was 0.67 s. In conclusion, we proved that the originally developed CADe system based on YOLO v3 could accurately and instantaneously detect colorectal lesions using the test dataset obtained from videos, mitigating operator selection biases.",
      "journal": "Diagnostics (Basel, Switzerland)",
      "year": "2021",
      "doi": "10.3390/diagnostics11101922",
      "authors": "Matsui Hiroaki et al.",
      "keywords": "artificial intelligence; colonoscopy; colorectal lesions; computer-aided detection; deep learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34679619/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8534444",
      "ft_text_length": 23068,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8534444)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "34686604",
      "title": "A language-matching model to improve equity and efficiency of COVID-19 contact tracing.",
      "abstract": "Contact tracing is a pillar of COVID-19 response, but language access and equity have posed major obstacles. COVID-19 has disproportionately affected minority communities with many non-English-speaking members. Language discordance can increase processing times and hamper the trust building necessary for effective contact tracing. We demonstrate how matching predicted patient language with contact tracer language can enhance contact tracing. First, we show how to use machine learning to combine information from sparse laboratory reports with richer census data to predict the language of an incoming case. Second, we embed this method in the highly demanding environment of actual contact tracing with high volumes of cases in Santa Clara County, CA. Third, we evaluate this language-matching intervention in a randomized controlled trial. We show that this low-touch intervention results in 1) significant time savings, shortening the time from opening of cases to completion of the initial interview by nearly 14 h and increasing same-day completion by 12%, and 2) improved engagement, reducing the refusal to interview by 4%. These findings have important implications for reducing social disparities in COVID-19; improving equity in healthcare access; and, more broadly, leveling language differences in public services.",
      "journal": "Proceedings of the National Academy of Sciences of the United States of America",
      "year": "2021",
      "doi": "10.1073/pnas.2109443118",
      "authors": "Lu Lisa et al.",
      "keywords": "COVID-19; contact tracing; health equity; language access; machine learning",
      "mesh_terms": "Algorithms; COVID-19; California; Communication Barriers; Contact Tracing; Female; Humans; Language; Machine Learning; Male; Pandemics; SARS-CoV-2; Surveys and Questionnaires; Trust",
      "pub_types": "Journal Article; Randomized Controlled Trial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34686604/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8639369",
      "ft_text_length": 33777,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8639369)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "34812384",
      "title": "Bias Analysis on Public X-Ray Image Datasets of Pneumonia and COVID-19 Patients.",
      "abstract": "Chest X-ray images are useful for early COVID-19 diagnosis with the advantage that X-ray devices are already available in health centers and images are obtained immediately. Some datasets containing X-ray images with cases (pneumonia or COVID-19) and controls have been made available to develop machine-learning-based methods to aid in diagnosing the disease. However, these datasets are mainly composed of different sources coming from pre-COVID-19 datasets and COVID-19 datasets. Particularly, we have detected a significant bias in some of the released datasets used to train and test diagnostic systems, which might imply that the results published are optimistic and may overestimate the actual predictive capacity of the techniques proposed. In this article, we analyze the existing bias in some commonly used datasets and propose a series of preliminary steps to carry out before the classic machine learning pipeline in order to detect possible biases, to avoid them if possible and to report results that are more representative of the actual predictive power of the methods under analysis.",
      "journal": "IEEE access : practical innovations, open solutions",
      "year": "2021",
      "doi": "10.1109/ACCESS.2021.3065456",
      "authors": "Catala Omar Del Tejo et al.",
      "keywords": "COVID-19; Deep learning; bias; chest X-ray; convolutional neural networks; saliency map; segmentation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34812384/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8545228",
      "ft_text_length": 43773,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8545228)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "34944430",
      "title": "xDEEP-MSI: Explainable Bias-Rejecting Microsatellite Instability Deep Learning System in Colorectal Cancer.",
      "abstract": "The prediction of microsatellite instability (MSI) using deep learning (DL) techniques could have significant benefits, including reducing cost and increasing MSI testing of colorectal cancer (CRC) patients. Nonetheless, batch effects or systematic biases are not well characterized in digital histology models and lead to overoptimistic estimates of model performance. Methods to not only palliate but to directly abrogate biases are needed. We present a multiple bias rejecting DL system based on adversarial networks for the prediction of MSI in CRC from tissue microarrays (TMAs), trained and validated in 1788 patients from EPICOLON and HGUA. The system consists of an end-to-end image preprocessing module that tile samples at multiple magnifications and a tissue classification module linked to the bias-rejecting MSI predictor. We detected three biases associated with the learned representations of a baseline model: the project of origin of samples, the patient's spot and the TMA glass where each spot was placed. The system was trained to directly avoid learning the batch effects of those variables. The learned features from the bias-ablated model achieved maximum discriminative power with respect to the task and minimal statistical mean dependence with the biases. The impact of different magnifications, types of tissues and the model performance at tile vs patient level is analyzed. The AUC at tile level, and including all three selected tissues (tumor epithelium, mucin and lymphocytic regions) and 4 magnifications, was 0.87 \u00b1 0.03 and increased to 0.9 \u00b1 0.03 at patient level. To the best of our knowledge, this is the first work that incorporates a multiple bias ablation technique at the DL architecture in digital pathology, and the first using TMAs for the MSI prediction task.",
      "journal": "Biomolecules",
      "year": "2021",
      "doi": "10.3390/biom11121786",
      "authors": "Bustos Aurelia et al.",
      "keywords": "adversarial networks; bias ablation; colorectal carcinoma; deep neural networks; digital pathology; microsatellite instability",
      "mesh_terms": "Adult; Aged; Aged, 80 and over; Algorithms; Bias; Biomarkers, Tumor; Colorectal Neoplasms; Computational Biology; Deep Learning; Female; Humans; Male; Microsatellite Instability; Middle Aged; Tissue Array Analysis",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34944430/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8699085",
      "ft_text_length": 45119,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8699085)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "35016819",
      "title": "Investigation of biases in convolutional neural networks for semantic segmentation using performance sensitivity analysis.",
      "abstract": "The application of deep neural networks for segmentation in medical imaging has gained substantial interest in recent years. In many cases, this variant of machine learning has been shown to outperform other conventional segmentation approaches. However, little is known about its general applicability. Especially the robustness against image modifications (e.g., intensity variations, contrast variations, spatial alignment) has hardly been investigated. Data augmentation is often used to compensate for sensitivity to such changes, although its effectiveness has not yet been studied. Therefore, the goal of this study was to systematically investigate the sensitivity to variations in input data with respect to segmentation of medical images using deep learning. This approach was tested with two publicly available segmentation frameworks (DeepMedic and TractSeg). In the case of DeepMedic, the performance was tested using ground truth data, while in the case of TractSeg, the STAPLE technique was employed. In both cases, sensitivity analysis revealed significant dependence of the segmentation performance on input variations. The effects of different data augmentation strategies were also shown, making this type of analysis a useful tool for selecting the right parameters for augmentation. The proposed analysis should be applied to any deep learning image segmentation approach, unless the assessment of sensitivity to input variations can be directly derived from the network.",
      "journal": "Zeitschrift fur medizinische Physik",
      "year": "2022",
      "doi": "10.1016/j.zemedi.2021.11.004",
      "authors": "G\u00fcllmar Daniel et al.",
      "keywords": "Convolutional neural network; Data augmentation; Semantic image segmentation; Sensitivity analysis",
      "mesh_terms": "Bias; Image Processing, Computer-Assisted; Machine Learning; Neural Networks, Computer; Semantics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35016819/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9948839",
      "ft_text_length": 42572,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9948839)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "35024857",
      "title": "Considerations for development of child abuse and neglect phenotype with implications for reduction of racial bias: a qualitative study.",
      "abstract": "OBJECTIVE: The study provides considerations for generating a phenotype of child abuse and neglect in Emergency Departments (ED) using secondary data from electronic health records (EHR). Implications will be provided for racial bias reduction and the development of further decision support tools to assist in identifying child abuse and neglect. MATERIALS AND METHODS: We conducted a qualitative study using in-depth interviews with 20 pediatric clinicians working in a single pediatric ED to gain insights about generating an EHR-based phenotype to identify children at risk for abuse and neglect. RESULTS: Three central themes emerged from the interviews: (1) Challenges in diagnosing child abuse and neglect, (2) Health Discipline Differences in Documentation Styles in EHR, and (3) Identification of potential racial bias through documentation. DISCUSSION: Our findings highlight important considerations for generating a phenotype for child abuse and neglect using EHR data. First, information-related challenges include lack of proper previous visit history due to limited information exchanges and scattered documentation within EHRs. Second, there are differences in documentation styles by health disciplines, and clinicians tend to document abuse in different document types within EHRs. Finally, documentation can help identify potential racial bias in suspicion of child abuse and neglect by revealing potential discrepancies in quality of care, and in the language used to document abuse and neglect. CONCLUSIONS: Our findings highlight challenges in building an EHR-based risk phenotype for child abuse and neglect. Further research is needed to validate these findings and integrate them into creation of an EHR-based risk phenotype.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2022",
      "doi": "10.1093/jamia/ocab275",
      "authors": "Landau Aviv Y et al.",
      "keywords": "child abuse and neglect; clinical decision support tool; electronic health records; pediatric emergency departments; racial bias",
      "mesh_terms": "Child; Child Abuse; Documentation; Electronic Health Records; Humans; Phenotype; Qualitative Research; Racism",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35024857/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8800508",
      "ft_text_length": 1754,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8800508)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "35075160",
      "title": "Peeking into a black box, the fairness and generalizability of a MIMIC-III benchmarking model.",
      "abstract": "As artificial intelligence (AI) makes continuous progress to improve quality of care for some patients by leveraging ever increasing amounts of digital health data, others are left behind. Empirical evaluation studies are required to keep biased AI models from reinforcing systemic health disparities faced by minority populations through dangerous feedback loops. The aim of this study is to raise broad awareness of the pervasive challenges around bias and fairness in risk prediction models. We performed a case study on a MIMIC-trained benchmarking model using a broadly applicable fairness and generalizability assessment framework. While open-science benchmarks are crucial to overcome many study limitations today, this case study revealed a strong class imbalance problem as well as fairness concerns for Black and publicly insured ICU patients. Therefore, we advocate for the widespread use of comprehensive fairness and performance assessment frameworks to effectively monitor and validate benchmark pipelines built on open data resources.",
      "journal": "Scientific data",
      "year": "2022",
      "doi": "10.1038/s41597-021-01110-7",
      "authors": "R\u00f6\u00f6sli Eliane et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35075160/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8786878",
      "ft_text_length": 51322,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8786878)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "35110547",
      "title": "A robust method for collider bias correction in conditional genome-wide association studies.",
      "abstract": "Estimated genetic associations with prognosis, or conditional on a phenotype (e.g. disease incidence), may be affected by collider bias, whereby conditioning on the phenotype induces associations between causes of the phenotype and prognosis. We propose a method, 'Slope-Hunter', that uses model-based clustering to identify and utilise the class of variants only affecting the phenotype to estimate the adjustment factor, assuming this class explains more variation in the phenotype than any other variant classes. Simulation studies show that our approach eliminates the bias and outperforms alternatives even in the presence of genetic correlation. In a study of fasting blood insulin levels (FI) conditional on body mass index, we eliminate paradoxical associations of the underweight loci: COBLLI; PPARG with increased FI, and reveal an association for the locus rs1421085 (FTO). In an analysis of a case-only study for breast cancer mortality, a single region remains associated with more pronounced results.",
      "journal": "Nature communications",
      "year": "2022",
      "doi": "10.1038/s41467-022-28119-9",
      "authors": "Mahmoud Osama et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Alpha-Ketoglutarate-Dependent Dioxygenase FTO; Bias; Body Mass Index; Breast Neoplasms; Computational Biology; Fasting; Genetic Predisposition to Disease; Genome-Wide Association Study; Humans; Machine Learning; Phenotype; Polymorphism, Single Nucleotide; Risk Factors",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35110547/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8810923",
      "ft_text_length": 96721,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8810923)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "35259009",
      "title": "Eliminating unintended bias in personalized policies using bias-eliminating adapted trees (BEAT).",
      "abstract": "SignificanceDecision makers now use algorithmic personalization for resource allocation decisions in many domains (e.g., medical treatments, hiring decisions, product recommendations, or dynamic pricing). An inherent risk of personalization is disproportionate targeting of individuals from certain protected groups. Existing solutions that firms use to avoid this bias often do not eliminate the bias and may even exacerbate it. We propose BEAT (bias-eliminating adapted trees) to ensure balanced allocation of resources across individuals-guaranteeing both group and individual fairness-while still leveraging the value of personalization. We validate our method using simulations as well as an online experiment with N = 3,146 participants. BEAT is easy to implement in practice, has desirable scalability properties, and is applicable to many personalization problems.",
      "journal": "Proceedings of the National Academy of Sciences of the United States of America",
      "year": "2022",
      "doi": "10.1073/pnas.2115293119",
      "authors": "Ascarza Eva et al.",
      "keywords": "algorithmic bias; discrimination; fairness; personalization; targeting",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35259009/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8931211",
      "ft_text_length": 44798,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8931211)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "35396246",
      "title": "Resampling to address inequities in predictive modeling of suicide deaths.",
      "abstract": "OBJECTIVE: Improve methodology for equitable suicide death prediction when using sensitive predictors, such as race/ethnicity, for machine learning and statistical methods. METHODS: Train predictive models, logistic regression, naive Bayes, gradient boosting (XGBoost) and random forests, using three resampling techniques (Blind, Separate, Equity) on emergency department (ED) administrative patient records. The Blind method resamples without considering racial/ethnic group. Comparatively, the Separate method trains disjoint models for each group and the Equity method builds a training set that is balanced both by racial/ethnic group and by class. RESULTS: Using the Blind method, performance range of the models' sensitivity for predicting suicide death between racial/ethnic groups (a measure of prediction inequity) was 0.47 for logistic regression, 0.37 for naive Bayes, 0.56 for XGBoost and 0.58 for random forest. By building separate models for different racial/ethnic groups or using the equity method on the training set, we decreased the range in performance to 0.16, 0.13, 0.19, 0.20 with Separate method, and 0.14, 0.12, 0.24, 0.13 for Equity method, respectively. XGBoost had the highest overall area under the curve (AUC), ranging from 0.69 to 0.79. DISCUSSION: We increased performance equity between different racial/ethnic groups and show that imbalanced training sets lead to models with poor predictive equity. These methods have comparable AUC scores to other work in the field, using only single ED administrative record data. CONCLUSION: We propose two methods to improve equity of suicide death prediction among different racial/ethnic groups. These methods may be applied to other sensitive characteristics to improve equity in machine learning with healthcare applications.",
      "journal": "BMJ health & care informatics",
      "year": "2022",
      "doi": "10.1136/bmjhci-2021-100456",
      "authors": "Reeves Majerle et al.",
      "keywords": "Data Science; Decision Trees; Machine Learning",
      "mesh_terms": "Area Under Curve; Bayes Theorem; Delivery of Health Care; Humans; Machine Learning; Suicide",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35396246/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8996002",
      "ft_text_length": 25535,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8996002)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "35396247",
      "title": "Evaluating algorithmic fairness in the presence of clinical guidelines: the case of atherosclerotic cardiovascular disease risk estimation.",
      "abstract": "OBJECTIVES: The American College of Cardiology and the American Heart Association guidelines on primary prevention of atherosclerotic cardiovascular disease (ASCVD) recommend using 10-year ASCVD risk estimation models to initiate statin treatment. For guideline-concordant decision-making, risk estimates need to be calibrated. However, existing models are often miscalibrated for race, ethnicity and sex based subgroups. This study evaluates two algorithmic fairness approaches to adjust the risk estimators (group recalibration and equalised odds) for their compatibility with the assumptions underpinning the guidelines' decision rules.MethodsUsing an updated pooled cohorts data set, we derive unconstrained, group-recalibrated and equalised odds-constrained versions of the 10-year ASCVD risk estimators, and compare their calibration at guideline-concordant decision thresholds. RESULTS: We find that, compared with the unconstrained model, group-recalibration improves calibration at one of the relevant thresholds for each group, but exacerbates differences in false positive and false negative rates between groups. An equalised odds constraint, meant to equalise error rates across groups, does so by miscalibrating the model overall and at relevant decision thresholds. DISCUSSION: Hence, because of induced miscalibration, decisions guided by risk estimators learned with an equalised odds fairness constraint are not concordant with existing guidelines. Conversely, recalibrating the model separately for each group can increase guideline compatibility, while increasing intergroup differences in error rates. As such, comparisons of error rates across groups can be misleading when guidelines recommend treating at fixed decision thresholds. CONCLUSION: The illustrated tradeoffs between satisfying a fairness criterion and retaining guideline compatibility underscore the need to evaluate models in the context of downstream interventions.",
      "journal": "BMJ health & care informatics",
      "year": "2022",
      "doi": "10.1136/bmjhci-2021-100460",
      "authors": "Foryciarz Agata et al.",
      "keywords": "BMJ Health Informatics; clinical; decision support systems; health equity; machine learning; medical informatics",
      "mesh_terms": "American Heart Association; Atherosclerosis; Cardiology; Cardiovascular Diseases; Humans; Hydroxymethylglutaryl-CoA Reductase Inhibitors; United States",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35396247/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8996004",
      "ft_text_length": 26173,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8996004)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "35396454",
      "title": "Cohort design and natural language processing to reduce bias in electronic health records research.",
      "abstract": "Electronic health record (EHR) datasets are statistically powerful but are subject to ascertainment bias and missingness. Using the Mass General Brigham multi-institutional EHR, we approximated a community-based cohort by sampling patients receiving longitudinal primary care between 2001-2018 (Community Care Cohort Project [C3PO], n\u2009=\u2009520,868). We utilized natural language processing (NLP) to recover vital signs from unstructured notes. We assessed the validity of C3PO by deploying established risk models for myocardial infarction/stroke and atrial fibrillation. We then compared C3PO to Convenience Samples including all individuals from the same EHR with complete data, but without a longitudinal primary care requirement. NLP reduced the missingness of vital signs by 31%. NLP-recovered vital signs were highly correlated with values derived from structured fields (Pearson r range 0.95-0.99). Atrial fibrillation and myocardial infarction/stroke incidence were lower and risk models were better calibrated in C3PO as opposed to the Convenience Samples (calibration error range for myocardial infarction/stroke: 0.012-0.030 in C3PO vs. 0.028-0.046 in Convenience Samples; calibration error for atrial fibrillation 0.028 in C3PO vs. 0.036 in Convenience Samples). Sampling patients receiving regular primary care and using NLP to recover missing data may reduce bias and maximize generalizability of EHR research.",
      "journal": "NPJ digital medicine",
      "year": "2022",
      "doi": "10.1038/s41746-022-00590-0",
      "authors": "Khurshid Shaan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35396454/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8993873",
      "ft_text_length": 59304,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8993873)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "35463778",
      "title": "Fairness in Cardiac Magnetic Resonance Imaging: Assessing Sex and Racial Bias in Deep Learning-Based Segmentation.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) techniques have been proposed for automation of cine CMR segmentation for functional quantification. However, in other applications AI models have been shown to have potential for sex and/or racial bias. The objective of this paper is to perform the first analysis of sex/racial bias in AI-based cine CMR segmentation using a large-scale database. METHODS: A state-of-the-art deep learning (DL) model was used for automatic segmentation of both ventricles and the myocardium from cine short-axis CMR. The dataset consisted of end-diastole and end-systole short-axis cine CMR images of 5,903 subjects from the UK Biobank database (61.5 \u00b1 7.1 years, 52% male, 81% white). To assess sex and racial bias, we compared Dice scores and errors in measurements of biventricular volumes and function between patients grouped by race and sex. To investigate whether segmentation bias could be explained by potential confounders, a multivariate linear regression and ANCOVA were performed. RESULTS: Results on the overall population showed an excellent agreement between the manual and automatic segmentations. We found statistically significant differences in Dice scores between races (white \u223c94% vs. minority ethnic groups 86-89%) as well as in absolute/relative errors in volumetric and functional measures, showing that the AI model was biased against minority racial groups, even after correction for possible confounders. The results of a multivariate linear regression analysis showed that no covariate could explain the Dice score bias between racial groups. However, for the Mixed and Black race groups, sex showed a weak positive association with the Dice score. The results of an ANCOVA analysis showed that race was the main factor that can explain the overall difference in Dice scores between racial groups. CONCLUSION: We have shown that racial bias can exist in DL-based cine CMR segmentation models when training with a database that is sex-balanced but not race-balanced such as the UK Biobank.",
      "journal": "Frontiers in cardiovascular medicine",
      "year": "2022",
      "doi": "10.3389/fcvm.2022.859310",
      "authors": "Puyol-Ant\u00f3n Esther et al.",
      "keywords": "cardiac magnetic resonance; deep learning; fair AI; inequality fairness in deep learning-based CMR segmentation; segmentation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35463778/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9021445",
      "ft_text_length": 39931,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9021445)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "35504931",
      "title": "Interpretability and fairness evaluation of deep learning models on MIMIC-IV dataset.",
      "abstract": "The recent release of large-scale healthcare datasets has greatly propelled the research of data-driven deep learning models for healthcare applications. However, due to the nature of such deep black-boxed models, concerns about interpretability, fairness, and biases in healthcare scenarios where human lives are at stake call for a careful and thorough examination of both datasets and models. In this work, we focus on MIMIC-IV (Medical Information Mart for Intensive Care, version IV), the largest publicly available healthcare dataset, and conduct comprehensive analyses of interpretability as well as dataset representation bias and prediction fairness of deep learning models for in-hospital mortality prediction. First, we analyze the interpretability of deep learning mortality prediction models and observe that (1) the best-performing interpretability method successfully identifies critical features for mortality prediction on various prediction models as well as recognizing new important features that domain knowledge does not consider; (2) prediction models rely on demographic features, raising concerns in fairness. Therefore, we then evaluate the fairness of models and do observe the unfairness: (1) there exists disparate treatment in prescribing mechanical ventilation among patient groups across ethnicity, gender and age; (2) models often rely on racial attributes unequally across subgroups to generate their predictions. We further draw concrete connections between interpretability methods and fairness metrics by showing how feature importance from interpretability methods can be beneficial in quantifying potential disparities in mortality predictors. Our analysis demonstrates that the prediction performance is not the only factor to consider when evaluating models for healthcare applications, since high prediction performance might be the result of unfair utilization of demographic features. Our findings suggest that future research in AI models for healthcare applications can benefit from utilizing the analysis workflow of interpretability and fairness as well as verifying if models achieve superior performance at the cost of introducing bias.",
      "journal": "Scientific reports",
      "year": "2022",
      "doi": "10.1038/s41598-022-11012-2",
      "authors": "Meng Chuizheng et al.",
      "keywords": "",
      "mesh_terms": "Benchmarking; Critical Care; Deep Learning; Forecasting; Hospital Mortality; Humans",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35504931/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9065125",
      "ft_text_length": 79564,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9065125)",
      "ft_reason": "Included: bias central + approach content (12 indicators)"
    },
    {
      "pmid": "35511151",
      "title": "An objective framework for evaluating unrecognized bias in medical AI models predicting COVID-19 outcomes.",
      "abstract": "OBJECTIVE: The increasing translation of artificial intelligence (AI)/machine learning (ML) models into clinical practice brings an increased risk of direct harm from modeling bias; however, bias remains incompletely measured in many medical AI applications. This article aims to provide a framework for objective evaluation of medical AI from multiple aspects, focusing on binary classification models. MATERIALS AND METHODS: Using data from over 56\u00a0000 Mass General Brigham (MGB) patients with confirmed severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), we evaluate unrecognized bias in 4 AI models developed during the early months of the pandemic in Boston, Massachusetts that predict risks of hospital admission, ICU admission, mechanical ventilation, and death after a SARS-CoV-2 infection purely based on their pre-infection longitudinal medical records. Models were evaluated both retrospectively and prospectively using model-level metrics of discrimination, accuracy, and reliability, and a novel individual-level metric for error. RESULTS: We found inconsistent instances of model-level bias in the prediction models. From an individual-level aspect, however, we found most all models performing with slightly higher error rates for older patients. DISCUSSION: While a model can be biased against certain protected groups (ie, perform worse) in certain tasks, it can be at the same time biased towards another protected group (ie, perform better). As such, current bias evaluation studies may lack a full depiction of the variable effects of a model on its subpopulations. CONCLUSION: Only a holistic evaluation, a diligent search for unrecognized bias, can provide enough information for an unbiased judgment of AI bias that can invigorate follow-up investigations on identifying the underlying roots of bias and ultimately make a change.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2022",
      "doi": "10.1093/jamia/ocac070",
      "authors": "Estiri Hossein et al.",
      "keywords": "COVID-19; bias; electronic health records; medical AI; predictive model",
      "mesh_terms": "Artificial Intelligence; COVID-19; Humans; Reproducibility of Results; Retrospective Studies; SARS-CoV-2",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35511151/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9277645",
      "ft_text_length": 25130,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9277645)",
      "ft_reason": "Included: bias central + approach content (9 indicators)"
    },
    {
      "pmid": "35652218",
      "title": "Artificial Intelligence and Machine Learning Technologies in Cancer Care: Addressing Disparities, Bias, and Data Diversity.",
      "abstract": "Artificial intelligence (AI) and machine learning (ML) technologies have not only tremendous potential to augment clinical decision-making and enhance quality care and precision medicine efforts, but also the potential to worsen existing health disparities without a thoughtful, transparent, and inclusive approach that includes addressing bias in their design and implementation along the cancer discovery and care continuum. We discuss applications of AI/ML tools in cancer and provide recommendations for addressing and mitigating potential bias with AI and ML technologies while promoting cancer health equity.",
      "journal": "Cancer discovery",
      "year": "2022",
      "doi": "10.1158/2159-8290.CD-22-0373",
      "authors": "Dankwa-Mullan Irene et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Humans; Machine Learning; Neoplasms; Precision Medicine",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35652218/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9662931",
      "ft_text_length": 20235,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9662931)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "35656120",
      "title": "Control of Variance and Bias in CT Image Processing with Variational Training of Deep Neural Networks.",
      "abstract": "Optimization of CT image quality typically involves balancing noise and bias. In filtered back-projection, this trade-off is controlled by the particular filter and cutoff frequency. In penalized-likelihood iterative reconstruction, the penalty weight serves the same function. Deep neural networks typically do not provide this tuneable control over output image properties. Models are often trained to minimize mean squared error which penalizes both variance and bias in image outputs but does not offer any control over the trade-off between the two. In this work, we propose a method for controlling the output image properties of neural networks with a new loss function called weighted covariance and bias (WCB). Our proposed method includes separate weighting parameters to control the relative importance of noise or bias reduction. Moreover, we show that tuning these weights enables targeted penalization of specific image features (e.g. spatial frequencies). To evaluate our method, we present a simulation study using digital anthropormorphic phantoms, physical simulation of non-ideal CT data, and image formation with various algorithms. We show that WCB offers a greater degree of control over trade-offs between variance and bias whereas MSE has only one configuration. We also show that WCB can be used to control specific image properties including variance, bias, spatial resolution, and the noise correlation of neural network outputs. Finally, we present a method to optimize the proposed weights for stimulus detectability. Our results demonstrate the potential for this new capability to control the image properties of DNN outputs and optimize image quality for the task-specific applications.",
      "journal": "Proceedings of SPIE--the International Society for Optical Engineering",
      "year": "2022",
      "doi": "10.1117/12.2612417",
      "authors": "Tivnan Matthew et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35656120/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9157378",
      "ft_text_length": 1718,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9157378)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "35699997",
      "title": "Fairness in Mobile Phone-Based Mental Health Assessment Algorithms: Exploratory Study.",
      "abstract": "BACKGROUND: Approximately 1 in 5 American adults experience mental illness every year. Thus, mobile phone-based mental health prediction apps that use phone data and artificial intelligence techniques for mental health assessment have become increasingly important and are being rapidly developed. At the same time, multiple artificial intelligence-related technologies (eg, face recognition and search results) have recently been reported to be biased regarding age, gender, and race. This study moves this discussion to a new domain: phone-based mental health assessment algorithms. It is important to ensure that such algorithms do not contribute to gender disparities through biased predictions across gender groups. OBJECTIVE: This research aimed to analyze the susceptibility of multiple commonly used machine learning approaches for gender bias in mobile mental health assessment and explore the use of an algorithmic disparate impact remover (DIR) approach to reduce bias levels while maintaining high accuracy. METHODS: First, we performed preprocessing and model training using the data set (N=55) obtained from a previous study. Accuracy levels and differences in accuracy across genders were computed using 5 different machine learning models. We selected the random forest model, which yielded the highest accuracy, for a more detailed audit and computed multiple metrics that are commonly used for fairness in the machine learning literature. Finally, we applied the DIR approach to reduce bias in the mental health assessment algorithm. RESULTS: The highest observed accuracy for the mental health assessment was 78.57%. Although this accuracy level raises optimism, the audit based on gender revealed that the performance of the algorithm was statistically significantly different between the male and female groups (eg, difference in accuracy across genders was 15.85%; P<.001). Similar trends were obtained for other fairness metrics. This disparity in performance was found to reduce significantly after the application of the DIR approach by adapting the data used for modeling (eg, the difference in accuracy across genders was 1.66%, and the reduction is statistically significant with P<.001). CONCLUSIONS: This study grounds the need for algorithmic auditing in phone-based mental health assessment algorithms and the use of gender as a protected attribute to study fairness in such settings. Such audits and remedial steps are the building blocks for the widespread adoption of fair and accurate mental health assessment algorithms in the future.",
      "journal": "JMIR formative research",
      "year": "2022",
      "doi": "10.2196/34366",
      "authors": "Park Jinkyung et al.",
      "keywords": "algorithmic bias; gender bias; health equity; health information systems; medical informatics; mental health; mobile phone",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35699997/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9240929",
      "ft_text_length": 35754,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9240929)",
      "ft_reason": "Included: bias central + approach content (9 indicators)"
    },
    {
      "pmid": "35756093",
      "title": "Machine Learning Models to Predict In-Hospital Mortality among Inpatients with COVID-19: Underestimation and Overestimation Bias Analysis in Subgroup Populations.",
      "abstract": "Prediction of the death among COVID-19 patients can help healthcare providers manage the patients better. We aimed to develop machine learning models to predict in-hospital death among these patients. We developed different models using different feature sets and datasets developed using the data balancing method. We used demographic and clinical data from a multicenter COVID-19 registry. We extracted 10,657 records for confirmed patients with PCR or CT scans, who were hospitalized at least for 24 hours at the end of March 2021. The death rate was 16.06%. Generally, models with 60 and 40 features performed better. Among the 240 models, the C5 models with 60 and 40 features performed well. The C5 model with 60 features outperformed the rest based on all evaluation metrics; however, in external validation, C5 with 32 features performed better. This model had high accuracy (91.18%), F-score (0.916), Area under the Curve (0.96), sensitivity (94.2%), and specificity (88%). The model suggested in this study uses simple and available data and can be applied to predict death among COVID-19 patients. Furthermore, we concluded that machine learning models may perform differently in different subpopulations in terms of gender and age groups.",
      "journal": "Journal of healthcare engineering",
      "year": "2022",
      "doi": "10.1155/2022/1644910",
      "authors": "Zarei Javad et al.",
      "keywords": "",
      "mesh_terms": "COVID-19; Hospital Mortality; Humans; Inpatients; Machine Learning; ROC Curve",
      "pub_types": "Journal Article; Multicenter Study; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35756093/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9226971",
      "ft_text_length": 22999,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9226971)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "35869152",
      "title": "Prediction performance and fairness heterogeneity in cardiovascular risk models.",
      "abstract": "Prediction models are commonly used to estimate risk for cardiovascular diseases, to inform diagnosis and management. However, performance may vary substantially across relevant subgroups of the population. Here we investigated heterogeneity of accuracy and fairness metrics across a variety of subgroups for risk prediction of two common diseases: atrial fibrillation (AF) and atherosclerotic cardiovascular disease (ASCVD). We calculated the Cohorts for Heart and Aging in Genomic Epidemiology Atrial Fibrillation (CHARGE-AF) score for AF and the Pooled Cohort Equations (PCE) score for ASCVD in three large datasets: Explorys Life Sciences Dataset (Explorys, n\u2009=\u200921,809,334), Mass General Brigham (MGB, n\u2009=\u2009520,868), and the UK Biobank (UKBB, n\u2009=\u2009502,521). Our results demonstrate important performance heterogeneity across subpopulations defined by age, sex, and presence of preexisting disease, with fairly consistent patterns across both scores. For example, using\u00a0CHARGE-AF, discrimination declined with increasing age, with a concordance index of 0.72 [95% CI 0.72-0.73] for the youngest (45-54\u00a0years) subgroup to 0.57 [0.56-0.58] for the oldest (85-90\u00a0years) subgroup in Explorys. Even though sex is not included in CHARGE-AF, the statistical parity difference (i.e., likelihood of being classified as high risk) was considerable between males and females within the 65-74\u00a0years subgroup with a value of -\u20090.33 [95% CI -\u20090.33 to -\u20090.33]. We also observed weak discrimination (i.e., <\u20090.7) and suboptimal calibration (i.e., calibration slope outside of 0.7-1.3) in large subsets of the population; for example, all individuals aged 75\u00a0years or older in Explorys (17.4%). Our findings highlight the need to characterize and quantify the behavior of clinical risk models within specific subpopulations so they can be used appropriately to facilitate more accurate, consistent, and equitable assessment of disease risk.",
      "journal": "Scientific reports",
      "year": "2022",
      "doi": "10.1038/s41598-022-16615-3",
      "authors": "Kartoun Uri et al.",
      "keywords": "",
      "mesh_terms": "Atherosclerosis; Atrial Fibrillation; Cardiovascular Diseases; Female; Heart Disease Risk Factors; Humans; Male; Middle Aged; Risk Assessment; Risk Factors",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35869152/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9307639",
      "ft_text_length": 31902,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9307639)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "36059892",
      "title": "Subpopulation-specific machine learning prognosis for underrepresented patients with double prioritized bias correction.",
      "abstract": "BACKGROUND: Many clinical datasets are intrinsically imbalanced, dominated by overwhelming majority groups. Off-the-shelf machine learning models that optimize the prognosis of majority patient types (e.g., healthy class) may cause substantial errors on the minority prediction class (e.g., disease class) and demographic subgroups (e.g., Black or young patients). In the typical one-machine-learning-model-fits-all paradigm, racial and age disparities are likely to exist, but unreported. In addition, some widely used whole-population metrics give misleading results. METHODS: We design a double prioritized (DP) bias correction technique to mitigate representational biases in machine learning-based prognosis. Our method trains customized machine learning models for specific ethnicity or age groups, a substantial departure from the one-model-predicts-all convention. We compare with other sampling and reweighting techniques in mortality and cancer survivability prediction tasks. RESULTS: We first provide empirical evidence showing various prediction deficiencies in a typical machine learning setting without bias correction. For example, missed death cases are 3.14 times higher than missed survival cases for mortality prediction. Then, we show DP consistently boosts the minority class recall for underrepresented groups, by up to 38.0%. DP also reduces relative disparities across race and age groups, e.g., up to 88.0% better than the 8 existing sampling solutions in terms of the relative disparity of minority class recall. Cross-race and cross-age-group evaluation also suggests the need for subpopulation-specific machine learning models. CONCLUSIONS: Biases exist in the widely accepted one-machine-learning-model-fits-all-population approach. We invent a bias correction method that produces specialized machine learning prognostication models for underrepresented racial and age groups. This technique may reduce potentially life-threatening prediction mistakes for minority populations.",
      "journal": "Communications medicine",
      "year": "2022",
      "doi": "10.1038/s43856-022-00165-w",
      "authors": "Afrose Sharmin et al.",
      "keywords": "Cancer; Prognosis",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36059892/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9436942",
      "ft_text_length": 57558,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9436942)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "36060496",
      "title": "Enabling Fairness in Healthcare Through Machine Learning.",
      "abstract": "The use of machine learning systems for decision-support in healthcare may exacerbate health inequalities. However, recent work suggests that algorithms trained on sufficiently diverse datasets could in principle combat health inequalities. One concern about these algorithms is that their performance for patients in traditionally disadvantaged groups exceeds their performance for patients in traditionally advantaged groups. This renders the algorithmic decisions unfair relative to the standard fairness metrics in machine learning. In this paper, we defend the permissible use of affirmative algorithms; that is, algorithms trained on diverse datasets that perform better for traditionally disadvantaged groups. Whilst such algorithmic decisions may be unfair, the fairness of algorithmic decisions is not the appropriate locus of moral evaluation. What matters is the fairness of final decisions, such as diagnoses, resulting from collaboration between clinicians and algorithms. We argue that affirmative algorithms can permissibly be deployed provided the resultant final decisions are fair.",
      "journal": "Ethics and information technology",
      "year": "2022",
      "doi": "10.1007/s10676-022-09658-7",
      "authors": "Grote Thomas et al.",
      "keywords": "Bias; Decision-making; Fairness; Healthcare; Machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36060496/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9428374",
      "ft_text_length": 52925,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9428374)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "36185063",
      "title": "The paradox of the artificial intelligence system development process: the use case of corporate wellness programs using smart wearables.",
      "abstract": "Artificial intelligence (AI) systems have been widely applied to various contexts, including high-stake decision processes in healthcare, banking, and judicial systems. Some developed AI models fail to offer a fair output for specific minority groups, sparking comprehensive discussions about AI fairness. We argue that the development of AI systems is marked by a central paradox: the less participation one stakeholder has within the AI system's life cycle, the more influence they have over the way the system will function. This means that the impact on the fairness of the system is in the hands of those who are less impacted by it. However, most of the existing works ignore how different aspects of AI fairness are dynamically and adaptively affected by different stages of AI system development. To this end, we present a use case to discuss fairness in the development of corporate wellness programs using smart wearables and AI algorithms to analyze data. The four key stakeholders throughout this type of AI system development process are presented. These stakeholders\u00a0are called service designer, algorithm designer, system deployer, and end-user. We identify three core aspects of AI fairness, namely, contextual fairness, model fairness, and device fairness. We propose a relative contribution of the four stakeholders to the three aspects of fairness. Furthermore, we propose the boundaries and interactions between the four roles, from which we make our conclusion about the possible unfairness in such an AI developing process.",
      "journal": "AI & society",
      "year": "2022",
      "doi": "10.1007/s00146-022-01562-4",
      "authors": "Angelucci Alessandra et al.",
      "keywords": "Artificial intelligence; Classification model; Corporate wellness program; Fairness; Smartwatches",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36185063/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9511446",
      "ft_text_length": 47038,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9511446)",
      "ft_reason": "Included: substantial approach content (3 indicators)"
    },
    {
      "pmid": "36194270",
      "title": "Mitigating bias in deep learning for diagnosis of coronary artery disease from myocardial perfusion SPECT images.",
      "abstract": "PURPOSE: Artificial intelligence (AI) has high diagnostic accuracy for coronary artery disease (CAD) from myocardial perfusion imaging (MPI). However, when trained using high-risk populations (such as patients with correlating invasive testing), the disease probability can be overestimated due to selection bias. We evaluated different strategies for training AI models to improve the calibration (accurate estimate of disease probability), using external testing. METHODS: Deep learning was trained using 828 patients from 3 sites, with MPI and invasive angiography within 6\u00a0months. Perfusion was assessed using upright (U-TPD) and supine total perfusion deficit (S-TPD). AI training without data augmentation (model 1) was compared to training with augmentation (increased sampling) of patients without obstructive CAD (model 2), and patients without CAD and TPD\u2009<\u20092% (model 3). All models were tested in an external population of patients with invasive angiography within 6\u00a0months (n\u2009=\u2009332) or low likelihood of CAD (n\u2009=\u2009179). RESULTS: Model 3 achieved the best calibration (Brier score 0.104 vs 0.121, p\u2009<\u20090.01). Improvement in calibration was particularly evident in women (Brier score 0.084 vs 0.124, p\u2009<\u20090.01). In external testing (n\u2009=\u2009511), the area under the receiver operating characteristic curve (AUC) was higher for model 3 (0.930), compared to U-TPD (AUC 0.897) and S-TPD (AUC 0.900, p\u2009<\u20090.01 for both). CONCLUSION: Training AI models with augmentation of low-risk patients can improve calibration of AI models developed to identify patients with CAD, allowing more accurate assignment of disease probability. This is particularly important in lower-risk populations and in women, where overestimation of disease probability could significantly influence down-stream patient management.",
      "journal": "European journal of nuclear medicine and molecular imaging",
      "year": "2023",
      "doi": "10.1007/s00259-022-05972-w",
      "authors": "Miller Robert J H et al.",
      "keywords": "Calibration; Deep learning; Diagnostic accuracy; Model training; Sex-specific analysis",
      "mesh_terms": "Humans; Female; Coronary Artery Disease; Artificial Intelligence; Deep Learning; Sensitivity and Specificity; Tomography, Emission-Computed, Single-Photon; Perfusion; Myocardial Perfusion Imaging; Coronary Angiography",
      "pub_types": "Journal Article; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36194270/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10042590",
      "ft_text_length": 1785,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10042590)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "36226507",
      "title": "Equity within AI systems: What can health leaders expect?",
      "abstract": "Artificial Intelligence (AI) for health has a great potential; it has already proven to be successful in enhancing patient outcomes, facilitating professional work and benefiting administration. However, AI presents challenges related to health equity defined as an opportunity for people to reach their fullest health potential. This article discusses the opportunities and challenges that AI presents in health and examines ways in which inequities related to AI can be mitigated.",
      "journal": "Healthcare management forum",
      "year": "2023",
      "doi": "10.1177/08404704221125368",
      "authors": "Gurevich Emma et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Health Equity",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36226507/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9976641",
      "ft_text_length": 19994,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9976641)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "36247412",
      "title": "Real-world performance, long-term efficacy, and absence of bias in the artificial intelligence enhanced electrocardiogram to detect left ventricular systolic dysfunction.",
      "abstract": "AIMS: Some artificial intelligence models applied in medical practice require ongoing retraining, introduce unintended racial bias, or have variable performance among different subgroups of patients. We assessed the real-world performance of the artificial intelligence-enhanced electrocardiogram to detect left ventricular systolic dysfunction with respect to multiple patient and electrocardiogram variables to determine the algorithm's long-term efficacy and potential bias in the absence of retraining. METHODS AND RESULTS: Electrocardiograms acquired in 2019 at Mayo Clinic in Minnesota, Arizona, and Florida with an echocardiogram performed within 14 days were analyzed (n = 44 986 unique patients). The area under the curve (AUC) was calculated to evaluate performance of the algorithm among age groups, racial and ethnic groups, patient encounter location, electrocardiogram features, and over time. The artificial intelligence-enhanced electrocardiogram to detect left ventricular systolic dysfunction had an AUC of 0.903 for the total cohort. Time series analysis of the model validated its temporal stability. Areas under the curve were similar for all racial and ethnic groups (0.90-0.92) with minimal performance difference between sexes. Patients with a 'normal sinus rhythm' electrocardiogram (n = 37 047) exhibited an AUC of 0.91. All other electrocardiogram features had areas under the curve between 0.79 and 0.91, with the lowest performance occurring in the left bundle branch block group (0.79). CONCLUSION: The artificial intelligence-enhanced electrocardiogram to detect left ventricular systolic dysfunction is stable over time in the absence of retraining and robust with respect to multiple variables including time, patient race, and electrocardiogram features.",
      "journal": "European heart journal. Digital health",
      "year": "2022",
      "doi": "10.1093/ehjdh/ztac028",
      "authors": "Harmon David M et al.",
      "keywords": "Arrhythmia; Artificial intelligence; Deep learning; Digital medicine; ECG; Heart failure",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36247412/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9558265",
      "ft_text_length": 22733,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9558265)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "36258241",
      "title": "Algorithmic fairness audits in intensive care medicine: artificial intelligence for all?",
      "abstract": "",
      "journal": "Critical care (London, England)",
      "year": "2022",
      "doi": "10.1186/s13054-022-04197-5",
      "authors": "van de Sande Davy et al.",
      "keywords": "Artificial intelligence; Bias; Equity; Intensive care",
      "mesh_terms": "Humans; Artificial Intelligence; Critical Care; Algorithms",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36258241/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9578232",
      "ft_text_length": 7222,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9578232)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "36284736",
      "title": "Ethical Redress of Racial Inequities in AI: Lessons from Decoupling Machine Learning from Optimization in Medical Appointment Scheduling.",
      "abstract": "An Artificial Intelligence algorithm trained on data that reflect racial biases may yield racially biased outputs, even if the algorithm on its own is unbiased. For example, algorithms used to schedule medical appointments in the USA predict that Black patients are at a higher risk of no-show than non-Black patients, though technically accurate given existing data that prediction results in Black patients being overwhelmingly scheduled in appointment slots that cause longer wait times than non-Black patients. This perpetuates racial inequity, in this case lesser access to medical care. This gives rise to one type of Accuracy-Fairness trade-off: preserve the efficiency offered by using AI to schedule appointments or discard that efficiency in order to avoid perpetuating ethno-racial disparities. Similar trade-offs arise in a range of AI applications including others in medicine, as well as in education, judicial systems, and public security, among others. This article presents a framework for addressing such trade-offs where Machine Learning and Optimization components of the algorithm are decoupled. Applied to medical appointment scheduling, our framework articulates four approaches intervening in different ways on different components of the algorithm. Each yields specific results, in one case preserving accuracy comparable to the current state-of-the-art while eliminating the disparity.",
      "journal": "Philosophy & technology",
      "year": "2022",
      "doi": "10.1007/s13347-022-00590-8",
      "authors": "Shanklin Robert et al.",
      "keywords": "Artificial Intelligence; Bias; Ethics; Healthcare; Machine Learning; Racial\u00a0Disparities",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36284736/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9584259",
      "ft_text_length": 39778,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9584259)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "36346213",
      "title": "Benchmarking the generalizability of brain age models: Challenges posed by scanner variance and prediction bias.",
      "abstract": "Machine learning has been increasingly applied to neuroimaging data to predict age, deriving a personalized biomarker with potential clinical applications. The scientific and clinical value of these models depends on their applicability to independently acquired scans from diverse sources. Accordingly, we evaluated the generalizability of two brain age models that were trained across the lifespan by applying them to three distinct early-life samples with participants aged 8-22\u2009years. These models were chosen based on the size and diversity of their training data, but they also differed greatly in their processing methods and predictive algorithms. Specifically, one brain age model was built by applying gradient tree boosting (GTB) to extracted features of cortical thickness, surface area, and brain volume. The other model applied a 2D convolutional neural network (DBN) to minimally preprocessed slices of T1-weighted scans. Additional model variants were created to understand how generalizability changed when each model was trained with data that became more similar to the test samples in terms of age and acquisition protocols. Our results illustrated numerous trade-offs. The GTB predictions were relatively more accurate overall and yielded more reliable predictions when applied to lower quality scans. In contrast, the DBN displayed the most utility in detecting associations between brain age gaps and cognitive functioning. Broadly speaking, the largest limitations affecting generalizability were acquisition protocol differences and biased brain age estimates. If such confounds could eventually be removed without post-hoc corrections, brain age predictions may have greater utility as personalized biomarkers of healthy aging.",
      "journal": "Human brain mapping",
      "year": "2023",
      "doi": "10.1002/hbm.26144",
      "authors": "Jirsaraie Robert J et al.",
      "keywords": "brain age; brain development; computational neuroscience; generalizability; machine learning",
      "mesh_terms": "Humans; Magnetic Resonance Imaging; Benchmarking; Brain; Neuroimaging; Longevity",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36346213/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9875922",
      "ft_text_length": 38696,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9875922)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "36414774",
      "title": "Mitigating the impact of biased artificial intelligence in emergency decision-making.",
      "abstract": "BACKGROUND: Prior research has shown that artificial intelligence (AI) systems often encode biases against minority subgroups. However, little work has focused on ways to mitigate the harm discriminatory algorithms can cause in high-stakes settings such as medicine. METHODS: In this study, we experimentally evaluated the impact biased AI recommendations have on emergency decisions, where participants respond to mental health crises by calling for either medical or police assistance. We recruited 438 clinicians and 516 non-experts to participate in our web-based experiment. We evaluated participant decision-making with and without advice from biased and unbiased AI systems. We also varied the style of the AI advice, framing it either as prescriptive recommendations or descriptive flags. RESULTS: Participant decisions are unbiased without AI advice. However, both clinicians and non-experts are influenced by prescriptive recommendations from a biased algorithm, choosing police help more often in emergencies involving African-American or Muslim men. Crucially, using descriptive flags rather than prescriptive recommendations allows respondents to retain their original, unbiased decision-making. CONCLUSIONS: Our work demonstrates the practical danger of using biased models in health contexts, and suggests that appropriately framing decision support can mitigate the effects of AI bias. These findings must be carefully considered in the many real-world clinical scenarios where inaccurate or biased models may be used to inform important decisions. Artificial intelligence (AI) systems that make decisions based on historical data are increasingly common in health care settings. However, many AI models exhibit problematic biases, as data often reflect human prejudices against minority groups. In this study, we used a web-based experiment to evaluate the impact biased models can have when used to inform human decisions. We found that though participants were not inherently biased, they were strongly influenced by advice from a biased model if it was offered prescriptively (i.e., \u201cyou should do X\u201d). This adherence led their decisions to be biased against African-American and Muslims individuals. However, framing the same advice descriptively (i.e., without recommending a specific action) allowed participants to remain fair. These results demonstrate that though discriminatory AI can lead to poor outcomes for minority groups, appropriately framing advice can help mitigate its effects.",
      "journal": "Communications medicine",
      "year": "2022",
      "doi": "10.1038/s43856-022-00214-4",
      "authors": "Adam Hammaad et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36414774/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9681767",
      "ft_text_length": 23381,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9681767)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "36541005",
      "title": "Algorithmic Fairness in the Roberts Court Era.",
      "abstract": "Scientists and policymakers alike have increasingly been interested in exploring ways to advance algorithmic fairness, recognizing not only the potential utility of algorithms in biomedical and digital health contexts but also that the unique challenges that algorithms-in a datafied culture such as the United States-pose for civil rights (including, but not limited to, privacy and nondiscrimination). In addition to the technical complexities, separation of powers issues are making the task even more daunting for policymakers-issues that might seem obscure to many scientists and technologists. While administrative agencies (such as the Federal Trade Commission) and legislators have been working to advance algorithmic fairness (in large part through comprehensive data privacy reform), recent judicial activism by the Roberts Court threaten to undermine those efforts. Scientists need to understand these legal developments so they can take appropriate action when contributing to a biomedical data ecosystem and designing, deploying, and maintaining algorithms for digital health. Here I highlight some of the recent actions taken by policymakers. I then review three recent Supreme Court cases (and foreshadow a fourth case) that illustrate the radical power grab by the Roberts Court, explaining for scientists how these drastic shifts in law will frustrate governmental approaches to algorithmic fairness and necessitate increased reliance by scientists on self-governance strategies to promote responsible and ethical practices.",
      "journal": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing",
      "year": "2023",
      "doi": "10.1145/3194770.3194776",
      "authors": "Wagner Jennifer K",
      "keywords": "",
      "mesh_terms": "United States; Humans; Ecosystem; Computational Biology; Privacy",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36541005/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9782697",
      "ft_text_length": 26900,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9782697)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "36577946",
      "title": "External control arm analysis: an evaluation of propensity score approaches, G-computation, and doubly debiased machine learning.",
      "abstract": "BACKGROUND: An external control arm is a cohort of control patients that are collected from data external to a single-arm trial. To provide an unbiased estimation of efficacy, the clinical profiles of patients from single and external arms should be aligned, typically using propensity score approaches. There are alternative approaches to infer efficacy based on comparisons between outcomes of single-arm patients and machine-learning predictions of control patient outcomes. These methods include G-computation and Doubly Debiased Machine Learning (DDML) and their evaluation for External Control Arms (ECA) analysis is insufficient. METHODS: We consider both numerical simulations and a trial replication procedure to evaluate the different statistical approaches: propensity score matching, Inverse Probability of Treatment Weighting (IPTW), G-computation, and DDML. The replication study relies on five type 2 diabetes randomized clinical trials granted by the Yale University Open Data Access (YODA) project. From the pool of five trials, observational experiments are artificially built by replacing a control arm from one trial by an arm originating from another trial and containing similarly-treated patients. RESULTS: Among the different statistical approaches, numerical simulations show that DDML has the smallest bias followed by G-computation. In terms of mean squared error, G-computation usually minimizes mean squared error. Compared to other methods, DDML has varying Mean Squared Error performances that improves with increasing sample sizes. For hypothesis testing, all methods control type I error and DDML is the most conservative. G-computation is the best method in terms of statistical power, and DDML has comparable power at [Formula: see text] but inferior ones for smaller sample sizes. The replication procedure also indicates that G-computation minimizes mean squared error whereas DDML has intermediate performances in between G-computation and propensity score approaches. The confidence intervals of G-computation are the narrowest whereas confidence intervals obtained with DDML are the widest for small sample sizes, which confirms its conservative nature. CONCLUSIONS: For external control arm analyses, methods based on outcome prediction models can reduce estimation error and increase statistical power compared to propensity score approaches.",
      "journal": "BMC medical research methodology",
      "year": "2022",
      "doi": "10.1186/s12874-022-01799-z",
      "authors": "Loiseau Nicolas et al.",
      "keywords": "Average treatment effect; Confounding variables; Counterfactual; Doubly robust; Observational study; Propensity score; Replication study",
      "mesh_terms": "Humans; Bias; Computer Simulation; Diabetes Mellitus, Type 2; Machine Learning; Propensity Score; Research Design; Randomized Controlled Trials as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36577946/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9795588",
      "ft_text_length": 60710,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9795588)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "36631555",
      "title": "Promoting racial equity in digital health: applying a cross-disciplinary equity framework.",
      "abstract": "Even as innovation occurs within digital medicine, challenges around equity and racial health disparities remain. Golden et al. evaluate structural racism in their recent paper focused on reproductive health. They recommend a framework to Remove, Repair, Restructure, and Remediate. We propose applying the framework to three areas within digital medicine: artificial intelligence (AI) applications, wearable devices, and telehealth. With this approach, we can continue to work towards an equitable future for digital medicine.",
      "journal": "NPJ digital medicine",
      "year": "2023",
      "doi": "10.1038/s41746-023-00747-5",
      "authors": "Raza Marium M et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36631555/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9833018",
      "ft_text_length": 9930,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9833018)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "36634514",
      "title": "Age-level bias correction in brain age prediction.",
      "abstract": "The predicted age difference (PAD) between an individual's predicted brain age and chronological age has been commonly viewed as a meaningful phenotype relating to aging and brain diseases. However, the systematic bias appears in the PAD achieved using machine learning methods. Recent studies have designed diverse bias correction methods to eliminate it for further downstream studies. Strikingly, here we demonstrate that bias still exists in the PAD of samples with the same age even after kind of correction. Therefore, current PAD may not be taken as a reliable phenotype and more investigations are needed to solve this fundamental defect. To this end, we propose an age-level bias correction method and demonstrate its efficacy in numerical experiments.",
      "journal": "NeuroImage. Clinical",
      "year": "2023",
      "doi": "10.1016/j.nicl.2023.103319",
      "authors": "Zhang Biao et al.",
      "keywords": "Age prediction; Bias correction; Human brain; MRI; Machine learning",
      "mesh_terms": "Magnetic Resonance Imaging; Brain; Machine Learning",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36634514/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9860514",
      "ft_text_length": 31278,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9860514)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "36647369",
      "title": "Gender and sex bias in COVID-19 epidemiological data through the lens of causality.",
      "abstract": "The COVID-19 pandemic has spurred a large amount of experimental and observational studies reporting clear correlation between the risk of developing severe COVID-19 (or dying from it) and whether the individual is male or female. This paper is an attempt to explain the supposed male vulnerability to COVID-19 using a causal approach. We proceed by identifying a set of confounding and mediating factors, based on the review of epidemiological literature and analysis of sex-dis-aggregated data. Those factors are then taken into consideration to produce explainable and fair prediction and decision models from observational data. The paper outlines how non-causal models can motivate discriminatory policies such as biased allocation of the limited resources in intensive care units (ICUs). The objective is to anticipate and avoid disparate impact and discrimination, by considering causal knowledge and causal-based techniques to compliment the collection and analysis of observational big-data. The hope is to contribute to more careful use of health related information access systems for developing fair and robust predictive models.",
      "journal": "Information processing & management",
      "year": "2023",
      "doi": "10.1016/j.ipm.2023.103276",
      "authors": "D\u00edaz-Rodr\u00edguez Natalia et al.",
      "keywords": "Artificial intelligence; COVID-19; Causal fairness; Causality; Equality; Explainability; Gender; Healthcare; Sex",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36647369/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9834203",
      "ft_text_length": 71577,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9834203)",
      "ft_reason": "Included: bias central + approach content (7 indicators)"
    },
    {
      "pmid": "36706849",
      "title": "Evaluating and mitigating bias in machine learning models for cardiovascular disease prediction.",
      "abstract": "OBJECTIVE: The study aims to investigate whether machine learning-based predictive models for cardiovascular disease (CVD) risk assessment show equivalent performance across demographic groups (such as race and gender) and if bias mitigation methods can reduce any bias present in the models. This is important as systematic bias may be introduced when collecting and preprocessing health data, which could affect the performance of the models on certain demographic sub-cohorts. The study is to investigate this using electronic health records data and various machine learning models. METHODS: The study used large de-identified Electronic Health Records data from Vanderbilt University Medical Center. Machine learning (ML) algorithms including logistic regression, random forest, gradient-boosting trees, and long short-term memory were applied to build multiple predictive models. Model bias and fairness were evaluated using equal opportunity difference (EOD, 0 indicates fairness) and disparate impact (DI, 1 indicates fairness). In our study, we also evaluated the fairness of a non-ML baseline model, the American Heart Association (AHA) Pooled Cohort Risk Equations (PCEs). Moreover, we compared the performance of three different de-biasing methods: removing protected attributes (e.g., race and gender), resampling the imbalanced training dataset by sample size, and resampling by the proportion of people with CVD outcomes. RESULTS: The study cohort included 109,490 individuals (mean [SD] age 47.4 [14.7] years; 64.5% female; 86.3% White; 13.7% Black). The experimental results suggested that most ML models had smaller EOD and DI than PCEs. For ML models, the mean EOD ranged from -0.001 to 0.018 and the mean DI ranged from 1.037 to 1.094 across race groups. There was a larger EOD and DI across gender groups, with EOD ranging from 0.131 to 0.136 and DI ranging from 1.535 to 1.587. For debiasing methods, removing protected attributes didn't significantly reduced the bias for most ML models. Resampling by sample size also didn't consistently decrease bias. Resampling by case proportion reduced the EOD and DI for gender groups but slightly reduced accuracy in many cases. CONCLUSIONS: Among the VUMC cohort, both PCEs and ML models were biased against women, suggesting the need to investigate and correct gender disparities in CVD risk prediction. Resampling by proportion reduced the bias for gender groups but not for race groups.",
      "journal": "Journal of biomedical informatics",
      "year": "2023",
      "doi": "10.1016/j.jbi.2023.104294",
      "authors": "Li Fuchen et al.",
      "keywords": "Bias mitigation; Cardiovascular diseases; Clinical predictive models; Electronic health records; Fairness; Machine learning",
      "mesh_terms": "Humans; Female; Middle Aged; Male; Cardiovascular Diseases; Machine Learning; Algorithms; Random Forest; Logistic Models",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36706849/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11104322",
      "ft_text_length": 2454,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11104322)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "36714611",
      "title": "Fairness in the prediction of acute postoperative pain using machine learning models.",
      "abstract": "INTRODUCTION: Overall performance of machine learning-based prediction models is promising; however, their generalizability and fairness must be vigorously investigated to ensure they perform sufficiently well for all patients. OBJECTIVE: This study aimed to evaluate prediction bias in machine learning models used for predicting acute postoperative pain. METHOD: We conducted a retrospective review of electronic health records for patients undergoing orthopedic surgery from June 1, 2011, to June 30, 2019, at the University of Florida Health system/Shands Hospital. CatBoost machine learning models were trained for predicting the binary outcome of low (\u22644) and high pain (>4). Model biases were assessed against seven protected attributes of age, sex, race, area deprivation index (ADI), speaking language, health literacy, and insurance type. Reweighing of protected attributes was investigated for reducing model bias compared with base models. Fairness metrics of equal opportunity, predictive parity, predictive equality, statistical parity, and overall accuracy equality were examined. RESULTS: The final dataset included 14,263 patients [age: 60.72 (16.03) years, 53.87% female, 39.13% low acute postoperative pain]. The machine learning model (area under the curve, 0.71) was biased in terms of age, race, ADI, and insurance type, but not in terms of sex, language, and health literacy. Despite promising overall performance in predicting acute postoperative pain, machine learning-based prediction models may be biased with respect to protected attributes. CONCLUSION: These findings show the need to evaluate fairness in machine learning models involved in perioperative pain before they are implemented as clinical decision support tools.",
      "journal": "Frontiers in digital health",
      "year": "2022",
      "doi": "10.3389/fdgth.2022.970281",
      "authors": "Davoudi Anis et al.",
      "keywords": "algorithmic bias; clinical decision support systems; machine learing; orthopedic procedures; postoperative pain",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36714611/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9874861",
      "ft_text_length": 39169,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9874861)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "36716365",
      "title": "Bias in machine learning models can be significantly mitigated by careful training: Evidence from neuroimaging studies.",
      "abstract": "Despite the great promise that machine learning has offered in many fields of medicine, it has also raised concerns about potential biases and poor generalization across genders, age distributions, races and ethnicities, hospitals, and data acquisition equipment and protocols. In the current study, and in the context of three brain diseases, we provide evidence which suggests that when properly trained, machine learning models can generalize well across diverse conditions and do not necessarily suffer from bias. Specifically, by using multistudy magnetic resonance imaging consortia for diagnosing Alzheimer's disease, schizophrenia, and autism spectrum disorder, we find that well-trained models have a high area-under-the-curve (AUC) on subjects across different subgroups pertaining to attributes such as gender, age, racial groups and different clinical studies and are unbiased under multiple fairness metrics such as demographic parity difference, equalized odds difference, equal opportunity difference, etc. We find that models that incorporate multisource data from demographic, clinical, genetic factors, and cognitive scores are also unbiased. These models have a better predictive AUC across subgroups than those trained only with imaging features, but there are also situations when these additional features do not help.",
      "journal": "Proceedings of the National Academy of Sciences of the United States of America",
      "year": "2023",
      "doi": "10.1073/pnas.2211613120",
      "authors": "Wang Rongguang et al.",
      "keywords": "MRI; algorithmic bias; heterogeneity; machine learning; neuroscience",
      "mesh_terms": "Humans; Male; Female; Autism Spectrum Disorder; Neuroimaging; Machine Learning; Magnetic Resonance Imaging; Alzheimer Disease; Bias",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36716365/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9962919",
      "ft_text_length": 14118,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9962919)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "36719717",
      "title": "Black and Latinx Primary Caregiver Considerations for Developing and Implementing a Machine Learning-Based Model for Detecting Child Abuse and Neglect With Implications for Racial Bias Reduction: Qualitative Interview Study With Primary Caregivers.",
      "abstract": "BACKGROUND: Child abuse and neglect, once viewed as a social problem, is now an epidemic. Moreover, health providers agree that existing stereotypes may link racial and social class issues to child abuse. The broad adoption of electronic health records (EHRs) in clinical settings offers a new avenue for addressing this epidemic. To reduce racial bias and improve the development, implementation, and outcomes of machine learning (ML)-based models that use EHR data, it is crucial to involve marginalized members of the community in the process. OBJECTIVE: This study elicited Black and Latinx primary caregivers' viewpoints regarding child abuse and neglect while living in underserved communities to highlight considerations for designing an ML-based model for detecting child abuse and neglect in emergency departments (EDs) with implications for racial bias reduction and future interventions. METHODS: We conducted a qualitative study using in-depth interviews with 20 Black and Latinx primary caregivers whose children were cared for at a single pediatric tertiary-care ED to gain insights about child abuse and neglect and their experiences with health providers. RESULTS: Three central themes were developed in the coding process: (1) primary caregivers' perspectives on the definition of child abuse and neglect, (2) primary caregivers' experiences with health providers and medical documentation, and (3) primary caregivers' perceptions of child protective services. CONCLUSIONS: Our findings highlight essential considerations from primary caregivers for developing an ML-based model for detecting child abuse and neglect in ED settings. This includes how to define child abuse and neglect from a primary caregiver lens. Miscommunication between patients and health providers can potentially lead to a misdiagnosis, and therefore, have a negative impact on medical documentation. Additionally, the outcome and application of the ML-based models for detecting abuse and neglect may cause additional harm than expected to the community. Further research is needed to validate these findings and integrate them into creating an ML-based model.",
      "journal": "JMIR formative research",
      "year": "2023",
      "doi": "10.2196/40194",
      "authors": "Landau Aviv Y et al.",
      "keywords": "abuse; child; child abuse and neglect; community; development; electronic health records; epidemic; implementation; machine learning; machine learning\u2013based risk models; model; neglect; pediatric emergency departments; primary caregivers",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36719717/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9929722",
      "ft_text_length": 36398,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9929722)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "36823101",
      "title": "Practical, epistemic and normative implications of algorithmic bias in healthcare artificial intelligence: a qualitative study of multidisciplinary expert perspectives.",
      "abstract": "BACKGROUND: There is a growing concern about artificial intelligence (AI) applications in healthcare that can disadvantage already under-represented and marginalised groups (eg, based on gender or race). OBJECTIVES: Our objectives are to canvas the range of strategies stakeholders endorse in attempting to mitigate algorithmic bias, and to consider the ethical question of responsibility for algorithmic bias. METHODOLOGY: The study involves in-depth, semistructured interviews with healthcare workers, screening programme managers, consumer health representatives, regulators, data scientists and developers. RESULTS: Findings reveal considerable divergent views on three key issues. First, views on whether bias is a problem in healthcare AI varied, with most participants agreeing bias is a problem (which we call the bias-critical view), a small number believing the opposite (the bias-denial view), and some arguing that the benefits of AI outweigh any harms or wrongs arising from the bias problem (the bias-apologist view). Second, there was a disagreement on the strategies to mitigate bias, and who is responsible for such strategies. Finally, there were divergent views on whether to include or exclude sociocultural identifiers (eg, race, ethnicity or gender-diverse identities) in the development of AI as a way to mitigate bias. CONCLUSION/SIGNIFICANCE: Based on the views of participants, we set out responses that stakeholders might pursue, including greater interdisciplinary collaboration, tailored stakeholder engagement activities, empirical studies to understand algorithmic bias and strategies to modify dominant approaches in AI development such as the use of participatory methods, and increased diversity and inclusion in research teams and research participant recruitment and selection.",
      "journal": "Journal of medical ethics",
      "year": "2025",
      "doi": "10.1136/jme-2022-108850",
      "authors": "Aquino Yves Saint James et al.",
      "keywords": "Decision Making; Ethics; Information Technology; Policy",
      "mesh_terms": "Humans; Artificial Intelligence; Qualitative Research; Algorithms; Female; Male; Bias; Delivery of Health Care; Health Personnel; Attitude of Health Personnel; Interviews as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36823101/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12171461",
      "ft_text_length": 43112,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12171461)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "36865183",
      "title": "Classification performance bias between training and test sets in a limited mammography dataset.",
      "abstract": "OBJECTIVES: To assess the performance bias caused by sampling data into training and test sets in a mammography radiomics study. METHODS: Mammograms from 700 women were used to study upstaging of ductal carcinoma in situ. The dataset was repeatedly shuffled and split into training (n=400) and test cases (n=300) forty times. For each split, cross-validation was used for training, followed by an assessment of the test set. Logistic regression with regularization and support vector machine were used as the machine learning classifiers. For each split and classifier type, multiple models were created based on radiomics and/or clinical features. RESULTS: Area under the curve (AUC) performances varied considerably across the different data splits (e.g., radiomics regression model: train 0.58-0.70, test 0.59-0.73). Performances for regression models showed a tradeoff where better training led to worse testing and vice versa. Cross-validation over all cases reduced this variability, but required samples of 500+ cases to yield representative estimates of performance. CONCLUSIONS: In medical imaging, clinical datasets are often limited to relatively small size. Models built from different training sets may not be representative of the whole dataset. Depending on the selected data split and model, performance bias could lead to inappropriate conclusions that might influence the clinical significance of the findings. Optimal strategies for test set selection should be developed to ensure study conclusions are appropriate.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2023",
      "doi": "10.1101/2023.02.15.23285985",
      "authors": "Hou Rui et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36865183/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9980247",
      "ft_text_length": 14772,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9980247)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "36865610",
      "title": "Identification of Social and Racial Disparities in Risk of HIV Infection in Florida using Causal AI Methods.",
      "abstract": "Florida -the 3rd most populous state in the USA-has the highest rates of Human Immunodeficiency Virus (HIV) infections and of unfavorable HIV outcomes, with marked social and racial disparities. In this work, we leveraged large-scale, real-world data, i.e., statewide surveillance records and publicly available data resources encoding social determinants of health (SDoH), to identify social and racial disparities contributing to individuals' risk of HIV infection. We used the Florida Department of Health's Syndromic Tracking and Reporting System (STARS) database (including 100,000+ individuals screened for HIV infection and their partners), and a novel algorithmic fairness assessment method -the Fairness-Aware Causal paThs decompoSition (FACTS)- merging causal inference and artificial intelligence. FACTS deconstructs disparities based on SDoH and individuals' characteristics, and can discover novel mechanisms of inequity, quantifying to what extent they could be reduced by interventions. We paired the deidentified demographic information (age, gender, drug use) of 44,350 individuals in STARS -with non-missing data on interview year, county of residence, and infection status- to eight SDoH, including access to healthcare facilities, % uninsured, median household income, and violent crime rate. Using an expert-reviewed causal graph, we found that the risk of HIV infection for African Americans was higher than for non- African Americans (both in terms of direct and total effect), although a null effect could not be ruled out. FACTS identified several paths leading to racial disparity in HIV risk, including multiple SDoH: education, income, violent crime, drinking, smoking, and rurality.",
      "journal": "Proceedings. IEEE International Conference on Bioinformatics and Biomedicine",
      "year": "2022",
      "doi": "10.1109/bibm55620.2022.9995662",
      "authors": "Prosperi Mattia et al.",
      "keywords": "artificial intelligence; causal inference; disparity; epidemiology; human immunodeficiency virus; machine learning; real-world data; social determinants of health; surveillance",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36865610/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9977319",
      "ft_text_length": 1713,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9977319)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "36908403",
      "title": "Exploring potential barriers in equitable access to pediatric diagnostic imaging using machine learning.",
      "abstract": "In this work, we examine magnetic resonance imaging (MRI) and ultrasound (US) appointments at the Diagnostic Imaging (DI) department of a pediatric hospital to discover possible relationships between selected patient features and no-show or long waiting room time endpoints. The chosen features include age, sex, income, distance from the hospital, percentage of non-English speakers in a postal code, percentage of single caregivers in a postal code, appointment time slot (morning, afternoon, evening), and day of the week (Monday to Sunday). We trained univariate Logistic Regression (LR) models using the training sets and identified predictive (significant) features that remained significant in the test sets. We also implemented multivariate Random Forest (RF) models to predict the endpoints. We achieved Area Under the Receiver Operating Characteristic Curve (AUC) of 0.82 and 0.73 for predicting no-show and long waiting room time endpoints, respectively. The univariate LR analysis on DI appointments uncovered the effect of the time of appointment during the day/week, and patients' demographics such as income and the number of caregivers on the no-shows and long waiting room time endpoints. For predicting no-show, we found age, time slot, and percentage of single caregiver to be the most critical contributors. Age, distance, and percentage of non-English speakers were the most important features for our long waiting room time prediction models. We found no sex discrimination among the scheduled pediatric DI appointments. Nonetheless, inequities based on patient features such as low income and language barrier did exist.",
      "journal": "Frontiers in public health",
      "year": "2023",
      "doi": "10.3389/fpubh.2023.968319",
      "authors": "Taheri-Shirazi Maryam et al.",
      "keywords": "appointment scheduling; logistic regression; no-show; random forest; waiting room time",
      "mesh_terms": "Humans; Child; Appointments and Schedules; Magnetic Resonance Imaging; Logistic Models; Hospitals; Machine Learning",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36908403/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9998668",
      "ft_text_length": 35422,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9998668)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "36942183",
      "title": "Racial Equity in Healthcare Machine Learning: Illustrating Bias in Models With Minimal Bias Mitigation.",
      "abstract": "Background and objective While the potential of\u00a0machine learning (ML) in healthcare to positively impact human health\u00a0continues to grow, the potential for inequity in these methods must be assessed. In this study, we aimed to evaluate the presence of racial bias when five of the most common ML algorithms are used to create models with minimal processing to reduce racial bias. Methods By utilizing a CDC public database, we constructed models for the prediction of healthcare access (binary variable). Using area under the curve (AUC) as our performance metric, we calculated race-specific performance comparisons for each ML algorithm. We bootstrapped our entire analysis 20 times to produce confidence intervals\u00a0for our AUC performance metrics. Results With the exception of only a few cases, we found that the performance for the White group was, in general, significantly higher than that of the other racial groups across all ML algorithms. Additionally, we found that the most accurate algorithm in our modeling was Extreme Gradient Boosting (XGBoost) followed by random forest, naive Bayes, support vector machine (SVM), and k-nearest neighbors (KNN). Conclusion Our study illustrates the predictive perils of incorporating minimal racial bias mitigation in ML models, resulting in predictive disparities by race. This is particularly concerning in the setting of evidence for limited bias mitigation in healthcare-related ML. There needs to be more conversation, research, and guidelines surrounding methods for racial bias assessment and mitigation in healthcare-related ML models, both those currently used and those in development.",
      "journal": "Cureus",
      "year": "2023",
      "doi": "10.7759/cureus.35037",
      "authors": "Barton Michael et al.",
      "keywords": "data science; health equity; healthcare technology; machine learning; racial bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36942183/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10023594",
      "ft_text_length": 20415,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10023594)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "36961506",
      "title": "Human-Centered Design to Address Biases in Artificial Intelligence.",
      "abstract": "The potential of artificial intelligence (AI) to reduce health care disparities and inequities is recognized, but it can also exacerbate these issues if not implemented in an equitable manner. This perspective identifies potential biases in each stage of the AI life cycle, including data collection, annotation, machine learning model development, evaluation, deployment, operationalization, monitoring, and feedback integration. To mitigate these biases, we suggest involving a diverse group of stakeholders, using human-centered AI principles. Human-centered AI can help ensure that AI systems are designed and used in a way that benefits patients and society, which can reduce health disparities and inequities. By recognizing and addressing biases at each stage of the AI life cycle, AI can achieve its potential in health care.",
      "journal": "Journal of medical Internet research",
      "year": "2023",
      "doi": "10.2196/43251",
      "authors": "Chen You et al.",
      "keywords": "AI; application; artificial intelligence; benefits; biases; biomedical; care; design; development; health; human-centered; human-centered AI; patient; research",
      "mesh_terms": "Humans; Artificial Intelligence; Machine Learning; Healthcare Disparities; Bias",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36961506/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10132017",
      "ft_text_length": 33881,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10132017)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "36991077",
      "title": "An adversarial training framework for mitigating algorithmic biases in clinical machine learning.",
      "abstract": "Machine learning is becoming increasingly prominent in healthcare. Although its benefits are clear, growing attention is being given to how these tools may exacerbate existing biases and disparities. In this study, we introduce an adversarial training framework that is capable of mitigating biases that may have been acquired through data collection. We demonstrate this proposed framework on the real-world task of rapidly predicting COVID-19, and focus on mitigating site-specific (hospital) and demographic (ethnicity) biases. Using the statistical definition of equalized odds, we show that adversarial training improves outcome fairness, while still achieving clinically-effective screening performances (negative predictive values >0.98). We compare our method to previous benchmarks, and perform prospective and external validation across four independent hospital cohorts. Our method can be generalized to any outcomes, models, and definitions of fairness.",
      "journal": "NPJ digital medicine",
      "year": "2023",
      "doi": "10.1038/s41746-023-00805-y",
      "authors": "Yang Jenny et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36991077/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10050816",
      "ft_text_length": 51400,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10050816)",
      "ft_reason": "Included: bias central + approach content (16 indicators)"
    },
    {
      "pmid": "36997578",
      "title": "A multi-institutional study using artificial intelligence to provide reliable and fair feedback to surgeons.",
      "abstract": "BACKGROUND: Surgeons who receive reliable feedback on their performance quickly master the skills necessary for surgery. Such performance-based feedback can be provided by a recently-developed artificial intelligence (AI) system that assesses a surgeon's skills based on a surgical video while simultaneously highlighting aspects of the video most pertinent to the assessment. However, it remains an open question whether these highlights, or explanations, are equally reliable for all surgeons. METHODS: Here, we systematically quantify the reliability of AI-based explanations on surgical videos from three hospitals across two continents by comparing them to explanations generated by humans experts. To improve the reliability of AI-based explanations, we propose the strategy of training with explanations -TWIX -which uses human explanations as supervision to explicitly teach an AI system to highlight important video frames. RESULTS: We show that while AI-based explanations often align with human explanations, they are not equally reliable for different sub-cohorts of surgeons (e.g., novices vs. experts), a phenomenon we refer to as an explanation bias. We also show that TWIX enhances the reliability of AI-based explanations, mitigates the explanation bias, and improves the performance of AI systems across hospitals. These findings extend to a training environment where medical students can be provided with feedback today. CONCLUSIONS: Our study informs the impending implementation of AI-augmented surgical training and surgeon credentialing programs, and contributes to the safe and fair democratization of surgery. Surgeons aim to master skills necessary for surgery. One such skill is suturing which involves connecting objects together through a series of stitches. Mastering these surgical skills can be improved by providing surgeons with feedback on the quality of their performance. However, such feedback is often absent from surgical practice. Although performance-based feedback can be provided, in theory, by recently-developed artificial intelligence (AI) systems that use a computational model to assess a surgeon\u2019s skill, the reliability of this feedback remains unknown. Here, we compare AI-based feedback to that provided by human experts and demonstrate that they often overlap with one another. We also show that explicitly teaching an AI system to align with human feedback further improves the reliability of AI-based feedback on new videos of surgery. Our findings outline the potential of AI systems to support the training of surgeons by providing feedback that is reliable and focused on a particular skill, and guide programs that give surgeons qualifications by complementing skill assessments with explanations that increase the trustworthiness of such assessments.",
      "journal": "Communications medicine",
      "year": "2023",
      "doi": "10.1038/s43856-023-00263-3",
      "authors": "Kiyasseh Dani et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36997578/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10063640",
      "ft_text_length": 56197,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10063640)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "36997642",
      "title": "Human visual explanations mitigate bias in AI-based assessment of surgeon skills.",
      "abstract": "Artificial intelligence (AI) systems can now reliably assess surgeon skills through videos of intraoperative surgical activity. With such systems informing future high-stakes decisions such as whether to credential surgeons and grant them the privilege to operate on patients, it is critical that they treat all surgeons fairly. However, it remains an open question whether surgical AI systems exhibit bias against surgeon sub-cohorts, and, if so, whether such bias can be mitigated. Here, we examine and mitigate the bias exhibited by a family of surgical AI systems-SAIS-deployed on videos of robotic surgeries from three geographically-diverse hospitals (USA and EU). We show that SAIS exhibits an underskilling bias, erroneously downgrading surgical performance, and an overskilling bias, erroneously upgrading surgical performance, at different rates across surgeon sub-cohorts. To mitigate such bias, we leverage a strategy -TWIX-which teaches an AI system to provide a visual explanation for its skill assessment that otherwise would have been provided by human experts. We show that whereas baseline strategies inconsistently mitigate algorithmic bias, TWIX can effectively mitigate the underskilling and overskilling bias while simultaneously improving the performance of these AI systems across hospitals. We discovered that these findings carry over to the training environment where we assess medical students' skills today. Our study is a critical prerequisite to the eventual implementation of AI-augmented global surgeon credentialing programs, ensuring that all surgeons are treated fairly.",
      "journal": "NPJ digital medicine",
      "year": "2023",
      "doi": "10.1038/s41746-023-00766-2",
      "authors": "Kiyasseh Dani et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36997642/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10063676",
      "ft_text_length": 59681,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10063676)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "37046593",
      "title": "From Head and Neck Tumour and Lymph Node Segmentation to Survival Prediction on PET/CT: An End-to-End Framework Featuring Uncertainty, Fairness, and Multi-Region Multi-Modal Radiomics.",
      "abstract": "Automatic delineation and detection of the primary tumour (GTVp) and lymph nodes (GTVn) using PET and CT in head and neck cancer and recurrence-free survival prediction can be useful for diagnosis and patient risk stratification. We used data from nine different centres, with 524 and 359 cases used for training and testing, respectively. We utilised posterior sampling of the weight space in the proposed segmentation model to estimate the uncertainty for false positive reduction. We explored the prognostic potential of radiomics features extracted from the predicted GTVp and GTVn in PET and CT for recurrence-free survival prediction and used SHAP analysis for explainability. We evaluated the bias of models with respect to age, gender, chemotherapy, HPV status, and lesion size. We achieved an aggregate Dice score of 0.774 and 0.760 on the test set for GTVp and GTVn, respectively. We observed a per image false positive reduction of 19.5% and 7.14% using the uncertainty threshold for GTVp and GTVn, respectively. Radiomics features extracted from GTVn in PET and from both GTVp and GTVn in CT are the most prognostic, and our model achieves a C-index of 0.672 on the test set. Our framework incorporates uncertainty estimation, fairness, and explainability, demonstrating the potential for accurate detection and risk stratification.",
      "journal": "Cancers",
      "year": "2023",
      "doi": "10.3390/cancers15071932",
      "authors": "Salahuddin Zohaib et al.",
      "keywords": "CT radiomics; PET radiomics; explainability; fair artificial intelligence; head and neck cancer; segmentation; uncertainty estimation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37046593/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10093277",
      "ft_text_length": 38588,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10093277)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "37063493",
      "title": "Patient-specific uncertainty and bias quantification of non-transparent convolutional neural network model through knowledge distillation and Bayesian deep learning.",
      "abstract": "Assessing the reliability of convolutional neural network (CNN)-based CT imaging techniques is critical for reliable deployment in practice. Some evaluation methods exist but require full access to target CNN architecture and training data, something not available for proprietary or commercial algorithms. Moreover, there is a lack of systematic evaluation methods. To address these issues, we propose a patient-specific uncertainty and bias quantification (UNIQ) method that integrates knowledge distillation and Bayesian deep learning. Knowledge distillation creates a transparent CNN (\"Student CNN\") to approximate the target non-transparent CNN (\"Teacher CNN\"). Student CNN is built as a Bayesian-deep-learning-based probabilistic CNN that, for each input, always generates statistical distribution of the corresponding outputs, and characterizes predictive mean and two major uncertainties - data and model uncertainty. UNIQ was evaluated using a low-dose CT denoising task. Patient and phantom scans with routine-dose and synthetic quarter-dose were used to create training, validation, and testing sets. To demonstrate, Unet and Resnet were used as backbones of Teacher CNN and Student CNN respectively and were trained using independent training sets. Student Resnet was qualitatively and quantitatively evaluated. The pixel-wise predictive mean, data uncertainty, and model uncertainty from Student Resnet were very similar to the counterparts from Teacher Unet (mean-absolute-error: predictive mean 1.5HU, data uncertainty 1.8HU, model uncertainty 1.3HU; mean 2D correlation coefficient: total uncertainty 0.90, data uncertainty 0.86, model uncertainty 0.83). The proposed UNIQ can potentially systematically characterize the reliability of non-transparent CNN models used in CT.",
      "journal": "Proceedings of SPIE--the International Society for Optical Engineering",
      "year": "2023",
      "doi": "10.1117/12.2654318",
      "authors": "Gong Hao et al.",
      "keywords": "Bayesian neural network; Deep learning; bias; knowledge distillation; uncertainty; x-ray CT",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37063493/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10100102",
      "ft_text_length": 1790,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10100102)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37090627",
      "title": "A Comprehensive and Bias-Free Machine Learning Approach for Risk Prediction of Preeclampsia with Severe Features in a Nulliparous Study Cohort.",
      "abstract": "OBJECTIVE: Preeclampsia is one of the leading causes of maternal morbidity, with consequences during and after pregnancy. Because of its diverse clinical presentation, preeclampsia is an adverse pregnancy outcome that is uniquely challenging to predict and manage. In this paper, we developed machine learning models that predict the onset of preeclampsia with severe features or eclampsia at discrete time points in a nulliparous pregnant study cohort. MATERIALS AND METHODS: The prospective study cohort to which we applied machine learning is the Nulliparous Pregnancy Outcomes Study: Monitoring Mothers-to-be (nuMoM2b) study, which contains information from eight clinical sites across the US. Maternal serum samples were collected for 1,857 individuals between the first and second trimesters. These patients with serum samples collected are selected as the final cohort. RESULTS: Our prediction models achieved an AUROC of 0.72 (95% CI, 0.69-0.76), 0.75 (95% CI, 0.71-0.79), and 0.77 (95% CI, 0.74-0.80), respectively, for the three visits. Our initial models were biased toward non-Hispanic black participants with a high predictive equality ratio of 1.31. We corrected this bias and reduced this ratio to 1.14. The top features stress the importance of using several tests, particularly for biomarkers and ultrasound measurements. Placental analytes were strong predictors for screening for the early onset of preeclampsia with severe features in the first two trimesters. CONCLUSION: Experiments suggest that it is possible to create racial bias-free early screening models to predict the patients at risk of developing preeclampsia with severe features or eclampsia nulliparous pregnant study cohort.",
      "journal": "Research square",
      "year": "2023",
      "doi": "10.21203/rs.3.rs-2635419/v1",
      "authors": "Lin Yun et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37090627/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10120773",
      "ft_text_length": 22540,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10120773)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "37125409",
      "title": "Toward fairness in artificial intelligence for medical image analysis: identification and mitigation of potential biases in the roadmap from data collection to model deployment.",
      "abstract": "PURPOSE: To recognize and address various sources of bias essential for algorithmic fairness and trustworthiness and to contribute to a just and equitable deployment of AI in medical imaging, there is an increasing interest in developing medical imaging-based machine learning methods, also known as medical imaging artificial intelligence (AI), for the detection, diagnosis, prognosis, and risk assessment of disease with the goal of clinical implementation. These tools are intended to help improve traditional human decision-making in medical imaging. However, biases introduced in the steps toward clinical deployment may impede their intended function, potentially exacerbating inequities. Specifically, medical imaging AI can propagate or amplify biases introduced in the many steps from model inception to deployment, resulting in a systematic difference in the treatment of different groups. APPROACH: Our multi-institutional team included medical physicists, medical imaging artificial intelligence/machine learning (AI/ML) researchers, experts in AI/ML bias, statisticians, physicians, and scientists from regulatory bodies. We identified sources of bias in AI/ML, mitigation strategies for these biases, and developed recommendations for best practices in medical imaging AI/ML development. RESULTS: Five main steps along the roadmap of medical imaging AI/ML were identified: (1)\u00a0data collection, (2)\u00a0data preparation and annotation, (3)\u00a0model development, (4)\u00a0model evaluation, and (5)\u00a0model deployment. Within these steps, or bias categories, we identified 29 sources of potential bias, many of which can impact multiple steps, as well as mitigation strategies. CONCLUSIONS: Our findings provide a valuable resource to researchers, clinicians, and the public at large.",
      "journal": "Journal of medical imaging (Bellingham, Wash.)",
      "year": "2023",
      "doi": "10.1117/1.JMI.10.6.061104",
      "authors": "Drukker Karen et al.",
      "keywords": "artificial intelligence; bias; fairness; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37125409/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10129875",
      "ft_text_length": 81384,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10129875)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "37128361",
      "title": "Assessing Phenotype Definitions for Algorithmic Fairness.",
      "abstract": "Phenotyping is a core, routine activity in observational health research. Cohorts impact downstream analyses, such as how a condition is characterized, how patient risk is defined, and what treatments are studied. It is thus critical to ensure that cohorts are representative of all patients, independently of their demographics or social determinants of health. In this paper, we propose a set of best practices to assess the fairness of phenotype definitions. We leverage established fairness metrics commonly used in predictive models and relate them to commonly used epidemiological metrics. We describe an empirical study for Crohn's disease and diabetes type 2, each with multiple phenotype definitions taken from the literature across gender and race. We show that the different phenotype definitions exhibit widely varying and disparate performance according to the different fairness metrics and subgroups. We hope that the proposed best practices can help in constructing fair and inclusive phenotype definitions.",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "year": "2022",
      "doi": "",
      "authors": "Sun Tony Y et al.",
      "keywords": "",
      "mesh_terms": "Humans; Crohn Disease; Phenotype",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37128361/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10148336",
      "ft_text_length": 1023,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10148336)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37128420",
      "title": "Fairly Predicting Graft Failure in Liver Transplant for Organ Assigning.",
      "abstract": "Liver transplant is an essential therapy performed for severe liver diseases. The fact of scarce liver resources makes the organ assigning crucial. Model for End-stage Liver Disease (MELD) score is a widely adopted criterion when making organ distribution decisions. However, it ignores post-transplant outcomes and organ/donor features. These limitations motivate the emergence of machine learning (ML) models. Unfortunately, ML models could be unfair and trigger bias against certain groups of people. To tackle this problem, this work proposes a fair machine learning framework targeting graft failure prediction in liver transplant. Specifically, knowledge distillation is employed to handle dense and sparse features by combining the advantages of tree models and neural networks. A two-step debiasing method is tailored for this framework to enhance fairness. Experiments are conducted to analyze unfairness issues in existing models and demonstrate the superiority of our method in both prediction and fairness performance.",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "year": "2022",
      "doi": "",
      "authors": "Ding Sirui et al.",
      "keywords": "",
      "mesh_terms": "Humans; End Stage Liver Disease; Liver Transplantation; Severity of Illness Index; Neural Networks, Computer; Machine Learning; Retrospective Studies",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37128420/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10148275",
      "ft_text_length": 1030,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10148275)",
      "ft_reason": "Included: substantial approach content (3 indicators)"
    },
    {
      "pmid": "37185650",
      "title": "Predictive care: a protocol for a computational ethnographic approach to building fair models of inpatient violence in emergency psychiatry.",
      "abstract": "INTRODUCTION: Managing violence or aggression is an ongoing challenge in emergency psychiatry. Many patients identified as being at risk do not go on to become violent or aggressive. Efforts to automate the assessment of risk involve training machine learning (ML) models on data from electronic health records (EHRs) to predict these behaviours. However, no studies to date have examined which patient groups may be over-represented in false positive predictions, despite evidence of social and clinical biases that may lead to higher perceptions of risk in patients defined by intersecting features (eg, race, gender). Because risk assessment can impact psychiatric care (eg, via coercive measures, such as restraints), it is unclear which patients might be underserved or harmed by the application of ML. METHODS AND ANALYSIS: We pilot a computational ethnography to study how the integration of ML into risk assessment might impact acute psychiatric care, with a focus on how EHR data is compiled and used to predict a risk of violence or aggression. Our objectives include: (1) evaluating an ML model trained on psychiatric EHRs to predict violent or aggressive incidents for intersectional bias; and (2) completing participant observation and qualitative interviews in an emergency psychiatric setting to explore how social, clinical and structural biases are encoded in the training data. Our overall aim is to study the impact of ML applications in acute psychiatry on marginalised and underserved patient groups. ETHICS AND DISSEMINATION: The project was approved by the research ethics board at The Centre for Addiction and Mental Health (053/2021). Study findings will be presented in peer-reviewed journals, conferences and shared with service users and providers.",
      "journal": "BMJ open",
      "year": "2023",
      "doi": "10.1136/bmjopen-2022-069255",
      "authors": "Sikstrom Laura et al.",
      "keywords": "ethnography; health equity; machine learning; psychiatry; risk assessment",
      "mesh_terms": "Humans; Inpatients; Violence; Aggression; Anthropology, Cultural; Psychiatry",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37185650/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10151964",
      "ft_text_length": 35648,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10151964)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "37211197",
      "title": "Multi-task learning with dynamic re-weighting to achieve fairness in healthcare predictive modeling.",
      "abstract": "The emphasis on fairness in predictive healthcare modeling has increased in popularity as an approach for overcoming biases in automated decision-making systems. The aim is to guarantee that sensitive characteristics like gender, race, and ethnicity do not influence prediction outputs. Numerous algorithmic strategies have been proposed to reduce bias in prediction results, mitigate prejudice toward minority groups and promote prediction fairness. The goal of these strategies is to ensure that model prediction performance does not exhibit significant disparity among sensitive groups. In this study, we propose a novel fairness-achieving scheme based on multitask learning, which fundamentally differs from conventional fairness-achieving techniques, including altering data distributions and constraint optimization through regularizing fairness metrics or tampering with prediction outcomes. By dividing predictions on different sub-populations into separate tasks, we view the fairness problem as a task-balancing problem. To ensure fairness during the model-training process, we suggest a novel dynamic re-weighting approach. Fairness is achieved by dynamically modifying the gradients of various prediction tasks during neural network back-propagation, and this novel technique applies to a wide range of fairness criteria. We conduct tests on a real-world use case to predict sepsis patients' mortality risk. Our approach satisfies that it can reduce the disparity between subgroups by 98% while only losing less than 4% of prediction accuracy.",
      "journal": "Journal of biomedical informatics",
      "year": "2023",
      "doi": "10.1016/j.jbi.2023.104399",
      "authors": "Li Can et al.",
      "keywords": "Fairness; Healthcare predictive modeling; Multi-task learning",
      "mesh_terms": "Humans; Learning; Benchmarking; Minority Groups; Neural Networks, Computer; Sepsis",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37211197/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10665114",
      "ft_text_length": 1557,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10665114)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37252970",
      "title": "Cohort bias in predictive risk assessments of future criminal justice system involvement.",
      "abstract": "Risk assessment instruments (RAIs) are widely used to aid high-stakes decision-making in criminal justice settings and other areas such as health care and child welfare. These tools, whether using machine learning or simpler algorithms, typically assume a time-invariant relationship between predictors and outcome. Because societies are themselves changing and not just individuals, this assumption may be violated in many behavioral settings, generating what we call cohort bias. Analyzing criminal histories in a cohort-sequential longitudinal study of children, we demonstrate that regardless of model type or predictor sets, a tool trained to predict the likelihood of arrest between the ages of 17 and 24 y on older birth cohorts systematically overpredicts the likelihood of arrest for younger birth cohorts over the period 1995 to 2020. Cohort bias is found for both relative and absolute risks, and it persists for all racial groups and within groups at highest risk for arrest. The results imply that cohort bias is an underappreciated mechanism generating inequality in contacts with the criminal legal system that is distinct from racial bias. Cohort bias is a challenge not only for predictive instruments with respect to crime and justice, but also for RAIs more broadly.",
      "journal": "Proceedings of the National Academy of Sciences of the United States of America",
      "year": "2023",
      "doi": "10.1073/pnas.2301990120",
      "authors": "Montana Erika et al.",
      "keywords": "bias; cohort; criminal justice; risk assessment; social change",
      "mesh_terms": "Child; Humans; Adolescent; Young Adult; Adult; Longitudinal Studies; Criminal Law; Crime; Cohort Studies; Risk Assessment",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37252970/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10265989",
      "ft_text_length": 38353,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10265989)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "37289496",
      "title": "Gender Bias When Using Artificial Intelligence to Assess Anorexia Nervosa on Social Media: Data-Driven Study.",
      "abstract": "BACKGROUND: Social media sites are becoming an increasingly important source of information about mental health disorders. Among them, eating disorders are complex psychological problems that involve unhealthy eating habits. In particular, there is evidence showing that signs and symptoms of anorexia nervosa can be traced in social media platforms. Knowing that input data biases tend to be amplified by artificial intelligence algorithms and, in particular, machine learning, these methods should be revised to mitigate biased discrimination in such important domains. OBJECTIVE: The main goal of this study was to detect and analyze the performance disparities across genders in algorithms trained for the detection of anorexia nervosa on social media posts. We used a collection of automated predictors trained on a data set in Spanish containing cases of 177 users that showed signs of anorexia (471,262 tweets) and 326 control cases (910,967 tweets). METHODS: We first inspected the predictive performance differences between the algorithms for male and female users. Once biases were detected, we applied a feature-level bias characterization to evaluate the source of such biases and performed a comparative analysis of such features and those that are relevant for clinicians. Finally, we showcased different bias mitigation strategies to develop fairer automated classifiers, particularly for risk assessment in sensitive domains. RESULTS: Our results revealed concerning predictive performance differences, with substantially higher false negative rates (FNRs) for female samples (FNR=0.082) compared with male samples (FNR=0.005). The findings show that biological processes and suicide risk factors were relevant for classifying positive male cases, whereas age, emotions, and personal concerns were more relevant for female cases. We also proposed techniques for bias mitigation, and we could see that, even though disparities can be mitigated, they cannot be eliminated. CONCLUSIONS: We concluded that more attention should be paid to the assessment of biases in automated methods dedicated to the detection of mental health issues. This is particularly relevant before the deployment of systems that are thought to assist clinicians, especially considering that the outputs of such systems can have an impact on the diagnosis of people at risk.",
      "journal": "Journal of medical Internet research",
      "year": "2023",
      "doi": "10.2196/45184",
      "authors": "Solans Noguero David et al.",
      "keywords": "anorexia nervosa; artificial intelligence; gender bias; social media",
      "mesh_terms": "Female; Humans; Male; Anorexia Nervosa; Artificial Intelligence; Sexism; Social Media; Feeding and Eating Disorders",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37289496/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10288345",
      "ft_text_length": 59535,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10288345)",
      "ft_reason": "Included: bias central + approach content (11 indicators)"
    },
    {
      "pmid": "37311802",
      "title": "Bias in AI-based models for medical applications: challenges and mitigation strategies.",
      "abstract": "Artificial intelligence systems are increasingly being applied to healthcare. In surgery, AI applications hold promise as tools to predict surgical outcomes, assess technical skills, or guide surgeons intraoperatively via computer vision. On the other hand, AI systems can also suffer from bias, compounding existing inequities in socioeconomic status, race, ethnicity, religion, gender, disability, or sexual orientation. Bias particularly impacts disadvantaged populations, which can be subject to algorithmic predictions that are less accurate or underestimate the need for care. Thus, strategies for detecting and mitigating bias are pivotal for creating AI technology that is generalizable and fair. Here, we discuss a recent study that developed a new strategy to mitigate bias in surgical AI systems.",
      "journal": "NPJ digital medicine",
      "year": "2023",
      "doi": "10.1038/s41746-023-00858-z",
      "authors": "Mittermaier Mirja et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37311802/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10264403",
      "ft_text_length": 8396,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10264403)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "37312937",
      "title": "Benzodiazepine-related dementia risks and protopathic biases revealed by multiple-kernel learning with electronic medical records.",
      "abstract": "OBJECTIVES: To simultaneously estimate how the risk of incident dementia nonlinearly varies with the administration period and cumulative dose of benzodiazepines, the duration of disorders with an indication for benzodiazepines, and other potential confounders, with the goal of settling the controversy over the role of benzodiazepines in the development of dementia. METHODS: The classical hazard model was extended using the techniques of multiple-kernel learning. Regularised maximum-likelihood estimation, including determination of hyperparameter values with 10-fold cross-validation, bootstrap goodness-of-fit test, and bootstrap estimation of confidence intervals, was applied to cohorts retrospectively extracted from electronic medical records of our university hospitals between 1 November 2004 and 31 July 2020. The analysis was mainly focused on 8160 patients aged 40 or older with new onset of insomnia, affective disorders, or anxiety disorders, who were followed up for 4.10\u00b13.47 years. RESULTS: Besides previously reported risk associations, we detected significant nonlinear risk variations over 2-4 years attributable to the duration of insomnia and anxiety disorders, and to the administration period of short-acting benzodiazepines. After nonlinear adjustment for potential confounders, we observed no significant risk associations with long-term use of benzodiazepines. CONCLUSIONS: The pattern of the detected nonlinear risk variations suggested reverse causation and confounding. Their putative bias effects over 2-4 years suggested similar biases in previously reported results. These results, together with the lack of significant risk associations with long-term use of benzodiazepines, suggested the need to reconsider previous results and methods for future analysis.",
      "journal": "Digital health",
      "year": "2023",
      "doi": "10.1177/20552076231178577",
      "authors": "Hayakawa Takashi et al.",
      "keywords": "Dementia; benzodiazepines; electronic medical records; kernel method; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37312937/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10259140",
      "ft_text_length": 67999,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10259140)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37318804",
      "title": "Racial and Ethnic Bias in Risk Prediction Models for Colorectal Cancer Recurrence When Race and Ethnicity Are Omitted as Predictors.",
      "abstract": "IMPORTANCE: Including race and ethnicity as a predictor in clinical risk prediction algorithms has received increased scrutiny, but there continues to be a lack of empirical studies addressing whether simply omitting race and ethnicity from the algorithms will ultimately affect decision-making for patients of minoritized racial and ethnic groups. OBJECTIVE: To examine whether including race and ethnicity as a predictor in a colorectal cancer recurrence risk algorithm is associated with racial bias, defined as racial and ethnic differences in model accuracy that could potentially lead to unequal treatment. DESIGN, SETTING, AND PARTICIPANTS: This retrospective prognostic study was conducted using data from a large integrated health care system in Southern California for patients with colorectal cancer who received primary treatment between 2008 and 2013 and follow-up until December 31, 2018. Data were analyzed from January 2021 to June 2022. MAIN OUTCOMES AND MEASURES: Four Cox proportional hazards regression prediction models were fitted to predict time from surveillance start to cancer recurrence: (1) a race-neutral model that explicitly excluded race and ethnicity as a predictor, (2) a race-sensitive model that included race and ethnicity, (3) a model with 2-way interactions between clinical predictors and race and ethnicity, and (4) separate models by race and ethnicity. Algorithmic fairness was assessed using model calibration, discriminative ability, false-positive and false-negative rates, positive predictive value (PPV), and negative predictive value (NPV). RESULTS: The study cohort included 4230 patients (mean [SD] age, 65.3 [12.5] years; 2034 [48.1%] female; 490 [11.6%] Asian, Hawaiian, or Pacific Islander; 554 [13.1%] Black or African American; 937 [22.1%] Hispanic; and 2249 [53.1%] non-Hispanic White). The race-neutral model had worse calibration, NPV, and false-negative rates among racial and ethnic minority subgroups than non-Hispanic White individuals (eg, false-negative rate for Hispanic patients: 12.0% [95% CI, 6.0%-18.6%]; for non-Hispanic White patients: 3.1% [95% CI, 0.8%-6.2%]). Adding race and ethnicity as a predictor improved algorithmic fairness in calibration slope, discriminative ability, PPV, and false-negative rates (eg, false-negative rate for Hispanic patients: 9.2% [95% CI, 3.9%-14.9%]; for non-Hispanic White patients: 7.9% [95% CI, 4.3%-11.9%]). Inclusion of race interaction terms or using race-stratified models did not improve model fairness, likely due to small sample sizes in subgroups. CONCLUSIONS AND RELEVANCE: In this prognostic study of the racial bias in a cancer recurrence risk algorithm, removing race and ethnicity as a predictor worsened algorithmic fairness in multiple measures, which could lead to inappropriate care recommendations for patients who belong to minoritized racial and ethnic groups. Clinical algorithm development should include evaluation of fairness criteria to understand the potential consequences of removing race and ethnicity for health inequities.",
      "journal": "JAMA network open",
      "year": "2023",
      "doi": "10.1001/jamanetworkopen.2023.18495",
      "authors": "Khor Sara et al.",
      "keywords": "",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Black or African American; Colorectal Neoplasms; Ethnicity; Hispanic or Latino; Minority Groups; Retrospective Studies; White; Asian American Native Hawaiian and Pacific Islander",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37318804/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10273018",
      "ft_text_length": 29070,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10273018)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37350883",
      "title": "Avoiding Biased Clinical Machine Learning Model Performance Estimates in the Presence of Label Selection.",
      "abstract": "When evaluating the performance of clinical machine learning models, one must consider the deployment population. When the population of patients with observed labels is only a subset of the deployment population (label selection), standard model performance estimates on the observed population may be misleading. In this study we describe three classes of label selection and simulate five causally distinct scenarios to assess how particular selection mechanisms bias a suite of commonly reported binary machine learning model performance metrics. Simulations reveal that when selection is affected by observed features, naive estimates of model discrimination may be misleading. When selection is affected by labels, naive estimates of calibration fail to reflect reality. We borrow traditional weighting estimators from causal inference literature and find that when selection probabilities are properly specified, they recover full population estimates. We then tackle the real-world task of monitoring the performance of deployed machine learning models whose interactions with clinicians feed-back and affect the selection mechanism of the labels. We train three machine learning models to flag low-yield laboratory diagnostics, and simulate their intended consequence of reducing wasteful laboratory utilization. We find that naive estimates of AUROC on the observed population undershoot actual performance by up to 20%. Such a disparity could be large enough to lead to the wrongful termination of a successful clinical decision support tool. We propose an altered deployment procedure, one that combines injected randomization with traditional weighted estimates, and find it recovers true model performance.",
      "journal": "AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science",
      "year": "2023",
      "doi": "",
      "authors": "Corbin Conor K et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37350883/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10283136",
      "ft_text_length": 1720,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10283136)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37377633",
      "title": "Bias Analysis in Healthcare Time Series (BAHT) Decision Support Systems from Meta Data.",
      "abstract": "One of the hindrances in the widespread acceptance of deep learning-based decision support systems in healthcare is bias. Bias in its many forms occurs in the datasets used to train and test deep learning models and is amplified when deployed in the real world, leading to challenges such as model drift. Recent advancements in the field of deep learning have led to the deployment of deployable automated healthcare diagnosis decision support systems at hospitals as well as tele-medicine through IoT devices. Research has been focused primarily on the development and improvement of these systems leaving a gap in the analysis of the fairness. The domain of FAccT ML (fairness, accountability, and transparency) accounts for the analysis of these deployable machine learning systems. In this work, we present a framework for bias analysis in healthcare time series (BAHT) signals such as electrocardiogram (ECG) and electroencephalogram (EEG). BAHT provides a graphical interpretive analysis of bias in the training, testing datasets in terms of protected variables, and analysis of bias amplification by the trained supervised learning model for time series healthcare decision support systems. We thoroughly investigate three prominent time series ECG and EEG healthcare datasets used for model training and research. We show the extensive presence of bias in the datasets leads to potentially biased or unfair machine-learning models. Our experiments also demonstrate the amplification of identified bias with an observed maximum of 66.66%. We investigate the effect of model drift due to unanalyzed bias in datasets and algorithms. Bias mitigation though prudent is a nascent area of research. We present experiments and analyze the most prevalently accepted bias mitigation strategies of under-sampling, oversampling, and the use of synthetic data for balancing the dataset through augmentation. It is important that healthcare models, datasets, and bias mitigation strategies should be properly analyzed for a fair unbiased delivery of service.",
      "journal": "Journal of healthcare informatics research",
      "year": "2023",
      "doi": "10.1007/s41666-023-00133-6",
      "authors": "Dakshit Sagnik et al.",
      "keywords": "Bias analysis; Bias mitigation; Decision support systems; Fairness; Synthetic data",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37377633/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10290973",
      "ft_text_length": 2052,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10290973)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "37390116",
      "title": "Community perspectives on AI/ML and health equity: AIM-AHEAD nationwide stakeholder listening sessions.",
      "abstract": "Artificial intelligence and machine learning (AI/ML) tools have the potential to improve health equity. However, many historically underrepresented communities have not been engaged in AI/ML training, research, and infrastructure development. Therefore, AIM-AHEAD (Artificial Intelligence/Machine Learning Consortium to Advance Health Equity and Researcher Diversity) seeks to increase participation and engagement of researchers and communities through mutually beneficial partnerships. The purpose of this paper is to summarize feedback from listening sessions conducted by the AIM-AHEAD Coordinating Center in February 2022, titled the \"AIM-AHEAD Community Building Convention (ACBC).\" A total of six listening sessions were held over three days. A total of 977 people registered with AIM-AHEAD to attend ACBC and 557 individuals attended the listening sessions across stakeholder groups. Facilitators led the conversation based on a series of guiding questions, and responses were captured through voice and chat via the Slido platform. A professional third-party provider transcribed the audio. Qualitative analysis included data from transcripts and chat logs. Thematic analysis was then used to identify common and unique themes across all transcripts. Six main themes arose from the sessions. Attendees felt that storytelling would be a powerful tool in communicating the impact of AI/ML in promoting health equity, trust building is vital and can be fostered through existing trusted relationships, and diverse communities should be involved every step of the way. Attendees shared a wealth of information that will guide AIM-AHEAD's future activities. The sessions highlighted the need for researchers to translate AI/ML concepts into vignettes that are digestible to the larger public, the importance of diversity, and how open-science platforms can be used to encourage multi-disciplinary collaboration. While the sessions confirmed some of the existing barriers in applying AI/ML for health equity, they also offered new insights that were captured in the six themes.",
      "journal": "PLOS digital health",
      "year": "2023",
      "doi": "10.1371/journal.pdig.0000288",
      "authors": "Vishwanatha Jamboor K et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37390116/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10313007",
      "ft_text_length": 52767,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10313007)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37463884",
      "title": "Detecting shortcut learning for fair medical AI using shortcut testing.",
      "abstract": "Machine learning (ML) holds great promise for improving healthcare, but it is critical to ensure that its use will not propagate or amplify health disparities. An important step is to characterize the (un)fairness of ML models-their tendency to perform differently across subgroups of the population-and to understand its underlying mechanisms. One potential driver of algorithmic unfairness, shortcut learning, arises when ML models base predictions on improper correlations in the training data. Diagnosing this phenomenon is difficult as sensitive attributes may be causally linked with disease. Using multitask learning, we propose a method to directly test for the presence of shortcut learning in clinical ML systems and demonstrate its application to clinical tasks in radiology and dermatology. Finally, our approach reveals instances when shortcutting is not responsible for unfairness, highlighting the need for a holistic approach to fairness mitigation in medical AI.",
      "journal": "Nature communications",
      "year": "2023",
      "doi": "10.1038/s41467-023-39902-7",
      "authors": "Brown Alexander et al.",
      "keywords": "",
      "mesh_terms": "Health Facilities; Machine Learning",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37463884/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10354021",
      "ft_text_length": 47215,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10354021)",
      "ft_reason": "Included: bias central + approach content (7 indicators)"
    },
    {
      "pmid": "37485306",
      "title": "Accelerating voxelwise annotation of cross-sectional imaging through AI collaborative labeling with quality assurance and bias mitigation.",
      "abstract": "BACKGROUND: precision-medicine quantitative tools for cross-sectional imaging require painstaking labeling of targets that vary considerably in volume, prohibiting scaling of data annotation efforts and supervised training to large datasets for robust and generalizable clinical performance. A straight-forward time-saving strategy involves manual editing of AI-generated labels, which we call AI-collaborative labeling (AICL). Factors affecting the efficacy and utility of such an approach are unknown. Reduction in time effort is not well documented. Further, edited AI labels may be prone to automation bias. PURPOSE: In this pilot, using a cohort of CTs with intracavitary hemorrhage, we evaluate both time savings and AICL label quality and propose criteria that must be met for using AICL annotations as a high-throughput, high-quality ground truth. METHODS: 57 CT scans of patients with traumatic intracavitary hemorrhage were included. No participant recruited for this study had previously interpreted the scans. nnU-net models trained on small existing datasets for each feature (hemothorax/hemoperitoneum/pelvic hematoma; n = 77-253) were used in inference. Two common scenarios served as baseline comparison- de novo expert manual labeling, and expert edits of trained staff labels. Parameters included time effort and image quality graded by a blinded independent expert using a 9-point scale. The observer also attempted to discriminate AICL and expert labels in a random subset (n = 18). Data were compared with ANOVA and post-hoc paired signed rank tests with Bonferroni correction. RESULTS: AICL reduced time effort 2.8-fold compared to staff label editing, and 8.7-fold compared to expert labeling (corrected p < 0.0006). Mean Likert grades for AICL (8.4, SD:0.6) were significantly higher than for expert labels (7.8, SD:0.9) and edited staff labels (7.7, SD:0.8) (corrected p < 0.0006). The independent observer failed to correctly discriminate AI and human labels. CONCLUSION: For our use case and annotators, AICL facilitates rapid large-scale curation of high-quality ground truth. The proposed quality control regime can be employed by other investigators prior to embarking on AICL for segmentation tasks in large datasets.",
      "journal": "Frontiers in radiology",
      "year": "2023",
      "doi": "10.3389/fradi.2023.1202412",
      "authors": "Dreizin David et al.",
      "keywords": "AI assisted annotation; AI-collaborative labeling; CT volumetry; artificial intelligence\u2014AI; computed tomography; human in the loop (HITL); quantitative visualization; trauma",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37485306/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10362988",
      "ft_text_length": 35009,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10362988)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37503225",
      "title": "Predicting prenatal depression and assessing model bias using machine learning models.",
      "abstract": "Perinatal depression (PND) is one of the most common medical complications during pregnancy and postpartum period, affecting 10-20% of pregnant individuals. Black and Latina women have higher rates of PND, yet they are less likely to be diagnosed and receive treatment. Machine learning (ML) models based on Electronic Medical Records (EMRs) have been effective in predicting postpartum depression in middle-class White women but have rarely included sufficient proportions of racial and ethnic minorities, which contributed to biases in ML models for minority women. Our goal is to determine whether ML models could serve to predict depression in early pregnancy in racial/ethnic minority women by leveraging EMR data. We extracted EMRs from a hospital in a large urban city that mostly served low-income Black and Hispanic women (N=5,875) in the U.S. Depressive symptom severity was assessed from a self-reported questionnaire, PHQ-9. We investigated multiple ML classifiers, used Shapley Additive Explanations (SHAP) for model interpretation, and determined model prediction bias with two metrics, Disparate Impact, and Equal Opportunity Difference. While ML model (Elastic Net) performance was low (ROCAUC=0.67), we identified well-known factors associated with PND, such as unplanned pregnancy and being single, as well as underexplored factors, such as self-report pain levels, lower levels of prenatal vitamin supplement intake, asthma, carrying a male fetus, and lower platelet levels blood. Our findings showed that despite being based on a sample mostly composed of 75% low-income minority women (54% Black and 27% Latina), the model performance was lower for these communities. In conclusion, ML models based on EMRs could moderately predict depression in early pregnancy, but their performance is biased against low-income minority women.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2023",
      "doi": "10.1101/2023.07.17.23292587",
      "authors": "Huang Yongchao et al.",
      "keywords": "Perinatal depression; electronic medical records; machine learning; model performance bias",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37503225/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10371186",
      "ft_text_length": 31961,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10371186)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "37514877",
      "title": "Development of Debiasing Technique for Lung Nodule Chest X-ray Datasets to Generalize Deep Learning Models.",
      "abstract": "Screening programs for early lung cancer diagnosis are uncommon, primarily due to the challenge of reaching at-risk patients located in rural areas far from medical facilities. To overcome this obstacle, a comprehensive approach is needed that combines mobility, low cost, speed, accuracy, and privacy. One potential solution lies in combining the chest X-ray imaging mode with federated deep learning, ensuring that no single data source can bias the model adversely. This study presents a pre-processing pipeline designed to debias chest X-ray images, thereby enhancing internal classification and external generalization. The pipeline employs a pruning mechanism to train a deep learning model for nodule detection, utilizing the most informative images from a publicly available lung nodule X-ray dataset. Histogram equalization is used to remove systematic differences in image brightness and contrast. Model training is then performed using combinations of lung field segmentation, close cropping, and rib/bone suppression. The resulting deep learning models, generated through this pre-processing pipeline, demonstrate successful generalization on an independent lung nodule dataset. By eliminating confounding variables in chest X-ray images and suppressing signal noise from the bone structures, the proposed deep learning lung nodule detection algorithm achieves an external generalization accuracy of 89%. This approach paves the way for the development of a low-cost and accessible deep learning-based clinical system for lung cancer screening.",
      "journal": "Sensors (Basel, Switzerland)",
      "year": "2023",
      "doi": "10.3390/s23146585",
      "authors": "Horry Michael J et al.",
      "keywords": "chest X-ray; confounding bias; deep learning; federated learning; lung cancer; model generalization",
      "mesh_terms": "Humans; Deep Learning; Neural Networks, Computer; X-Rays; Early Detection of Cancer; Lung Neoplasms; Lung",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37514877/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10385599",
      "ft_text_length": 33739,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10385599)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "37599147",
      "title": "Illness severity assessment of older adults in critical illness using machine learning (ELDER-ICU): an international multicentre study with subgroup bias evaluation.",
      "abstract": "BACKGROUND: Comorbidity, frailty, and decreased cognitive function lead to a higher risk of death in elderly patients (more than 65 years of age) during acute medical events. Early and accurate illness severity assessment can support appropriate decision making for clinicians caring for these patients. We aimed to develop ELDER-ICU, a machine learning model to assess the illness severity of older adults admitted to the intensive care unit (ICU) with cohort-specific calibration and evaluation for potential model bias. METHODS: In this retrospective, international multicentre study, the ELDER-ICU model was developed using data from 14 US hospitals, and validated in 171 hospitals from the USA and Netherlands. Data were extracted from the Medical Information Mart for Intensive Care database, electronic ICU Collaborative Research Database, and Amsterdam University Medical Centers Database. We used six categories of data as predictors, including demographics and comorbidities, physical frailty, laboratory tests, vital signs, treatments, and urine output. Patient data from the first day of ICU stay were used to predict in-hospital mortality. We used the eXtreme Gradient Boosting algorithm (XGBoost) to develop models and the SHapley Additive exPlanations method to explain model prediction. The trained model was calibrated before internal, external, and temporal validation. The final XGBoost model was compared against three other machine learning algorithms and five clinical scores. We performed subgroup analysis based on age, sex, and race. We assessed the discrimination and calibration of models using the area under receiver operating characteristic (AUROC) and standardised mortality ratio (SMR) with 95% CIs. FINDINGS: Using the development dataset (n=50\u2008366) and predictive model building process, the XGBoost algorithm performed the best in all types of validations compared with other machine learning algorithms and clinical scores (internal validation with 5037 patients from 14 US hospitals, AUROC=0\u00b7866 [95% CI 0\u00b7851-0\u00b7880]; external validation in the US population with 20\u2008541 patients from 169 hospitals, AUROC=0\u00b7838 [0\u00b7829-0\u00b7847]; external validation in European population with 2411 patients from one hospital, AUROC=0\u00b7833 [0\u00b7812-0\u00b7853]; temporal validation with 4311 patients from one hospital, AUROC=0\u00b7884 [0\u00b7869-0\u00b7897]). In the external validation set (US population), the median AUROCs of bias evaluations covering eight subgroups were above 0\u00b781, and the overall SMR was 0\u00b799 (0\u00b796-1\u00b703). The top ten risk predictors were the minimum Glasgow Coma Scale score, total urine output, average respiratory rate, mechanical ventilation use, best state of activity, Charlson Comorbidity Index score, geriatric nutritional risk index, code status, age, and maximum blood urea nitrogen. A simplified model containing only the top 20 features (ELDER-ICU-20) had similar predictive performance to the full model. INTERPRETATION: The ELDER-ICU model reliably predicts the risk of in-hospital mortality using routinely collected clinical features. The predictions could inform clinicians about patients who are at elevated risk of deterioration. Prospective validation of this model in clinical practice and a process for continuous performance monitoring and model recalibration are needed. FUNDING: National Institutes of Health, National Natural Science Foundation of China, National Special Health Science Program, Health Science and Technology Plan of Zhejiang Province, Fundamental Research Funds for the Central Universities, Drug Clinical Evaluate Research of Chinese Pharmaceutical Association, and National Key R&D Program of China.",
      "journal": "The Lancet. Digital health",
      "year": "2023",
      "doi": "10.1016/S2589-7500(23)00128-0",
      "authors": "Liu Xiaoli et al.",
      "keywords": "",
      "mesh_terms": "United States; Aged; Humans; Critical Illness; Frailty; Retrospective Studies; Intensive Care Units; Machine Learning",
      "pub_types": "Multicenter Study; Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37599147/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12557411",
      "ft_text_length": 28819,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12557411)",
      "ft_reason": "Included: bias central + approach content (7 indicators)"
    },
    {
      "pmid": "37600144",
      "title": "Translating Intersectionality to Fair Machine Learning in Health Sciences.",
      "abstract": "Fairness approaches in machine learning should involve more than assessment of performance metrics across groups. Shifting the focus away from model metrics, we reframe fairness through the lens of intersectionality, a Black feminist theoretical framework that contextualizes individuals in interacting systems of power and oppression.",
      "journal": "Nature machine intelligence",
      "year": "2023",
      "doi": "10.1038/s42256-023-00651-3",
      "authors": "Lett Elle et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37600144/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10437125",
      "ft_text_length": 335,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10437125)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37609150",
      "title": "Toward MR protocol-agnostic, bias-corrected brain age predicted from clinical-grade MRIs.",
      "abstract": "The predicted brain age minus the chronological age ('brain-PAD') could become a clinical biomarker. However, most brain age methods were developed to use research-grade high-resolution T1-weighted MRIs, limiting their applicability to clinical-grade MRIs from multiple protocols. To overcome this, we adopted a double transfer learning approach to develop a brain age model agnostic to modality, resolution, or slice orientation. Using 6,224 clinical MRIs among 7 modalities, scanned from 1,540 patients using 8 scanners among 15 + facilities of the University of Florida's Health System, we retrained a convolutional neural network (CNN) to predict brain age from synthetic research-grade magnetization-prepared rapid gradient-echo MRIs (MPRAGEs) generated by a deep learning-trained 'super-resolution' method. We also modeled the \"regression dilution bias\", a typical overestimation of younger ages and underestimation of older ages, which correction is paramount for personalized brain age-based biomarkers. This bias was independent of modality or scanner and generalizable to new samples, allowing us to add a bias-correction layer to the CNN. The mean absolute error in test samples was 4.67-6.47 years across modalities, with similar accuracy between original MPRAGEs and their synthetic counterparts. Brain-PAD was also reliable across modalities. We demonstrate the feasibility of clinical-grade brain age predictions, contributing to personalized medicine.",
      "journal": "Research square",
      "year": "2023",
      "doi": "10.21203/rs.3.rs-3229072/v1",
      "authors": "Valdes-Hernandez Pedro et al.",
      "keywords": "Clinical Multimodal MRI; DeepBrainNet; Synthetic MPRAGE; brain age gap; brain-PAD; research-grade MRI; transfer learning",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37609150/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10441510",
      "ft_text_length": 40681,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10441510)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37609241",
      "title": "Enhancing Fairness in Disease Prediction by Optimizing Multiple Domain Adversarial Networks.",
      "abstract": "Predictive models in biomedicine need to ensure equitable and reliable outcomes for the populations they are applied to. Unfortunately, biases in medical predictions can lead to unfair treatment and widening disparities, underscoring the need for effective techniques to address these issues. To enhance fairness, we introduce a framework based on a Multiple Domain Adversarial Neural Network (MDANN), which incorporates multiple adversarial components. In an MDANN, an adversarial module is applied to learn a fair pattern by negative gradients back-propagating across multiple sensitive features (i.e., characteristics of individuals that should not be used to discriminate unfairly between individuals when making predictions or decisions.) We leverage loss functions based on the Area Under the Receiver Operating Characteristic Curve (AUC) to address the class imbalance, promoting equitable classification performance for minority groups (e.g., a subset of the population that is underrepresented or disadvantaged.) Moreover, we utilize pre-trained convolutional autoencoders (CAEs) to extract deep representations of data, aiming to enhance prediction accuracy and fairness. Combining these mechanisms, we alleviate biases and disparities to provide reliable and equitable disease prediction. We empirically demonstrate that the MDANN approach leads to better accuracy and fairness in predicting disease progression using brain imaging data for Alzheimer's Disease and Autism populations than state-of-the-art techniques.",
      "journal": "bioRxiv : the preprint server for biology",
      "year": "2023",
      "doi": "10.1101/2023.08.04.551906",
      "authors": "Li Bin et al.",
      "keywords": "Fairness; adversarial training; biases; health disparities; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37609241/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10441334",
      "ft_text_length": 34657,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10441334)",
      "ft_reason": "Included: bias central + approach content (9 indicators)"
    },
    {
      "pmid": "37615031",
      "title": "Algorithmic fairness and bias mitigation for clinical machine learning with deep reinforcement learning.",
      "abstract": "As models based on machine learning continue to be developed for healthcare applications, greater effort is needed to ensure that these technologies do not reflect or exacerbate any unwanted or discriminatory biases that may be present in the data. Here we introduce a reinforcement learning framework capable of mitigating biases that may have been acquired during data collection. In particular, we evaluated our model for the task of rapidly predicting COVID-19 for patients presenting to hospital emergency departments and aimed to mitigate any site (hospital)-specific and ethnicity-based biases present in the data. Using a specialized reward function and training procedure, we show that our method achieves clinically effective screening performances, while significantly improving outcome fairness compared with current benchmarks and state-of-the-art machine learning methods. We performed external validation across three independent hospitals, and additionally tested our method on a patient intensive care unit discharge status task, demonstrating model generalizability.",
      "journal": "Nature machine intelligence",
      "year": "2023",
      "doi": "10.1038/s42256-023-00697-3",
      "authors": "Yang Jenny et al.",
      "keywords": "Diagnosis; Medical ethics; Translational research",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37615031/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10442224",
      "ft_text_length": 65943,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10442224)",
      "ft_reason": "Included: bias central + approach content (10 indicators)"
    },
    {
      "pmid": "37693388",
      "title": "Assessing Racial and Ethnic Bias in Text Generation for Healthcare-Related Tasks by ChatGPT1.",
      "abstract": "Large Language Models (LLM) are AI tools that can respond human-like to voice or free-text commands without training on specific tasks. However, concerns have been raised about their potential racial bias in healthcare tasks. In this study, ChatGPT was used to generate healthcare-related text for patients with HIV, analyzing data from 100 deidentified electronic health record encounters. Each patient's data were fed four times with all information remaining the same except for race/ethnicity (African American, Asian, Hispanic White, Non-Hispanic White). The text output was analyzed for sentiment, subjectivity, reading ease, and most used words by race/ethnicity and insurance type. Results showed that instructions for African American, Asian, Hispanic White, and Non-Hispanic White patients had an average polarity of 0.14, 0.14, 0.15, and 0.14, respectively, with an average subjectivity of 0.46 for all races/ethnicities. The differences in polarity and subjectivity across races/ethnicities were not statistically significant. However, there was a statistically significant difference in word frequency across races/ethnicities and a statistically significant difference in subjectivity across insurance types with commercial insurance eliciting the most subjective responses and Medicare and other payer types the lowest. The study suggests that ChatGPT is relatively invariant to race/ethnicity and insurance type in terms of linguistic and readability measures. Further studies are needed to validate these results and assess their implications.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2023",
      "doi": "10.1101/2023.08.28.23294730",
      "authors": "Hanna John J et al.",
      "keywords": "Large Language Model; artificial intelligence; bias; racism; reading ease; sentiment analysis; word frequency",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37693388/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10491360",
      "ft_text_length": 11838,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10491360)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "37745001",
      "title": "Detecting biased validation of predictive models in the positive-unlabeled setting: disease gene prioritization case study.",
      "abstract": "MOTIVATION: Positive-unlabeled data consists of points with either positive or unknown labels. It is widespread in medical, genetic, and biological settings, creating a high demand for predictive positive-unlabeled models. The performance of such models is usually estimated using validation sets, assumed to be selected completely at random (SCAR) from known positive examples. For certain metrics, this assumption enables unbiased performance estimation when treating positive-unlabeled data as positive/negative. However, the SCAR assumption is often adopted without proper justifications, simply for the sake of convenience. RESULTS: We provide an algorithm that under the weak assumptions of a lower bound on the number of positive examples can test for the violation of the SCAR assumption. Applying it to the problem of gene prioritization for complex genetic traits, we illustrate that the SCAR assumption is often violated there, causing the inflation of performance estimates, which we refer to as validation bias. We estimate the potential impact of validation bias on performance estimation. Our analysis reveals that validation bias is widespread in gene prioritization data and can significantly overestimate the performance of models. This finding elucidates the discrepancy between the reported good performance of models and their limited practical applications. AVAILABILITY AND IMPLEMENTATION: Python code with examples of application of the validation bias detection algorithm is available at github.com/ArtomovLab/ValidationBias.",
      "journal": "Bioinformatics advances",
      "year": "2023",
      "doi": "10.1093/bioadv/vbad128",
      "authors": "Molotkov Ivan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37745001/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10517638",
      "ft_text_length": 31369,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10517638)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "37746321",
      "title": "Raising awareness of sex and gender bias in artificial intelligence and health.",
      "abstract": "Historically, biomedical research has been led by and focused on men. The recent introduction of Artificial Intelligence (AI) in this area has further proven this practice to be discriminatory for other sexes and genders, more noticeably for women. To move towards a fair AI development, it is essential to include sex and gender diversity both in research practices and in the workplace. In this context, the Bioinfo4women (B4W) program of the Barcelona Supercomputing Center (i) promotes the participation of women scientists by improving their visibility, (ii) fosters international collaborations between institutions and programs and (iii) advances research on sex and gender bias in AI and health. In this article, we discuss methodology and results of a series of conferences, titled \u00e2\u20ac\u0153Sex and Gender Bias in Artificial Intelligence and Health, organized by B4W and La Caixa Foundation from March to June 2021 in Barcelona, Spain. The series consisted of nine hybrid events, composed of keynote sessions and seminars open to the general audience, and two working groups with invited experts from different professional backgrounds (academic fields such as biology, engineering, and sociology, as well as NGOs, journalists, lawyers, policymakers, industry). Based on this awareness-raising action, we distilled key recommendations to facilitate the inclusion of sex and gender perspective into public policies, educational programs, industry, and biomedical research, among other sectors, and help overcome sex and gender biases in AI and health.",
      "journal": "Frontiers in global women's health",
      "year": "2023",
      "doi": "10.3389/fgwh.2023.970312",
      "authors": "Busl\u00f3n Nataly et al.",
      "keywords": "AI; bias in science; gender bias; gender policies; health",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37746321/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10512182",
      "ft_text_length": 32142,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10512182)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37782875",
      "title": "Promoting Equity In Clinical Decision Making: Dismantling Race-Based Medicine.",
      "abstract": "As the use of artificial intelligence has spread rapidly throughout the US health care system, concerns have been raised about racial and ethnic biases built into the algorithms that often guide clinical decision making. Race-based medicine, which relies on algorithms that use race as a proxy for biological differences, has led to treatment patterns that are inappropriate, unjust, and harmful to minoritized racial and ethnic groups. These patterns have contributed to persistent disparities in health and health care. To reduce these disparities, we recommend a race-aware approach to clinical decision support that considers social and environmental factors such as structural racism and social determinants of health. Recent policy changes in medical specialty societies and innovations in algorithm development represent progress on the path to dismantling race-based medicine. Success will require continued commitment and sustained efforts among stakeholders in the health care, research, and technology sectors. Increasing the diversity of clinical trial populations, broadening the focus of precision medicine, improving education about the complex factors shaping health outcomes, and developing new guidelines and policies to enable culturally responsive care are important next steps.",
      "journal": "Health affairs (Project Hope)",
      "year": "2023",
      "doi": "10.1377/hlthaff.2023.00545",
      "authors": "Hernandez-Boussard Tina et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Delivery of Health Care; Ethnicity; Clinical Decision-Making; Racism; Health Equity",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37782875/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10849087",
      "ft_text_length": 17086,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10849087)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37790354",
      "title": "Measuring and Reducing Racial Bias in a Pediatric Urinary Tract Infection Model.",
      "abstract": "Clinical predictive models that include race as a predictor have the potential to exacerbate disparities in healthcare. Such models can be respecified to exclude race or optimized to reduce racial bias. We investigated the impact of such respecifications in a predictive model - UTICalc - which was designed to reduce catheterizations in young children with suspected urinary tract infections. To reduce racial bias, race was removed from the UTICalc logistic regression model and replaced with two new features. We compared the two versions of UTICalc using fairness and predictive performance metrics to understand the effects on racial bias. In addition, we derived three new models for UTICalc to specifically improve racial fairness. Our results show that, as predicted by previously described impossibility results, fairness cannot be simultaneously improved on all fairness metrics, and model respecification may improve racial fairness but decrease overall predictive performance.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2023",
      "doi": "10.1101/2023.09.18.23295660",
      "authors": "Anderson Joshua W et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37790354/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10543246",
      "ft_text_length": 997,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10543246)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37794418",
      "title": "Sources of bias and limitations of thrombinography: inner filter effect and substrate depletion at the edge of failure algorithm.",
      "abstract": "BACKGROUND: Fluorogenic thrombin generation (TG) is a global hemostasis assay that provides an overall representation of hemostasis potential. However, the accurate detection of thrombin activity in plasma may be affected by artifacts inherent to the assay-associated fluorogenic substrate. The significance of the fluorogenic artifacts or their corrections has not been studied in hemophilia treatment applications. METHODS: We sought to investigate TG in hemophilia plasma samples under typical and worst-case fluorogenic artifact conditions and assess the performance of artifact correction algorithms. Severe hemophilic plasma with or without added Factor VIII (FVIII) was evaluated using commercially available and in-house TG reagents, instruments, and software packages. The inner filter effect (IFE) was induced by spiking elevated amounts of fluorophore 7-amino-4-methylcoumarin (AMC) into plasma prior to the TG experiment. Substrate consumption was modeled by adding decreasing amounts of Z-Gly-Gly-Arg-AMC (ZGGR-AMC) to plasma or performing TG in antithrombin deficient plasma. RESULTS: All algorithms corrected the AMC-induced IFE and antithrombin-deficiency induced substrate consumption up to a certain level of either artifact (edge of failure) upon which TG results were not returned or overestimated. TG values in FVIII deficient (FVIII-DP) or supplemented plasma were affected similarly. Normalization of FVIII-DP resulted in a more accurate correction of substrate artifacts than algorithmic methods. CONCLUSIONS: Correction algorithms may be effective in situations of moderate fluorogenic substrate artifacts inherent to highly procoagulant samples, but correction may not be required under typical conditions for hemophilia treatment studies if TG parameters can be normalized to a reference plasma sample.",
      "journal": "Thrombosis journal",
      "year": "2023",
      "doi": "10.1186/s12959-023-00549-5",
      "authors": "Jackson Joseph W et al.",
      "keywords": "Factor VIII; Hemophilia A; Hemostasis; Thrombin, Blood coagulation test",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37794418/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10548689",
      "ft_text_length": 61327,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10548689)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37803009",
      "title": "Improving model fairness in image-based computer-aided diagnosis.",
      "abstract": "Deep learning has become a popular tool for computer-aided diagnosis using medical images, sometimes matching or exceeding the performance of clinicians. However, these models can also reflect and amplify human bias, potentially resulting inaccurate missed diagnoses. Despite this concern, the problem of improving model fairness in medical image classification by deep learning has yet to be fully studied. To address this issue, we propose an algorithm that leverages the marginal pairwise equal opportunity to reduce bias in medical image classification. Our evaluations across four tasks using four independent large-scale cohorts demonstrate that our proposed algorithm not only improves fairness in individual and intersectional subgroups but also maintains overall performance. Specifically, the relative change in pairwise fairness difference between our proposed model and the baseline model was reduced by over 35%, while the relative change in AUC value was typically within 1%. By reducing the bias generated by deep learning models, our proposed approach can potentially alleviate concerns about the fairness and reliability of image-based computer-aided diagnosis.",
      "journal": "Nature communications",
      "year": "2023",
      "doi": "10.1038/s41467-023-41974-4",
      "authors": "Lin Mingquan et al.",
      "keywords": "",
      "mesh_terms": "Humans; Reproducibility of Results; Diagnosis, Computer-Assisted; Algorithms; Computers",
      "pub_types": "Journal Article; Research Support, N.I.H., Intramural; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37803009/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10558498",
      "ft_text_length": 43279,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10558498)",
      "ft_reason": "Included: bias central + approach content (9 indicators)"
    },
    {
      "pmid": "37828119",
      "title": "Clinically informed machine learning elucidates the shape of hospice racial disparities within hospitals.",
      "abstract": "Racial disparities in hospice care are well documented for patients with cancer, but the existence, direction, and extent of disparity findings are contradictory across the literature. Current methods to identify racial disparities aggregate data to produce single-value quality measures that exclude important patient quality elements and, consequently, lack information to identify actionable equity improvement insights. Our goal was to develop an explainable machine learning approach that elucidates healthcare disparities and provides more actionable quality improvement information. We infused clinical information with engineering systems modeling and data science to develop a time-by-utilization profile per patient group at each hospital using US Medicare hospice utilization data for a cohort of patients with advanced (poor-prognosis) cancer that died April-December 2016. We calculated the difference between group profiles for people of color and white people to identify racial disparity signatures. Using machine learning, we clustered racial disparity signatures across hospitals and compared these clusters to classic quality measures and hospital characteristics. With 45,125 patients across 362 hospitals, we identified 7 clusters; 4 clusters (n\u2009=\u2009190 hospitals) showed more hospice utilization by people of color than white people, 2 clusters (n\u2009=\u2009106) showed more hospice utilization by white people than people of color, and 1 cluster (n\u2009=\u200966) showed no difference. Within-hospital racial disparity behaviors cannot be predicted from quality measures, showing how the true shape of disparities can be distorted through the lens of quality measures. This approach elucidates the shape of hospice racial disparities algorithmically from the same data used to calculate quality measures.",
      "journal": "NPJ digital medicine",
      "year": "2023",
      "doi": "10.1038/s41746-023-00925-5",
      "authors": "Khayal Inas S et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37828119/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10570342",
      "ft_text_length": 36644,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10570342)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37886534",
      "title": "FaIRClocks: Fair and Interpretable Representation of the Clock Drawing Test for mitigating classifier bias against lower educational groups.",
      "abstract": "The clock drawing test (CDT) is a neuropsychological assessment tool to evaluate a patient's cognitive ability. In this study, we developed a Fair and Interpretable Representation of Clock drawing tests (FaIRClocks) to evaluate and mitigate bias against people with lower education while predicting their cognitive status. We represented clock drawings with a 10-dimensional latent embedding using Relevance Factor Variational Autoencoder (RF-VAE) network pretrained on publicly available clock drawings from the National Health and Aging Trends Study (NHATS) dataset. These embeddings were later fine-tuned for predicting three cognitive scores: the Mini-Mental State Examination (MMSE) total score, attention composite z-score (ATT-C), and memory composite z-score (MEM-C). The classifiers were initially tested to see their relative performance in patients with low education (<= 8 years) versus patients with higher education (> 8 years). Results indicated that the initial unweighted classifiers confounded lower education with cognitive impairment, resulting in a 100% type I error rate for this group. Thereby, the samples were re-weighted using multiple fairness metrics to achieve balanced performance. In summary, we report the FaIRClocks model, which a) can identify attention and memory deficits using clock drawings and b) exhibits identical performance between people with higher and lower education levels.",
      "journal": "Research square",
      "year": "2023",
      "doi": "10.21203/rs.3.rs-3398970/v1",
      "authors": "Zhang Jiaqing et al.",
      "keywords": "AI Fairness; Mini-mental state examination; Relevance Factor Variational Autoencoder; attention; memory; semi-supervised deep learning",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37886534/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10602062",
      "ft_text_length": 27626,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10602062)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "37902833",
      "title": "Architectural Design of a Blockchain-Enabled, Federated Learning Platform for Algorithmic Fairness in Predictive Health Care: Design Science Study.",
      "abstract": "BACKGROUND: Developing effective and generalizable predictive models is critical for disease prediction and clinical decision-making, often requiring diverse samples to mitigate population bias and address algorithmic fairness. However, a major challenge is to retrieve learning models across multiple institutions without bringing in local biases and inequity, while preserving individual patients' privacy at each site. OBJECTIVE: This study aims to understand the issues of bias and fairness in the machine learning process used in the predictive health care domain. We proposed a software architecture that integrates federated learning and blockchain to improve fairness, while maintaining acceptable prediction accuracy and minimizing overhead costs. METHODS: We improved existing federated learning platforms by integrating blockchain through an iterative design approach. We used the design science research method, which involves 2 design cycles (federated learning for bias mitigation and decentralized architecture). The design involves a bias-mitigation process within the blockchain-empowered federated learning framework based on a novel architecture. Under this architecture, multiple medical institutions can jointly train predictive models using their privacy-protected data effectively and efficiently and ultimately achieve fairness in decision-making in the health care domain. RESULTS: We designed and implemented our solution using the Aplos smart contract, microservices, Rahasak blockchain, and Apache Cassandra-based distributed storage. By conducting 20,000 local model training iterations and 1000 federated model training iterations across 5 simulated medical centers as peers in the Rahasak blockchain network, we demonstrated how our solution with an improved fairness mechanism can enhance the accuracy of predictive diagnosis. CONCLUSIONS: Our study identified the technical challenges of prediction biases faced by existing predictive models in the health care domain. To overcome these challenges, we presented an innovative design solution using federated learning and blockchain, along with the adoption of a unique distributed architecture for a fairness-aware system. We have illustrated how this design can address privacy, security, prediction accuracy, and scalability challenges, ultimately improving fairness and equity in the predictive health care domain.",
      "journal": "Journal of medical Internet research",
      "year": "2023",
      "doi": "10.2196/46547",
      "authors": "Liang Xueping et al.",
      "keywords": "bias; blockchain; fairness; federated learning; health care; implementation; privacy; proof of concept; software",
      "mesh_terms": "Humans; Blockchain; Hospitals; Awareness; Clinical Decision-Making; Machine Learning",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37902833/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10644196",
      "ft_text_length": 43476,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10644196)",
      "ft_reason": "Included: bias central + approach content (15 indicators)"
    },
    {
      "pmid": "37921762",
      "title": "Fairness of Machine Learning Algorithms for Predicting Foregone Preventive Dental Care for Adults.",
      "abstract": "IMPORTANCE: Access to routine dental care prevents advanced dental disease and improves oral and overall health. Identifying individuals at risk of foregoing preventive dental care can direct prevention efforts toward high-risk populations. OBJECTIVE: To predict foregone preventive dental care among adults overall and in sociodemographic subgroups and to assess the algorithmic fairness. DESIGN, SETTING, AND PARTICIPANTS: This prognostic study was a secondary analyses of longitudinal data from the US Medical Expenditure Panel Survey (MEPS) from 2016 to 2019, each with 2 years of follow-up. Participants included adults aged 18 years and older. Data analysis was performed from December 2022 to June 2023. EXPOSURE: A total of 50 predictors, including demographic and socioeconomic characteristics, health conditions, behaviors, and health services use, were assessed. MAIN OUTCOMES AND MEASURES: The outcome of interest was foregoing preventive dental care, defined as either cleaning, general examination, or an appointment with the dental hygienist, in the past year. RESULTS: Among 32\u202f234 participants, the mean (SD) age was 48.5 (18.2) years and 17\u202f386 participants (53.9%) were female; 1935 participants (6.0%) were Asian, 5138 participants (15.9%) were Black, 7681 participants (23.8%) were Hispanic, 16\u202f503 participants (51.2%) were White, and 977 participants (3.0%) identified as other (eg, American Indian and Alaska Native) or multiple racial or ethnic groups. There were 21\u202f083 (65.4%) individuals who missed preventive dental care in the past year. The algorithms demonstrated high performance, achieving an area under the receiver operating characteristic curve (AUC) of 0.84 (95% CI, 0.84-0.85) in the overall population. While the full sample model performed similarly when applied to White individuals and older adults (AUC, 0.88; 95% CI, 0.87-0.90), there was a loss of performance for other subgroups. Removing the subgroup-sensitive predictors (ie, race and ethnicity, age, and income) did not impact model performance. Models stratified by race and ethnicity performed similarly or worse than the full model for all groups, with the lowest performance for individuals who identified as other or multiple racial groups (AUC, 0.76; 95% CI, 0.70-0.81). Previous pattern of dental visits, health care utilization, dental benefits, and sociodemographic characteristics were the highest contributing predictors to the models' performance. CONCLUSIONS AND RELEVANCE: Findings of this prognostic study using cohort data suggest that tree-based ensemble machine learning models could accurately predict adults at risk of foregoing preventive dental care and demonstrated bias against underrepresented sociodemographic groups. These results highlight the importance of evaluating model fairness during development and testing to avoid exacerbating existing biases.",
      "journal": "JAMA network open",
      "year": "2023",
      "doi": "10.1001/jamanetworkopen.2023.41625",
      "authors": "Schuch Helena Silveira et al.",
      "keywords": "",
      "mesh_terms": "Humans; Aged; Ethnicity; Racial Groups; Algorithms; Machine Learning; Dental Care",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37921762/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10625037",
      "ft_text_length": 24492,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10625037)",
      "ft_reason": "Included: bias central + approach content (7 indicators)"
    },
    {
      "pmid": "37923795",
      "title": "Early and fair COVID-19 outcome risk assessment using robust feature selection.",
      "abstract": "Personalized medicine plays an important role in treatment optimization for COVID-19 patient management. Early treatment in patients at high risk of severe complications is vital to prevent death and ventilator use. Predicting COVID-19 clinical outcomes using machine learning may provide a fast and data-driven solution for optimizing patient care by estimating the need for early treatment. In addition, it is essential to accurately predict risk across demographic groups, particularly those underrepresented in existing models. Unfortunately, there is a lack of studies demonstrating the equitable performance of machine learning models across patient demographics. To overcome this existing limitation, we generate a robust machine learning model to predict patient-specific risk of death or ventilator use in COVID-19 positive patients using features available at the time of diagnosis. We establish the value of our solution across patient demographics, including gender and race. In addition, we improve clinical trust in our automated predictions by generating interpretable patient clustering, patient-level clinical feature importance, and global clinical feature importance within our large real-world COVID-19 positive patient dataset. We achieved 89.38% area under receiver operating curve (AUROC) performance for severe outcomes prediction and our robust feature ranking approach identified the presence of dementia as a key indicator for worse patient outcomes. We also demonstrated that our deep-learning clustering approach outperforms traditional clustering in separating patients by severity of outcome based on mutual information performance. Finally, we developed an application for automated and fair patient risk assessment with minimal manual data entry using existing data exchange standards.",
      "journal": "Scientific reports",
      "year": "2023",
      "doi": "10.1038/s41598-023-36175-4",
      "authors": "Giuste Felipe O et al.",
      "keywords": "",
      "mesh_terms": "Humans; COVID-19; Risk Assessment; Outcome Assessment, Health Care; Prognosis; Machine Learning; Retrospective Studies",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37923795/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10624921",
      "ft_text_length": 43399,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10624921)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "37978250",
      "title": "Ethnic disparity in diagnosing asymptomatic bacterial vaginosis using machine learning.",
      "abstract": "While machine learning (ML) has shown great promise in medical diagnostics, a major challenge is that ML models do not always perform equally well among ethnic groups. This is alarming for women's health, as there are already existing health disparities that vary by ethnicity. Bacterial Vaginosis (BV) is a common vaginal syndrome among women of reproductive age and has clear diagnostic differences among ethnic groups. Here, we investigate the ability of four ML algorithms to diagnose BV. We determine the fairness in the prediction of asymptomatic BV using 16S rRNA sequencing data from Asian, Black, Hispanic, and white women. General purpose ML model performances vary based on ethnicity. When evaluating the metric of false positive or false negative rate, we find that models perform least effectively for Hispanic and Asian women. Models generally have the highest performance for white women and the lowest for Asian women. These findings demonstrate a need for improved methodologies to increase model fairness for predicting BV.",
      "journal": "NPJ digital medicine",
      "year": "2023",
      "doi": "10.1038/s41746-023-00953-1",
      "authors": "Celeste Cameron et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37978250/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10656445",
      "ft_text_length": 42063,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10656445)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37990734",
      "title": "Improving Fairness in AI Models on Electronic Health Records: The Case for Federated Learning Methods.",
      "abstract": "Developing AI tools that preserve fairness is of critical importance, specifically in high-stakes applications such as those in healthcare. However, health AI models' overall prediction performance is often prioritized over the possible biases such models could have. In this study, we show one possible approach to mitigate bias concerns by having healthcare institutions collaborate through a federated learning paradigm (FL; which is a popular choice in healthcare settings). While FL methods with an emphasis on fairness have been previously proposed, their underlying model and local implementation techniques, as well as their possible applications to the healthcare domain remain widely underinvestigated. Therefore, we propose a comprehensive FL approach with adversarial debiasing and a fair aggregation method, suitable to various fairness metrics, in the healthcare domain where electronic health records are used. Not only our approach explicitly mitigates bias as part of the optimization process, but an FL-based paradigm would also implicitly help with addressing data imbalance and increasing the data size, offering a practical solution for healthcare applications. We empirically demonstrate our method's superior performance on multiple experiments simulating large-scale real-world scenarios and compare it to several baselines. Our method has achieved promising fairness performance with the lowest impact on overall discrimination performance (accuracy). Our code is available at https://github.com/healthylaife/FairFedAvg.",
      "journal": "FAccT '23 : Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. Association for Computing Machinery",
      "year": "2023",
      "doi": "10.1145/3593013.3594102",
      "authors": "Poulain Raphael et al.",
      "keywords": "Adversarial Fairness; Algorithmic Fairness; Federated Learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37990734/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10661580",
      "ft_text_length": 1547,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10661580)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "38014176",
      "title": "DeepN4: Learning N4ITK Bias Field Correction for T1-weighted Images.",
      "abstract": "T1-weighted (T1w) MRI has low frequency intensity artifacts due to magnetic field inhomogeneities. Removal of these biases in T1w MRI images is a critical preprocessing step to ensure spatially consistent image interpretation. N4ITK bias field correction, the current state-of-the-art, is implemented in such a way that makes it difficult to port between different pipelines and workflows, thus making it hard to reimplement and reproduce results across local, cloud, and edge platforms. Moreover, N4ITK is opaque to optimization before and after its application, meaning that methodological development must work around the inhomogeneity correction step. Given the importance of bias fields correction in structural preprocessing and flexible implementation, we pursue a deep learning approximation / reinterpretation of the N4ITK bias fields correction to create a method which is portable, flexible, and fully differentiable. In this paper, we trained a deep learning network \"DeepN4\" on eight independent cohorts from 72 different scanners and age ranges with N4ITK-corrected T1w MRI and bias field for supervision in log space. We found that we can closely approximate N4ITK bias fields correction with na\u00efve networks. We evaluate the peak signal to noise ratio (PSNR) in test dataset against the N4ITK corrected images. The median PSNR of corrected images between N4ITK and DeepN4 was 47.96 dB. In addition, we assess the DeepN4 model on eight additional external datasets and show the generalizability of the approach. This study establishes that incompatible N4ITK preprocessing steps can be closely approximated by na\u00efve deep neural networks, facilitating more flexibility. All code and models are released at https://github.com/MASILab/DeepN4.",
      "journal": "Research square",
      "year": "2023",
      "doi": "10.21203/rs.3.rs-3585882/v1",
      "authors": "Kanakaraj Praitayini et al.",
      "keywords": "3D U-Net; N4ITK; T1-weighted images; bias field correction; inhomogeneity",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38014176/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10680935",
      "ft_text_length": 17610,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10680935)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38038606",
      "title": "Reducing Ophthalmic Health Disparities Through Transfer Learning: A Novel Application to Overcome Data Inequality.",
      "abstract": "PURPOSE: Race disparities in the healthcare system and the resulting inequality in clinical data among different races hinder the ability to generate equitable prediction results. This study aims to reduce healthcare disparities arising from data imbalance by leveraging advanced transfer learning (TL) methods. METHOD: We examined the ophthalmic healthcare disparities at a population level using electronic medical records data from a study cohort (N = 785) receiving care at an academic institute. Regression-based TL models were usesd, transferring valuable information from the dominant racial group (White) to improve visual field mean deviation (MD) rate of change prediction particularly for data-disadvantaged African American (AA) and Asian racial groups. Prediction results of TL models were compared with two conventional approaches. RESULTS: Disparities in socioeconomic status and baseline disease severity were observed among the AA and Asian racial groups. The TL approach achieved marked to comparable improvement in prediction accuracy compared to the two conventional approaches as evident by smaller mean absolute errors or mean square errors. TL identified distinct key features of visual field MD rate of change for each racial group. CONCLUSIONS: The study introduces a novel application of TL that improved reliability of the analysis in comparison with conventional methods, especially in small sample size groups. This can improve assessment of healthcare disparity and subsequent remedy approach. TRANSLATIONAL RELEVANCE: TL offers an equitable and efficient approach to mitigate healthcare disparities analysis by enhancing prediction performance for data-disadvantaged group.",
      "journal": "Translational vision science & technology",
      "year": "2023",
      "doi": "10.1167/tvst.12.12.2",
      "authors": "Lee TingFang et al.",
      "keywords": "",
      "mesh_terms": "Humans; Black or African American; Healthcare Disparities; Machine Learning; Reproducibility of Results; White; Asian",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38038606/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10697175",
      "ft_text_length": 27101,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10697175)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38074789",
      "title": "Risk of Bias in Chest Radiography Deep Learning Foundation Models.",
      "abstract": "PURPOSE: To analyze a recently published chest radiography foundation model for the presence of biases that could lead to subgroup performance disparities across biologic sex and race. MATERIALS AND METHODS: This Health Insurance Portability and Accountability Act-compliant retrospective study used 127\u2009118 chest radiographs from 42\u2009884 patients (mean age, 63 years \u00b1 17 [SD]; 23\u2009623 male, 19\u2009261 female) from the CheXpert dataset that were collected between October 2002 and July 2017. To determine the presence of bias in features generated by a chest radiography foundation model and baseline deep learning model, dimensionality reduction methods together with two-sample Kolmogorov-Smirnov tests were used to detect distribution shifts across sex and race. A comprehensive disease detection performance analysis was then performed to associate any biases in the features to specific disparities in classification performance across patient subgroups. RESULTS: Ten of 12 pairwise comparisons across biologic sex and race showed statistically significant differences in the studied foundation model, compared with four significant tests in the baseline model. Significant differences were found between male and female (P < .001) and Asian and Black (P < .001) patients in the feature projections that primarily capture disease. Compared with average model performance across all subgroups, classification performance on the \"no finding\" label decreased between 6.8% and 7.8% for female patients, and performance in detecting \"pleural effusion\" decreased between 10.7% and 11.6% for Black patients. CONCLUSION: The studied chest radiography foundation model demonstrated racial and sex-related bias, which led to disparate performance across patient subgroups; thus, this model may be unsafe for clinical applications.Keywords: Conventional Radiography, Computer Application-Detection/Diagnosis, Chest Radiography, Bias, Foundation Models Supplemental material is available for this article. Published under a CC BY 4.0 license.See also commentary by Czum and Parr in this issue.",
      "journal": "Radiology. Artificial intelligence",
      "year": "2023",
      "doi": "10.1148/ryai.230060",
      "authors": "Glocker Ben et al.",
      "keywords": "Bias; Chest Radiography; Computer Application-Detection/Diagnosis; Conventional Radiography; Foundation Models",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38074789/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10698597",
      "ft_text_length": 33266,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10698597)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "38075950",
      "title": "The AI cycle of health inequity and digital ageism: mitigating biases through the EU regulatory framework on medical devices.",
      "abstract": "The use of Artificial Intelligence (AI) medical devices is rapidly growing. Although AI may benefit the quality and safety of healthcare for older adults, it simultaneously introduces new ethical and legal issues. Many AI medical devices exhibit age-related biases. The first part of this paper explains how 'digital ageism' is produced throughout the entire lifecycle of medical AI and may lead to health inequity for older people: systemic, avoidable differences in the health status of different population groups. This paper takes digital ageism as a use case to show the potential inequitable effects of AI, conceptualized as the 'AI cycle of health inequity'. The second part of this paper explores how the European Union (EU) regulatory framework addresses the issue of digital ageism. It argues that the negative effects of age-related bias in AI medical devices are insufficiently recognized within the regulatory framework of the EU Medical Devices Regulation and the new AI Act. It concludes that while the EU framework does address some of the key issues related to technical biases in AI medical devices by stipulating rules for performance and data quality, it does not account for contextual biases, therefore neglecting part of the AI cycle of health inequity.",
      "journal": "Journal of law and the biosciences",
      "year": "2023",
      "doi": "10.1093/jlb/lsad031",
      "authors": "van Kolfschooten Hannah",
      "keywords": "EU regulation; ageism; artificial intelligence; bias; discrimination; medical devices",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38075950/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10709664",
      "ft_text_length": 61941,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10709664)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "38076213",
      "title": "Eliminating Algorithmic Racial Bias in Clinical Decision Support Algorithms: Use Cases from the Veterans Health Administration.",
      "abstract": "The Veterans Health Administration uses equity- and evidence-based principles to examine, correct, and eliminate use of potentially biased clinical equations and predictive models. We discuss the processes, successes, challenges, and next steps in four examples. We detail elimination of the race modifier for estimated kidney function and discuss steps to achieve more equitable pulmonary function testing measurement. We detail the use of equity lenses in two predictive clinical modeling tools: Stratification Tool for Opioid Risk Mitigation (STORM) and Care Assessment Need (CAN) predictive models. We conclude with consideration of ways to advance racial health equity in clinical decision support algorithms.",
      "journal": "Health equity",
      "year": "2023",
      "doi": "10.1089/heq.2023.0037",
      "authors": "List Justin M et al.",
      "keywords": "Veterans Health Administration; clinical equations; health equity; predictive models; racial bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38076213/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10698768",
      "ft_text_length": 25511,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10698768)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38100101",
      "title": "Guiding Principles to Address the Impact of Algorithm Bias on Racial and Ethnic Disparities in Health and Health Care.",
      "abstract": "IMPORTANCE: Health care algorithms are used for diagnosis, treatment, prognosis, risk stratification, and allocation of resources. Bias in the development and use of algorithms can lead to worse outcomes for racial and ethnic minoritized groups and other historically marginalized populations such as individuals with lower income. OBJECTIVE: To provide a conceptual framework and guiding principles for mitigating and preventing bias in health care algorithms to promote health and health care equity. EVIDENCE REVIEW: The Agency for Healthcare Research and Quality and the National Institute for Minority Health and Health Disparities convened a diverse panel of experts to review evidence, hear from stakeholders, and receive community feedback. FINDINGS: The panel developed a conceptual framework to apply guiding principles across an algorithm's life cycle, centering health and health care equity for patients and communities as the goal, within the wider context of structural racism and discrimination. Multiple stakeholders can mitigate and prevent bias at each phase of the algorithm life cycle, including problem formulation (phase 1); data selection, assessment, and management (phase 2); algorithm development, training, and validation (phase 3); deployment and integration of algorithms in intended settings (phase 4); and algorithm monitoring, maintenance, updating, or deimplementation (phase 5). Five principles should guide these efforts: (1) promote health and health care equity during all phases of the health care algorithm life cycle; (2) ensure health care algorithms and their use are transparent and explainable; (3) authentically engage patients and communities during all phases of the health care algorithm life cycle and earn trustworthiness; (4) explicitly identify health care algorithmic fairness issues and trade-offs; and (5) establish accountability for equity and fairness in outcomes from health care algorithms. CONCLUSIONS AND RELEVANCE: Multiple stakeholders must partner to create systems, processes, regulations, incentives, standards, and policies to mitigate and prevent algorithmic bias. Reforms should implement guiding principles that support promotion of health and health care equity in all phases of the algorithm life cycle as well as transparency and explainability, authentic community engagement and ethical partnerships, explicit identification of fairness issues and trade-offs, and accountability for equity and fairness.",
      "journal": "JAMA network open",
      "year": "2023",
      "doi": "10.1001/jamanetworkopen.2023.45050",
      "authors": "Chin Marshall H et al.",
      "keywords": "",
      "mesh_terms": "United States; Humans; Health Promotion; Racial Groups; Academies and Institutes; Algorithms; Health Equity",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38100101/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11181958",
      "ft_text_length": 20316,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11181958)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "38153778",
      "title": "Empathy and Equity: Key Considerations for Large Language Model Adoption in Health Care.",
      "abstract": "The growing presence of large language models (LLMs) in health care applications holds significant promise for innovative advancements in patient care. However, concerns about ethical implications and potential biases have been raised by various stakeholders. Here, we evaluate the ethics of LLMs in medicine along 2 key axes: empathy and equity. We outline the importance of these factors in novel models of care and develop frameworks for addressing these alongside LLM deployment.",
      "journal": "JMIR medical education",
      "year": "2023",
      "doi": "10.2196/51199",
      "authors": "Koranteng Erica et al.",
      "keywords": "AI; ChatGPT; LLMs; artificial intelligence; bias; care; development; empathy; equity; ethical implication; ethics; framework; health care application; language model; large language models; model; patient care",
      "mesh_terms": "Humans; Empathy; Health Facilities; Language; Medicine; Delivery of Health Care",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38153778/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10884892",
      "ft_text_length": 9231,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10884892)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38160296",
      "title": "Quantifying Health Outcome Disparity in Invasive Methicillin-Resistant Staphylococcus aureus Infection using Fairness Algorithms on Real-World Data.",
      "abstract": "This study quantifies health outcome disparities in invasive Methicillin-Resistant Staphylococcus aureus (MRSA) infections by leveraging a novel artificial intelligence (AI) fairness algorithm, the Fairness-Aware Causal paThs (FACTS) decomposition, and applying it to real-world electronic health record (EHR) data. We spatiotemporally linked 9 years of EHRs from a large healthcare provider in Florida, USA, with contextual social determinants of health (SDoH). We first created a causal structure graph connecting SDoH with individual clinical measurements before/upon diagnosis of invasive MRSA infection, treatments, side effects, and outcomes; then, we applied FACTS to quantify outcome potential disparities of different causal pathways including SDoH, clinical and demographic variables. We found moderate disparity with respect to demographics and SDoH, and all the top ranked pathways that led to outcome disparities in age, gender, race, and income, included comorbidity. Prior kidney impairment, vancomycin use, and timing were associated with racial disparity, while income, rurality, and available healthcare facilities contributed to gender disparity. From an intervention standpoint, our results highlight the necessity of devising policies that consider both clinical factors and SDoH. In conclusion, this work demonstrates a practical utility of fairness AI methods in public health settings.",
      "journal": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing",
      "year": "2024",
      "doi": "10.15620/cdc:82532",
      "authors": "Jun Inyoung et al.",
      "keywords": "",
      "mesh_terms": "Humans; Methicillin-Resistant Staphylococcus aureus; Staphylococcal Infections; Artificial Intelligence; Community-Acquired Infections; Computational Biology; Algorithms; Outcome Assessment, Health Care; Anti-Bacterial Agents",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38160296/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10795837",
      "ft_text_length": 19916,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10795837)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38199323",
      "title": "Measuring Implicit Bias in ICU Notes Using Word-Embedding Neural Network Models.",
      "abstract": "BACKGROUND: Language in nonmedical data sets is known to transmit human-like biases when used in natural language processing (NLP) algorithms that can reinforce disparities. It is unclear if NLP algorithms of medical notes could lead to similar transmissions of biases. RESEARCH QUESTION: Can we identify implicit bias in clinical notes, and are biases stable across time and geography? STUDY DESIGN AND METHODS: To determine whether different racial and ethnic descriptors are similar contextually to stigmatizing language in ICU notes and whether these relationships are stable across time and geography, we identified notes on critically ill adults admitted to the University of California, San Francisco (UCSF), from 2012 through 2022 and to Beth Israel Deaconess Hospital (BIDMC) from 2001 through 2012. Because word meaning is derived largely from context, we trained unsupervised word-embedding algorithms to measure the similarity (cosine similarity) quantitatively of the context between a racial or ethnic descriptor (eg, African-American) and a stigmatizing target word (eg, nonco-operative) or group of words (violence, passivity, noncompliance, nonadherence). RESULTS: In UCSF notes, Black descriptors were less likely to be similar contextually to violent words compared with White descriptors. Contrastingly, in BIDMC notes, Black descriptors were more likely to be similar contextually to violent words compared with White descriptors. The UCSF data set\u00a0also showed that Black descriptors were more similar contextually to passivity and noncompliance words compared with Latinx descriptors. INTERPRETATION: Implicit bias is identifiable in ICU notes. Racial and ethnic group descriptors carry different contextual relationships to stigmatizing words, depending on when and where notes were written. Because NLP models seem able to transmit implicit bias from training data, use of NLP algorithms in clinical prediction could reinforce disparities. Active debiasing strategies may be necessary to achieve algorithmic fairness when using language models in clinical research.",
      "journal": "Chest",
      "year": "2024",
      "doi": "10.1016/j.chest.2023.12.031",
      "authors": "Cobert Julien et al.",
      "keywords": "critical care; inequity; linguistics; machine learning; natural language processing",
      "mesh_terms": "Humans; Natural Language Processing; Intensive Care Units; Neural Networks, Computer; Algorithms; Critical Illness; Bias; Electronic Health Records; Male; Female",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38199323/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11317817",
      "ft_text_length": 2098,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11317817)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38214974",
      "title": "Enhancing Health Equity by Predicting Missed Appointments in Health Care: Machine Learning Study.",
      "abstract": "BACKGROUND: The phenomenon of patients missing booked appointments without canceling them-known as Did Not Show (DNS), Did Not Attend (DNA), or Failed To Attend (FTA)-has a detrimental effect on patients' health and results in massive health care resource wastage. OBJECTIVE: Our objective was to develop machine learning (ML) models and evaluate their performance in predicting the likelihood of DNS for hospital outpatient appointments at the MidCentral District Health Board (MDHB) in New Zealand. METHODS: We sourced 5 years of MDHB outpatient records (a total of 1,080,566 outpatient visits) to build the ML prediction models. We developed 3 ML models using logistic regression, random forest, and Extreme Gradient Boosting (XGBoost). Subsequently, 10-fold cross-validation and hyperparameter tuning were deployed to minimize model bias and boost the algorithms' prediction strength. All models were evaluated against accuracy, sensitivity, specificity, and area under the receiver operating characteristic (AUROC) curve metrics. RESULTS: Based on 5 years of MDHB data, the best prediction classifier was XGBoost, with an area under the curve (AUC) of 0.92, sensitivity of 0.83, and specificity of 0.85. The patients' DNS history, age, ethnicity, and appointment lead time significantly contributed to DNS prediction. An ML system trained on a large data set can produce useful levels of DNS prediction. CONCLUSIONS: This research is one of the very first published studies that use ML technologies to assist with DNS management in New Zealand. It is a proof of concept and could be used to benchmark DNS predictions for the MDHB and other district health boards. We encourage conducting additional qualitative research to investigate the root cause of DNS issues and potential solutions. Addressing DNS using better strategies potentially can result in better utilization of health care resources and improve health equity.",
      "journal": "JMIR medical informatics",
      "year": "2024",
      "doi": "10.2196/48273",
      "authors": "Yang Yi et al.",
      "keywords": "Did Not Attend; Did Not Show; appointment nonadherence; data analytics; decision support system; health care operation; health equity; machine learning; patients no-show; prediction; predictive modeling",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38214974/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10818230",
      "ft_text_length": 40738,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10818230)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38222427",
      "title": "Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint.",
      "abstract": "Clinical trials are indispensable in developing new treatments, but they face obstacles in patient recruitment and retention, hindering the enrollment of necessary participants. To tackle these challenges, deep learning frameworks have been created to match patients to trials. These frameworks calculate the similarity between patients and clinical trial eligibility criteria, considering the discrepancy between inclusion and exclusion criteria. Recent studies have shown that these frameworks outperform earlier approaches. However, deep learning models may raise fairness issues in patient-trial matching when certain sensitive groups of individuals are underrepresented in clinical trials, leading to incomplete or inaccurate data and potential harm. To tackle the issue of fairness, this work proposes a fair patient-trial matching framework by generating a patient-criterion level fairness constraint. The proposed framework considers the inconsistency between the embedding of inclusion and exclusion criteria among patients of different sensitive groups. The experimental results on real-world patient-trial and patient-criterion matching tasks demonstrate that the proposed framework can successfully alleviate the predictions that tend to be biased.",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "year": "2023",
      "doi": "",
      "authors": "Chang Chia-Yuan et al.",
      "keywords": "",
      "mesh_terms": "Humans; Patient Selection; Clinical Trials as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38222427/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10785912",
      "ft_text_length": 1260,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10785912)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38223304",
      "title": "Double/debiased machine learning for logistic partially linear model.",
      "abstract": "We propose double/debiased machine learning approaches to infer a parametric component of a logistic partially linear model. Our framework is based on a Neyman orthogonal score equation consisting of two nuisance models for the nonparametric component of the logistic model and conditional mean of the exposure with the control group. To estimate the nuisance models, we separately consider the use of high dimensional (HD) sparse regression and (nonparametric) machine learning (ML) methods. In the HD case, we derive certain moment equations to calibrate the first order bias of the nuisance models, which preserves the model double robustness property. In the ML case, we handle the nonlinearity of the logit link through a novel and easy-to-implement 'full model refitting' procedure. We evaluate our methods through simulation and apply them in assessing the effect of the emergency contraceptive pill on early gestation and new births based on a 2008 policy reform in Chile.",
      "journal": "The econometrics journal",
      "year": "2021",
      "doi": "10.1093/ectj/utab019",
      "authors": "Liu Molei et al.",
      "keywords": "C14; Logistic partially linear model; calibration; double machine learning; double robustness; regularized regression",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38223304/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10786638",
      "ft_text_length": 989,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10786638)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38260285",
      "title": "Evaluating and Improving the Performance and Racial Fairness of Algorithms for GFR Estimation.",
      "abstract": "Data-driven clinical prediction algorithms are used widely by clinicians. Understanding what factors can impact the performance and fairness of data-driven algorithms is an important step towards achieving equitable healthcare. To investigate the impact of modeling choices on the algorithmic performance and fairness, we make use of a case study to build a prediction algorithm for estimating glomerular filtration rate (GFR) based on the patient's electronic health record (EHR). We compare three distinct approaches for estimating GFR: CKD-EPI equations, epidemiological models, and EHR-based models. For epidemiological models and EHR-based models, four machine learning models of varying computational complexity (i.e., linear regression, support vector machine, random forest regression, and neural network) were compared. Performance metrics included root mean squared error (RMSE), median difference, and the proportion of GFR estimates within 30% of the measured GFR value (P30). Differential performance between non-African American and African American group was used to assess algorithmic fairness with respect to race. Our study showed that the variable race had a negligible effect on error, accuracy, and differential performance. Furthermore, including more relevant clinical features (e.g., common comorbidities of chronic kidney disease) and using more complex machine learning models, namely random forest regression, significantly lowered the estimation error of GFR. However, the difference in performance between African American and non-African American patients did not decrease, where the estimation error for African American patients remained consistently higher than non-African American patients, indicating that more objective patient characteristics should be discovered and included to improve algorithm performance.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2024",
      "doi": "10.1101/2024.01.07.24300943",
      "authors": "Zhang Linying et al.",
      "keywords": "algorithmic fairness; electronic health record; glomerular filtration rate; machine learning; predictive modeling",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38260285/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10802656",
      "ft_text_length": 22563,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10802656)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "38264718",
      "title": "Unified fair federated learning for digital healthcare.",
      "abstract": "Federated learning (FL) is a promising approach for healthcare institutions to train high-quality medical models collaboratively while protecting sensitive data privacy. However, FL models encounter fairness issues at diverse levels, leading to performance disparities across different subpopulations. To address this, we propose Federated Learning with Unified Fairness Objective (FedUFO), a unified framework consolidating diverse fairness levels within FL. By leveraging distributionally robust optimization and a unified uncertainty set, it ensures consistent performance across all subpopulations and enhances the overall efficacy of FL in healthcare and other domains while maintaining accuracy levels comparable with those of existing methods. Our model was validated by applying it to four digital healthcare tasks using real-world datasets in federated settings. Our collaborative machine learning paradigm not only promotes artificial intelligence in digital healthcare but also fosters social equity by embodying fairness.",
      "journal": "Patterns (New York, N.Y.)",
      "year": "2024",
      "doi": "10.1016/j.patter.2023.100907",
      "authors": "Zhang Fengda et al.",
      "keywords": "algorithmic fairness; digital healthcare; federated learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38264718/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10801255",
      "ft_text_length": 51194,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10801255)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "38288010",
      "title": "Identifying and handling data bias within primary healthcare data using synthetic data generators.",
      "abstract": "Advanced synthetic data generators can simulate data samples that closely resemble sensitive personal datasets while significantly reducing the risk of individual identification. The use of these advanced generators holds enormous potential in the medical field, as it allows for the simulation and sharing of sensitive patient data. This enables the development and rigorous validation of novel AI technologies for accurate diagnosis and efficient disease management. Despite the availability of massive ground truth datasets (such as UK-NHS databases that contain millions of patient records), the risk of biases being carried over to data generators still exists. These biases may arise from the under-representation of specific patient cohorts due to cultural sensitivities within certain communities or standardised data collection procedures. Machine learning models can exhibit bias in various forms, including the under-representation of certain groups in the data. This can lead to missing data and inaccurate correlations and distributions, which may also be reflected in synthetic data. Our paper aims to improve synthetic data generators by introducing probabilistic approaches to first detect difficult-to-predict data samples in ground truth data and then boost them when applying the generator. In addition, we explore strategies to generate synthetic data that can reduce bias and, at the same time, improve the performance of predictive models.",
      "journal": "Heliyon",
      "year": "2024",
      "doi": "10.1016/j.heliyon.2024.e24164",
      "authors": "Draghi Barbara et al.",
      "keywords": "Bayesian networks; Data bias; Machine learning; Over-sampling; Synthetic data generators",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38288010/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10823075",
      "ft_text_length": 39605,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10823075)",
      "ft_reason": "Included: bias central + approach content (11 indicators)"
    },
    {
      "pmid": "38315667",
      "title": "Assessment of differentially private synthetic data for utility and fairness in end-to-end machine learning pipelines for tabular data.",
      "abstract": "Differentially private (DP) synthetic datasets are a solution for sharing data while preserving the privacy of individual data providers. Understanding the effects of utilizing DP synthetic data in end-to-end machine learning pipelines impacts areas such as health care and humanitarian action, where data is scarce and regulated by restrictive privacy laws. In this work, we investigate the extent to which synthetic data can replace real, tabular data in machine learning pipelines and identify the most effective synthetic data generation techniques for training and evaluating machine learning models. We systematically investigate the impacts of differentially private synthetic data on downstream classification tasks from the point of view of utility as well as fairness. Our analysis is comprehensive and includes representatives of the two main types of synthetic data generation algorithms: marginal-based and GAN-based. To the best of our knowledge, our work is the first that: (i) proposes a training and evaluation framework that does not assume that real data is available for testing the utility and fairness of machine learning models trained on synthetic data; (ii) presents the most extensive analysis of synthetic dataset generation algorithms in terms of utility and fairness when used for training machine learning models; and (iii) encompasses several different definitions of fairness. Our findings demonstrate that marginal-based synthetic data generators surpass GAN-based ones regarding model training utility for tabular data. Indeed, we show that models trained using data generated by marginal-based algorithms can exhibit similar utility to models trained using real data. Our analysis also reveals that the marginal-based synthetic data generated using AIM and MWEM PGM algorithms can train models that simultaneously achieve utility and fairness characteristics close to those obtained by models trained with real data.",
      "journal": "PloS one",
      "year": "2024",
      "doi": "10.1371/journal.pone.0297271",
      "authors": "Pereira Mayana et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Health Facilities; Interior Design and Furnishings; Knowledge; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38315667/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10843030",
      "ft_text_length": 61044,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10843030)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "38322109",
      "title": "Accelerating health disparities research with artificial intelligence.",
      "abstract": "",
      "journal": "Frontiers in digital health",
      "year": "2024",
      "doi": "10.3389/fdgth.2024.1330160",
      "authors": "Green B Lee et al.",
      "keywords": "artificial intelligence; bias; equity; health disparities; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38322109/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10844447",
      "ft_text_length": 10953,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10844447)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38324545",
      "title": "Classification performance bias between training and test sets in a limited mammography dataset.",
      "abstract": "OBJECTIVES: To assess the performance bias caused by sampling data into training and test sets in a mammography radiomics study. METHODS: Mammograms from 700 women were used to study upstaging of ductal carcinoma in situ. The dataset was repeatedly shuffled and split into training (n = 400) and test cases (n = 300) forty times. For each split, cross-validation was used for training, followed by an assessment of the test set. Logistic regression with regularization and support vector machine were used as the machine learning classifiers. For each split and classifier type, multiple models were created based on radiomics and/or clinical features. RESULTS: Area under the curve (AUC) performances varied considerably across the different data splits (e.g., radiomics regression model: train 0.58-0.70, test 0.59-0.73). Performances for regression models showed a tradeoff where better training led to worse testing and vice versa. Cross-validation over all cases reduced this variability, but required samples of 500+ cases to yield representative estimates of performance. CONCLUSIONS: In medical imaging, clinical datasets are often limited to relatively small size. Models built from different training sets may not be representative of the whole dataset. Depending on the selected data split and model, performance bias could lead to inappropriate conclusions that might influence the clinical significance of the findings. ADVANCES IN KNOWLEDGE: Performance bias can result from model testing when using limited datasets. Optimal strategies for test set selection should be developed to ensure study conclusions are appropriate.",
      "journal": "PloS one",
      "year": "2024",
      "doi": "10.1371/journal.pone.0282402",
      "authors": "Hou Rui et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Mammography; Machine Learning; Retrospective Studies",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38324545/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10849231",
      "ft_text_length": 18253,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10849231)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38335791",
      "title": "(Predictable) performance bias in unsupervised anomaly detection.",
      "abstract": "BACKGROUND: With the ever-increasing amount of medical imaging data, the demand for algorithms to assist clinicians has amplified. Unsupervised anomaly detection (UAD) models promise to aid in the crucial first step of disease detection. While previous studies have thoroughly explored fairness in supervised models in healthcare, for UAD, this has so far been unexplored. METHODS: In this study, we evaluated how dataset composition regarding subgroups manifests in disparate performance of UAD models along multiple protected variables on three large-scale publicly available chest X-ray datasets. Our experiments were validated using two state-of-the-art UAD models for medical images. Finally, we introduced subgroup-AUROC (sAUROC), which aids in quantifying fairness in machine learning. FINDINGS: Our experiments revealed empirical \"fairness laws\" (similar to \"scaling laws\" for Transformers) for training-dataset composition: Linear relationships between anomaly detection performance within a subpopulation and its representation in the training data. Our study further revealed performance disparities, even in the case of balanced training data, and compound effects that exacerbate the drop in performance for subjects associated with multiple adversely affected groups. INTERPRETATION: Our study quantified the disparate performance of UAD models against certain demographic subgroups. Importantly, we showed that this unfairness cannot be mitigated by balanced representation alone. Instead, the representation of some subgroups seems harder to learn by UAD models than that of others. The empirical \"fairness laws\" discovered in our study make disparate performance in UAD models easier to estimate and aid in determining the most desirable dataset composition. FUNDING: European Research Council Deep4MI.",
      "journal": "EBioMedicine",
      "year": "2024",
      "doi": "10.1016/j.ebiom.2024.105002",
      "authors": "Meissen Felix et al.",
      "keywords": "Algorithmic bias; Anomaly detection; Artificial intelligence; Machine learning; Subgroup disparities",
      "mesh_terms": "Humans; Algorithms; Hydrolases; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38335791/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10873649",
      "ft_text_length": 31768,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10873649)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38360543",
      "title": "Calibration and XGBoost reweighting to reduce coverage and non-response biases in overlapping panel surveys: application to the Healthcare and Social Survey.",
      "abstract": "BACKGROUND: Surveys have been used worldwide to provide information on the COVID-19 pandemic impact so as to prepare and deliver an effective Public Health response. Overlapping panel surveys allow longitudinal estimates and more accurate cross-sectional estimates to be obtained thanks to the larger sample size. However, the problem of non-response is particularly aggravated in the case of panel surveys due to population fatigue with repeated surveys. OBJECTIVE: To develop a new reweighting method for overlapping panel surveys affected by non-response. METHODS: We chose the Healthcare and Social Survey which has an overlapping panel survey design with measurements throughout 2020 and 2021, and random samplings stratified by province and degree of urbanization. Each measurement comprises two samples: a longitudinal sample taken from previous measurements and a new sample taken at each measurement. RESULTS: Our reweighting methodological approach is the result of a two-step process: the original sampling design weights are corrected by modelling non-response with respect to the longitudinal sample obtained in a previous measurement using machine learning techniques, followed by calibration using the auxiliary information available at the population level. It is applied to the estimation of totals, proportions, ratios, and differences between measurements, and to gender gaps in the variable of self-perceived general health. CONCLUSION: The proposed method produces suitable estimators for both cross-sectional and longitudinal samples. For addressing future health crises such as COVID-19, it is therefore necessary to reduce potential coverage and non-response biases in surveys by means of utilizing reweighting techniques as proposed in this study.",
      "journal": "BMC medical research methodology",
      "year": "2024",
      "doi": "10.1186/s12874-024-02171-z",
      "authors": "Castro Luis et al.",
      "keywords": "COVID-19; Machine learning; Non-response bias; Panel surveys; Public health; Sampling",
      "mesh_terms": "Humans; Cross-Sectional Studies; Calibration; Pandemics; Surveys and Questionnaires; COVID-19; Bias; Delivery of Health Care",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38360543/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10868104",
      "ft_text_length": 137853,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10868104)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "38388855",
      "title": "Assessing supervisor versus trainee viewpoints of entrustment through cognitive and affective lenses: an artificial intelligence investigation of bias in feedback.",
      "abstract": "The entrustment framework redirects assessment from considering only trainees' competence to decision-making about their readiness to perform clinical tasks independently. Since trainees and supervisors both contribute to entrustment decisions, we examined the cognitive and affective factors that underly their negotiation of trust, and whether trainee demographic characteristics may bias them. Using a document analysis approach, we adapted large language models (LLMs) to examine feedback dialogs (N\u2009=\u200924,187, each with an associated entrustment rating) between medical student trainees and their clinical supervisors. We compared how trainees and supervisors differentially documented feedback dialogs about similar tasks by identifying qualitative themes and quantitatively assessing their correlation with entrustment ratings. Supervisors' themes predominantly reflected skills related to patient presentations, while trainees' themes were broader-including clinical performance and personal qualities. To examine affect, we trained an LLM to measure feedback sentiment. On average, trainees used more negative language (5.3% lower probability of positive sentiment, p\u2009<\u20090.05) compared to supervisors, while documenting higher entrustment ratings (+\u20090.08 on a 1-4 scale, p\u2009<\u20090.05). We also found biases tied to demographic characteristics: trainees' documentation reflected more positive sentiment in the case of male trainees (+\u20091.3%, p\u2009<\u20090.05) and of trainees underrepresented in medicine (UIM) (+\u20091.3%, p\u2009<\u20090.05). Entrustment ratings did not appear to reflect these biases, neither when documented by trainee nor supervisor. As such, bias appeared to influence the emotive language trainees used to document entrustment more than the degree of entrustment they experienced. Mitigating these biases is nonetheless important because they may affect trainees' assimilation into their roles and formation of trusting relationships.",
      "journal": "Advances in health sciences education : theory and practice",
      "year": "2024",
      "doi": "10.1007/s10459-024-10311-9",
      "authors": "Gin Brian C et al.",
      "keywords": "Artificial intelligence; Clinical supervision; Entrustment; Feedback; Gender bias; Large language models; Natural language processing",
      "mesh_terms": "Humans; Trust; Clinical Competence; Students, Medical; Male; Artificial Intelligence; Female; Feedback; Cognition; Bias; Formative Feedback",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38388855/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11549112",
      "ft_text_length": 52414,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11549112)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38402414",
      "title": "Evaluating and improving health equity and fairness of polygenic scores.",
      "abstract": "Polygenic scores (PGSs) are quantitative metrics for predicting phenotypic values, such as human height or disease status. Some PGS methods require only summary statistics of a relevant genome-wide association study (GWAS) for their score. One such method is Lassosum, which inherits the model selection advantages of Lasso to select a meaningful subset of the GWAS single-nucleotide polymorphisms as predictors from their association statistics. However, even efficient scores like Lassosum, when derived from European-based GWASs, are poor predictors of phenotype for subjects of non-European ancestry; that is, they have limited portability to other ancestries. To increase the portability of Lassosum, when GWAS information and estimates of linkage disequilibrium are available for both ancestries, we propose Joint-Lassosum (JLS). In the simulation settings we explore, JLS provides more accurate PGSs compared to other methods, especially when measured in terms of fairness. In analyses of UK Biobank data, JLS was computationally more efficient but slightly less accurate than a Bayesian comparator, SDPRX. Like all PGS methods, JLS requires selection of predictors, which are determined by data-driven tuning parameters. We describe a new approach to selecting tuning parameters and note its relevance for model selection for any PGS. We also draw connections to the literature on algorithmic fairness and discuss how JLS can help mitigate fairness-related harms that might result from the use of PGSs in clinical settings. While no PGS method is likely to be universally portable, due to the diversity of human populations and unequal information content of GWASs for different ancestries, JLS is an effective approach for enhancing portability and reducing predictive bias.",
      "journal": "HGG advances",
      "year": "2024",
      "doi": "10.1016/j.xhgg.2024.100280",
      "authors": "Zhang Tianyu et al.",
      "keywords": "",
      "mesh_terms": "Humans; Bayes Theorem; Genome-Wide Association Study; Health Equity; Benchmarking; Computer Simulation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38402414/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10937319",
      "ft_text_length": 35307,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10937319)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38422347",
      "title": "Evaluating Algorithmic Bias in 30-Day Hospital Readmission Models: Retrospective Analysis.",
      "abstract": "BACKGROUND: The adoption of predictive algorithms in health care comes with the potential for algorithmic bias, which could exacerbate existing disparities. Fairness metrics have been proposed to measure algorithmic bias, but their application to real-world tasks is limited. OBJECTIVE: This study aims to evaluate the algorithmic bias associated with the application of common 30-day hospital readmission models and assess the usefulness and interpretability of selected fairness metrics. METHODS: We used 10.6 million adult inpatient discharges from Maryland and Florida from 2016 to 2019 in this retrospective study. Models predicting 30-day hospital readmissions were evaluated: LACE Index, modified HOSPITAL score, and modified Centers for Medicare & Medicaid Services (CMS) readmission measure, which were applied as-is (using existing coefficients) and retrained (recalibrated with 50% of the data). Predictive performances and bias measures were evaluated for all, between Black and White populations, and between low- and other-income groups. Bias measures included the parity of false negative rate (FNR), false positive rate (FPR), 0-1 loss, and generalized entropy index. Racial bias represented by FNR and FPR differences was stratified to explore shifts in algorithmic bias in different populations. RESULTS: The retrained CMS model demonstrated the best predictive performance (area under the curve: 0.74 in Maryland and 0.68-0.70 in Florida), and the modified HOSPITAL score demonstrated the best calibration (Brier score: 0.16-0.19 in Maryland and 0.19-0.21 in Florida). Calibration was better in White (compared to Black) populations and other-income (compared to low-income) groups, and the area under the curve was higher or similar in the Black (compared to White) populations. The retrained CMS and modified HOSPITAL score had the lowest racial and income bias in Maryland. In Florida, both of these models overall had the lowest income bias and the modified HOSPITAL score showed the lowest racial bias. In both states, the White and higher-income populations showed a higher FNR, while the Black and low-income populations resulted in a higher FPR and a higher 0-1 loss. When stratified by hospital and population composition, these models demonstrated heterogeneous algorithmic bias in different contexts and populations. CONCLUSIONS: Caution must be taken when interpreting fairness measures' face value. A higher FNR or FPR could potentially reflect missed opportunities or wasted resources, but these measures could also reflect health care use patterns and gaps in care. Simply relying on the statistical notions of bias could obscure or underplay the causes of health disparity. The imperfect health data, analytic frameworks, and the underlying health systems must be carefully considered. Fairness measures can serve as a useful routine assessment to detect disparate model performances but are insufficient to inform mechanisms or policy changes. However, such an assessment is an important first step toward data-driven improvement to address existing health disparities.",
      "journal": "Journal of medical Internet research",
      "year": "2024",
      "doi": "10.2196/47125",
      "authors": "Wang H Echo et al.",
      "keywords": "algorithmic bias; health disparity; hospital readmission; model bias; model fairness; predictive models; retrospective analysis",
      "mesh_terms": "Aged; Adult; Humans; United States; Patient Readmission; Retrospective Studies; Medicare; Hospitals; Florida",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, P.H.S.; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38422347/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11066744",
      "ft_text_length": 47608,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11066744)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "38438371",
      "title": "Enhancing the fairness of AI prediction models by Quasi-Pareto improvement among heterogeneous thyroid nodule population.",
      "abstract": "Artificial Intelligence (AI) models for medical diagnosis often face challenges of generalizability and fairness. We highlighted the algorithmic unfairness in a large thyroid ultrasound dataset with significant diagnostic performance disparities across subgroups linked causally to sample size imbalances. To address this, we introduced the Quasi-Pareto Improvement (QPI) approach and a deep learning implementation (QP-Net) combining multi-task learning and domain adaptation to improve model performance among disadvantaged subgroups without compromising overall population performance. On the thyroid ultrasound dataset, our method significantly mitigated the area under curve (AUC) disparity for three less-prevalent subgroups by 0.213, 0.112, and 0.173 while maintaining the AUC for dominant subgroups; we also further confirmed the generalizability of our approach on two public datasets: the ISIC2019 skin disease dataset and the CheXpert chest radiograph dataset. Here we show the QPI approach to be widely applicable in promoting AI for equitable healthcare outcomes.",
      "journal": "Nature communications",
      "year": "2024",
      "doi": "10.1038/s41467-024-44906-y",
      "authors": "Yao Siqiong et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Thyroid Nodule; Area Under Curve; Health Facilities; Sample Size",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38438371/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10912763",
      "ft_text_length": 81863,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10912763)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "38471396",
      "title": "Drop the shortcuts: image augmentation improves fairness and decreases AI detection of race and other demographics from medical images.",
      "abstract": "BACKGROUND: It has been shown that AI models can learn race on medical images, leading to algorithmic bias. Our aim in this study was to enhance the fairness of medical image models by eliminating bias related to race, age, and sex. We hypothesise models may be learning demographics via shortcut learning and combat this using image augmentation. METHODS: This study included 44,953 patients who identified as Asian, Black, or White (mean age, 60.68 years\u00a0\u00b118.21; 23,499 women) for a total of 194,359 chest X-rays (CXRs) from MIMIC-CXR database. The included CheXpert images comprised 45,095 patients (mean age 63.10 years\u00a0\u00b118.14; 20,437 women) for a total of 134,300 CXRs were used for external validation. We also collected 1195 3D brain magnetic resonance imaging (MRI) data from the ADNI database, which included 273 participants with an average age of 76.97 years\u00a0\u00b114.22, and 142 females. DL models were trained on either non-augmented or augmented images and assessed using disparity metrics. The features learned by the models were analysed using task transfer experiments and model visualisation techniques. FINDINGS: In the detection of radiological findings, training a model using augmented CXR images was shown to reduce disparities in error rate among racial groups (-5.45%), age groups (-13.94%), and sex (-22.22%). For AD detection, the model trained with augmented MRI images was shown 53.11% and 31.01% reduction of disparities in error rate among age and sex groups, respectively. Image augmentation led to a reduction in the model's ability to identify demographic attributes and resulted in the model trained for clinical purposes incorporating fewer demographic features. INTERPRETATION: The model trained using the augmented images was less likely to be influenced by demographic information in detecting image labels. These results demonstrate that the proposed augmentation scheme could enhance the fairness of interpretations by DL models when dealing with data from patients with different demographic backgrounds. FUNDING: National Science and Technology Council (Taiwan), National Institutes of Health.",
      "journal": "EBioMedicine",
      "year": "2024",
      "doi": "10.1016/j.ebiom.2024.105047",
      "authors": "Wang Ryan et al.",
      "keywords": "Augmentation; Bias mitigation; Deep learning; Fairness; Shortcuts",
      "mesh_terms": "Aged; Female; Humans; Middle Aged; Benchmarking; Black People; Brain; Demography; Learning; United States; Asian People; White People; Male; Black or African American",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38471396/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10945176",
      "ft_text_length": 51692,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10945176)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "38486100",
      "title": "Preserving fairness and diagnostic accuracy in private large-scale AI models for medical imaging.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) models are increasingly used in the medical domain. However, as medical data is highly sensitive, special precautions to ensure its protection are required. The gold standard for privacy preservation is the introduction of differential privacy (DP) to model training. Prior work indicates that DP has negative implications on model accuracy and fairness, which are unacceptable in medicine and represent a main barrier to the widespread use of privacy-preserving techniques. In this work, we evaluated the effect of privacy-preserving training of AI models regarding accuracy and fairness compared to non-private training. METHODS: We used two datasets: (1) A large dataset (N\u2009=\u2009193,311) of high quality clinical chest radiographs, and (2) a dataset (N\u2009=\u20091625) of 3D abdominal computed tomography (CT) images, with the task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC). Both were retrospectively collected and manually labeled by experienced radiologists. We then compared non-private deep convolutional neural networks (CNNs) and privacy-preserving (DP) models with respect to privacy-utility trade-offs measured as area under the receiver operating characteristic curve (AUROC), and privacy-fairness trade-offs, measured as Pearson's r or Statistical Parity Difference. RESULTS: We find that, while the privacy-preserving training yields lower accuracy, it largely does not amplify discrimination against age, sex or co-morbidity. However, we find an indication that difficult diagnoses and subgroups suffer stronger performance hits in private training. CONCLUSIONS: Our study shows that - under the challenging realistic circumstances of a real-life clinical dataset - the privacy-preserving training of diagnostic deep learning models is possible with excellent diagnostic accuracy and fairness. Artificial intelligence (AI), in which computers can learn to do tasks that normally require human intelligence, is particularly useful in medical imaging. However, AI should be used in a way that preserves patient privacy. We explored the balance between maintaining patient data privacy and AI performance in medical imaging. We use an approach called differential privacy to protect the privacy of patients\u2019 images. We show that, although training AI with differential privacy leads to a slight decrease in accuracy, it does not substantially increase bias against different age groups, genders, or patients with multiple health conditions. However, we notice that AI faces more challenges in accurately diagnosing complex cases and specific subgroups when trained under these privacy constraints. These findings highlight the importance of designing AI systems that are both privacy-conscious and capable of reliable diagnoses across patient groups.",
      "journal": "Communications medicine",
      "year": "2024",
      "doi": "10.1038/s43856-024-00462-6",
      "authors": "Tayebi Arasteh Soroosh et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38486100/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10940659",
      "ft_text_length": 45029,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10940659)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38487797",
      "title": "FRAMM: Fair ranking with missing modalities for clinical trial site selection.",
      "abstract": "The underrepresentation of gender, racial, and ethnic minorities in clinical trials is a problem undermining the efficacy of treatments on minorities and preventing precise estimates of the effects within these subgroups. We propose FRAMM, a deep reinforcement learning framework for fair trial site selection to help address this problem. We focus on two real-world challenges: the data modalities used to guide selection are often incomplete for many potential trial sites, and the site selection needs to simultaneously optimize for both enrollment and diversity. To address the missing data challenge, FRAMM has a modality encoder with a masked cross-attention mechanism for bypassing missing data. To make efficient trade-offs, FRAMM uses deep reinforcement learning with a reward function designed to simultaneously optimize for both enrollment and fairness. We evaluate FRAMM using real-world historical clinical trials and show that it outperforms the leading baseline in enrollment-only settings while also greatly improving diversity.",
      "journal": "Patterns (New York, N.Y.)",
      "year": "2024",
      "doi": "10.1016/j.patter.2024.100944",
      "authors": "Theodorou Brandon et al.",
      "keywords": "deep learning; fairness in healthcare; fairness in machine learning; learning to ranking; machine learning for healthcare; missing data; reinforcement learning; trial site selection",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38487797/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10935501",
      "ft_text_length": 43932,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10935501)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "38511501",
      "title": "Leveraging large language models to foster equity in healthcare.",
      "abstract": "OBJECTIVES: Large language models (LLMs) are poised to change care delivery, but their impact on health equity is unclear. While marginalized populations have been historically excluded from early technology developments, LLMs present an opportunity to change our approach to developing, evaluating, and implementing new technologies. In this perspective, we describe the role of LLMs in supporting health equity. MATERIALS AND METHODS: We apply the National Institute on Minority Health and Health Disparities (NIMHD) research framework to explore the use of LLMs for health equity. RESULTS: We present opportunities for how LLMs can improve health equity across individual, family and organizational, community, and population health. We describe emerging concerns including biased data, limited technology diffusion, and privacy. Finally, we highlight recommendations focused on prompt engineering, retrieval augmentation, digital inclusion, transparency, and bias mitigation. CONCLUSION: The potential of LLMs to support health equity depends on making health equity a focus from the start.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2024",
      "doi": "10.1093/jamia/ocae055",
      "authors": "Rodriguez Jorge A et al.",
      "keywords": "artificial intelligence; digital inclusion; health disparities; health equity",
      "mesh_terms": "Health Equity; Humans; United States; Delivery of Health Care; Healthcare Disparities; Language",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38511501/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11339521",
      "ft_text_length": 1099,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11339521)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38529077",
      "title": "A survey of recent methods for addressing AI fairness and bias in biomedicine.",
      "abstract": "OBJECTIVES: Artificial intelligence (AI) systems have the potential to revolutionize clinical practices, including improving diagnostic accuracy and surgical decision-making, while also reducing costs and manpower. However, it is important to recognize that these systems may perpetuate social inequities or demonstrate biases, such as those based on race or gender. Such biases can occur before, during, or after the development of AI models, making it critical to understand and address potential biases to enable the accurate and reliable application of AI models in clinical settings. To mitigate bias concerns during model development, we surveyed recent publications on different debiasing methods in the fields of biomedical natural language processing (NLP) or computer vision (CV). Then we discussed the methods, such as data perturbation and adversarial learning, that have been applied in the biomedical domain to address bias. METHODS: We performed our literature search on PubMed, ACM digital library, and IEEE Xplore of relevant articles published between January 2018 and December 2023 using multiple combinations of keywords. We then filtered the result of 10,041 articles automatically with loose constraints, and manually inspected the abstracts of the remaining 890 articles to identify the 55 articles included in this review. Additional articles in the references are also included in this review. We discuss each method and compare its strengths and weaknesses. Finally, we review other potential methods from the general domain that could be applied to biomedicine to address bias and improve fairness. RESULTS: The bias of AIs in biomedicine can originate from multiple sources such as insufficient data, sampling bias and the use of health-irrelevant features or race-adjusted algorithms. Existing debiasing methods that focus on algorithms can be categorized into distributional or algorithmic. Distributional methods include data augmentation, data perturbation, data reweighting methods, and federated learning. Algorithmic approaches include unsupervised representation learning, adversarial learning, disentangled representation learning, loss-based methods and causality-based methods.",
      "journal": "ArXiv",
      "year": "2024",
      "doi": "10.48550/arXiv.2110.08527",
      "authors": "Yang Yifan et al.",
      "keywords": "AI; bias; biomedicine; fairness",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38529077/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10962742",
      "ft_text_length": 37936,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10962742)",
      "ft_reason": "Included: bias central + approach content (13 indicators)"
    },
    {
      "pmid": "38531676",
      "title": "Preparing for the bedside-optimizing a postpartum depression risk prediction model for clinical implementation in a health system.",
      "abstract": "OBJECTIVE: We developed and externally validated a machine-learning model to predict postpartum depression (PPD) using data from electronic health records (EHRs). Effort is under way to implement the PPD prediction model within the EHR system for clinical decision support. We describe the pre-implementation evaluation process that considered model performance, fairness, and clinical appropriateness. MATERIALS AND METHODS: We used EHR data from an academic medical center (AMC) and a clinical research network database from 2014 to 2020 to evaluate the predictive performance and net benefit of the PPD risk model. We used area under the curve and sensitivity as predictive performance and conducted a decision curve analysis. In assessing model fairness, we employed metrics such as disparate impact, equal opportunity, and predictive parity with the White race being the privileged value. The model was also reviewed by multidisciplinary experts for clinical appropriateness. Lastly, we debiased the model by comparing 5 different debiasing approaches of fairness through blindness and reweighing. RESULTS: We determined the classification threshold through a performance evaluation that prioritized sensitivity and decision curve analysis. The baseline PPD model exhibited some unfairness in the AMC data but had a fair performance in the clinical research network data. We revised the model by fairness through blindness, a debiasing approach that yielded the best overall performance and fairness, while considering clinical appropriateness suggested by the expert reviewers. DISCUSSION AND CONCLUSION: The findings emphasize the need for a thorough evaluation of intervention-specific models, considering predictive performance, fairness, and appropriateness before clinical implementation.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2024",
      "doi": "10.1093/jamia/ocae056",
      "authors": "Liu Yifan et al.",
      "keywords": "electronic health record; health equity; machine learning implementation; postpartum depression",
      "mesh_terms": "Humans; Depression, Postpartum; Female; Electronic Health Records; Machine Learning; Risk Assessment; Decision Support Systems, Clinical",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38531676/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11105144",
      "ft_text_length": 39041,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11105144)",
      "ft_reason": "Included: substantial approach content (14 indicators)"
    },
    {
      "pmid": "38570587",
      "title": "Fairness and bias correction in machine learning for depression prediction across four study populations.",
      "abstract": "A significant level of stigma and inequality exists in mental healthcare, especially in under-served populations. Inequalities are reflected in the data collected for scientific purposes. When not properly accounted for, machine learning (ML) models learned from data can reinforce these structural inequalities or biases. Here, we present a systematic study of bias in ML models designed to predict depression in four different case studies covering different countries and populations. We find that standard ML approaches regularly present biased behaviors. We also show that mitigation techniques, both standard and our own post-hoc method, can be effective in reducing the level of unfair bias. There is no one best ML model for depression prediction that provides equality of outcomes. This emphasizes the importance of analyzing fairness during model selection and transparent reporting about the impact of debiasing interventions. Finally, we also identify positive habits and open challenges that practitioners could follow to enhance fairness in their models.",
      "journal": "Scientific reports",
      "year": "2024",
      "doi": "10.1038/s41598-024-58427-7",
      "authors": "Dang Vien Ngoc et al.",
      "keywords": "Algorithmic fairness; Bias mitigation; Machine learning for depression prediction; Novel post-hoc method; Psychiatric healthcare equity",
      "mesh_terms": "Humans; Depression; Bias; Habits; Health Facilities; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38570587/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10991528",
      "ft_text_length": 58067,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10991528)",
      "ft_reason": "Included: bias central + approach content (18 indicators)"
    },
    {
      "pmid": "38573370",
      "title": "Algor-ethics: charting the ethical path for AI in critical care.",
      "abstract": "The integration of Clinical Decision Support Systems (CDSS) based on artificial intelligence (AI) in healthcare is groundbreaking evolution with enormous potential, but its development and ethical implementation, presents unique challenges, particularly in critical care, where physicians often deal with life-threating conditions requiring rapid actions and patients unable to participate in the decisional process. Moreover, development of AI-based CDSS is complex and should address different sources of bias, including data acquisition, health disparities, domain shifts during clinical use, and cognitive biases in decision-making. In this scenario algor-ethics is mandatory and emphasizes the integration of 'Human-in-the-Loop' and 'Algorithmic Stewardship' principles, and the benefits of advanced data engineering. The establishment of Clinical AI Departments (CAID) is necessary to lead AI innovation in healthcare, ensuring ethical integrity and human-centered development in this rapidly evolving field.",
      "journal": "Journal of clinical monitoring and computing",
      "year": "2024",
      "doi": "10.1007/s10877-024-01157-y",
      "authors": "Montomoli Jonathan et al.",
      "keywords": "Algorethics; Artificial intelligence; Data engineering; Ethics; Machine learning",
      "mesh_terms": "Humans; Artificial Intelligence; Critical Care; Decision Support Systems, Clinical; Algorithms; Clinical Decision-Making",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38573370/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11297831",
      "ft_text_length": 27756,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11297831)",
      "ft_reason": "Included: substantial approach content (3 indicators)"
    },
    {
      "pmid": "38590455",
      "title": "A Gender-Bias-Mitigated, Data-Driven Precision Medicine System to Assist in the Selection of Biological Treatments of Grade 3 and 4 Knee Osteoarthritis: Development and Preliminary Validation of precisionKNEE.",
      "abstract": "Objective To identify key variables predictive of patient responses to microfragmented adipose tissue (MFAT) treatment in knee osteoarthritis (KOA) and evaluate its potential to delay or mitigate the need for total knee replacement (TKR). Methods We utilised a dataset comprising 329 patients treated with MFAT for KOA, incorporating variables such as gender, age, BMI, arthritic aetiology, radiological grade, and Oxford Knee Scores (OKS) pre- and post-treatment. We employed random forest regressors for model training and testing, with gender bias mitigation and outlier detection to enhance prediction accuracy. Model performance was assessed through root mean squared error (RMSE) and mean absolute error (MAE), with further validation in a TKR-suitable patient subset. Results The model achieved a test RMSE of 6.72 and an MAE of 5.38, reflecting moderate predictive accuracy across the patient cohort. Stratification by gender revealed no statistically significant differences between actual and predicted OKS improvements (p-values: males = 0.93, females = 0.92). For the subset of patients suitable for TKR, the model presented an increased RMSE of 9.77 and MAE of 7.81, indicating reduced accuracy in this group. The decision tree analysis identified pre-operative OKS, radiological grade, and gender as significant predictors of post-treatment outcomes, with pre-operative OKS being the most critical determinant. Patients with lower pre-operative OKS showed varying responses based on radiological severity and gender, suggesting a nuanced interaction between these factors in determining treatment efficacy. Conclusion This study highlights the potential of MFAT as a non-surgical alternative for KOA treatment, emphasising the importance of personalised patient assessments. While promising, the predictive model warrants further refinement and validation with a larger, more diverse dataset to improve its utility in clinical decision-making for KOA management.",
      "journal": "Cureus",
      "year": "2024",
      "doi": "10.7759/cureus.55832",
      "authors": "Heidari Nima et al.",
      "keywords": "artificial intelligence in health care; knee osteoarthritis/ koa; microfragmented fat injection; non-surgical treatment; orthopaedic biologics; precision medicine; total knee replacement(tkr)",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38590455/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11000206",
      "ft_text_length": 18686,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11000206)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38600282",
      "title": "Generative models improve fairness of medical classifiers under distribution shifts.",
      "abstract": "Domain generalization is a ubiquitous challenge for machine learning in healthcare. Model performance in real-world conditions might be lower than expected because of discrepancies between the data encountered during deployment and development. Underrepresentation of some groups or conditions during model development is a common cause of this phenomenon. This challenge is often not readily addressed by targeted data acquisition and 'labeling' by expert clinicians, which can be prohibitively expensive or practically impossible because of the rarity of conditions or the available clinical expertise. We hypothesize that advances in generative artificial intelligence can help mitigate this unmet need in a steerable fashion, enriching our training dataset with synthetic examples that address shortfalls of underrepresented conditions or subgroups. We show that diffusion models can automatically learn realistic augmentations from data in a label-efficient manner. We demonstrate that learned augmentations make models more robust and statistically fair in-distribution and out of distribution. To evaluate the generality of our approach, we studied three distinct medical imaging contexts of varying difficulty: (1) histopathology, (2) chest X-ray and (3) dermatology images. Complementing real samples with synthetic ones improved the robustness of models in all three medical tasks and increased fairness by improving the accuracy of clinical diagnosis within underrepresented groups, especially out of distribution.",
      "journal": "Nature medicine",
      "year": "2024",
      "doi": "10.1038/s41591-024-02838-6",
      "authors": "Ktena Ira et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38600282/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11031395",
      "ft_text_length": 110878,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11031395)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38606108",
      "title": "Impact of bias field correction on 0.35 T pelvic MR images: evaluation on generative adversarial network-based OARs' auto-segmentation and visual grading assessment.",
      "abstract": "PURPOSE: Magnetic resonance imaging (MRI)-guided radiotherapy enables adaptive treatment plans based on daily anatomical changes and accurate organ visualization. However, the bias field artifact can compromise image quality, affecting diagnostic accuracy and quantitative analyses. This study aims to assess the impact of bias field correction on 0.35 T pelvis MRIs by evaluating clinical anatomy visualization and generative adversarial network (GAN) auto-segmentation performance. MATERIALS AND METHODS: 3D simulation MRIs from 60 prostate cancer patients treated on MR-Linac (0.35 T) were collected and preprocessed with the N4ITK algorithm for bias field correction. A 3D GAN architecture was trained, validated, and tested on 40, 10, and 10 patients, respectively, to auto-segment the organs at risk (OARs) rectum and bladder. The GAN was trained and evaluated either with the original or the bias-corrected MRIs. The Dice similarity coefficient (DSC) and 95th percentile Hausdorff distance (HD95th) were computed for the segmented volumes of each patient. The Wilcoxon signed-rank test assessed the statistical difference of the metrics within OARs, both with and without bias field correction. Five radiation oncologists blindly scored 22 randomly chosen patients in terms of overall image quality and visibility of boundaries (prostate, rectum, bladder, seminal vesicles) of the original and bias-corrected MRIs. Bennett's S score and Fleiss' kappa were used to assess the pairwise interrater agreement and the interrater agreement among all the observers, respectively. RESULTS: In the test set, the GAN trained and evaluated on original and bias-corrected MRIs showed DSC/HD95th of 0.92/5.63\u00a0mm and 0.92/5.91\u00a0mm for the bladder and 0.84/10.61\u00a0mm and 0.83/9.71\u00a0mm for the rectum. No statistical differences in the distribution of the evaluation metrics were found neither for the bladder (DSC: p = 0.07; HD95th: p = 0.35) nor for the rectum (DSC: p = 0.32; HD95th: p = 0.63). From the clinical visual grading assessment, the bias-corrected MRI resulted mostly in either no change or an improvement of the image quality and visualization of the organs' boundaries compared with the original MRI. CONCLUSION: The bias field correction did not improve the anatomy visualization from a clinical point of view and the OARs' auto-segmentation outputs generated by the GAN.",
      "journal": "Frontiers in oncology",
      "year": "2024",
      "doi": "10.3389/fonc.2024.1294252",
      "authors": "Vagni Marica et al.",
      "keywords": "0.35 T MRIgRT; N4ITK algorithm; bias field artifact; generative adversarial networks; prostate cancer; visual grading assessment",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38606108/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11007142",
      "ft_text_length": 23775,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11007142)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38638298",
      "title": "Exploring the impact of missingness on racial disparities in predictive performance of a machine learning model for emergency department triage.",
      "abstract": "OBJECTIVE: To investigate how missing data in the patient problem list may impact racial disparities in the predictive performance of a machine learning (ML) model for emergency department (ED) triage. MATERIALS AND METHODS: Racial disparities may exist in the missingness of EHR data (eg, systematic differences in access, testing, and/or treatment) that can impact model predictions across racialized patient groups. We use an ML model that predicts patients' risk for adverse events to produce triage-level recommendations, patterned after a clinical decision support tool deployed at multiple EDs. We compared the model's predictive performance on sets of observed (problem list data at the point of triage) versus manipulated (updated to the more complete problem list at the end of the encounter) test data. These differences were compared between Black and non-Hispanic White patient groups using multiple performance measures relevant to health equity. RESULTS: There were modest, but significant, changes in predictive performance comparing the observed to manipulated models across both Black and non-Hispanic White patient groups; c-statistic improvement ranged between 0.027 and 0.058. The manipulation produced no between-group differences in c-statistic by race. However, there were small between-group differences in other performance measures, with greater change for non-Hispanic White patients. DISCUSSION: Problem list missingness impacted model performance for both patient groups, with marginal differences detected by race. CONCLUSION: Further exploration is needed to examine how missingness may contribute to racial disparities in clinical model predictions across settings. The novel manipulation method demonstrated may aid future research.",
      "journal": "JAMIA open",
      "year": "2023",
      "doi": "10.1093/jamiaopen/ooad107",
      "authors": "Teeple Stephanie et al.",
      "keywords": "clinical; decision support systems; health equity; triage",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38638298/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11025382",
      "ft_text_length": 37796,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11025382)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38638465",
      "title": "Optimal site selection strategies for urban parks green spaces under the joint perspective of spatial equity and social equity.",
      "abstract": "Urban park green spaces (UPGS) are a crucial element of social public resources closely related to the health and well-being of urban residents, and issues of equity have always been a focal point of concern. This study takes the downtown area of Nanchang as an example and uses more accurate point of interest (POI) and area of interest (AOI) data as analysis sources. The improved Gaussian two-step floating catchment area (G2SFCA) and spatial autocorrelation models are then used to assess the spatial and social equity in the study area, and the results of the two assessments were coupled to determine the optimization objective using the community as the smallest unit. Finally, the assessment results are combined with the k-means algorithm and particle swarm algorithm (PSO) to propose practical optimization strategies with the objectives of minimum walking distance and maximum fairness. The results indicate (1) There are significant differences in UPGS accessibility among residents with different walking distances, with the more densely populated Old Town and Honggu Tan areas having lower average accessibility and being the main areas of hidden blindness, while the fringe areas in the northern and south-western parts of the city are the main areas of visible blindness. (2) Overall, the UPGS accessibility in Nanchang City exhibits a spatial pattern of decreasing from the east, south, and west to the center. Nanchang City is in transition towards improving spatial and social equity while achieving basic regional equity. (3) There is a spatial positive correlation between socioeconomic level and UPGS accessibility, reflecting certain social inequity. (4) Based on the above research results, the UPGS layout optimization scheme was proposed, 29 new UPGS locations and regions were identified, and the overall accessibility was improved by 2.76. The research methodology and framework can be used as a tool to identify the underserved areas of UPGS and optimize the spatial and social equity of UPGS, which is in line with the current trend of urban development in the world and provides a scientific basis for urban infrastructure planning and spatial resource allocation.",
      "journal": "Frontiers in public health",
      "year": "2024",
      "doi": "10.3389/fpubh.2024.1310340",
      "authors": "Zhao Youqiang et al.",
      "keywords": "Gaussian two-step floating catchment area; accessibility; park quality; social equity; spatial equity; urban park green spaces",
      "mesh_terms": "Humans; Parks, Recreational; Cities; Spatial Analysis; Social Class; Blindness",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38638465/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11024374",
      "ft_text_length": 48688,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11024374)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38649446",
      "title": "Measuring algorithmic bias to analyze the reliability of AI tools that predict depression risk using smartphone sensed-behavioral data.",
      "abstract": "AI tools intend to transform mental healthcare by providing remote estimates of depression risk using behavioral data collected by sensors embedded in smartphones. While these tools accurately predict elevated depression symptoms in small, homogenous populations, recent studies show that these tools are less accurate in larger, more diverse populations. In this work, we show that accuracy is reduced because sensed-behaviors are unreliable predictors of depression across individuals: sensed-behaviors that predict depression risk are inconsistent across demographic and socioeconomic subgroups. We first identified subgroups where a developed AI tool underperformed by measuring algorithmic bias, where subgroups with depression were incorrectly predicted to be at lower risk than healthier subgroups. We then found inconsistencies between sensed-behaviors predictive of depression across these subgroups. Our findings suggest that researchers developing AI tools predicting mental health from sensed-behaviors should think critically about the generalizability of these tools, and consider tailored solutions for targeted populations.",
      "journal": "Npj mental health research",
      "year": "2024",
      "doi": "10.1038/s44184-024-00057-y",
      "authors": "Adler Daniel A et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38649446/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11035598",
      "ft_text_length": 44020,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11035598)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38681756",
      "title": "Improving Equity in Deep Learning Medical Applications with the Gerchberg-Saxton Algorithm.",
      "abstract": "Deep learning (DL) has gained prominence in healthcare for its ability to facilitate early diagnosis, treatment identification with associated prognosis, and varying patient outcome predictions. However, because of highly variable medical practices and unsystematic data collection approaches, DL can unfortunately exacerbate biases and distort estimates. For example, the presence of sampling bias poses a significant challenge to the efficacy and generalizability of any statistical model. Even with DL approaches, selection bias can lead to inconsistent, suboptimal, or inaccurate model results, especially for underrepresented populations. Therefore, without addressing bias, wider implementation of DL approaches can potentially cause unintended harm. In this paper, we studied a novel method for bias reduction that leverages the frequency domain transformation via the Gerchberg-Saxton and corresponding impact on the outcome from a racio-ethnic bias perspective.",
      "journal": "Journal of healthcare informatics research",
      "year": "2024",
      "doi": "10.1007/s41666-024-00163-8",
      "authors": "Ay Seha et al.",
      "keywords": "Deep learning; MIMIC-III; Medical decision-making; Mortality rate prediction; Racial bias mitigation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38681756/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11052977",
      "ft_text_length": 44127,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11052977)",
      "ft_reason": "Included: bias central + approach content (7 indicators)"
    },
    {
      "pmid": "38685924",
      "title": "Health equity assessment of machine learning performance (HEAL): a framework and dermatology AI model case study.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) has repeatedly been shown to encode historical inequities in healthcare. We aimed to develop a framework to quantitatively assess the performance equity of health AI technologies and to illustrate its utility via a case study. METHODS: Here, we propose a methodology to assess whether health AI technologies prioritise performance for patient populations experiencing worse outcomes, that is complementary to existing fairness metrics. We developed the Health Equity Assessment of machine Learning performance (HEAL) framework designed to quantitatively assess the performance equity of health AI technologies via a four-step interdisciplinary process to understand and quantify domain-specific criteria, and the resulting HEAL metric. As an illustrative case study (analysis conducted between October 2022 and January 2023), we applied the HEAL framework to a dermatology AI model. A set of 5420 teledermatology cases (store-and-forward cases from patients of 20 years or older, submitted from primary care providers in the USA and skin cancer clinics in Australia), enriched for diversity in age, sex and race/ethnicity, was used to retrospectively evaluate the AI model's HEAL metric, defined as the likelihood that the AI model performs better for subpopulations with worse average health outcomes as compared to others. The likelihood that AI performance was anticorrelated to pre-existing health outcomes was estimated using bootstrap methods as the probability that the negated Spearman's rank correlation coefficient (i.e., \"R\") was greater than zero. Positive values of R suggest that subpopulations with poorer health outcomes have better AI model performance. Thus, the HEAL metric, defined as p (R\u00a0>0), measures how likely the AI technology is to prioritise performance for subpopulations with worse average health outcomes as compared to others (presented as a percentage below). Health outcomes were quantified as disability-adjusted life years (DALYs) when grouping by sex and age, and years of life lost (YLLs) when grouping by race/ethnicity. AI performance was measured as top-3 agreement with the reference diagnosis from a panel of 3 dermatologists per case. FINDINGS: Across all dermatologic conditions, the HEAL metric was 80.5% for prioritizing AI performance of racial/ethnic subpopulations based on YLLs, and 92.1% and 0.0% respectively for prioritizing AI performance of sex and age subpopulations based on DALYs. Certain dermatologic conditions were significantly associated with greater AI model performance compared to a reference category of less common conditions. For skin cancer conditions, the HEAL metric was 73.8% for prioritizing AI performance of age subpopulations based on DALYs. INTERPRETATION: Analysis using the proposed HEAL framework showed that the dermatology AI model prioritised performance for race/ethnicity, sex (all conditions) and age (cancer conditions) subpopulations with respect to pre-existing health disparities. More work is needed to investigate ways of promoting equitable AI performance across age for non-cancer conditions and to better understand how AI models can contribute towards improving equity in health outcomes. FUNDING: Google LLC.",
      "journal": "EClinicalMedicine",
      "year": "2024",
      "doi": "10.1016/j.eclinm.2024.102479",
      "authors": "Schaekermann Mike et al.",
      "keywords": "Artificial intelligence; Dermatology; Health equity; Machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38685924/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11056401",
      "ft_text_length": 43404,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11056401)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38689643",
      "title": "Reinvestigating the performance of artificial intelligence classification algorithms on COVID-19 X-Ray and CT images.",
      "abstract": "There are concerns that artificial intelligence (AI) algorithms may create underdiagnosis bias by mislabeling patient individuals with certain attributes (e.g., female and young) as healthy. Addressing this bias is crucial given the urgent need for AI diagnostics facing rapidly spreading infectious diseases like COVID-19. We find the prevalent AI diagnostic models show an underdiagnosis rate among specific patient populations, and the underdiagnosis rate is higher in some intersectional specific patient populations (for example, females aged 20-40 years). Additionally, we find training AI models on heterogeneous datasets (positive and negative samples from different datasets) may lead to poor model generalization. The model's classification performance varies significantly across test sets, with the accuracy of the better performance being over 40% higher than that of the poor performance. In conclusion, we developed an AI bias analysis pipeline to help researchers recognize and address biases that impact medical equality and ethics.",
      "journal": "iScience",
      "year": "2024",
      "doi": "10.1016/j.isci.2024.109712",
      "authors": "Cao Rui et al.",
      "keywords": "Artificial intelligence applications; Health informatics; Microbiology",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38689643/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11059117",
      "ft_text_length": 37823,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11059117)",
      "ft_reason": "Included: substantial approach content (3 indicators)"
    },
    {
      "pmid": "38696359",
      "title": "Achieving health equity through conversational AI: A roadmap for design and implementation of inclusive chatbots in healthcare.",
      "abstract": "BACKGROUND: The rapid evolution of conversational and generative artificial intelligence (AI) has led to the increased deployment of AI tools in healthcare settings. While these conversational AI tools promise efficiency and expanded access to healthcare services, there are growing concerns ethically, practically and in terms of inclusivity. This study aimed to identify activities which reduce bias in conversational AI and make their designs and implementation more equitable. METHODS: A qualitative research approach was employed to develop an analytical framework based on the content analysis of 17 guidelines about AI use in clinical settings. A stakeholder consultation was subsequently conducted with a total of 33 ethnically diverse community members, AI designers, industry experts and relevant health professionals to further develop a roadmap for equitable design and implementation of conversational AI in healthcare. Framework analysis was conducted on the interview data. RESULTS: A 10-stage roadmap was developed to outline activities relevant to equitable conversational AI design and implementation phases: 1) Conception and planning, 2) Diversity and collaboration, 3) Preliminary research, 4) Co-production, 5) Safety measures, 6) Preliminary testing, 7) Healthcare integration, 8) Service evaluation and auditing, 9) Maintenance, and 10) Termination. DISCUSSION: We have made specific recommendations to increase conversational AI's equity as part of healthcare services. These emphasise the importance of a collaborative approach and the involvement of patient groups in navigating the rapid evolution of conversational AI technologies. Further research must assess the impact of recommended activities on chatbots' fairness and their ability to reduce health inequalities.",
      "journal": "PLOS digital health",
      "year": "2024",
      "doi": "10.1371/journal.pdig.0000492",
      "authors": "Nadarzynski Tom et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38696359/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11065243",
      "ft_text_length": 81363,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11065243)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38701410",
      "title": "mbDecoda: a debiased approach to compositional data analysis for microbiome surveys.",
      "abstract": "Potentially pathogenic or probiotic microbes can be identified by comparing their abundance levels between healthy and diseased populations, or more broadly, by linking microbiome composition with clinical phenotypes or environmental factors. However, in microbiome studies, feature tables provide relative rather than absolute abundance of each feature in each sample, as the microbial loads of the samples and the ratios of sequencing depth to microbial load are both unknown and subject to considerable variation. Moreover, microbiome abundance data are count-valued, often over-dispersed and contain a substantial proportion of zeros. To carry out differential abundance analysis while addressing these challenges, we introduce mbDecoda, a model-based approach for debiased analysis of sparse compositions of microbiomes. mbDecoda employs a zero-inflated negative binomial model, linking mean abundance to the variable of interest through a log link function, and it accommodates the adjustment for confounding factors. To efficiently obtain maximum likelihood estimates of model parameters, an Expectation Maximization algorithm is developed. A minimum coverage interval approach is then proposed to rectify compositional bias, enabling accurate and reliable absolute abundance analysis. Through extensive simulation studies and analysis of real-world microbiome datasets, we demonstrate that mbDecoda compares favorably with state-of-the-art methods in terms of effectiveness, robustness and reproducibility.",
      "journal": "Briefings in bioinformatics",
      "year": "2024",
      "doi": "10.1093/bib/bbae205",
      "authors": "Zong Yuxuan et al.",
      "keywords": "ANCOM; Bias correction; Differential abundance testing; Zero-inflated models",
      "mesh_terms": "Microbiota; Humans; Algorithms; Data Analysis",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38701410/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11066923",
      "ft_text_length": 86350,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11066923)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38723025",
      "title": "Development and preliminary testing of Health Equity Across the AI Lifecycle (HEAAL): A framework for healthcare delivery organizations to mitigate the risk of AI solutions worsening health inequities.",
      "abstract": "The use of data-driven technologies such as Artificial Intelligence (AI) and Machine Learning (ML) is growing in healthcare. However, the proliferation of healthcare AI tools has outpaced regulatory frameworks, accountability measures, and governance standards to ensure safe, effective, and equitable use. To address these gaps and tackle a common challenge faced by healthcare delivery organizations, a case-based workshop was organized, and a framework was developed to evaluate the potential impact of implementing an AI solution on health equity. The Health Equity Across the AI Lifecycle (HEAAL) is co-designed with extensive engagement of clinical, operational, technical, and regulatory leaders across healthcare delivery organizations and ecosystem partners in the US. It assesses 5 equity assessment domains-accountability, fairness, fitness for purpose, reliability and validity, and transparency-across the span of eight key decision points in the AI adoption lifecycle. It is a process-oriented framework containing 37 step-by-step procedures for evaluating an existing AI solution and 34 procedures for evaluating a new AI solution in total. Within each procedure, it identifies relevant key stakeholders and data sources used to conduct the procedure. HEAAL guides how healthcare delivery organizations may mitigate the potential risk of AI solutions worsening health inequities. It also informs how much resources and support are required to assess the potential impact of AI solutions on health inequities.",
      "journal": "PLOS digital health",
      "year": "2024",
      "doi": "10.1371/journal.pdig.0000390",
      "authors": "Kim Jee Young et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38723025/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11081364",
      "ft_text_length": 44755,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11081364)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38725085",
      "title": "Preoperative prediction model for risk of readmission after total joint replacement surgery: a random forest approach leveraging NLP and unfairness mitigation for improved patient care and cost-effectiveness.",
      "abstract": "BACKGROUND: The Center for Medicare and Medicaid Services (CMS) imposes payment penalties for readmissions following total joint replacement surgeries. This study focuses on total hip, knee, and shoulder arthroplasty procedures as they account for most joint replacement surgeries. Apart from being a burden to healthcare systems, readmissions are also troublesome for patients. There are several studies which only utilized structured data from Electronic Health Records (EHR) without considering any gender and payor bias adjustments. METHODS: For this study, dataset of 38,581 total knee, hip, and shoulder replacement surgeries performed from 2015 to 2021 at Novant Health was gathered. This data was used to train a random forest machine learning model to predict the combined endpoint of emergency department (ED) visit or unplanned readmissions within 30 days of discharge or discharge to Skilled Nursing Facility (SNF) following the surgery. 98 features of laboratory results, diagnoses, vitals, medications, and utilization history were extracted. A natural language processing (NLP) model finetuned from Clinical BERT was used to generate an NLP risk score feature for each patient based on their clinical notes. To address societal biases, a feature bias analysis was performed in conjunction with propensity score matching. A threshold optimization algorithm from the Fairlearn toolkit was used to mitigate gender and payor biases to promote fairness in predictions. RESULTS: The model achieved an Area Under the Receiver Operating characteristic Curve (AUROC) of 0.738 (95% confidence interval, 0.724 to 0.754) and an Area Under the Precision-Recall Curve (AUPRC) of 0.406 (95% confidence interval, 0.384 to 0.433). Considering an outcome prevalence of 16%, these metrics indicate the model's ability to accurately discriminate between readmission and non-readmission cases within the context of total arthroplasty surgeries while adjusting patient scores in the model to mitigate bias based on patient gender and payor. CONCLUSION: This work culminated in a model that identifies the most predictive and protective features associated with the combined endpoint. This model serves as a tool to empower healthcare providers to proactively intervene based on these influential factors without introducing bias towards protected patient classes, effectively mitigating the risk of negative outcomes and ultimately improving quality of care regardless of socioeconomic factors.",
      "journal": "Journal of orthopaedic surgery and research",
      "year": "2024",
      "doi": "10.1186/s13018-024-04774-0",
      "authors": "Digumarthi Varun et al.",
      "keywords": "Classification; Fairlearn; Natural language processing; Orthopedic; Predictive model",
      "mesh_terms": "Humans; Patient Readmission; Female; Male; Machine Learning; Aged; Cost-Benefit Analysis; Natural Language Processing; Middle Aged; Arthroplasty, Replacement, Knee; Arthroplasty, Replacement, Hip; Arthroplasty, Replacement; Risk Assessment; Preoperative Period; Aged, 80 and over; Quality Improvement; Random Forest",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38725085/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11084055",
      "ft_text_length": 48271,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11084055)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "38782170",
      "title": "Causal fairness assessment of treatment allocation with electronic health records.",
      "abstract": "OBJECTIVE: Healthcare continues to grapple with the persistent issue of treatment disparities, sparking concerns regarding the equitable allocation of treatments in clinical practice. While various fairness metrics have emerged to assess fairness in decision-making processes, a growing focus has been on causality-based fairness concepts due to their capacity to mitigate confounding effects and reason about bias. However, the application of causal fairness notions in evaluating the fairness of clinical decision-making with electronic health record (EHR) data remains an understudied domain. This study aims to address the methodological gap in assessing causal fairness of treatment allocation with electronic health records data. In addition, we investigate the impact of social determinants of health on the assessment of causal fairness of treatment allocation. METHODS: We propose a causal fairness algorithm to assess fairness in clinical decision-making. Our algorithm accounts for the heterogeneity of patient populations and identifies potential unfairness in treatment allocation by conditioning on patients who have the same likelihood to benefit from the treatment. We apply this framework to a patient cohort with coronary artery disease derived from an EHR database to evaluate the fairness of treatment decisions. RESULTS: Our analysis reveals notable disparities in coronary artery bypass grafting (CABG) allocation among different patient groups. Women were found to be 4.4%-7.7% less likely to receive CABG than men in two out of four treatment response strata. Similarly, Black or African American patients were 5.4%-8.7% less likely to receive CABG than others in three out of four response strata. These results were similar when social determinants of health (insurance and area deprivation index) were dropped from the algorithm. These findings highlight the presence of disparities in treatment allocation among similar patients, suggesting potential unfairness in the clinical decision-making process. CONCLUSION: This study introduces a novel approach for assessing the fairness of treatment allocation in healthcare. By incorporating responses to treatment into fairness framework, our method explores the potential of quantifying fairness from a causal perspective using EHR data. Our research advances the methodological development of fairness assessment in healthcare and highlight the importance of causality in determining treatment fairness.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104656",
      "authors": "Zhang Linying et al.",
      "keywords": "Causal fairness; Electronic health record; Health equity; Machine learning; Principal fairness",
      "mesh_terms": "Humans; Electronic Health Records; Algorithms; Male; Female; Clinical Decision-Making; Coronary Artery Disease; Healthcare Disparities; Middle Aged; Social Determinants of Health; Causality",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38782170/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11180553",
      "ft_text_length": 33535,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11180553)",
      "ft_reason": "Included: bias central + approach content (7 indicators)"
    },
    {
      "pmid": "38785733",
      "title": "Parkinson's Disease Recognition Using Decorrelated Convolutional Neural Networks: Addressing Imbalance and Scanner Bias in rs-fMRI Data.",
      "abstract": "Parkinson's disease (PD) is a neurodegenerative and progressive disease that impacts the nerve cells in the brain and varies from person to person. The exact cause of PD is still unknown, and the diagnosis of PD does not include a specific objective test with certainty. Although deep learning has made great progress in medical neuroimaging analysis, these methods are very susceptible to biases present in neuroimaging datasets. An innovative decorrelated deep learning technique is introduced to mitigate class bias and scanner bias while simultaneously focusing on finding distinguishing characteristics in resting-state functional MRI (rs-fMRI) data, which assists in recognizing PD with good accuracy. The decorrelation function reduces the nonlinear correlation between features and bias in order to learn bias-invariant features. The publicly available Parkinson's Progression Markers Initiative (PPMI) dataset, referred to as a single-scanner imbalanced dataset in this study, was used to validate our method. The imbalanced dataset problem affects the performance of the deep learning framework by overfitting to the majority class. To resolve this problem, we propose a new decorrelated convolutional neural network (DcCNN) framework by applying decorrelation-based optimization to convolutional neural networks (CNNs). An analysis of evaluation metrics comparisons shows that integrating the decorrelation function boosts the performance of PD recognition by removing class bias. Specifically, our DcCNN models perform significantly better than existing traditional approaches to tackle the imbalance problem. Finally, the same framework can be extended to create scanner-invariant features without significantly impacting the performance of a model. The obtained dataset is a multiscanner dataset, which leads to scanner bias due to the differences in acquisition protocols and scanners. The multiscanner dataset is a combination of two publicly available datasets, namely, PPMI and FTLDNI-the frontotemporal lobar degeneration neuroimaging initiative (NIFD) dataset. The results of t-distributed stochastic neighbor embedding (t-SNE) and scanner classification accuracy of our proposed feature extraction-DcCNN (FE-DcCNN) model validated the effective removal of scanner bias. Our method achieves an average accuracy of 77.80% on a multiscanner dataset for differentiating PD from a healthy control, which is superior to the DcCNN model trained on a single-scanner imbalanced dataset.",
      "journal": "Biosensors",
      "year": "2024",
      "doi": "10.3390/bios14050259",
      "authors": "Patil Pranita et al.",
      "keywords": "DcCNN; FE-DcCNN; Parkinson\u2019s disease; class bias; decorrelation; deep learning; invariant features; rs-fMRI image; scanner bias",
      "mesh_terms": "Parkinson Disease; Humans; Magnetic Resonance Imaging; Neural Networks, Computer; Deep Learning; Brain; Image Processing, Computer-Assisted; Neuroimaging",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38785733/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11117585",
      "ft_text_length": 59157,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11117585)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38791736",
      "title": "Tailoring Household Disaster Preparedness Interventions to Reduce Health Disparities: Nursing Implications from Machine Learning Importance Features from the 2018-2020 FEMA National Household Survey.",
      "abstract": "Tailored disaster preparedness interventions may be more effective and equitable, yet little is known about specific factors associated with disaster household preparedness for older adults and/or those with African American/Black identities. This study aims to ascertain differences in the importance features of machine learning models of household disaster preparedness for four groups to inform culturally tailored intervention recommendations for nursing practice. A machine learning model was developed and tested by combining data from the 2018, 2019, and 2020 Federal Emergency Management Agency National Household Survey. The primary outcome variable was a composite readiness score. A total of 252 variables from 15,048 participants were included. Over 10% of the sample self-identified as African American/Black and 30.3% reported being 65 years of age or older. Importance features varied regarding financial and insurance preparedness, information seeking and transportation between groups. These results reiterate the need for targeted interventions to support financial resilience and equitable resource access. Notably, older adults with Black racial identities were the only group where TV, TV news, and the Weather Channel was a priority feature for household disaster preparedness. Additionally, reliance on public transportation was most important among older adults with Black racial identities, highlighting priority needs for equity in disaster preparedness and policy.",
      "journal": "International journal of environmental research and public health",
      "year": "2024",
      "doi": "10.3390/ijerph21050521",
      "authors": "Shukla Meghna et al.",
      "keywords": "disaster preparedness; disasters; health disparities; machine learning",
      "mesh_terms": "Humans; Machine Learning; Aged; Male; Middle Aged; Female; Disaster Planning; Adult; Surveys and Questionnaires; Family Characteristics; Black or African American; Young Adult; Adolescent; United States; Health Status Disparities; Civil Defense",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38791736/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11121406",
      "ft_text_length": 48471,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11121406)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38814939",
      "title": "Identifying bias in models that detect vocal fold paralysis from audio recordings using explainable machine learning and clinician ratings.",
      "abstract": "Detecting voice disorders from voice recordings could allow for frequent, remote, and low-cost screening before costly clinical visits and a more invasive laryngoscopy examination. Our goals were to detect unilateral vocal fold paralysis (UVFP) from voice recordings using machine learning, to identify which acoustic variables were important for prediction to increase trust, and to determine model performance relative to clinician performance. Patients with confirmed UVFP through endoscopic examination (N = 77) and controls with normal voices matched for age and sex (N = 77) were included. Voice samples were elicited by reading the Rainbow Passage and sustaining phonation of the vowel \"a\". Four machine learning models of differing complexity were used. SHapley Additive exPlanations (SHAP) was used to identify important features. The highest median bootstrapped ROC AUC score was 0.87 and beat clinician's performance (range: 0.74-0.81) based on the recordings. Recording durations were different between UVFP recordings and controls due to how that data was originally processed when storing, which we can show can classify both groups. And counterintuitively, many UVFP recordings had higher intensity than controls, when UVFP patients tend to have weaker voices, revealing a dataset-specific bias which we mitigate in an additional analysis. We demonstrate that recording biases in audio duration and intensity created dataset-specific differences between patients and controls, which models used to improve classification. Furthermore, clinician's ratings provide further evidence that patients were over-projecting their voices and being recorded at a higher amplitude signal than controls. Interestingly, after matching audio duration and removing variables associated with intensity in order to mitigate the biases, the models were able to achieve a similar high performance. We provide a set of recommendations to avoid bias when building and evaluating machine learning models for screening in laryngology.",
      "journal": "PLOS digital health",
      "year": "2024",
      "doi": "10.1371/journal.pdig.0000516",
      "authors": "Low Daniel M et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38814939/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11139298",
      "ft_text_length": 65111,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11139298)",
      "ft_reason": "Included: bias central + approach content (9 indicators)"
    },
    {
      "pmid": "38827048",
      "title": "Measuring and Reducing Racial Bias in a Pediatric Urinary Tract Infection Model.",
      "abstract": "Clinical predictive models that include race as a predictor have the potential to exacerbate disparities in healthcare. Such models can be respecified to exclude race or optimized to reduce racial bias. We investigated the impact of such respecifications in a predictive model\u00a0- UTICalc\u00a0- which was designed to reduce catheterizations in young children with suspected urinary tract infections. To reduce racial bias, race was removed from the UTICalc logistic regression model and replaced with two new features. We compared the two versions of UTICalc using fairness and predictive performance metrics to understand the effects on racial bias. In addition, we derived three new models for UTICalc to specifically improve racial fairness. Our results show that, as predicted by previously described impossibility results, fairness cannot be simultaneously improved on all fairness metrics, and model respecification may improve racial fairness but decrease overall predictive performance.",
      "journal": "AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science",
      "year": "2024",
      "doi": "",
      "authors": "Anderson Joshua W et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38827048/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11141814",
      "ft_text_length": 988,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11141814)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38827050",
      "title": "FERI: A Multitask-based Fairness Achieving Algorithm with Applications to Fair Organ Transplantation.",
      "abstract": "Liver transplantation often faces fairness challenges across subgroups defined by sensitive attributes such as age group, gender, and race/ethnicity. Machine learning models for outcome prediction can introduce additional biases. Therefore, we introduce Fairness through the Equitable Rate of Improvement in Multitask Learning (FERI) algorithm for fair predictions of graft failure risk in liver transplant patients. FERI constrains subgroup loss by balancing learning rates and preventing subgroup dominance in the training process. Our results show that FERI maintained high predictive accuracy with AUROC and AUPRC comparable to baseline models. More importantly, FERI demonstrated an ability to improve fairness without sacrificing accuracy. Specifically, for the gender, FERI reduced the demographic parity disparity by 71.74%, and for the age group, it decreased the equalized odds disparity by 40.46%. Therefore, the FERI algorithm advanced fairness-aware predictive modeling in healthcare and provides an invaluable tool for equitable healthcare systems.",
      "journal": "AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science",
      "year": "2024",
      "doi": "10.1101/2023.08.04.551906",
      "authors": "Li Can et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38827050/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11141863",
      "ft_text_length": 1070,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11141863)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "38827072",
      "title": "PFERM: A Fair Empirical Risk Minimization Approach with Prior Knowledge.",
      "abstract": "Fairness is crucial in machine learning to prevent bias based on sensitive attributes in classifier predictions. However, the pursuit of strict fairness often sacrifices accuracy, particularly when significant prevalence disparities exist among groups, making classifiers less practical. For example, Alzheimer's disease (AD) is more prevalent in women than men, making equal treatment inequitable for females. Accounting for prevalence ratios among groups is essential for fair decision-making. In this paper, we introduce prior knowledge for fairness, which incorporates prevalence ratio information into the fairness constraint within the Empirical Risk Minimization (ERM) framework. We develop the Prior-knowledge-guided Fair ERM (PFERM) framework, aiming to minimize expected risk within a specified function class while adhering to a prior-knowledge-guided fairness constraint. This approach strikes a flexible balance between accuracy and fairness. Empirical results confirm its effectiveness in preserving fairness without compromising accuracy.",
      "journal": "AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science",
      "year": "2024",
      "doi": "",
      "authors": "Hou Bojian et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38827072/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11141835",
      "ft_text_length": 1053,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11141835)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38834557",
      "title": "Fairer AI in ophthalmology via implicit fairness learning for mitigating sexism and ageism.",
      "abstract": "The transformative role of artificial intelligence (AI) in various fields highlights the need for it to be both accurate and fair. Biased medical AI systems pose significant potential risks to achieving fair and equitable healthcare. Here, we show an implicit fairness learning approach to build a fairer ophthalmology AI (called FairerOPTH) that mitigates sex (biological attribute) and age biases in AI diagnosis of eye diseases. Specifically, FairerOPTH incorporates the causal relationship between fundus features and eye diseases, which is relatively independent of sensitive attributes such as race, sex, and age. We demonstrate on a large and diverse collected dataset that FairerOPTH significantly outperforms several state-of-the-art approaches in terms of diagnostic accuracy and fairness for 38 eye diseases in ultra-widefield imaging and 16 eye diseases in narrow-angle imaging. This work demonstrates the significant potential of implicit fairness learning in promoting equitable treatment for patients regardless of their sex or age.",
      "journal": "Nature communications",
      "year": "2024",
      "doi": "10.1038/s41467-024-48972-0",
      "authors": "Tan Weimin et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Female; Male; Ophthalmology; Sexism; Ageism; Eye Diseases; Middle Aged; Adult; Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38834557/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11150422",
      "ft_text_length": 53560,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11150422)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "38844546",
      "title": "Assessing calibration and bias of a deployed machine learning malnutrition prediction model within a large healthcare system.",
      "abstract": "Malnutrition is a frequently underdiagnosed condition leading to increased morbidity, mortality, and healthcare costs. The Mount Sinai Health System (MSHS) deployed a machine learning model (MUST-Plus) to detect malnutrition upon hospital admission. However, in diverse patient groups, a poorly calibrated model may lead to misdiagnosis, exacerbating health care disparities. We explored the model's calibration across different variables and methods to improve calibration. Data from adult patients admitted to five MSHS hospitals from January 1, 2021 - December 31, 2022, were analyzed. We compared MUST-Plus prediction to the registered dietitian's formal assessment. Hierarchical calibration was assessed and compared between the recalibration sample (N\u2009=\u200949,562) of patients admitted between January 1, 2021 - December 31, 2022, and the hold-out sample (N\u2009=\u200917,278) of patients admitted between January 1, 2023 - September 30, 2023. Statistical differences in calibration metrics were tested using bootstrapping with replacement. Before recalibration, the overall model calibration intercept was -1.17 (95% CI: -1.20, -1.14), slope was 1.37 (95% CI: 1.34, 1.40), and Brier score was 0.26 (95% CI: 0.25, 0.26). Both weak and moderate measures of calibration were significantly different between White and Black patients and between male and female patients. Logistic recalibration significantly improved calibration of the model across race and gender in the hold-out sample. The original MUST-Plus model showed significant differences in calibration between White vs. Black patients. It also overestimated malnutrition in females compared to males. Logistic recalibration effectively reduced miscalibration across all patient subgroups. Continual monitoring and timely recalibration can improve model accuracy.",
      "journal": "NPJ digital medicine",
      "year": "2024",
      "doi": "10.1038/s41746-024-01141-5",
      "authors": "Liou Lathan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38844546/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11156633",
      "ft_text_length": 26894,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11156633)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38858466",
      "title": "Mitigating machine learning bias between high income and low-middle income countries for enhanced model fairness and generalizability.",
      "abstract": "Collaborative efforts in artificial intelligence (AI) are increasingly common between high-income countries (HICs) and low- to middle-income countries (LMICs). Given the resource limitations often encountered by LMICs, collaboration becomes crucial for pooling resources, expertise, and knowledge. Despite the apparent advantages, ensuring the fairness and equity of these collaborative models is essential, especially considering the distinct differences between LMIC and HIC hospitals. In this study, we show that collaborative AI approaches can lead to divergent performance outcomes across HIC and LMIC settings, particularly in the presence of data imbalances. Through a real-world COVID-19 screening case study, we demonstrate that implementing algorithmic-level bias mitigation methods significantly improves outcome fairness between HIC and LMIC sites while maintaining high diagnostic sensitivity. We compare our results against previous benchmarks, utilizing datasets from four independent United Kingdom Hospitals and one Vietnamese hospital, representing HIC and LMIC settings, respectively.",
      "journal": "Scientific reports",
      "year": "2024",
      "doi": "10.1038/s41598-024-64210-5",
      "authors": "Yang Jenny et al.",
      "keywords": "",
      "mesh_terms": "Humans; COVID-19; Machine Learning; Developing Countries; Developed Countries; SARS-CoV-2; United Kingdom; Bias; Vietnam; Income; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38858466/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11164855",
      "ft_text_length": 65434,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11164855)",
      "ft_reason": "Included: bias central + approach content (10 indicators)"
    },
    {
      "pmid": "38875540",
      "title": "Developing Ethics and Equity Principles, Terms, and Engagement Tools to Advance Health Equity and Researcher Diversity in AI and Machine Learning: Modified Delphi Approach.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) and machine learning (ML) technology design and development continues to be rapid, despite major limitations in its current form as a practice and discipline to address all sociohumanitarian issues and complexities. From these limitations emerges an imperative to strengthen AI and ML literacy in underserved communities and build a more diverse AI and ML design and development workforce engaged in health research. OBJECTIVE: AI and ML has the potential to account for and assess a variety of factors that contribute to health and disease and to improve prevention, diagnosis, and therapy. Here, we describe recent activities within the Artificial Intelligence/Machine Learning Consortium to Advance Health Equity and Researcher Diversity (AIM-AHEAD) Ethics and Equity Workgroup (EEWG) that led to the development of deliverables that will help put ethics and fairness at the forefront of AI and ML applications to build equity in biomedical research, education, and health care. METHODS: The AIM-AHEAD EEWG was created in 2021 with 3 cochairs and 51 members in year 1 and 2 cochairs and ~40 members in year 2. Members in both years included AIM-AHEAD principal investigators, coinvestigators, leadership fellows, and research fellows. The EEWG used a modified Delphi approach using polling, ranking, and other exercises to facilitate discussions around tangible steps, key terms, and definitions needed to ensure that ethics and fairness are at the forefront of AI and ML applications to build equity in biomedical research, education, and health care. RESULTS: The EEWG developed a set of ethics and equity principles, a glossary, and an interview guide. The ethics and equity principles comprise 5 core principles, each with subparts, which articulate best practices for working with stakeholders from historically and presently underrepresented communities. The glossary contains 12 terms and definitions, with particular emphasis on optimal development, refinement, and implementation of AI and ML in health equity research. To accompany the glossary, the EEWG developed a concept relationship diagram that describes the logical flow of and relationship between the definitional concepts. Lastly, the interview guide provides questions that can be used or adapted to garner stakeholder and community perspectives on the principles and glossary. CONCLUSIONS: Ongoing engagement is needed around our principles and glossary to identify and predict potential limitations in their uses in AI and ML research settings, especially for institutions with limited resources. This requires time, careful consideration, and honest discussions around what classifies an engagement incentive as meaningful to support and sustain their full engagement. By slowing down to meet historically and presently underresourced institutions and communities where they are and where they are capable of engaging and competing, there is higher potential to achieve needed diversity, ethics, and equity in AI and ML implementation in health research.",
      "journal": "JMIR AI",
      "year": "2023",
      "doi": "10.2196/52888",
      "authors": "Hendricks-Sturrup Rachele et al.",
      "keywords": "AI; Delphi; ML; artificial intelligence; disparities; disparity; engagement; equitable; equities; equity; ethic; ethical; ethics; fair; fairness; health disparities; health equity; humanitarian; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38875540/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11041493",
      "ft_text_length": 36123,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11041493)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38876453",
      "title": "Assessing fairness in machine learning models: A study of racial bias using matched counterparts in mortality prediction for patients with chronic diseases.",
      "abstract": "OBJECTIVE: Existing approaches to fairness evaluation often overlook systematic differences in the social determinants of health, like demographics and socioeconomics, among comparison groups, potentially leading to inaccurate or even contradictory conclusions. This study aims to evaluate racial disparities in predicting mortality among patients with chronic diseases using a fairness detection method that considers systematic differences. METHODS: We created five datasets from Mass General Brigham's electronic health records (EHR), each focusing on a different chronic condition: congestive heart failure (CHF), chronic kidney disease (CKD), chronic obstructive pulmonary disease (COPD), chronic liver disease (CLD), and dementia. For each dataset, we developed separate machine learning models to predict 1-year mortality and examined racial disparities by comparing prediction performances between Black and White individuals. We compared racial fairness evaluation between the overall Black and White individuals versus their counterparts who were Black and matched White individuals identified by propensity score matching, where the systematic differences were mitigated. RESULTS: We identified significant differences between Black and White individuals in age, gender, marital status, education level, smoking status, health insurance type, body mass index, and Charlson comorbidity index (p-value\u00a0<\u00a00.001). When examining matched Black and White subpopulations identified through propensity score matching, significant differences between particular covariates existed. We observed weaker significance levels in the CHF cohort for insurance type (p\u00a0=\u00a00.043), in the CKD cohort for insurance type (p\u00a0=\u00a00.005) and education level (p\u00a0=\u00a00.016), and in the dementia cohort for body mass index (p\u00a0=\u00a00.041); with no significant differences for other covariates. When examining mortality prediction models across the five study cohorts, we conducted a comparison of fairness evaluations before and after mitigating systematic differences. We revealed significant differences in the CHF cohort with p-values of 0.021 and 0.001 in terms of F1 measure and Sensitivity for the AdaBoost model, and p-values of 0.014 and 0.003 in terms of F1 measure and Sensitivity for the MLP model, respectively. DISCUSSION AND CONCLUSION: This study contributes to research on fairness assessment by focusing on the examination of systematic disparities and underscores the potential for revealing racial bias in machine learning models used in clinical settings.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104677",
      "authors": "Wang Yifei et al.",
      "keywords": "Chronic Disease; Electronic Health Records; Fairness Analysis; Machine Learning; Mortality Prediction; Racism",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Black or African American; Chronic Disease; Electronic Health Records; Heart Failure; Machine Learning; Pulmonary Disease, Chronic Obstructive; Racism; White; Health Status Disparities",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38876453/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11272432",
      "ft_text_length": 2554,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11272432)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "38914859",
      "title": "Deep learning-based prediction of one-year mortality in Finland is an accurate but unfair aging marker.",
      "abstract": "Short-term mortality risk, which is indicative of individual frailty, serves as a marker for aging. Previous age clocks focused on predicting either chronological age or longer-term mortality. Aging clocks predicting short-term mortality are lacking and their algorithmic fairness remains unexamined. We developed a deep learning model to predict 1-year mortality using nationwide longitudinal data from the Finnish population (FinRegistry; n\u2009=\u20095.4 million), incorporating more than 8,000 features spanning up to 50 years. We achieved an area under the curve (AUC) of 0.944, outperforming a baseline model that included only age and sex (AUC\u2009=\u20090.897). The model generalized well to different causes of death (AUC\u2009>\u20090.800 for 45 of 50 causes), including coronavirus disease 2019, which was absent in the training data. Performance varied among demographics, with young females exhibiting the best and older males the worst results. Extensive prediction fairness analyses highlighted disparities among disadvantaged groups, posing challenges to equitable integration into public health interventions. Our model accurately identified short-term mortality risk, potentially serving as a population-wide aging marker.",
      "journal": "Nature aging",
      "year": "2024",
      "doi": "10.1038/s43587-024-00657-5",
      "authors": "Vabalas Andrius et al.",
      "keywords": "",
      "mesh_terms": "Humans; Deep Learning; Finland; Male; Female; Middle Aged; Aged; Mortality; Aging; Adult; Aged, 80 and over; COVID-19; Young Adult; Frailty; Adolescent",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38914859/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11257968",
      "ft_text_length": 55654,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11257968)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "38918281",
      "title": "Background removal for debiasing computer-aided cytological diagnosis.",
      "abstract": "To address the background-bias problem in computer-aided cytology caused by microscopic slide deterioration, this article proposes a deep learning approach for cell segmentation and background removal without requiring cell annotation. A U-Net-based model was trained to separate cells from the background in an unsupervised manner by leveraging the redundancy of the background and the sparsity of cells in liquid-based cytology (LBC) images. The experimental results demonstrate that the U-Net-based model trained on a small set of cytology images can exclude background features and accurately segment cells. This capability is beneficial for debiasing in the detection and classification of the cells of interest in oral LBC. Slide deterioration can significantly affect deep learning-based cell classification. Our proposed method effectively removes background features at no cost of cell annotation, thereby enabling accurate cytological diagnosis through the deep learning of microscopic slide images.",
      "journal": "International journal of computer assisted radiology and surgery",
      "year": "2024",
      "doi": "10.1007/s11548-024-03169-0",
      "authors": "Takeda Keita et al.",
      "keywords": "Data cleaning; Deep learning; Oral cytology; Robust principal component analysis; U-Net",
      "mesh_terms": "Humans; Deep Learning; Diagnosis, Computer-Assisted; Cytodiagnosis; Microscopy; Image Interpretation, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38918281/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11541310",
      "ft_text_length": 59226,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11541310)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38929638",
      "title": "The Sociodemographic Biases in Machine Learning Algorithms: A Biomedical Informatics Perspective.",
      "abstract": "Artificial intelligence models represented in machine learning algorithms are promising tools for risk assessment used to guide clinical and other health care decisions. Machine learning algorithms, however, may house biases that propagate stereotypes, inequities, and discrimination that contribute to socioeconomic health care disparities. The biases include those related to some sociodemographic characteristics such as race, ethnicity, gender, age, insurance, and socioeconomic status from the use of erroneous electronic health record data. Additionally, there is concern that training data and algorithmic biases in large language models pose potential drawbacks. These biases affect the lives and livelihoods of a significant percentage of the population in the United States and globally. The social and economic consequences of the associated backlash cannot be underestimated. Here, we outline some of the sociodemographic, training data, and algorithmic biases that undermine sound health care risk assessment and medical decision-making that should be addressed in the health care system. We present a perspective and overview of these biases by gender, race, ethnicity, age, historically marginalized communities, algorithmic bias, biased evaluations, implicit bias, selection/sampling bias, socioeconomic status biases, biased data distributions, cultural biases and insurance status bias, conformation bias, information bias and anchoring biases and make recommendations to improve large language model training data, including de-biasing techniques such as counterfactual role-reversed sentences during knowledge distillation, fine-tuning, prefix attachment at training time, the use of toxicity classifiers, retrieval augmented generation and algorithmic modification to mitigate the biases moving forward.",
      "journal": "Life (Basel, Switzerland)",
      "year": "2024",
      "doi": "10.3390/life14060652",
      "authors": "Franklin Gillian et al.",
      "keywords": "algorithms; artificial intelligence; bias; biomedical informatics; electronic health records; health care; machine learning; models; sociodemographic",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38929638/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11204917",
      "ft_text_length": 38096,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11204917)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "38942737",
      "title": "Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging.",
      "abstract": "OBJECTIVE: Artificial intelligence (AI) models trained using medical images for clinical tasks often exhibit bias in the form of subgroup performance disparities. However, since not all sources of bias in real-world medical imaging data are easily identifiable, it is challenging to comprehensively assess their impacts. In this article, we introduce an analysis framework for systematically and objectively investigating the impact of biases in medical images on AI models. MATERIALS AND METHODS: Our framework utilizes synthetic neuroimages with known disease effects and sources of bias. We evaluated the impact of bias effects and the efficacy of 3 bias mitigation strategies in counterfactual data scenarios on a convolutional neural network (CNN) classifier. RESULTS: The analysis revealed that training a CNN model on the datasets containing bias effects resulted in expected subgroup performance disparities. Moreover, reweighing was the most successful bias mitigation strategy for this setup. Finally, we demonstrated that explainable AI methods can aid in investigating the manifestation of bias in the model using this framework. DISCUSSION: The value of this framework is showcased in our findings on the impact of bias scenarios and efficacy of bias mitigation in a deep learning model pipeline. This systematic analysis can be easily expanded to conduct further controlled in silico trials in other investigations of bias in medical imaging AI. CONCLUSION: Our novel methodology for objectively studying bias in medical imaging AI can help support the development of clinical decision-support tools that are robust and responsible.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2024",
      "doi": "10.1093/jamia/ocae165",
      "authors": "Stanley Emma A M et al.",
      "keywords": "algorithmic bias; artificial intelligence; bias mitigation; synthetic data",
      "mesh_terms": "Humans; Artificial Intelligence; Bias; Neural Networks, Computer; Diagnostic Imaging; Neuroimaging",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38942737/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11491635",
      "ft_text_length": 31599,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11491635)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "38947177",
      "title": "Unbiasing Fairness Evaluation of Radiology AI Model.",
      "abstract": "Fairness of artificial intelligence and machine learning models, often caused by imbalanced datasets, has long been a concern. While many efforts aim to minimize model bias, this study suggests that traditional fairness evaluation methods may be biased, highlighting the need for a proper evaluation scheme with multiple evaluation metrics due to varying results under different criteria. Moreover, the limited data size of minority groups introduces significant data uncertainty, which can undermine the judgement of fairness. This paper introduces an innovative evaluation approach that estimates data uncertainty in minority groups through bootstrapping from majority groups for a more objective statistical assessment. Extensive experiments reveal that traditional evaluation methods might have drawn inaccurate conclusions about model fairness. The proposed method delivers an unbiased fairness assessment by adeptly addressing the inherent complications of model evaluation on imbalanced datasets. The results show that such comprehensive evaluation can provide more confidence when adopting those models.",
      "journal": "Meta-radiology",
      "year": "2024",
      "doi": "10.1016/j.metrad.2024.100084",
      "authors": "Liang Yuxuan et al.",
      "keywords": "Data uncertainty; Deep learning; Evaluation metrics; Fairness; Medical imaging",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38947177/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11210324",
      "ft_text_length": 1111,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11210324)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "38962581",
      "title": "Challenges in Reducing Bias Using Post-Processing Fairness for Breast Cancer Stage Classification with Deep Learning.",
      "abstract": "Breast cancer is the most common cancer affecting women globally. Despite the significant impact of deep learning models on breast cancer diagnosis and treatment, achieving fairness or equitable outcomes across diverse populations remains a challenge when some demographic groups are underrepresented in the training data. We quantified the bias of models trained to predict breast cancer stage from a dataset consisting of 1000 biopsies from 842 patients provided by AIM-Ahead (Artificial Intelligence/Machine Learning Consortium to Advance Health Equity and Researcher Diversity). Notably, the majority of data (over 70%) were from White patients. We found that prior to post-processing adjustments, all deep learning models we trained consistently performed better for White patients than for non-White patients. After model calibration, we observed mixed results, with only some models demonstrating improved performance. This work provides a case study of bias in breast cancer medical imaging models and highlights the challenges in using post-processing to attempt to achieve fairness.",
      "journal": "Algorithms",
      "year": "2024",
      "doi": "10.3390/a17040141",
      "authors": "Soltan Armin et al.",
      "keywords": "algorithmic fairness; breast cancer; deep learning; equalized odds; equalized opportunity; post-processing method",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38962581/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11221567",
      "ft_text_length": 16848,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11221567)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "38985468",
      "title": "Fairness in Predicting Cancer Mortality Across Racial Subgroups.",
      "abstract": "IMPORTANCE: Machine learning has potential to transform cancer care by helping clinicians prioritize patients for serious illness conversations. However, models need to be evaluated for unequal performance across racial groups (ie, racial bias) so that existing racial disparities are not exacerbated. OBJECTIVE: To evaluate whether racial bias exists in a predictive machine learning model that identifies 180-day cancer mortality risk among patients with solid malignant tumors. DESIGN, SETTING, AND PARTICIPANTS: In this cohort study, a machine learning model to predict cancer mortality for patients aged 21 years or older diagnosed with cancer between January 2016 and December 2021 was developed with a random forest algorithm using retrospective data from the Mount Sinai Health System cancer registry, Social Security Death Index, and electronic health records up to the date when databases were accessed for cohort extraction (February 2022). EXPOSURE: Race category. MAIN OUTCOMES AND MEASURES: The primary outcomes were model discriminatory performance (area under the receiver operating characteristic curve [AUROC], F1 score) among each race category (Asian, Black, Native American, White, and other or unknown) and fairness metrics (equal opportunity, equalized odds, and disparate impact) among each pairwise comparison of race categories. True-positive rate ratios represented equal opportunity; both true-positive and false-positive rate ratios, equalized odds; and the percentage of predictive positive rate ratios, disparate impact. All metrics were estimated as a proportion or ratio, with variability captured through 95% CIs. The prespecified criterion for the model's clinical use was a threshold of at least 80% for fairness metrics across different racial groups to ensure the model's prediction would not be biased against any specific race. RESULTS: The test validation dataset included 43\u202f274 patients with balanced demographics. Mean (SD) age was 64.09 (14.26) years, with 49.6% older than 65 years. A total of 53.3% were female; 9.5%, Asian; 18.9%, Black; 0.1%, Native American; 52.2%, White; and 19.2%, other or unknown race; 0.1% had missing race data. A total of 88.9% of patients were alive, and 11.1% were dead. The AUROCs, F1 scores, and fairness metrics maintained reasonable concordance among the racial subgroups: the AUROCs ranged from 0.75 (95% CI, 0.72-0.78) for Asian patients and 0.75 (95% CI, 0.73-0.77) for Black patients to 0.77 (95% CI, 0.75-0.79) for patients with other or unknown race; F1 scores, from 0.32 (95% CI, 0.32-0.33) for White patients to 0.40 (95% CI, 0.39-0.42) for Black patients; equal opportunity ratios, from 0.96 (95% CI, 0.95-0.98) for Black patients compared with White patients to 1.02 (95% CI, 1.00-1.04) for Black patients compared with patients with other or unknown race; equalized odds ratios, from 0.87 (95% CI, 0.85-0.92) for Black patients compared with White patients to 1.16 (1.10-1.21) for Black patients compared with patients with other or unknown race; and disparate impact ratios, from 0.86 (95% CI, 0.82-0.89) for Black patients compared with White patients to 1.17 (95% CI, 1.12-1.22) for Black patients compared with patients with other or unknown race. CONCLUSIONS AND RELEVANCE: In this cohort study, the lack of significant variation in performance or fairness metrics indicated an absence of racial bias, suggesting that the model fairly identified cancer mortality risk across racial groups. It remains essential to consistently review the model's application in clinical settings to ensure equitable patient care.",
      "journal": "JAMA network open",
      "year": "2024",
      "doi": "10.1001/jamanetworkopen.2024.21290",
      "authors": "Ganta Teja et al.",
      "keywords": "",
      "mesh_terms": "Humans; Neoplasms; Female; Male; Middle Aged; Aged; Machine Learning; Retrospective Studies; Adult; Racial Groups; Cohort Studies; Racism",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38985468/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11238025",
      "ft_text_length": 25200,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11238025)",
      "ft_reason": "Included: bias central + approach content (7 indicators)"
    },
    {
      "pmid": "38997335",
      "title": "Fair AI-powered orthopedic image segmentation: addressing bias and promoting equitable healthcare.",
      "abstract": "AI-powered segmentation of hip and knee bony anatomy has revolutionized orthopedics, transforming pre-operative planning and post-operative assessment. Despite the remarkable advancements in AI algorithms for medical imaging, the potential for biases inherent within these models remains largely unexplored. This study tackles these concerns by thoroughly re-examining AI-driven segmentation for hip and knee bony anatomy. While advanced imaging modalities like CT and MRI offer comprehensive views, plain radiographs (X-rays) predominate the standard initial clinical assessment due to their widespread availability, low cost, and rapid acquisition. Hence, we focused on plain radiographs to ensure the utilization of our contribution in diverse healthcare settings, including those with limited access to advanced imaging technologies. This work provides insights into the underlying causes of biases in AI-based knee and hip image segmentation through an extensive evaluation, presenting targeted mitigation strategies to alleviate biases related to sex, race, and age, using an automatic segmentation that is fair, impartial, and safe in the context of AI. Our contribution can enhance inclusivity, ethical practices, equity, and an unbiased healthcare environment with advanced clinical outcomes, aiding decision-making and osteoarthritis research. Furthermore, we have made all the codes and datasets publicly and freely accessible to promote open scientific research.",
      "journal": "Scientific reports",
      "year": "2024",
      "doi": "10.1038/s41598-024-66873-6",
      "authors": "Siddiqui Ismaeel A et al.",
      "keywords": "",
      "mesh_terms": "Humans; Male; Female; Artificial Intelligence; Middle Aged; Image Processing, Computer-Assisted; Bias; Knee Joint; Knee; Adult; Algorithms; Hip Joint; Magnetic Resonance Imaging; Aged; Tomography, X-Ray Computed; Orthopedics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38997335/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11245517",
      "ft_text_length": 52519,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11245517)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "39037993",
      "title": "A PROGRESS-driven approach to cognitive outcomes after traumatic brain injury: A study protocol for advancing equity, diversity, and inclusion through knowledge synthesis and mobilization.",
      "abstract": "Evidence syntheses for advancing equitable traumatic brain injury (TBI) research, policy, and practice presents formidable challenges. Research and clinical frameworks are currently not specific to equity, diversity, and inclusion considerations, despite evidence that persons with TBI live in societies in which power imbalances and systems of social dominance may privilege some people and marginalize others. The present protocol outlines a strategy for a research program, supported by the Canadian Institutes of Health Research, that explores the integration of PROGRESS-Plus parameters in research with the goal of advancing open-science databases and tools to improve our understanding of equity in cognitive and brain health outcomes in TBI. PROGRESS-Plus is a framework outlining social, economic, and cultural parameters that may influence health opportunities and outcomes (e.g., place of residence, race, occupation, gender, etc.). A multistep research program is proposed to support three objectives: (1) organizing existing data on TBI-induced changes in cognition and brain health into a template to facilitate future research, including research using machine learning techniques; (2) updating published evidence with a more rigorous approach to the consideration of PROGRESS-Plus parameters; and (3) mobilizing knowledge on the current state of evidence that is relevant, equitable, and accessible. This program facilitates partnerships with knowledge users across clinical, research, academic, and community sectors to address the three research objectives through a unifying workflow of exchange, synthesis, and knowledge mobilization. We anticipate that this global collaboration between topic experts and community leaders in equity in brain health will add significant value to the field of TBI by promoting equity-transformative advancements in knowledge synthesis, policy, and practice.",
      "journal": "PloS one",
      "year": "2024",
      "doi": "10.1371/journal.pone.0307418",
      "authors": "Sant'Ana Thaisa Tylinski et al.",
      "keywords": "",
      "mesh_terms": "Humans; Brain Injuries, Traumatic; Cognition; Canada; Health Equity; Diversity, Equity, Inclusion",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39037993/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11262676",
      "ft_text_length": 32947,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11262676)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39046989",
      "title": "Evaluating and mitigating unfairness in multimodal remote mental health assessments.",
      "abstract": "Research on automated mental health assessment tools has been growing in recent years, often aiming to address the subjectivity and bias that existed in the current clinical practice of the psychiatric evaluation process. Despite the substantial health and economic ramifications, the potential unfairness of those automated tools was understudied and required more attention. In this work, we systematically evaluated the fairness level in a multimodal remote mental health dataset and an assessment system, where we compared the fairness level in race, gender, education level, and age. Demographic parity ratio (DPR) and equalized odds ratio (EOR) of classifiers using different modalities were compared, along with the F1 scores in different demographic groups. Post-training classifier threshold optimization was employed to mitigate the unfairness. No statistically significant unfairness was found in the composition of the dataset. Varying degrees of unfairness were identified among modalities, with no single modality consistently demonstrating better fairness across all demographic variables. Post-training mitigation effectively improved both DPR and EOR metrics at the expense of a decrease in F1 scores. Addressing and mitigating unfairness in these automated tools are essential steps in fostering trust among clinicians, gaining deeper insights into their use cases, and facilitating their appropriate utilization.",
      "journal": "PLOS digital health",
      "year": "2024",
      "doi": "10.1371/journal.pdig.0000413",
      "authors": "Jiang Zifan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39046989/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11268595",
      "ft_text_length": 34222,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11268595)",
      "ft_reason": "Included: bias central + approach content (12 indicators)"
    },
    {
      "pmid": "39075127",
      "title": "Developing a fair and interpretable representation of the clock drawing test for mitigating low education and racial bias.",
      "abstract": "The clock drawing test (CDT) is a neuropsychological assessment tool to screen an individual's cognitive ability. In this study, we developed a Fair and Interpretable Representation of Clock drawing test (FaIRClocks) to evaluate and mitigate classification bias against people with less than 8\u00a0years of education, while screening their cognitive function using an array of neuropsychological measures. In this study, we represented clock drawings by a priorly published 10-dimensional deep learning feature set trained on publicly available data from the National Health and Aging Trends Study (NHATS). These embeddings were further fine-tuned with clocks from a preoperative cognitive screening program at the University of Florida to predict three cognitive scores: the Mini-Mental State Examination (MMSE) total score, an attention composite z-score (ATT-C), and a memory composite z-score (MEM-C). ATT-C and MEM-C scores were developed by averaging z-scores based on normative references. The cognitive screening classifiers were initially tested to see their relative performance in patients with low years of education (<\u2009=\u20098\u00a0years) versus patients with higher education (>\u20098\u00a0years) and race. Results indicated that the initial unweighted classifiers confounded lower education with cognitive compromise resulting in a 100% type I error rate for this group. Thereby, the samples were re-weighted using multiple fairness metrics to achieve sensitivity/specificity and positive/negative predictive value (PPV/NPV) balance across groups. In summary, we report the FaIRClocks model, with promise to help identify and mitigate bias against people with less than 8\u00a0years of education during preoperative cognitive screening.",
      "journal": "Scientific reports",
      "year": "2024",
      "doi": "10.1038/s41598-024-68481-w",
      "authors": "Zhang Jiaqing et al.",
      "keywords": "AI Fairness; Attention; Memory; Mini-mental state examination; Relevance factor variational autoencoder; Semi-supervised deep learning",
      "mesh_terms": "Humans; Male; Female; Aged; Educational Status; Racism; Neuropsychological Tests; Cognition; Cognitive Dysfunction; Aged, 80 and over; Mental Status and Dementia Tests; Middle Aged; Deep Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39075127/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11286895",
      "ft_text_length": 45232,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11286895)",
      "ft_reason": "Included: bias central + approach content (9 indicators)"
    },
    {
      "pmid": "39085344",
      "title": "A novel approach for assessing fairness in deployed machine learning algorithms.",
      "abstract": "Fairness in machine learning (ML) emerges as a critical concern as AI systems increasingly influence diverse aspects of society, from healthcare decisions to legal judgments. Many studies show evidence of unfair ML outcomes. However, the current body of literature lacks a statistically validated approach that can evaluate the fairness of a deployed ML algorithm against a dataset. A novel evaluation approach is introduced in this research based on k-fold cross-validation and statistical t-tests to assess the fairness of ML algorithms. This approach was exercised across five benchmark datasets using six classical ML algorithms. Considering four fair ML definitions guided by the current literature, our analysis showed that the same dataset generates a fair outcome for one ML algorithm but an unfair result for another. Such an observation reveals complex, context-dependent fairness issues in ML, complicated further by the varied operational mechanisms of the underlying ML models. Our proposed approach enables researchers to check whether deploying any ML algorithms against a protected attribute within datasets is fair. We also discuss the broader implications of the proposed approach, highlighting a notable variability in its fairness outcomes. Our discussion underscores the need for adaptable fairness definitions and the exploration of methods to enhance the fairness of ensemble approaches, aiming to advance fair ML practices and ensure equitable AI deployment across societal sectors.",
      "journal": "Scientific reports",
      "year": "2024",
      "doi": "10.1038/s41598-024-68651-w",
      "authors": "Uddin Shahadat et al.",
      "keywords": "Fair machine learning; Fairness; Machine learning",
      "mesh_terms": "Machine Learning; Algorithms; Humans",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39085344/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11291763",
      "ft_text_length": 37834,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11291763)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "39115640",
      "title": "Identifying sources of bias when testing three available algorithms for quantifying white matter lesions: BIANCA, LPA and LGA.",
      "abstract": "Brain magnetic resonance imaging frequently reveals white matter lesions (WMLs) in older adults. They are often associated with cognitive impairment and risk of dementia. Given the continuous search for the optimal segmentation algorithm, we broke down this question by exploring whether the output of algorithms frequently used might be biased by the presence of different influencing factors. We studied the impact of age, sex, blood glucose levels, diabetes, systolic blood pressure and hypertension on automatic WML segmentation algorithms. We evaluated three widely used algorithms (BIANCA, LPA and LGA) using the population-based 1000BRAINS cohort (N\u2009=\u20091166, aged 18-87, 523 females, 643 males). We analysed two main aspects. Firstly, we examined whether training data (TD) characteristics influenced WML estimations, assessing the impact of relevant factors in the TD. Secondly, algorithm's output and performance within selected subgroups defined by these factors were assessed. Results revealed that BIANCA's WML estimations are influenced by the characteristics present in the TD. LPA and LGA consistently provided lower WML estimations compared to BIANCA's output when tested on participants under 67 years of age without risk cardiovascular factors. Notably, LPA and LGA showed reduced accuracy for these participants. However, LPA and LGA showed better performance for older participants presenting cardiovascular risk factors. Results suggest that incorporating comprehensive cohort factors like diverse age, sex and participants with and without hypertension in the TD could enhance WML-based analyses and mitigate potential sources of bias. LPA and LGA are a fast and valid option for older participants with cardiovascular risk factors.",
      "journal": "GeroScience",
      "year": "2025",
      "doi": "10.1007/s11357-024-01306-w",
      "authors": "Miller Tatiana et al.",
      "keywords": "BIANCA; LGA; LPA; Training data characteristics; White matter lesion",
      "mesh_terms": "Humans; Female; Male; Aged; Middle Aged; Algorithms; Adult; Aged, 80 and over; Magnetic Resonance Imaging; White Matter; Adolescent; Young Adult; Bias; Risk Factors",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39115640/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11872996",
      "ft_text_length": 47581,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11872996)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39116187",
      "title": "Targeting Machine Learning and Artificial Intelligence Algorithms in Health Care to Reduce Bias and Improve Population Health.",
      "abstract": "Policy Points Artificial intelligence (AI) is disruptively innovating health care and surpassing our ability to define its boundaries and roles in health care and regulate its application in legal and ethical ways. Significant progress has been made in governance in the United States and the European Union. It is incumbent on developers, end users, the public, providers, health care systems, and policymakers to collaboratively ensure that we adopt a national AI health strategy that realizes the Quintuple Aim; minimizes race-based medicine; prioritizes transparency, equity, and algorithmic vigilance; and integrates the patient and community voices throughout all aspects of AI development and deployment.",
      "journal": "The Milbank quarterly",
      "year": "2024",
      "doi": "10.1111/1468-0009.12712",
      "authors": "Hurd Thelma C et al.",
      "keywords": "algorithmic bias; artificial intelligence; ethics; machine learning; minority health",
      "mesh_terms": "Humans; Artificial Intelligence; Machine Learning; Population Health; United States; Delivery of Health Care; Algorithms; Bias",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39116187/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11576591",
      "ft_text_length": 49778,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11576591)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "39119589",
      "title": "Deconstructing demographic bias in speech-based machine learning models for digital health.",
      "abstract": "INTRODUCTION: Machine learning (ML) algorithms have been heralded as promising solutions to the realization of assistive systems in digital healthcare, due to their ability to detect fine-grain patterns that are not easily perceived by humans. Yet, ML algorithms have also been critiqued for treating individuals differently based on their demography, thus propagating existing disparities. This paper explores gender and race bias in speech-based ML algorithms that detect behavioral and mental health outcomes. METHODS: This paper examines potential sources of bias in the data used to train the ML, encompassing acoustic features extracted from speech signals and associated labels, as well as in the ML decisions. The paper further examines approaches to reduce existing bias via using the features that are the least informative of one's demographic information as the ML input, and transforming the feature space in an adversarial manner to diminish the evidence of the demographic information while retaining information about the focal behavioral and mental health state. RESULTS: Results are presented in two domains, the first pertaining to gender and race bias when estimating levels of anxiety, and the second pertaining to gender bias in detecting depression. Findings indicate the presence of statistically significant differences in both acoustic features and labels among demographic groups, as well as differential ML performance among groups. The statistically significant differences present in the label space are partially preserved in the ML decisions. Although variations in ML performance across demographic groups were noted, results are mixed regarding the models' ability to accurately estimate healthcare outcomes for the sensitive groups. DISCUSSION: These findings underscore the necessity for careful and thoughtful design in developing ML models that are capable of maintaining crucial aspects of the data and perform effectively across all populations in digital healthcare applications.",
      "journal": "Frontiers in digital health",
      "year": "2024",
      "doi": "10.3389/fdgth.2024.1351637",
      "authors": "Yang Michael et al.",
      "keywords": "anxiety; demographic bias; depression; fairness; machine learning; speech",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39119589/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11306200",
      "ft_text_length": 59032,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11306200)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "39133537",
      "title": "Artificial Intelligence as a Potential Catalyst to a More Equitable Cancer Care.",
      "abstract": "As we enter the era of digital interdependence, artificial intelligence (AI) emerges as a key instrument to transform health care and address disparities and barriers in access to services. This viewpoint explores AI's potential to reduce inequalities in cancer care by improving diagnostic accuracy, optimizing resource allocation, and expanding access to medical care, especially in underserved communities. Despite persistent barriers, such as socioeconomic and geographical disparities, AI can significantly improve health care delivery. Key applications include AI-driven health equity monitoring, predictive analytics, mental health support, and personalized medicine. This viewpoint highlights the need for inclusive development practices and ethical considerations to ensure diverse data representation and equitable access. Emphasizing the role of AI in cancer care, especially in low- and middle-income countries, we underscore the importance of collaborative and multidisciplinary efforts to integrate AI effectively and ethically into health systems. This call to action highlights the need for further research on user experiences and the unique social, cultural, and political barriers to AI implementation in cancer care.",
      "journal": "JMIR cancer",
      "year": "2024",
      "doi": "10.2196/57276",
      "authors": "Garcia-Saiso Sebastian et al.",
      "keywords": "AI; artificial intelligence; cancer; cancer care; catalyst; change; changes; cost; costs; demographic; digital health; epidemiological; equality; health system; healthcare; mHealth; mobile health; public health",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39133537/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11347894",
      "ft_text_length": 26358,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11347894)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "39163597",
      "title": "Mitigating Sociodemographic Bias in Opioid Use Disorder Prediction: Fairness-Aware Machine Learning Framework.",
      "abstract": "BACKGROUND: Opioid use disorder (OUD) is a critical public health crisis in the United States, affecting >5.5 million Americans in 2021. Machine learning has been used to predict patient risk of incident OUD. However, little is known about the fairness and bias of these predictive models. OBJECTIVE: The aims of this study are two-fold: (1) to develop a machine learning bias mitigation algorithm for sociodemographic features and (2) to develop a fairness-aware weighted majority voting (WMV) classifier for OUD prediction. METHODS: We used the 2020 National Survey on Drug and Health data to develop a neural network (NN) model using stochastic gradient descent (SGD; NN-SGD) and an NN model using Adam (NN-Adam) optimizers and evaluated sociodemographic bias by comparing the area under the curve values. A bias mitigation algorithm, based on equality of odds, was implemented to minimize disparities in specificity and recall. Finally, a WMV classifier was developed for fairness-aware prediction of OUD. To further analyze bias detection and mitigation, we did a 1-N matching of OUD to non-OUD cases, controlling for socioeconomic variables, and evaluated the performance of the proposed bias mitigation algorithm and WMV classifier. RESULTS: Our bias mitigation algorithm substantially reduced bias with NN-SGD, by 21.66% for sex, 1.48% for race, and 21.04% for income, and with NN-Adam by 16.96% for sex, 8.87% for marital status, 8.45% for working condition, and 41.62% for race. The fairness-aware WMV classifier achieved a recall of 85.37% and 92.68% and an accuracy of 58.85% and 90.21% using NN-SGD and NN-Adam, respectively. The results after matching also indicated remarkable bias reduction with NN-SGD and NN-Adam, respectively, as follows: sex (0.14% vs 0.97%), marital status (12.95% vs 10.33%), working condition (14.79% vs 15.33%), race (60.13% vs 41.71%), and income (0.35% vs 2.21%). Moreover, the fairness-aware WMV classifier achieved high performance with a recall of 100% and 85.37% and an accuracy of 73.20% and 89.38% using NN-SGD and NN-Adam, respectively. CONCLUSIONS: The application of the proposed bias mitigation algorithm shows promise in reducing sociodemographic bias, with the WMV classifier confirming bias reduction and high performance in OUD prediction.",
      "journal": "JMIR AI",
      "year": "2024",
      "doi": "10.2196/55820",
      "authors": "Yaseliani Mohammad et al.",
      "keywords": "bias mitigation; fairness and bias; machine learning; majority voting; opioid use disorder",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39163597/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11372321",
      "ft_text_length": 61594,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11372321)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "39184970",
      "title": "Computational Simulation of Virtual Patients Reduces Dataset Bias and Improves Machine Learning-Based Detection of ARDS from Noisy Heterogeneous ICU Datasets.",
      "abstract": "Goal: Machine learning (ML) technologies that leverage large-scale patient data are promising tools predicting disease evolution in individual patients. However, the limited generalizability of ML models developed on single-center datasets, and their unproven performance in real-world settings, remain significant constraints to their widespread adoption in clinical practice. One approach to tackle this issue is to base learning on large multi-center datasets. However, such heterogeneous datasets can introduce further biases driven by data origin, as data structures and patient cohorts may differ between hospitals. Methods: In this paper, we demonstrate how mechanistic virtual patient (VP) modeling can be used to capture specific features of patients' states and dynamics, while reducing biases introduced by heterogeneous datasets. We show how VP modeling can be used for data augmentation through identification of individualized model parameters approximating disease states of patients with suspected acute respiratory distress syndrome (ARDS) from observational data of mixed origin. We compare the results of an unsupervised learning method (clustering) in two cases: where the learning is based on original patient data and on data derived in the matching procedure of the VP model to real patient data. Results: More robust cluster configurations were observed in clustering using the model-derived data. VP model-based clustering also reduced biases introduced by the inclusion of data from different hospitals and was able to discover an additional cluster with significant ARDS enrichment. Conclusions: Our results indicate that mechanistic VP modeling can be used to significantly reduce biases introduced by learning from heterogeneous datasets and to allow improved discovery of patient cohorts driven exclusively by medical conditions.",
      "journal": "IEEE open journal of engineering in medicine and biology",
      "year": "2024",
      "doi": "10.1109/OJEMB.2023.3243190",
      "authors": "Sharafutdinov Konstantin et al.",
      "keywords": "ARDS; computational simulation; dataset bias; machine learning; virtual patients",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39184970/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11342939",
      "ft_text_length": 44753,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11342939)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39186324",
      "title": "Sex-Based Performance Disparities in Machine Learning Algorithms for Cardiac Disease Prediction: Exploratory Study.",
      "abstract": "BACKGROUND: The presence of bias in artificial intelligence has garnered increased attention, with inequities in algorithmic performance being exposed across the fields of criminal justice, education, and welfare services. In health care, the inequitable performance of algorithms across demographic groups may widen health inequalities. OBJECTIVE: Here, we identify and characterize bias in cardiology algorithms, looking specifically at algorithms used in the management of heart failure. METHODS: Stage 1 involved a literature search of PubMed and Web of Science for key terms relating to cardiac machine learning (ML) algorithms. Papers that built ML models to predict cardiac disease were evaluated for their focus on demographic bias in model performance, and open-source data sets were retained for our investigation. Two open-source data sets were identified: (1) the University of California Irvine Heart Failure data set and (2) the University of California Irvine Coronary Artery Disease data set. We reproduced existing algorithms that have been reported for these data sets, tested them for sex biases in algorithm performance, and assessed a range of remediation techniques for their efficacy in reducing inequities. Particular attention was paid to the false negative rate (FNR), due to the clinical significance of underdiagnosis and missed opportunities for treatment. RESULTS: In stage 1, our literature search returned 127 papers, with 60 meeting the criteria for a full review and only 3 papers highlighting sex differences in algorithm performance. In the papers that reported sex, there was a consistent underrepresentation of female patients in the data sets. No papers investigated racial or ethnic differences. In stage 2, we reproduced algorithms reported in the literature, achieving mean accuracies of 84.24% (SD 3.51%) for data set 1 and 85.72% (SD 1.75%) for data set 2 (random forest models). For data set 1, the FNR was significantly higher for female patients in 13 out of 16 experiments, meeting the threshold of statistical significance (-17.81% to -3.37%; P<.05). A smaller disparity in the false positive rate was significant for male patients in 13 out of 16 experiments (-0.48% to +9.77%; P<.05). We observed an overprediction of disease for male patients (higher false positive rate) and an underprediction of disease for female patients (higher FNR). Sex differences in feature importance suggest that feature selection needs to be demographically tailored. CONCLUSIONS: Our research exposes a significant gap in cardiac ML research, highlighting that the underperformance of algorithms for female patients has been overlooked in the published literature. Our study quantifies sex disparities in algorithmic performance and explores several sources of bias. We found an underrepresentation of female patients in the data sets used to train algorithms, identified sex biases in model error rates, and demonstrated that a series of remediation techniques were unable to address the inequities present.",
      "journal": "Journal of medical Internet research",
      "year": "2024",
      "doi": "10.2196/46936",
      "authors": "Straw Isabel et al.",
      "keywords": "artificial intelligence; cardiac; cardiac disease; cardiology; health care; health equity; heart failure; inequality; machine learning; management; medicine; performance; quantitative evaluation; sex",
      "mesh_terms": "Humans; Machine Learning; Female; Male; Algorithms; Heart Diseases; Sex Factors",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39186324/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11384168",
      "ft_text_length": 41608,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11384168)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "39198519",
      "title": "Acquisition parameters influence AI recognition of race in chest x-rays and mitigating these factors reduces underdiagnosis bias.",
      "abstract": "A core motivation for the use of artificial intelligence (AI) in medicine is to reduce existing healthcare disparities. Yet, recent studies have demonstrated two distinct findings: (1) AI models can show performance biases in underserved populations, and (2) these same models can be directly trained to recognize patient demographics, such as predicting self-reported race from medical images alone. Here, we investigate how these findings may be related, with an end goal of reducing a previously identified underdiagnosis bias. Using two popular chest x-ray datasets, we first demonstrate that technical parameters related to image acquisition and processing influence AI models trained to predict patient race, where these results partly reflect underlying biases in the original clinical datasets. We then find that mitigating the observed differences through a demographics-independent calibration strategy reduces the previously identified bias. While many factors likely contribute to AI bias and demographics prediction, these results highlight the importance of carefully considering data acquisition and processing parameters in AI development and healthcare equity more broadly.",
      "journal": "Nature communications",
      "year": "2024",
      "doi": "10.1038/s41467-024-52003-3",
      "authors": "Lotter William",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Bias; Radiography, Thoracic; Male; Female; Racial Groups; Healthcare Disparities; Middle Aged; Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39198519/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11358468",
      "ft_text_length": 57246,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11358468)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "39242810",
      "title": "MyThisYourThat for interpretable identification of systematic bias in federated learning for biomedical images.",
      "abstract": "Distributed collaborative learning is a promising approach for building predictive models for privacy-sensitive biomedical images. Here, several data owners (clients) train a joint model without sharing their original data. However, concealed systematic biases can compromise model performance and fairness. This study presents MyThisYourThat (MyTH) approach, which adapts an interpretable prototypical part learning network to a distributed setting, enabling each client to visualize feature differences learned by others on their own image: comparing one client's 'This' with others' 'That'. Our setting demonstrates four clients collaboratively training two diagnostic classifiers on a benchmark X-ray dataset. Without data bias, the global model reaches 74.14% balanced accuracy for cardiomegaly and 74.08% for pleural effusion. We show that with systematic visual bias in one client, the performance of global models drops to near-random. We demonstrate how differences between local and global prototypes reveal biases and allow their visualization on each client's data without compromising privacy.",
      "journal": "NPJ digital medicine",
      "year": "2024",
      "doi": "10.1038/s41746-024-01226-1",
      "authors": "Naumova Klavdiia et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39242810/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11379706",
      "ft_text_length": 41261,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11379706)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "39259582",
      "title": "Development of Lung Cancer Risk Prediction Machine Learning Models for Equitable Learning Health System: Retrospective Study.",
      "abstract": "BACKGROUND: A significant proportion of young at-risk patients and nonsmokers are excluded by the current guidelines for lung cancer (LC) screening, resulting in low-screening adoption. The vision of the US National Academy of Medicine to transform health systems into learning health systems (LHS) holds promise for bringing necessary structural changes to health care, thereby addressing the exclusivity and adoption issues of LC screening. OBJECTIVE: This study aims to realize the LHS vision by designing an equitable, machine learning (ML)-enabled LHS unit for LC screening. It focuses on developing an inclusive and practical LC risk prediction model, suitable for initializing the ML-enabled LHS (ML-LHS) unit. This model aims to empower primary physicians in a clinical research network, linking central hospitals and rural clinics, to routinely deliver risk-based screening for enhancing LC early detection in broader populations. METHODS: We created a standardized data set of health factors from 1397 patients with LC and 1448 control patients, all aged 30 years and older, including both smokers and nonsmokers, from a hospital's electronic medical record system. Initially, a data-centric ML approach was used to create inclusive ML models for risk prediction from all available health factors. Subsequently, a quantitative distribution of LC health factors was used in feature engineering to refine the models into a more practical model with fewer variables. RESULTS: The initial inclusive 250-variable XGBoost model for LC risk prediction achieved performance metrics of 0.86 recall, 0.90 precision, and 0.89 accuracy. Post feature refinement, a practical 29-variable XGBoost model was developed, displaying performance metrics of 0.80 recall, 0.82 precision, and 0.82 accuracy. This model met the criteria for initializing the ML-LHS unit for risk-based, inclusive LC screening within clinical research networks. CONCLUSIONS: This study designed an innovative ML-LHS unit for a clinical research network, aiming to sustainably provide inclusive LC screening to all at-risk populations. It developed an inclusive and practical XGBoost model from hospital electronic medical record data, capable of initializing such an ML-LHS unit for community and rural clinics. The anticipated deployment of this ML-LHS unit is expected to significantly improve LC-screening rates and early detection among broader populations, including those typically overlooked by existing screening guidelines.",
      "journal": "JMIR AI",
      "year": "2024",
      "doi": "10.2196/56590",
      "authors": "Chen Anjun et al.",
      "keywords": "AI; LHS; ML; artificial intelligence; early detection; learning health system; lung cancer; machine learning; predictive model; risk prediction",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39259582/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11425024",
      "ft_text_length": 30754,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11425024)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39265321",
      "title": "VASARI-auto: Equitable, efficient, and economical featurisation of glioma MRI.",
      "abstract": "The VASARI MRI feature set is a quantitative system designed to standardise glioma imaging descriptions. Though effective, deriving VASARI is time-consuming and seldom used clinically. We sought to resolve this problem with software automation and machine learning. Using glioma data from 1172 patients, we developed VASARI-auto, an automated labelling software applied to open-source lesion masks and an openly available tumour segmentation model. Consultant neuroradiologists independently quantified VASARI features in 100 held-out glioblastoma cases. We quantified 1) agreement across neuroradiologists and VASARI-auto, 2) software equity, 3) an economic workforce analysis, and 4) fidelity in predicting survival. Tumour segmentation was compatible with the current state of the art and equally performant regardless of age or sex. A modest inter-rater variability between in-house neuroradiologists was comparable to between neuroradiologists and VASARI-auto, with far higher agreement between VASARI-auto methods. The time for neuroradiologists to derive VASARI was substantially higher than VASARI-auto (mean time per case 317 vs. 3\u00a0s). A UK hospital workforce analysis forecast that three years of VASARI featurisation would demand 29,777 consultant neuroradiologist workforce hours and >\u00a31.5 ($1.9) million, reducible to 332\u00a0hours of computing time (and \u00a3146 of power) with VASARI-auto. The best-performing survival model utilised VASARI-auto features instead of those derived by neuroradiologists. VASARI-auto is a highly efficient and equitable automated labelling system, a favourable economic profile if used as a decision support tool, and non-inferior survival prediction. Future work should iterate upon and integrate such tools to enhance patient care.",
      "journal": "NeuroImage. Clinical",
      "year": "2024",
      "doi": "10.1016/j.nicl.2024.103668",
      "authors": "Ruffle James K et al.",
      "keywords": "Artificial intelligence; Decision support; Deep learning; Glioma; Medical imaging; Radiology; VASARI",
      "mesh_terms": "Humans; Glioma; Magnetic Resonance Imaging; Brain Neoplasms; Male; Female; Middle Aged; Adult; Machine Learning; Aged; Software; Image Interpretation, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39265321/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11415871",
      "ft_text_length": 49917,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11415871)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "39309255",
      "title": "Beyond ideals: why the (medical) AI industry needs to motivate behavioural change in line with fairness and transparency values, and how it can do it.",
      "abstract": "Artificial intelligence (AI) is increasingly relied upon by clinicians for making diagnostic and treatment decisions, playing an important role in imaging, diagnosis, risk analysis, lifestyle monitoring, and health information management. While research has identified biases in healthcare AI systems and proposed technical solutions to address these, we argue that effective solutions require human engagement. Furthermore, there is a lack of research on how to motivate the adoption of these solutions and promote investment in designing AI systems that align with values such as transparency and fairness from the outset. Drawing on insights from psychological theories, we assert the need to understand the values that underlie decisions made by individuals involved in creating and deploying AI systems. We describe how this understanding can be leveraged to increase engagement with de-biasing and fairness-enhancing practices within the AI healthcare industry, ultimately leading to sustained behavioral change via autonomy-supportive communication strategies rooted in motivational and social psychology theories. In developing these pathways to engagement, we consider the norms and needs that govern the AI healthcare domain, and we evaluate incentives for maintaining the status quo against economic, legal, and social incentives for behavior change in line with transparency and fairness values.",
      "journal": "AI & society",
      "year": "2024",
      "doi": "10.1007/s00146-023-01684-3",
      "authors": "Liefgreen Alice et al.",
      "keywords": "Artificial intelligence; Behaviour change; Bias; Fairness; Healthcare; Medicine; Motivation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39309255/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11415467",
      "ft_text_length": 60044,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11415467)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "39313595",
      "title": "A toolbox for surfacing health equity harms and biases in large language models.",
      "abstract": "Large language models (LLMs) hold promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities. Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity. We present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and conduct a large-scale empirical case study with the Med-PaLM 2 LLM. Our contributions include a multifactorial framework for human assessment of LLM-generated answers for biases and EquityMedQA, a collection of seven datasets enriched for adversarial queries. Both our human assessment framework and our dataset design process are grounded in an iterative participatory approach and review of Med-PaLM 2 answers. Through our empirical study, we find that our approach surfaces biases that may be missed by narrower evaluation approaches. Our experience underscores the importance of using diverse assessment methodologies and involving raters of varying backgrounds and expertise. While our approach is not sufficient to holistically assess whether the deployment of an artificial intelligence (AI) system promotes equitable health outcomes, we hope that it can be leveraged and built upon toward a shared goal of LLMs that promote accessible and equitable healthcare.",
      "journal": "Nature medicine",
      "year": "2024",
      "doi": "10.1038/s41591-024-03258-2",
      "authors": "Pfohl Stephen R et al.",
      "keywords": "",
      "mesh_terms": "Health Equity; Humans; Language; Bias; Artificial Intelligence; Healthcare Disparities",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39313595/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11645264",
      "ft_text_length": 77439,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11645264)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "39316436",
      "title": "Equity in Digital Mental Health Interventions in the United States: Where to Next?",
      "abstract": "Health care technologies have the ability to bridge or hinder equitable care. Advocates of digital mental health interventions (DMHIs) report that such technologies are poised to reduce the documented gross health care inequities that have plagued generations of people seeking care in the United States. This is due to a multitude of factors such as their potential to revolutionize access; mitigate logistical barriers to in-person mental health care; and leverage patient inputs to formulate tailored, responsive, and personalized experiences. Although we agree with the potential of DMHIs to advance health equity, we articulate several steps essential to mobilize and sustain meaningful forward progression in this endeavor, reflecting on decades of research and learnings drawn from multiple fields of expertise and real-world experience. First, DMHI manufacturers must build diversity, equity, inclusion, and belonging (DEIB) processes into the full spectrum of product evolution itself (eg, product design, evidence generation) as well as into the fabric of internal company practices (eg, talent recruitment, communication principles, and advisory boards). Second, awareness of the DEIB efforts-or lack thereof-in DMHI research trials is needed to refine and optimize future study design for inclusivity as well as proactively address potential barriers to doing so. Trials should incorporate thoughtful, inclusive, and creative approaches to recruitment, enrollment, and measurement of social determinants of health and self-identity, as well as a prioritization of planned and exploratory analyses examining outcomes across various groups of people. Third, mental health care advocacy, research funding policies, and local and federal legislation can advance these pursuits, with directives from the US Preventive Services Taskforce, National Institutes of Health, and Food and Drug Administration applied as poignant examples. For products with artificial intelligence/machine learning, maintaining a \"human in the loop\" as well as prespecified and adaptive analytic frameworks to monitor and remediate potential algorithmic bias can reduce the risk of increasing inequity. Last, but certainly not least, is a call for partnership and transparency within and across ecosystems (academic, industry, payer, provider, regulatory agencies, and value-based care organizations) to reliably build health equity into real-world DMHI product deployments and evidence-generation strategies. All these considerations should also extend into the context of an equity-informed commercial strategy for DMHI manufacturers and health care organizations alike. The potential to advance health equity in innovation with DMHI is apparent. We advocate the field's thoughtful and evergreen advancement in inclusivity, thereby redefining the mental health care experience for this generation and those to come.",
      "journal": "Journal of medical Internet research",
      "year": "2024",
      "doi": "10.2196/59939",
      "authors": "Robinson Athena et al.",
      "keywords": "Digital Mental Health Interventions; access to health care; health equity; health plan implementations; mental health",
      "mesh_terms": "Humans; United States; Mental Health Services; Mental Health; Health Equity; Telemedicine; Healthcare Disparities",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39316436/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11462105",
      "ft_text_length": 37771,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11462105)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39357045",
      "title": "Leveraging Temporal Trends for Training Contextual Word Embeddings to Address Bias in Biomedical Applications: Development Study.",
      "abstract": "BACKGROUND: Women have been underrepresented in clinical trials for many years. Machine-learning models trained on clinical trial abstracts may capture and amplify biases in the data. Specifically, word embeddings are models that enable representing words as vectors and are the building block of most natural language processing systems. If word embeddings are trained on clinical trial abstracts, predictive models that use the embeddings will exhibit gender performance gaps. OBJECTIVE: We aim to capture temporal trends in clinical trials through temporal distribution matching on contextual word embeddings (specifically, BERT) and explore its effect on the bias manifested in downstream tasks. METHODS: We present TeDi-BERT, a method to harness the temporal trend of increasing women's inclusion in clinical trials to train contextual word embeddings. We implement temporal distribution matching through an adversarial classifier, trying to distinguish old from new clinical trial abstracts based on their embeddings. The temporal distribution matching acts as a form of domain adaptation from older to more recent clinical trials. We evaluate our model on 2 clinical tasks: prediction of unplanned readmission to the intensive care unit and hospital length of stay prediction. We also conduct an algorithmic analysis of the proposed method. RESULTS: In readmission prediction, TeDi-BERT achieved area under the receiver operating characteristic curve of 0.64 for female patients versus the baseline of 0.62 (P<.001), and 0.66 for male patients versus the baseline of 0.64 (P<.001). In the length of stay regression, TeDi-BERT achieved a mean absolute error of 4.56 (95% CI 4.44-4.68) for female patients versus 4.62 (95% CI 4.50-4.74, P<.001) and 4.54 (95% CI 4.44-4.65) for male patients versus 4.6 (95% CI 4.50-4.71, P<.001). CONCLUSIONS: In both clinical tasks, TeDi-BERT improved performance for female patients, as expected; but it also improved performance for male patients. Our results show that accuracy for one gender does not need to be exchanged for bias reduction, but rather that good science improves clinical results for all. Contextual word embedding models trained to capture temporal trends can help mitigate the effects of bias that changes over time in the training data.",
      "journal": "JMIR AI",
      "year": "2024",
      "doi": "10.2196/49546",
      "authors": "Agmon Shunit et al.",
      "keywords": "BERT; NLP; algorithms; bias; gender; natural language processing; statistical models; word embeddings",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39357045/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11483253",
      "ft_text_length": 38960,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11483253)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "39369018",
      "title": "A fair individualized polysocial risk score for identifying increased social risk in type 2 diabetes.",
      "abstract": "Racial and ethnic minorities bear a disproportionate burden of type 2 diabetes (T2D) and its complications, with social determinants of health (SDoH) recognized as key drivers of these disparities. Implementing efficient and effective social needs management strategies is crucial. We propose a machine learning analytic pipeline to calculate the individualized polysocial risk score (iPsRS), which can identify T2D patients at high social risk for hospitalization, incorporating explainable AI techniques and algorithmic fairness optimization. We use electronic health records (EHR) data from T2D patients in the University of Florida Health Integrated Data Repository, incorporating both contextual SDoH (e.g., neighborhood deprivation) and person-level SDoH (e.g., housing instability). After fairness optimization across racial and ethnic groups, the iPsRS achieved a C statistic of 0.71 in predicting 1-year hospitalization. Our iPsRS can fairly and accurately screen patients with T2D who are at increased social risk for hospitalization.",
      "journal": "Nature communications",
      "year": "2024",
      "doi": "10.1038/s41467-024-52960-9",
      "authors": "Huang Yu et al.",
      "keywords": "",
      "mesh_terms": "Adult; Aged; Female; Humans; Male; Middle Aged; Diabetes Mellitus, Type 2; Electronic Health Records; Ethnicity; Florida; Hospitalization; Machine Learning; Risk Assessment; Risk Factors; Social Determinants of Health; Racial Groups",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39369018/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11455957",
      "ft_text_length": 37901,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11455957)",
      "ft_reason": "Included: bias central + approach content (14 indicators)"
    },
    {
      "pmid": "39371141",
      "title": "Evaluating and Reducing Subgroup Disparity in AI Models: An Analysis of Pediatric COVID-19 Test Outcomes.",
      "abstract": "Artificial Intelligence (AI) fairness in healthcare settings has attracted significant attention due to the concerns to propagate existing health disparities. Despite ongoing research, the frequency and extent of subgroup fairness have not been sufficiently studied. In this study, we extracted a nationally representative pediatric dataset (ages 0-17, n=9,935) from the US National Health Interview Survey (NHIS) concerning COVID-19 test outcomes. For subgroup disparity assessment, we trained 50 models using five machine learning algorithms. We assessed the models' area under the curve (AUC) on 12 small (<15% of the total n) subgroups defined using social economic factors versus the on the overall population. Our results show that subgroup disparities were prevalent (50.7%) in the models. Subgroup AUCs were generally lower, with a mean difference of 0.01, ranging from -0.29 to +0.41. Notably, the disparities were not always statistically significant, with four out of 12 subgroups having statistically significant disparities across models. Additionally, we explored the efficacy of synthetic data in mitigating identified disparities. The introduction of synthetic data enhanced subgroup disparity in 57.7% of the models. The mean AUC disparities for models with synthetic data decreased on average by 0.03 via resampling and 0.04 via generative adverbial network methods.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2024",
      "doi": "10.1101/2024.09.18.24313889",
      "authors": "Libin Alexander et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39371141/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11451670",
      "ft_text_length": 16564,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11451670)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "39384748",
      "title": "Enhancing fairness in AI-enabled medical systems with the attribute neutral framework.",
      "abstract": "Questions of unfairness and inequity pose critical challenges to the successful deployment of artificial intelligence (AI) in healthcare settings. In AI models, unequal performance across protected groups may be partially attributable to the learning of spurious or otherwise undesirable correlations between sensitive attributes and disease-related information. Here, we introduce the Attribute Neutral Framework, designed to disentangle biased attributes from disease-relevant information and subsequently neutralize them to improve representation across diverse subgroups. Within the framework, we develop the Attribute Neutralizer (AttrNzr) to generate neutralized data, for which protected attributes can no longer be easily predicted by humans or by machine learning classifiers. We then utilize these data to train the disease diagnosis model (DDM). Comparative analysis with other unfairness mitigation algorithms demonstrates that AttrNzr outperforms in reducing the unfairness of the DDM while maintaining DDM's overall disease diagnosis performance. Furthermore, AttrNzr supports the simultaneous neutralization of multiple attributes and demonstrates utility even when applied solely during the training phase, without being used in the test phase. Moreover, instead of introducing additional constraints to the DDM, the AttrNzr directly addresses a root cause of unfairness, providing a model-independent solution. Our results with AttrNzr highlight the potential of data-centered and model-independent solutions for fairness challenges in AI-enabled medical systems.",
      "journal": "Nature communications",
      "year": "2024",
      "doi": "10.1038/s41467-024-52930-1",
      "authors": "Hu Lianting et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Algorithms; Machine Learning; Delivery of Health Care",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39384748/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11464531",
      "ft_text_length": 85032,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11464531)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "39386055",
      "title": "The Impact of Race, Ethnicity, and Sex on Fairness in Artificial Intelligence for Glaucoma Prediction Models.",
      "abstract": "OBJECTIVE: Despite advances in artificial intelligence (AI) in glaucoma prediction, most works lack multicenter focus and do not consider fairness concerning sex, race, or ethnicity. This study aims to examine the impact of these sensitive attributes on developing fair AI models that predict glaucoma progression to necessitating incisional glaucoma surgery. DESIGN: Database study. PARTICIPANTS: Thirty-nine thousand ninety patients with glaucoma, as identified by International Classification of Disease codes from 7 academic eye centers participating in the Sight OUtcomes Research Collaborative. METHODS: We developed XGBoost models using 3 approaches: (1) excluding sensitive attributes as input features, (2) including them explicitly as input features, and (3) training separate models for each group. Model input features included demographic details, diagnosis codes, medications, and clinical information (intraocular pressure, visual acuity, etc.), from electronic health records. The models were trained on patients from 5 sites (N\u00a0=\u00a027\u00a0999) and evaluated on a held-out internal test set (N\u00a0=\u00a03499) and 2 external test sets consisting of N\u00a0=\u00a01550 and N\u00a0=\u00a02542 patients. MAIN OUTCOMES AND MEASURES: Area under the receiver operating characteristic curve (AUROC) and equalized odds on the test set and external sites. RESULTS: Six thousand six hundred eighty-two (17.1%) of 39\u00a0090 patients underwent glaucoma surgery with a mean age of 70.1 (standard deviation 14.6) years, 54.5% female, 62.3% White, 22.1% Black, and 4.7% Latinx/Hispanic. We found that not including the sensitive attributes led to better classification performance (AUROC: 0.77-0.82) but worsened fairness when evaluated on the internal test set. However, on external test sites, the opposite was true: including sensitive attributes resulted in better classification performance (AUROC: external #1 - [0.73-0.81], external #2 - [0.67-0.70]), but varying degrees of fairness for sex and race as measured by equalized odds. CONCLUSIONS: Artificial intelligence models predicting whether patients with glaucoma progress to surgery demonstrated bias with respect to sex, race, and ethnicity. The effect of sensitive attribute inclusion and exclusion on fairness and performance varied based on internal versus external test sets. Prior to deployment, AI models should be evaluated for fairness on the target population. FINANCIAL DISCLOSURES: Proprietary or commercial disclosure may be found in the Footnotes and Disclosures at the end of this article.",
      "journal": "Ophthalmology science",
      "year": "2025",
      "doi": "10.1016/j.xops.2024.100596",
      "authors": "Ravindranath Rohith et al.",
      "keywords": "Bias; Fairness; Glaucoma; Health disparities; Machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39386055/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11462200",
      "ft_text_length": 33357,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11462200)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "39399154",
      "title": "Predicting Prenatal Depression and Assessing Model Bias Using Machine Learning Models.",
      "abstract": "BACKGROUND: Perinatal depression is one of the most common medical complications during pregnancy and postpartum period, affecting 10% to 20% of pregnant individuals, with higher rates among Black and Latina women who are also less likely to be diagnosed and treated. Machine learning (ML) models based on electronic medical records (EMRs) have effectively predicted postpartum depression in middle-class White women but have rarely included sufficient proportions of racial/ethnic minorities, which has contributed to biases in ML models. Our goal is to determine whether ML models could predict depression in early pregnancy in racial/ethnic minority women by leveraging EMR data. METHODS: We extracted EMRs from a large U.S. urban hospital serving mostly low-income Black and Hispanic women (n\u00a0= 5875). Depressive symptom severity was assessed using the Patient Health Questionnaire-9 self-report questionnaire. We investigated multiple ML classifiers using Shapley additive explanations for model interpretation and determined prediction bias with 4 metrics: disparate impact, equal opportunity difference, and equalized odds (standard deviations of true positives and false positives). RESULTS: Although the best-performing ML model's (elastic net) performance was low (area under the receiver operating characteristic curve\u00a0= 0.61), we identified known perinatal depression risk factors such as unplanned pregnancy and being single and underexplored factors such as self-reported pain, lower prenatal vitamin intake, asthma, carrying a male fetus, and lower platelet levels. Despite the sample comprising mostly low-income minority women (54% Black, 27% Latina), the model performed worse for these communities (area under the receiver operating characteristic curve: 57% Black, 59% Latina women vs. 64% White women). CONCLUSIONS: EMR-based ML models could moderately predict early pregnancy depression but exhibited biased performance against low-income minority women. Perinatal depression affects 10% to 20% of pregnant individuals, with higher rates among racial/ethnic minorities who are underdiagnosed and undertreated. This study used machine learning models on electronic medical record data from a hospital serving mostly low-income Black and Hispanic women to predict early pregnancy depression. While the best model performed moderately well, it exhibited bias, predicting depression less accurately for Black and Latina women compared with White women. The study identified some known risk factors such as unplanned pregnancy and underexplored factors such as self-reported pain, lower prenatal vitamin intake, and carrying a male fetus that may contribute to perinatal depression.",
      "journal": "Biological psychiatry global open science",
      "year": "2024",
      "doi": "10.1016/j.bpsgos.2024.100376",
      "authors": "Huang Yongchao et al.",
      "keywords": "Electronic medical records; Health disparities; Machine learning; Model performance bias; Perinatal depression",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39399154/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11470166",
      "ft_text_length": 33004,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11470166)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "39427028",
      "title": "Guidance for unbiased predictive information for healthcare decision-making and equity (GUIDE): considerations when race may be a prognostic factor.",
      "abstract": "Clinical prediction models (CPMs) are tools that compute the risk of an outcome given a set of patient characteristics and are routinely used to inform patients, guide treatment decision-making, and resource allocation. Although much hope has been placed on CPMs to mitigate human biases, CPMs may potentially contribute to racial disparities in decision-making and resource allocation. While some policymakers, professional organizations, and scholars have called for eliminating race as a variable from CPMs, others raise concerns that excluding race may exacerbate healthcare disparities and this controversy remains unresolved. The Guidance for Unbiased predictive Information for healthcare Decision-making and Equity (GUIDE) provides expert guidelines for model developers and health system administrators on the transparent use of race in CPMs and mitigation of algorithmic bias across contexts developed through a 5-round, modified Delphi process from a diverse 14-person technical expert panel (TEP). Deliberations affirmed that race is a social construct and that the goals of prediction are distinct from those of causal inference, and emphasized: the importance of decisional context (e.g., shared decision-making versus healthcare rationing); the conflicting nature of different anti-discrimination principles (e.g., anticlassification versus antisubordination principles); and the importance of identifying and balancing trade-offs in achieving equity-related goals with race-aware versus race-unaware CPMs for conditions where racial identity is prognostically informative. The GUIDE, comprising 31 key items in the development and use of CPMs in healthcare, outlines foundational principles, distinguishes between bias and fairness, and offers guidance for examining subgroup invalidity and using race as a variable in CPMs. This GUIDE presents a living document that supports appraisal and reporting of bias in CPMs to support best practice in CPM development and use.",
      "journal": "NPJ digital medicine",
      "year": "2024",
      "doi": "10.1038/s41746-024-01245-y",
      "authors": "Ladin Keren et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39427028/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11490638",
      "ft_text_length": 55682,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11490638)",
      "ft_reason": "Included: bias central + approach content (10 indicators)"
    },
    {
      "pmid": "39433945",
      "title": "Evaluation and mitigation of cognitive biases in medical language models.",
      "abstract": "Increasing interest in applying large language models (LLMs) to medicine is due in part to their impressive performance on medical exam questions. However, these exams do not capture the complexity of real patient-doctor interactions because of factors like patient compliance, experience, and cognitive bias. We hypothesized that LLMs would produce less accurate responses when faced with clinically biased questions as compared to unbiased ones. To test this, we developed the BiasMedQA dataset, which consists of 1273 USMLE questions modified to replicate common clinically relevant cognitive biases. We assessed six LLMs on BiasMedQA and found that GPT-4 stood out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which showed large drops in performance. Additionally, we introduced three bias mitigation strategies, which improved but did not fully restore accuracy. Our findings highlight the need to improve LLMs' robustness to cognitive biases, in order to achieve more reliable applications of LLMs in healthcare.",
      "journal": "NPJ digital medicine",
      "year": "2024",
      "doi": "10.1038/s41746-024-01283-6",
      "authors": "Schmidgall Samuel et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39433945/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11494053",
      "ft_text_length": 39163,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11494053)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "39437384",
      "title": "Gender Bias in AI's Perception of Cardiovascular Risk.",
      "abstract": "The study investigated gender bias in GPT-4's assessment of coronary artery disease risk by presenting identical clinical vignettes of men and women with and without psychiatric comorbidities. Results suggest that psychiatric conditions may influence GPT-4's coronary artery disease risk assessment among men and women.",
      "journal": "Journal of medical Internet research",
      "year": "2024",
      "doi": "10.2196/54242",
      "authors": "Achtari Margaux et al.",
      "keywords": "AI; CAD; artery; artificial intelligence; cardiovascular; chatbot: health care; coronary; coronary artery disease; gender; gender bias; gender equity; men: women; risk",
      "mesh_terms": "Humans; Female; Male; Sexism; Cardiovascular Diseases; Middle Aged; Risk Assessment; Artificial Intelligence; Adult; Heart Disease Risk Factors; Coronary Artery Disease",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39437384/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11538872",
      "ft_text_length": 6111,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11538872)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39441784",
      "title": "Conceptualizing bias in EHR data: A case study in performance disparities by demographic subgroups for a pediatric obesity incidence classifier.",
      "abstract": "Electronic Health Records (EHRs) are increasingly used to develop machine learning models in predictive medicine. There has been limited research on utilizing machine learning methods to predict childhood obesity and related disparities in classifier performance among vulnerable patient subpopulations. In this work, classification models are developed to recognize pediatric obesity using temporal condition patterns obtained from patient EHR data in a U.S. study population. We trained four machine learning algorithms (Logistic Regression, Random Forest, Gradient Boosted Trees, and Neural Networks) to classify cases and controls as obesity positive or negative, and optimized hyperparameter settings through a bootstrapping methodology. To assess the classifiers for bias, we studied model performance by population subgroups then used permutation analysis to identify the most predictive features for each model and the demographic characteristics of patients with these features. Mean AUC-ROC values were consistent across classifiers, ranging from 0.72-0.80. Some evidence of bias was identified, although this was through the models performing better for minority subgroups (African Americans and patients enrolled in Medicaid). Permutation analysis revealed that patients from vulnerable population subgroups were over-represented among patients with the most predictive diagnostic patterns. We hypothesize that our models performed better on under-represented groups because the features more strongly associated with obesity were more commonly observed among minority patients. These findings highlight the complex ways that bias may arise in machine learning models and can be incorporated into future research to develop a thorough analytical approach to identify and mitigate bias that may arise from features and within EHR datasets when developing more equitable models.",
      "journal": "PLOS digital health",
      "year": "2024",
      "doi": "10.1371/journal.pdig.0000642",
      "authors": "Campbell Elizabeth A et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39441784/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11498669",
      "ft_text_length": 38458,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11498669)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "39502657",
      "title": "Raising awareness of potential biases in medical machine learning: Experience from a Datathon.",
      "abstract": "OBJECTIVE: To challenge clinicians and informaticians to learn about potential sources of bias in medical machine learning models through investigation of data and predictions from an open-source severity of illness score. METHODS: Over a two-day period (total elapsed time approximately 28 hours), we conducted a datathon that challenged interdisciplinary teams to investigate potential sources of bias in the Global Open Source Severity of Illness Score. Teams were invited to develop hypotheses, to use tools of their choosing to identify potential sources of bias, and to provide a final report. RESULTS: Five teams participated, three of which included both informaticians and clinicians. Most (4/5) used Python for analyses, the remaining team used R. Common analysis themes included relationship of the GOSSIS-1 prediction score with demographics and care related variables; relationships between demographics and outcomes; calibration and factors related to the context of care; and the impact of missingness. Representativeness of the population, differences in calibration and model performance among groups, and differences in performance across hospital settings were identified as possible sources of bias. DISCUSSION: Datathons are a promising approach for challenging developers and users to explore questions relating to unrecognized biases in medical machine learning algorithms.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2024",
      "doi": "10.1101/2024.10.21.24315543",
      "authors": "Hochheiser Harry et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39502657/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11537317",
      "ft_text_length": 12146,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11537317)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "39541580",
      "title": "Economics and Equity of Large Language Models: Health Care Perspective.",
      "abstract": "Large language models (LLMs) continue to exhibit noteworthy capabilities across a spectrum of areas, including emerging proficiencies across the health care continuum. Successful LLM implementation and adoption depend on digital readiness, modern infrastructure, a trained workforce, privacy, and an ethical regulatory landscape. These factors can vary significantly across health care ecosystems, dictating the choice of a particular LLM implementation pathway. This perspective discusses 3 LLM implementation pathways-training from scratch pathway (TSP), fine-tuned pathway (FTP), and out-of-the-box pathway (OBP)-as potential onboarding points for health systems while facilitating equitable adoption. The choice of a particular pathway is governed by needs as well as affordability. Therefore, the risks, benefits, and economics of these pathways across 4 major cloud service providers (Amazon, Microsoft, Google, and Oracle) are presented. While cost comparisons, such as on-demand and spot pricing across the cloud service providers for the 3 pathways, are presented for completeness, the usefulness of managed services and cloud enterprise tools is elucidated. Managed services can complement the traditional workforce and expertise, while enterprise tools, such as federated learning, can overcome sample size challenges when implementing LLMs using health care data. Of the 3 pathways, TSP is expected to be the most resource-intensive regarding infrastructure and workforce while providing maximum customization, enhanced transparency, and performance. Because TSP trains the LLM using enterprise health care data, it is expected to harness the digital signatures of the population served by the health care system with the potential to impact outcomes. The use of pretrained models in FTP is a limitation. It may impact its performance because the training data used in the pretrained model may have hidden bias and may not necessarily be health care-related. However, FTP provides a balance between customization, cost, and performance. While OBP can be rapidly deployed, it provides minimal customization and transparency without guaranteeing long-term availability. OBP may also present challenges in interfacing seamlessly with downstream applications in health care settings with variations in pricing and use over time. Lack of customization in OBP can significantly limit its ability to impact outcomes. Finally, potential applications of LLMs in health care, including conversational artificial intelligence, chatbots, summarization, and machine translation, are highlighted. While the 3 implementation pathways discussed in this perspective have the potential to facilitate equitable adoption and democratization of LLMs, transitions between them may be necessary as the needs of health systems evolve. Understanding the economics and trade-offs of these onboarding pathways can guide their strategic adoption and demonstrate value while impacting health care outcomes favorably.",
      "journal": "Journal of medical Internet research",
      "year": "2024",
      "doi": "10.2196/64226",
      "authors": "Nagarajan Radha et al.",
      "keywords": "LLM; cloud; cloud service providers; democratization; economics; equity; health care; health outcome; implementation; large language model",
      "mesh_terms": "Humans; Delivery of Health Care; Language",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39541580/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11605263",
      "ft_text_length": 90101,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11605263)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39569213",
      "title": "FAIM: Fairness-aware interpretable modeling for trustworthy machine learning in healthcare.",
      "abstract": "The escalating integration of machine learning in high-stakes fields such as healthcare raises substantial concerns about model fairness. We propose an interpretable framework, fairness-aware interpretable modeling (FAIM), to improve model fairness without compromising performance, featuring an interactive interface to identify a \"fairer\" model from a set of high-performing models and promoting the integration of data-driven evidence and clinical expertise to enhance contextualized fairness. We demonstrate FAIM's value in reducing intersectional biases arising from race and sex by predicting hospital admission with two real-world databases, the Medical Information Mart for Intensive Care IV Emergency Department (MIMIC-IV-ED) and the database collected from Singapore General Hospital Emergency Department (SGH-ED). For both datasets, FAIM models not only exhibit satisfactory discriminatory performance but also significantly mitigate biases as measured by well-established fairness metrics, outperforming commonly used bias mitigation methods. Our approach demonstrates the feasibility of improving fairness without sacrificing performance and provides a modeling mode that invites domain experts to engage, fostering a multidisciplinary effort toward tailored AI fairness.",
      "journal": "Patterns (New York, N.Y.)",
      "year": "2024",
      "doi": "10.1016/j.patter.2024.101059",
      "authors": "Liu Mingxuan et al.",
      "keywords": "AI fairness; fairness in healthcare; interpretable machine learning; trustworthy machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39569213/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11573921",
      "ft_text_length": 50208,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11573921)",
      "ft_reason": "Included: bias central + approach content (10 indicators)"
    },
    {
      "pmid": "39575595",
      "title": "Evaluating a predictive model of avoidable hospital events for race- and sex-based bias.",
      "abstract": "OBJECTIVE: To evaluate whether race- and sex-based biases are present in a predictive model of avoidable hospital (AH) events. STUDY SETTING AND DESIGN: We examined whether Medicare fee-for-service (FFS) beneficiaries in Maryland with similar risk scores differed in true AH event risk on the basis of race or sex (n\u2009=\u2009324,834). This was operationalized as a logistic regression of true AH events on race or sex with fixed effects for risk score percentile. DATA SOURCES AND ANALYTIC SAMPLE: Beneficiary-level risk scores were derived from 36 months of Medicare FFS claims (April 2019-March 2022) and generated in May 2022. True AH events were observed in claims from June 2022. PRINCIPAL FINDINGS: Black patients had higher average risk scores than White patients; however, the likelihood of experiencing an AH event did not differ by race when controlling for predicted risk (Marginal Effect [ME]\u2009=\u20090.0003, 95%CI -0.0003 to 0.0009). AH event likelihood was lower in males when controlling for risk level; however, the effect was small (ME\u2009=\u2009-0.0008, 95% CI -0.0013 to -0.0003) and it did not differ by sex for the target group for intervention (ME\u2009=\u20090.0002, 95% CI -0.0031 to 0.0036). CONCLUSIONS: We implemented a simple bias assessment methodology and found no evidence of meaningful race- or sex-based bias in this model. We encourage the incorporation of bias checks into predictive model development and monitoring processes.",
      "journal": "Health services research",
      "year": "2025",
      "doi": "10.1111/1475-6773.14409",
      "authors": "Goetschius Leigh et al.",
      "keywords": "Medicare; avoidable hospitalization; biases; machine learning; predictive modeling",
      "mesh_terms": "Aged; Aged, 80 and over; Female; Humans; Male; Black or African American; Fee-for-Service Plans; Maryland; Medicare; Racial Groups; Sex Factors; United States; White",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39575595/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12120512",
      "ft_text_length": 1438,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12120512)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39582990",
      "title": "Artificial intelligence: Applications in cardio-oncology and potential impact on racial disparities.",
      "abstract": "Numerous cancer therapies have detrimental cardiovascular effects on cancer survivors. Cardiovascular toxicity can span the course of cancer treatment and is influenced by several factors. To mitigate these risks, cardio-oncology has evolved, with an emphasis on prevention and treatment of cardiovascular complications resulting from the presence of cancer and cancer therapy. Artificial intelligence (AI) holds multifaceted potential to enhance cardio-oncologic outcomes. AI algorithms are currently utilizing clinical data input to identify patients at risk for cardiac complications. Additional application opportunities for AI in cardio-oncology involve multimodal cardiovascular imaging, where algorithms can also utilize imaging input to generate predictive risk profiles for cancer patients. The impact of AI extends to digital health tools, playing a pivotal role in the development of digital platforms and wearable technologies. Multidisciplinary teams have been formed to implement and evaluate the efficacy of these technologies, assessing AI-driven clinical decision support tools. Other avenues similarly support practical application of AI in clinical practice, such as incorporation into electronic health records (EHRs) to detect patients at risk for cardiovascular diseases. While these AI applications may help improve preventive measures and facilitate tailored treatment to patients, they are also capable of perpetuating and exacerbating healthcare disparities, if trained on limited, homogenous datasets. However, if trained and operated appropriately, AI holds substantial promise in positively influencing clinical practice in cardio-oncology. In this review, we explore the impact of AI on cardio-oncology care, particularly regarding predicting cardiotoxicity from cancer treatments, while addressing racial and ethnic biases in algorithmic implementation.",
      "journal": "American heart journal plus : cardiology research and practice",
      "year": "2024",
      "doi": "10.1016/j.ahjo.2024.100479",
      "authors": "Echefu Gift et al.",
      "keywords": "Artificial intelligence; Cancer; Cardio-oncology; Cardiovascular disease; Racial disparities",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39582990/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11583718",
      "ft_text_length": 66839,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11583718)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "39602221",
      "title": "Ensuring Appropriate Representation in Artificial Intelligence-Generated Medical Imagery: Protocol for a Methodological Approach to Address Skin Tone Bias.",
      "abstract": "BACKGROUND: In medical education, particularly in anatomy and dermatology, generative artificial intelligence (AI) can be used to create customized illustrations. However, the underrepresentation of darker skin tones in medical textbooks and elsewhere, which serve as training data for AI, poses a significant challenge in ensuring diverse and inclusive educational materials. OBJECTIVE: This study aims to evaluate the extent of skin tone diversity in AI-generated medical images and to test whether the representation of skin tones can be improved by modifying AI prompts to better reflect the demographic makeup of the US population. METHODS: In total, 2 standard AI models (Dall-E [OpenAI] and Midjourney [Midjourney Inc]) each generated 100 images of people with psoriasis. In addition, a custom model was developed that incorporated a prompt injection aimed at \"forcing\" the AI (Dall-E 3) to reflect the skin tone distribution of the US population according to the 2012 American National Election Survey. This custom model generated another set of 100 images. The skin tones in these images were assessed by 3 researchers using the New Immigrant Survey skin tone scale, with the median value representing each image. A chi-square goodness of fit analysis compared the skin tone distributions from each set of images to that of the US population. RESULTS: The standard AI models (Dalle-3 and Midjourney) demonstrated a significant difference between the expected skin tones of the US population and the observed tones in the generated images (P<.001). Both standard AI models overrepresented lighter skin. Conversely, the custom model with the modified prompt yielded a distribution of skin tones that closely matched the expected demographic representation, showing no significant difference (P=.04). CONCLUSIONS: This study reveals a notable bias in AI-generated medical images, predominantly underrepresenting darker skin tones. This bias can be effectively addressed by modifying AI prompts to incorporate real-life demographic distributions. The findings emphasize the need for conscious efforts in AI development to ensure diverse and representative outputs, particularly in educational and medical contexts. Users of generative AI tools should be aware that these biases exist, and that similar tendencies may also exist in other types of generative AI (eg, large language models) and in other characteristics (eg, sex, gender, culture, and ethnicity). Injecting demographic data into AI prompts may effectively counteract these biases, ensuring a more accurate representation of the general population.",
      "journal": "JMIR AI",
      "year": "2024",
      "doi": "10.2196/58275",
      "authors": "O'Malley Andrew et al.",
      "keywords": "AI images; United States; anatomy; artificial intelligence; dermatology; digital imagery; educational material; generative AI; medical education; medical imaging; psoriasis; skin; skin tone",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39602221/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11635324",
      "ft_text_length": 22043,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11635324)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "39625723",
      "title": "Evaluating Bias-Mitigated Predictive Models of Perinatal Mood and Anxiety Disorders.",
      "abstract": "IMPORTANCE: Machine learning for augmented screening of perinatal mood and anxiety disorders (PMADs) requires thorough consideration of clinical biases embedded in electronic health records (EHRs) and rigorous evaluations of model performance. OBJECTIVE: To mitigate bias in predictive models of PMADs trained on commonly available EHRs. DESIGN, SETTING, AND PARTICIPANTS: This diagnostic study collected data as part of a quality improvement initiative from 2020 to 2023 at Cedars-Sinai Medical Center in Los Angeles, California. The study inclusion criteria were birthing patients aged 14 to 59 years with live birth records and admission to the postpartum unit or the maternal-fetal care unit after delivery. EXPOSURE: Patient-reported race and ethnicity (7 levels) obtained through EHRs. MAIN OUTCOMES AND MEASURES: Logistic regression, random forest, and extreme gradient boosting models were trained to predict 2 binary outcomes: moderate to high-risk (positive) screen assessed using the 9-item Patient Health Questionnaire (PHQ-9), and the Edinburgh Postnatal Depression Scale (EPDS). Each model was fitted with or without reweighing data during preprocessing and evaluated through repeated K-fold cross validation. In every iteration, each model was evaluated on its area under the receiver operating curve (AUROC) and on 2 fairness metrics: demographic parity (DP), and difference in false negatives between races and ethnicities (relative to non-Hispanic White patients). RESULTS: Among 19\u202f430 patients in this study, 1402 (7%) identified as African American or Black, 2371 (12%) as Asian American and Pacific Islander; 1842 (10%) as Hispanic White, 10\u202f942 (56.3%) as non-Hispanic White, 606 (3%) as multiple races, 2146 (11%) as other (not further specified), and 121 (<1%) did not provide this information. The mean (SD) age was 34.1 (4.9) years, and all patients identified as female. Racial and ethnic minority patients were significantly more likely than non-Hispanic White patients to screen positive on both the PHQ-9 (odds ratio, 1.47 [95% CI, 1.23-1.77]) and the EPDS (odds ratio, 1.38 [95% CI, 1.20-1.57]). Mean AUROCs ranged from 0.610 to 0.635 without reweighing (baseline), and from 0.602 to 0.622 with reweighing. Baseline models predicted significantly greater prevalence of postpartum depression for patients who were not non-Hispanic White relative to those who were (mean DP, 0.238 [95% CI, 0.231-0.244]; P\u2009<\u2009.001) and displayed significantly lower false-negative rates (mean difference, -0.184 [95% CI, -0.195 to -0.174]; P\u2009<\u2009.001). Reweighing significantly reduced differences in DP (mean DP with reweighing, 0.022 [95% CI, 0.017-0.026]; P\u2009<\u2009.001) and false-negative rates (mean difference with reweighing, 0.018 [95% CI, 0.008-0.028]; P\u2009<\u2009.001) between racial and ethnic groups. CONCLUSIONS AND RELEVANCE: In this diagnostic study of predictive models of postpartum depression, clinical prediction models trained to predict psychometric screening results from commonly available EHRs achieved modest performance and were less likely to widen existing health disparities in PMAD diagnosis and potentially treatment. These findings suggest that is critical for researchers and physicians to consider their model design (eg, desired target and predictor variables) and evaluate model bias to minimize health disparities.",
      "journal": "JAMA network open",
      "year": "2024",
      "doi": "10.1001/jamanetworkopen.2024.38152",
      "authors": "Wong Emily F et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Adult; Pregnancy; Anxiety Disorders; Mood Disorders; Adolescent; Young Adult; Electronic Health Records; Machine Learning; Bias; Pregnancy Complications; Middle Aged; Logistic Models",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39625723/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11615713",
      "ft_text_length": 26732,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11615713)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "39636692",
      "title": "Intersection of Performance, Interpretability, and Fairness in Neural Prototype Tree for Chest X-Ray Pathology Detection: Algorithm Development and Validation Study.",
      "abstract": "BACKGROUND: While deep learning classifiers have shown remarkable results in detecting chest X-ray (CXR) pathologies, their adoption in clinical settings is often hampered by the lack of transparency. To bridge this gap, this study introduces the neural prototype tree (NPT), an interpretable image classifier that combines the diagnostic capability of deep learning models and the interpretability of the decision tree for CXR pathology detection. OBJECTIVE: This study aimed to investigate the utility of the NPT classifier in 3 dimensions, including performance, interpretability, and fairness, and subsequently examined the complex interaction between these dimensions. We highlight both local and global explanations of the NPT classifier and discuss its potential utility in clinical settings. METHODS: This study used CXRs from the publicly available Chest X-ray 14, CheXpert, and MIMIC-CXR datasets. We trained 6 separate classifiers for each CXR pathology in all datasets, 1 baseline residual neural network (ResNet)-152, and 5 NPT classifiers with varying levels of interpretability. Performance, interpretability, and fairness were measured using the area under the receiver operating characteristic curve (ROC AUC), interpretation complexity (IC), and mean true positive rate (TPR) disparity, respectively. Linear regression analyses were performed to investigate the relationship between IC and ROC AUC, as well as between IC and mean TPR disparity. RESULTS: The performance of the NPT classifier improved as the IC level increased, surpassing that of ResNet-152 at IC level 15 for the Chest X-ray 14 dataset and IC level 31 for the CheXpert and MIMIC-CXR datasets. The NPT classifier at IC level 1 exhibited the highest degree of unfairness, as indicated by the mean TPR disparity. The magnitude of unfairness, as measured by the mean TPR disparity, was more pronounced in groups differentiated by age (chest X-ray 14 0.112, SD 0.015; CheXpert 0.097, SD 0.010; MIMIC 0.093, SD 0.017) compared to sex (chest X-ray 14 0.054 SD 0.012; CheXpert 0.062, SD 0.008; MIMIC 0.066, SD 0.013). A significant positive relationship between interpretability (ie, IC level) and performance (ie, ROC AUC) was observed across all CXR pathologies (P<.001). Furthermore, linear regression analysis revealed a significant negative relationship between interpretability and fairness (ie, mean TPR disparity) across age and sex subgroups (P<.001). CONCLUSIONS: By illuminating the intricate relationship between performance, interpretability, and fairness of the NPT classifier, this research offers insightful perspectives that could guide future developments in effective, interpretable, and equitable deep learning classifiers for CXR pathology detection.",
      "journal": "JMIR formative research",
      "year": "2024",
      "doi": "10.2196/59045",
      "authors": "Chen Hongbo et al.",
      "keywords": "chest x-ray; deep learning; explainable artificial intelligence; fairness; interpretability; thoracic pathology",
      "mesh_terms": "Humans; Radiography, Thoracic; Male; Female; Neural Networks, Computer; ROC Curve; Algorithms; Deep Learning; Middle Aged; Adult; Aged; Area Under Curve; Decision Trees",
      "pub_types": "Journal Article; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39636692/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11659703",
      "ft_text_length": 51863,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11659703)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "39652409",
      "title": "The roadmap to integrate diversity, equity, and inclusion in hematology clinical trials: an American Society of Hematology initiative.",
      "abstract": "Clinical trial design for classical hematologic diseases is difficult because samples sizes are often small and not representative of the disease population. The American Society of Hematology initiated a roadmap project to identify barriers and make progress to integrate diversity, equity, and inclusion into trial design and conduct. Focus groups of international experts from across the clinical trial ecosystem were conducted. Eight issues identified include (1) harmonization of demographic terminology; (2) engagement of lived experience experts across the entire study timeline; (3) awareness of how implicit biases impede patient enrollment; (4) the need for institutional review boards to uphold the justice principle of clinical trial enrollment; (5) broadening of eligibility criteria; (6) decentralized trial design; (7) improving access to clinical trial information; and (8) increased community physician involvement. By addressing these issues, the hematology community can promote accessible and inclusive trials that will further inform research, clinical decision-making, and care for patients.",
      "journal": "Blood advances",
      "year": "2025",
      "doi": "10.1182/bloodadvances.2024013945",
      "authors": "Kuaban Alice et al.",
      "keywords": "",
      "mesh_terms": "Humans; Clinical Trials as Topic; Hematology; United States; Cultural Diversity; Patient Selection; Societies, Medical",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39652409/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11869854",
      "ft_text_length": 31235,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11869854)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39671594",
      "title": "Survival After Radical Cystectomy for Bladder Cancer: Development of a Fair Machine Learning Model.",
      "abstract": "BACKGROUND: Prediction models based on machine learning (ML) methods are being increasingly developed and adopted in health care. However, these models may be prone to bias and considered unfair if they demonstrate variable performance in population subgroups. An unfair model is of particular concern in bladder cancer, where disparities have been identified in sex and racial subgroups. OBJECTIVE: This study aims (1) to develop a ML model to predict survival after radical cystectomy for bladder cancer and evaluate for potential model bias in sex and racial subgroups; and (2) to compare algorithm unfairness mitigation techniques to improve model fairness. METHODS: We trained and compared various ML classification algorithms to predict 5-year survival after radical cystectomy using the National Cancer Database. The primary model performance metric was the F1-score. The primary metric for model fairness was the equalized odds ratio (eOR). We compared 3 algorithm unfairness mitigation techniques to improve eOR. RESULTS: We identified 16,481 patients; 23.1% (n=3800) were female, and 91.5% (n=15,080) were \"White,\" 5% (n=832) were \"Black,\" 2.3% (n=373) were \"Hispanic,\" and 1.2% (n=196) were \"Asian.\" The 5-year mortality rate was 75% (n=12,290). The best naive model was extreme gradient boosting (XGBoost), which had an F1-score of 0.860 and eOR of 0.619. All unfairness mitigation techniques increased the eOR, with correlation remover showing the highest increase and resulting in a final eOR of 0.750. This mitigated model had F1-scores of 0.86, 0.904, and 0.824 in the full, Black male, and Asian female test sets, respectively. CONCLUSIONS: The ML model predicting survival after radical cystectomy exhibited bias across sex and racial subgroups. By using algorithm unfairness mitigation techniques, we improved algorithmic fairness as measured by the eOR. Our study highlights the role of not only evaluating for model bias but also actively mitigating such disparities to ensure equitable health care delivery. We also deployed the first web-based fair ML model for predicting survival after radical cystectomy.",
      "journal": "JMIR medical informatics",
      "year": "2024",
      "doi": "10.2196/63289",
      "authors": "Carbunaru Samuel et al.",
      "keywords": "algorithmic fairness; bias; bladder cancer; fairness; health equity; healthcare disparities; machine learning; model; mortality rate; prediction; radical cystectomy; survival",
      "mesh_terms": "Humans; Urinary Bladder Neoplasms; Cystectomy; Machine Learning; Female; Male; Aged; Middle Aged; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39671594/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11694706",
      "ft_text_length": 28997,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11694706)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "39671751",
      "title": "Where, why, and how is bias learned in medical image analysis models? A study of bias encoding within convolutional networks using synthetic data.",
      "abstract": "BACKGROUND: Understanding the mechanisms of algorithmic bias is highly challenging due to the complexity and uncertainty of how various unknown sources of bias impact deep learning models trained with medical images. This study aims to bridge this knowledge gap by studying where, why, and how biases from medical images are encoded in these models. METHODS: We systematically studied layer-wise bias encoding in a convolutional neural network for disease classification using synthetic brain magnetic resonance imaging data with known disease and bias effects. We quantified the degree to which disease-related information, as well as morphology-based and intensity-based biases were represented within the learned features of the model. FINDINGS: Although biases were encoded throughout the model, a stronger encoding did not necessarily lead to the model using these biases as a shortcut for disease classification. We also observed that intensity-based effects had a greater influence on shortcut learning compared to morphology-based effects when multiple biases were present. INTERPRETATION: We believe that these results constitute an important first step towards a deeper understanding of algorithmic bias in deep learning models trained using medical imaging data. This study also showcases the benefits of utilising controlled, synthetic bias scenarios for objectively studying the mechanisms of shortcut learning. FUNDING: Alberta Innovates, Natural Sciences and Engineering Research Council of Canada, Killam Trusts, Parkinson Association of Alberta, River Fund at Calgary Foundation, Canada Research Chairs Program.",
      "journal": "EBioMedicine",
      "year": "2025",
      "doi": "10.1016/j.ebiom.2024.105501",
      "authors": "Stanley Emma A M et al.",
      "keywords": "Algorithmic bias; Artificial intelligence; Synthetic data",
      "mesh_terms": "Bias; Humans; Image Processing, Computer-Assisted; Convolutional Neural Networks; Diagnostic Imaging; Algorithms; Brain; Magnetic Resonance Imaging; Databases, Factual",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39671751/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11700256",
      "ft_text_length": 45970,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11700256)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "39677419",
      "title": "Not the Models You Are Looking For: Traditional ML Outperforms LLMs in Clinical Prediction Tasks.",
      "abstract": "OBJECTIVES: To determine the extent to which current Large Language Models (LLMs) can serve as substitutes for traditional machine learning (ML) as clinical predictors using data from electronic health records (EHRs), we investigated various factors that can impact their adoption, including overall performance, calibration, fairness, and resilience to privacy protections that reduce data fidelity. MATERIALS AND METHODS: We evaluated GPT-3.5, GPT-4, and ML (as gradient-boosting trees) on clinical prediction tasks in EHR data from Vanderbilt University Medical Center and MIMIC IV. We measured predictive performance with AUROC and model calibration using Brier Score. To evaluate the impact of data privacy protections, we assessed AUROC when demographic variables are generalized. We evaluated algorithmic fairness using equalized odds and statistical parity across race, sex, and age of patients. We also considered the impact of using in-context learning by incorporating labeled examples within the prompt. RESULTS: Traditional ML (AUROC: 0.847, 0.894 (VUMC, MIMIC)) substantially outperformed GPT-3.5 (AUROC: 0.537, 0.517) and GPT-4 (AUROC: 0.629, 0.602) (with and without in-context learning) in predictive performance and output probability calibration (Brier Score (ML vs GPT-3.5 vs GPT-4): 0.134 versus 0.384 versus 0.251, 0.042 versus 0.06 versus 0.219). Traditional ML is more robust than GPT-3.5 and GPT-4 to generalizing demographic information to protect privacy. GPT-4 is the fairest model according to our selected metrics but at the cost of poor model performance. CONCLUSION: These findings suggest that LLMs are much less effective and robust than locally-trained ML for clinical prediction tasks, but they are getting better over time.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2024",
      "doi": "10.1101/2024.12.03.24318400",
      "authors": "Brown Katherine E et al.",
      "keywords": "Large language models; clinical prediction models; fairness; privacy",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39677419/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11643212",
      "ft_text_length": 28264,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11643212)",
      "ft_reason": "Included: substantial approach content (3 indicators)"
    },
    {
      "pmid": "39682584",
      "title": "Mitigating Algorithmic Bias in AI-Driven Cardiovascular Imaging for Fairer Diagnostics.",
      "abstract": "Background/Objectives: The research addresses algorithmic bias in deep learning models for cardiovascular risk prediction, focusing on fairness across demographic and socioeconomic groups to mitigate health disparities. It integrates fairness-aware algorithms, susceptible carrier-infected-recovered (SCIR) models, and interpretability frameworks to combine fairness with actionable AI insights supported by robust segmentation and classification metrics. Methods: The research utilised quantitative 3D/4D heart magnetic resonance imaging and tabular datasets from the Cardiac Atlas Project's (CAP) open challenges to explore AI-driven methodologies for mitigating algorithmic bias in cardiac imaging. The SCIR model, known for its robustness, was adapted with the Capuchin algorithm, adversarial debiasing, Fairlearn, and post-processing with equalised odds. The robustness of the SCIR model was further demonstrated in the fairness evaluation metrics, which included demographic parity, equal opportunity difference (0.037), equalised odds difference (0.026), disparate impact (1.081), and Theil Index (0.249). For interpretability, YOLOv5, Mask R-CNN, and ResNet18 were implemented with LIME and SHAP. Bias mitigation improved disparate impact (0.80 to 0.95), reduced equal opportunity difference (0.20 to 0.05), and decreased false favourable rates for males (0.0059 to 0.0033) and females (0.0096 to 0.0064) through balanced probability adjustment. Results: The SCIR model outperformed the SIR model (recovery rate: 1.38 vs 0.83) with a -10% transmission bias impact. Parameters (\u03b2=0.5, \u03b4=0.2, \u03b3=0.15) reduced susceptible counts to 2.53\u00d710-12 and increased recovered counts to 9.98 by t=50. YOLOv5 achieved high Intersection over Union (IoU) scores (94.8%, 93.7%, 80.6% for normal, severe, and abnormal cases). Mask R-CNN showed 82.5% peak confidence, while ResNet demonstrated a 10.4% accuracy drop under noise. Performance metrics (IoU: 0.91-0.96, Dice: 0.941-0.980, Kappa: 0.95) highlighted strong predictive accuracy and reliability. Conclusions: The findings validate the effectiveness of fairness-aware algorithms in addressing cardiovascular predictive model biases. The integration of fairness and explainable AI not only promotes equitable diagnostic precision but also significantly reduces diagnostic disparities across vulnerable populations. This reduction in disparities is a key outcome of the research, enhancing clinical trust in AI-driven systems. The promising results of this study pave the way for future work that will explore scalability in real-world clinical settings and address limitations such as computational complexity in large-scale data processing.",
      "journal": "Diagnostics (Basel, Switzerland)",
      "year": "2024",
      "doi": "10.3390/diagnostics14232675",
      "authors": "Sufian Md Abu et al.",
      "keywords": "LIME; Mask R-CNN; ResNet-18; SCIR model; SHAP; YOLOv5; adversarial debiasing; algorithmic bias; cardiovascular risk prediction; demographic fairness; fairness-aware AI; predictive analytics",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39682584/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11640708",
      "ft_text_length": 104591,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11640708)",
      "ft_reason": "Included: bias central + approach content (23 indicators)"
    },
    {
      "pmid": "39688772",
      "title": "A Bias Network Approach (BNA) to Encourage Ethical Reflection Among AI Developers.",
      "abstract": "We introduce the Bias Network Approach (BNA) as a sociotechnical method for AI developers to identify, map, and relate biases across the AI development process. This approach addresses the limitations of what we call the \"isolationist approach to AI bias,\" a trend in AI literature where biases are seen as separate occurrences linked to specific stages in an AI pipeline. Dealing with these multiple biases can trigger a sense of excessive overload in managing each potential bias individually or promote the adoption of an uncritical approach to understanding the influence of biases in developers' decision-making. The BNA fosters dialogue and a critical stance among developers, guided by external experts, using graphical representations to depict biased connections. To test the BNA, we conducted a pilot case study on the \"waiting list\" project, involving a small AI developer team creating a healthcare waiting list NPL model in Chile. The analysis showed promising findings: (i) the BNA aids in visualizing interconnected biases and their impacts, facilitating ethical reflection in a more accessible way; (ii) it promotes transparency in decision-making throughout AI development; and (iii) more focus is necessary on professional biases and material limitations as sources of bias in AI development.",
      "journal": "Science and engineering ethics",
      "year": "2024",
      "doi": "10.1007/s11948-024-00526-9",
      "authors": "Arriagada-Bruneau Gabriela et al.",
      "keywords": "AI bias; AI ethics; Decision-making; Professional bias; Sociotechnical",
      "mesh_terms": "Humans; Decision Making; Bias; Artificial Intelligence; Pilot Projects; Chile",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39688772/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11652403",
      "ft_text_length": 44255,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11652403)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "39702673",
      "title": "Mitigation of AI adoption bias through an improved autonomous AI system for diabetic retinal disease.",
      "abstract": "Where adopted, Autonomous artificial Intelligence (AI) for Diabetic Retinal Disease (DRD) resolves longstanding racial, ethnic, and socioeconomic disparities, but AI adoption bias persists. This preregistered trial determined sensitivity and specificity of a previously FDA authorized AI, improved to compensate for lower contrast and smaller imaged area of a widely adopted, lower cost, handheld fundus camera (RetinaVue700, Baxter Healthcare, Deerfield, IL) to identify DRD in participants with diabetes without known DRD, in primary care. In 626 participants (1252 eyes) 50.8% male, 45.7% Hispanic, 17.3% Black, DRD prevalence was 29.0%, all prespecified non-inferiority endpoints were met and no racial, ethnic or sex bias was identified, against a Wisconsin Reading Center level I prognostic standard using widefield stereoscopic photography and macular Optical Coherence Tomography. Results suggest this improved autonomous AI system can mitigate AI adoption bias, while preserving safety and efficacy, potentially contributing to rapid scaling of health access equity. ClinicalTrials.gov NCT05808699 (3/29/2023).",
      "journal": "NPJ digital medicine",
      "year": "2024",
      "doi": "10.1038/s41746-024-01389-x",
      "authors": "Abr\u00e0moff Michael D et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39702673/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11659561",
      "ft_text_length": 48110,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11659561)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39704065",
      "title": "Facilitators and Barriers to Increasing Equity in Cystic Fibrosis Newborn Screening Algorithms.",
      "abstract": "BACKGROUND: Newborn screening (NBS) for cystic fibrosis (CF) was universally implemented in the United States in 2010 to improve disease outcomes. Despite universal screening, disparities in outcomes currently exist between people with CF (PwCF) with Black/African, Asian, Indigenous, and Latino/Hispanic ancestry in comparison to PwCF of European ancestry. This is in part because CFTR panels used for newborn screening are often based on variants common in European ancestries leading to higher rates of false negatives for PwCF from minoritized racial and ethnic groups. METHODS: This study investigated how states evaluate and update their CFNBS algorithms through semi-structured interviews with professionals from four states with ethnically diverse populations and one national consultant. Interviews were transcribed verbatim and analyzed through inductive thematic analysis. RESULTS: Five themes were identified encompassing facilitators, barriers, and motivations for evaluating and updating CF NBS algorithms. Facilitators of effective evaluation and updating of algorithms included effective communication with CF clinical centers and extensive support for CF as compared to other conditions. Although participants stated that their respective NBS programs were aware of the disparate impact of their CF panels on PwCF from minoritized racial and ethnic groups, motivations to decrease this disparity were hampered by a range of funding and logistical barriers, such as limited information about false negative cases and difficulties incorporating next generation sequencing technology. CONCLUSIONS: This study shed light on the experiences of states considering alterations to their CFNBS panels, revealing several key barriers and facilitators to implementing equitable CFNBS algorithms.",
      "journal": "Pediatric pulmonology",
      "year": "2025",
      "doi": "10.1002/ppul.27449",
      "authors": "Madden Kellyn et al.",
      "keywords": "cystic fibrosis; health disparities; health equity; newborn screening",
      "mesh_terms": "Humans; Cystic Fibrosis; Neonatal Screening; Infant, Newborn; Algorithms; United States; Healthcare Disparities; Female; Male; Cystic Fibrosis Transmembrane Conductance Regulator",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39704065/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11984462",
      "ft_text_length": 1781,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11984462)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39716098",
      "title": "A comprehensive and bias-free machine learning approach for risk prediction of preeclampsia with severe features in a nulliparous study cohort.",
      "abstract": "Preeclampsia is one of the leading causes of maternal morbidity, with consequences during and after pregnancy. Because of its diverse clinical presentation, preeclampsia is an adverse pregnancy outcome that is uniquely challenging to predict and manage. In this paper, we developed racial bias-free machine learning models that predict the onset of preeclampsia with severe features or eclampsia at discrete time points in a nulliparous pregnant study cohort. To focus on those most at risk, we selected probands with severe PE (sPE). Those with mild preeclampsia, superimposed preeclampsia, and new onset hypertension were excluded.The prospective study cohort to which we applied machine learning is the Nulliparous Pregnancy Outcomes Study: Monitoring Mothers-to-be (nuMoM2b) study, which contains information from eight clinical sites across the US. Maternal serum samples were collected for 1,857 individuals between the first and second trimesters. These patients with serum samples collected are selected as the final cohort.Our prediction models achieved an AUROC of 0.72 (95% CI, 0.69-0.76), 0.75 (95% CI, 0.71-0.79), and 0.77 (95% CI, 0.74-0.80), respectively, for the three visits. Our initial models were biased toward non-Hispanic black participants with a high predictive equality ratio of 1.31. We corrected this bias and reduced this ratio to 1.14. This lowers the rate of false positives in our predictive model for the non-Hispanic black participants. The exact cause of the bias is still under investigation, but previous studies have recognized PLGF as a potential bias-inducing factor. However, since our model includes various factors that exhibit a positive correlation with PLGF, such as blood pressure measurements and BMI, we have employed an algorithmic approach to disentangle this bias from the model.The top features of our built model stress the importance of using several tests, particularly for biomarkers (BMI and blood pressure measurements) and ultrasound measurements. Placental analytes (PLGF and Endoglin) were strong predictors for screening for the early onset of preeclampsia with severe features in the first two trimesters.",
      "journal": "BMC pregnancy and childbirth",
      "year": "2024",
      "doi": "10.1186/s12884-024-06988-w",
      "authors": "Lin Yun C et al.",
      "keywords": "Ensemble\u00a0model; Fairness in machine learning; Machine learning; PlGF; Preeclampsia; Preeclampsia with severe features",
      "mesh_terms": "Humans; Pregnancy; Female; Pre-Eclampsia; Machine Learning; Adult; Prospective Studies; Parity; Risk Assessment; Placenta Growth Factor; Risk Factors; Biomarkers; Severity of Illness Index; Cohort Studies; Pregnancy Trimester, First; Pregnancy Trimester, Second",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39716098/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11667971",
      "ft_text_length": 45682,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11667971)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "39738228",
      "title": "Attention-guided convolutional network for bias-mitigated and interpretable oral lesion classification.",
      "abstract": "Accurate diagnosis of oral lesions, early indicators of oral cancer, is a complex clinical challenge. Recent advances in deep learning have demonstrated potential in supporting clinical decisions. This paper introduces a deep learning model for classifying oral lesions, focusing on accuracy, interpretability, and reducing dataset bias. The model integrates three components: (i) a Classification Stream, utilizing a CNN to categorize images into 16 lesion types (baseline model), (ii) a Guidance Stream, which aligns class activation maps with clinically relevant areas using ground truth segmentation masks (GAIN model), and (iii) an Anatomical Site Prediction Stream, improving interpretability by predicting lesion location (GAIN+ASP model). The development dataset comprised 2765 intra-oral digital images of 16 lesion types from 1079 patients seen at an oral pathology clinic between 1999 and 2021. The GAIN model demonstrated a 7.2% relative improvement in accuracy over the baseline for 16-class classification, with superior class-specific balanced accuracy and AUC scores. Additionally, the GAIN model enhanced lesion localization and improved the alignment between attention maps and ground truth. The proposed models also exhibited greater robustness against dataset bias, as shown in ablation studies.",
      "journal": "Scientific reports",
      "year": "2024",
      "doi": "10.1038/s41598-024-81724-0",
      "authors": "Patel Adeetya et al.",
      "keywords": "Bias mitigation; CNN; Guided attention inference network; Interpretability; Oral lesion diagnosis",
      "mesh_terms": "Humans; Mouth Neoplasms; Deep Learning; Neural Networks, Computer; Bias; Image Processing, Computer-Assisted",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39738228/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11685657",
      "ft_text_length": 52915,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11685657)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "39738436",
      "title": "Assessing the documentation of publicly available medical image and signal datasets and their impact on bias using the BEAMRAD tool.",
      "abstract": "Medical datasets are vital for advancing Artificial Intelligence (AI) in healthcare. Yet biases in these datasets on which deep-learning models are trained can compromise reliability. This study investigates biases stemming from dataset-creation practices. Drawing on existing guidelines, we first developed a BEAMRAD tool to assess the documentation of public Magnetic Resonance Imaging (MRI); Color Fundus Photography (CFP), and Electrocardiogram (ECG) datasets. In doing so, we provide an overview of the biases that may emerge due to inadequate dataset documentation. Second, we examine the current state of documentation for public medical images and signal data. Our research reveals that there is substantial variance in the documentation of image and signal datasets, even though guidelines have been developed in medical imaging. This indicates that dataset documentation is subject to individual discretionary decisions. Furthermore, we find that aspects such as hardware and data acquisition details are commonly documented, while information regarding data annotation practices, annotation error quantification, or data limitations are not consistently reported. This risks having considerable implications for the abilities of data users to detect potential sources of bias through these respective aspects and develop reliable and robust models that can be adapted for clinical practice.",
      "journal": "Scientific reports",
      "year": "2024",
      "doi": "10.1038/s41598-024-83218-5",
      "authors": "Galanty Maria et al.",
      "keywords": "",
      "mesh_terms": "Humans; Magnetic Resonance Imaging; Documentation; Bias; Electrocardiography; Deep Learning; Artificial Intelligence; Diagnostic Imaging; Datasets as Topic; Reproducibility of Results",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39738436/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11686007",
      "ft_text_length": 57322,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11686007)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "39767481",
      "title": "AI in Biomedicine-A Forward-Looking Perspective on Health Equity.",
      "abstract": "As new artificial intelligence (AI) tools are being developed and as AI continues to revolutionize healthcare, its potential to advance health equity is increasingly recognized. The 2024 Research Centers in Minority Institutions (RCMI) Consortium National Conference session titled \"Artificial Intelligence: Safely, Ethically, and Responsibly\" brought together experts from diverse institutions to explore AI's role and challenges in advancing health equity. This report summarizes presentations and discussions from the conference focused on AI's potential and its challenges, particularly algorithmic bias, transparency, and the under-representation of minority groups in AI datasets. Key topics included AI's predictive and generative capabilities in healthcare, ethical governance, and key national initiatives, like AIM-AHEAD. The session highlighted the critical role of RCMI institutions in fostering diverse AI/machine learning research and in developing culturally competent AI tools. Other discussions included AI's capacity to improve patient outcomes, especially for underserved communities, and underscored the necessity for robust ethical standards, a diverse AI and scientific workforce, transparency, and inclusive data practices. The engagement of RCMI institutions is critical to ensure practices in AI development and deployment which prioritize health equity, thus paving the way for a more inclusive AI-driven healthcare system.",
      "journal": "International journal of environmental research and public health",
      "year": "2024",
      "doi": "10.3390/ijerph21121642",
      "authors": "Kumar Deepak et al.",
      "keywords": "RCMI; artificial intelligence; augmented intelligence; health disparities; health equity; health ethics; machine learning",
      "mesh_terms": "Health Equity; Artificial Intelligence; Humans; Biomedical Research",
      "pub_types": "Journal Article; Conference Proceedings",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39767481/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11675414",
      "ft_text_length": 20013,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11675414)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39803613",
      "title": "Validation, bias assessment, and optimization of the UNAFIED 2-year risk prediction model for undiagnosed atrial fibrillation using national electronic health data.",
      "abstract": "BACKGROUND: Prediction models for atrial fibrillation (AF) may enable earlier detection and guideline-directed treatment decisions. However, model bias may lead to inaccurate predictions and unintended consequences. OBJECTIVE: The purpose of this study was to validate, assess bias, and improve generalizability of \"UNAFIED-10,\" a 2-year, 10-variable predictive model of undiagnosed AF in a national data set (originally developed using the Indiana Network for Patient Care regional data). METHODS: UNAFIED-10 was validated and optimized using Optum de-identified electronic health record data set. AF diagnoses were recorded in the January 2018-December 2019 period (outcome period), with January 2016-December 2017 as the baseline period. Validation cohorts (patients with AF and non-AF controls, aged \u226540 years) comprised the full imbalanced and randomly sampled balanced data sets. Model performance and bias in patient subpopulations based on sex, insurance, race, and region were evaluated. RESULTS: Of the 6,058,657 eligible patients (mean age 60 \u00b1 12 years), 4.1% (n = 246,975) had their first AF diagnosis within the outcome period. The validated UNAFIED-10 model achieved a higher C-statistic (0.85 [95% confidence interval 0.85-0.86] vs 0.81 [0.80-0.81]) and sensitivity (86% vs 74%) but lower specificity (66% vs 74%) than the original UNAFIED-10 model. During retraining and optimization, the variables insurance, shock, and albumin were excluded to address bias and improve generalizability. This generated an 8-variable model (UNAFIED-8) with consistent performance. CONCLUSION: UNAFIED-10, developed using regional patient data, displayed consistent performance in a large national data set. UNAFIED-8 is more parsimonious and generalizable for using advanced analytics for AF detection. Future directions include validation on additional data sets.",
      "journal": "Heart rhythm O2",
      "year": "2024",
      "doi": "10.1016/j.hroo.2024.09.010",
      "authors": "Ateya Mohammad et al.",
      "keywords": "Atrial fibrillation; Electronic health record; Machine learning; Predictive model; Screening",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39803613/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11721729",
      "ft_text_length": 37367,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11721729)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "39825353",
      "title": "Mitigating bias in AI mortality predictions for minority populations: a transfer learning approach.",
      "abstract": "BACKGROUND: The COVID-19 pandemic has highlighted the crucial role of artificial intelligence (AI) in predicting mortality and guiding healthcare decisions. However, AI models may perpetuate or exacerbate existing health disparities due to demographic biases, particularly affecting racial and ethnic minorities. The objective of this study is to investigate the demographic biases in AI models predicting COVID-19 mortality and to assess the effectiveness of transfer learning in improving model fairness across diverse demographic groups. METHODS: This retrospective cohort study used a population-based dataset of COVID-19 cases from the Centers for Disease Control and Prevention (CDC), spanning the years 2020-2024. The study analyzed AI model performance across different racial and ethnic groups and employed transfer learning techniques to improve model fairness by adapting pre-trained models to the specific demographic and clinical characteristics of the population. RESULTS: Decision Tree (DT) and Random Forest (RF) models consistently showed improvements in accuracy, precision, and ROC-AUC scores for Non-Hispanic Black, Hispanic/Latino, and Asian populations. The most significant precision improvement was observed in the DT model for Hispanic/Latino individuals, which increased from 0.3805 to 0.5265. The precision for Asians or Pacific Islanders in the DT model increased from 0.4727 to 0.6071, and for Non-Hispanic Blacks, it rose from 0.5492 to 0.6657. Gradient Boosting Machines (GBM) produced mixed results, showing accuracy and precision improvements for Non-Hispanic Black and Asian groups, but declines for the Hispanic/Latino and American Indian groups, with the most significant decline in precision, which dropped from 0.4612 to 0.2406 in the American Indian group. Logistic Regression (LR) demonstrated minimal changes across all metrics and groups. For the Non-Hispanic American Indian group, most models showed limited benefits, with several performance metrics either remaining stable or declining. CONCLUSIONS: This study demonstrates the potential of AI in predicting COVID-19 mortality while also underscoring the critical need to address demographic biases. The application of transfer learning significantly improved the predictive performance of models across various racial and ethnic groups, suggesting these techniques are effective in mitigating biases and promoting fairness in AI models.",
      "journal": "BMC medical informatics and decision making",
      "year": "2025",
      "doi": "10.1186/s12911-025-02862-7",
      "authors": "Gu Tianshu et al.",
      "keywords": "Artificial intelligence; COVID-19; Demographic bias; Health disparities; Mortality prediction; Transfer learning",
      "mesh_terms": "Humans; COVID-19; Retrospective Studies; Artificial Intelligence; Minority Groups; United States; Male; Female; Middle Aged; Machine Learning; Adult; Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39825353/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11742213",
      "ft_text_length": 29348,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11742213)",
      "ft_reason": "Included: bias central + approach content (13 indicators)"
    },
    {
      "pmid": "39831110",
      "title": "Bias in Prediction Models to Identify Patients With Colorectal Cancer at High Risk for Readmission After Resection.",
      "abstract": "PURPOSE: Machine learning algorithms are used for predictive modeling in medicine, but studies often do not evaluate or report on the potential biases of the models. Our purpose was to develop clinical prediction models for readmission after surgery in colorectal cancer (CRC) patients and to examine their potential for racial bias. METHODS: We used the 2012-2020 American College of Surgeons' National Surgical Quality Improvement Program (ACS-NSQIP) Participant Use File and Targeted Colectomy File. Patients were categorized into four race groups - White, Black or African American, Other, and Unknown/Not Reported. Potential predictive features were identified from studies of risk factors of 30-day readmission in CRC patients. We compared four machine learning-based methods - logistic regression (LR), multilayer perceptron (MLP), random forest (RF), and XGBoost (XGB). Model bias was assessed using false negative rate (FNR) difference, false positive rate (FPR) difference, and disparate impact. RESULTS: In all, 112,077 patients were included, 67.2% of whom were White, 9.2% Black, 5.6% Other race, and 18% with race not recorded. There were significant differences in the AUROC, FPR and FNR between race groups across all models. Notably, patients in the 'Other' race category had higher FNR compared to Black patients in all but the XGB model, while Black patients had higher FPR than White patients in some models. Patients in the 'Other' category consistently had the lowest FPR. Applying the 80% rule for disparate impact, the models consistently met the threshold for unfairness for the 'Other' race category. CONCLUSION: Predictive models for 30-day readmission after colorectal surgery may perform unequally for different race groups, potentially propagating to inequalities in delivery of care and patient outcomes if the predictions from these models are used to direct care.",
      "journal": "JCO clinical cancer informatics",
      "year": "2024",
      "doi": "10.1200/CCI.23.00194",
      "authors": "Lucas Mary M et al.",
      "keywords": "",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Bias; Black or African American; Colectomy; Colorectal Neoplasms; Machine Learning; Patient Readmission; Risk Assessment; Risk Factors; White",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39831110/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11741203",
      "ft_text_length": 1896,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11741203)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39836496",
      "title": "Development of secure infrastructure for advancing generative artificial intelligence research in healthcare at an academic medical center.",
      "abstract": "BACKGROUND: Generative AI, particularly large language models (LLMs), holds great potential for improving patient care and operational efficiency in healthcare. However, the use of LLMs is complicated by regulatory concerns around data security and patient privacy. This study aimed to develop and evaluate a secure infrastructure that allows researchers to safely leverage LLMs in healthcare while ensuring HIPAA compliance and promoting equitable AI. MATERIALS AND METHODS: We implemented a private Azure OpenAI Studio deployment with secure API-enabled endpoints for researchers. Two use cases were explored, detecting falls from electronic health records (EHR) notes and evaluating bias in mental health prediction using fairness-aware prompts. RESULTS: The framework provided secure, HIPAA-compliant API access to LLMs, allowing researchers to handle sensitive data safely. Both use cases highlighted the secure infrastructure's capacity to protect sensitive patient data while supporting innovation. DISCUSSION AND CONCLUSION: This centralized platform presents a scalable, secure, and HIPAA-compliant solution for healthcare institutions aiming to integrate LLMs into clinical research.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2025",
      "doi": "10.1093/jamia/ocaf005",
      "authors": "Ng Madelena Y et al.",
      "keywords": "artificial intelligence; healthcare informatics infrastructure; large language models; large-scale research; privacy; security; technology adoption",
      "mesh_terms": "Artificial Intelligence; Academic Medical Centers; Electronic Health Records; Computer Security; Humans; Health Insurance Portability and Accountability Act; United States; Confidentiality; Generative Artificial Intelligence",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39836496/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11833461",
      "ft_text_length": 11235,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11833461)",
      "ft_reason": "Included: substantial approach content (3 indicators)"
    },
    {
      "pmid": "39842001",
      "title": "Discrimination of Radiologists' Experience Level Using Eye-Tracking Technology and Machine Learning: Case Study.",
      "abstract": "BACKGROUND: Perception-related errors comprise most diagnostic mistakes in radiology. To mitigate this problem, radiologists use personalized and high-dimensional visual search strategies, otherwise known as search patterns. Qualitative descriptions of these search patterns, which involve the physician verbalizing or annotating the order he or she analyzes the image, can be unreliable due to discrepancies in what is reported versus the actual visual patterns. This discrepancy can interfere with quality improvement interventions and negatively impact patient care. OBJECTIVE: The objective of this study is to provide an alternative method for distinguishing between radiologists by means of captured eye-tracking data such that the raw gaze (or processed fixation data) can be used to discriminate users based on subconscious behavior in visual inspection. METHODS: We present a novel discretized feature encoding based on spatiotemporal binning of fixation data for efficient geometric alignment and temporal ordering of eye movement when reading chest x-rays. The encoded features of the eye-fixation data are used by machine learning classifiers to discriminate between faculty and trainee radiologists. A clinical trial case study was conducted using metrics such as the area under the curve, accuracy, F1-score, sensitivity, and specificity to evaluate the discriminability between the 2 groups regarding their level of experience. The classification performance was then compared with state-of-the-art methodologies. In addition, a repeatability experiment using a separate dataset, experimental protocol, and eye tracker was performed with 8 participants to evaluate the robustness of the proposed approach. RESULTS: The numerical results from both experiments demonstrate that classifiers using the proposed feature encoding methods outperform the current state-of-the-art in differentiating between radiologists in terms of experience level. An average performance gain of 6.9% is observed compared with traditional features while classifying experience levels of radiologists. This gain in accuracy is also substantial across different eye tracker-collected datasets, with improvements of 6.41% using the Tobii eye tracker and 7.29% using the EyeLink eye tracker. These results signify the potential impact of the proposed method for identifying radiologists' level of expertise and those who would benefit from additional training. CONCLUSIONS: The effectiveness of the proposed spatiotemporal discretization approach, validated across diverse datasets and various classification metrics, underscores its potential for objective evaluation, informing targeted interventions and training strategies in radiology. This research advances reliable assessment tools, addressing challenges in perception-related errors to enhance patient care outcomes.",
      "journal": "JMIR formative research",
      "year": "2025",
      "doi": "10.2196/53928",
      "authors": "Martinez Stanford et al.",
      "keywords": "classification; education; experience; experience level determination; eye movement; eye-tracking; fixation; gaze; image; machine learning; radiology; radiology education; search pattern; search pattern feature extraction; spatio-temporal; x-ray",
      "mesh_terms": "Humans; Machine Learning; Eye-Tracking Technology; Radiologists; Clinical Competence; Diagnostic Errors; Eye Movements",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39842001/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11799805",
      "ft_text_length": 42212,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11799805)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39875475",
      "title": "Large language models improve the identification of emergency department visits for symptomatic kidney stones.",
      "abstract": "Recent advancements of large language models (LLMs) like generative pre-trained transformer 4 (GPT-4) have generated significant interest among the scientific community. Yet, the potential of these models to be utilized in clinical settings remains largely unexplored. In this study, we investigated the abilities of multiple LLMs and traditional machine learning models to analyze emergency department (ED) reports and determine if the corresponding visits were due to symptomatic kidney stones. Leveraging a dataset of manually annotated ED reports, we developed strategies to enhance LLMs including prompt optimization, zero- and few-shot prompting, fine-tuning, and prompt augmentation. Further, we implemented fairness assessment and bias mitigation methods to investigate the potential disparities by LLMs with respect to race and gender. A clinical expert manually assessed the explanations generated by GPT-4 for its predictions to determine if they were sound, factually correct, unrelated to the input prompt, or potentially harmful. The best results were achieved by GPT-4 (macro-F1\u2009=\u20090.833, 95% confidence interval [CI] 0.826-0.841) and GPT-3.5 (macro-F1\u2009=\u20090.796, 95% CI 0.796-0.796). Ablation studies revealed that the initial pre-trained GPT-3.5 model benefits from fine-tuning. Adding demographic information and prior disease history to the prompts allows LLMs to make better decisions. Bias assessment found that GPT-4 exhibited no racial or gender disparities, in contrast to GPT-3.5, which failed to effectively model racial diversity.",
      "journal": "Scientific reports",
      "year": "2025",
      "doi": "10.1038/s41598-025-86632-5",
      "authors": "Bejan Cosmin A et al.",
      "keywords": "GPT-3.5; GPT-4; Kidney stones; LLMs; Large language models; Llama-2; Nephrolithiasis",
      "mesh_terms": "Humans; Kidney Calculi; Emergency Service, Hospital; Female; Male; Machine Learning; Middle Aged; Emergency Room Visits; Large Language Models",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39875475/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11775227",
      "ft_text_length": 31580,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11775227)",
      "ft_reason": "Included: substantial approach content (10 indicators)"
    },
    {
      "pmid": "39881067",
      "title": "Fairness in Low Birthweight Predictive Models: Implications of Excluding Race/Ethnicity.",
      "abstract": "CONTEXT: To evaluate algorithmic fairness in low birthweight predictive models. STUDY DESIGN: This study analyzed insurance claims (n\u2009=\u20099,990,990; 2013-2021) linked with birth certificates (n\u2009=\u2009173,035; 2014-2021) from the Arkansas All Payers Claims Database (APCD). METHODS: Low birthweight (<\u20092500\u00a0g) predictive models included four approaches (logistic, elastic net, linear discriminate analysis, and gradient boosting machines [GMB]) with and without racial/ethnic information. Model performance was assessed overall, among Hispanic individuals, and among non-Hispanic White, Black, Native Hawaiian/Other Pacific Islander, and Asian individuals using multiple measures of predictive performance (i.e., AUC [area under the receiver operating characteristic curve] scores, calibration, sensitivity, and specificity). RESULTS: AUC scores were lower (underperformed) for Black and Asian individuals relative to White individuals. In the strongest performing model (i.e., GMB), the AUC scores for Black (0.718 [95% CI: 0.705-0.732]) and Asian (0.655 [95% CI: 0.582-0.728]) populations were lower than the AUC for White individuals (0.764 [95% CI: 0.754-0.775 ]). Model performance measured using AUC was comparable in models that included and excluded race/ethnicity; however, sensitivity (i.e., the percent of records correctly predicted as \"low birthweight\" among those who actually had low birthweight) was lower and calibration was weaker, suggesting underprediction for Black individuals when race/ethnicity were excluded. CONCLUSIONS: This study found that racially blind models resulted in underprediction and reduced algorithmic performance, measured using sensitivity and calibration, for Black populations. Such under prediction could unfairly decrease resource allocation needed to reduce perinatal health inequities. Population health management programs should carefully consider algorithmic fairness in predictive models and associated resource allocation decisions.",
      "journal": "Journal of racial and ethnic health disparities",
      "year": "2025",
      "doi": "10.1007/s40615-025-02296-x",
      "authors": "Brown Clare C et al.",
      "keywords": "Equity; Algorithmic fairness; Low birthweight",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39881067/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12304234",
      "ft_text_length": 1977,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12304234)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39883934",
      "title": "Designing Health Recommender Systems to Promote Health Equity: A Socioecological Perspective.",
      "abstract": "Health recommender systems (HRS) have the capability to improve human-centered care and prevention by personalizing content, such as health interventions or health information. HRS, an emerging and developing field, can play a unique role in the digital health field as they can offer relevant recommendations, not only based on what users themselves prefer and may be receptive to, but also using data about wider spheres of influence over human behavior, including peers, families, communities, and societies. We identify and discuss how HRS could play a unique role in decreasing health inequities. We use the socioecological model, which provides representations of how multiple, nested levels of influence (eg, community, institutional, and policy factors) interact to shape individual health. This perspective helps illustrate how HRS could address not just individual health factors but also the structural barriers-such as access to health care, social support, and access to healthy food-that shape health outcomes at various levels. Based on this analysis, we then discuss the challenges and future research priorities. We find that despite the potential for targeting more complex systemic challenges to obtaining good health, current HRS are still focused on individual health behaviors, often do not integrate the lived experiences of users in the design, and have had limited reach and effectiveness for individuals from low socioeconomic status and racial or ethnic minoritized backgrounds. In this viewpoint, we argue that a new design paradigm is necessary in which HRS focus on incorporating structural barriers to good health in addition to user preferences. HRS should be designed with an emphasis on health systems, which also includes incorporating decolonial perspectives of well-being that challenge prevailing medical models. Furthermore, potential lies in evaluating the health equity effects of HRS and leveraging collected data to influence policy. With changes in practices and with an intentional equity focus, HRS could play a crucial role in health promotion and decreasing health inequities.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/60138",
      "authors": "Figueroa Caroline A et al.",
      "keywords": "AI; artificial intelligence; digital devices; digital health; digital health intervention; health behavior; health behaviors; health equity; health inequities; health promotion; health recommender systems; patient centric; socioecological",
      "mesh_terms": "Humans; Health Equity; Health Promotion; Health Services Accessibility",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39883934/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11826946",
      "ft_text_length": 22253,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11826946)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39886186",
      "title": "From prediction to practice: mitigating bias and data shift in machine-learning models for chemotherapy-induced organ dysfunction across unseen cancers.",
      "abstract": "OBJECTIVES: Routine monitoring of renal and hepatic function during chemotherapy ensures that treatment-related organ damage has not occurred and clearance of subsequent treatment is not hindered; however, frequency and timing are not optimal. Model bias and data heterogeneity concerns have hampered the ability of machine learning (ML) to be deployed into clinical practice. This study aims to develop models that could support individualised decisions on the timing of renal and hepatic monitoring while exploring the effect of data shift on model performance. METHODS AND ANALYSIS: We used retrospective data from three UK hospitals to develop and validate ML models predicting unacceptable rises in creatinine/bilirubin post cycle 3 for patients undergoing treatment for the following cancers: breast, colorectal, lung, ovarian and diffuse large B-cell lymphoma. RESULTS: We extracted 3614 patients with no missing blood test data across cycles 1-6 of chemotherapy treatment. We improved on previous work by including predictions post cycle 3. Optimised for sensitivity, we achieve F2 scores of 0.7773 (bilirubin) and 0.6893 (creatinine) on unseen data. Performance is consistent on tumour types unseen during training (F2 bilirubin: 0.7423, F2 creatinine: 0.6820). CONCLUSION: Our technique highlights the effectiveness of ML in clinical settings, demonstrating the potential to improve the delivery of care. Notably, our ML models can generalise to unseen tumour types. We propose gold-standard bias mitigation steps for ML models: evaluation on multisite data, thorough patient population analysis, and both formalised bias measures and model performance comparisons on patient subgroups. We demonstrate that data aggregation techniques have unintended consequences on model bias.",
      "journal": "BMJ oncology",
      "year": "2024",
      "doi": "10.1136/bmjonc-2024-000430",
      "authors": "Watson Matthew et al.",
      "keywords": "Adverse effects; Chemotherapy",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39886186/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11557724",
      "ft_text_length": 35543,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11557724)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "39901187",
      "title": "Accounting for racial bias and social determinants of health in a model of hypertension control.",
      "abstract": "BACKGROUND: Hypertension control remains a critical problem and most of the existing literature views it from a clinical perspective, overlooking the role of sociodemographic factors. This study aims to identify patients with not well-controlled hypertension using readily available demographic and socioeconomic features and elucidate important predictive variables. METHODS: In this retrospective cohort study, records from 1/1/2012 to 1/1/2020 at the Boston Medical Center were used. Patients with either a hypertension diagnosis or related records (\u2265\u2009130\u00a0mmHg systolic or\u2009\u2265\u200990\u00a0mmHg diastolic, n\u2009=\u2009164,041) were selected. Models were developed to predict which patients had uncontrolled hypertension defined as systolic blood pressure (SBP) records exceeding 160\u00a0mmHg. RESULTS: The predictive model of high SBP reached an Area Under the Receiver Operating Characteristic Curve of 74.49%\u2009\u00b1\u20090.23%. Age, race, Social Determinants of Health (SDoH), mental health, and cigarette use were predictive of high SBP. Being Black or having critical social needs led to higher probability of uncontrolled SBP. To mitigate model bias and elucidate differences in predictive variables, two separate models were trained for Black and White patients. Black patients face a 4.7 \u00d7 higher False Positive Rate (FPR) and a 0.58 \u00d7 lower False Negative Rate (FNR) compared to White patients. Decision threshold differentiation was implemented to equalize FNR. Race-specific models revealed different sets of social variables predicting high SBP, with Black patients being affected by structural barriers (e.g., food and transportation) and White patients by personal and demographic factors (e.g., marital status). CONCLUSIONS: Models using non-clinical factors can predict which patients exhibit poorly controlled hypertension. Racial and SDoH variables are significant predictors but lead to biased predictive models. Race-specific models are not sufficient to resolve such biases and require further decision threshold tuning. A host of structural socioeconomic factors are identified to be targeted to reduce disparities in hypertension control.",
      "journal": "BMC medical informatics and decision making",
      "year": "2025",
      "doi": "10.1186/s12911-025-02873-4",
      "authors": "Hu Yang et al.",
      "keywords": "Hypertension; Machine learning; Racial bias; Social determinants of health",
      "mesh_terms": "Humans; Hypertension; Social Determinants of Health; Female; Middle Aged; Male; Retrospective Studies; Aged; Racism; Adult; Boston; Black or African American; White",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39901187/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11792567",
      "ft_text_length": 42105,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11792567)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39904407",
      "title": "Evaluating the Bias, type I error and statistical power of the prior Knowledge-Guided integrated likelihood estimation (PIE) for bias reduction in EHR based association studies.",
      "abstract": "OBJECTIVES: Binary outcomes in electronic health records (EHR) derived using automated phenotype algorithms may suffer from phenotyping error, resulting in bias in association estimation. Huang et al. [1] proposed the Prior Knowledge-Guided Integrated Likelihood Estimation (PIE) method to mitigate the estimation bias, however, their investigation focused on point estimation without statistical inference, and the evaluation of PIE therein using simulation was a proof-of-concept with only a limited scope of scenarios. This study aims to comprehensively assess PIE's performance including (1) how well PIE performs under a wide spectrum of operating characteristics of phenotyping algorithms under real-world scenarios (e.\u00a0g., low prevalence, low sensitivity, high specificity); (2) beyond point estimation, how much variation of the PIE estimator was introduced by the prior distribution; and (3) from a hypothesis testing point of view, if PIE improves type I error and statistical power relative to the na\u00efve method (i.e., ignoring the phenotyping error). METHODS: Synthetic data and use-case analysis were utilized to evaluate PIE. The synthetic data were generated under diverse outcome prevalence, phenotyping algorithm sensitivity, and association effect sizes. Simulation studies compared PIE under different prior distributions with the na\u00efve method, assessing bias, variance, type I error, and power. Use-case analysis compared the performance of PIE and the na\u00efve method in estimating the association of multiple predictors with COVID-19 infection. RESULTS: PIE exhibited reduced bias compared to the na\u00efve method across varied simulation settings, with comparable type I error and power. As the effect size became larger, the bias reduced by PIE was larger. PIE has superior performance when prior distributions aligned closely with true phenotyping algorithm characteristics. Impact of prior quality was minor for low-prevalence outcomes but large for common outcomes. In use-case analysis, PIE maintains a relatively accurate estimation across different scenarios, particularly outperforming the na\u00efve approach under large effect sizes. CONCLUSION: PIE effectively mitigates estimation bias in a wide spectrum of real-world settings, particularly with accurate prior information. Its main benefit lies in bias reduction rather than hypothesis testing. The impact of the prior is small for low-prevalence outcomes.",
      "journal": "Journal of biomedical informatics",
      "year": "2025",
      "doi": "10.1016/j.jbi.2025.104787",
      "authors": "Jing Naimin et al.",
      "keywords": "Association study; Bias reduction; Electronic health record; Phenotyping error",
      "mesh_terms": "Electronic Health Records; Humans; Algorithms; Likelihood Functions; Bias; COVID-19; Phenotype; SARS-CoV-2; Computer Simulation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39904407/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12180398",
      "ft_text_length": 17787,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12180398)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39910290",
      "title": "An institutional framework to support ethical fair and equitable artificial intelligence augmented care.",
      "abstract": "Coordinated access to multi-domain health data can facilitate the development and implementation of artificial intelligence-augmented clinical decision support (AI-CDS). However, scalable institutional frameworks supporting these activities are lacking. We present the PULSE framework, aimed to establish an integrative and ethically governed ecosystem for the patient-guided, patient-contextualized use of multi-domain health data for AI-augmented care. We describe deliverables related to stakeholder engagement and infrastructure development to support routine engagement of patients for consent-guided data abstraction, pre-processing, and cloud migration to support AI-CDS model development and surveillance. Central focus is placed on the routine collection of social determinants of health and patient self-reported health status to contextualize and evaluate models for fair and equitable use. Inaugural feasibility is reported for over 30,000 consecutively engaged patients. The described framework, conceptually developed to support a multi-site cardiovascular institute, is translatable to other disease domains, offering a validated architecture for use by large-scale tertiary care institutions.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01490-9",
      "authors": "Dykstra Steven et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39910290/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11799513",
      "ft_text_length": 36589,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11799513)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "39919295",
      "title": "The Efficacy of Conversational AI in Rectifying the Theory-of-Mind and Autonomy Biases: Comparative Analysis.",
      "abstract": "BACKGROUND: The increasing deployment of conversational artificial intelligence (AI) in mental health interventions necessitates an evaluation of their efficacy in rectifying cognitive biases and recognizing affect in human-AI interactions. These biases are particularly relevant in mental health contexts as they can exacerbate conditions such as depression and anxiety by reinforcing maladaptive thought patterns or unrealistic expectations in human-AI interactions. OBJECTIVE: This study aimed to assess the effectiveness of therapeutic chatbots (Wysa and Youper) versus general-purpose language models (GPT-3.5, GPT-4, and Gemini Pro) in identifying and rectifying cognitive biases and recognizing affect in user interactions. METHODS: This study used constructed case scenarios simulating typical user-bot interactions to examine how effectively chatbots address selected cognitive biases. The cognitive biases assessed included theory-of-mind biases (anthropomorphism, overtrust, and attribution) and autonomy biases (illusion of control, fundamental attribution error, and just-world hypothesis). Each chatbot response was evaluated based on accuracy, therapeutic quality, and adherence to cognitive behavioral therapy principles using an ordinal scale to ensure consistency in scoring. To enhance reliability, responses underwent a double review process by 2 cognitive scientists, followed by a secondary review by a clinical psychologist specializing in cognitive behavioral therapy, ensuring a robust assessment across interdisciplinary perspectives. RESULTS: This study revealed that general-purpose chatbots outperformed therapeutic chatbots in rectifying cognitive biases, particularly in overtrust bias, fundamental attribution error, and just-world hypothesis. GPT-4 achieved the highest scores across all biases, whereas the therapeutic bot Wysa scored the lowest. Notably, general-purpose bots showed more consistent accuracy and adaptability in recognizing and addressing bias-related cues across different contexts, suggesting a broader flexibility in handling complex cognitive patterns. In addition, in affect recognition tasks, general-purpose chatbots not only excelled but also demonstrated quicker adaptation to subtle emotional nuances, outperforming therapeutic bots in 67% (4/6) of the tested biases. CONCLUSIONS: This study shows that, while therapeutic chatbots hold promise for mental health support and cognitive bias intervention, their current capabilities are limited. Addressing cognitive biases in AI-human interactions requires systems that can both rectify and analyze biases as integral to human cognition, promoting precision and simulating empathy. The findings reveal the need for improved simulated emotional intelligence in chatbot design to provide adaptive, personalized responses that reduce overreliance and encourage independent coping skills. Future research should focus on enhancing affective response mechanisms and addressing ethical concerns such as bias mitigation and data privacy to ensure safe, effective AI-based mental health support.",
      "journal": "JMIR mental health",
      "year": "2025",
      "doi": "10.2196/64396",
      "authors": "Rz\u0105deczka Marcin et al.",
      "keywords": "AI; affect recognition; artificial intelligence; bias rectification; chatbots; cognitive bias; conversational artificial intelligence; digital mental health",
      "mesh_terms": "Humans; Artificial Intelligence; Theory of Mind; Communication; Personal Autonomy; Adult; Male; Cognitive Behavioral Therapy; Female",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39919295/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11845887",
      "ft_text_length": 56028,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11845887)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "39953146",
      "title": "Improving medical machine learning models with generative balancing for equity and excellence.",
      "abstract": "Applying machine learning to clinical outcome prediction is challenging due to imbalanced datasets and sensitive tasks that contain rare yet critical outcomes and where equitable treatment across diverse patient groups is essential. Despite attempts, biases in predictions persist, driven by disparities in representation and exacerbated by the scarcity of positive labels, perpetuating health inequities. This paper introduces FairPlay, a synthetic data generation approach leveraging large language models, to address these issues. FairPlay enhances algorithmic performance and reduces bias by creating realistic, anonymous synthetic patient data that improves representation and augments dataset patterns while preserving privacy. Through experiments on multiple datasets, we demonstrate that FairPlay boosts mortality prediction performance across diverse subgroups, achieving up to a 21% improvement in F1 Score without requiring additional data or altering downstream training pipelines. Furthermore, FairPlay consistently reduces subgroup performance gaps, as shown by universal improvements in performance and fairness metrics across four experimental setups.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01438-z",
      "authors": "Theodorou Brandon et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39953146/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11828851",
      "ft_text_length": 53352,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11828851)",
      "ft_reason": "Included: bias central + approach content (10 indicators)"
    },
    {
      "pmid": "39958549",
      "title": "Enhancing rectal cancer liver metastasis prediction: Magnetic resonance imaging-based radiomics, bias mitigation, and regulatory considerations.",
      "abstract": "In this article, we comment on the article by Long et al published in the recent issue of the World Journal of Gastrointestinal Oncology. Rectal cancer patients are at risk for developing metachronous liver metastasis (MLM), yet early prediction remains challenging due to variations in tumor heterogeneity and the limitations of traditional diagnostic methods. Therefore, there is an urgent need for non-invasive techniques to improve patient outcomes. Long et al's study introduces an innovative magnetic resonance imaging (MRI)-based radiomics model that integrates high-throughput imaging data with clinical variables to predict MLM. The study employed a 7:3 split to generate training and validation datasets. The MLM prediction model was constructed using the training set and subsequently validated on the validation set using area under the curve (AUC) and dollar-cost averaging metrics to assess performance, robustness, and generalizability. By employing advanced algorithms, the model provides a non-invasive solution to assess tumor heterogeneity for better metastasis prediction, enabling early intervention and personalized treatment planning. However, variations in MRI parameters, such as differences in scanning resolutions and protocols across facilities, patient heterogeneity (e.g., age, comorbidities), and external factors like carcinoembryonic antigen levels introduce biases. Additionally, confounding factors such as diagnostic staging methods and patient comorbidities require further validation and adjustment to ensure accuracy and generalizability. With evolving Food and Drug Administration regulations on machine learning models in healthcare, compliance and careful consideration of these regulatory requirements are essential to ensuring safe and effective implementation of this approach in clinical practice. In the future, clinicians may be able to utilize data-driven, patient-centric artificial intelligence (AI)-enhanced imaging tools integrated with clinical data, which would help improve early detection of MLM and optimize personalized treatment strategies. Combining radiomics, genomics, histological data, and demographic information can significantly enhance the accuracy and precision of predictive models.",
      "journal": "World journal of gastrointestinal oncology",
      "year": "2025",
      "doi": "10.4251/wjgo.v17.i2.102151",
      "authors": "Zhang Yuwei",
      "keywords": "Bias mitigation; Food and Drug Administration regulations; Machine learning; Magnetic resonance imaging variability; Metachronous liver metastasis; Predictive modeling; Radiomics; Rectal cancer",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39958549/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11756008",
      "ft_text_length": 6582,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11756008)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "39974004",
      "title": "Algorithms to Improve Fairness in Medicare Risk Adjustment.",
      "abstract": "IMPORTANCE: Payment system design creates incentives that impact healthcare spending, access, and outcomes. With Medicare Advantage accounting for more than half of Medicare spending, changes to its risk adjustment algorithm have the potential for broad consequences. OBJECTIVE: To develop risk adjustment algorithms that can achieve fair spending targets, and compare their performance to a baseline that emulates the least squares regression approach used by the Centers for Medicare and Medicaid Services. DESIGN: Retrospective analysis of Traditional Medicare enrollment and claims data between January 2017 and December 2020. Diagnoses in claims were mapped to Hierarchical Condition Categories (HCCs). Algorithms used demographic indicators and HCCs from one calendar year to predict Medicare spending in the subsequent year. SETTING: Data from Medicare beneficiaries with documented residence in the United States or Puerto Rico. PARTICIPANTS: A random 20% sample of beneficiaries enrolled in Traditional Medicare. Included beneficiaries were aged 65 years and older, and did not have Medicaid dual eligibility. Race/ethnicity was assigned using the Research Triangle Institute enhanced indicator. MAIN OUTCOME AND MEASURES: Prospective healthcare spending by Medicare. Overall performance was measured by payment system fit and mean absolute error. Net compensation was used to assess group-level fairness. RESULTS: The main analysis included 4,398,035 Medicare beneficiaries with a mean age of 75.2 years and mean annual Medicare spending of $8,345. Out-of-sample payment system fit for the baseline regression was 12.7%. Constrained regression and post-processing both achieved fair spending targets, while maintaining payment system fit values of 12.6% and 12.7%, respectively. Whereas post-processing only increased mean payments for beneficiaries in minoritized racial/ethnic groups, constrained regression increased mean payments for beneficiaries in minoritized racial/ethnic groups and beneficiaries in other groups residing in counties with greater exposure to socioeconomic factors that can adversely affect health outcomes. CONCLUSIONS AND RELEVANCE: Constrained regression and post-processing can incorporate fairness objectives in the Medicare risk adjustment algorithm with minimal reduction in overall fit.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.01.25.25321057",
      "authors": "Reitsma Marissa B et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39974004/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11838972",
      "ft_text_length": 22239,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11838972)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39974139",
      "title": "Assessing Algorithm Fairness Requires Adjustment for Risk Distribution Differences: Re-Considering the Equal Opportunity Criterion.",
      "abstract": "The proliferation of algorithm-assisted decision making has prompted calls for careful assessment of algorithm fairness. One popular fairness metric, equal opportunity, demands parity in true positive rates (TPRs) across different population subgroups. However, we highlight a critical but overlooked weakness in this measure: at a given decision threshold, TPRs vary when the underlying risk distribution varies across subgroups, even if the model equally captures the underlying risks. Failure to account for variations in risk distributions may lead to misleading conclusions on performance disparity. To address this issue, we introduce a novel metric called adjusted TPR (aTPR), which modifies subgroup-specific TPRs to reflect performance relative to the risk distribution in a common reference subgroup. Evaluating fairness using aTPRs promotes equal treatment for equal risk by reflecting whether individuals with similar underlying risks have similar opportunities of being identified as high risk by the model, regardless of subgroup membership. We demonstrate our method through numerical experiments that explore a range of differential calibration relationships and in a real-world data set that predicts 6-month mortality risk in an in-patient sample in order to increase timely referrals for palliative care consultations.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.01.31.25321489",
      "authors": "Hegarty Sarah E et al.",
      "keywords": "Algorithm fairness; Clinical decision-making; Equal opportunity; High-risk identification; Risk distribution",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39974139/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11838655",
      "ft_text_length": 35733,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11838655)",
      "ft_reason": "Included: bias central + approach content (11 indicators)"
    },
    {
      "pmid": "40000544",
      "title": "Innovating Challenges and Experiences in Emory Health AI Bias Datathon: Experience Report.",
      "abstract": "This paper presents an in-depth analysis of the Emory Health AI (Artificial Intelligence) Bias Datathon held in August 2023, providing insights into the experiences gained during the event. The datathon, focusing on health-related issues, attracted diverse participants, including professionals, researchers, and students from various backgrounds. The paper discusses the preparation, organization, and execution of the datathon, detailing the registration process, team formulation, dataset creation, and logistical aspects. We also explore the achievements and personal experiences of participants, highlighting their resilience, dedication, and innovative contributions. The findings include a breakdown of participant demographics, responses to post-event surveys, and participant backgrounds. Observing the trends, we believe the lessons learned, and the overall impact of the Emory Health AI Bias Datathon on the participants and the field of health data science will contribute significantly in organizing future datathons.",
      "journal": "Journal of imaging informatics in medicine",
      "year": "2025",
      "doi": "10.1007/s10278-024-01367-5",
      "authors": "Paddo Atika Rahman et al.",
      "keywords": "Datathon; Healthcare innovation; Participant experiences; Post-event survey",
      "mesh_terms": "Humans; Artificial Intelligence; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40000544/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12701117",
      "ft_text_length": 32544,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12701117)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40001699",
      "title": "A Conceptual Framework for Applying Ethical Principles of AI to Medical Practice.",
      "abstract": "Artificial Intelligence (AI) is reshaping healthcare through advancements in clinical decision support and diagnostic capabilities. While human expertise remains foundational to medical practice, AI-powered tools are increasingly matching or exceeding specialist-level performance across multiple domains, paving the way for a new era of democratized healthcare access. These systems promise to reduce disparities in care delivery across demographic, racial, and socioeconomic boundaries by providing high-quality diagnostic support at scale. As a result, advanced healthcare services can be affordable to all populations, irrespective of demographics, race, or socioeconomic background. The democratization of such AI tools can reduce the cost of care, optimize resource allocation, and improve the quality of care. In contrast to humans, AI can potentially uncover complex relationships in the data from a large set of inputs and generate new evidence-based knowledge in medicine. However, integrating AI into healthcare raises several ethical and philosophical concerns, such as bias, transparency, autonomy, responsibility, and accountability. In this study, we examine recent advances in AI-enabled medical image analysis, current regulatory frameworks, and emerging best practices for clinical integration. We analyze both technical and ethical challenges inherent in deploying AI systems across healthcare institutions, with particular attention to data privacy, algorithmic fairness, and system transparency. Furthermore, we propose practical solutions to address key challenges, including data scarcity, racial bias in training datasets, limited model interpretability, and systematic algorithmic biases. Finally, we outline a conceptual algorithm for responsible AI implementations and identify promising future research and development directions.",
      "journal": "Bioengineering (Basel, Switzerland)",
      "year": "2025",
      "doi": "10.3390/bioengineering12020180",
      "authors": "Jha Debesh et al.",
      "keywords": "artificial intelligence (AI); ethical AI; philosophical AI; trustworthy AI",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40001699/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11851997",
      "ft_text_length": 49463,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11851997)",
      "ft_reason": "Included: substantial approach content (3 indicators)"
    },
    {
      "pmid": "40026843",
      "title": "Equitable hospital length of stay prediction for patients with learning disabilities and multiple long-term conditions using machine learning.",
      "abstract": "PURPOSE: Individuals with learning disabilities (LD) often face higher rates of premature mortality and prolonged hospital stays compared to the general population. Predicting the length of stay (LOS) for patients with LD and multiple long-term conditions (MLTCs) is critical for improving patient care and optimising medical resource allocation. However, there is limited research on the application of machine learning (ML) models to this population. Furthermore, approaches designed for the general population often lack generalisability and fairness, particularly when applied across sensitive groups within their cohort. METHOD: This study analyses hospitalisations of 9,618 patients with LD in Wales using electronic health records (EHR) from the SAIL Databank. A Random Forest (RF) ML model was developed to predict hospital LOS, incorporating demographics, medication history, lifestyle factors, and 39 long-term conditions. To address fairness concerns, two bias mitigation techniques were applied: a post-processing threshold optimiser and an in-processing reductions method using an exponentiated gradient. These methods aimed to minimise performance discrepancies across ethnic groups while ensuring robust model performance. RESULTS: The RF model outperformed other state-of-the-art models, achieving an area under the curve of 0.759 for males and 0.756 for females, a false negative rate of 0.224 for males and 0.229 for females, and a balanced accuracy of 0.690 for males and 0.689 for females. Bias mitigation algorithms reduced disparities in prediction performance across ethnic groups, with the threshold optimiser yielding the most notable improvements. Performance metrics, including false positive rate and balanced accuracy, showed significant enhancements in fairness for the male cohort. CONCLUSION: This study demonstrates the feasibility of applying ML models to predict LOS for patients with LD and MLTCs, while addressing fairness through bias mitigation techniques. The findings highlight the potential for equitable healthcare predictions using EHR data, paving the way for improved clinical decision-making and resource management.",
      "journal": "Frontiers in digital health",
      "year": "2025",
      "doi": "10.3389/fdgth.2025.1538793",
      "authors": "Abakasanga Emeka et al.",
      "keywords": "bias mitigation; exponentiated gradient; learning disabilities; length of stay; threshold optimiser",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40026843/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11868268",
      "ft_text_length": 53383,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11868268)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "40027644",
      "title": "Towards machine learning fairness in classifying multicategory causes of deaths in colorectal or lung cancer patients.",
      "abstract": "Classification of patient multicategory survival outcomes is important for personalized cancer treatments. Machine Learning (ML) algorithms have increasingly been used to inform healthcare decisions, but these models are vulnerable to biases in data collection and algorithm creation. ML models have previously been shown to exhibit racial bias, but their fairness towards patients from different age and sex groups have yet to be studied. Therefore, we compared the multimetric performances of 5 ML models (random forests, multinomial logistic regression, linear support vector classifier, linear discriminant analysis, and multilayer perceptron) when classifying colorectal cancer patients (n=515) of various age, sex, and racial groups using the TCGA data. All five models exhibited biases for these sociodemographic groups. We then repeated the same process on lung adenocarcinoma (n=589) to validate our findings. Surprisingly, most models tended to perform more poorly overall for the largest sociodemographic groups. Methods to optimize model performance, including testing the model on merged age, sex, or racial groups, and creating a model trained on and used for an individual or merged sociodemographic group, show potential to reduce disparities in model performance for different groups. Notably, these methods may be used to improve ML fairness while avoiding penalizing the model for exhibiting bias and thus sacrificing overall performance.",
      "journal": "bioRxiv : the preprint server for biology",
      "year": "2025",
      "doi": "10.1101/2025.02.14.638368",
      "authors": "Feng Catherine H et al.",
      "keywords": "Colorectal cancer; feature selection; machine learning; multilabel classification; survival; transcriptomics",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40027644/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11870570",
      "ft_text_length": 22526,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11870570)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40038550",
      "title": "Physician clinical decision modification and bias assessment in a randomized controlled trial of AI assistance.",
      "abstract": "BACKGROUND: Artificial intelligence assistance in clinical decision making shows promise, but concerns exist about potential exacerbation of demographic biases in healthcare. This study aims to evaluate how physician clinical decisions and biases are influenced by AI assistance in a chest pain triage scenario. METHODS: A randomized, pre post-intervention study was conducted with 50 US-licensed physicians who reviewed standardized chest pain video vignettes featuring either a white male or Black female patient. Participants answered clinical questions about triage, risk assessment, and treatment before and after receiving GPT-4 generated recommendations. Clinical decision accuracy was evaluated against evidence-based guidelines. RESULTS: Here we show that physicians are willing to modify their clinical decisions based on GPT-4 assistance, leading to improved accuracy scores from 47% to 65% in the white male patient group and 63% to 80% in the Black female patient group. The accuracy improvement occurs without introducing or exacerbating demographic biases, with both groups showing similar magnitudes of improvement (18%). A post-study survey indicates that 90% of physicians expect AI tools to play a significant role in future clinical decision making. CONCLUSIONS: Physician clinical decision making can be augmented by AI assistance while maintaining equitable care across patient demographics. These findings suggest a path forward for AI clinical decision support that improves medical care without amplifying healthcare disparities. Doctors sometimes make different medical decisions for patients based on their race or gender, even when the symptoms are the same and the advice should be similar. New artificial intelligence (AI) tools such as GPT-4 are becoming available to assist doctors when making clinical decisions. Our study looked at whether using AI would impact performance and bias during doctor decision making. We investigated how doctors respond to AI suggestions when evaluating chest pain, a common but serious medical concern. We showed 50 doctors a video of either a white male or Black female patient describing chest pain symptoms, and asked them to make medical decisions. The doctors then received suggestions from an AI system and could change their decisions. We found that doctors were willing to consider the AI\u2019s suggestions and made more accurate medical decisions after receiving this help. This improvement in decision-making happened equally for all patients, regardless of their race or gender, suggesting AI tools could help improve medical care without increasing bias.",
      "journal": "Communications medicine",
      "year": "2025",
      "doi": "10.1038/s43856-025-00781-2",
      "authors": "Goh Ethan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40038550/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11880198",
      "ft_text_length": 21012,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11880198)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40047796",
      "title": "Comparison of Machine Learning Algorithms Identifying Children at Increased Risk of Out-of-Home Placement: Development and Practical Considerations.",
      "abstract": "OBJECTIVE: To develop a machine learning (ML) algorithm capable of identifying children at risk of out-of-home placement among a Medicaid-insured population. STUDY SETTING AND DESIGN: The study population includes children enrolled in a Medicaid accountable care organization between 2018 and 2022 in two nonurban Ohio counties served by the Centers for Medicare and Medicaid Services-funded Integrated Care for Kids Model. Using a retrospective cohort, we developed and compared a set of ML algorithms to identify children at risk of out-of-home placement within one\u2009year. ML algorithms tested include least absolute shrinkage and selection operator (LASSO)-regularized logistic regression and eXtreme gradient-boosted trees (XGBoost). We compared both modeling approaches with and without race as a candidate predictor. Performance metrics included the area under the receiver operating characteristic curve (AUROC) and the corrected partial AUROC at specificities \u2265\u200990% (pAUROC90). Algorithmic bias was tested by comparing pAUROC90 across each model between Black and White children. DATA SOURCES AND ANALYTIC SAMPLE: The modeling dataset was comprised of Medicaid claims and patient demographics data from Partners For Kids, a pediatric accountable care organization. PRINCIPAL FINDINGS: Overall, XGBoost models outperformed LASSO models. When race was included in the model, XGBoost had an AUROC of 0.78 (95% confidence interval [CI]: 0.77-0.79) while the LASSO model had an AUROC of 0.75 (95% CI: 0.74-0.77). When race was excluded from the model, XGBoost had an AUROC of 0.76 (95% CI: 0.74-0.77) while LASSO had an AUROC of 0.73 (95% CI: 0.72-0.74). CONCLUSIONS: The more complex XGBoost outperformed the simpler LASSO in predicting out-of-home placement and had less evidence of racial bias. This study highlights the complexities of developing predictive models in systems with known racial disparities and illustrates what can be accomplished when ML developers and policy leaders collaborate to maximize data to meet the needs of children and families.",
      "journal": "Health services research",
      "year": "2025",
      "doi": "10.1111/1475-6773.14601",
      "authors": "Gorham Tyler J et al.",
      "keywords": "Medicaid; accountable care organization; machine learning; out\u2010of\u2010home placement; predictive modeling",
      "mesh_terms": "Humans; Machine Learning; Medicaid; United States; Retrospective Studies; Male; Female; Algorithms; Child; Child, Preschool; Ohio; Accountable Care Organizations; Foster Home Care; Infant; Logistic Models; ROC Curve; Adolescent; Risk Assessment",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40047796/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12277119",
      "ft_text_length": 33820,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12277119)",
      "ft_reason": "Included: substantial approach content (4 indicators)"
    },
    {
      "pmid": "40056436",
      "title": "Large language models are less effective at clinical prediction tasks than locally trained machine learning models.",
      "abstract": "OBJECTIVES: To determine the extent to which current large language models (LLMs) can serve as substitutes for traditional machine learning (ML) as clinical predictors using data from electronic health records (EHRs), we investigated various factors that can impact their adoption, including overall performance, calibration, fairness, and resilience to privacy protections that reduce data fidelity. MATERIALS AND METHODS: We evaluated GPT-3.5, GPT-4, and traditional ML (as gradient-boosting trees) on clinical prediction tasks in EHR data from Vanderbilt University Medical Center (VUMC) and MIMIC IV. We measured predictive performance with area under the receiver operating characteristic (AUROC) and model calibration using Brier Score. To evaluate the impact of data privacy protections, we assessed AUROC when demographic variables are generalized. We evaluated algorithmic fairness using equalized odds and statistical parity across race, sex, and age of patients. We also considered the impact of using in-context learning by incorporating labeled examples within the prompt. RESULTS: Traditional ML [AUROC: 0.847, 0.894 (VUMC, MIMIC)] substantially outperformed GPT-3.5 (AUROC: 0.537, 0.517) and GPT-4 (AUROC: 0.629, 0.602) (with and without in-context learning) in predictive performance and output probability calibration [Brier Score (ML vs GPT-3.5 vs GPT-4): 0.134 vs 0.384 vs 0.251, 0.042 vs 0.06 vs 0.219)]. DISCUSSION: Traditional ML is more robust than GPT-3.5 and GPT-4 in generalizing demographic information to protect privacy. GPT-4 is the fairest model according to our selected metrics but at the cost of poor model performance. CONCLUSION: These findings suggest that non-fine-tuned LLMs are less effective and robust than locally trained ML for clinical prediction tasks, but they are improving across releases.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2025",
      "doi": "10.1093/jamia/ocaf038",
      "authors": "Brown Katherine E et al.",
      "keywords": "clinical prediction models; fairness; large language models; privacy",
      "mesh_terms": "Machine Learning; Humans; Electronic Health Records; Female; ROC Curve; Male; Algorithms; Middle Aged; Large Language Models",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40056436/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12012369",
      "ft_text_length": 36659,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12012369)",
      "ft_reason": "Included: substantial approach content (3 indicators)"
    },
    {
      "pmid": "40079820",
      "title": "Emerging algorithmic bias: fairness drift as the next dimension of model maintenance and sustainability.",
      "abstract": "OBJECTIVES: While performance drift of clinical prediction models is well-documented, the potential for algorithmic biases to emerge post-deployment has had limited characterization. A better understanding of how temporal model performance may shift across subpopulations is required to incorporate fairness drift into model maintenance strategies. MATERIALS AND METHODS: We explore fairness drift in a national population over 11 years, with and without model maintenance aimed at sustaining population-level performance. We trained random forest models predicting 30-day post-surgical readmission, mortality, and pneumonia using 2013 data from US Department of Veterans Affairs facilities. We evaluated performance quarterly from 2014 to 2023 by self-reported race and sex. We estimated discrimination, calibration, and accuracy, and operationalized fairness using metric parity measured as the gap between disadvantaged and advantaged groups. RESULTS: Our cohort included 1\u00a0739\u00a0666 surgical cases. We observed fairness drift in both the original and temporally updated models. Model updating had a larger impact on overall performance than fairness gaps. During periods of stable fairness, updating models at the population level increased, decreased, or did not impact fairness gaps. During periods of fairness drift, updating models restored fairness in some cases and exacerbated fairness gaps in others. DISCUSSION: This exploratory study highlights that algorithmic fairness cannot be assured through one-time assessments during model development. Temporal changes in fairness may take multiple forms and interact with model updating strategies in unanticipated ways. CONCLUSION: Equitable and sustainable clinical artificial intelligence deployments will require novel methods to monitor algorithmic fairness, detect emerging bias, and adopt model updates that promote fairness.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2025",
      "doi": "10.1093/jamia/ocaf039",
      "authors": "Davis Sharon E et al.",
      "keywords": "algorithmic fairness; dataset shift; model updating; performance drift; predictive analytics",
      "mesh_terms": "Humans; Algorithms; United States; Female; Male; Bias; Middle Aged; Models, Statistical; Patient Readmission; Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40079820/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12012346",
      "ft_text_length": 35551,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12012346)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "40080818",
      "title": "Assessing Racial and Ethnic Bias in Text Generation by Large Language Models for Health Care-Related Tasks: Cross-Sectional Study.",
      "abstract": "BACKGROUND: Racial and ethnic bias in large language models (LLMs) used for health care tasks is a growing concern, as it may contribute to health disparities. In response, LLM operators implemented safeguards against prompts that are overtly seeking certain biases. OBJECTIVE: This study aims to investigate a potential racial and ethnic bias among 4 popular LLMs: GPT-3.5-turbo (OpenAI), GPT-4 (OpenAI), Gemini-1.0-pro (Google), and Llama3-70b (Meta) in generating health care consumer-directed text in the absence of overtly biased queries. METHODS: In this cross-sectional study, the 4 LLMs were prompted to generate discharge instructions for patients with HIV. Each patient's encounter deidentified metadata including race/ethnicity as a variable was passed over in a table format through a prompt 4 times, altering only the race/ethnicity information (African American, Asian, Hispanic White, and non-Hispanic White) each time, while keeping all other information constant. The prompt requested the model to write discharge instructions for each encounter without explicitly mentioning race or ethnicity. The LLM-generated instructions were analyzed for sentiment, subjectivity, reading ease, and word frequency by race/ethnicity. RESULTS: The only observed statistically significant difference between race/ethnicity groups was found in entity count (GPT-4, df=42, P=.047). However, post hoc chi-square analysis for GPT-4's entity counts showed no significant pairwise differences among race/ethnicity categories after Bonferroni correction. CONCLUSIONS: A total of 4 LLMs were relatively invariant to race/ethnicity in terms of linguistic and readability measures. While our study used proxy linguistic and readability measures to investigate racial and ethnic bias among 4 LLM responses in a health care-related task, there is an urgent need to establish universally accepted standards for measuring bias in LLM-generated responses. Further studies are needed to validate these results and assess their implications.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/57257",
      "authors": "Hanna John J et al.",
      "keywords": "ChatGPT; artificial intelligence; bias; consumer-directed; cross sectional; healthcare; human immunodeficiency virus; large language models; racism; reading ease; sentiment analysis; task; text generation; word frequency",
      "mesh_terms": "Cross-Sectional Studies; Humans; Ethnicity; Racism; Racial Groups; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40080818/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11950697",
      "ft_text_length": 17745,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11950697)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40087260",
      "title": "Developing and Applying the BE-FAIR Equity Framework to a Population Health Predictive Model: A Retrospective Observational Cohort Study.",
      "abstract": "BACKGROUND: Population health programs rely on healthcare predictive models to allocate resources, yet models can perpetuate biases that exacerbate health disparities among marginalized communities. OBJECTIVE: We developed the\u00a0Bias-reduction and Equity Framework for Assessing, Implementing, and Redesigning (BE-FAIR) healthcare predictive models, an applied framework tested within a large health system using a population health predictive model, aiming to minimize bias and enhance equity. DESIGN: Retrospective cohort study conducted at an academic medical center. Data collected from September 30, 2020, to October 1, 2022, were analyzed to assess bias resulting from model use. PARTICIPANTS: Primary care or payer-attributed patients at the medical center identified through electronic health records and claims data. Participants were stratified by race-ethnicity, gender, and social vulnerability defined by the Healthy Places Index (HPI). INTERVENTION: BE-FAIR implementation involved steps such as an anti-racism lens application, de-siloed team structure, historical intervention review, disaggregated data analysis, and calibration evaluation. MAIN MEASURES: The primary outcome was the calibration and discrimination of the model across different demographic groups, measured by logistic regression and area under the receiver operating characteristic curve (AUROC). RESULTS: The study population consisted of 114,311 individuals with a mean age of 43.4 years (SD 24.0 years), 55.4% female, and 59.5% white/Caucasian. Calibration differed by race-ethnicity and HPI with significantly lower predicted probabilities of hospitalization for African Americans (0.129\u00b10.051, p=0.016), Hispanics (0.133\u00b10.047, p=0.004), AAPI (0.120\u00b10.051, p=0.018), and multi-race (0.245\u00b10.087, p=0.005) relative to white/Caucasians and for individuals in low HPI areas (0 - 25%, 0.178\u00b10.042, p<0.001; 25 - 50%, 0.129\u00b10.044, p=0.003). AUROC values varied among demographic groups. CONCLUSIONS: The BE-FAIR framework offers a practical approach to address bias in healthcare predictive models, guiding model development, and implementation. By identifying and mitigating biases, BE-FAIR enhances the fairness and equity of healthcare delivery, particularly for minoritized groups, paving the way for more inclusive and effective population health strategies.",
      "journal": "Journal of general internal medicine",
      "year": "2025",
      "doi": "10.1007/s11606-025-09462-1",
      "authors": "Gupta Reshma et al.",
      "keywords": "bias; equity framework; fairness; population health; predictive models",
      "mesh_terms": "Humans; Retrospective Studies; Female; Male; Middle Aged; Population Health; Adult; Health Equity; Healthcare Disparities",
      "pub_types": "Journal Article; Observational Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40087260/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12405130",
      "ft_text_length": 39031,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12405130)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "40100898",
      "title": "What makes clinical machine learning fair? A practical ethics framework.",
      "abstract": "Machine learning (ML) can offer a tremendous contribution to medicine by streamlining decision-making, reducing mistakes, improving clinical accuracy and ensuring better patient outcomes. The prospects of a widespread and rapid integration of machine learning in clinical workflow have attracted considerable attention including due to complex ethical implications-algorithmic bias being among the most frequently discussed ML models. Here we introduce and discuss a practical ethics framework inductively-generated via normative analysis of the practical challenges in developing an actual clinical ML model (see case study). The framework is usable to identify, measure and address bias in clinical machine learning models, thus improving fairness as to both model performance and health outcomes. We detail a proportionate approach to ML bias by defining the demands of fair ML in light of what is ethically justifiable and, at the same time, technically feasible in light of inevitable trade-offs. Our framework enables ethically robust and transparent decision-making both in the design and the context-dependent aspects of ML bias mitigation, thus improving accountability for both developers and clinical users.",
      "journal": "PLOS digital health",
      "year": "2025",
      "doi": "10.1371/journal.pdig.0000728",
      "authors": "Hoche Marine et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40100898/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11918422",
      "ft_text_length": 49565,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11918422)",
      "ft_reason": "Included: substantial approach content (10 indicators)"
    },
    {
      "pmid": "40111867",
      "title": "Easy ensemble classifier-group and intersectional fairness and threshold (EEC-GIFT): a fairness-aware machine learning framework for lung cancer screening eligibility using real-world data.",
      "abstract": "BACKGROUND: We use real-world data to develop a lung cancer screening (LCS) eligibility mechanism that is both accurate and free from racial bias. METHODS: Our data came from the Prostate, Lung, Colorectal, and Ovarian (PLCO) cancer screening trial. We built a systematic fairness-aware machine learning framework by integrating a Group and Intersectional Fairness and Threshold (GIFT) strategy with an easy ensemble classifier-(EEC-) or logistic regression-(LR-) based model. The best LCS eligibility mechanism EEC-GIFT* and LR-GIFT* were applied to the testing dataset and their performances were compared to the 2021 US Preventive Services Task Force (USPSTF) criteria and PLCOM2012 model. The equal opportunity difference (EOD) of developing lung cancer between Black and White smokers was used to evaluate mechanism fairness. RESULTS: The fairness of LR-GIFT* or EEC-GIFT* during training was notably greater than that of the LR or EEC models without greatly reducing their accuracy. During testing, the EEC-GIFT* (85.16% vs 78.08%, P\u2009<\u2009.001) and LR-GIFT* (85.98% vs 78.08%, P\u2009<\u2009.001) models significantly improved sensitivity without sacrificing specificity compared to the 2021 USPSTF criteria. The EEC-GIFT* (0.785 vs 0.788, P\u2009=\u2009.28) and LR-GIFT* (0.785 vs 0.788, P\u2009=\u2009.30) showed similar area under receiver operating characteristic curve values compared to the PLCOM2012 model. While the average EODs between Blacks and Whites were significant for the 2021 USPSTF criteria (0.0673, P\u2009<\u2009.001), PLCOM2012 (0.0566, P\u2009<\u2009.001), and LR-GIFT* (0.0081, P\u2009<\u2009.001), the EEC-GIFT* model was unbiased (0.0034, P\u2009=\u2009.07). CONCLUSION: Our EEC-GIFT* LCS eligibility mechanism can significantly mitigate racial biases in eligibility determination without compromising its predictive performance.",
      "journal": "JNCI cancer spectrum",
      "year": "2025",
      "doi": "10.1093/jncics/pkaf030",
      "authors": "Conahan Piyawan et al.",
      "keywords": "",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Black or African American; Early Detection of Cancer; Eligibility Determination; Logistic Models; Lung Neoplasms; Machine Learning; Mass Screening; White",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40111867/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11986816",
      "ft_text_length": 31779,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11986816)",
      "ft_reason": "Included: bias central + approach content (14 indicators)"
    },
    {
      "pmid": "40111994",
      "title": "Evaluating fairness of machine learning prediction of prolonged wait times in Emergency Department with Interpretable eXtreme gradient boosting.",
      "abstract": "It is essential to evaluate performance and assess quality before applying artificial intelligence (AI) and machine learning (ML) models to clinical practice. This study utilized ML to predict patient wait times in the Emergency Department (ED), determine model performance accuracies, and conduct fairness evaluations to further assess ethnic disparities in using ML for wait time prediction among different patient populations in the ED. This retrospective observational study included adult patients (age \u226518 years) in the ED (n=173,856 visits) who were assigned an Emergency Severity Index (ESI) level of 3 at triage. Prolonged wait time was defined as waiting time \u226530 minutes. We employed extreme gradient boosting (XGBoost) for predicting prolonged wait times. Model performance was assessed with accuracy, recall, precision, F1 score, and false negative rate (FNR). To perform the global and local interpretation of feature importance, we utilized Shapley additive explanations (SHAP) to interpret the output from the XGBoost model. Fairness in ML models were evaluated across sensitive attributes (sex, race and ethnicity, and insurance status) at both subgroup and individual levels. We found that nearly half (48.43%, 84,195) of ED patient visits demonstrated prolonged ED wait times. XGBoost model exhibited moderate accuracy performance (AUROC=0.81). When fairness was evaluated with FNRs, unfairness existed across different sensitive attributes (male vs. female, Hispanic vs. Non-Hispanic White, and patients with insurances vs. without insurance). The predicted FNRs were lower among females, Hispanics, and patients without insurance compared to their counterparts. Therefore, XGBoost model demonstrated acceptable performance in predicting prolonged wait times in ED visits. However, disparities arise in predicting patients with different sex, race and ethnicity, and insurance status. To enhance the utility of ML model predictions in clinical practice, conducting performance assessments and fairness evaluations are crucial.",
      "journal": "PLOS digital health",
      "year": "2025",
      "doi": "10.1371/journal.pdig.0000751",
      "authors": "Wang Hao et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40111994/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11925291",
      "ft_text_length": 38702,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11925291)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "40113922",
      "title": "Digital pathways connecting social and biological factors to health outcomes and equity.",
      "abstract": "Digital pathways extend conventional connections between social and biological factors and health outcomes, significantly influencing health equity. Data representation bias and distribution shifts are key mechanisms through which determinants of health impact generalizability of artificial intelligence (AI) models and subsequently affect health outcomes and equity. These mechanisms provide critical targets for algorithmic interventions, which can lead to Pareto improvements in AI model performance across diverse populations, thereby mitigating health disparities.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01564-8",
      "authors": "Cui Yan",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40113922/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11926183",
      "ft_text_length": 8591,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11926183)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "40122892",
      "title": "Towards fairness-aware and privacy-preserving enhanced collaborative learning for healthcare.",
      "abstract": "The widespread integration of AI algorithms in healthcare has sparked ethical concerns, particularly regarding privacy and fairness. Federated Learning (FL) offers a promising solution to learn from a broad spectrum of patient data without directly accessing individual records, enhancing privacy while facilitating knowledge sharing across distributed data sources. However, healthcare institutions face significant variations in access to crucial computing resources, with resource budgets often linked to demographic and socio-economic factors, exacerbating unfairness in participation. While heterogeneous federated learning methods allow healthcare institutions with varying computational capacities to collaborate, they fail to address the performance gap between resource-limited and resource-rich institutions. As a result, resource-limited institutions may receive suboptimal models, further reinforcing disparities in AI-driven healthcare outcomes. Here, we propose a resource-adaptive framework for collaborative learning that dynamically adjusts to varying computational capacities, ensuring fair participation. Our approach enhances model accuracy, safeguards patient privacy, and promotes equitable access to trustworthy and efficient AI-driven healthcare solutions.",
      "journal": "Nature communications",
      "year": "2025",
      "doi": "10.1038/s41467-025-58055-3",
      "authors": "Zhang Feilong et al.",
      "keywords": "",
      "mesh_terms": "Humans; Privacy; Delivery of Health Care; Cooperative Behavior; Algorithms; Artificial Intelligence; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40122892/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11930927",
      "ft_text_length": 93561,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11930927)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "40127903",
      "title": "PROBAST+AI: an updated quality, risk of bias, and applicability assessment tool for prediction models using regression or artificial intelligence methods.",
      "abstract": "The Prediction model Risk Of Bias ASsessment Tool (PROBAST) is used to assess the quality, risk of bias, and applicability of prediction models or algorithms and of prediction model/algorithm studies. Since PROBAST\u2019s introduction in 2019, much progress has been made in the methodology for prediction modelling and in the use of artificial intelligence, including machine learning, techniques. An update to PROBAST-2019 is thus needed. This article describes the development of PROBAST+AI. PROBAST+AI consists of two distinctive parts: model development and model evaluation. For model development, PROBAST+AI users assess quality and applicability using 16 targeted signalling questions. For model evaluation, PROBAST+AI users assess the risk of bias and applicability using 18 targeted signalling questions. Both parts contain four domains: participants and data sources, predictors, outcome, and analysis. Applicability of the prediction model is rated for the participants and data sources, predictors, and outcome domains. PROBAST+AI may replace the original PROBAST tool and allows all key stakeholders (eg, model developers, AI companies, researchers, editors, reviewers, healthcare professionals, guideline developers, and policy organisations) to examine the quality, risk of bias, and applicability of any type of prediction model in the healthcare sector, irrespective of whether regression modelling or AI techniques are used.",
      "journal": "BMJ (Clinical research ed.)",
      "year": "2025",
      "doi": "10.1136/bmj-2024-082505",
      "authors": "Moons Karel G M et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40127903/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11931409",
      "ft_text_length": 64414,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11931409)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "40138420",
      "title": "Demographic bias of expert-level vision-language foundation models in medical imaging.",
      "abstract": "Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit training annotations. However, it is crucial to ensure that these AI models do not mirror or amplify human biases, disadvantaging historically marginalized groups such as females or Black patients. In this study, we investigate the algorithmic fairness of state-of-the-art vision-language foundation models in chest x-ray diagnosis across five globally sourced datasets. Our findings reveal that compared to board-certified radiologists, these foundation models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups such as Black female patients. Such biases present over a wide range of pathologies and demographic attributes. Further analysis of the model embedding uncovers its substantial encoding of demographic information. Deploying medical AI systems with biases can intensify preexisting care disparities, posing potential challenges to equitable healthcare access and raising ethical questions about their clinical applications.",
      "journal": "Science advances",
      "year": "2025",
      "doi": "10.1126/sciadv.adq0305",
      "authors": "Yang Yuzhe et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Artificial Intelligence; Diagnostic Imaging; Algorithms; Male; Demography",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40138420/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11939055",
      "ft_text_length": 43175,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11939055)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "40144330",
      "title": "Artificial Intelligence to Promote Racial and Ethnic Cardiovascular Health Equity.",
      "abstract": "PURPOSE OF REVIEW: The integration of artificial intelligence (AI) in medicine holds promise for transformative advancements aimed at improving healthcare outcomes. Amidst this promise, AI has been envisioned as a tool to detect and mitigate racial and ethnic inequity known to plague current cardiovascular care. However, this enthusiasm is dampened by the recognition that AI itself can harbor and propagate biases, necessitating a careful approach to ensure equity. This review highlights topics in the landscape of AI in cardiology, its role in identifying and addressing healthcare inequities, promoting diversity in research, concerns surrounding its applications, and proposed strategies for fostering equitable utilization. RECENT FINDINGS: Artificial intelligence has proven to be a valuable tool for clinicians in diagnosing and mitigating racial and ethnic inequities in cardiology, as well as the promotion of diversity in research. This promise is counterbalanced by the cautionary reality that AI can inadvertently perpetuate existent biases stemming from limited diversity in training data, inherent biases within datasets, and inadequate bias detection and monitoring mechanisms. Recognizing these concerns, experts emphasize the need for rigorous efforts to address these limitations in the development and deployment of AI within medicine. SUMMARY: Implementing AI in cardiovascular care to identify and address racial and ethnic inequities requires careful design and execution, beginning with meticulous data collection and a thorough review of training datasets. Furthermore, ensuring equitable performance involves rigorous testing and continuous surveillance of algorithms. Lastly, the promotion of diversity in the AI workforce and engagement of stakeholders are crucial to the advancement of equity to ultimately realize the potential for artificial intelligence for cardiovascular health equity.",
      "journal": "Current cardiovascular risk reports",
      "year": "2024",
      "doi": "10.1007/s12170-024-00745-6",
      "authors": "Amponsah Daniel et al.",
      "keywords": "Artificial Intelligence; Cardiovascular Equity; Disparities; Diversity; Machine Learning; Racial and Ethnic Inequity",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40144330/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11938301",
      "ft_text_length": 1918,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11938301)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40150706",
      "title": "Simplatab: An Automated Machine Learning Framework for Radiomics-Based Bi-Parametric MRI Detection of Clinically Significant Prostate Cancer.",
      "abstract": "BACKGROUND: Prostate cancer (PCa) diagnosis using MRI is often challenged by lesion variability. METHODS: This study introduces Simplatab, an open-source automated machine learning (AutoML) framework designed for, but not limited to, automating the entire machine Learning pipeline to facilitate the detection of clinically significant prostate cancer (csPCa) using radiomics features. Unlike existing AutoML tools such as Auto-WEKA, Auto-Sklearn, ML-Plan, ATM, Google AutoML, and TPOT, Simplatab offers a comprehensive, user-friendly framework that integrates data bias detection, feature selection, model training with hyperparameter optimization, explainable AI (XAI) analysis, and post-training model vulnerabilities detection. Simplatab requires no coding expertise, provides detailed performance reports, and includes robust data bias detection, making it particularly suitable for clinical applications. RESULTS: Evaluated on a large pan-European cohort of 4816 patients from 12 clinical centers, Simplatab supports multiple machine learning algorithms. The most notable features that differentiate Simplatab include ease of use, a user interface accessible to those with no coding experience, comprehensive reporting, XAI integration, and thorough bias assessment, all provided in a human-understandable format. CONCLUSIONS: Our findings indicate that Simplatab can significantly enhance the usability, accountability, and explainability of machine learning in clinical settings, thereby increasing trust and accessibility for AI non-experts.",
      "journal": "Bioengineering (Basel, Switzerland)",
      "year": "2025",
      "doi": "10.3390/bioengineering12030242",
      "authors": "Zaridis Dimitrios I et al.",
      "keywords": "AutoML; MRI; artificial intelligence; automated machine learning framework; open source; prostate cancer; radiomics",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40150706/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11939345",
      "ft_text_length": 48669,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11939345)",
      "ft_reason": "Included: substantial approach content (7 indicators)"
    },
    {
      "pmid": "40152140",
      "title": "Predicting postoperative chronic opioid use with fair machine learning models integrating multi-modal data sources: a demonstration of ethical machine learning in healthcare.",
      "abstract": "OBJECTIVE: Building upon our previous work on predicting chronic opioid use using electronic health records (EHR) and wearable data, this study leveraged the Health Equity Across the AI Lifecycle (HEAAL) framework to (a) fine tune the previously built model with genomic data and evaluate model performance in predicting chronic opioid use and (b) apply IBM's AIF360 pre-processing toolkit to mitigate bias related to gender and race and evaluate the model performance using various fairness metrics. MATERIALS AND METHODS: Participants included approximately 271 All of Us Research Program subjects with EHR, wearable, and genomic data. We fine-tuned 4 machine learning models on the new dataset. The SHapley Additive exPlanations (SHAP) technique identified the best-performing predictors. A preprocessing toolkit boosted fairness by gender and race. RESULTS: The genetic data enhanced model performance from the prior model, with the area under the curve improving from 0.90 (95% CI, 0.88-0.92) to 0.95 (95% CI, 0.89-0.95). Key predictors included Dopamine D1 Receptor (DRD1) rs4532, general type of surgery, and time spent in physical activity. The reweighing preprocessing technique applied to the stacking algorithm effectively improved the model's fairness across racial and gender groups without compromising performance. CONCLUSION: We leveraged 2 dimensions of the HEAAL framework to build a fair artificial intelligence (AI) solution. Multi-modal datasets (including wearable and genetic data) and applying bias mitigation strategies can help models to more fairly and accurately assess risk across diverse populations, promoting fairness in AI in healthcare.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2025",
      "doi": "10.1093/jamia/ocaf053",
      "authors": "Soley Nidhi et al.",
      "keywords": "All of Us; chronic opioid use; ethical machine learning; multimodal dataset; responsible AI",
      "mesh_terms": "Humans; Machine Learning; Electronic Health Records; Male; Female; Opioid-Related Disorders; Postoperative Pain; Wearable Electronic Devices; Analgesics, Opioid; Middle Aged; Adult; Information Sources",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40152140/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12089784",
      "ft_text_length": 1677,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12089784)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "40175463",
      "title": "Investigation on potential bias factors in histopathology datasets.",
      "abstract": "Deep neural networks (DNNs) have demonstrated remarkable capabilities in medical applications, including digital pathology, where they excel at analyzing complex patterns in medical images to assist in accurate disease diagnosis and prognosis. However, concerns have arisen about potential biases in The Cancer Genome Atlas (TCGA) dataset, a comprehensive repository of digitized histopathology data and serves as both a training and validation source for deep learning models, suggesting that over-optimistic results of model performance may be due to reliance on biased features rather than histological characteristics. Surprisingly, recent studies have confirmed the existence of site-specific bias in the embedded features extracted for cancer-type discrimination, leading to high accuracy in acquisition site classification. This biased behavior motivated us to conduct an in-depth analysis to investigate potential causes behind this unexpected biased ability toward site-specific pattern recognition. The analysis was conducted on two cutting-edge DNN models: KimiaNet, a state-of-the-art DNN trained on TCGA images, and the self-trained EfficientNet. In this research study, the balanced accuracy metric is used to evaluate the performance of a model trained to classify data centers, which was originally designed to learn cancerous patterns, with the aim of investigating the potential factors contributing to the higher balanced accuracy in data center detection.",
      "journal": "Scientific reports",
      "year": "2025",
      "doi": "10.1038/s41598-025-89210-x",
      "authors": "Kheiri Farnaz et al.",
      "keywords": "Digital pathology; Histopathology; Learning models; Site-specific bias",
      "mesh_terms": "Humans; Neoplasms; Neural Networks, Computer; Deep Learning; Bias; Image Processing, Computer-Assisted; Databases, Factual",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40175463/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11965531",
      "ft_text_length": 85835,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11965531)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "40196288",
      "title": "Navigating Fairness in AI-based Prediction Models: Theoretical Constructs and Practical Applications.",
      "abstract": "Artificial Intelligence (AI)-based prediction models, including risk scoring systems and decision support systems, are increasingly adopted in healthcare. Addressing AI fairness is essential to fighting health disparities and achieving equitable performance and patient outcomes. Numerous and conflicting definitions of fairness complicate this effort. This paper aims to structure the transition of AI fairness from theory to practical application with appropriate fairness metrics. For 27 definitions of fairness identified in the recent literature, we assess the relation with the model's intended use, type of decision influenced and ethical principles of distributive justice. We advocate that due to limitations in some notions of fairness, clinical utility, performance-based metrics (area under the receiver operating characteristic curve), calibration, and statistical parity are the most relevant group-based metrics for medical applications. Through two use cases, we demonstrate that different metrics may be applicable depending on the intended use and ethical framework. Our approach provides a foundation for AI developers and assessors by assessing model fairness and the impact of bias mitigation strategies, hence promoting more equitable AI-based implementations.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.03.24.25324500",
      "authors": "van der Meijden S L et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40196288/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11974802",
      "ft_text_length": 18082,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11974802)",
      "ft_reason": "Included: bias central + approach content (12 indicators)"
    },
    {
      "pmid": "40199877",
      "title": "Achieving flexible fairness metrics in federated medical imaging.",
      "abstract": "The rapid adoption of Artificial Intelligence (AI) in medical imaging raises fairness and privacy concerns across demographic groups, especially in diagnosis and treatment decisions. While federated learning (FL) offers decentralized privacy preservation, current frameworks often prioritize collaboration fairness over group fairness, risking healthcare disparities. Here we present FlexFair, an innovative FL framework designed to address both fairness and privacy challenges. FlexFair incorporates a flexible regularization term to facilitate the integration of multiple fairness criteria, including equal accuracy, demographic parity, and equal opportunity. Evaluated across four clinical applications (polyp segmentation, fundus vascular segmentation, cervical cancer segmentation, and skin disease diagnosis), FlexFair outperforms state-of-the-art methods in both fairness and accuracy. Moreover, we curate a multi-center dataset for cervical cancer segmentation that includes 678 patients from four hospitals. This diverse dataset allows for a more comprehensive analysis of model performance across different population groups, ensuring the findings are applicable to a broader range of patients.",
      "journal": "Nature communications",
      "year": "2025",
      "doi": "10.1038/s41467-025-58549-0",
      "authors": "Xing Huijun et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Artificial Intelligence; Diagnostic Imaging; Uterine Cervical Neoplasms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40199877/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11978761",
      "ft_text_length": 59400,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11978761)",
      "ft_reason": "Included: bias central + approach content (7 indicators)"
    },
    {
      "pmid": "40218157",
      "title": "Deep Learning-Based Glaucoma Detection Using Clinical Notes: A Comparative Study of Long Short-Term Memory and Convolutional Neural Network Models.",
      "abstract": "Background/Objectives: Glaucoma is the second-leading cause of irreversible blindness globally. Retinal images such as color fundus photography have been widely used to detect glaucoma. However, little is known about the effectiveness of using raw clinical notes generated by glaucoma specialists in detecting glaucoma. This study aims to investigate the capability of deep learning approaches to detect glaucoma from clinical notes based on a real-world dataset including 10,000 patients. Different popular models are explored to predict the binary glaucomatous status defined from a comprehensive vision function assessment. Methods: We compared multiple deep learning architectures, including Long Short-Term Memory (LSTM) networks, Convolutional Neural Networks (CNNs), and transformer-based models BERT and BioBERT. LSTM exploits temporal feature dependencies within the clinical notes, while CNNs focus on extracting local textual features, and transformer-based models leverage self-attention to capture rich contextual information and feature correlations. We also investigated the group disparities of deep learning for glaucoma detection in various demographic groups. Results: The experimental results indicate that the CNN model achieved an Overall AUC of 0.80, slightly outperforming LSTM by 0.01. Both models showed disparities and biases in performance across different racial groups. However, the CNN showed reduced group disparities compared to LSTM across Asian, Black, and White groups, meaning it has the advantage of achieving more equitable outcomes. Conclusions: This study demonstrates the potential of deep learning models to detect glaucoma from clinical notes and highlights the need for fairness-aware modeling to address health disparities.",
      "journal": "Diagnostics (Basel, Switzerland)",
      "year": "2025",
      "doi": "10.3390/diagnostics15070807",
      "authors": "Mohammadjafari Ali et al.",
      "keywords": "AI healthcare; CNN; LSTM; clinical notes; deep learning; fairness-aware modeling; glaucoma detection",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40218157/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11988537",
      "ft_text_length": 27727,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11988537)",
      "ft_reason": "Included: substantial approach content (6 indicators)"
    },
    {
      "pmid": "40253926",
      "title": "Exploring trade-offs in equitable stroke risk prediction with parity-constrained and race-free models.",
      "abstract": "A recent analysis of common stroke risk prediction models showed that performance differs between Black and White subgroups, and that applying standard machine learning methods does not reduce these disparities. There have been calls in the clinical literature to correct such disparities by removing race as a predictor (i.e., race-free models). Alternatively, a variety of machine learning methods have been proposed to constrain differences in model predictions between racial groups. In this work, we compare these approaches for equitable stroke risk prediction. We begin by proposing a discrete-time, neural network-based time-to-event model that incorporates a parity constraint designed to make predictions more similar between groups. Using harmonized data from Framingham Offspring, MESA, and ARIC studies, we develop both parity-constrained and unconstrained stroke risk prediction models, then compare their performance with race-free models in a held-out test set and a secondary validation set (REGARDS). Our evaluation includes both intra-group and inter-group performance metrics for right-censored time to event outcomes. Results illustrate a fundamental trade-off in which parity-constrained models must sacrifice intra-group calibration to improve inter-group discrimination performance, while the race-free models strike a balance between the two. Consequently, the choice of model must depend on the potential benefits and harms associated with the intended clinical use. All models as well as code implementing our approach are available in a public repository. More broadly, these results provide a roadmap for development of equitable clinical risk prediction models and illustrate both merits and limitations of a race-free approach.",
      "journal": "Artificial intelligence in medicine",
      "year": "2025",
      "doi": "10.1016/j.artmed.2025.103130",
      "authors": "Engelhard Matthew et al.",
      "keywords": "Algorithmic bias; Algorithmic fairness; Data harmonization; Machine learning; Risk prediction; Stroke",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Black or African American; Machine Learning; Neural Networks, Computer; Risk Assessment; Risk Factors; Stroke; White",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40253926/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12133243",
      "ft_text_length": 1760,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12133243)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40277252",
      "title": "Racial, ethnic, and sex disparities in buprenorphine treatment from emergency departments by discharge diagnosis.",
      "abstract": "OBJECTIVES: Racial and sex disparities are noted in the administration and prescribing of buprenorphine from emergency departments (EDs) nationally. It is unknown whether disparities persist when accounting for the specific discharge diagnosis addressed during encounters such as opioid overdose or withdrawal. METHODS: We conducted a cross-sectional analysis of opioid-related ED encounters from January 2020 through December 2023 using a national database, Epic Cosmos. We analyzed the effect of opioid encounter subtype-overdose or withdrawal-on receipt of buprenorphine using multivariable logistic regression adjusting for demographics and measured confounding variables. Encounter subtypes were defined by diagnosis codes and buprenorphine receipt was defined as administration or prescribing. We evaluated for racial, ethnic, and sex disparities within encounter subtypes for withdrawal and overdose. RESULTS: We examined 1,088,033 opioid-related encounters. Adjusted odds for buprenorphine receipt were greater for encounters involving withdrawal (odds ratio [OR]\u20092.22, 95% CI 2.18-2.26), though reduced for overdose (OR 0.52, 95% CI 0.51-0.53) and other opioid complications (OR 0.69, 95% CI 0.64-0.70). Males were more likely to receive buprenorphine (OR 1.18, 95% CI 1.16-1.19) than females. All racial minorities excepting American Indian/Native American patients (OR 1.04, 95% CI 1.00-1.08) were less likely to receive buprenorphine than White patients (Asian OR 0.85, 95% CI 0.79-0.81; Black OR 0.80, 95% CI 0.79-0.81; Native Hawaiian/Pacific Islander OR 0.79, 95% CI 0.71-0.89). Subtype analyses indicated decreased odds for buprenorphine receipt for female patients across all subtypes. An increased odds for buprenorphine receipt among Black patients (OR 1.04, 95% CI 1.01-1.07; ref. White race) was noted in encounters involving opioid withdrawal but disparities persisted for opioid overdose. CONCLUSIONS: The administration and prescribing of buprenorphine in the ED is heavily influenced by the presence of opioid withdrawal. Disparities disadvantage female patients and racial minorities. Some racial disparities, particularly among Black patients, are not evident when solely considering encounters involving opioid withdrawal. System-level interventions are needed to address disparities and improve the equitable uptake of ED-initiated buprenorphine.",
      "journal": "Academic emergency medicine : official journal of the Society for Academic Emergency Medicine",
      "year": "2025",
      "doi": "10.1111/acem.70035",
      "authors": "Chhabra Neeraj et al.",
      "keywords": "",
      "mesh_terms": "Humans; Buprenorphine; Female; Male; Emergency Service, Hospital; Cross-Sectional Studies; Healthcare Disparities; Adult; Opioid-Related Disorders; Middle Aged; Patient Discharge; Ethnicity; Sex Factors; Racial Groups; United States; Opiate Substitution Treatment; Opiate Overdose",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40277252/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12353228",
      "ft_text_length": 23679,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12353228)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40309720",
      "title": "The imperative of diversity and equity for the adoption of responsible AI in healthcare.",
      "abstract": "Artificial Intelligence (AI) in healthcare holds transformative potential but faces critical challenges in ethical accountability and systemic inequities. Biases in AI models, such as lower diagnosis rates for Black women or gender stereotyping in Large Language Models, highlight the urgent need to address historical and structural inequalities in data and development processes. Disparities in clinical trials and datasets, often skewed toward high-income, English-speaking regions, amplify these issues. Moreover, the underrepresentation of marginalized groups among AI developers and researchers exacerbates these challenges. To ensure equitable AI, diverse data collection, federated data-sharing frameworks, and bias-correction techniques are essential. Structural initiatives, such as fairness audits, transparent AI model development processes, and early registration of clinical AI models, alongside inclusive global collaborations like TRAIN-Europe and CHAI, can drive responsible AI adoption. Prioritizing diversity in datasets and among developers and researchers, as well as implementing transparent governance will foster AI systems that uphold ethical principles and deliver equitable healthcare outcomes globally.",
      "journal": "Frontiers in artificial intelligence",
      "year": "2025",
      "doi": "10.3389/frai.2025.1577529",
      "authors": "Hilling Denise E et al.",
      "keywords": "artificial intelligence; bias; diversity; equity; healthcare",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40309720/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12040885",
      "ft_text_length": 9043,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12040885)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40323319",
      "title": "Debiased machine learning for ultra-high dimensional mediation analysis.",
      "abstract": "MOTIVATION: In ultra-high dimensional mediation analysis, confounding variables can influence both mediators and outcomes through complex functional forms. While machine learning (ML) approaches are effective at modeling such complex relationships, they can introduce bias when estimating mediation effects. In this article, we propose a debiased ML framework that mitigates this bias, enabling accurate identification of key mediators and precise estimation and inference of their respective contributions. RESULTS: We construct an orthogonalized score function and use cross-fitting to reduce bias introduced by ML. To tackle ultra-high dimensional potential mediators, we implement screening and regularization techniques for variable selection and effect estimation. For statistical inference of the mediators' contributions, we use an adjusted Sobel-type test. Simulation results demonstrate the superior performance of the proposed method in handling complex confounding. Applying this method to Alzheimer's Disease Neuroimaging Initiative data, we identify several cytosine-phosphate-guanine sites where DNA methylation mediates the effect of body mass index on Alzheimer's Disease. AVAILABILITY AND IMPLEMENTATION: The R function DML_HDMA implementing the proposed methods is available online at https://github.com/Wei-Kecheng/DML_HDMA.",
      "journal": "Bioinformatics (Oxford, England)",
      "year": "2025",
      "doi": "10.1093/bioinformatics/btaf282",
      "authors": "Wei Kecheng et al.",
      "keywords": "",
      "mesh_terms": "Machine Learning; Humans; Alzheimer Disease; DNA Methylation; Algorithms; Neuroimaging; Body Mass Index",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40323319/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12198499",
      "ft_text_length": 46526,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12198499)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "40343639",
      "title": "Shortcut learning leads to sex bias in deep learning models for photoacoustic tomography.",
      "abstract": "PURPOSE: Shortcut learning has been identified as a source of algorithmic unfairness in medical imaging artificial intelligence (AI), but its impact on photoacoustic tomography (PAT), particularly concerning sex bias, remains underexplored. This study investigates this issue using peripheral artery disease (PAD) diagnosis as a specific clinical application. METHODS: To examine the potential for sex bias due to shortcut learning in convolutional neural network (CNNs) and assess how such biases might affect diagnostic predictions, we created training and test datasets with varying PAD prevalence between sexes. Using these datasets, we explored (1) whether CNNs can classify the sex from imaging data, (2) how sex-specific prevalence shifts impact PAD diagnosis performance and underdiagnosis disparity between sexes, and (3) how similarly CNNs encode sex and PAD features. RESULTS: Our study with 147 individuals demonstrates that CNNs can classify the sex from calf muscle PAT images, achieving an AUROC of 0.75. For PAD diagnosis, models trained on data with imbalanced sex-specific disease prevalence experienced significant performance drops (up to 0.21 AUROC) when applied to balanced test sets. Additionally, greater imbalances in sex-specific prevalence within the training data exacerbated underdiagnosis disparities between sexes. Finally, we identify evidence of shortcut learning by demonstrating the effective reuse of learned feature representations between PAD diagnosis and sex classification tasks. CONCLUSION: CNN-based models trained on PAT data may engage in shortcut learning by leveraging sex-related features, leading to biased and unreliable diagnostic predictions. Addressing demographic-specific prevalence imbalances and preventing shortcut learning is critical for developing models in the medical field that are both accurate and equitable across diverse patient populations.",
      "journal": "International journal of computer assisted radiology and surgery",
      "year": "2025",
      "doi": "10.1007/s11548-025-03370-9",
      "authors": "Knopp Marcel et al.",
      "keywords": "Peripheral artery disease (PAD); Photoacoustic tomography (PAT); Sex Bias in AI; Shortcut learning",
      "mesh_terms": "Humans; Deep Learning; Male; Female; Photoacoustic Techniques; Peripheral Arterial Disease; Sexism; Aged; Middle Aged; Tomography",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40343639/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12226672",
      "ft_text_length": 32586,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12226672)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "40351508",
      "title": "Evaluating algorithmic bias on biomarker classification of breast cancer pathology reports.",
      "abstract": "OBJECTIVES: This work evaluated algorithmic bias in biomarkers classification using electronic pathology reports from female breast cancer cases. Bias was assessed across 5 subgroups: cancer registry, race, Hispanic ethnicity, age at diagnosis, and socioeconomic status. MATERIALS AND METHODS: We utilized 594\u00a0875 electronic pathology reports from 178\u00a0121 tumors diagnosed in Kentucky, Louisiana, New Jersey, New Mexico, Seattle, and Utah to train 2 deep-learning algorithms to classify breast cancer patients using their biomarkers test results. We used balanced error rate (BER), demographic parity (DP), equalized odds (EOD), and equal opportunity (EOP) to assess bias. RESULTS: We found differences in predictive accuracy between registries, with the highest accuracy in the registry that contributed the most data (Seattle Registry, BER ratios for all registries >1.25). BER showed no significant algorithmic bias in extracting biomarkers (estrogen receptor, progesterone receptor, human epidermal growth factor receptor 2) for race, Hispanic ethnicity, age at diagnosis, or socioeconomic subgroups (BER ratio <1.25). DP, EOD, and EOP all showed insignificant results. DISCUSSION: We observed significant differences in BER by registry, but no significant bias using the DP, EOD, and EOP metrics for socio-demographic or racial categories. This highlights the importance of employing a diverse set of metrics for a comprehensive evaluation of model fairness. CONCLUSION: A thorough evaluation of algorithmic biases that may affect equality in clinical care is a critical step before deploying algorithms in the real world. We found little evidence of algorithmic bias in our biomarker classification tool. Artificial intelligence tools to expedite information extraction from clinical records could accelerate clinical trial matching and improve care.",
      "journal": "JAMIA open",
      "year": "2025",
      "doi": "10.1093/jamiaopen/ooaf033",
      "authors": "Tschida Jordan et al.",
      "keywords": "algorithmic bias; artificial intelligence; biomarkers; breast cancer; population-level",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40351508/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12063583",
      "ft_text_length": 1860,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12063583)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "40354109",
      "title": "Evaluation and Bias Analysis of Large Language Models in Generating Synthetic Electronic Health Records: Comparative Study.",
      "abstract": "BACKGROUND: Synthetic electronic health records (EHRs) generated by large language models (LLMs) offer potential for clinical education and model training while addressing privacy concerns. However, performance variations and demographic biases in these models remain underexplored, posing risks to equitable health care. OBJECTIVE: This study aimed to systematically assess the performance of various LLMs in generating synthetic EHRs and to critically evaluate the presence of gender and racial biases in the generated outputs. We focused on assessing the completeness and representativeness of these EHRs across 20 diseases with varying demographic prevalence. METHODS: A framework was developed to generate 140,000 synthetic EHRs using 10 standardized prompts across 7 LLMs. The electronic health record performance score (EPS) was introduced to quantify completeness, while the statistical parity difference (SPD) was proposed to assess the degree and direction of demographic bias. Chi-square tests were used to evaluate the presence of bias across demographic groups. RESULTS: Larger models exhibited superior performance but heightened biases. The Yi-34B achieved the highest EPS (96.8), while smaller models (Qwen-1.8B: EPS=63.35) underperformed. Sex polarization emerged: female-dominated diseases (eg, multiple sclerosis) saw amplified female representation in outputs (Qwen-14B: 973/1000, 97.3% female vs 564,424/744,778, 75.78% real; SPD=+21.50%), while balanced diseases and male-dominated diseases skewed the male group (eg, hypertension Llama 2-13 B: 957/1000, 95.7% male vs 79,540,040/152,466,669, 52.17% real; SPD=+43.50%). Racial bias patterns revealed that some models overestimated the representation of White (eg, Yi-6B: mean SPD +14.40%, SD 16.22%) or Black groups (eg, Yi-34B: mean SPD +14.90%, SD 27.16%), while most models systematically underestimated the representation of Hispanic (average SPD across 7 models is -11.93%, SD 8.36%) and Asian groups (average SPD across 7 models is -0.77%, SD 11.99%). CONCLUSIONS: Larger models, such as Yi-34B, Qwen-14B, and Llama 2 to 13 B, showed improved performance in generating more comprehensive EHRs, as reflected in higher EPS values. However, this increased performance was accompanied by a notable escalation in both gender and racial biases, highlighting a performance-bias trade-off. The study identified 4 key findings as follows: (1) as model size increased, EHR generation improved, but demographic biases also became more pronounced; (2) biases were observed across all models, not just the larger ones; (3) gender bias closely aligned with real-world disease prevalence, while racial bias was evident in only a subset of diseases; and (4) racial biases varied, with some diseases showing overrepresentation of White or Black populations and underrepresentation of Hispanic and Asian groups. These findings underline the need for effective bias mitigation strategies and the development of benchmarks to ensure fairness in artificial intelligence applications for health care.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/65317",
      "authors": "Huang Ruochen et al.",
      "keywords": "artificial intelligence; electronic health records; gender bias; large language models; performance evaluation; racial bias",
      "mesh_terms": "Electronic Health Records; Humans; Female; Male; Bias; Large Language Models",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40354109/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12107208",
      "ft_text_length": 60014,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12107208)",
      "ft_reason": "Included: bias central + approach content (9 indicators)"
    },
    {
      "pmid": "40382499",
      "title": "Fair ultrasound diagnosis via adversarial protected attribute aware perturbations on latent embeddings.",
      "abstract": "Deep learning techniques have significantly enhanced the convenience and precision of ultrasound image diagnosis, particularly in the crucial step of lesion segmentation. However, recent studies reveal that both train-from-scratch models and pre-trained models often exhibit performance disparities across sex and age attributes, leading to biased diagnoses for different subgroups. In this paper, we propose APPLE, a novel approach designed to mitigate unfairness without altering the parameters of the base model. APPLE achieves this by learning fair perturbations in the latent space through a generative adversarial network. Extensive experiments on both a publicly available dataset and an in-house ultrasound image dataset demonstrate that our method improves segmentation and diagnostic fairness across all sensitive attributes and various backbone architectures compared to the base models. Through this study, we aim to highlight the critical importance of fairness in medical segmentation and contribute to the development of a more equitable healthcare system.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01641-y",
      "authors": "Xu Zikang et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40382499/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12085594",
      "ft_text_length": 40078,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12085594)",
      "ft_reason": "Included: bias central + approach content (7 indicators)"
    },
    {
      "pmid": "40384063",
      "title": "Large Language Models in Medicine: Clinical Applications, Technical Challenges, and Ethical Considerations.",
      "abstract": "OBJECTIVES: This study presents a comprehensive review of the clinical applications, technical challenges, and ethical considerations associated with using large language models (LLMs) in medicine. METHODS: A literature survey of peer-reviewed articles, technical reports, and expert commentary from relevant medical and artificial intelligence journals was conducted. Key clinical application areas, technical limitations (e.g., accuracy, validation, transparency), and ethical issues (e.g., bias, safety, accountability, privacy) were identified and analyzed. RESULTS: LLMs have potential in clinical documentation assistance, decision support, patient communication, and workflow optimization. The level of supporting evidence varies; documentation support applications are relatively mature, whereas autonomous diagnostics continue to face notable limitations regarding accuracy and validation. Key technical challenges include model hallucination, lack of robust clinical validation, integration issues, and limited transparency. Ethical concerns involve algorithmic bias risking health inequities, threats to patient safety from inaccuracies, unclear accountability, data privacy, and impacts on clinician-patient interactions. CONCLUSIONS: LLMs possess transformative potential for clinical medicine, particularly by augmenting clinician capabilities. However, substantial technical and ethical hurdles necessitate rigorous research, validation, clearly defined guidelines, and human oversight. Existing evidence supports an assistive rather than autonomous role, mandating careful, evidence-based integration that prioritizes patient safety and equity.",
      "journal": "Healthcare informatics research",
      "year": "2025",
      "doi": "10.4258/hir.2025.31.2.114",
      "authors": "Jung Kyu-Hwan",
      "keywords": "Artificial Intelligence; Clinical Decision Support Systems; Medical Ethics; Medical Informatics Applications; Natural Language Processing",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40384063/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12086438",
      "ft_text_length": 22250,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12086438)",
      "ft_reason": "Included: substantial approach content (3 indicators)"
    },
    {
      "pmid": "40395418",
      "title": "Racial and ethnic disparities in aortic stenosis within a universal healthcare system characterized by natural language processing for targeted intervention.",
      "abstract": "AIMS: Aortic stenosis (AS) is a condition marked by high morbidity and mortality in severe, symptomatic cases without intervention via transcatheter aortic valve implantation (TAVI) or surgical aortic valve replacement (SAVR). Racial and ethnic disparities in access to these treatments have been documented, particularly in North America, where socioeconomic factors such as health insurance confound analyses. This study evaluates disparities in AS management across racial and ethnic groups, accounting for socioeconomic deprivation, using an artificial intelligence (AI) framework. METHODS AND RESULTS: We conducted a retrospective cohort study using a natural language processing pipeline to analyse both structured and unstructured data from > 1 million patients at a London hospital. Key variables included age, sex, self-reported race and ethnicity, AS severity, and socioeconomic status. The primary outcomes were rates of valvular intervention and all-cause mortality. Among 6967 patients with AS, Black patients were younger, more symptomatic, and more comorbid than White patients. Black patients with objective evidence of AS on echocardiography were less likely to receive a clinical diagnosis than White patients. In severe AS, TAVI and SAVR procedures were performed at lower rates among Black patients than among White patients, with a longer time to SAVR. In multivariate analysis of severe AS, controlling for socioeconomic status, Black patients experienced higher mortality (hazard ratio = 1.42, 95% confidence interval = 1.05-1.92, P = 0.02). CONCLUSION: An AI framework characterizes racial and ethnic disparities in AS management, which persist in a universal healthcare system, highlighting targets for future healthcare interventions.",
      "journal": "European heart journal. Digital health",
      "year": "2025",
      "doi": "10.1093/ehjdh/ztaf018",
      "authors": "Biswas Dhruva et al.",
      "keywords": "Aortic stenosis; Artificial intelligence; Ethnicity; Health equity; Transcatheter aortic valve replacement",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40395418/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12088714",
      "ft_text_length": 41363,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12088714)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40404862",
      "title": "Mitigating bias in prostate cancer diagnosis using synthetic data for improved AI driven Gleason grading.",
      "abstract": "Prostate cancer (PCa) is a leading cause of cancer-related mortality in men, with Gleason grading critical for prognosis and treatment decisions. Machine learning (ML) models offer potential for automated grading but are limited by dataset biases, staining variability, and data scarcity, reducing their generalizability. This study employs generative adversarial networks (GANs) to generate high-quality synthetic histopathological images to address these challenges. A conditional GAN (dcGAN) was developed and validated using expert pathologist review and Spatial Heterogeneous Recurrence Quantification Analysis (SHRQA), achieving 80% diagnostic quality approval. A convolutional neural network (EfficientNet) was trained on original and synthetic images and validated across TCGA, PANDA Challenge, and MAST trial datasets. Integrating synthetic images improved classification accuracy for Gleason 3 (26%, p\u2009=\u20090.0010), Gleason 4 (15%, p\u2009=\u20090.0274), and Gleason 5 (32%, p\u2009<\u20090.0001), with sensitivity and specificity reaching 81% and 92%, respectively. This study demonstrates that synthetic data significantly enhances ML-based Gleason grading accuracy and improves reproducibility, providing a scalable AI-driven solution for precision oncology.",
      "journal": "NPJ precision oncology",
      "year": "2025",
      "doi": "10.1038/s41698-025-00934-5",
      "authors": "Van Booven Derek J et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40404862/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12098719",
      "ft_text_length": 41120,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12098719)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40417489",
      "title": "Ensuring Fairness in Detecting Mild Cognitive Impairment with MRI.",
      "abstract": "Machine learning (ML) algorithms play a crucial role in the early and accurate diagnosis of Alzheimer's Disease (AD), which is essential for effective treatment planning. However, existing methods are not well-suited for identifying Mild Cognitive Impairment (MCI), a critical transitional stage between normal aging and AD. This inadequacy is primarily due to label imbalance and bias from different sensitve attributes in MCI classification. To overcome these challenges, we have designed an end-to-end fairness-aware approach for label-imbalanced classification, tailored specifically for neuroimaging data. This method, built on the recently developed FACIMS framework, integrates into STREAMLINE, an automated ML environment. We evaluated our approach against nine other ML algorithms and found that it achieves comparable balanced accuracy to other methods while prioritizing fairness in classifications with five different sensitive attributes. This analysis contributes to the development of equitable and reliable ML diagnostics for MCI detection.",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "year": "2024",
      "doi": "",
      "authors": "Tong Boning et al.",
      "keywords": "",
      "mesh_terms": "Cognitive Dysfunction; Humans; Magnetic Resonance Imaging; Machine Learning; Algorithms; Alzheimer Disease",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40417489/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12099326",
      "ft_text_length": 1056,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12099326)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40435158",
      "title": "Is there a competitive advantage to using multivariate statistical or machine learning methods over the Bross formula in the hdPS framework for bias and variance estimation?",
      "abstract": "PURPOSE: We aim to evaluate various proxy selection methods within the context of high-dimensional propensity score (hdPS) analysis. This study aimed to systematically evaluate and compare the performance of traditional statistical methods and machine learning approaches within the hdPS framework, focusing on key metrics such as bias, standard error (SE), and coverage, under various exposure and outcome prevalence scenarios. METHODS: We conducted a plasmode simulation study using data from the National Health and Nutrition Examination Survey (NHANES) cycles from 2013 to 2018. We compared methods including the kitchen sink model, Bross-based hdPS, Hybrid hdPS, LASSO, Elastic Net, Random Forest, XGBoost, and Genetic Algorithm (GA). The performance of each inverse probability weighted method was assessed based on bias, MSE, coverage probability, and SE estimation across three epidemiological scenarios: frequent exposure and outcome, rare exposure and frequent outcome, and frequent exposure and rare outcome. RESULTS: XGBoost consistently demonstrated strong performance in terms of MSE and coverage, making it effective for scenarios prioritizing precision. However, it exhibited higher bias, particularly in rare exposure scenarios, suggesting it is less suited when minimizing bias is critical. In contrast, GA showed significant limitations, with consistently high bias and MSE, making it the least reliable method. Bross-based hdPS, and Hybrid hdPS methods provided a balanced approach, with low bias and moderate MSE, though coverage varied depending on the scenario. Rare outcome scenarios generally resulted in lower MSE and better precision, while rare exposure scenarios were associated with higher bias and MSE. Notably, traditional statistical approaches such as forward selection and backward elimination performed comparably to more sophisticated machine learning methods in terms of bias and coverage, suggesting that these simpler approaches may be viable alternatives due to their computational efficiency. CONCLUSION: The results highlight the importance of selecting hdPS methods based on the specific characteristics of the data, such as exposure and outcome prevalence. While advanced machine learning methods such as XGBoost can enhance precision, simpler methods such as forward selection or backward elimination may offer similar performance in terms of bias and coverage with fewer computational demands. Tailoring the choice of method to the epidemiological scenario is essential for optimizing the balance between bias reduction and precision.",
      "journal": "PloS one",
      "year": "2025",
      "doi": "10.1371/journal.pone.0324639",
      "authors": "Ehsanul Karim Mohammad et al.",
      "keywords": "",
      "mesh_terms": "Machine Learning; Humans; Bias; Propensity Score; Nutrition Surveys; Algorithms; Multivariate Analysis; Models, Statistical; Computer Simulation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40435158/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12118903",
      "ft_text_length": 47875,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12118903)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "40440013",
      "title": "Intersectional and Marginal Debiasing in Prediction Models for Emergency Admissions.",
      "abstract": "IMPORTANCE: Fair clinical prediction models are crucial for achieving equitable health outcomes. Intersectionality has been applied to develop algorithms that address discrimination among intersections of protected attributes (eg, Black women rather than Black persons or women separately), yet most fair algorithms default to marginal debiasing, optimizing performance across simplified patient subgroups. OBJECTIVE: To assess the extent to which simplifying patient subgroups during training is associated with intersectional subgroup performance in emergency department (ED) admission models. DESIGN, SETTING, AND PARTICIPANTS: This prognostic study of admission prediction models used retrospective data from ED visits to Beth Israel Deaconess Medical Center Medical Information Mart for Intensive Care IV (MIMIC-IV; n\u2009=\u2009160\u202f016) from January 1, 2011, to December 31, 2019, and Boston Children's Hospital (BCH; n\u2009=\u200922\u202f222) from June 1 through August 13, 2019. Statistical analysis was conducted from January 2022 to August 2024. MAIN OUTCOMES AND MEASURES: The primary outcome was admission to an in-patient service. The accuracy of admission predictions among intersectional subgroups was measured under variations on model training with respect to optimizing for group level performance. Under different fairness definitions (calibration, error rate balance) and modeling methods (linear, nonlinear), overall performance and subgroup performance of marginal debiasing approaches were compared with intersectional debiasing approaches. Subgroups were defined by self-reported race and ethnicity and gender. Measures include area under the receiver operator characteristic curve (AUROC), area under the precision recall curve, subgroup calibration error, and false-negative rates. RESULTS: The MIMIC-IV cohort included 160\u202f016 visits (mean [SD] age, 53.0 [19.3] years; 57.4% female patients; 0.3% American Indian or Alaska Native patients, 3.7% Asian patients, 26.2% Black patients, 10.0% Hispanic or Latino patients, and 59.7% White patients; 29.5% admitted) and the BCH cohort included 22\u202f222 visits (mean [SD] age, 8.2 [6.8] years; 52.1% male patients; 0.1% American Indian or Alaska Native patients, 4.0% Asian patients, 19.7% Black patients, 30.6% Hispanic or Latino patients, 0.2% Native Hawaiian or Pacific Islander patients, 37.7% White patients; 16.3% admitted). Among MIMIC-IV groups, intersectional debiasing was associated with a reduced subgroup calibration error from 0.083 to 0.065 (22.3%), while marginal fairness debiasing was associated with a reduced subgroup calibration error from 0.083 to 0.074 (11.3%; difference, 11.1%); among BCH groups, intersectional debiasing was associated with a reduced subgroup calibration error from 0.111 to 0.080 (28.3%), while marginal fairness debiasing was associated with a reduced subgroup calibration error from 0.111 to 0.086 (22.6%; difference, 5.7%). Among MIMIC-IV groups, intersectional debiasing was associated with lowered subgroup false-negative rates from 0.142 to 0.125 (11.9%), while marginal debiasing was associated with lowered subgroup false-negative rates from 0.142 to 0.132 (6.8%; difference, 5.1%). Fairness improvements did not decrease overall accuracy compared with baseline models (eg, MIMIC-IV: mean [SD] AUROC, 0.85 [0.00], both models). Intersectional debiasing was associated with lowered error rates in several intersectional subpopulations compared with other strategies. CONCLUSIONS AND RELEVANCE: This study suggests that intersectional debiasing better mitigates performance disparities across intersecting groups than marginal debiasing for admission prediction. Intersectionally debiased models were associated with reduced group-specific errors without compromising overall accuracy. Clinical risk prediction models should consider incorporating intersectional debiasing into their development.",
      "journal": "JAMA network open",
      "year": "2025",
      "doi": "10.1001/jamanetworkopen.2025.12947",
      "authors": "Lett Elle et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Emergency Service, Hospital; Male; Retrospective Studies; Patient Admission; Adult; Middle Aged; Boston; Algorithms; Models, Statistical",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40440013/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12123471",
      "ft_text_length": 27852,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12123471)",
      "ft_reason": "Included: bias central + approach content (9 indicators)"
    },
    {
      "pmid": "40440407",
      "title": "Standardization and accuracy of race and ethnicity data: Equity implications for medical AI.",
      "abstract": "The rapid integration of artificial intelligence (AI) into healthcare has raised many concerns about race bias in AI models. Yet, overlooked in this dialogue is the lack of quality control for the accuracy of patient race and ethnicity (r/e) data in electronic health records (EHR). This article critically examines the factors driving inaccurate and unrepresentative r/e datasets. These include conceptual uncertainties about how to categorize races and ethnicity, shortcomings in data collection practices, EHR standards, and the misclassification of patients' race or ethnicity. To address these challenges, we propose a two-pronged action plan. First, we present a set of best practices for healthcare systems and medical AI researchers to improve r/e data accuracy. Second, we call for developers of medical AI models to transparently warrant the quality of their r/e data. Given the ethical and scientific imperatives of ensuring high-quality r/e data in AI-driven healthcare, we argue that these steps should be taken immediately.",
      "journal": "PLOS digital health",
      "year": "2025",
      "doi": "10.1371/journal.pdig.0000807",
      "authors": "Tsalidis Alexandra et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40440407/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12121746",
      "ft_text_length": 26000,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12121746)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "40444224",
      "title": "The effectiveness, equity and explainability of health service resource allocation-with applications in kidney transplantation & family planning.",
      "abstract": "INTRODUCTION: Halfway to the deadline of the 2030 agenda, humankind continues to face long-standing yet urgent policy and management challenges to address resource shortages and deliver on Sustainable Development Goal 3; health and well-being for all at all ages. More than half of the global population lacks access to essential health services. Additional resources are required and need to be allocated effectively and equitably. Resource allocation models, however, have struggled to accurately predict effects and to present optimal allocations, thus hampering effectiveness and equity improvement. The current advances in machine learning present opportunities to better predict allocation effects and to prescribe solutions that better balance effectiveness and equity. The most advanced of these models tend to be \"black box\" models that lack explainability. This lack of explainability is problematic as it can clash with professional values and hide biases that negatively impact effectiveness and equity. METHODS: Through a novel theoretical framework and two diverse case studies, this manuscript explores the trade-offs between effectiveness, equity, and explainability. The case studies consider family planning in a low income country and kidney allocation in a high income country. RESULTS: Both case studies find that the least explainable models hardly offer improvements in effectiveness and equity over explainable alternatives. DISCUSSION: As this may more widely apply to health resource allocation decisions, explainable analytics, which are more likely to be trusted and used, might better enable progress towards SDG3 for now. Future research on explainability, also in relation to equity and fairness of allocation policies, can help deliver on the promise of advanced predictive and prescriptive analytics.",
      "journal": "Frontiers in health services",
      "year": "2025",
      "doi": "10.3389/frhs.2025.1545864",
      "authors": "van de Klundert Joris et al.",
      "keywords": "effectiveness; equity; explainability; explainable AI; family planning; healthcare analytics; kidney allocation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40444224/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12119484",
      "ft_text_length": 61845,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12119484)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40445905",
      "title": "Towards robust medical machine olfaction: Debiasing GC-MS data enhances prostate cancer diagnosis from urine volatiles.",
      "abstract": "Prostate cancer (PCa) is a major, and increasingly global, health concern with current screening and diagnostic tools' severe limitations causing unnecessary, invasive biopsy procedures. While gas chromatography-mass spectrometry (GC-MS) has been used to detect urinary volatile organic compounds (VOCs) associated with PCa, efforts to identify consistent molecular biomarkers have failed to generalize across studies. Inspired by the olfactory diagnostic capabilities of medical detection dogs, we do not reduce chromatograms to a list of compounds and concentrations. Instead, we deploy a machine learning approach that bypasses molecular identification: PCa \"scent character\" signatures are extracted from raw time series data transformed into image representations for classification via convolutional neural networks. To address confounding factors such as sample-source bias, we implement a multi-step pre-processing and debiasing pipeline, including empirical Bayes correction, baseline drift removal, and domain adversarial learning. The resulting model achieves classification performance on par with similarly trained canines, achieving a recall of 88% and an F1-score of 0.78. These findings demonstrate that, at least in the context of PCa detection from urine, machine learning-based scent signature analysis can serve as a fully non-invasive diagnostic alternative, with these early results being also relevant to the wider emergent field of medical machine olfaction.",
      "journal": "PloS one",
      "year": "2025",
      "doi": "10.1371/journal.pone.0314742",
      "authors": "Rotteveel Adan et al.",
      "keywords": "",
      "mesh_terms": "Male; Prostatic Neoplasms; Volatile Organic Compounds; Gas Chromatography-Mass Spectrometry; Humans; Machine Learning; Dogs; Animals; Smell; Bayes Theorem; Neural Networks, Computer",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40445905/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12124533",
      "ft_text_length": 41588,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12124533)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "40463575",
      "title": "Leveraging neighborhood-level Information to Improve Model Fairness in Predicting Prenatal Depression.",
      "abstract": "IMPORTANCE: Perinatal depression (PND) affects 10-20% of pregnant women, with significant racial disparities in prevalence, screening, and treatment. Neighborhood-level factors significantly influence PND risk, particularly among women of color, but current machine learning models using electronic medical records (EMRs) rarely incorporate neighborhood characteristics. OBJECTIVE: To determine whether integrating neighborhood-level information with EMRs improves fairness in PND prediction while identifying key neighborhood factors influencing model bias across racial/ethnic groups. DESIGN SETTING AND PARTICIPANTS: Study of 6,137 pregnant women who received care at a large urban academic hospital from 2010-2019, comprising 58% Non-Hispanic Black (NHB), 10% Non-Hispanic White (NHW), and 28% Hispanic (H) individuals, with depression status determined by PHQ-9 scores. EXPOSURES: 125 neighborhood-level factors from Chicago Health Atlas merged with 61 EMR features based on residential location. MAIN OUTCOMES AND MEASURES: Model performance (ROCAUC, PRAUC) and fairness metrics (disparate impact, equal opportunity difference, equalized odds). Feature importance analyzed using Shapley values and the impact of each neighborhood factor on model bias were evaluated. Results Models integrating neighborhood-level measures showed moderate predictive performance (ROCAUC: NHB 55%, NHW 57%, H 58%) while significantly improving fairness metrics compared to EMR-only models (p<0.05). Factors, such as suicide mortality rate and neighborhood safety rate, helped reduce bias. NHB women showed stronger correlations between PND risk factors and neighborhood variables compared to other groups. Most neighborhood factors had differential impacts across racial/ethnic groups, increasing bias for NHB women while reducing it for Hispanic women. CONCLUSIONS AND RELEVANCE: Incorporating neighborhood-level information enhances fairness in PND prediction while maintaining predictive capability. The differential impact of neighborhood factors across racial/ethnic groups highlights the importance of considering neighborhood context in clinical risk assessment to reduce disparities in prenatal depression care.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.05.12.25327329",
      "authors": "Huang Yongchao et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40463575/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12132113",
      "ft_text_length": 2215,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12132113)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "40467886",
      "title": "Racial bias in AI-mediated psychiatric diagnosis and treatment: a qualitative comparison of four large language models.",
      "abstract": "Artificial intelligence (AI), particularly large language models (LLMs), is increasingly integrated into mental health care. This study examined racial bias in psychiatric diagnosis and treatment across four leading LLMs: Claude, ChatGPT, Gemini, and NewMes-15 (a local, medical-focused LLaMA 3 variant). Ten psychiatric patient cases representing five diagnoses were presented to these models under three conditions: race-neutral, race-implied, and race-explicitly stated (i.e., stating patient is African American). The models' diagnostic recommendations and treatment plans were qualitatively evaluated by a clinical psychologist and a social psychologist, who scored 120 outputs for bias by comparing responses generated under race-neutral, race-implied, and race-explicit conditions. Results indicated that LLMs often proposed inferior treatments when patient race was explicitly or implicitly indicated, though diagnostic decisions demonstrated minimal bias. NewMes-15 exhibited the highest degree of racial bias, while Gemini showed the least. These findings underscore critical concerns about the potential for AI to perpetuate racial disparities in mental healthcare, emphasizing the necessity of rigorous bias assessment in algorithmic medical decision support systems.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01746-4",
      "authors": "Bouguettaya Ayoub et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40467886/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12137607",
      "ft_text_length": 25155,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12137607)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "40473916",
      "title": "Identifying and mitigating algorithmic bias in the safety net.",
      "abstract": "Algorithmic bias occurs when predictive model performance varies meaningfully across sociodemographic classes, exacerbating systemic healthcare disparities. NYC Health + Hospitals, an urban safety net system, assessed bias in two binary classification models in our electronic medical record: one predicting acute visits for asthma and one predicting unplanned readmissions. We evaluated differences in subgroup performance across race/ethnicity, sex, language, and insurance using equal opportunity difference (EOD), a metric comparing false negative rates. The most biased classes (race/ethnicity for asthma, insurance for readmission) were targeted for mitigation using threshold adjustment, which adjusts subgroup thresholds to minimize EOD, and reject option classification, which re-classifies scores near the threshold by subgroup. Successful mitigation was defined as 1) absolute subgroup EODs <5 percentage points, 2) accuracy reduction <10%, and 3) alert rate change <20%. Threshold adjustment met these criteria; reject option classification did not. We introduce a Supplementary Playbook outlining our approach for low-resource bias mitigation.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01732-w",
      "authors": "Mackin Shaina et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40473916/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12141433",
      "ft_text_length": 49049,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12141433)",
      "ft_reason": "Included: bias central + approach content (19 indicators)"
    },
    {
      "pmid": "40487862",
      "title": "Gender Disparities in Artificial Intelligence-Generated Images of Hospital Leadership in the United States.",
      "abstract": "OBJECTIVE: To evaluate demographic representation in artificial intelligence (AI)-generated images of hospital leadership roles and compare them with real-world data from US hospitals. PATIENTS AND METHODS: This cross-sectional study, conducted from October 1, 2024 to October 31, 2024, analyzed images generated by 3 AI text-to-image models: Midjourney 6.0, OpenAI ChatGPT DALL-E 3, and Google Gemini Imagen 3. Standardized prompts were used to create 1200 images representing 4 key leadership roles: chief executive officers, chief medical officers, chief nursing officers, and chief financial officers. Real-world demographic data from 4397 US hospitals showed that chief executive officers were 73.2% men; chief financial officers, 65.2% men; chief medical officers, 85.7% men; and chief nursing officers, 9.4% men (overall: 60.1% men). The primary outcome was gender representation, with secondary outcomes including race/ethnicity and age. Two independent reviewers assessed images, with interrater reliability evaluated using Cohen \u03ba. RESULTS: Interrater agreement was high for gender (\u03ba=0.998) and moderate for race/ethnicity (\u03ba=0.670) and age (\u03ba=0.605). DALL-E overrepresented men (86.5%) and White individuals (94.5%). Midjourney showed improved gender balance (69.5% men) but overrepresented White individuals (75.0%). Imagen achieved near gender parity (50.3% men) but remained predominantly White (51.5%). Statistically significant differences were observed across models and between models and real-world demographics. CONCLUSION: Artificial intelligence text-to-image models reflect and amplify systemic biases, overrepresenting men and White leaders, while underrepresenting diversity. Ethical AI practices, including diverse training data sets and fairness-aware algorithms, are essential to ensure equitable representation in health care leadership.",
      "journal": "Mayo Clinic proceedings. Digital health",
      "year": "2025",
      "doi": "10.1016/j.mcpdig.2025.100218",
      "authors": "Gisselbaek Mia et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40487862/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12140938",
      "ft_text_length": 25802,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12140938)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "40495929",
      "title": "Evaluating artificial intelligence bias in nephrology: the role of diversity, equity, and inclusion in AI-driven decision-making and ethical regulation.",
      "abstract": "BACKGROUND: The integration of Artificial Intelligence (AI) in nephrology has raised concerns regarding bias, fairness, and ethical decision-making, particularly in the context of Diversity, Equity, and Inclusion (DEI). AI-driven models, including Large Language Models (LLMs) like ChatGPT, may unintentionally reinforce existing disparities in patient care and workforce recruitment. This study investigates how AI models (ChatGPT 3.5 and 4.0) handle DEI-related ethical considerations in nephrology, highlighting the need for improved regulatory oversight to ensure equitable AI deployment. METHODS: The study was conducted in March 2024 using ChatGPT 3.5 and 4.0. Eighty simulated cases were developed to assess ChatGPT's decision-making across diverse nephrology topics. ChatGPT was instructed to respond to questions considering factors such as age, sex, gender identity, race, ethnicity, religion, cultural beliefs, socioeconomic status, education level, family structure, employment, insurance, geographic location, disability, mental health, language proficiency, and technology access. RESULTS: ChatGPT 3.5 provided a response to all scenario questions and did not refuse to make decisions under any circumstances. This contradicts the essential DEI principle of avoiding decisions based on potentially discriminatory criteria. In contrast, ChatGPT 4.0 declined to make decisions based on potentially discriminatory criteria in 13 (16.3%) scenarios during the first round and in 5 (6.3%) during the second round. CONCLUSION: While ChatGPT 4.0 shows improvement in ethical AI decision-making, its limited recognition of bias and DEI considerations underscores the need for robust AI regulatory frameworks in nephrology. AI governance must incorporate structured DEI guidelines, ongoing bias detection mechanisms, and ethical oversight to prevent AI-driven disparities in clinical practice and workforce recruitment. This study emphasizes the importance of transparency, fairness, and inclusivity in AI development, calling for collaborative efforts between AI developers, nephrologists, policymakers, and patient communities to ensure AI serves as an equitable tool in nephrology.",
      "journal": "Frontiers in artificial intelligence",
      "year": "2025",
      "doi": "10.3389/frai.2025.1525937",
      "authors": "Balakrishnan Suryanarayanan et al.",
      "keywords": "ChatGPT; artificial intelligence; bias detection; clinical implications; decision-making; diversity, equity, and inclusion; ethical AI regulation; nephrology",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40495929/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12150873",
      "ft_text_length": 28782,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12150873)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "40502243",
      "title": "A Generalized Tool to Assess Algorithmic Fairness in Disease Phenotype Definitions.",
      "abstract": "For evidence from observational studies to be reliable, researchers must ensure that the patient populations of interest are accurately defined. However, disease definitions can be extremely difficult to standardize and implement accurately across different datasets and study requirements. Furthermore, in this context, they must also ensure that populations are represented fairly to accurately reflect populations' various demographic dynamics and to not overgeneralize across non-applicable populations. In this work, we present a generalized tool to assess the fairness of disease definitions by evaluating their implementation across common fairness metrics. Our approach calculates fairness metrics and provides a robust method to examine coarse and strongly intersecting populations across many characteristics. We highlight workflows when working with disease definitions, provide an example analysis using an OMOP CDM patient database, and discuss potential directions for future improvement and research.",
      "journal": "AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science",
      "year": "2025",
      "doi": "",
      "authors": "Zelko Jacob S et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40502243/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12150753",
      "ft_text_length": 1018,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12150753)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40502572",
      "title": "Evaluating the performance and potential bias of predictive models for detection of transthyretin cardiac amyloidosis.",
      "abstract": "BACKGROUND: Delays in the diagnosis of transthyretin amyloid cardiomyopathy (ATTR-CM) contribute to the significant morbidity of the condition, especially in the era of disease-modifying therapies. Screening for ATTR-CM with AI and other algorithms may improve timely diagnosis, but these algorithms have not been directly compared. OBJECTIVES: The aim of this study was to compare the performance of four algorithms for ATTR-CM detection in a heart failure population and assess the risk for harms due to model bias. METHODS: We identified patients in an integrated health system from 2010-2022 with ATTR-CM and age- and sex-matched them to controls with heart failure to target 5% prevalence. We compared the performance of a claims-based random forest model (Huda et al. model), a regression-based score (Mayo ATTR-CM), and two deep learning echo models (EchoNet-LVH and EchoGo \u00ae Amyloidosis). We evaluated for bias using standard fairness metrics. RESULTS: The analytical cohort included 176 confirmed cases of ATTR-CM and 3192 control patients with 79.2% self-identified as White and 9.0% as Black. The Huda et al. model performed poorly (AUC 0.49). Both deep learning echo models had a higher AUC when compared to the Mayo ATTR-CM Score (EchoNet-LVH 0.88; EchoGo Amyloidosis 0.92; Mayo ATTR-CM Score 0.79; DeLong P<0.001 for both). Bias auditing met fairness criteria for equal opportunity among patients who identified as Black. CONCLUSIONS: Deep learning, echo-based models to detect ATTR-CM demonstrated best overall discrimination when compared to two other models in external validation with low risk of harms due to racial bias.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2024.10.09.24315202",
      "authors": "Hourmozdi Jonathan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40502572/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12155028",
      "ft_text_length": 1652,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12155028)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "40514386",
      "title": "Uncovering ethical biases in publicly available fetal ultrasound datasets.",
      "abstract": "We explore biases present in publicly available fetal ultrasound (US) imaging datasets, currently at the disposal of researchers to train deep learning (DL) algorithms for prenatal diagnostics. As DL increasingly permeates the field of medical imaging, the urgency to critically evaluate the fairness of benchmark public datasets used to train them grows. Our thorough investigation reveals a multifaceted bias problem, encompassing issues such as lack of demographic representativeness, limited diversity in clinical conditions depicted, and variability in US technology used across datasets. We argue that these biases may significantly influence DL model performance, which may lead to inequities in healthcare outcomes. To address these challenges, we recommend a multilayered approach. This includes promoting practices that ensure data inclusivity, such as diversifying data sources and populations, and refining model strategies to better account for population variances. These steps will enhance the trustworthiness of DL algorithms in fetal US analysis.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01739-3",
      "authors": "Fiorentino Maria Chiara et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40514386/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12166052",
      "ft_text_length": 76501,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12166052)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "40517328",
      "title": "FairICP: identifying biases and increasing transparency at the point of care in post-implementation clinical decision support using inductive conformal prediction.",
      "abstract": "OBJECTIVES: Fairness concerns stemming from known and unknown biases in healthcare practices have raised questions about the trustworthiness of Artificial Intelligence (AI)-driven Clinical Decision Support Systems (CDSS). Studies have shown unforeseen performance disparities in subpopulations when applied to clinical settings different from training. Existing unfairness mitigation strategies often struggle with scalability and accessibility, while their pursuit of group-level prediction performance parity does not effectively translate into fairness at the point of care. This study introduces FairICP, a flexible and cost-effective post-implementation framework based on Inductive Conformal Prediction (ICP), to provide users with actionable knowledge of model uncertainty due to subpopulation level biases at the point of care. MATERIALS AND METHODS: FairICP applies ICP to identify the model's scope of competence through group specific calibration, ensuring equitable prediction reliability by filtering predictions that fall within the trusted competence boundaries. We evaluated FairICP against four benchmarks on three medical imaging modalities: (1) Cardiac Magnetic Resonance Imaging (MRI), (2) Chest X-ray and (3) Dermatology Imaging, acquired from both private and large public datasets. Frameworks are assessed on prediction performance enhancement and unfairness mitigation capabilities. RESULTS: Compared to the baseline, FairICP improved prediction accuracy by 7.2% and reduced the accuracy gap between the privileged and unprivileged subpopulations by 2.2% on average across all three datasets. DISCUSSION AND CONCLUSION: Our work provides a robust solution to promote trust and transparency in AI-CDSS, fostering equality and equity in healthcare for diverse patient populations. Such post-process methods are critical to enabling a robust framework for AI-CDSS implementation and monitoring for healthcare settings.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2025",
      "doi": "10.1093/jamia/ocaf095",
      "authors": "Sun Xiaotan et al.",
      "keywords": "artificial intelligence; clinical decision support; fairness; model uncertainty",
      "mesh_terms": "Humans; Decision Support Systems, Clinical; Artificial Intelligence; Point-of-Care Systems; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40517328/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12277691",
      "ft_text_length": 1944,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12277691)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40553457",
      "title": "Forewarning Artificial Intelligence about Cognitive Biases.",
      "abstract": "Artificial intelligence models display human-like cognitive biases when generating medical recommendations. We tested whether an explicit forewarning, \"Please keep in mind cognitive biases and other pitfalls of reasoning,\" might mitigate biases in OpenAI's generative pretrained transformer large language model. We used 10 clinically nuanced cases to test specific biases with and without a forewarning. Responses from the forewarning group were 50% longer and discussed cognitive biases more than 100 times more frequently compared with responses from the control group. Despite these differences, the forewarning decreased overall bias by only 6.9%, and no bias was extinguished completely. These findings highlight the need for clinician vigilance when interpreting generated responses that might appear seemingly thoughtful and deliberate.HighlightsArtificial intelligence models can be warned to avoid racial and gender bias.Forewarning artificial intelligence models to avoid cognitive biases does not adequately mitigate multiple pitfalls of reasoning.Critical reasoning remains an important clinical skill for practicing physicians.",
      "journal": "Medical decision making : an international journal of the Society for Medical Decision Making",
      "year": "2025",
      "doi": "10.1177/0272989X251346788",
      "authors": "Wang Jonathan et al.",
      "keywords": "algorithm bias; artificial intelligence; bias in medicine; clinical decision making; cognitive psychology; large language model",
      "mesh_terms": "Humans; Artificial Intelligence; Cognition; Female; Male; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40553457/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12413502",
      "ft_text_length": 8021,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12413502)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40574796",
      "title": "From Biased Selective Labels to Pseudo-Labels: An Expectation-Maximization Framework for Learning from Biased Decisions.",
      "abstract": "Selective labels occur when label observations are subject to a decision-making process; e.g., diagnoses that depend on the administration of laboratory tests. We study a clinically-inspired selective label problem called disparate censorship, where labeling biases vary across subgroups and unlabeled individuals are imputed as \"negative\" (i.e., no diagnostic test = no illness). Machine learning models na\u00efvely trained on such labels could amplify labeling bias. Inspired by causal models of selective labels, we propose Disparate Censorship Expectation-Maximization (DCEM), an algorithm for learning in the presence of disparate censorship. We theoretically analyze how DCEM mitigates the effects of disparate censorship on model performance. We validate DCEM on synthetic data, showing that it improves bias mitigation (area between ROC curves) without sacrificing discriminative performance (AUC) compared to baselines. We achieve similar results in a sepsis classification task using clinical data.",
      "journal": "Proceedings of machine learning research",
      "year": "2024",
      "doi": "",
      "authors": "Chang Trenton et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40574796/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12199211",
      "ft_text_length": 1008,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12199211)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40595988",
      "title": "Beyond accuracy: a framework for evaluating algorithmic bias and performance, applied to automated sleep scoring.",
      "abstract": "Recent advancements in artificial intelligence (AI) have significantly improved sleep-scoring algorithms, bringing their performance close to the theoretical limit of approximately 80%, which aligns with inter-scorer agreement levels. While this suggests the problem is technically solved, clinical adoption remains challenging due to ethical and regulatory requirements for rigorous validation, fairness, and human oversight. Existing validation methods, such as Bland-Altman analysis, often rely on simple correlation metrics, overlooking potential non-linear influences of external factors (e.g., demographic or clinical variables) on systematic predictive errors (biases) in derived clinical markers. Additionally, performance metrics are typically reported as the mean of on-subject results, neglecting critical scenarios-such as different quantiles-that could better convey the algorithm's capabilities and limitations to clinicians as end-users. To address this gap, we propose a universal framework for quantifying both performance metrics and biases in predictive algorithmic tools. Our approach extends conventional validation methods by analyzing how external factors shape the entire distribution of predictive performance and errors, rather than just the expected mean. Applying it to the widely recognized U-Sleep and YASA sleep-scoring algorithms, we identify biases-such as age-related shifts-indicating missing input information or imbalances in training data. Despite these biases, we illustrate that both algorithms maintain non-inferior performance in the risk assessment of sleep apnea based on prediction-derived markers, highlighting the potential and clinical utility of algorithmic insights.",
      "journal": "Scientific reports",
      "year": "2025",
      "doi": "10.1038/s41598-025-06019-4",
      "authors": "Bechny Michal et al.",
      "keywords": "",
      "mesh_terms": "Humans; Algorithms; Polysomnography; Sleep; Artificial Intelligence; Male; Female; Adult; Middle Aged; Bias; Reproducibility of Results",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40595988/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12215420",
      "ft_text_length": 134142,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12215420)",
      "ft_reason": "Included: bias central + approach content (7 indicators)"
    },
    {
      "pmid": "40605830",
      "title": "Generative AI in Medicine: Pioneering Progress or Perpetuating Historical Inaccuracies? Cross-Sectional Study Evaluating Implicit Bias.",
      "abstract": "BACKGROUND: Generative artificial intelligence (gAI) models, such as DALL-E 2, are promising tools that can generate novel images or artwork based on text input. However, caution is warranted, as these tools generate information based on historical data and are thus at risk of propagating past learned inequities. Women in medicine have routinely been underrepresented in academic and clinical medicine and the stereotype of a male physician persists. OBJECTIVE: The primary objective is to evaluate implicit bias among gAI across medical specialties. METHODS: To evaluate for potential implicit bias, 100 photographs for each medical specialty were generated using the gAI platform DALL-E2. For each specialty, DALL-E2 was queried with \"An American [specialty name].\" Our primary endpoint was to compare the gender distribution of gAI photos to the current distribution in the United States. Our secondary endpoint included evaluating the racial distribution. gAI photos were classified according to perceived gender and race based on a unanimous consensus among a diverse group of medical residents. The proportion of gAI women subjects was compared for each medical specialty to the most recent Association of American Medical Colleges report for physician workforce and active residents using \u03c72 analysis. RESULTS: A total of 1900 photos across 19 medical specialties were generated. Compared to physician workforce data, AI significantly overrepresented women in 7/19 specialties and underrepresented women in 6/19 specialties. Women were significantly underrepresented compared to the physician workforce by 18%, 18%, and 27% in internal medicine, family medicine, and pediatrics, respectively. Compared to current residents, AI significantly underrepresented women in 12/19 specialties, ranging from 10% to 36%. Additionally, women represented <50% of the demographic for 17/19 specialties by gAI. CONCLUSIONS: gAI created a sample population of physicians that underrepresented women when compared to both the resident and active physician workforce. Steps must be taken to train datasets in order to represent the diversity of the incoming physician workforce.",
      "journal": "JMIR AI",
      "year": "2025",
      "doi": "10.2196/56891",
      "authors": "Sutera Philip et al.",
      "keywords": "AI bias; Artificial Intelligence; bias; generative artificial intelligence; historical inequity; implicit bias; social inequity; workforce diversity",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40605830/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12223688",
      "ft_text_length": 12109,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12223688)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40610592",
      "title": "A three-tier AI solution for equitable glaucoma diagnosis across China's hierarchical healthcare system.",
      "abstract": "Artificial intelligence (AI) offers a solution to glaucoma care inequities driven by uneven resource distribution, but its real-world implementation remains limited. Here, we introduce Multi-Glau, an three-tier AI system tailored to China's hierarchical healthcare system to promote health equity in glaucoma care, even in settings with limited equipment. The system comprises three modules: (1) a screening module for primary hospitals that eliminates reliance on imaging; (2) a pre-diagnosis module for handling incomplete data in secondary hospitals, and (3) a definitive diagnosis module for the precise diagnosis of glaucoma severity in tertiary hospitals. Multi-Glau achieved high performance (AUC: 0.9254 for screening, 0.8650 for pre-diagnosis, and 0.9516 for definitive diagnosis), with its generalizability confirmed through multicenter validation. Multi-Glau outperformed state-of-the-art models, particularly in handling missing data and providing precise glaucoma severity diagnosis, while improving ophthalmologists' performance. These results demonstrate Multi-Glau's potential to bridge diagnostic gaps across hospital tiers and enhance equitable healthcare access.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01835-4",
      "authors": "Zhou Yi et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40610592/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12226717",
      "ft_text_length": 58187,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12226717)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "40616933",
      "title": "Evaluating the Performance and Potential Bias of Predictive Models for Detection of Transthyretin Cardiac Amyloidosis.",
      "abstract": "BACKGROUND: Delays in the diagnosis of transthyretin amyloid cardiomyopathy (ATTR-CM) contribute to the significant morbidity of the condition, especially in the era of disease-modifying therapies. Screening for ATTR-CM with artificial intelligence and other algorithms may improve timely diagnosis, but these algorithms have not been directly compared. OBJECTIVES: The aim of this study was to compare the performance of 4 algorithms for ATTR-CM detection in a heart failure population and assess the risk for harms due to model bias. METHODS: We identified patients in an integrated health system from 2010 to 2022 with ATTR-CM and age- and sex-matched them to controls with heart failure to target 5% prevalence. We compared the performance of a claims-based random forest model (Huda et al model), a regression-based score (Mayo ATTR-CM), and 2 deep learning echo models (EchoNet-LVH and EchoGo Amyloidosis). We evaluated for bias using standard fairness metrics. RESULTS: The analytical cohort included 176 confirmed cases of ATTR-CM and 3,192 control patients with 79.2% self-identified as White and 9.0% as Black. The Huda et al model performed poorly (AUC: 0.49). Both deep learning echo models had a higher AUC when compared to the Mayo ATTR-CM Score (EchoNet-LVH 0.88; EchoGo Amyloidosis 0.92; Mayo ATTR-CM Score 0.79; DeLong P < 0.001 for both). Bias auditing met fairness criteria for equal opportunity among patients who identified as Black. CONCLUSIONS: Deep learning, echo-based models to detect ATTR-CM demonstrated best overall discrimination when compared to 2 other models in external validation with low risk of harms due to racial bias.",
      "journal": "JACC. Advances",
      "year": "2025",
      "doi": "10.1016/j.jacadv.2025.101901",
      "authors": "Hourmozdi Jonathan et al.",
      "keywords": "amyloidosis; artificial intelligence; health care disparities; heart failure; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40616933/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12272418",
      "ft_text_length": 32029,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12272418)",
      "ft_reason": "Included: bias central + approach content (10 indicators)"
    },
    {
      "pmid": "40624414",
      "title": "Fairness of machine learning readmission predictions following open ventral hernia repair.",
      "abstract": "INTRODUCTION: Few models have predicted readmission following open ventral hernia repair (VHR), and none have assessed fairness. Fairness evaluation assesses whether predictive performance is similar across demographic groups, ensuring that biases are not propagated. Therefore, we generated an interpretable machine learning model to predict readmission following open VHR while assessing fairness. METHODS: NSQIP (2018-2021) was queried for open VHR. We developed an XGBoost model to predict unplanned readmissions within 30\u00a0days of surgery with fivefold cross-validation. Performance and fairness were assessed by demographic groups: gender (female vs. male), ethnicity (Hispanic vs. non-Hispanic), and race (non-White vs. White). We identified influential features within demographic groups using SHapley Additive exPlanations (SHAP). RESULTS: 59,482 patients were included with a readmission rate of 5.5%. The model had an AUC of 0.72 and a Brier score of 0.16. Fairness metrics revealed minimal performance differences between demographic groups. SHAP revealed that influential factors were similar across demographic groups and included days from operation to discharge, morbidity probability, and operative time. CONCLUSION: Using interpretable machine learning, we identified unique predictors for unplanned readmission following open VHR. Fairness metrics revealed minimal differences in performance between demographic groups. SHAP showed similar influential factors across demographic groups. Future surgical machine learning models should similarly assess models using fairness metrics and interpretation of predictions.",
      "journal": "Surgical endoscopy",
      "year": "2025",
      "doi": "10.1007/s00464-025-11927-7",
      "authors": "Zander Tyler et al.",
      "keywords": "Algorithmic bias; Fairness; Interpretable machine learning; Risk prediction",
      "mesh_terms": "Humans; Machine Learning; Patient Readmission; Hernia, Ventral; Female; Male; Herniorrhaphy; Middle Aged; Aged; Postoperative Complications; Retrospective Studies; Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40624414/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12345327",
      "ft_text_length": 1629,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12345327)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "40629303",
      "title": "Evaluating accountability, transparency, and bias in AI-assisted healthcare decision- making: a qualitative study of healthcare professionals' perspectives in the UK.",
      "abstract": "BACKGROUND: While artificial intelligence (AI) has emerged as a powerful tool for enhancing diagnostic accuracy and streamlining workflows, key ethical questions remain insufficiently explored\u2014particularly around accountability, transparency, and bias. These challenges become especially critical in domains such as pathology and blood sciences, where opaque AI algorithms and non-representative datasets can impact clinical outcomes. The present work focuses on a single NHS context and does not claim broader generalization. METHODS: We conducted a local qualitative study across multiple healthcare facilities in a single NHS Trust in the West Midlands, United Kingdom, to investigate healthcare professionals\u2019 experiences and perceptions of AI-assisted decision-making. Forty participants\u2014including clinicians, healthcare administrators, and AI developers\u2014took part in semi-structured interviews or focus groups. Transcribed data were analyzed using Braun and Clarke\u2019s thematic analysis framework, allowing us to identify core themes relating to the benefits of AI, ethical challenges, and potential mitigation strategies. RESULTS: Participants reported notable gains in diagnostic efficiency and resource allocation, underscoring AI\u2019s potential to reduce turnaround times for routine tests and enhance detection of abnormalities. Nevertheless, accountability surfaced as a pervasive concern: while clinicians felt ultimately liable for patient outcomes, they also relied on AI-generated insights, prompting questions about liability if systems malfunctioned. Transparency emerged as another major theme, with clinicians emphasizing the difficulty of trusting \u201cblack box\u201d models that lack clear rationale or interpretability\u2014particularly for rare or complex cases. Bias was repeatedly cited, especially when algorithms underperformed in minority patient groups or in identifying atypical presentations. These issues raised doubts about the fairness and reliability of AIassisted diagnoses. CONCLUSIONS: Although AI demonstrates promise for improving efficiency and patient care, unresolved ethical complexities around accountability, transparency, and bias may erode stakeholder confidence and compromise patient safety. Participants called for clearer regulatory frameworks, inclusive training datasets, and stronger clinician\u2013developer collaboration. Future research should incorporate patient perspectives, investigate long-term impacts of AI-driven clinical decisions, and refine ethical guidelines to ensure equitable, responsible AI deployment. TRIAL REGISTRATION: : Not applicable. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1186/s12910-025-01243-z.",
      "journal": "BMC medical ethics",
      "year": "2025",
      "doi": "10.1186/s12910-025-01243-z",
      "authors": "Nouis Saoudi Ce et al.",
      "keywords": "Accountability; Artificial intelligence; Bias; Clinical Decision-Making; Electronic health record; Healthcare ethics; Qualitative research; Transparency",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40629303/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12235780",
      "ft_text_length": 45715,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12235780)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40639839",
      "title": "Bias in vital signs? Machine learning models can learn patients' race or ethnicity from the values of vital signs alone.",
      "abstract": "OBJECTIVES: To investigate whether machine learning (ML) algorithms can learn racial or ethnic information from the vital signs alone. METHODS: A retrospective cohort study of critically ill patients between 2014 and 2015 from the multicentre eICU-CRD critical care database involving 335 intensive care units in 208 US hospitals, containing 200\u2009859 admissions. We extracted 10\u2009763 critical care admissions of patients aged 18 and over, alive during the first 24 hours after admission, with recorded race or ethnicity as well as at least two measurements of heart rate, oxygen saturation, respiratory rate and blood pressure. Pairs of subgroups were matched based on age, gender, admission diagnosis and disease severity. XGBoost, Random Forest and Logistic Regression algorithms were used to predict recorded race or ethnicity based on the values of vital signs. RESULTS: Models derived from only four vital signs can predict patients' recorded race or ethnicity with an area under the curve (AUC) of 0.74 (\u00b10.030) between White and Black patients, AUC of 0.74 (\u00b10.030) between Hispanic and Black patients and AUC of 0.67 (\u00b10.072) between Hispanic and White patients, even when controlling for known factors. There were very small, but statistically significant differences between heart rate, oxygen saturation and blood pressure, but not respiration rate and invasively measured oxygen saturation. DISCUSSION: ML algorithms can extract racial or ethnicity information from vital signs alone across diverse patient populations, even when controlling for known biases such as pulse oximetry variations and comorbidities. The model correctly classified the race or ethnicity in two out of three patients, indicating that this outcome is not random. CONCLUSION: Vital signs embed racial information that can be learnt by ML algorithms, posing a significant risk to equitable clinical decision-making. Mitigating measures might be challenging, considering the fundamental role of vital signs in clinical decision-making.",
      "journal": "BMJ health & care informatics",
      "year": "2025",
      "doi": "10.1136/bmjhci-2024-101098",
      "authors": "Velichkovska Bojana et al.",
      "keywords": "Artificial intelligence; Decision Support Systems, Clinical; Electronic Health Records; Health Equity",
      "mesh_terms": "Humans; Machine Learning; Vital Signs; Retrospective Studies; Male; Female; Middle Aged; Ethnicity; Aged; Racial Groups; Intensive Care Units; Adult; United States; Critical Illness; Algorithms; Bias; Respiratory Rate; Heart Rate",
      "pub_types": "Journal Article; Multicenter Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40639839/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12258377",
      "ft_text_length": 26702,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12258377)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40640864",
      "title": "Benchmarking bias in embeddings of healthcare AI models: using SD-WEAT for detection and measurement across sensitive populations.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) has been shown to exhibit and perpetuate human biases; recent research efforts have focused on measuring bias within the input embeddings of AI language models, especially with non-binary classifications that are common in medicine and healthcare scenarios. For instance, ethnicity-linked terms might include categories such as Asian, Black, Hispanic, and White, complicating the definition of \u2013 traditionally binary \u2013 attribute groups. In this study, we aimed to develop a new framework to detect and measure inherent medical biases based on SD-WEAT (Standard Deviation - Word Embedding Association Test). Compared to its predecessor, WEAT, SD-WEAT was able to measure bias among multi-level attribute groups common in the field of medicine, such as age, race, and region. METHODS: We constructed a collection of medicine-based benchmarks that can be used to detect and measure biases among sex, ethnicities, and medical conditions. Then, we evaluated a collection of language models, including GloVe, BERT, LegalBERT, BioBERT, GPT-2, and BioGPT, and determined which had potential undesirable or desirable healthcare biases. RESULTS: With the presented framework, we were able to detect and measure a significant presence of bias among gender-linked (P\u2009<\u20090.01) and ethnicity-linked (P\u2009<\u20090.01) medical conditions for a biomedicine-focused language model (e.g., BioBERT) compared to general BERT models. In addition, we demonstrated that SD-WEAT was capable of simultaneously handling multiple attribute groups, detecting and measuring bias among a collection of ethnicity-linked medical conditions and multiple ethnic/racial groups. CONCLUSIONS: To conclude, we presented an AI bias measurement framework, based on SD-WEAT. This framework provided a promising approach to detect and measure biases in language models that have been applied in biomedical/healthcare text analysis. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1186/s12911-025-03102-8.",
      "journal": "BMC medical informatics and decision making",
      "year": "2025",
      "doi": "10.1186/s12911-025-03102-8",
      "authors": "Gray Magnus et al.",
      "keywords": "Artificial intelligence; Bias; Bias measurement; Healthcare; Input embeddings; Language models; Medicine; Natural Language processing",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40640864/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12247235",
      "ft_text_length": 38163,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12247235)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "40644462",
      "title": "Raising awareness of potential biases in medical machine learning: Experience from a Datathon.",
      "abstract": "OBJECTIVE: To challenge clinicians and informaticians to learn about potential sources of bias in medical machine learning models through investigation of data and predictions from an open-source severity of illness score. METHODS: Over a two-day period (total elapsed time approximately 28 hours), we conducted a datathon that challenged interdisciplinary teams to investigate potential sources of bias in the Global Open Source Severity of Illness Score. Teams were invited to develop hypotheses, to use tools of their choosing to identify potential sources of bias, and to provide a final report. RESULTS: Five teams participated, three of which included both informaticians and clinicians. Most (4/5) used Python for analyses, the remaining team used R. Common analysis themes included relationship of the GOSSIS-1 prediction score with demographics and care related variables; relationships between demographics and outcomes; calibration and factors related to the context of care; and the impact of missingness. Representativeness of the population, differences in calibration and model performance among groups, and differences in performance across hospital settings were identified as possible sources of bias. DISCUSSION: Datathons are a promising approach for challenging developers and users to explore questions relating to unrecognized biases in medical machine learning algorithms.",
      "journal": "PLOS digital health",
      "year": "2025",
      "doi": "10.1371/journal.pdig.0000932",
      "authors": "Hochheiser Harry et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40644462/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12250157",
      "ft_text_length": 19341,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12250157)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "40658470",
      "title": "Deconfounded and debiased estimation for high-dimensional linear regression under hidden confounding with application to omics data.",
      "abstract": "MOTIVATION: A critical challenge in observational studies arises from the presence of hidden confounders in high-dimensional data. This leads to biases in causal effect estimation due to both hidden confounding and high-dimensional estimation. Some classical deconfounding methods are inadequate for high-dimensional scenarios and typically require prior information on hidden confounders. We propose a two-step deconfounded and debiased estimation for high-dimensional linear regression with hidden confounding. RESULTS: First, we reduce hidden confounding via spectral transformation. Second, we correct bias from the weighted \u21131 penalty, commonly used in high-dimensional estimation, by inverting the Karush-Kuhn-Tucker conditions and solving convex optimization programs. This deconfounding technique by spectral transformation requires no prior knowledge of hidden confounders. This novel debiasing approach improves over recent work by not assuming a sparse precision matrix, making it more suitable for cases with intrinsic covariate correlations. Simulations show that the proposed method corrects both biases and provides more precise coefficient estimates than existing approaches. We also apply the proposed method to a deoxyribonucleic acid methylation dataset from the Alzheimer's disease (AD) neuroimaging initiative database to investigate the association between cerebrospinal fluid tau protein levels and AD severity. AVAILABILITY AND IMPLEMENTATION: The code for the proposed method is available on GitHub (https://github.com/Li-Zhaoy/Dec-Deb.git) and archived on Zenodo (DOI: https://10.5281/zenodo.15478745).",
      "journal": "Bioinformatics (Oxford, England)",
      "year": "2025",
      "doi": "10.1093/bioinformatics/btaf400",
      "authors": "Li Zhaoyang et al.",
      "keywords": "",
      "mesh_terms": "Humans; Linear Models; Alzheimer Disease; Algorithms; DNA Methylation; Computational Biology; Computer Simulation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40658470/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12381636",
      "ft_text_length": 33643,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12381636)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "40661475",
      "title": "Advancing Fair and Explainable Machine Learning for Neuroimaging Dementia Pattern Classification in Multi-Ethnic Populations.",
      "abstract": "Dementia, a degenerative disease affecting millions globally, is projected to triple by 2050. Early and precise diagnosis is essential for effective treatment and improved quality of life. However, current diagnostic approaches frequently demonstrate inconsistent precision and impartiality, particularly among diverse cultural groups. This study investigates performance discrepancies in dementia classification among White American, African American, and Hispanic populations. We reveal significant cross-group bias, particularly when models trained on one group are tested on another. To address this, we introduce a novel combination of few-shot learning and domain alignment to improve model adaptability across underrepresented populations. Our results show that these techniques substantially reduce inter-group performance gaps, especially between White American and Hispanic cohorts. This finding highlights the crucial need for fairness-aware strategies and the inclusion of diverse populations in training data to ensure accurate and equitable dementia diagnoses.",
      "journal": "bioRxiv : the preprint server for biology",
      "year": "2025",
      "doi": "10.1101/2025.06.09.658375",
      "authors": "Ho Ngoc-Huynh et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40661475/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12258978",
      "ft_text_length": 1074,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12258978)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40665345",
      "title": "Potential to perpetuate social biases in health care by Chinese large language models: a model evaluation study.",
      "abstract": "BACKGROUND: Large language models (LLMs) may perpetuate or amplify social biases toward patients. We systematically assessed potential biases of three popular Chinese LLMs in clinical application scenarios. METHODS: We tested whether Qwen, Erine, and Baichuan encode social biases for patients of different sex, ethnicity, educational attainment, income level, and health insurance status. First, we prompted LLMs to generate clinical cases for medical education (n\u2009=\u20098,289) and compared the distribution of patient characteristics in LLM-generated cases with national distributions in China. Second, New England Journal of Medicine Healer clinical vignettes were used to prompt LLMs to generate differential diagnoses and treatment plans (n\u2009=\u200945,600), with variations analyzed based on sociodemographic characteristics. Third, we prompted LLMs to assess patient needs (n\u2009=\u200951,039) based on clinical cases, revealing any implicit biases toward patients with different characteristics. RESULTS: The three LLMs showed social biases toward patients with different characteristics to varying degrees in medical education, diagnostic and treatment recommendation, and patient needs assessment. These biases were more frequent in relation to sex, ethnicity, income level, and health insurance status, compared to educational attainment. Overall, the three LLMs failed to appropriately model the sociodemographic diversity of medical conditions, consistently over-representing male, high-education and high-income populations. They also showed a higher referral rate, indicating potential refusal to treat patients, for minority ethnic groups and those without insurance or living with low incomes. The three LLMs were more likely to recommend pain medications for males, and considered patients with higher educational attainment, Han ethnicity, higher income, and those with health insurance as having healthier relationships with others. INTERPRETATION: Our findings broaden the scopes of potential biases inherited in LLMs and highlight the urgent need for systematic and continuous assessments of social biases in LLMs in real-world clinical applications.",
      "journal": "International journal for equity in health",
      "year": "2025",
      "doi": "10.1186/s12939-025-02581-5",
      "authors": "Liu Chenxi et al.",
      "keywords": "Diagnosis and treatment; Large Language Models; Medical Education; Patient Assessment; Social Bias",
      "mesh_terms": "Female; Humans; Male; China; Healthcare Disparities; Large Language Models; East Asian People",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40665345/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12265265",
      "ft_text_length": 45869,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12265265)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40666330",
      "title": "Auditor Models to Suppress Poor AI Predictions Can Improve Human-AI Collaborative Performance.",
      "abstract": "OBJECTIVE: Healthcare decisions are increasingly made with the assistance of machine learning (ML). ML has been known to have unfairness - inconsistent outcomes across subpopulations. Clinicians interacting with these systems can perpetuate such unfairness by overreliance. Recent work exploring ML suppression - silencing predictions based on auditing the ML - shows promise in mitigating performance issues originating from overreliance. This study aims to evaluate the impact of suppression on collaboration fairness and evaluate ML uncertainty as desiderata to audit the ML. MATERIALS AND METHODS: We used data from the Vanderbilt University Medical Center electronic health record (n = 58,817) and the MIMIC-IV-ED dataset (n = 363,145) to predict likelihood of death or ICU transfer and likelihood of 30-day readmission. Our simulation study used gradient-boosted trees as well as an artificially high-performing oracle model. We derived clinician decisions directly from the dataset and simulated clinician acceptance of ML predictions based on previous empirical work on acceptance of CDS alerts. We measured performance as area under the receiver operating characteristic curve and algorithmic fairness using absolute averaged odds difference. RESULTS: When the ML outperforms humans, suppression outperforms the human alone (p < 0.034) and at least does not degrade fairness. When the human outperforms the ML, suppression outperforms the human (p < 5.2 \u00d7 10-5) but the human is fairer than suppression (p < 0.0019). Finally, incorporating uncertainty quantification into suppression approaches can improve performance. CONCLUSION: Suppression of poor-quality ML predictions through an auditor model shows promise in improving collaborative human-AI performance and fairness.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.06.24.25330212",
      "authors": "Brown Katherine E et al.",
      "keywords": "artificial intelligence; human-AI collaboration; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40666330/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12262782",
      "ft_text_length": 27099,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12262782)",
      "ft_reason": "Included: substantial approach content (3 indicators)"
    },
    {
      "pmid": "40667329",
      "title": "GhostBuster: A Deep-Learning-based, Literature-Unbiased Gene Prioritization Tool for Gene Annotation Prediction.",
      "abstract": "All genes are not equal before literature. Despite the explosion of genomic data, a significant proportion of human protein-coding genes remain poorly characterized (\"ghost genes\"). Due to sociological dynamics in research, scientific literature disproportionately focuses on already well-annotated genes, reinforcing existing biases (bandwagon effect). This literature bias often permeates machine learning (ML) models trained on gene annotation tasks, leading to predictions that favor well-studied genes. Consequently, standard ML performance metrics may overestimate biological relevance by overfitting literature-derived patterns. To address this challenge, we developed GhostBuster, an encoder-decoder ML platform designed to predict gene functions, disease associations and interactions while minimizing literature bias. We first compared the impact of biased (Gene Ontology) versus unbiased training datasets (LINCS, TCGA, STRING). While literature-biased sources yielded higher ML metrics, they also amplified bias by prioritizing well-characterized genes. In contrast, models trained on unbiased datasets were 2-3\u00d7 more effective at identifying recently discovered gene annotations. Notably, one of the unbiased channels (TCGA), combined minimal amounts of literature bias with robust performance, at a test ROC-AUC of 0.8-0.95. We demonstrate that GhostBuster can be applied to predict novel gene functions, refine pathway memberships, and prioritize intergenic GWAS hits. As the first ML framework explicitly designed to counteract literature bias, GhostBuster offers a powerful tool for uncovering the roles of understudied genes in cellular function, disease, and molecular networks.",
      "journal": "bioRxiv : the preprint server for biology",
      "year": "2025",
      "doi": "10.1101/2025.06.22.660948",
      "authors": "Deangeli Giulio et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40667329/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12262676",
      "ft_text_length": 89297,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12262676)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40683994",
      "title": "Demographic inaccuracies and biases in the depiction of patients by artificial intelligence text-to-image generators.",
      "abstract": "The wide usage of artificial intelligence (AI) text-to-image generators raises concerns about the role of AI in amplifying misconceptions in healthcare. This study therefore evaluated the demographic accuracy and potential biases in the depiction of patients by four commonly used text-to-image generators. A total of 9060 images of patients with 29 different diseases was generated using Adobe Firefly, Bing Image Generator, Meta Imagine, and Midjourney. Twelve independent raters determined the sex, age, weight, and race and ethnicity of the patients depicted. Comparison to the real-world epidemiology showed that the generated images failed to depict demographical characteristics such as sex, age, and race and ethnicity accurately. In addition, we observed an over-representation of White and normal weight individuals. Inaccuracies and biases may stem from non-representative and non-specific training data as well as insufficient or misdirected bias mitigation strategies. In consequence, new strategies to counteract such inaccuracies and biases are needed.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01817-6",
      "authors": "Wiegand Tim Luca Till et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40683994/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12276360",
      "ft_text_length": 33732,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12276360)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "40692125",
      "title": "Artificial intelligence-simplified information to advance reproductive genetic literacy and health equity.",
      "abstract": "STUDY QUESTION: Can artificial intelligence (AI) and large language models (LLMs) effectively simplify patient education materials (PEMs) to advance reproductive genetic literacy and health equity? SUMMARY ANSWER: LLMs offer a promising approach to support healthcare professionals in generating effective, and simplified PEMs. WHAT IS KNOWN ALREADY: Reproductive genetic testing and counseling holds the potential to support a personalized approach to reduce the burden of genetic disorders. However, its uptake remains limited due to the complexity of the tests and the way that PEMs have been designed. This is more prominent in reproductive genetic testing, as vulnerability of patients may lead to over- or under-use of genetic testing technologies. STUDY DESIGN, SIZE, DURATION: We carried out a comparative observational study to evaluate the capacity of four AI/LLMs to simplify PEMs (n\u2009=\u200930) in reproductive genetics and assessing the clinical accuracy of simplified versions (n\u2009=\u2009120) by experts (n\u2009=\u200930). Additionally, we devised a graphical user interface (GUI) to support real-time text simplification and readability analysis. PARTICIPANTS/MATERIALS, SETTING, METHODS: We collected 30 PEMs covering six topics in reproductive genetics from well-recognized platforms, such as WHO, MedlinePlus, and Johns Hopkins. Each PEM was processed by four AI/LLMs (GPT-3.5, GPT-4, Copilot, Gemini) using a fixed prompt, resulting in 120 simplified outputs. We measured readability improvements using five validated metrics, such as simple measure of gobbledygook, each capturing distinct textual characteristics such as sentence length and word complexity. To evaluate clinical reliability of the simplified outputs, a panel of experts (n\u2009=\u200930) in reproductive genetics independently scored each text (3 per text). MAIN RESULTS AND THE ROLE OF CHANCE: All four LLMs significantly improved the readability of the PEMs (P-values <0.001), reducing text complexity to an average 6th-7th grade reading level. While Gemini and Copilot achieved the highest improvement in readability scores, GPT-4 received the highest expert rating across all criteria-accuracy (4.1\u202f\u00b1\u202f0.9), completeness (4.2\u202f\u00b1\u202f0.8), and relevance of omissions (4.0\u202f\u00b1\u202f0.9; P\u2009<\u200910-8). These findings highlight the importance of balancing readability with content integrity to support informed decision-making, as excessive simplification may compromise essential medical information. We devised an open-access GUI that provides real-time PEM simplification and readability analysis to support the integration of AI-assisted approaches in clinical practice (https://huggingface.co/spaces/CellularGenomicMedicine/HealthLiteracyEvaluator). LIMITATIONS, REASONS FOR CAUTION: Careful evaluation of LLM-simplified PEMs is required to ensure that simplification does not lead to omission of critical information. In addition, in this study, we report only the readability improvements of AI-generated texts and expert evaluations. To truly assess the potential of these tools in advancing reproductive genetic literacy and promoting health equity, real-world patient feedback is essential. WIDER IMPLICATIONS OF THE FINDINGS: Integrating AI/LLM into patient education strategies may advance health equity by improving understanding and facilitating informed decision-making. Thereby, more effective engagement of patients in reproductive genetic testing programs by assisting them with well-informed decision-making. STUDY FUNDING/COMPETING INTEREST(S): The EVA specialty program (KP111513) of MUMC+, the Horizon-Europe (NESTOR-101120075), the Estonian Research Council (PRG1076), the Horizon-2020 innovation (ERIN-EU952516) grants of the European Commission, the Swedish Research Council (grant no. 2024-02530), and the Novo Nordisk Foundation (grant no. NNF24OC0092384). The authors declare no conflict of interest relevant to this study. TRIAL REGISTRATION NUMBER: N/A.",
      "journal": "Human reproduction (Oxford, England)",
      "year": "2025",
      "doi": "10.1093/humrep/deaf135",
      "authors": "Naghdi Marjan et al.",
      "keywords": "AI; LLMs; artificial intelligence; health literacy; large language models; patient education material (PEM); readability; reproductive genetics; simplification",
      "mesh_terms": "Female; Humans; Artificial Intelligence; Genetic Testing; Health Equity; Health Literacy; Patient Education as Topic",
      "pub_types": "Comparative Study; Journal Article; Observational Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40692125/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12408898",
      "ft_text_length": 31227,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12408898)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "40718760",
      "title": "A fair machine learning model to predict flares of systemic lupus erythematosus.",
      "abstract": "OBJECTIVE: Systemic lupus erythematosus (SLE) is a chronic autoimmune disease that disproportionately affects women and racial/ethnic minority groups. Predicting disease flares is essential for improving patient outcomes, yet few studies integrate both clinical and social determinants of health (SDoH). We therefore developed FLAME (FLAre Machine learning prediction of SLE), a machine learning pipeline that uses electronic health records (EHRs) and contextual-level SDoH to predict 3-month flare risk, emphasizing explainability and fairness. MATERIALS AND METHODS: We conducted a retrospective cohort study of 28\u2009433 patients with SLE from the University of Florida Health (2011-2022), linked to 675 contextual-level SDoH variables. We used XGBoost and logistic regression models to predict 3-month flare risk, evaluating model performance using the area under the receiver operating characteristic (AUROC). We applied SHapley Additive exPlanations (SHAP) values and causal structure learning to identify key predictors. Fairness was assessed using the equality of opportunity metric, measured by the false-negative rate across racial/ethnic groups. RESULTS: The FLAME model, incorporating clinical and contextual-level SDoH, achieved an AUROC of 0.66. The clinical-only model performed slightly better (AUROC of 0.67), while the SDoH-only model had lower performance (AUROC of 0.54). SHAP analysis identified headache, organic brain syndrome, and pyuria as key predictors. Causal learning revealed interactions between clinical factors and contextual-level SDoH. Fairness assessments showed no significant biases across groups. DISCUSSION: FLAME offers a fair and interpretable approach to predicting SLE flares, providing meaningful insights that may guide future clinical interventions. CONCLUSIONS: FLAME shows promise as an EHR-based tool to support personalized, equitable, and holistic SLE care.",
      "journal": "JAMIA open",
      "year": "2025",
      "doi": "10.1093/jamiaopen/ooaf072",
      "authors": "Li Yongqiu et al.",
      "keywords": "fairness; machine learning; prediction; social determinants of health; systemic lupus erythematosus",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40718760/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12296391",
      "ft_text_length": 37230,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12296391)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "40726749",
      "title": "Empirical Comparison of Post-processing Debiasing Methods for Machine Learning Classifiers in Healthcare.",
      "abstract": "UNLABELLED: Machine learning classifiers in healthcare tend to reproduce or exacerbate existing health disparities due to inherent biases in training data. This relevant issue has brought the attention of researchers in both healthcare and other domains, proposing techniques that deal with it in different stages of the machine learning process. Post-processing methods adjust model predictions to ensure fairness without interfering in the learning process nor requiring access to the original training data, preserving privacy and enabling the application to any trained model. This study rigorously compares state-of-the-art debiasing methods within the family of post-processing techniques across a wide range of synthetic and real-world (healthcare) datasets, by means of different performance and fairness metrics. Our experiments reveal the strengths and weaknesses of each method, examining the trade-offs between group fairness and predictive performance, as well as among different notions of group fairness. Additionally, we analyze the impact on untreated attributes to ensure overall bias mitigation. Our comprehensive evaluation provides insights into how these debiasing methods can be optimally implemented in healthcare settings to balance accuracy and fairness. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007/s41666-025-00196-7.",
      "journal": "Journal of healthcare informatics research",
      "year": "2025",
      "doi": "10.1007/s41666-025-00196-7",
      "authors": "Dang Vien Ngoc et al.",
      "keywords": "Algorithmic bias; Fairness; Healthcare; Machine learning classifiers; Post-processing",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40726749/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12290158",
      "ft_text_length": 72708,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12290158)",
      "ft_reason": "Included: bias central + approach content (17 indicators)"
    },
    {
      "pmid": "40729959",
      "title": "Exploring Gender Bias in AI for Personalized Medicine: Focus Group Study With Trans Community Members.",
      "abstract": "BACKGROUND: This paper explores the perception and application of artificial intelligence (AI) for personalized medicine within the trans community, an often-overlooked demographic in the broader scope of precision medicine. Despite growing advancements in AI-driven health care solutions, little research has been dedicated to understanding how these technologies can be tailored to meet the unique health care needs of trans individuals. Addressing this gap is crucial for ensuring that precision medicine is genuinely inclusive and effective for all populations. OBJECTIVE: This study aimed to identify the specific challenges, obstacles, and potential solutions associated with the deployment of AI technologies in the development of personalized medicine for trans people. This research emphasizes a trans-inclusive and multidisciplinary perspective, highlighting the importance of cultural competence and community engagement in the design and implementation of AI-driven health care solutions. METHODS: A communicative methodology was applied in this study, prioritizing the active involvement of end-users and stakeholders through egalitarian dialogue that recognizes and values cultural intelligence. The methodological design included iterative consultations with trans community representatives to cocreate the research workflow and adapt data collection instruments accordingly. This participatory approach ensured that the perspectives and lived experiences of trans individuals were integral to the research process. Data collection was conducted through 3 focus groups with 16 trans adults, aimed at discussing the challenges, risks, and transformative potential of AI in precision medicine. RESULTS: Analysis of the focus group discussions revealed several critical barriers impacting the integration of AI in personalized medicine for trans people, including concerns around data privacy, biases in algorithmic decision-making, and the lack of tailored health care data reflective of trans experiences. Participants expressed apprehensions about potential misdiagnoses or inappropriate treatments due to cisnormative data models. However, they also identified opportunities for AI to enhance health care outcomes, advocating for community-led data collection initiatives and improved algorithmic transparency. Proposed solutions included enhancing datasets with trans-specific health markers, incorporating community voices in AI development processes, and prioritizing ethical frameworks that respect gender diversity. CONCLUSIONS: This study underscores the necessity for a trans-inclusive approach to precision medicine, facilitated by AI technologies that are sensitive to the health care needs and lived realities of trans people. By addressing the identified challenges and adopting community-driven solutions, AI has the potential to bridge existing health care gaps and improve the quality of life for trans individuals. This research contributes to the growing discourse on equitable health care innovation, calling for more inclusive AI design practices that extend the benefits of precision medicine to marginalized communities.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/72325",
      "authors": "Busl\u00f3n Nataly et al.",
      "keywords": "artificial intelligence; artificial intelligence biases; health biases; precision medicine; trans people; transgender medicine",
      "mesh_terms": "Humans; Precision Medicine; Artificial Intelligence; Focus Groups; Male; Female; Adult; Sexism; Transgender Persons; Middle Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40729959/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12307004",
      "ft_text_length": 44510,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12307004)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40756387",
      "title": "Ethical AI in medical text generation: balancing innovation with privacy in public health.",
      "abstract": "INTRODUCTION: The integration of artificial intelligence (AI) into medical text generation is transforming public health by enhancing clinical documentation, patient education, and decision support. However, the widespread deployment of AI in this domain introduces significant ethical challenges, including fairness, privacy protection, and accountability. Traditional AI-driven medical text generation models often inherit biases from training data, resulting in disparities in healthcare communication across different demographic groups. Moreover, ensuring patient data confidentiality while maintaining transparency in AI-generated content remains a critical concern. Existing approaches either lack robust bias mitigation mechanisms or fail to provide interpretable and privacy-preserving outputs, compromising ethical compliance and regulatory adherence. METHODS: To address these challenges, this paper proposes an innovative framework that combines privacy-preserving AI techniques with interpretable model architectures to achieve ethical compliance in medical text generation. The method employs a hybrid approach that integrates knowledge-based reasoning with deep learning, ensuring both accuracy and transparency. Privacy-enhancing technologies, such as homomorphic encryption and secure multi-party computation, are incorporated to safeguard sensitive medical data throughout the text generation process. Fairness-aware training protocols are introduced to mitigate biases in generated content and enhance trustworthiness. RESULTS AND DISCUSSION: The proposed approach effectively addresses critical challenges of bias, privacy, and interpretability in medical text generation. By combining symbolic reasoning with data-driven learning and embedding ethical principles at the system design level, the framework ensures regulatory alignment and improves public trust. This methodology lays the groundwork for broader deployment of ethically sound AI systems in healthcare communication.",
      "journal": "Frontiers in public health",
      "year": "2025",
      "doi": "10.3389/fpubh.2025.1583507",
      "authors": "Liang Mingpei",
      "keywords": "AI ethics; bias mitigation; ethical challenges; healthcare regulation; legal compliance; medical AI; privacy protection; text generation",
      "mesh_terms": "Humans; Artificial Intelligence; Public Health; Confidentiality; Privacy; Electronic Health Records",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40756387/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12313694",
      "ft_text_length": 64732,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12313694)",
      "ft_reason": "Included: substantial approach content (13 indicators)"
    },
    {
      "pmid": "40764552",
      "title": "Toward a fair, gender-debiased classifier for the diagnosis of attention deficit/hyperactivity disorder- a Machine-Learning based classification study.",
      "abstract": "BACKGROUND: Attention deficit/hyperactivity disorder (ADHD) is the most common neurodevelopmental disorder. Gender disparities in the diagnosis of ADHD have been reported, suggesting that females tend to be diagnosed later in life than males are. The delayed diagnosis in females has been attributed to an inequality in the diagnostic criteria, failing to focus on the gender differences regarding symptomatology, comorbidity, and societal factors contributing to this disparity. METHODS: In this study, we introduced debiased classifiers for the diagnosis of ADHD via different bias mitigation algorithms of the AI Fairness 360 toolbox on a training dataset of 400 children and adolescents with and without ADHD (98 females, 25 ADHD patients, 73 typically developing females), a subsample of the Child Mind Institute dataset. Test data were acquired in an earlier study. Two datasets were used, one including personal characteristic features, scores of the clinical questionnaire Child Behavior Checklist, and wavelet variance coefficients as quantifiers of neural dynamics (fMRI), a second dataset included personal characteristic features, scores of the clinical questionnaire Child Behavior Checklist, and radiomic features of neural structure (sMRI). RESULTS: We found that the reweighed XGBoost model achieved the best accuracy and highest fairness in both datasets. Using model explanation, we showed how reweighing influenced feature importance at the global and local levels. CONCLUSION: Based on methodological characteristics and insights from global and local model explana-tion, we discuss the reasons of these findings and conclude, that using the combination of bias mitigation and model explanation, improved classification models can be achieved.",
      "journal": "BMC medical informatics and decision making",
      "year": "2025",
      "doi": "10.1186/s12911-025-03126-0",
      "authors": "Neufang Susanne et al.",
      "keywords": "AI fairness 360; Attention deficit/hyperactivity disorder; Bias detection and mitigation; Gender bias; Machine learning; Model explanation",
      "mesh_terms": "Humans; Attention Deficit Disorder with Hyperactivity; Male; Child; Female; Machine Learning; Adolescent; Sex Factors",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40764552/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12326834",
      "ft_text_length": 42778,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12326834)",
      "ft_reason": "Included: bias central + approach content (19 indicators)"
    },
    {
      "pmid": "40771228",
      "title": "Algorithmic bias in public health AI: a silent threat to equity in low-resource settings.",
      "abstract": "",
      "journal": "Frontiers in public health",
      "year": "2025",
      "doi": "10.3389/fpubh.2025.1643180",
      "authors": "Joseph Jeena",
      "keywords": "algorithmic bias; artificial intelligence in public health; digital health disparities; health equity; inclusive AI; low-resource settings",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40771228/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12325396",
      "ft_text_length": 16502,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12325396)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "40773445",
      "title": "Reducing bias in coronary heart disease prediction using Smote-ENN and PCA.",
      "abstract": "Coronary heart disease (CHD) is a major cardiovascular disorder that poses significant threats to global health and is increasingly affecting younger populations. Its treatment and prevention face challenges such as high costs, prolonged recovery periods, and limited efficacy of traditional methods. Additionally, the complexity of diagnostic indicators and the global shortage of medical professionals further complicate accurate diagnosis. This study employs machine learning techniques to analyze CHD-related pathogenic factors and proposes an efficient diagnostic and predictive framework. To address the data imbalance issue, SMOTE-ENN is utilized, and five machine learning algorithms-Decision Trees, KNN, SVM, XGBoost, and Random Forest-are applied for classification tasks. Principal Component Analysis (PCA) and Grid Search are used to optimize the models, with evaluation metrics including accuracy, precision, recall, F1-score, and AUC. According to the random forest model's optimization experiment, the initial unbalanced data's accuracy was 85.26%, and the F1-score was 12.58%. The accuracy increased to 92.16% and the F1-score reached 93.85% after using SMOTE-ENN for data balancing, which is an increase of 6.90% and 81.27%, respectively; the model accuracy increased to 97.91% and the F1-score increased to 97.88% after adding PCA feature dimensionality reduction processing, which is an increase of 5.75% and 4.03%, respectively, compared with the SMOTE-ENN stage. This indicates that combining data balancing and feature dimensionality reduction techniques significantly improves model accuracy and makes the random forest model the best model. This study provides an efficient diagnostic tool for CHD, alleviates the challenges posed by limited medical resources, and offers a scientific foundation for precise prevention and intervention strategies.",
      "journal": "PloS one",
      "year": "2025",
      "doi": "10.1371/journal.pone.0327569",
      "authors": "Wei Xinyi et al.",
      "keywords": "",
      "mesh_terms": "Humans; Coronary Disease; Principal Component Analysis; Machine Learning; Algorithms; Decision Trees; Male; Support Vector Machine; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40773445/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12331108",
      "ft_text_length": 51722,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12331108)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "40794953",
      "title": "Towards machine learning fairness in classifying multicategory causes of deaths in colorectal or lung cancer patients.",
      "abstract": "Classification of patient multicategory survival outcomes is important for personalized cancer treatments. Machine learning (ML) algorithms have increasingly been used to inform healthcare decisions, but these models are vulnerable to biases in data collection and algorithm creation. ML models have previously been shown to exhibit racial bias, but their fairness towards patients from different age and sex groups have yet to be studied. Therefore, we compared the multimetric performances of five ML models (random forests, multinomial logistic regression, linear support vector classifier, linear discriminant analysis, and multilayer perceptron) when classifying colorectal cancer patients (n\u2009=\u2009589) of various age, sex, and racial groups using The Cancer Genome Atlas data. All five models exhibited biases for these sociodemographic groups. We then repeated the same process on lung adenocarcinoma (n\u2009=\u2009515) to validate our findings. Surprisingly, most models tended to perform more poorly overall for the largest sociodemographic groups. Methods to optimize model performance, including testing the model on merged age, sex, or racial groups, and creating a model trained on and used for an individual or merged sociodemographic group, show potential to reduce disparities in model performance for different groups. This is supported by our regression analysis showing associations between model choice and methodology used with reduced performance disparities across demographic subgroups. Notably, these methods may be used to improve ML fairness while avoiding penalizing the model for exhibiting bias and thus sacrificing overall performance.",
      "journal": "Briefings in bioinformatics",
      "year": "2025",
      "doi": "10.1093/bib/bbaf398",
      "authors": "Feng Catherine H et al.",
      "keywords": "colorectal cancer; lung cancer; machine learning; machine learning fairness; multilabel classification; survival",
      "mesh_terms": "Humans; Lung Neoplasms; Machine Learning; Colorectal Neoplasms; Male; Female; Middle Aged; Aged; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40794953/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12342732",
      "ft_text_length": 49479,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12342732)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40825541",
      "title": "Efficient Detection of Stigmatizing Language in Electronic Health Records via In-Context Learning: Comparative Analysis and Validation Study.",
      "abstract": "BACKGROUND: The presence of stigmatizing language within electronic health records (EHRs) poses significant risks to patient care by perpetuating biases. While numerous studies have explored the use of supervised machine learning models to detect stigmatizing language automatically, these models require large, annotated datasets, which may not always be readily available. In-context learning (ICL) has emerged as a data-efficient alternative, allowing large language models to adapt to tasks using only instructions and examples. OBJECTIVE: We aimed to investigate the efficacy of ICL in detecting stigmatizing language within EHRs under data-scarce conditions. METHODS: We analyzed 5043 sentences from the Medical Information Mart for Intensive Care-IV dataset, which contains EHRs from patients admitted to the emergency department at the Beth Israel Deaconess Medical Center. We compared ICL with zero-shot (textual entailment), few-shot (SetFit), and supervised fine-tuning approaches. The ICL approach used 4 prompting strategies: generic, chain of thought, clue and reasoning prompting, and a newly introduced stigma detection guided prompt. Model fairness was evaluated using the equal performance criterion, measuring true positive rate, false positive rate, and F1-score disparities across protected attributes, including sex, age, and race. RESULTS: In the zero-shot setting, the best-performing ICL model, GEMMA-2, achieved a mean F1-score of 0.858 (95% CI 0.854-0.862), showing an 18.7% improvement over the best textual entailment model, DEBERTA-M (mean F1-score 0.723, 95% CI 0.718-0.728; P<.001). In the few-shot setting, the top ICL model, LLAMA-3, outperformed the leading SetFit models by 21.2%, 21.4%, and 12.3% with 4, 8, and 16 annotations per class, respectively (P<.001). Using 32 labeled instances, the best ICL model achieved a mean F1-score of 0.901 (95% CI 0.895-0.907), only 3.2% lower than the best supervised fine-tuning model, ROBERTA (mean F1-score 0.931, 95% CI 0.924-0.938), which was trained on 3543 labeled instances. Under the conditions tested, fairness evaluation revealed that supervised fine-tuning models exhibited greater bias compared with ICL models in the zero-shot, 4-shot, 8-shot, and 16-shot settings, as measured by true positive rate, false positive rate, and F1-score disparities. CONCLUSIONS: ICL offers a robust and flexible solution for detecting stigmatizing language in EHRs, offering a more data-efficient and equitable alternative to conventional machine learning methods. These findings suggest that ICL could enhance bias detection in clinical documentation while reducing the reliance on extensive labeled datasets.",
      "journal": "JMIR medical informatics",
      "year": "2025",
      "doi": "10.2196/68955",
      "authors": "Chen Hongbo et al.",
      "keywords": "artificial intelligence; electronic health record; fairness; few-shot; in-context learning; large language model; machine learning; prompting strategy; stigmatizing language; text classification; zero-shot",
      "mesh_terms": "Humans; Electronic Health Records; Machine Learning; Language; Social Stigma; Stereotyping; Male; Female",
      "pub_types": "Journal Article; Comparative Study; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40825541/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12402740",
      "ft_text_length": 102699,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12402740)",
      "ft_reason": "Included: substantial approach content (10 indicators)"
    },
    {
      "pmid": "40841556",
      "title": "Racial and socioeconomic disparities in long term survival after surgery and radiation for spinal cord hemangioblastoma.",
      "abstract": "Spinal cord hemangioblastomas are rare, benign, intradural tumors that, despite their nonmalignant histopathology, can lead to substantial neurological morbidity. While disparities in outcomes based on race and socioeconomic status have been well-documented in other spinal tumor populations, their role in spinal cord hemangioblastoma remains poorly understood. In this study, we utilize the National Cancer Database (NCDB) to evaluate the influence of race, socioeconomic factors, and healthcare access on survival outcomes in patients with spinal cord hemangioblastoma. Additionally, we explore the utility of machine learning-based survival models to improve individualized risk prediction and to identify key clinical and sociodemographic determinants of long-term survival. Patients diagnosed with spinal cord hemangioblastoma were identified from the National Cancer Database (NCDB) using ICD-O-3 histology and topography codes. Demographic, socioeconomic, and clinical variables were compared across racial groups (White, Black and Asian). Long-term overall survival (OS) was defined as survival beyond 10\u00a0years. Kaplan-Meier and multivariable Cox regression analyses were used to evaluate survival outcomes and identify independent predictors of mortality. Tumor size was stratified using the cohort-wide mean (62.2\u00a0mm) for interpretability. Temporal trends in racial distribution and surgical technique (open vs. MIS) were assessed using Mann-Kendall trend testing. Gradient Boosting Survival, Cox proportional hazards, and Random Survival Forest models were developed and validated for mortality prediction. The best-performing model was interpreted using SHAP analysis. A total of 716 adult patients with spinal cord hemangioblastoma were analyzed, with the majority being White (83.7%), followed by Black (12.3%) and Asian (4%). Significant differences were observed across racial groups in age, insurance status, income quartiles, and comorbidity scores, though sex distribution and facility type utilization were comparable. Most patients were treated at academic centers, and surgery alone was the predominant treatment modality, with no racial disparities in extent of resection or use of radiation. Kaplan-Meier analysis showed significantly higher 10-year and long-term mortality in White patients; however, race was not an independent predictor in multivariable Cox regression, where increased age, higher CDCC scores, urban residence, and treatment at comprehensive community cancer centers were associated with worse survival. Surgery, with or without radiation, was protective compared to radiation alone. Temporal analysis showed stable racial distribution and minimal uptake of minimally invasive surgery from 2010 to 2017. The Gradient Boosting Survival model achieved the highest predictive performance (AUC\u2009=\u20090.8214; C-index\u2009=\u20090.7817), with age, facility type, and comorbidity burden identified as the strongest predictors of mortality in SHAP analysis. A publicly available web-based calculator was developed based on this model to provide individualized survival estimates. Racial and socioeconomic disparities were associated with differences in clinical outcomes on univariate analysis. However, race and insurance status were not independent predictors of mortality in multivariable-adjusted models. This suggests that the observed survival differences may be explained by confounding factors, such as comorbidity burden, treatment modality, or access to specialized care. Notably, poorer survival was independently associated with treatment at Comprehensive Community Cancer Programs and with higher comorbidity scores, underscoring the importance of ensuring equitable access to high-volume, specialized centers. Lastly, the Gradient Boosting Survival model enhanced mortality risk prediction by incorporating both clinical and socioeconomic variables, supporting its potential utility in guiding targeted interventions to improve long-term outcomes.",
      "journal": "Scientific reports",
      "year": "2025",
      "doi": "10.1038/s41598-025-13330-7",
      "authors": "Ghaith Abdul Karim et al.",
      "keywords": "Racial disparities; Socioeconomics; Spinal cord hemangioblastomas; Survival machine learning models",
      "mesh_terms": "Humans; Female; Male; Spinal Cord Neoplasms; Middle Aged; Hemangioblastoma; Adult; Healthcare Disparities; Aged; Socioeconomic Factors; Kaplan-Meier Estimate; Racial Groups; Socioeconomic Disparities in Health",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40841556/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12371076",
      "ft_text_length": 38551,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12371076)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "40866555",
      "title": "Quantifying device type and handedness biases in a remote Parkinson's disease AI-powered assessment.",
      "abstract": "We investigate issues pertaining to algorithmic fairness and digital health equity within the context of using machine learning to predict Parkinson's Disease (PD) with data recorded from structured assessments of finger and hand movements. We evaluate the impact of demographic bias and bias related to device type and handedness. We collected data from 251 participants (99 with PD or suspected PD, 152 without PD or any suspicion of PD). Using a random forest model, we observe 92% accuracy, 94% AUROC, 86% sensitivity, 92% specificity, and 84% F1-score. When examining only F1-score differences across groups, no significant bias appears. However, a closer look reveals bias regarding positive prediction and error rates. While we find that sex and ethnicity have no statistically significant impact on PD predictions, biases exist regarding device type and dominant hand, as evidenced by disparate impact and equalized odds. Our findings suggest that remote digital health diagnostics may exhibit underrecognized biases related to handedness and device characteristics, the latter of which can act as a proxy for socioeconomic factors.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01934-2",
      "authors": "Tumpa Zerin Nasrin et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40866555/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12391457",
      "ft_text_length": 51760,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12391457)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "40905712",
      "title": "Detecting, Characterizing, and Mitigating Implicit and Explicit Racial Biases in Health Care Datasets With Subgroup Learnability: Algorithm Development and Validation Study.",
      "abstract": "BACKGROUND: The growing adoption of diagnostic and prognostic algorithms in health care has led to concerns about the perpetuation of algorithmic bias against disadvantaged groups of individuals. Deep learning methods to detect and mitigate bias have revolved around modifying models, optimization strategies, and threshold calibration with varying levels of success and tradeoffs. However, there have been limited substantive efforts to address bias at the level of the data used to generate algorithms in health care datasets. OBJECTIVE: The aim of this study is to create a simple metric (AEquity) that uses a learning curve approximation to distinguish and mitigate bias via guided dataset collection or relabeling. METHODS: We demonstrate this metric in 2 well-known examples, chest X-rays and health care cost utilization, and detect novel biases in the National Health and Nutrition Examination Survey. RESULTS: We demonstrated that using AEquity to guide data-centric collection for each diagnostic finding in the chest radiograph dataset decreased bias by between 29% and 96.5% when measured by differences in area under the curve. Next, we wanted to examine (1) whether AEquity worked on intersectional populations and (2) if AEquity is invariant to different types of fairness metrics, not just area under the curve. Subsequently, we examined the effect of AEquity on mitigating bias when measured by false negative rate, precision, and false discovery rate for Black patients on Medicaid. When we examined Black patients on Medicaid, at the intersection of race and socioeconomic status, we found that AEquity-based interventions reduced bias across a number of different fairness metrics including overall false negative rate by 33.3% (bias reduction absolute=1.88\u00d710-1, 95% CI 1.4\u00d710-1 to 2.5\u00d710-1; bias reduction of 33.3%, 95% CI 26.6%-40%; precision bias by 7.50\u00d710-2, 95% CI 7.48\u00d710-2 to 7.51\u00d710-2; bias reduction of 94.6%, 95% CI 94.5%-94.7%; false discovery rate by 94.5%; absolute bias reduction=3.50\u00d710-2, 95% CI 3.49\u00d710-2 to 3.50\u00d710-2). Similarly, AEquity-guided data collection demonstrated bias reduction of up to 80% on mortality prediction with the National Health and Nutrition Examination Survey (bias reduction absolute=0.08, 95% CI 0.07-0.09). Then, we wanted to compare AEquity to state-of-the-art data-guided debiasing measures such as balanced empirical risk minimization and calibration. Consequently, we benchmarked against balanced empirical risk minimization and calibration and showed that AEquity-guided data collection outperforms both standard approaches. Moreover, we demonstrated that AEquity works on fully connected networks; convolutional neural networks such as ResNet-50; transformer architectures such as VIT-B-16, a vision transformer with 86 million parameters; and nonparametric methods such as Light Gradient-Boosting Machine. CONCLUSIONS: In short, we demonstrated that AEquity is a robust tool by applying it to different datasets, algorithms, and intersectional analyses and measuring its effectiveness with respect to a range of traditional fairness metrics.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/71757",
      "authors": "Gulamali Faris et al.",
      "keywords": "bias; data-centric artificial intelligence; fairness; machine learning",
      "mesh_terms": "Bias; Humans; Datasets as Topic; Routinely Collected Health Data; Radiography, Thoracic; Health Care Costs; Black or African American; Medicaid; Race Factors; Social Class; Nutrition Surveys; Machine Learning; Data Curation; Mortality; United States",
      "pub_types": "Journal Article; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40905712/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12410029",
      "ft_text_length": 47563,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12410029)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "40927497",
      "title": "Beyond the Algorithm: A Perspective on Tackling Bias and Cultural Sensitivity in AI-Guided Aesthetic Standards for Cosmetic Surgery in the Middle East and North Africa (MENA) Region.",
      "abstract": "Artificial intelligence (AI) is increasingly reshaping cosmetic surgery by enhancing surgical planning, predicting outcomes, and enabling objective aesthetic assessment. Through narrative synthesis of existing literature and case studies, this perspective paper explores the issue of algorithmic bias in AI-powered aesthetic technologies and presents a framework for culturally sensitive application within cosmetic surgery practices in the Middle East and North Africa (MENA) region. Existing AI systems are predominantly trained on datasets that underrepresent MENA phenotypes, resulting in aesthetic recommendations that disproportionately reflect Western beauty ideals. The MENA region, however, encompasses a broad spectrum of beauty standards that merge traditional cultural aesthetics with modern global trends, posing unique challenges for AI integration. To ensure ethical and clinically relevant deployment, AI systems must undergo fundamental changes in algorithm design, including the incorporation of culturally diverse datasets with adequate MENA representation, implementation of cultural competency principles, and active collaboration with regional healthcare professionals. The framework outlines concrete criteria for evaluating cultural representativeness in AI training data and outcome assessments, supporting future empirical validation. Developing culturally aware AI tools is both a moral obligation and a clinical priority. This framework provides both a moral imperative and clinical pathway for ensuring AI serves to support, rather than homogenize, the region's diverse aesthetic traditions.",
      "journal": "Clinical, cosmetic and investigational dermatology",
      "year": "2025",
      "doi": "10.2147/CCID.S543045",
      "authors": "Makhseed Abdulrahman et al.",
      "keywords": "MENA region; aesthetic medicine; algorithmic bias; artificial intelligence; cosmetic surgery; cultural competency; facial analysis; health equity; medical ethics",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40927497/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12416507",
      "ft_text_length": 33538,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12416507)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "40933771",
      "title": "Bias correction for nonignorable missing counts of areal HIV new diagnosis.",
      "abstract": "Public health data, such as HIV new diagnoses, are often left-censored due to confidentiality issues. Standard analysis approaches that assume censored values as missing at random often lead to biased estimates and inferior predictions. Motivated by the Philadelphia areal counts of HIV new diagnosis for which all values less than or equal to 5 are suppressed, we propose two methods to reduce the adverse influence of missingness on predictions and imputation of areal HIV new diagnoses. One is the likelihood-based method that integrates the missing mechanism into the likelihood function, and the other is a nonparametric algorithm for matrix factorization imputation. Numerical studies and the Philadelphia data analysis demonstrate that the two proposed methods can significantly improve prediction and imputation based on left-censored HIV data. We also compare the two methods on their robustness to model misspecification and find that both methods appear to be robust for prediction, while their performance for imputation depends on model specification.",
      "journal": "Stat",
      "year": "2023",
      "doi": "10.1002/sta4.555",
      "authors": "Qu Tianyi et al.",
      "keywords": "left-censored; likelihood; matrix factorization; missing value; spatiotemporal data",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40933771/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12419480",
      "ft_text_length": 41698,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12419480)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40940655",
      "title": "FanFAIR: sensitive data sets semi-automatic fairness assessment.",
      "abstract": "BACKGROUND: Research has shown how data sets convey social bias in Artificial Intelligence systems, especially those based on machine learning. A biased data set is not representative of reality and might contribute to perpetuate societal biases within the model. To tackle this problem, it is important to understand how to avoid biases, errors, and unethical practices while creating the data sets. In order to provide guidance for the use of data sets in contexts of critical decision-making, such as health decisions, we identified six fundamental data set features (balance, numerosity, unevenness, compliance, quality, incompleteness) that could affect model fairness. These features were the foundation for the FanFAIR framework. RESULTS: We extended the FanFAIR framework for the semi-automated evaluation of fairness in data sets, by combining statistical information on data with qualitative features. In particular, we present an improved version of FanFAIR which introduces novel outlier detection capabilities working in multivariate fashion, using two state-of-the-art methods: the Empirical Cumulative-distribution Outlier Detection (ECOD) and Isolation Forest. We also introduce a novel metric for data set balance, based on an entropy measure. CONCLUSION: We addressed the issue of how much (un)fairness can be included in a data set used for machine learning research, focusing on classification issues. We developed a rule-based approach based on fuzzy logic that combines these characteristics into a single score and enables a semi-automatic evaluation of a data set in algorithmic fairness research. Our tool produces a detailed visual report about the fairness of the data set. We show the effectiveness of FanFAIR by applying the method on two open data sets.",
      "journal": "BMC medical informatics and decision making",
      "year": "2025",
      "doi": "10.1186/s12911-025-03184-4",
      "authors": "Gallese Chiara et al.",
      "keywords": "Data bias; Fairness; Fuzzy logic; Trustworthy artificial intelligence",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40940655/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12427094",
      "ft_text_length": 79598,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12427094)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "40969781",
      "title": "Using a Large Language Model (ChatGPT-4o) to Assess the Risk of Bias in Randomized Controlled Trials of Medical Interventions: Interrater Agreement With Human Reviewers.",
      "abstract": "BACKGROUND: Risk of bias (RoB) assessment is a highly skilled task that is time-consuming and subject to human error. RoB automation tools have previously used machine learning models built using relatively small task-specific training sets. Large language models (LLMs; e.g., ChatGPT) are complex models built using non-task-specific Internet-scale training sets. They demonstrate human-like abilities and might be able to support tasks like RoB assessment. METHODS: Following a published peer-reviewed protocol, we randomly sampled 100 Cochrane reviews. New or updated reviews that evaluated medical interventions, included \u2265\u20091 eligible trial, and presented human consensus assessments using Cochrane RoB1 or RoB2 were eligible. We excluded reviews performed under emergency conditions (e.g., COVID-19), and those on public health or welfare. We randomly sampled one trial from each review. Trials using individual- or cluster-randomized designs were eligible. We extracted human consensus RoB assessments of the trials from the reviews, and methods texts from the trials. We used 25 review-trial pairs to develop a ChatGPT prompt to assess RoB using trial methods text. We used the prompt and the remaining 75 review-trial pairs to estimate human-ChatGPT agreement for \"Overall RoB\" (primary outcome) and \"RoB due to the randomization process\", and ChatGPT-ChatGPT (intrarater) agreement for \"Overall RoB\". We used ChatGPT-4o (February 2025) throughout. RESULTS: The 75 reviews were sampled from 35 Cochrane review groups, and all used RoB1. The 75 trials spanned five decades, and all but one were published in English. Human-ChatGPT agreement for \"Overall RoB\" assessment was 50.7% (95% CI 39.3%-62.0%), substantially higher than expected by chance (p\u2009=\u20090.0015). Human-ChatGPT agreement for \"RoB due to the randomization process\" was 78.7% (95% CI 69.4%-88.0%; p\u2009<\u20090.001). ChatGPT-ChatGPT agreement was 74.7% (95% CI 64.8%-84.6%; p\u2009<\u20090.001). CONCLUSIONS: ChatGPT appears to have some ability to assess RoB and is unlikely to be guessing or \"hallucinating\". The estimated agreement for \"Overall RoB\" is well above estimates of agreement reported for some human reviewers, but below the highest estimates. LLM-based systems for assessing RoB may be able to help streamline and improve evidence synthesis production.",
      "journal": "Cochrane evidence synthesis and methods",
      "year": "2025",
      "doi": "10.1002/cesm.70048",
      "authors": "Rose Christopher James et al.",
      "keywords": "ChatGPT; LLM; RoB; artificial intelligence; evidence synthesis; large language model; risk of bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40969781/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12442625",
      "ft_text_length": 31148,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12442625)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41001477",
      "title": "Clinically Informed Semi-Supervised Learning Improves Disease Annotation and Equity from Electronic Health Records: A Glaucoma Case Study.",
      "abstract": "Clinical notes represent a vast but underutilized source of information for disease characterization, whereas structured electronic health record (EHR) data such as ICD codes are often noisy, incomplete, and too coarse to capture clinical complexity. These limitations constrain the accuracy of datasets used to investigate disease pathogenesis and progression and to develop robust artificial intelligence (AI) systems. To address this challenge, we introduce Ci-SSGAN (Clinically Informed Semi-Supervised Generative Adversarial Network), a novel framework that leverages large-scale unlabeled clinical text to reannotate patient conditions with improved accuracy and equity. As a case study, we applied Ci-SSGAN to glaucoma, a leading cause of irreversible blindness characterized by pronounced racial and ethnic disparities. Trained on 2.1 million ophthalmology notes, Ci-SSGAN achieved 0.85 accuracy and 0.95 AUROC, representing a 10.19% AUROC improvement compared to ICD-based labels (0.74 accuracy, 0.85 AUROC). Ci-SSGAN also narrowed subgroup performance gaps, with F1 gains for Black patients (+0.05), women (+0.06), and younger patients (+0.033). By integrating semi-supervised learning and demographic conditioning, Ci-SSGAN minimizes reliance on expert annotations, making AI development more accessible to resource-constrained healthcare systems.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.09.12.25335665",
      "authors": "Moradi Mousa et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41001477/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12458537",
      "ft_text_length": 36662,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12458537)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "41001488",
      "title": "New Model, Old Risks? Sociodemographic Bias and Adversarial Hallucinations Vulnerability in GPT-5.",
      "abstract": "Extending our validated benchmarking work, GPT-5 showed no improvement in sociodemographic-linked decision variation compared with GPT-4o and seemed to be worse on several endpoints. We re-tested GPT-5 with a fixed pipeline: 500 physician-validated emergency vignettes, each replayed across 32 sociodemographic labels plus an unlabeled control, answering the same four questions (triage, further testing, treatment level, and need for mental-health assessment). This design holds clinical content constant to isolate the effect of the label. GPT-5 reproduced subgroup-linked variation, with higher assigned urgency and less advanced testing for several historically marginalized and intersectional groups. Notably, several LGBTQIA+ labels were flagged for mental-health screening in 100% of cases, versus ~41-73% for comparable groups with GPT-4o. Additionally, in an adversarial re-run that inserted one fabricated medical detail into otherwise standard clinical cases, GPT-5 adopted or elaborated on the fabrication in 65% of runs (vs 53% for GPT-4o). A single mitigation prompt reduced this to 7.67%.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.09.19.25336180",
      "authors": "Omar Mahmud et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41001488/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12458962",
      "ft_text_length": 7107,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12458962)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41036091",
      "title": "Evaluating the impact of data biases on algorithmic fairness and clinical utility of machine learning models for prolonged opioid use prediction.",
      "abstract": "OBJECTIVES: The growing use of machine learning (ML) in healthcare raises concerns about how data biases affect real-world model performance. While existing frameworks evaluate algorithmic fairness, they often overlook the impact of bias on generalizability and clinical utility, which are critical for safe deployment. Building on prior methods, this study extends bias analysis to include clinical utility, addressing a key gap between fairness evaluation and decision-making. MATERIALS AND METHODS: We applied a 3-phase evaluation to a previously developed model predicting prolonged opioid use (POU), validated on Veterans Health Administration (VHA) data. The analysis included internal and external validation, model retraining on VHA data, and subgroup evaluation across demographic, vulnerable, risk, and comorbidity groups. We assessed performance using area under the receiver operating characteristic curve (AUROC), calibration, and decision curve analysis, incorporating standardized net-benefits to evaluate clinical utility alongside fairness and generalizability. RESULTS: The internal cohort (N\u2009=\u200941\u2009929) had a 14.7% POU prevalence, compared to 34.3% in the external VHA cohort (N\u2009=\u2009397\u2009150). The model's AUROC decreased from 0.74 in the internal test cohort to 0.70 in the full external cohort. Subgroup-level performance averaged 0.69 (SD\u2009=\u20090.01), showing minimal deviation from the external cohort overall. Retraining on VHA data improved AUROCs to 0.82. Clinical utility analysis showed systematic shifts in net-benefit across threshold probabilities. DISCUSSION: While the POU model showed generalizability and fairness internally, external validation and retraining revealed performance and utility shifts across subgroups. CONCLUSION: Population-specific biases affect clinical utility-an often-overlooked dimension in fairness evaluation-a key need to ensure equitable benefits across diverse patient groups.",
      "journal": "JAMIA open",
      "year": "2025",
      "doi": "10.1093/jamiaopen/ooaf115",
      "authors": "Naderalvojoud Behzad et al.",
      "keywords": "algorithmic fairness; clinical utility; data bias; generalizability; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41036091/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12483547",
      "ft_text_length": 28965,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12483547)",
      "ft_reason": "Included: bias central + approach content (13 indicators)"
    },
    {
      "pmid": "41107862",
      "title": "Navigating fairness aspects of clinical prediction models.",
      "abstract": "BACKGROUND: Algorithms are increasingly used in healthcare, yet most algorithms lack thorough evaluation and impact assessment across diverse populations. This absence of comprehensive scrutiny introduces a significant risk of inequitable clinical outcomes, particularly between different demographic and socioeconomic groups. MAIN BODY: Societal biases-rooted in structural inequalities and systemic discrimination-often shape the data used to develop these algorithms. When such biases become embedded into predictive models, algorithms frequently favor privileged populations, further deepening existing inequalities. Without proactive efforts to identify and mitigate these biases, algorithms risk disproportionately harming already marginalized groups, widening the gap between advantaged and disadvantaged patients. Various statistical metrics are available to assess algorithmic fairness, each addressing different dimensions of disparity in predictive performance across population groups. However, understanding and applying these fairness metrics in real-world healthcare settings remains limited. Transparency in both the development and communication of algorithms is essential to building a more equitable healthcare system. Openly addressing fairness concerns fosters trust and accountability, ensuring that fairness considerations become an integral part of algorithm design and implementation rather than an afterthought. Using a participatory approach involving three clinicians and three patients with lived experience of type 2 diabetes, we developed a set of guiding questions to help healthcare professionals assess algorithms critically, challenge existing practices, and stimulate discussions. CONCLUSIONS: We aim to direct healthcare professionals on navigating the complexities of bias in healthcare algorithms by encouraging critical thinking about biases present in society, data, algorithms, and healthcare systems.",
      "journal": "BMC medicine",
      "year": "2025",
      "doi": "10.1186/s12916-025-04340-3",
      "authors": "Chakradeo Kaustubh et al.",
      "keywords": "Algorithmic fairness; Algorithms; Clinical decision rules; Clinical decision-making; Delivery of healthcare; Fairness; Health inequities; Health personnel; Prediction algorithms; Risk factors",
      "mesh_terms": "Humans; Algorithms; Diabetes Mellitus, Type 2; Healthcare Disparities",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41107862/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12535043",
      "ft_text_length": 44776,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12535043)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "41118361",
      "title": "Multilingual voice-enabled informatics tools: Catalyst for equitable AI in HIV and HIV-comorbidity healthcare management.",
      "abstract": "Human Immunodeficiency Virus (henceforth HIV) is a global health problem, presently with no known cure. Africa has one of the highest incidences of HIV. Nigeria, within the West African (WA) region, is one of the largest economies on the continent. However, the country continues to struggle with HIV, with approximately 2 million individuals currently infected and experiencing ongoing transmissions. Management of the disease has been difficult due to communication barriers between English-speaking medical practitioners and indigenous patients in rural and suburban regions of the country and bordering countries. In this paper, we used fuzzy logic and voice-enabled technology to create WAHMIDS (West African HIV and HIV-comorbidity Multilingual Indigenous Diagnostic Software) and WAHMIMA (West African HIV Multilingual Informatics Mobile Application), which are health apps designed to help diagnose HIV and manage related health issues in both rural and urban areas for people who speak different indigenous languages in West Africa. Additionally, illustrations of the application of this tool to HIV diagnosis, using existing HIV data, are demonstrated. We expect that these tools will assist English-speaking medical workers and inhabitants of West African communities in their efforts to control HIV transmissions. These informatics tools have the potential to help prescribe medications for HIV and HIV-comorbidity patients. We anticipate that these informatics tools will help address healthcare disparities and promote diversity, equality, and inclusion by reducing the gaps in healthcare delivery between different regions and facilitating the collection of diverse patient data, which is essential for developing and planning more inclusive and accurate healthcare strategies in the West African sub-region.",
      "journal": "PloS one",
      "year": "2025",
      "doi": "10.1371/journal.pone.0332573",
      "authors": "Oluwagbemi Olugbenga Oluseun et al.",
      "keywords": "",
      "mesh_terms": "Humans; HIV Infections; Nigeria; Multilingualism; Artificial Intelligence; Language; Delivery of Health Care",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41118361/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12539699",
      "ft_text_length": 59813,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12539699)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41127255",
      "title": "Ethical sourcing in the context of health data supply chain management: a value sensitive design approach.",
      "abstract": "OBJECTIVE: The Bridge2AI program is establishing rules of practice for creating ethically sourced health data repositories to support the effective use of ML/AI in biomedical and behavioral research. Given the initially undefined nature of ethically sourced data, this work concurrently developed definitions and guidelines alongside repository creation, grounded in a practical, operational framework. MATERIALS AND METHODS: A Value Sensitive Design (VSD) approach was used to explore ethical tensions across stages of health data repository development. The conceptual investigation drew from supply chain management (SCM) processes to (1) identify actors who would interact with or be affected by the data repository use and outcomes; (2) determine what values to consider (ie, traceability accountability, security); and (3) analyze and document value trade-offs (ie, balancing risks of harm to improvements in healthcare). This SCM framework provides operational guidance for managing complex, multi-source data flows with embedded bias mitigation strategies. RESULTS: This conceptual investigation identified the actors, values, and tensions that influence ethical sourcing when creating a health data repository. The SCM steps provide a scaffolding to support ethical sourcing across the pre-model stages of health data repository development. Ethical sourcing includes documenting data provenance, articulating expectations for experts, and practices for ensuring data privacy, equity, and public benefit. Challenges include risks of ethics washing and highlight the need for transparent, value-driven practices. DISCUSSION: Integrating VSD with SCM frameworks enables operationalization of ethical values, improving data integrity, mitigating biases, and enhancing trust. This approach highlights how foundational decisions influence repository quality and AI/ML system usability, addressing provenance, traceability, redundancy, and risk management central to ethical data sourcing. CONCLUSION: To create authentic, impactful health data repositories that serve public health goals, organizations must prioritize transparency, accountability, and operational frameworks like SCM that comprehensively address the complexities and risks inherent in data stewardship.",
      "journal": "JAMIA open",
      "year": "2025",
      "doi": "10.1093/jamiaopen/ooaf101",
      "authors": "Nebeker Camille et al.",
      "keywords": "artificial intelligence; ethically sourced; health data repository; machine learning; research ethics",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41127255/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12539179",
      "ft_text_length": 36800,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12539179)",
      "ft_reason": "Included: substantial approach content (3 indicators)"
    },
    {
      "pmid": "41141904",
      "title": "Assessment of demographic bias in retinal age prediction machine learning models.",
      "abstract": "The retinal age gap, defined as the difference between the predicted retinal age and chronological age, is an emerging biomarker for many eye conditions and even non-ocular diseases. Machine learning (ML) models are commonly used for retinal age prediction. However, biases in ML models may lead to unfair predictions for some demographic groups, potentially exacerbating health disparities. This retrospective cross-sectional study evaluated demographic biases related to sex and ethnicity in retinal age prediction models using retinal imaging data (color fundus photography [CFP], optical coherence tomography [OCT], and combined CFP\u202f+\u202fOCT) from 9,668 healthy individuals (mean age 56.8\u202fyears; 52% female) in the UK Biobank. The RETFound foundation model was fine-tuned to predict retinal age, and bias was assessed by comparing mean absolute error (MAE) and retinal age gaps across demographic groups. The combined CFP\u202f+\u202fOCT model achieved the lowest MAE (3.01\u202fyears), outperforming CFP-only (3.40\u202fyears) and OCT-only (4.37\u202fyears) models. Significant sex differences were observed only in the CFP model (p\u202f<\u202f0.001), while significant ethnicity differences appeared only in the OCT model (p\u202f<\u202f0.001). No significant sex/ethnicity differences were observed in the combined model. These results demonstrate that retinal age prediction models can exhibit biases, and that these biases, along with model accuracy, are influenced by the choice of imaging modality (CFP, OCT, or combined). Identifying and addressing sources of bias is essential for safe and reliable clinical implementation. Our results emphasize the importance of comprehensive bias assessments and prospective validation, ensuring that advances in machine learning and artificial intelligence benefit all patient populations.",
      "journal": "Frontiers in artificial intelligence",
      "year": "2025",
      "doi": "10.3389/frai.2025.1653153",
      "authors": "Nielsen Christopher et al.",
      "keywords": "bias; machine learning; multimodal imaging; retinal age prediction; retinal imaging",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41141904/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12549555",
      "ft_text_length": 12910,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12549555)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41146192",
      "title": "Reducing inequalities using an unbiased machine learning approach to identify births with the highest risk of preventable neonatal deaths.",
      "abstract": "BACKGROUND: Despite contemporaneous declines in neonatal mortality, recent studies show the existence of left-behind populations that continue to have higher mortality rates than the national averages. Additionally, many of these deaths are from preventable causes. This reality creates the need for more precise methods to identify high-risk births, allowing policymakers to target them more effectively. This study fills this gap by developing unbiased machine-learning approaches to more accurately identify births with a high risk of neonatal deaths from preventable causes. METHODS: We link administrative databases from the Brazilian health ministry to obtain birth and death records in the country from 2015 to 2017. The final dataset comprises 8,797,968 births, of which 59,615 newborns died before reaching 28 days alive (neonatal deaths). These neonatal deaths are categorized into preventable deaths (42,290) and non-preventable deaths (17,325). Our analysis identifies the death risk of the former group, as they are amenable to policy interventions. We train six machine-learning algorithms, test their performance on unseen data, and evaluate them using a new policy-oriented metric. To avoid biased policy recommendations, we also investigate how our approach impacts disadvantaged populations. RESULTS: XGBoost was the best-performing algorithm for our task, with the 5% of births identified as highest risk by the model accounting for over 85% of the observed deaths. Furthermore, the risk predictions exhibit no statistical differences in the proportion of actual preventable deaths from disadvantaged populations, defined by race, education, marital status, and maternal age. These results are similar for other threshold levels. CONCLUSIONS: We show that, by using publicly available administrative data sets and ML methods, it is possible to identify the births with the highest risk of preventable deaths with a high degree of accuracy. This is useful for policymakers as they can target health interventions to those who need them the most and where they can be effective without producing bias against disadvantaged populations. Overall, our approach can guide policymakers in reducing neonatal mortality rates and their health inequalities. Finally, it can be adapted for use in other developing countries.",
      "journal": "Population health metrics",
      "year": "2025",
      "doi": "10.1186/s12963-025-00420-x",
      "authors": "Ramos Antonio P et al.",
      "keywords": "Algorithmic bias; Health inequality; Machine learning; Neonatal mortality; Program targeting",
      "mesh_terms": "Humans; Machine Learning; Infant, Newborn; Brazil; Infant Mortality; Female; Infant; Perinatal Death; Socioeconomic Factors; Male; Cause of Death; Algorithms; Databases, Factual; Risk Factors",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41146192/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12557940",
      "ft_text_length": 49581,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12557940)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41152446",
      "title": "A bias-resilient client selection analysis for federated brain tumor segmentation.",
      "abstract": "Brain tumor segmentation is difficult because of a number of technical problems, including its complex morphology, individual differences in anatomy, irregular shapes, overlapping, homogeneous gray matter and white matter intensity values, abnormalities that might not contrast with normal tissues, and the possibility of additional complications from various modalities. Expert radiologists may make different conclusions as a result of these difficulties. Regarding this, deep learning techniques, particularly CNN models, can be trained to handle these MRI artifacts and automatically extract features that the human eye is unable to detect, such as variations in shape, texture, and color. Deep learning models may effectively learn features across various modalities, but they are data-hungry techniques that could be enhanced with additional annotated data. Yet, data privacy is the main barrier to the real use of data centralization. To deal with these challenges proposed a federated learning approach. The proposed federated learning enables the decentralized learning of a shared model while sharing data. However, the traditional paradigm introduced in the literature involves institutional biases that have an impact on distributed learning. Proposed Fed_WCE_BTD (Federated Learning with Weak Client Elimination for Brain Tumor Detection) is a combination of a modified UNet architecture and federated. In addition, proposed method uses an optimal adaptive client selection strategy by carefully choosing each client based on their unique strengths. Our contribution to this crucial and costly diagnostic is being validated using the BRATS 2021 dataset, taking into account the slicing of brain tumors. The goal of this research is to outperform non-federated learning or perform on par with non-federated environments. At present, the suggested model is outperforming the others by 1% for detection of enhancing tumor and necrosis. The efficiency of the proposed federated learning was demonstrated by considerably higher dice-coefficient of enhancing tumor (p< 0.05) as compared to non-federated learning. However, the edema identification dice coefficient is 80%, which is similar to the baseline.",
      "journal": "Scientific reports",
      "year": "2025",
      "doi": "10.1038/s41598-025-21548-8",
      "authors": "Zahoora Umme et al.",
      "keywords": "Brain Tumor Segmentation; Client Selection; Federated Learning; Segmentation; U-Net",
      "mesh_terms": "Humans; Brain Neoplasms; Magnetic Resonance Imaging; Deep Learning; Image Processing, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41152446/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12568973",
      "ft_text_length": 44995,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12568973)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41158472",
      "title": "Humanizing pulmonary care in the era of acoustic artificial intelligence: toward global health equity.",
      "abstract": "",
      "journal": "Frontiers in medicine",
      "year": "2025",
      "doi": "10.3389/fmed.2025.1666820",
      "authors": "Irfan Bilal et al.",
      "keywords": "acoustic AI; acoustic diagnostics; algorithmic accountability; artificial intelligence; global health; health equity; pulmonary care; respiratory diagnostics",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41158472/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12554545",
      "ft_text_length": 17425,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12554545)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "41174617",
      "title": "Joint explainable and fair AI in healthcare.",
      "abstract": "The nature of decisions in the healthcare domain necessitates accurate, interpretable, and reliable AI solutions. Explanation Guided Learning (EGL) explores the integration of explanation annotations into learning models to align human and model explanations. In this paper, we propose Explanation Constraints Guided Learning (ECGL), a novel approach inspired by the augmented Lagrangian method that integrates domain-specific explanation constraints directly into model training. The goal is to enhance both predictive accuracy and interpretability, making machine learning models more trustworthy. Experimental results on both tabular and image datasets demonstrate that ECGL maintains high accuracy while incorporating fairness and interpretability constraints. Specifically, ECGL improves predictive accuracy on the diabetes dataset compared to the base model and enhances feature alignment, with SHAP analysis. On average, a 36.8% increase in SHAP importance demonstrates that ECGL effectively aligns model explanations with domain knowledge. Furthermore, ECGL improves the identification of clinically significant regions in pneumonia X-ray images, as validated by both improved Equalized Odds Ratio (EOR) and GradCAM visualizations. ECGL achieves a 13% improvement in the EOR fairness metric, indicating better consistency of predictive performance across different groups. These results confirm that ECGL successfully balances performance, fairness, and interpretability, positioning it as a promising approach for trustworthy healthcare AI applications.",
      "journal": "BMC medical informatics and decision making",
      "year": "2025",
      "doi": "10.1186/s12911-025-03233-y",
      "authors": "Shahbazian Reza et al.",
      "keywords": "Artificial intelligence; Augmented lagrangian; Explainable guided learning (EGL); Fairness; Healthcare",
      "mesh_terms": "Humans; Machine Learning; Medical Informatics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41174617/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12577397",
      "ft_text_length": 79750,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12577397)",
      "ft_reason": "Included: bias central + approach content (14 indicators)"
    },
    {
      "pmid": "41216267",
      "title": "Explainable Transfer Learning with Residual Attention BiLSTM for Prognosis of Ischemic Heart Disease.",
      "abstract": "BACKGROUND: Early and accurate prediction of Ischemic Heart Disease (IHD) is critical to reducing cardiovascular mortality through timely intervention. While deep learning (DL) models have shown promise in disease prediction, many lack interpretability, generalizability, and fairness-particularly when deployed across demographically diverse populations. These shortcomings limit clinical adoption and risk reinforcing healthcare disparities. METHODS: This study proposes a novel model: X-TLRABiLSTM (Explainable Transfer Learning-based Residual Attention Bidirectional LSTM). The architecture integrates transfer learning from pre-trained cardiovascular models into a BiLSTM framework with residual attention layers to improve temporal feature extraction and convergence. To ensure transparency, the model incorporates SHAP (SHapley Additive exPlanations) to quantify the contribution of each clinical feature to the final prediction. Additionally, a demographic reweighting strategy is applied to the training process to reduce bias across subgroups defined by age, gender, and ethnicity. The model was evaluated on the UCI Heart Disease dataset using 10-fold cross-validation. RESULTS: The X-TLRABiLSTM model achieved a classification accuracy of 98.2%, with an F1-score of 98.1% and an AUC of 99.1%, outperforming standard ML classifiers and state-of-the-art DL baselines. SHAP-based interpretability analysis highlighted clinically relevant predictors such as chest pain type, ST depression, and thalassemia. A fairness-aware reweighting strategy was applied during training, and fairness evaluation revealed minimal performance disparity across demographic subgroups, with F1-score gaps \u2264 0.6% and error rate gaps \u2264 0.4%. Confusion matrix analysis demonstrated low false-positive and false-negative rates, reinforcing the model's reliability for clinical deployment. CONCLUSIONS: X-TLRABiLSTM offers a highly accurate, interpretable, and demographically fair framework for IHD prognosis. By combining transfer learning, residual attention, explainable AI, and fairness-aware optimization, this model advances trustworthy AI in healthcare. Its successful performance on benchmark clinical data supports its potential for real-world integration in ethical, AI-assisted cardiovascular diagnostics.",
      "journal": "F1000Research",
      "year": "2025",
      "doi": "10.12688/f1000research.166307.3",
      "authors": "D Cenitta et al.",
      "keywords": "Bidirectional LSTM (BiLSTM); Clinical Decision Support Systems; Deep Learning; Demographic Bias Mitigation; Explainable AI (XAI); Fairness in AI; Ischemic Heart Disease (IHD); Residual Attention Mechanism; SHAP; Transfer Learning",
      "mesh_terms": "Humans; Myocardial Ischemia; Prognosis; Deep Learning; Male; Female; Middle Aged; Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41216267/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12596549",
      "ft_text_length": 48785,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12596549)",
      "ft_reason": "Included: substantial approach content (5 indicators)"
    },
    {
      "pmid": "41223209",
      "title": "Evaluating the impact of sex bias on AI models in musculoskeletal ultrasound of joint recess distension.",
      "abstract": "With the increasing integration of artificial intelligence (AI) in healthcare, concerns about bias in AI models have emerged, particularly regarding demographic factors. In medical imaging, biases in training datasets can significantly impact diagnostic accuracy, leading to unequal healthcare outcomes. This study assessed the impact of sex bias on AI models for diagnosing knee joint recess distension using ultrasound imaging. We utilized a retrospective dataset from community clinics across Canada, comprising 5,000 de-identified MSKUS images categorized by sex and clinical findings. Two binary convolutional neural network (BCNN) classifiers were developed to detect synovial recess distension and determine patient sex. The dataset was balanced across sex and joint recess distension, with models trained using advanced data augmentation and validated through both individual and mixed demographic scenarios using a 5-fold cross-validation strategy. Our BCNN classifiers showed that AI performance varied significantly based on the training data's demographic characteristics. Models trained exclusively on female datasets achieved higher sensitivity and accuracy but exhibited decreased specificity when applied to male images, suggesting a tendency to overfit female-specific features. Conversely, classifiers trained on balanced datasets displayed enhanced generalizability. This was evident from the classification heatmaps, which varied less between sexes, aligning more closely with clinically relevant features. The study highlights the critical influence of demographic biases on the diagnostic accuracy of AI models in medical imaging. Our results demonstrate the necessity for thorough cross-demographic validation and training on diverse datasets to mitigate biases. These findings are based on a supervised CNN model; evaluating whether they extend to other architectures, such as self-supervised learning (SSL) methods, foundation models, and Vision Transformers (ViTs), remains a direction for future research.",
      "journal": "PloS one",
      "year": "2025",
      "doi": "10.1371/journal.pone.0332716",
      "authors": "Mendez M et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Male; Ultrasonography; Retrospective Studies; Middle Aged; Artificial Intelligence; Knee Joint; Adult; Neural Networks, Computer; Sexism; Aged; Canada",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41223209/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12611148",
      "ft_text_length": 39394,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12611148)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "41236165",
      "title": "Public Justification and Normatively Meaningful Bias: Against Imposing Egalitarian Accounts of Algorithmic Bias.",
      "abstract": "In a number of policy, institutional, activist and advocacy contexts, attributing bias to an algorithm does not just describe the algorithm but also imposes a particular, normatively laden conception of bias on others. Given the normative content of such bias attributions, this would involve making moral demands on others to rectify the algorithm, compensate the victims of such bias and/or not unselectively deploy the algorithm. It is also the case that moral demands, especially in the above-mentioned contexts, are subject to a public justification requirement. As it turns out, the dominant accounts of bias in the literature presuppose some version of egalitarianism about justice and that any action that causally contributes to an unjust situation is itself wrong. Since these presuppositions are subject to reasonable disagreement, bias attributions in such situations are wrong because they violate the public justification requirement. In response, we develop a publicly justifiable conception of algorithmic bias.",
      "journal": "Bioethics",
      "year": "2026",
      "doi": "10.1111/bioe.70047",
      "authors": "Muralidharan Anantharaman et al.",
      "keywords": "algorithmic bias; egalitarianism; normatively meaningful bias; public justification; responsibility; structural injustice",
      "mesh_terms": "Humans; Algorithms; Social Justice; Bias; Morals; Ethical Theory",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41236165/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7618478",
      "ft_text_length": 53758,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC7618478)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41254805",
      "title": "Demographic biases in AI-generated simulated patient cohorts: a comparative analysis against census benchmarks.",
      "abstract": "BACKGROUND: Generative artificial intelligence models are being introduced as low-cost tools for creating simulated patient cohorts in undergraduate medical education. Their educational value, however, depends on the extent to which the synthetic populations mirror real-world demographic diversity. We therefore assessed whether two commonly deployed large language models produce patient profiles that reflect the current age, sex, and ethnic composition of the UK. METHODS: GPT-3.5-turbo-0125 and GPT-4-mini-2024-07-18 were each prompted, without demographic steering, to generate 250 UK-based 'patients'. Age was returned directly by the model; sex and ethnicity were inferred from given and family names using a validated census-derived classifier. Observed frequencies for each demographic variable were compared with England and Wales 2021 census expectations by chi-square goodness-of-fit tests. RESULTS: Both cohorts diverged significantly from census benchmarks (p\u2009<\u20090.0001 for every variable). Age distributions showed an absence of very young and older individuals, with certain middle-aged groups overrepresented (GPT-3.5: \u03c72(17)\u2009=\u20091310.4, p\u2009<\u20090.0001; GPT4mini: \u03c72(17)\u2009=\u20091866.1, p\u2009<\u20090.0001). Neither model produced patients younger than 25 years; GPT-3.5 generated no one older than 47 years and GPT-4-mini no one older than 56 years. Gender proportions also differed markedly, skewing heavily toward males (GPT-3.5: \u03c72(1)\u2009=\u200923.84, p\u2009<\u20090.0001; GPT4mini: \u03c72(1)\u2009=\u2009191.7, p\u2009<\u20090.0001). Male patients constituted 64.7% and 92.8% of the two cohorts. Name diversity was limited: GPT-3.5 yielded 104 unique first-last-name combinations, whereas GPT-4-mini produced only nine. Ethnic profiles were similarly imbalanced, featuring overrepresentation of some groups and complete absence of others (\u03c72(10)\u2009=\u200942.19, p\u2009<\u20090.0001). CONCLUSIONS: In their default state, the evaluated models create synthetic patient pools that exclude younger, older, female and most minority-ethnic representations. Such demographically narrow outputs threaten to normalise biased clinical expectations and may undermine efforts to prepare students for equitable practice. Baseline auditing of model behaviour is therefore essential, providing a benchmark against which prompt-engineering or data-curation strategies can be evaluated before generative systems are integrated into formal curricula.",
      "journal": "Advances in simulation (London, England)",
      "year": "2025",
      "doi": "10.1186/s41077-025-00385-9",
      "authors": "Veenhuizen Miriam et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41254805/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12625206",
      "ft_text_length": 24664,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12625206)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41255970",
      "title": "Clinically Informed Semi-Supervised Learning Improves Disease Annotation and Equity from Electronic Health Records: A Glaucoma Case Study.",
      "abstract": "Clinical notes represent a vast but underutilized source of information for disease characterization, whereas structured electronic health record (EHR) data such as ICD codes are often noisy, incomplete, and too coarse to capture clinical complexity. These limitations constrain the accuracy of datasets used to investigate disease pathogenesis and progression and to develop robust artificial intelligence (AI) systems. To address this challenge, we introduce Ci-SSGAN (Clinically Informed Semi-Supervised Generative Adversarial Network), a novel framework that leverages large-scale unlabeled clinical text to reannotate patient conditions with improved accuracy and equity. As a case study, we applied Ci-SSGAN to glaucoma, a leading cause of irreversible blindness characterized by pronounced racial and ethnic disparities. Trained on 2.1 million ophthalmology notes, Ci-SSGAN achieved 0.85 accuracy and 0.95 AUROC, representing a 10.19% AUROC improvement compared to ICD-based labels (0.74 accuracy, 0.85 AUROC). Ci-SSGAN also narrowed subgroup performance gaps, with F1 gains for Black patients (+ 0.05), women (+ 0.06), and younger patients (+ 0.033). By integrating semi-supervised learning and demographic conditioning, Ci-SSGAN minimizes reliance on expert annotations, making AI development more accessible to resource-constrained healthcare systems.",
      "journal": "Research square",
      "year": "2025",
      "doi": "10.21203/rs.3.rs-7546650/v1",
      "authors": "Moradi Mousa et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41255970/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12622191",
      "ft_text_length": 36822,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12622191)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "41256428",
      "title": "A Systematic Fairness Evaluation of Racial Bias in Alzheimer's Disease Diagnosis Using Machine Learning Models.",
      "abstract": "INTRODUCTION: Alzheimer's disease (AD) is a major global health concern, expected to affect 12.7 million Americans by 2050. Machine learning (ML) algorithms have been developed for AD diagnosis and progression prediction, but the lack of racial diversity in clinical datasets raises concerns about their generalizability across demographic groups, particularly underrepresented populations. Studies show ML algorithms can inherit biases from data, leading to biased AD predictions. METHODS: This study investigates the fairness of ML models in AD diagnosis. We hypothesize that models trained on a single racial group perform well within that group but poorly in others. We employ feature selection and model training techniques to improve fairness. RESULTS: Our findings support our hypothesis that ML models trained on one group underperform on others. We also demonstrated that applying fairness techniques to ML models reduces their bias. DISCUSSION: This study highlights the need for racial diversity in datasets and fair models for AD prediction.",
      "journal": "bioRxiv : the preprint server for biology",
      "year": "2025",
      "doi": "10.1101/2025.09.30.678854",
      "authors": "Baddam Neha Goud et al.",
      "keywords": "Algorithmic Bias; Alzheimer\u2019s Disease; Fairness; Machine Learning",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41256428/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12622001",
      "ft_text_length": 25619,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12622001)",
      "ft_reason": "Included: bias central + approach content (10 indicators)"
    },
    {
      "pmid": "41260014",
      "title": "Knowledge-informed deep learning to mitigate bias in joint air pollutant prediction.",
      "abstract": "Accurate prediction of atmospheric air pollutants is critical for public health protection and environmental management. Traditional machine learning (ML) methods achieve high spatial resolution but lack physicochemical constraints, leading to systematic biases that compromise exposure estimates for epidemiological studies. Chemical transport models incorporate atmospheric physics but require expensive parameterization and often fail to capture local-scale variability crucial for health impact assessment. This gap between data-driven accuracy and physical realism presents a major obstacle to advancing air quality science. We address this challenge through a novel physics-informed deep learning framework that integrates advection-diffusion equations and fluid dynamics constraints directly into neural network architectures for multi-pollutant prediction. Our approach models air pollutant pairs across geographically distinct domains (NO2/NOx for California; PM2.5/PM10 for mainland China), providing a comprehensive framework for physics-constrained atmospheric modeling at high resolution. Through an efficient framework, our methodology demonstrates that incorporating proxy advection and diffusion fields as physical constraints fundamentally alters learning dynamics, reducing generalization error and eliminating systematic bias inherent in data-driven approaches while improving computational efficiency compared to graph networks. Site-based validation reveals unprecedented bias reduction: 21%-42% for nitrogen oxides and 16%-17% for particulate matter compared to the baseline deep learning methods. Our methodology uniquely generates physically interpretable parameters while providing explicit uncertainty quantification through ensemble techniques. The substantial bias reduction coupled with physically interpretable parameters has immediate implications for improving air pollutant exposure assessment and understanding in epidemiological research, potentially transforming health effect evaluations that rely on accurate spatial predictions.",
      "journal": "Environment international",
      "year": "2025",
      "doi": "10.1016/j.envint.2025.109915",
      "authors": "Li Lianfa et al.",
      "keywords": "Air pollution; Bias mitigation; Deep learning; Joint prediction; Knowledge fusion; Physics-informed modeling",
      "mesh_terms": "Deep Learning; Air Pollutants; Air Pollution; Particulate Matter; Environmental Monitoring; California; China; Nitrogen Oxides",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41260014/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12810719",
      "ft_text_length": 74668,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12810719)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41264928",
      "title": "Reasons for Perceived Everyday Discrimination, Quality of Life, and Psychosocial Health of Breast Cancer Survivors: A Cross-Sectional Cluster Analysis.",
      "abstract": "IntroductionDiscrimination exacerbates disparities among breast cancer survivors (BCS), yet how different reasons for experiencing perceived discrimination (e.g., race, age) influence health remains understudied. We explored the association between self-reported discrimination, psychosocial health, and quality of life (QOL), identified clusters based on reasons for perceived discrimination, and examined differences in QOL and psychosocial outcomes between these clusters.MethodsIn this cross-sectional study, we examined correlations between reasons for perceived discrimination (Everyday Discrimination Scale; EDS), QOL domains (cognitive, physical, social, emotional, and functional QOL measured with FACT-G), social dysfunction (Social Difficulties Inventory), and a psychological distress composite score (included measures of stress [Perceived Stress Scale], anxiety [PROMIS Anxiety], and depression [PROMIS Depression]), among 174 breast cancer survivors (stage 0-IV; \u226521\u00a0years). We used k-modes clustering to identify discrimination groups. Differences in demographics, clinical characteristics, and outcomes across clusters were assessed using Chi-square, analysis of variance, covariance, or non-parametric tests, followed by post hoc analyses.ResultsOverall, experiences of discrimination were associated with poorer QOL and psychosocial health (|0.306|<r<|0.452|, P < 0.001). Six distinct clusters emerged based on reasons for perceived discrimination from the EDS. Compared to Cluster 4 (no discrimination), participants in Cluster 1 (discrimination due to gender, age, and physical characteristics) had lower cognitive and physical QOL (4.3 < mean difference [MD]< 5.0, P < 0.001). Participants in Cluster 3 (discrimination due to physical characteristics) had poorer functional QOL, greater social disfunction, and higher psychological distress composite scores (0.3<MD <9.4, P < 0.001) than Cluster 4. Differences between Clusters 2 (discrimination due to gender) and 5 (discrimination due to gender, race/ethnicity) with all other Clusters were not statistically significant (P > 0.05).ConclusionQOL and psychosocial health scores varied between clusters based on reasons for perceived discrimination. Future interventions to improve QOL for breast cancer survivors should consider addressing stigma related to gender, physical appearance, and other forms of discrimination.",
      "journal": "Cancer control : journal of the Moffitt Cancer Center",
      "year": "2025",
      "doi": "10.1177/10732748251399963",
      "authors": "Franco-Rocha Oscar Y et al.",
      "keywords": "discrimination; machine learning; psycho-oncology; quality of life; social phenotype",
      "mesh_terms": "Humans; Quality of Life; Female; Cross-Sectional Studies; Breast Neoplasms; Middle Aged; Cancer Survivors; Cluster Analysis; Adult; Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41264928/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12638705",
      "ft_text_length": 40888,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12638705)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41273675",
      "title": "Equity-promoting integer programming approaches for medical resident rotation scheduling.",
      "abstract": "Motivated by our collaboration with a residency program at an academic health system, we propose new integer programming (IP) approaches for the resident-to-rotation assignment problem (RRAP). Given sets of residents, resident classes, and departments, as well as a block structure for each class, staffing needs, rotation requirements for each class, program rules, and resident vacation requests, the RRAP involves finding a feasible year-long rotation schedule that specifies resident assignments to rotations and vacation times. We first present an IP formulation for the RRAP, which mimics the manual method for generating rotation schedules in practice and can be easily implemented and efficiently solved using off-the-shelf optimization software. However, it can lead to disparities in satisfying vacation requests among residents. To mitigate such disparities, we derive an equity-promoting counterpart that finds an optimal rotation schedule, maximizing the number of satisfied vacation requests while minimizing a measure of disparity in satisfying these requests. Then, we propose a computationally efficient Pareto Search Algorithm capable of finding the complete set of Pareto optimal solutions to the equity-promoting IP within a time that is suitable for practical implementation. Additionally, we present a user-friendly tool that implements the proposed models to automate the generation of the rotation schedule. Finally, we construct diverse RRAP instances based on data from our collaborator and conduct extensive experiments to illustrate the potential practical benefits of our proposed approaches. Our results demonstrate the computational efficiency and implementability of our approaches and underscore their potential to enhance fairness in resident rotation scheduling.",
      "journal": "Health care management science",
      "year": "2025",
      "doi": "10.1007/s10729-025-09736-4",
      "authors": "Li Shutian et al.",
      "keywords": "Fairness; Integer programming; Operations management; Operations research; Optimization; Resident scheduling",
      "mesh_terms": "Internship and Residency; Humans; Personnel Staffing and Scheduling; Algorithms; Academic Medical Centers",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41273675/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12743667",
      "ft_text_length": 232245,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12743667)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41282840",
      "title": "Nonfasting, Telehealth-Ready LDL-C Testing With Machine Learning to Improve Cardiovascular Access and Equity.",
      "abstract": "IMPORTANCE: Current LDL-C testing requires 9-12 hour fasting and in-person visits, creating an access crisis: 40% of lipid panels occur outside fasting windows (yielding unreliable results), 60% of US counties lack cardiology services, and millions of patients with diabetes cannot safely fast. Meanwhile, telehealth infrastructure expanded 38-fold post-COVID, yet lipid workflows remain anchored to 1970s protocols. This mismatch drives ~20 million unnecessary repeat visits annually, disproportionately burdening Medicaid populations, essential workers, and rural communities. OBJECTIVE: To demonstrate that machine learning can transform lipid testing from a fasting-dependent, clinic-based bottleneck into an accurate, equitable, telehealth-ready service-eliminating three structural barriers simultaneously: fasting requirements, in-person visits, and racial algorithmic bias. DESIGN SETTING AND PARTICIPANTS: Cross-sectional analysis of All of Us Research Program (n=3,477; test n=696). Crucially, 40.1% were tested outside traditional fasting windows, reflecting real-world practice. We evaluated performance stratified by fasting status, telehealth feasibility (labs-only configuration), racial equity metrics, and economic impact. MAIN OUTCOMES AND MEASURES: Primary: MAE and calibration in non-fasting states. Secondary: Labs-only non-inferiority (\u00b10.5 mg dL-1margin); racial equity (Black-White performance gap); economic savings from eliminated repeat visits; and classification accuracy at treatment thresholds (70, 100, 130 mg dL-1). RESULTS: The ML system demonstrated paradoxical superiority in non-fasting conditions-precisely when needed most. While equations deteriorated (Friedewald MAE 29.1 vs 25.9 mg dL-1fasting, slopes 0.58-0.61), ML maintained accuracy (24.0 vs 23.2 mg dL-1, slopes 0.99-1.07), achieving 17.2% improvement over Friedewald when non-fasting vs 10.4% fasting. Labs-only configuration proved non-inferior (MAE=-0.12, p<0.001), enabling national retail-pharmacy and home-testing workflows. The system achieved racial equity without race input (Black-White gap -0.19 mg dL-1, CI includes zero) while providing greatest improvement for Black patients (19% vs 11% for White). Economically, eliminating 4,000 repeat visits per 10,000 tests helps address an estimated $2 billion annual repeat-testing cost burden and yields $815,000 total savings per 10,000 tests ($245,000 direct healthcare, $570,000 patient costs), with break-even at just 750 tests. CONCLUSIONS AND RELEVANCE: This ML approach helps address an estimated $2 billion annual problem of repeat testing while tackling three critical quality gaps in cardiovascular prevention: delayed treatment initiation, poor monitoring adherence, and specialty access barriers. By enabling accurate non-fasting, telehealth-compatible, race-free LDL-C estimation, it transforms lipid testing from an access barrier into an access enabler-particularly for the Medicaid, Medicare Advantage, and rural populations who drive both cost and outcomes in value-based care. From a technical standpoint, implementation requires only routine labs and <100 ms computation, making deployment feasible with existing infrastructure.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.10.27.25338909",
      "authors": "Doku Ronald et al.",
      "keywords": "cardiovascular quality improvement; health equity; healthcare delivery; low-density lipoprotein cholesterol; machine learning; non-fasting lipid panel; telehealth; value-based care",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41282840/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12636691",
      "ft_text_length": 42680,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12636691)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "41292658",
      "title": "Develop and Validate A Fair Machine Learning Model to Indentify Patients with High Care-Continuity in Electronic Health Records Data.",
      "abstract": "OBJECTIVES: Electronic health record (EHR) data often missed care outside a given health system, resulting in data discontinuity. We aimed to: (1) quantify misclassification across levels of EHR data discontinuity and identify an optimal continuity threshold. (2) develop a machine learning (ML) model to predict EHR continuity and optimize fairness across racial and ethnic groups, and (3) externally validate the EHR continuity prediction model using an independent dataset. MATERIALS AND METHODS: We used linked OneFlorida+ EHR-Medicaid claims data for model development and REACHnet EHR-Louisiana Blue Cross Blue Shield (LABlue) claims data for external validation. A novel Harmonized Encounter Proportion Score (HEPS) was applied to quantify patient-level EHR data continuity and the impact on misclassification of 42 clinical variables. ML models were trained using routinely available demographic, clinical, and healthcare utilization features derived from structured EHR data. RESULTS: Higher EHR data continuity was associated with lower rates of misclassification. A HEPS threshold of approximately 30% effectively distinguished patients with sufficient data continuity. ML models demonstrated strong performance in predicting high continuity (AUROC=0.77). Fairness assessments showed bias against Hispanic group, which was substantially improved following bias mitigation procedures. Model performance remained robust and fair in the external validation. DISCUSSION: Our study offers a practical metric for quantifying care continuity in EHR networks. The current ML model incorporating EHR-routinely collected information can accurately identify patients with high care continuity. CONCLUSIONS: We developed a generalizable care-continuity classification tool that can be easily applied across EHR systems, strengthening the rigor of EHR-based research.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.11.11.25339938",
      "authors": "Lee Yao An et al.",
      "keywords": "Data continuity; EHR; Machine learning prediction",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41292658/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12642717",
      "ft_text_length": 27356,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12642717)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "41296741",
      "title": "Evaluating algorithmic fairness of machine learning models in predicting underweight, overweight, and adiposity across socioeconomic and caste groups in India: evidence from the longitudinal ageing study in India.",
      "abstract": "Machine learning (ML) models are increasingly applied to predict body mass index (BMI) and related outcomes, yet their fairness across socioeconomic and caste groups remains uncertain, particularly in contexts of structural inequality. Using nationally representative data from more than 55,000 adults aged 45 years and older in the Longitudinal Ageing Study in India (LASI), we evaluated the accuracy and fairness of multiple ML algorithms-including Random Forest, XGBoost, Gradient Boosting, LightGBM, Deep Neural Networks, and Deep Cross Networks-alongside logistic regression for predicting underweight, overweight, and central adiposity. Models were trained on 80% of the data and tested on 20%, with performance assessed using AUROC, accuracy, sensitivity, specificity, and precision. Fairness was evaluated through subgroup analyses across socioeconomic and caste groups and equity-based metrics such as Equalized Odds and Demographic Parity. Feature importance was examined using SHAP values, and bias-mitigation methods were implemented at pre-processing, in-processing, and post-processing stages. Tree-based models, particularly LightGBM and Gradient Boosting, achieved the highest AUROC values (0.79-0.84). Incorporating socioeconomic and health-related variables improved prediction, but fairness gaps persisted: performance declined for scheduled tribes and lower socioeconomic groups. SHAP analyses identified grip strength, gender, and residence as key drivers of prediction differences. Among mitigation strategies, Reject Option Classification and Equalized Odds Post-processing moderately reduced subgroup disparities but sometimes decreased overall performance, whereas other approaches yielded minimal gains. ML models can effectively predict obesity and adiposity risk in India, but addressing bias is essential for equitable application. Continued refinement of fairness-aware ML methods is needed to support inclusive and effective public-health decision-making.",
      "journal": "PLOS digital health",
      "year": "2025",
      "doi": "10.1371/journal.pdig.0000951",
      "authors": "Lee John Tayu et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41296741/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12654920",
      "ft_text_length": 30779,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12654920)",
      "ft_reason": "Included: bias central + approach content (14 indicators)"
    },
    {
      "pmid": "41326301",
      "title": "AI models, bias and data sharing efforts to tackle Alzheimer's disease and related dementias.",
      "abstract": "Artificial intelligence (AI), often seen as a harbinger of future innovation, also presents a dilemma: it can perpetuate existing human biases. However, this issue is not novel or unique to AI. Humans have long been the progenitors of biases, and AI, as a product of human creation, often mirrors these inherent tendencies. Here, we present a perspective on the development and use of AI, recognizing it as a tool influenced by human input and societal norms, rather than an autonomous entity. Modern efforts to technologically enabled data collection approaches and model development, particularly in the context of Alzheimer's disease and related dementias, can potentially reduce bias in AI. We also highlight the importance of data sharing from existing legacy cohorts to help accelerate ongoing AI model development efforts for greater scientific good and clinical care.",
      "journal": "The journal of prevention of Alzheimer's disease",
      "year": "2025",
      "doi": "10.1016/j.tjpad.2025.100400",
      "authors": "Kolachalama Vijaya B et al.",
      "keywords": "(AI); Artificial intelligence; Bias; Data sharing",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41326301/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12811759",
      "ft_text_length": 30978,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12811759)",
      "ft_reason": "Included: bias central + approach content (11 indicators)"
    },
    {
      "pmid": "41334074",
      "title": "Identifying Bias at Scale in Clinical Notes Using Large Language Models.",
      "abstract": "OBJECTIVE: To evaluate whether generative pretrained transformer (GPT)-4 can detect and revise biased language in emergency department (ED) notes, against human-adjudicated gold-standard labels, and to identify modifiable factors associated with biased documentation. PATIENTS AND METHODS: We randomly sampled 50,000 ED medical and nursing notes from the Mount Sinai Health System (January 1, 2023, to December 31, 2023). We also randomly sampled 500 discharge notes from the Medical Information Mart for Intensive Care IV database. The GPT-4 flagged 4 types of bias: discrediting, stigmatizing/labeling, judgmental, and stereotyping. Two human reviewers verified model detections. We used multivariable logistic regression to examine associations between bias and health care utilization, presenting problems (eg, substance use), shift timing, and provider type. We then asked physicians to rate GPT-4's proposed language revisions on a 10-point scale. RESULTS: The GPT-4 showed 97.6% sensitivity and 85.7% specificity compared with the human review. Biased language appeared in 6.5% (3229 of 50,000) of Mount Sinai notes and 7.4% (37 of 500) of Medical Information Mart for Intensive Care IV notes. In adjusted models, frequent health care utilization (adjusted odds ratio [aOR], 2.85; 95% CI, 1.95-4.17), substance use presentations (aOR, 3.09; 95% CI, 2.51-3.80), and overnight shifts (aOR, 1.37; 95% CI, 1.23-1.52) showed elevated odds of biased documentation. Physicians were more likely to include bias than nurses (aOR, 2.26; 95% CI, 2.07-2.46); GPT-4's recommended revisions received mean physician ratings above 9 of 10. CONCLUSION: The study showed that GPT-4 accurately detects biased language in clinical notes, identifies modifiable contributors to that bias, and delivers physician-endorsed revisions. This approach may help mitigate documentation bias and reduce disparities in care.",
      "journal": "Mayo Clinic proceedings. Digital health",
      "year": "2025",
      "doi": "10.1016/j.mcpdig.2025.100296",
      "authors": "Apakama Donald U et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41334074/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12666851",
      "ft_text_length": 33306,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12666851)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "41334247",
      "title": "Evaluating sociodemographic bias in a deployed machine-learned patient deterioration model.",
      "abstract": "BACKGROUND: Bias evaluations of machine learning (ML) models often focus on performance in research settings, with limited assessment of downstream bias following clinical deployment. The objective of this study was to evaluate whether CHARTwatch, a real-time ML early warning system for inpatient deterioration, demonstrated algorithmic bias in model performance, or produced disparities in care processes, and outcomes across patient sociodemographic groups. METHODS: We evaluated CHARTwatch implementation on the internal medicine service at a large academic hospital. Patient outcomes during the intervention period (November 1, 2020-June 1, 2022) were compared to the control period (November 1, 2016-December 31, 2019) using propensity score overlap weighting. We evaluated differences across key sociodemographic subgroups, including age, sex, homelessness, and neighborhood-level socioeconomic and racialized composition. Outcomes included model performance (sensitivity and specificity), processes of care, and patient outcomes (non-palliative in-hospital death). RESULTS: Among 12\u2009877 patients (9079 control, 3798 intervention), 13.3% were experiencing homelessness and 36.9% lived in the quintile with the highest neighborhood racialized and newcomer populations. Model sensitivity was 70.1% overall, with no significant variation across subgroups. Model specificity varied by age, <60 years: 93% (95% Confidence Interval [CI] 91-95%), 60-80 years: 90% (95%CI 87-92%), and >80 years: 84% (95%CI 79-88%), P\u2009<\u2009.001, but not other subgroups. CHARTwatch implementation was associated with an increase in code status documentation among patients experiencing homelessness, without significant differences in other care processes or outcomes. CONCLUSION: CHARTwatch model performance and impact were generally consistent across measured sociodemographic subgroups. ML-based clinical decision support tools, and associated standardization of care, may reduce existing inequities, as was observed for code status orders among patients experiencing homelessness. This evaluation provides a framework for future bias assessments of deployed ML-CDS tools.",
      "journal": "JAMIA open",
      "year": "2025",
      "doi": "10.1093/jamiaopen/ooaf158",
      "authors": "Colacci Michael et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41334247/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12668680",
      "ft_text_length": 30666,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12668680)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "41351059",
      "title": "A fairness-aware machine learning framework for maternal health in Ghana: integrating explainability, bias mitigation, and causal inference for ethical AI deployment.",
      "abstract": "BACKGROUND: Antenatal care (ANC) uptake in Ghana remains inequitable, with socioeconomic and geographic disparities limiting progress toward universal maternal health coverage (SDG 3). We present a novel, fairness-aware machine learning framework for predicting antenatal care uptake among women in Ghana, integrating explainability, bias mitigation, and causal inference to support ethical artificial intelligence (AI) deployment in low- and middle-income countries. METHODS: Using the 2022 Ghana Demographic and Health Survey (n\u2009=\u20093,314 eligible women with a recent live birth), we applied multiple imputation by chained equations (m\u2009=\u200910), appropriate categorical encoding, and synthetic minority oversampling (SMOTE) within training folds. Four supervised models (logistic regression, random forest, XGBoost, support vector machine) underwent stratified 5\u2011fold nested cross\u2011validation with cost\u2011sensitive threshold optimization (selected probability threshold\u2009=\u20090.45). Explainability (SHAP), fairness auditing (AIF360; metrics: statistical parity difference, disparate impact, equal opportunity difference, average odds difference, theil index), preprocessing mitigation (reweighing), counterfactual explanations (DiCE), and cautious treatment effect estimation (causal forests within a double machine learning framework) were integrated. Performance metrics included accuracy, precision, recall, F1, ROC\u2011AUC, minority class PR\u2011AUC, balanced accuracy, calibration (Brier score), and decision curve net benefit. RESULTS: The optimized random forest model achieved the highest accuracy (0.68) and recall (0.84) in identifying women with inadequate ANC contacts. Calibration was strong, with a brier score of 0.158, a calibration slope of 0.97, and an intercept of \u2212\u20090.02. Fairness auditing revealed baseline disparities in model predictions across wealth, region, ethnicity, and religion, with a statistical parity difference for wealth status of 0.182 and a Disparate Impact of 1.62. Following reweighting, disparate impact improved into the fairness range (0.92; within the recommended 0.8\u20131.25 interval), and statistical parity difference reduced to \u2212\u20090.028. Counterfactual analysis indicated that education, wealth, media exposure, and health worker contacts were the most modifiable factors for improving ANC uptake. Exploratory causal inference using double machine learning suggested that improving wealth status and education could be associated with a 16% (Average Treatment Effect [ATE]\u2009=\u20090.163) and 14% (ATE\u2009=\u20090.142) increase, respectively, in the probability of adequate ANC, with greater effects observed among urban and educated subgroups. Adjusted odds ratio (AOR) analysis showed that women in the richest quintile were nearly twice as likely to receive adequate ANC (AOR\u2009=\u20091.91, 95% CI: 1.44\u20132.53; p\u2009<\u20090.001), while those in the poorest quintile had significantly lower odds (AOR\u2009=\u20090.58, 95% CI: 0.45\u20130.75; p\u2009<\u20090.001). Additional significant predictors included health insurance coverage (AOR\u2009=\u20091.74, 95% CI: 1.19\u20132.55), health worker contacts (AOR\u2009=\u20091.33, 95% CI: 1.11\u20131.58), and pregnancy intention (AOR\u2009=\u20091.54, 95% CI: 1.30\u20131.82). CONCLUSION: This integrated, fairness-aware machine learning framework suggest robust, equitable, and actionable prediction of ANC uptake among Ghanaian women. Key modifiable determinants include wealth, education, and healthcare access barriers. The framework offers a replicable, ethical blueprint for transparent and fair AI deployment in maternal health, supporting targeted interventions to advance universal access to quality care in Ghana. Policymakers and health managers can leverage these AI tools to identify high-risk women, monitor intervention impacts, and allocate resources more equitably, advancing progress toward universal access to quality maternal care in Ghana.",
      "journal": "BioData mining",
      "year": "2025",
      "doi": "10.1186/s13040-025-00505-1",
      "authors": "Osborne Augustus et al.",
      "keywords": "Antenatal care; Causal forests; Counterfactuals; Explainable AI; Fairness; Ghana; Health equity; Machine learning; Maternal health",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41351059/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12781815",
      "ft_text_length": 51131,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12781815)",
      "ft_reason": "Included: bias central + approach content (20 indicators)"
    },
    {
      "pmid": "41355748",
      "title": "Detecting Sociodemographic Biases in the Content and Quality of Large Language Model-Generated Nursing Care: Cross-Sectional Simulation Study.",
      "abstract": "BACKGROUND: Large language models (LLMs) are increasingly applied in health care. However, concerns remain that their nursing care recommendations may reflect patients' sociodemographic attributes rather than clinical needs. While this risk is acknowledged, there is a lack of empirical evidence evaluating sociodemographic bias in LLM-generated nursing care plans. OBJECTIVE: To investigate potential biases in nursing care plans generated by LLMs, we focused on whether outputs differ systematically based on patients' sociodemographic characteristics and assessed the implications for equitable nursing care. METHODS: We used a mixed methods simulation study. A standardized clinical vignette experiment was used to prompt GPT-4 to generate 9600 nursing care plans for 96 patient profiles with varying sociodemographic characteristics (eg, sex, age, income, education, and residence). We first conducted a quantitative analysis of all plans, assessing variations in thematic content. Subsequently, a panel of senior nursing experts evaluated the clinical quality (eg, safety, applicability, and completeness) of a stratified subsample of 500 plans. RESULTS: We analyzed 9600 LLM-generated nursing care plans and identified 8 consistent themes. Communication and Education (99.98%) and Emotional Support (99.97%) were nearly universal, while Nurse Training and Event Analysis were least frequent (39.3%). Multivariable analyses revealed systematic sociodemographic disparities. Care plans generated for low-income patient profiles were less likely to include the theme Environmental Adjustment (adjusted relative risk [aRR] 0.90). Profiles with lower education were associated with an increased likelihood of including Family Support (aRR 1.10). Similarly, plans generated for older patient profiles were more likely to contain recommendations for Pain Management (aRR 1.33) and Family Support (aRR 1.62) but were less likely to mention Nurse Training (aRR 0.78). Sex and regional differences were also significant. Expert review of 500 plans showed high overall quality (mean 4.47), with strong interrater reliability (\u03ba=0.76-0.81). However, urban profiles had higher completeness (\u03b2=.22) and applicability (\u03b2=.14) but lower safety scores (\u03b2=-0.09). These findings demonstrate that LLM-generated care plans exhibit systematic sociodemographic bias, raising important implications for fairness and safe deployment in nursing practice. CONCLUSIONS: This study identified that LLMs systematically reproduce sociodemographic biases in the generation of nursing care plans. These biases appear in two forms: they shape the thematic content and influence expert-rated clinical quality. These findings reveal a substantial risk that such models may reinforce existing health inequities. To our knowledge, this is the first empirical evidence documenting these nuanced biases in nursing. The study also contributes a replicable framework for evaluating LLM-generated care plans. Finally, it underscores the critical need for robust human oversight to ensure that artificial intelligence serves as a tool for advancing equity rather than perpetuating disparities.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/78132",
      "authors": "Bai Nan et al.",
      "keywords": "biases; health care equity; large language models; mixed methods study; nursing care; sociodemographic",
      "mesh_terms": "Humans; Cross-Sectional Studies; Female; Male; Language; Nursing Care; Middle Aged; Adult; Aged; Sociodemographic Factors; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41355748/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12683325",
      "ft_text_length": 40056,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12683325)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "41356360",
      "title": "Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data.",
      "abstract": "Achieving robust performance and fairness across diverse patient populations remains a central challenge in developing clinically deployable deep learning models for diagnostic imaging. Synthetic data generation has emerged as a promising strategy to address current limitations in dataset scale and diversity. In this study, we introduce RoentGen-v2, a state-of-the-art text-to-image diffusion model for chest radiographs that enables fine-grained control over both radiographic findings and patient demographic attributes, including sex, age, and race/ethnicity. RoentGen-v2 is the first model to generate clinically plausible chest radiographs with explicit demographic conditioning, facilitating the creation of a large, demographically balanced synthetic dataset comprising over 565,000 images. We use this large synthetic dataset to evaluate optimal training pipelines for downstream disease classification models. In contrast to prior work that combines real and synthetic data naively, we propose an improved training strategy that leverages synthetic data for supervised pretraining, followed by fine-tuning on real data. Through extensive evaluation on over 137,000 held-out chest radiographs from five institutions, we demonstrate that synthetic pretraining consistently improves model performance, generalization to out-of-distribution settings, and fairness across demographic subgroups defined across varying fairness metrics. Across datasets, synthetic pretraining led to a 6.5% accuracy increase in the performance of downstream classification models, compared to a modest 2.7% increase when naively combining real and synthetic data. We observe this performance improvement simultaneously with the reduction of the underdiagnosis fairness gap by 19.3%, with marked improvements across intersectional subgroups of sex, age, and race/ethnicity. Our proposed data-centric training approach that combines high-fidelity synthetic training data with multi-stage training pipelines is label-efficient, reducing reliance on large quantities of annotated real data. These results highlight the potential of demographically controllable synthetic imaging to advance equitable and generalizable medical deep learning under real-world data constraints. We open source our code, trained models, and synthetic dataset.",
      "journal": "Research square",
      "year": "2025",
      "doi": "10.21203/rs.3.rs-7687810/v1",
      "authors": "Moroianu Stefania L et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41356360/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12676388",
      "ft_text_length": 50863,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12676388)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "41382245",
      "title": "Automating ACMG variant classifications with BIAS-2015 v2.1.1: algorithm analysis and benchmark against the FDA-approved eRepo dataset.",
      "abstract": "BACKGROUND: In 2015, the American College of Medical Genetics and Genomics (ACMG), in collaboration with the Association of Molecular Pathologists (AMP), published guidelines for interpreting and classifying germline genomic variants. These guidelines defined five categories: benign, likely benign, uncertain significance, likely pathogenic, and pathogenic, with 28 criteria but no specific implementation algorithms. METHODS: Here we present Bitscopic Interpreting ACMG Standards 2015 (BIAS-2015 v2.1.1), an open-source software that automates the classification of variants based on 19 ACMG criteria while enabling user-defined weighting and manual adjustments for clinical contexts. BIAS-2015 supports high-throughput classification via command line, along with a web-based graphical user interface (GUI), enabling variant review, modification, and interactive curation. RESULTS: Using genomic data from the FDA-recognized ClinGen Evidence Repository (eRepo v2.2.0), we evaluated BIAS-2015\u2019s sensitivity, specificity, and F1 values with expert curation. BIAS-2015 demonstrated superior performance to InterVar, achieving a pathogenic sensitivity of 73.99% (vs. 64.31%), benign sensitivity of 80.23% (vs. 53.91%), and a 11x speed improvement, classifying 1,327 variants per second. CONCLUSIONS: BIAS-2015 provides an accurate, scalable, and transparent ACMG classification framework. By standardizing ACMG interpretation and providing transparent rule-based logic, BIAS-2015 enables reproducible and comparable variant classification across research and clinical laboratories. All code and the interactive variant curation platform are available on GitHub. https://github.com/bitscopic/BIAS-2015 SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1186/s13073-025-01581-y.",
      "journal": "Genome medicine",
      "year": "2025",
      "doi": "10.1186/s13073-025-01581-y",
      "authors": "Eisenhart Chris et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41382245/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12706976",
      "ft_text_length": 56288,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12706976)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "41385778",
      "title": "Hype vs Reality in the Integration of Artificial Intelligence in Clinical Workflows.",
      "abstract": "Artificial intelligence (AI) has the capacity to transform health care by improving clinical decision-making, optimizing workflows, and enhancing patient outcomes. However, this potential remains limited by a complex set of technological, human, and ethical barriers that constrain its safe and equitable implementation. This paper argues for a holistic, systems-based approach to AI integration that addresses these challenges as interconnected rather than isolated. It identifies key technological barriers, including limited explainability, algorithmic bias, integration and interoperability issues, lack of generalizability, and difficulties in validation. Human factors such as resistance to change, insufficient stakeholder engagement, and education and resource constraints further impede adoption, whereas ethical and legal challenges related to liability, privacy, informed consent, and inequity compound these obstacles. Addressing these issues requires transparent model design, diverse datasets, participatory development, and adaptive governance. Recommendations emerging from this synthesis are as follows: (1) establish standardized international regulatory and governance frameworks; (2) promote multidisciplinary co-design involving clinicians, developers, and patients; (3) invest in clinician education, AI literacy, and continuous training; (4) ensure equitable resource allocation through dedicated funding and public-private partnerships; (5) prioritize multimodal, explainable, and ethically aligned AI development; and (6) focus on long-term evaluation of AI in real-world settings to ensure adaptive, transparent, and inclusive deployment. Adopting these measures can align innovation with accountability, enabling health care systems to harness AI's transformative potential responsibly and sustainably to advance patient care and health equity.",
      "journal": "JMIR formative research",
      "year": "2025",
      "doi": "10.2196/70921",
      "authors": "Abd-Alrazaq Alaa et al.",
      "keywords": "AI; artificial intelligence; challenges; clinical workflow; ethics; health care; human factors; regulation; solutions; technology",
      "mesh_terms": "Artificial Intelligence; Humans; Workflow",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41385778/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12700513",
      "ft_text_length": 50147,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12700513)",
      "ft_reason": "Included: substantial approach content (6 indicators)"
    },
    {
      "pmid": "41393018",
      "title": "Implicit bias in digital health: systematic biases in large language models' representation of global public health attitudes and challenges to health equity.",
      "abstract": "INTRODUCTION: As emerging instruments in digital health, large language models (LLMs) assimilate values and attitudes from human-generated data, thereby possessing the latent capacity to reflect public health perspectives. This study investigates into the representational biases of LLMs through the lens of health equity. We propose and empirically validate a three-dimensional explanatory framework encompassing Data Resources, Opinion Distribution, and Prompt Language, positing that prompts are not just communicative media but critical conduits that embed cultural context. METHODS: Utilizing a selection of prominent LLMs from the United States and China-namely Gemini 2.5 Pro, GPT-5, DeepSeek-V3, and Qwen 3. We conduct a systematic empirical analysis of their performance in representing health attitudes across diverse nations and demographic strata. RESULTS: Our findings demonstrate that: first, the accessibility of data resources is a primary determinant of an LLM's representational fidelity for internet users and nations with high internet penetration. Second, a greater consensus in public health opinion correlates with an increased propensity for the models to replicate the dominant viewpoint. Third, a significant \"native language association\" is observed, wherein Gemini 2.5 Pro and DeepSeek-V3 exhibit superior performance when prompted in their respective native languages. Conversely, models with enhanced multilingual proficiencies, such as GPT-5.0 and Qwen 3, display greater cross-lingual consistency. DISCUSSION: This paper not only quantifies the degree to which these leading LLMs reflect public health attitudes but also furnishes a robust analytical pathway for dissecting the underlying mechanisms of their representational biases. These findings bear profound implications for the advancement of health equity in the artificial intelligence era.",
      "journal": "Frontiers in public health",
      "year": "2025",
      "doi": "10.3389/fpubh.2025.1705082",
      "authors": "Gao Yuan et al.",
      "keywords": "algorithmic audit; digital health; health equity; large language models; representational bias",
      "mesh_terms": "Humans; Health Equity; Public Health; Language; China; Global Health; United States; Large Language Models; Digital Health",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41393018/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12698530",
      "ft_text_length": 63651,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12698530)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "41395651",
      "title": "Literature-informed ensemble machine learning for three-year diabetic kidney disease risk prediction in type 2 diabetes: Development, validation, and deployment of the PSMMC NephraRisk model.",
      "abstract": "INTRODUCTION: Diabetic kidney disease (DKD) and diabetic nephropathy (DN) affect around 40% of diabetic patients but lack accurate risk prediction tools that include social determinants and demographic complexity. We developed and validated an ensemble machine learning model for three-year DKD/DN risk prediction with deployment readiness. METHODS: We analysed 18\u2009742 eligible adult type 2 diabetic patients from Prince Sultan Military Medical City (PSMMC) registry between 2019 and 2024 in Riyadh, Saudi Arabia. Using temporal patient-level splitting, we developed a stacked ensemble model (LightGBM + CoxBoost) with several features including multiple literature-informed imputed variables including family history, non-steroidal anti-inflammatory drug (NSAID) use, socioeconomic deprivation, diabetic retinopathy severity, and antihypertensive medications, imputed via Bayesian multiple imputation by chained equations (MICE) with external study priors. Primary outcome was incident/progressive DKD/DN within 3\u2009years' timeframe. We assessed discrimination, calibration, model utilisation, and algorithmic fairness. RESULTS: The final model achieved excellent discrimination (receiver operating characteristic [AUROC] of 0.852, 95% CI 0.847-0.857) and near-perfect calibration (slope 0.98, intercept -0.012) on multi-trial validation. Decision curve evaluation demonstrated superior net benefit (+22 events prevented per 1000 patients at 10% threshold) compared to treat-all strategies. Bootstrap validation showed minimal optimism in discrimination (C-statistic optimism\u2009=\u20090.005). No algorithmic bias was detected across demographic subgroups (maximum |\u0394-AUROC|\u2009=\u20090.010). Prior sensitivity analysis confirmed validity and significance (AUROC variation \u22640.008). The model was engineered and deployed as an interactive web-based application (https://nephrarisk.streamlit.app/). CONCLUSIONS: Our developed and demonstrated model provided accurate and well-fair DKD/DN risk prediction with excellent calibration, allowing for better decision making with deployment as a web-based research tool and framework for future prospective clinical validation. Further validation and testing are warranted from different centres and healthcare systems to increase confidence and dissemination of our model findings for better utilisation purposes in the future.",
      "journal": "Diabetes, obesity & metabolism",
      "year": "2026",
      "doi": "10.1111/dom.70385",
      "authors": "Tourkmani Ayla M et al.",
      "keywords": "diabetes; diabetic kidney disease; diabetic nephropathy; glycaemic control; renal functions",
      "mesh_terms": "Humans; Diabetic Nephropathies; Diabetes Mellitus, Type 2; Machine Learning; Male; Female; Middle Aged; Risk Assessment; Adult; Risk Factors; Registries; Aged",
      "pub_types": "Journal Article; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41395651/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12890761",
      "ft_text_length": 93156,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12890761)",
      "ft_reason": "Included: substantial approach content (6 indicators)"
    },
    {
      "pmid": "41402410",
      "title": "SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models.",
      "abstract": "Automatic sleep staging with deep learning has advanced considerably, yet clinical adoption remains hindered by limited generalization, model bias, and inconsistent evaluation practices. We present SLEEPYLAND, an open-source framework comprising ~ 220,000\u2009h of in-domain and ~ 84,000\u2009h of out-of-domain polysomnographic recordings, spanning diverse ages, disorders, and hardware configurations. We release pre-trained state-of-the-art models, evaluating them across single- and multi-channel EEG/EOG setups. We introduce SOMNUS, an ensemble that integrates models via soft-voting, achieving robust performance across 24 datasets (macro-F1, 68.7-87.2%), outperforming individual models in 94.9% of cases and exceeding prior state-of-the-art. Exploiting the Bern-Sleep-Wake-Registry (N\u2009=\u20096633), we show that while SOMNUS improves generalization, no model architecture consistently minimizes model demographic/clinical bias. On multi-annotated datasets, SOMNUS surpasses the best human scorer (macro-F1, 85.2% vs 80.8% on DOD-H, and 80.2% vs 75.9% on DOD-O), more closely reproducing consensus. Finally, ensemble disagreement metrics predict scorer ambiguity (ROC-AUC 82.8%), providing reliable proxies for human uncertainty.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-02237-2",
      "authors": "Rossi Alvise Dei et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41402410/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12816009",
      "ft_text_length": 99484,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12816009)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "41406939",
      "title": "Contrastive learning enhances fairness in pathology artificial intelligence systems.",
      "abstract": "AI-enhanced pathology evaluation systems hold significant potential to improve cancer diagnosis but frequently exhibit biases against underrepresented populations due to limited diversity in training data. Here, we present the Fairness-aware Artificial Intelligence Review for Pathology (FAIR-Path), a framework that leverages contrastive learning and weakly supervised machine learning to mitigate bias in AI-based pathology evaluation. In a pan-cancer AI fairness analysis spanning 20 cancer types, we identify significant performance disparities in 29.3% of diagnostic tasks across demographic groups defined by self-reported race, gender, and age. FAIR-Path effectively mitigates 88.5% of these disparities, with external validation showing a 91.1% reduction in performance gaps across 15 independent cohorts. We find that variations in somatic mutation prevalence among populations contribute to these performance disparities. FAIR-Path represents a promising step toward addressing fairness challenges in AI-powered pathology diagnoses and provides a robust framework for mitigating bias in medical AI applications.",
      "journal": "Cell reports. Medicine",
      "year": "2025",
      "doi": "10.1016/j.xcrm.2025.102527",
      "authors": "Lin Shih-Yen et al.",
      "keywords": "AI; algorithmic bias; artificial intelligence; bias mitigation; cancer diagnosis; contrastive learning; deep learning; fairness; pathology; weakly supervised learning",
      "mesh_terms": "Humans; Artificial Intelligence; Neoplasms; Male; Female; Pathology; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41406939/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12765949",
      "ft_text_length": 73760,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12765949)",
      "ft_reason": "Included: bias central + approach content (12 indicators)"
    },
    {
      "pmid": "41414851",
      "title": "Clinical validation of AI-assisted contouring in prostate radiation therapy treatment planning: Highlighting automation bias and the need for standardized quality assurance.",
      "abstract": "PURPOSE: This study evaluated the impact of a commercial AI-assisted contouring tool on intra- and inter-observer variability in prostate radiation therapy and assessed the dosimetric consequences of geometric contour differences. METHODS: Two experienced radiation oncologists independently delineated clinical target volume (CTV) and organs at risk (OARs) for prostate cancer patients. Manual contours (Cman) and AI-generated contours (CAI) were compared with adjusted AI contours (CAI,adj). A consensus reference (Cref) served as the benchmark. To evaluate clinical impact, treatment plans were recalculated and replanned on each contour set under identical beam geometries to assess dose-volume histogram (DVH) parameters. RESULTS: AI-assisted contouring significantly improved both intra- and inter-observer agreement. Inter-observer analysis revealed that the Dice similarity coefficient (DSCs) for CTV increased from 0.78 (\u00b1\u00a00.11) for Cman to 0.89 (\u00b1\u00a00.09) for CAI, adj. Similarly, intra-observer analysis revealed that both oncologists showed significantly higher DSCs for CAI, adj compared to Cman. A thorough geometric comparison to the Cref revealed that while adjustments to CAI improved accuracy, they generally did not surpass Cman for CTV and rectum. Dosimetric analyses demonstrated that, under fixed plan geometry, both Cman and CAI,adj contours yielded lower planning target volume (PTV) D95% values compared with Cref, whereas after replanning, all plans met institutional criteria with no clinically significant differences among contour sets. CONCLUSION: AI-assisted contouring in prostate radiotherapy reduced intra- and inter-observer variability and improved contouring consistency. However, CAI, adj did not consistently surpass Cman, especially for the CTV and rectum, where automation bias or selective clinical acceptance may have influenced edits. Fixed-plan recalculations revealed dose differences from minor geometric deviations. These findings underscore the importance of structured quality assurance (QA) and human oversight to mitigate automation bias while leveraging AI's efficiency. The single-institution design with two oncologists and one AI software limits generalizability, underscoring the need for multi-observer validation.",
      "journal": "Journal of applied clinical medical physics",
      "year": "2026",
      "doi": "10.1002/acm2.70425",
      "authors": "Arjmandi Najmeh et al.",
      "keywords": "Automation bias; Auto\u2010contouring; Deep learning; Inter\u2010observer variability; Intra\u2010observer variability; Quality assurance; Radiotherapy",
      "mesh_terms": "Humans; Radiotherapy Planning, Computer-Assisted; Prostatic Neoplasms; Male; Radiotherapy Dosage; Organs at Risk; Quality Assurance, Health Care; Radiotherapy, Intensity-Modulated; Artificial Intelligence; Automation",
      "pub_types": "Journal Article; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41414851/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12715409",
      "ft_text_length": 51132,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12715409)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "41419484",
      "title": "Connecting algorithmic fairness and fair outcomes in a sociotechnical simulation case study of AI-assisted healthcare.",
      "abstract": "Artificial intelligence (AI) has vast potential for improving healthcare delivery, but concerns regarding biases in these systems have raised important questions regarding fairness when deployed clinically. Most prior studies on fairness in clinical AI focus solely on performance disparities between subpopulations, which often fall short of connecting the technical outputs of AI systems with sociotechnical outcomes. In this work, we present a simulation-based approach to explore how statistical definitions of algorithmic fairness translate to fairness in long-term outcomes, using AI-assisted breast cancer screening as a case example. We evaluate four fairness criteria and their impact on mortality rates and socioeconomic disparities, while also considering how clinical decision makers' reliance on AI and patients' access to healthcare affect outcomes. Our results highlight how algorithmic fairness does not directly translate into fair and equitable outcomes, underscoring the importance of integrating sociotechnical perspectives to gain a holistic understanding of fairness in healthcare AI.",
      "journal": "Nature communications",
      "year": "2025",
      "doi": "10.1038/s41467-025-67470-5",
      "authors": "Stanley Emma A M et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Breast Neoplasms; Algorithms; Female; Delivery of Health Care; Computer Simulation; Early Detection of Cancer",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41419484/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12824401",
      "ft_text_length": 75044,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12824401)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "41425941",
      "title": "Generative AI in microbial evolution and resistance: toward robust, explainable, and equitable predictions.",
      "abstract": "Antimicrobial resistance (AMR) is one of the most urgent challenges in modern microbiology, both an evolutionary inevitability and a global health crisis shaped by clinical practices, ecological disruption, and social inequities. Generative artificial intelligence (AI) and large language models (LLMs) present new opportunities to anticipate resistance pathways, design novel antimicrobial agents, and guide interventions that are informed by evolutionary dynamics. Their successful integration, however, depends on addressing three fundamental imperatives. The first is evolutionary robustness, requiring models that incorporate mutation, horizontal gene transfer, and adaptive landscapes to move beyond retrospective classification toward predictive evolutionary inference. The second is explainability and biosafety, which demand interpretable and biologically credible outputs that clinicians, microbiologists, and policymakers can trust, while safeguarding against dual use risks. The third is data equity, which calls for strategies that mitigate structural biases in global microbial datasets and ensure that predictive systems serve the populations most affected by AMR. This Perspective advances the view that generative AI must be conceived as a transformative epistemic infrastructure that is evolution aware, transparent, and globally inclusive, capable of supporting sustainable drug discovery, adaptive surveillance, and equitable microbiological futures.",
      "journal": "Frontiers in microbiology",
      "year": "2025",
      "doi": "10.3389/fmicb.2025.1705320",
      "authors": "Sufi Fahim",
      "keywords": "GPT in medical domain; antimicrobial resistance; drug discovery; explainability and biosafety; explainable AI; generative artificial intelligence; microbial evolution",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41425941/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12714630",
      "ft_text_length": 16425,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12714630)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "41454038",
      "title": "Clinically informed semi-supervised learning improves disease annotation and equity from electronic health records: a glaucoma case study.",
      "abstract": "Clinical notes represent a vast but underutilized source of information for disease characterization, whereas structured electronic health record (EHR) data such as ICD codes are often noisy, incomplete, and too coarse to capture clinical complexity. These limitations constrain the accuracy of datasets used to investigate disease pathogenesis and progression and to develop robust artificial intelligence (AI) systems. To address this challenge, we introduce Ci-SSGAN (Clinically Informed Semi-Supervised Generative Adversarial Network), a novel framework that leverages large-scale unlabeled clinical text to reannotate patient conditions with improved accuracy and equity. As a case study, we applied Ci-SSGAN to glaucoma, a leading cause of irreversible blindness characterized by pronounced racial and ethnic disparities. Trained on a demographically balanced subset of 349587 unlabeled ophthalmology notes and 2954 expert-annotated notes (drawn from an institutional corpus of 2.1 million notes), Ci-SSGAN achieved 0.85 accuracy and 0.95 AUROC, representing a 10.19% AUROC improvement compared to ICD-based labels (0.74 accuracy, 0.85 AUROC). Ci-SSGAN also narrowed subgroup performance gaps, with F1 gains for Black patients (+\u20090.05), women (+\u20090.06), and younger patients (+\u20090.033). By integrating semi-supervised learning and demographic conditioning, Ci-SSGAN minimizes reliance on expert annotations, making AI development more accessible to resource-constrained healthcare systems.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-02267-w",
      "authors": "Moradi Mousa et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41454038/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12847826",
      "ft_text_length": 88723,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12847826)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "41458068",
      "title": "Conformal uncertainty quantification to evaluate predictive fairness of foundation AI model for skin lesion classes across patient demographics.",
      "abstract": "Deep learning based diagnostic AI systems based on medical images are starting to provide similar performance as human experts. However, these data-hungry complex systems are inherently black boxes and therefore slow to be adopted for high-risk applications like healthcare. This problem of lack of transparency is exacerbated in the case of recent large foundation models, which are trained in a self-supervised manner on millions of data points to provide robust generalisation across a range of downstream tasks. The embeddings generated from them happen through a process that is not interpretable, and hence not easily trustable for clinical applications. To address this timely issue, we deploy conformal analysis to quantify the predictive uncertainty of a vision transformer (ViT)-based foundation model across patient demographics with respect to sex, age, and ethnicity for the task of skin lesion classification using several public benchmark datasets. The significant advantage of this method is that conformal analysis is method independent, and it not only provides a coverage guarantee at the population level but also provides an uncertainty score for each individual. This is used to demonstrate the effectiveness of utilizing these embeddings for specialized tasks like diagnostic classification, meanwhile reducing computational costs. Secondly, the public benchmark datasets we used had severe class imbalance in terms of the number of samples in different classes. We used a model-agnostic dynamic F1-score-based sampling during model training, which helped to stabilize the class imbalance. We investigate the effects on uncertainty quantification (UQ) with or without this bias mitigation step. Thus, our results show how this can be used as a fairness metric to evaluate the robustness of the feature embeddings of the foundation model (Google DermFoundation), advancing the trustworthiness and fairness of clinical AI.",
      "journal": "Health information science and systems",
      "year": "2026",
      "doi": "10.1007/s13755-025-00412-z",
      "authors": "Bhattacharyya Swarnava et al.",
      "keywords": "Algorithmic fairness; Class imbalance; Conformal prediction; Foundation models; Skin lesion classification; Transparent trustworthy AI; Uncertainty quantification; Vision transformer (ViT)",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41458068/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12738507",
      "ft_text_length": 47288,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12738507)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "41459572",
      "title": "Machine learning-based mortality prediction in critically ill patients with hypertension: comparative analysis, fairness, and interpretability.",
      "abstract": "BACKGROUND: Hypertension is a leading global health concern, significantly contributing to cardiovascular, cerebrovascular, and renal diseases. In critically ill patients, hypertension poses increased risks of complications and mortality. Early and accurate mortality prediction in this population is essential for timely intervention and improved outcomes. Machine learning (ML) and deep learning (DL) approaches offer promising solutions by leveraging high-dimensional electronic health record (EHR) data. OBJECTIVE: To develop and evaluate ML and DL models for predicting in-hospital mortality in hypertensive patients using the MIMIC-IV critical care dataset, and to assess the fairness and interpretability of the models. METHODS: We developed four ML models-gradient boosting machine (GBM), logistic regression, support vector machine (SVM), and random forest-and two DL models-multilayer perceptron (MLP) and long short-term memory (LSTM). A comprehensive set of features, including demographics, lab values, vital signs, comorbidities, and ICU-specific variables, were extracted or engineered. Models were trained using 5-fold cross-validation and evaluated on a separate test set. Feature importance was analyzed using SHapley Additive exPlanations (SHAP) values, and fairness was assessed using demographic parity difference (DPD) and equalized odds difference (EOD), with and without the application of debiasing techniques. RESULTS: The GBM model outperformed all other models, with an AUC-ROC score of 96.3%, accuracy of 89.4%, sensitivity of 87.8%, specificity of 90.7%, and F1 score of 89.2%. Key features contributing to mortality prediction included Glasgow Coma Scale (GCS) scores, Braden Scale scores, blood urea nitrogen, age, red cell distribution width (RDW), bicarbonate, and lactate levels. Fairness analysis revealed that models trained on the top 30 most important features demonstrated lower DPD and EOD, suggesting reduced bias. Debiasing methods improved fairness in models trained with all features but had limited effects on models using the top 30 features. CONCLUSION: ML models show strong potential for mortality prediction in critically ill hypertensive patients. Feature selection not only enhances interpretability and reduces computational complexity but may also contribute to improved model fairness. These findings support the integration of interpretable and equitable AI tools in critical care settings to assist with clinical decision-making.",
      "journal": "Frontiers in artificial intelligence",
      "year": "2025",
      "doi": "10.3389/frai.2025.1686378",
      "authors": "Zhang Shenghan et al.",
      "keywords": "SHAP; deep learning; fairness in AI; hypertension; intensive care unit; machine learning; mortality prediction",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41459572/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12738824",
      "ft_text_length": 52609,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12738824)",
      "ft_reason": "Included: bias central + approach content (19 indicators)"
    },
    {
      "pmid": "41460829",
      "title": "When noise mitigates bias in human-algorithm decision-making: An agent-based model.",
      "abstract": "Algorithmic systems increasingly inform human decision-making in domains such as criminal justice, healthcare, and finance. Although algorithms can exhibit bias, they are much less prone to undesirable variability in judgments (noise) than human decision-makers. While presented as an advantageous feature of algorithmic advice, we actually know little about how (biased) algorithmic advice interacts with noisy human judgment. Does undesirable variability in human judgment decrease under noiseless algorithmic advice? Is bias in human judgment exacerbated or mitigated by noise in advice? To answer these questions, we built an agent-based model that simulates the judgment of decision-makers receiving guidance from a (more or less) biased algorithm or a (more or less) biased and noisy human advisor. The model simulations show that, contrary to expectations, noise can be desirable: human noise can mitigate the harms of algorithmic bias by dampening the influence of algorithmic advice. Noise in human advice leads decision-makers to rely more heavily on their prior beliefs, an emergent behavior with implications for belief updating. When decision-makers' prior beliefs are polarized, an asymmetry occurs: decision-makers respond only to interventionist advice and not to non-interventionist cues. Finally, the model simulations show that population-level variability in decision-making stems from occasion noise in the environment and not from noise in human advice. This result challenges the common wisdom that population-level noise can be straightforwardly decomposed into individual-level sources and questions the feasibility of noise audits in organizations. Together, these findings demonstrate that the absence of noise as a feature of algorithmic advice is not generally desirable, suggesting critical implications for how human-algorithm systems are designed, regulated, and evaluated.",
      "journal": "PloS one",
      "year": "2025",
      "doi": "10.1371/journal.pone.0339273",
      "authors": "Poodiack Parsons Spencer et al.",
      "keywords": "",
      "mesh_terms": "Humans; Algorithms; Decision Making; Bias; Judgment; Computer Simulation; Models, Theoretical",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41460829/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12747352",
      "ft_text_length": 62881,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12747352)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "41482490",
      "title": "A bias field correction workflow based on generative adversarial network for abdominal cancers treated with 0.35T MR-LINAC.",
      "abstract": "PURPOSE: In this study, a bias field correction workflow was proposed to improve the flexibility and generalizability of the generative adversarial network (GAN) model for abdominal cancer patients treated with a 0.35T magnetic resonance imaging linear accelerator (MR-LINAC) system. METHODS: Model training was performed using brain MR images acquired on a 3T diagnostic scanner, while model testing was performed using abdominal MR images obtained using a 0.35T MR-LINAC system. The performance of the proposed workflow was first compared with the GAN model using root-mean-square error (RMSE), peak signal-to-noise ratio (PSNR), and structural similarity index measure (SSIM). To assess the impact of the workflow on image segmentation, it was also compared with the N4ITK algorithm. Segmentation was performed using the k-means clustering algorithm with three clusters corresponding to air, fat, and soft tissue. Segmentation accuracy was then evaluated using the Dice similarity coefficient (DSC). RESULTS: The RMSE values were 30.59, 12.06, 10.37 for the bias field-corrupted images (IIN), GAN-corrected images (IGAN), and images corrected with the proposed workflow (IOUT), respectively. Corresponding PSNR values were 42.34, 46.04, 47.04 dB, and SSIM values were 0.84, 0.96, 0.98. For segmentation accuracy, the mean DSC for air masks was 0.95, 0.97, and 0.97; for fat masks, 0.61, 0.71, and 0.74; and for soft tissue masks, 0.60, 0.68, and 0.69, corresponding to IIN, N4ITK-corrected images (IN4ITK), and IOUT, respectively CONCLUSION: By effectively mitigating bias field artifacts, the proposed workflow has the potential to strengthen the clinical utility of MRI-guided adaptive radiotherapy for abdominal cancers, ensuring safer and more accurate radiation delivery.",
      "journal": "Journal of applied clinical medical physics",
      "year": "2026",
      "doi": "10.1002/acm2.70448",
      "authors": "Yang Ching-Ching et al.",
      "keywords": "0.35T MR\u2010LINAC; bias field artifacts; generative adversarial network",
      "mesh_terms": "Humans; Abdominal Neoplasms; Workflow; Magnetic Resonance Imaging; Algorithms; Image Processing, Computer-Assisted; Radiotherapy Planning, Computer-Assisted; Particle Accelerators; Radiotherapy Dosage; Radiotherapy, Intensity-Modulated; Neural Networks, Computer; Generative Adversarial Networks",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41482490/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12758996",
      "ft_text_length": 25894,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12758996)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "41487722",
      "title": "Artificial Intelligence-Driven Risk Stratification in Chronic Kidney Disease Progression: Minimizing Bias via Race-Specific Algorithms.",
      "abstract": "Background Chronic kidney disease (CKD) is a prevalent condition that affects a substantial portion of the adult population and progresses unevenly across different demographic groups. Recent updates to estimated glomerular filtration rate (eGFR) estimation have removed race adjustments to promote greater equity. Yet, the impact of such changes on model performance and fairness across populations remains uncertain. Objective To ascertain whether, in comparison to a traditional pooled (or \"race-blind\") model, a race-specific, modular deep-learning architecture can enhance clinical utility and fairness in a five-year CKD-progression prediction. Methods We retrospectively pooled ~30,000 patients with stage 1-4 CKD from databases such as the National Health and Nutrition Examination Survey (NHANES), UK Biobank, and Chronic Renal Insufficiency Cohort\u00a0Study (CRIC), and two U.S. health-system electronic health records (EHRs). The endpoint was \u226540% sustained eGFR decline, \u22655 ml/min/1.73 m\u00b2/year drop, or kidney-failure event within five years. Two fully connected neural-network strategies were trained: (i) a pooled model on all races without race as an input; (ii) a modular model comprising separate subnetworks for Black and White patients, sharing architecture but trained on race-specific data. Performance was evaluated by discrimination (area under the curve or AUC), calibration, decision-curve net benefit, and fairness metrics (predictive parity, equalized odds, statistical parity). Results Overall AUCs were comparable (pooled 0.79, modular 0.80). The pooled model systematically underestimated risk in Black patients (calibration-in-the-large -3.8 percentage points (pp)) and yielded unequal positive predictive value (PPV 67.5% Black vs 58.6% White patients). The modular model virtually eliminated calibration bias (intercept \u22640.5 pp) and aligned PPV across races (~64% each) while preserving discrimination. Decision-curve analysis showed a small but consistent net-benefit gain for the modular approach at clinically relevant thresholds (10-35% risk). Trade-offs remained in equalized-odds: the modular model showed higher sensitivity for Black patients (510/840, 60.7%) than for White patients (294/900, 32.7%), though at the cost of a larger false-positive-rate disparity (365/2,160, 16.9% vs 144/3,600, 4.0%). Overall, CKD progression occurred in 1,820/7,500 (24%) patients - 840/3,000 (28%) Black and 900/4,500 (20%) White patients. Conclusions Ongoing monitoring and stakeholder-guided threshold setting are crucial to balance competing fairness criteria. Race-specific modular artificial intelligence (AI) models offer a practical route toward fairer, precision risk stratification by correcting miscalibration and PPV inequities inherent in pooled, race-blind CKD risk tools without sacrificing accuracy.",
      "journal": "Cureus",
      "year": "2025",
      "doi": "10.7759/cureus.98319",
      "authors": "Behmard Nima et al.",
      "keywords": "algorithmic fairness; artificial intelligence; calibration; chronic kidney disease; decision curve analysis; health equity; predictive parity; race-specific modeling",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41487722/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12757500",
      "ft_text_length": 76241,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12757500)",
      "ft_reason": "Included: bias central + approach content (22 indicators)"
    },
    {
      "pmid": "41497309",
      "title": "Mitigating Bias in Opportunistic Screening for MACE with Causal Reasoning.",
      "abstract": "Mitigating population drift is vital for developing robust AI models for clinical use. While current methodologies focus on reducing demographic bias in disease predictions, they overlook the significant impact of chronic comorbidities. Addressing these complexities is essential to enhance predictive accuracy and reliability across diverse patient demographics, ultimately improving healthcare outcomes. We propose a causal reasoning framework to address selection bias in opportunistic screening for 1-year composite MACE risk using chest X-ray images. Training in high-risk primarily Caucasian patients (43% MACE event), the model was evaluated in a lower-risk emergency department setting (12.8% MACE event) and a relatively lower-risk external Asian patient population (23.81% MACE event) to assess selection bias effects. We benchmarked our approach against a high-performance disease classification model, a propensity score matching strategy, and a debiasing model for unknown biases. The causal+confounder framework achieved an AUC of 0.75 and 0.7 on Shift data and Shift external, outperforming baselines, and a comparable AUC of 0.7 on internal data despite penalties for confounders. It minimized disparities in confounding factors and surpassed traditional and state-of-the-art debiasing methods. Experimental data show that integrating causal reasoning and confounder adjustments in AI models enhances their effectiveness. This approach shows promise for creating fair and robust clinical decision support systems that account for population shifts, ultimately improving the reliability and ethical integrity of AI-driven clinical decision-making.",
      "journal": "IEEE transactions on artificial intelligence",
      "year": "2025",
      "doi": "10.1109/tai.2025.3567961",
      "authors": "Pi Jialu et al.",
      "keywords": "Major adverse cardiovascular events; medical imaging; population shift; predictive models",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41497309/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12768338",
      "ft_text_length": 1662,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12768338)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "41502566",
      "title": "Considerations for evaluating the practical utility of machine learning in suicide risk estimation: the role of cost and equity.",
      "abstract": "A key vulnerability in modeling suicide death is a lack of precision and therefore estimates are thought as ultimately unhelpful to clinicians, even with more advanced or nuanced machine learning (ML) techniques. We sought to fill several conceptual gaps by assessing performance, focusing on the precision-recall tradeoff, across multiple techniques, and with ad hoc contextualization for sensitivity, cost-balance, and fairness. To identify robust, differential performances of a cross section of ML techniques on a suicide risk task, emphasizing overall AUPRC maximization and downstream effects on hypothetical decision support. A retrospective cohort was selected for patients receiving care or having died per the Office of the Medical Examiner (OCME), between 2017 and 2020 using the Maryland Suicide Datawarehouse (MSDW). AUPRC-optimized settings yielded cross-validated AUPRC significantly improved over logistic regressions, especially for XGBoost in both hospital discharge (AUPRC: 0.667; PPV: 0.941) and commercial claims records (AUPRC: 0.558; PPV: 0.857). F-Beta statistics revealed that when precision is preferred (e.g., 99.9 percentile), XGBoost are among the most efficient tools, while random forest and MLP are better when sensitivity is preferred (90 percentile or lower). No algorithmic bias was identified by age, sex or race, but significant changes in performance are noted with certain clinical characteristics. To our knowledge, this is the first use of an AUPRC-maxima optimization for ML tools with predicting suicide death. The utility of suicide risk models in clinical decision support is discussed as being tied to innate class imbalance challenges in model training, with recommendations being provided on how to better evaluate performance.",
      "journal": "Research square",
      "year": "2025",
      "doi": "10.21203/rs.3.rs-8216032/v1",
      "authors": "Kitchen Christopher et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41502566/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12772703",
      "ft_text_length": 28085,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12772703)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "41513661",
      "title": "Shipped and shifted: modeling collection-induced bias in microbiome multi-omics using a tractable fermentation system.",
      "abstract": "Large-scale, decentralized microbiome sampling surveys and citizen science initiatives often require periods of storage at ambient temperature, potentially altering sample composition during collection and transport. We developed a generalizable framework to quantify and model these biases using sourdough as a tractable fermentation system, with samples subjected to controlled storage conditions (4\u2009\u00b0C, 17\u2009\u00b0C, 30\u2009\u00b0C, regularly sampled up to 28 days). Machine-learning models paired with multi-omics profiling-including microbiome, targeted and untargeted metabolome profiling, and cultivation-revealed temperature-dependent shifts in bacterial community structure and metabolic profiles, while fungal communities remained stable. Storage induced ecological restructuring, marked by reduced network modularity and increased centrality of dominant taxa at higher temperatures. Notably, storage duration and temperature were strongly encoded in the multi-omics data, with temperature exerting a more pronounced influence than time. 24 of the top 25 predictors of storage condition were metabolites, underscoring functional layers as both sensitive to and informative of environmental exposure. These findings demonstrate that even short-term ambient storage (<2 days) can substantially reshape microbiome, metabolome, and biochemical profiles, posing risks to data comparability in decentralized studies and emphasizing the need to recognize and address such biases. Critically, the high predictability of storage history offers a path toward bias detection and correction- particularly when standardized collection protocols are infeasible, as is common in decentralized sampling contexts. Our approach enables robust quantification and modeling of such storage effects across multi-omics datasets, unlocking more accurate interpretation of large-scale microbiome surveys.",
      "journal": "NPJ biofilms and microbiomes",
      "year": "2026",
      "doi": "10.1038/s41522-025-00909-1",
      "authors": "Meyer Annina R et al.",
      "keywords": "",
      "mesh_terms": "Fermentation; Microbiota; Bacteria; Temperature; Fungi; Metabolome; Metabolomics; Machine Learning; Specimen Handling; Bread; Multiomics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41513661/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12901149",
      "ft_text_length": 74131,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12901149)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41522832",
      "title": "Evaluation and improvement of algorithmic fairness for COVID-19 severity classification using Explainable Artificial Intelligence-based bias mitigation.",
      "abstract": "OBJECTIVES: The COVID-19 pandemic has highlighted the growing reliance on machine learning (ML) models for predicting disease severity, which is important for clinical decision-making and equitable resource allocation. While achieving high predictive accuracy is important, ensuring fairness in the prediction output of these models is equally important to prevent bias-driven disparities in healthcare. This study evaluates fairness in a machine learning-based COVID-19 severity classification model and proposes an Explainable AI (XAI)-based bias mitigation strategy to address sex-related bias. MATERIALS AND METHODS: Using data from the Quebec Biobank, we developed an XGBoost-based multi-class classification model. Fairness was assessed using Subset Accuracy Parity Difference (SAPD) and Label-wise Equal Opportunity Difference (LEOD) metrics. Four bias mitigation strategies were implemented and evaluated: Fair Representation Learning, Fair Classifier Using Constraints, Adversarial Debiasing, and our proposed XAI-based method utilizing SHapley Additive exPlanations (SHAP) method for feature importance analysis. RESULTS: The study cohort included 1642 COVID-19 positive older adults (mean age: 77.5), balance equally between males and females. The baseline (unmitigated) classification model achieved 90.68% accuracy but exhibited a 10.11% Subset Accuracy Parity Difference between sexes, indicating a relatively large bias. The introduced XAI-based method demonstrated a better trade-off between model performance and fairness compared to existing bias mitigation methods by identifying sex-sensitive feature interactions and integrating them into the model re-training. DISCUSSION: Traditional fairness interventions often compromise accuracy to a greater extent. Our XAI-based method achieves the best balance between classification performance and bias, enhancing its clinical applicability. CONCLUSION: The XAI-driven bias mitigation intervention effectively reduces sex-based disparities in COVID-19 severity prediction without the significant accuracy loss observed in traditional methods. This approach provides a framework for developing fair and accurate clinical decision support systems for older adults, which ensures equitable care in clinical risk stratification and resource allocation.",
      "journal": "JAMIA open",
      "year": "2026",
      "doi": "10.1093/jamiaopen/ooaf171",
      "authors": "Nejadshamsi Shayan et al.",
      "keywords": "COVID-19 severity; bias mitigation; explainable AI; fairness; machine learning classifier",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41522832/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12790453",
      "ft_text_length": 48528,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12790453)",
      "ft_reason": "Included: bias central + approach content (18 indicators)"
    },
    {
      "pmid": "41523301",
      "title": "The role of artificial intelligence in promoting kindness in society: A qualitative study to advance well-being, equity, and positive social change.",
      "abstract": "Background and Objective: Artificial Intelligence (AI) has the potential to influence social behavior and public well-being. This study investigates AI's role in promoting kindness and identifies ethical strategies to foster empathy, support, and compassionate communities. Materials and Methods: This qualitative content analysis involved purposive sampling of 30 participants, including AI experts, social scientists, religious leaders, developers, and users. Semi-structured interviews were conducted until theoretical saturation was reached. Data were analyzed using Granheim and Lundman's method. Trustworthiness was ensured using Lincoln and Guba's criteria: credibility (via member checking), transferability (detailed contextual descriptions), dependability (audit trail), and confirmability (researcher reflexivity and documentation). Results: Five key themes emerged: emotional well-being, social justice, behavioral change, ethical responsibility, and education. AI fosters prosocial behavior, enhances social inclusion, and promotes sustainable relationships. Conclusion: When ethically and thoughtfully applied, AI can support and complement, but not replace, human relationships, particularly in promoting kindness and prosocial behaviors. This approach can strengthen social cohesion, enhance public health, and improve community well-being. Explicitly acknowledging AI's supportive role reinforces the manuscript's ethical positioning and conceptual clarity.",
      "journal": "Health psychology open",
      "year": "2026",
      "doi": "10.1177/20551029251413309",
      "authors": "Ebrahimian Ali et al.",
      "keywords": "artificial intelligence; equity; kindness; public health; social cohesion",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41523301/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12783588",
      "ft_text_length": 46411,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12783588)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41531745",
      "title": "Fairness-aware K-means clustering in digital mental health for higher education students: a generalizable framework for equitable clustering.",
      "abstract": "OBJECTIVES: Higher education students, particularly those from underrepresented backgrounds, experience heightened levels of anxiety, depression, and burnout. Clinical informatics approaches leveraging K-means clustering can aid in mental health risk stratification, yet they often exacerbate disparities. We present a socially fair clustering framework that ensures equitable clustering costs across demographic groups while minimizing within-cluster variability. MATERIALS AND METHODS: Our framework compares standard and socially fair K-means clustering to assess the impact of demographic disparities. It identifies factors affecting clustering across demographics using omnibus and post hoc statistical tests. Subsequently, it quantifies the influence of statistically significant factors on cluster development. We illustrate our approach by identifying racially equitable clusters of mental health among students surveyed by the Healthy Minds Network. RESULTS: The socially fair clustering approach reduces disparities in clustering costs by as much as 30% across racial groups while maintaining consistency with standard K-means solutions in socioeconomically homogenous populations. Discrimination experiences were the strongest indicator of poorer mental health, whereas stable financial conditions and robust social engagement promoted resilience. DISCUSSION: Integrating fairness constraints into clustering algorithms reduces disparities in risk stratification and provides insights into socioeconomic drivers of student well-being. Our findings suggest that standard models may overpathologize middle-risk cohorts, whereas fairness-aware clustering yields partitions that better capture disparities. CONCLUSION: Our work demonstrates how integrating fairness-aware objectives into clustering algorithms can enhance equity in partitioning systems. The framework we present is broadly applicable to clustering problems across various biomedical informatics domains.",
      "journal": "JAMIA open",
      "year": "2026",
      "doi": "10.1093/jamiaopen/ooaf174",
      "authors": "Alluri Priyanshu et al.",
      "keywords": "cluster analysis; machine learning; medical informatics; mental health; risk assessment",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41531745/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12794019",
      "ft_text_length": 32648,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12794019)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "41536573",
      "title": "Understanding and Addressing Bias in Artificial Intelligence Systems: A Primer for the Emergency Medicine Physician.",
      "abstract": "Artificial intelligence (AI) tools and technologies are increasingly being integrated into emergency medicine (EM) practice, not only offering potential benefits such as improved efficiency, better patient experience, and increased safety, but also resulting in potential risks including exacerbation of biases. These biases, inadvertently embedded in AI algorithms or training data, can adversely affect clinical decision making for diverse patient populations. Bias is a universal human attribute, subject to introduction into any human interaction. The risk with AI is magnification of, or even normalization of, patterns of biases across the health care ecosystem within tools that in time may be considered authoritative. This article, the work of members of the American College of Emergency Physicians (ACEP) AI Task Force, aims to equip emergency physicians (EPs) with a practical framework for understanding, identifying, and addressing bias in clinical and operational AI tools encountered in the emergency department (ED). For this publication, we have defined bias as a systematic flaw in a decision-making process that results in unfair or unintended outcomes that can be inadvertently embedded in AI algorithms or training data. This can result in adverse effects on clinical decision making for diverse patient populations. We begin by reviewing common sources of AI bias relevant to EM, including data, algorithmic, measurement, and human-interaction factors, and then, we discuss the potential pitfalls. Following this, we use illustrative examples from EM practice (eg, triage tools, risk stratification, and medical devices) to demonstrate how bias can manifest. We subsequently discuss the evolving regulatory landscape, structured assessment frameworks (including predeployment, continuous monitoring, and postdeployment steps), key principles (like sociotechnical perspectives and stakeholder engagement), and specific tools. Finally, this review outlines the EP's vital role in mitigation of AI-related biases through advocacy, local validation, clinical feedback, demanding transparency, and maintaining clinical judgment over automation.",
      "journal": "Journal of the American College of Emergency Physicians open",
      "year": "2026",
      "doi": "10.1016/j.acepjo.2025.100311",
      "authors": "Abbott Ethan E et al.",
      "keywords": "artificial intelligence; bias; emergency medicine",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41536573/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12797052",
      "ft_text_length": 31244,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12797052)",
      "ft_reason": "Included: bias central + approach content (12 indicators)"
    },
    {
      "pmid": "41540090",
      "title": "DermNet: integrative CNN-ViT architecture for bias mitigation in dermatological diagnostics using advanced unsupervised lesion segmentation.",
      "abstract": "In this paper, we propose a method for reducing the bias in skin disease identification for people of color with the aid of lesion only zero shot unsupervised approach that is then passed to the classifier Dermnet comprising of a hybrid Vision Transformer and Convolutional Neural Network, achieving robust validation accuracy of approximately 81%. Our Segmentation without training with labeled data as is the case with traditional U-Net has achieved an IOU of 90% across all skin colors in segmenting the lesion from skin effectively eradicating the impact of skin in the classification of disease.",
      "journal": "Scientific reports",
      "year": "2026",
      "doi": "10.1038/s41598-026-35697-x",
      "authors": "Imran Muhammad Huzaifa et al.",
      "keywords": "",
      "mesh_terms": "Humans; Neural Networks, Computer; Skin Diseases; Algorithms; Skin; Image Processing, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41540090/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12880972",
      "ft_text_length": 59322,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12880972)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "41551000",
      "title": "A predictive model for cognitive decline using social determinants of health.",
      "abstract": "BACKGROUND: Early diagnosis of Alzheimer's disease and related dementias (AD/ADRD) is critical but often constrained by limited access to fluid and imaging biomarkers, particularly in low-resource settings. OBJECTIVE: To develop and evaluate a predictive model for cognitive decline using survey-based data, with attention to model interpretability and fairness. METHODS: Using data from the Mexican Health and Aging Study (MHAS), a nationally representative longitudinal survey of adults aged 50 and older (N = 4095), we developed a machine learning model to predict future cognitive scores. The model was trained on survey data from 2003 to 2012, encompassing demographic, lifestyle, and social determinants of health (SDoH) variables. A stacked ensemble approach combined five base models-Random Forest, LightGBM, XGBoost, Lasso, and K-Nearest Neighbors-with a Ridge regression meta-model. RESULTS: The model achieved a root-mean-square error (RMSE) of 39.25 (95 % CI: 38.12-40.52), representing 10.2 % of the cognitive score range, on a 20 % held-out test set. Features influencing predictions, included education level, age, reading behavior, floor material, mother's education level, social activity frequency, the interaction between the number of living children and age, and overall engagement in activities. Fairness analyses revealed model biases in underrepresented subgroups within the dataset, such as individuals with 7-9 years of education. DISCUSSION: These findings highlight the potential of using accessible, low-cost SDoH survey data for predicting risk of cognitive decline in aging populations. They also underscore the importance of incorporating fairness metrics into predictive modeling pipelines to ensure equitable performance across diverse groups.",
      "journal": "JAR life",
      "year": "2026",
      "doi": "10.1016/j.jarlif.2025.100056",
      "authors": "He Yingnan et al.",
      "keywords": "Aging; Bias analysis; Cognitive decline; Health disparities, predictive modeling; Interpretable models; Machine learning; Mexican health and aging study (MHAS); Social determinants of health (SDoH); Stacked model",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41551000/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12809124",
      "ft_text_length": 30956,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12809124)",
      "ft_reason": "Included: substantial approach content (6 indicators)"
    },
    {
      "pmid": "41558734",
      "title": "Bridging Global Disparities in Drug Allergy Through AI-Assisted Training for Non-Specialists: Findings From the Multinational ADAPT-2 Course.",
      "abstract": "BACKGROUND: Mislabelled drug allergy (DA) remains a global public health challenge. A prior randomised trial (ADAPT) demonstrated that an intensive educational course improved DA knowledge and confidence among non-specialists. However, ADAPT was restricted to English-speaking participants and its generalisability remains unknown. To address this, a multinational implementation study expanding ADAPT (ADAPT-2) was performed. METHODS: Non-allergist physicians from Colombo (Sri Lanka), Guangzhou and Shenzhen (Mainland China), Hong Kong (Special Administrative Region of China) and Perth (Australia) completed a standardised DA educational course. In Mainland China, training was delivered via AI-assisted video localisation (converted into Mandarin while preserving the speaker's voice with lip-synced adaptation). DA knowledge, confidence and practice were assessed before and after completion. Subgroup analyses compared pre-post changes between Advanced Economies (AE: Australia, Hong Kong) and Emerging Economies (EE: Mainland China, Sri Lanka). RESULTS: Of 181 participants, overall baseline knowledge (53.5%\u2009\u00b1\u200917.2%) and confidence (47.5%\u2009\u00b1\u200922.7%) scores were suboptimal. EE participants had a lower knowledge level than AE (49.1%\u2009\u00b1\u200915.5% vs. 70.1%\u2009\u00b1\u200912.7%; p\u2009<\u20090.001). Following ADAPT-2, both knowledge (72.5%\u2009\u00b1\u200916.0%, p\u2009<\u20090.001) and confidence (71.3%\u2009\u00b1\u200917.5%, p\u2009<\u20090.001) scores significantly improved across all groups. ADAPT-2 delivered by AI-assisted video localisation was non-inferior to the English course in effectiveness (p\u2009>\u20090.05) and achieved high participant satisfaction (98.9% as 'somewhat clear' or better in clarity). CONCLUSIONS: Deficits in DA knowledge persist widely among non-specialists, with marked disparities between AE and EE. ADAPT-2 bridged these gaps by universal improvements in both DA knowledge and confidence. AI-assisted training represents a scalable, equitable strategy for global implementation of standardised and evidence-based DA education. TRIAL REGISTRATION: ADAPT: NCT06399601.",
      "journal": "Clinical and experimental allergy : journal of the British Society for Allergy and Clinical Immunology",
      "year": "2026",
      "doi": "10.1111/cea.70213",
      "authors": "Mak Hugo W F et al.",
      "keywords": "artificial intelligence; disparities; drug allergy; education; non\u2010allergist",
      "mesh_terms": "Humans; Female; Male; Adult; Middle Aged; Health Knowledge, Attitudes, Practice; Hong Kong",
      "pub_types": "Journal Article; Multicenter Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41558734/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12879266",
      "ft_text_length": 26946,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12879266)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41561161",
      "title": "Large language model bias auditing for periodontal diagnosis using an ambiguity-probe methodology: a pilot study.",
      "abstract": "BACKGROUND: Large Language Models (LLMs) in healthcare holds immense promise yet carries the risk of perpetuating social biases. While artificial intelligence (AI) fairness is a growing concern, a gap exists in understanding how these models perform under conditions of clinical ambiguity, a common feature in real-world practice. METHODS: We conducted a study using an ambiguity-probe methodology with a set of 42 sociodemographic personas and 15 clinical vignettes based on the 2018 classification of periodontal diseases. Ten were clear-cut scenarios with established ground truths, while five were intentionally ambiguous. OpenAI's GPT-4o and Google's Gemini 2.5 Pro were prompted to provide periodontal stage and grade assessments using 630 vignette-persona combinations per model. RESULTS: In clear-cut scenarios, GPT-4o demonstrated significantly higher combined (stage and grade) accuracy (70.5%) than Gemini Pro (33.3%). However, a robust fairness analysis using cumulative link models with false discovery rate correction revealed no statistically significant sociodemographic bias in either model. This finding held true across both clear-cut and ambiguous clinical scenarios. CONCLUSION: To our knowledge, this is among the first study to use simulated clinical ambiguity to reveal the distinct ethical fingerprints of LLMs in a dental context. While LLM performance gaps exist, our analysis decouples accuracy from fairness, demonstrating that both models maintain sociodemographic neutrality. We identify that the observed errors are not bias, but rather diagnostic boundary instability. This highlights a critical need for future research to differentiate between these two distinct types of model failure to build genuinely reliable AI.",
      "journal": "Frontiers in digital health",
      "year": "2025",
      "doi": "10.3389/fdgth.2025.1687820",
      "authors": "Nantakeeratipat Teerachate",
      "keywords": "AI bias; GPT-4o; Gemini Pro; clinical ambiguity; dental informatics; ethical auditing; health inequities; large language models",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41561161/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12812596",
      "ft_text_length": 22467,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12812596)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "41574036",
      "title": "Artificial intelligence methods to detect heart failure with preserved ejection fraction within electronic health records: an equitable disease detection model.",
      "abstract": "AIMS: Heart failure with preserved ejection fraction (HFpEF) accounts for approximately half of all heart failure cases, with high levels of morbidity and mortality. However, many patients who meet diagnostic criteria for HFpEF do not have a documented diagnosis, particularly in non-White populations where conventional risk scores may underestimate risk. Our aim was to develop and validate a diagnostic prediction model to detect HFpEF based on ESC criteria, AIM-HFpEF. METHODS AND RESULTS: We applied natural language processing (NLP) and machine learning methods to routinely collected electronic health record (EHR) data from a tertiary centre hospital trust in London, UK, to derive the AIM-HFpEF model. We then externally validated the model and performed benchmarking against existing HFpEF prediction models (H2FPEF and HFpEF-ABA) for diagnostic power on the entire external cohort and in patients of non-White ethnicity and patients from areas of increased socioeconomic deprivation. An XGBoost model combining demographic, clinical, and echocardiogram data showed strong diagnostic performance in the derivation dataset [n = 3173, AUC = 0.88, (95% CI, 0.85-0.91)] and validation cohort [n = 5383, AUC: 0.88 (95% CI, 0.86-0.90)]. Diagnostic performance was maintained in patients of non-White ethnicity [AUC = 0.89 (95% CI, 0.85-0.93)] and patients from areas of high socioeconomic deprivation [AUC = 0.90 (95% CI, 0.85-0.95)]. In contrast, AIM-HFpEF demonstrated favourable performance relative to the H2FPEF and HFpEF-ABA models. AIM-HFpEF model probabilities were associated with an increased risk of death, hospitalization, and stroke in the external validation cohort (P < 0.001, P = 0.01, P < 0.001, respectively, for highest vs. middle tertile). CONCLUSION: AIM-HFpEF represents a validated equitable diagnostic model for HFpEF, which can be embedded within an EHR to allow for fully automated HFpEF detection.",
      "journal": "European heart journal. Digital health",
      "year": "2026",
      "doi": "10.1093/ehjdh/ztaf107",
      "authors": "Wu Jack et al.",
      "keywords": "Electronic health records; Heart failure with preserved ejection fraction; Prediction model",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41574036/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12821069",
      "ft_text_length": 38695,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12821069)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "41577988",
      "title": "Sex disparities in deep learning estimation of ejection fraction from cardiac magnetic resonance imaging.",
      "abstract": "The advent of artificial intelligence in cardiovascular imaging holds immense potential for earlier diagnoses, precision medicine, and improved disease management. However, the presence of sex-based disparities and strategies to mitigate biases in deep learning models for cardiac imaging remain understudied. In this study, we analyzed algorithmic bias in a foundation model that was pretrained on cardiac magnetic resonance imaging and radiology reports from multiple institutes and finetuned to estimate ejection fraction (EF) on the UK Biobank dataset. The model performed significantly worse in EF estimation for females than males in the diagnosis of reduced EF. Algorithmic fairness did not improve despite masking of protected attributes in radiology reports and data resampling, although explicit input of sex in model finetuning may improve EF estimation in some cases. The underdiagnosis of reduced EF among females holds critical implications for the exacerbation of existing sex-based disparities in cardiovascular health. We advise caution in the development of models for cardiovascular imaging to avoid such pitfalls.",
      "journal": "NPJ digital medicine",
      "year": "2026",
      "doi": "10.1038/s41746-025-02330-6",
      "authors": "Kaur Dhamanpreet et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41577988/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12894861",
      "ft_text_length": 40941,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12894861)",
      "ft_reason": "Included: bias central + approach content (8 indicators)"
    },
    {
      "pmid": "41589030",
      "title": "Mitigating Disparities in Prostate Cancer Survival Prediction Through Fairness-Aware Machine Learning Models.",
      "abstract": "PURPOSE: Prediction models can contribute to disparities in care by performing unequally across demographic groups. While fairness-aware methods have been explored for binary outcomes, applications to survival analysis remain limited. This study compares two fairness-aware deep learning survival models to mitigate racial disparities in predicting survival after radical prostatectomy for prostate cancer. METHODS: We used the National Cancer Database to train deep Cox proportional hazards models for overall survival. Two fairness-aware approaches, Fair Deep Cox Proportional Hazards Model (Fair DCPH) and Group Distributionally Robust Optimization Deep Cox Proportional Hazards Model (GroupDRO DCPH), were compared against a standard Deep Cox model (Baseline). Model fairness was assessed via cross-group and within-group concordance indices (C-index). RESULTS: Among 418,968 included patients, 78.5% were White, with smaller proportions of Black (13.2%), Hispanic (4.5%), Asian (1.9%), and Other (2.0%) patients. The baseline DCPH model achieved a cross-group C-index of 0.699 for White patients but showed reduced performance for Black (0.678) and Hispanic (0.689) patients. Fairness-aware models improved cross-group C-indices; for Black patients, cross-group C-index increased to 0.692 (Fair DCPH) and 0.696 (GroupDRO DCPH); for Hispanic patients, to 0.693 and 0.697, respectively. Cross-group C-index also improved in the Asian subgroup, where the C-index rose from 0.696 (Baseline DCPH) to 0.702 (Fair DCPH) and 0.707 (GroupDRO DCPH), with minimal performance loss observed for White patients. CONCLUSION: We benchmark two fairness-aware survival models that address racial disparities in post-prostatectomy survival prediction. These methods can be extended to other time-to-event models to ensure equitable care supported by fair prediction models.",
      "journal": "Cancer medicine",
      "year": "2026",
      "doi": "10.1002/cam4.71544",
      "authors": "Do Hyungrok et al.",
      "keywords": "bias; fairness; machine learning; prostate cancer; survival",
      "mesh_terms": "Humans; Male; Prostatic Neoplasms; Middle Aged; Aged; Prostatectomy; Healthcare Disparities; Machine Learning; Proportional Hazards Models; United States",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41589030/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12835780",
      "ft_text_length": 49223,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12835780)",
      "ft_reason": "Included: bias central + approach content (13 indicators)"
    },
    {
      "pmid": "41607892",
      "title": "Can AI developers avoid bias in public health applications?",
      "abstract": "Developments in the field of engineering biology and artificial intelligence have made it increasingly possible to deliver personalised treatments which are tailored to the individual and can help prevent illnesses before they occur. While such advancements have important implications for public health, the use of AI-enabled personalised treatments comes with potential downsides, not least of which is the potential for bias which may cause harm to certain subpopulations. As one of the key actors in the AI development pipeline, developers are ideally placed to ensure that treatments are designed in an equitable manner. However, existing bias mitigation strategies often fail to consider the practical challenges faced by developers which can significantly impact their abilities to detect and remove bias from any treatments which they help to design. In this paper, we highlight some of the practical challenges that developers face in mitigating bias. We also consider the implications of acknowledging such limitations for attributing responsibility related to bias mitigation.",
      "journal": "Frontiers in public health",
      "year": "2025",
      "doi": "10.3389/fpubh.2025.1752729",
      "authors": "Harms Rebekah J et al.",
      "keywords": "artificial intelligence; bias; engineering biology; public health; responsibility",
      "mesh_terms": "Humans; Artificial Intelligence; Public Health; Bias; Precision Medicine",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41607892/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12835397",
      "ft_text_length": 20322,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12835397)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "41624564",
      "title": "Generative pre-trained transformer reinforces historical gender bias in diagnosing women's cardiovascular symptoms.",
      "abstract": "AIMS: Large language models (LLMs) such as GPT are increasingly used to generate clinical teaching cases and support diagnostic reasoning. However, biases in their training data may skew the portrayal and interpretation of cardiovascular symptoms in women, potentially leading to delayed or inaccurate diagnoses. We assessed GPT-4o's and GPT-4's gender representation in simulated cardiovascular cases and GPT-4o's diagnostic performance across genders using real patient notes. METHODS AND RESULTS: First, GPT-4o and GPT-4 were each prompted to generate 15 000 simulated cases spanning 15 cardiovascular conditions with known gender prevalence differences. The model's gender distributions were compared to U.S. prevalence data from large national datasets (Centers for Disease Control and Prevention and National Inpatient Sample) using FDR-corrected \u03c7\u00b2 tests, finding a significant deviation (P < 0.0001). In 14 GPT-4-generated conditions (93%), male patients were overrepresented compared to females by a mean of 30% (SD 8.6%). Second, fifty de-identified cardiovascular patient notes were extracted from the MIMIC-IV-Note database. Patient gender was systematically swapped in each note, and GPT-4o was asked to produce differential diagnoses for each version (10 000 total prompts). Diagnostic accuracy across genders was determined by comparing model outputs to actual discharge diagnoses via FDR-corrected Mann-Whitney U tests, revealing significant diagnostic accuracy differences in 11 cases (22%). Female patients received lower accuracy scores than males for key conditions like coronary artery disease (P < 0.01), abdominal aortic aneurysm (P < 1.0 \u00d7 10-9), and atrial fibrillation (P < 0.01). CONCLUSION: GPT-4o underrepresented women in simulated cardiovascular scenarios and less accurately diagnosed female patients with critical conditions. These biases risk reinforcing historical disparities in cardiovascular care. Future efforts should focus on bias detection and mitigation.",
      "journal": "European heart journal. Digital health",
      "year": "2026",
      "doi": "10.1093/ehjdh/ztaf131",
      "authors": "Krieger Katherine et al.",
      "keywords": "Cardiovascular Diagnosis; GPT; Gender Bias; Large Language Model; Medical Education",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41624564/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12853122",
      "ft_text_length": 16988,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12853122)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "41626936",
      "title": "Quantitative bias analysis for unmeasured confounding in unanchored population-adjusted indirect comparisons.",
      "abstract": "Unanchored population-adjusted indirect comparisons (PAICs) such as matching-adjusted indirect comparison (MAIC) and simulated treatment comparison (STC) attracted a significant attention in the health technology assessment field in recent years. These methods allow for indirect comparisons by balancing different patient characteristics in single-arm studies in the case where individual patient-level data are only available for one study. However, the validity of findings from unanchored MAIC/STC analyses is frequently questioned by decision makers, due to the assumption that all potential prognostic factors and effect modifiers are accounted for. Addressing this critical concern, we introduce a sensitivity analysis algorithm for unanchored PAICs by extending quantitative bias analysis techniques traditionally used in epidemiology. Our proposed sensitivity analysis involves simulating important covariates that were not reported by the comparator study when conducting unanchored STC and enables the formal evaluating of the impact of unmeasured confounding in a quantitative manner without additional assumptions. We demonstrate the practical application of this method through a real-world case study of metastatic colorectal cancer, highlighting its utility in enhancing the robustness and credibility of unanchored PAIC results. Our findings emphasise the necessity of formal quantitative sensitivity analysis in interpreting unanchored PAIC results, as it quantifies the robustness of conclusions regarding potential unmeasured confounders and supports more robust, reliable, and informative decision-making in healthcare.",
      "journal": "Research synthesis methods",
      "year": "2025",
      "doi": "10.1017/rsm.2025.13",
      "authors": "Ren Shijie et al.",
      "keywords": "indirect treatment comparison; population-adjustment; quantitative bias analysis; unanchored simulated treatment comparison; unmeasured confounding",
      "mesh_terms": "Humans; Bias; Algorithms; Confounding Factors, Epidemiologic; Colorectal Neoplasms; Computer Simulation; Technology Assessment, Biomedical; Research Design; Reproducibility of Results; Data Interpretation, Statistical; Models, Statistical",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41626936/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12527536",
      "ft_text_length": 53440,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12527536)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41632023",
      "title": "Fairness Correction in COVID-19 Predictive Models Using Demographic Optimization: Algorithm Development and Validation Study.",
      "abstract": "BACKGROUND: COVID-19 forecasting models have been used to inform decision-making around resource allocation and intervention decisions, such as hospital beds or stay-at-home orders. State-of-the-art forecasting models often use multimodal data, including mobility or sociodemographic data, to enhance COVID-19 case prediction models. Nevertheless, related work has revealed under-reporting bias in COVID-19 cases as well as sampling bias in mobility data for certain minority racial and ethnic groups, which affects the fairness of COVID-19 predictions across racial and ethnic groups. OBJECTIVE: This study aims to introduce a fairness correction method that works for forecasting COVID-19 cases at an aggregate geographic level. METHODS: We use hard and soft error parity analyses on existing fairness frameworks and demonstrate that our proposed method, Demographic Optimization (DemOpts), performs better in both scenarios. RESULTS: We first demonstrate that state-of-the-art COVID-19 deep learning models produce mean prediction errors that are significantly different across racial and ethnic groups at larger geographic scales. We then propose a novel debiasing method, DemOpts, to increase the fairness of deep learning-based forecasting models trained on potentially biased datasets. Our results show that DemOpts can achieve better error parity than other state-of-the-art debiasing approaches, thus effectively reducing the differences in the mean error distributions across racial and ethnic groups. CONCLUSIONS: We introduce DemOpts, which reduces error parity differences compared with other approaches and generates fairer forecasting models compared with other approaches in the literature.",
      "journal": "Online journal of public health informatics",
      "year": "2026",
      "doi": "10.2196/78235",
      "authors": "Awasthi Naman et al.",
      "keywords": "COVID-19 forecasting; deep learning model; fairness; regression; time series model",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41632023/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12866456",
      "ft_text_length": 46610,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12866456)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "41667212",
      "title": "Mechanistic interpretability of reinforcement learning in Medicaid care coordination.",
      "abstract": "OBJECTIVE: To expose reasoning pathways of a reinforcement learning policy for Medicaid care coordination, develop an error taxonomy and implement fairness-aware guardrails. DESIGN: Retrospective interpretability audit using attention analysis, Shapley explanations, sparse autoencoder feature discovery and blinded clinician adjudication. SETTING: Medicaid care coordination programmes in Washington, Virginia and Ohio (July 2023-June 2025). PARTICIPANTS: 250\u2009000 intervention decisions; 200 divergent cases reviewed by five clinicians. MAIN OUTCOME MEASURES: Calibrated harm prediction; algorithmic clearance and residual harm rates; error taxonomy frequencies; subgroup fairness metrics. RESULTS: The conformal model achieved area under the receiver operating characteristic curve of 0.80 (95% CI 0.78 to 0.82), clearing 89.5% (95% CI 88.9% to 90.1%) of decisions with 1.22% (95% CI 1.14% to 1.30%) residual harm versus 6.67% (95% CI 6.02% to 7.32%) for flagged decisions. Sparse autoencoders identified seven reasoning motifs linking social determinants to clinical cascades. The error taxonomy revealed premise errors (48%, 95%\u2009CI 41% to 55%), calibration failures (27%, 95%\u2009CI 21% to 33%) and contextual blind spots (25%, 95%\u2009CI 19% to 31%). Divergence was higher for telehealth visits (11.2%) and behavioural health patients (10.7% vs 6.9%, p<0.001). Fairness optimisation reduced race-group disparity by 37% (95% CI 22% to 48%) and sex-group disparity by 28% (95% CI 14% to 39%). Reviewers rated 23% (95% CI 17% to 29%) of overridden recommendations as well-matched, confirming appropriate human oversight. CONCLUSIONS: Mechanistic interpretability transforms opaque algorithmic assistance into auditable decision support, providing a governance scaffold for clinical artificial intelligence deployment.",
      "journal": "BMJ health & care informatics",
      "year": "2026",
      "doi": "10.1136/bmjhci-2025-101935",
      "authors": "Basu Sanjay et al.",
      "keywords": "BMJ Health Informatics; Decision Making, Computer-Assisted",
      "mesh_terms": "Humans; United States; Medicaid; Retrospective Studies; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41667212/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12911724",
      "ft_text_length": 25718,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12911724)",
      "ft_reason": "Included: substantial approach content (7 indicators)"
    },
    {
      "pmid": "41683311",
      "title": "An Equity Audit of a Statewide Cardiometabolic Risk Reduction Pilot Programme for Women with a History of Gestational Diabetes.",
      "abstract": "BACKGROUND: This equity audit assessed enrolment and completion of a state-funded cardiometabolic risk-reduction programme for women with prior gestational diabetes in Victoria, Australia. The analyses compared completion rates between the standard prevention programme Life! with one specifically adapted for women with prior gestational diabetes (Life! GDM) using the PROGRESS equity framework. METHODS: Women with a history of GDM in the Life! GDM or the mainstream Life! programme in 2022-2025 were included. Multinomial logistic regression was used to impute categorical variables, logistic regression for binary variables, and linear regression for continuous variables. Estimates were combined across imputed datasets using Rubin's rules. RESULTS: A total of 2261 women were included: 370 in Life! GDM, and 1891 in Life! from 2022 to 2025, with completion rates of 36.7% and 52.2%, respectively. Compared with women in Life!, women in Life! GDM were more likely to come from non-English-speaking backgrounds, particularly South and Central Asian (30.5% vs. 17.0%) and South-East Asian backgrounds (13.0% vs. 4.3%). After multiple imputation, multivariable logistic regression showed that none of the examined participant characteristics were significantly associated with programme completion in Life! GDM. In the Life! cohort, completion was significantly associated with marital status, with single participants having lower odds of completion (OR = 0.59, 95% CI: 0.41-0.85), and with referral channel, with self-referral associated with higher odds of completion (OR = 1.71, 95% CI: 1.39-2.12). CONCLUSIONS: The adapted programme appeared to have reached more culturally and linguistically diverse women; however, lower completion among those experiencing disadvantage highlights the need for enhanced support and retention strategies to ensure equitable postpartum diabetes prevention.",
      "journal": "Nutrients",
      "year": "2026",
      "doi": "10.3390/nu18030489",
      "authors": "Dou Yuqi et al.",
      "keywords": "completion; engagement; equity; gestational diabetes mellitus; migrants; type 2 diabetes",
      "mesh_terms": "Humans; Female; Diabetes, Gestational; Pregnancy; Adult; Pilot Projects; Risk Reduction Behavior; Victoria; Logistic Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41683311/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12899937",
      "ft_text_length": 29906,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12899937)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "34657287",
      "title": "Using machine learning to advance disparities research: Subgroup analyses of access to opioid treatment.",
      "abstract": "OBJECTIVE: To operationalize an intersectionality framework using a novel statistical approach and with these efforts, improve the estimation of disparities in access (i.e., wait time to treatment entry) to opioid use disorder (OUD) treatment beyond race. DATA SOURCE: Sample of 941,286 treatment episodes collected in 2015-2017 in the United States from the Treatment Episodes Data Survey (TEDS-A) and a subset from California (n\u00a0=\u00a0188,637) and Maryland (n\u00a0=\u00a0184,276), states with the largest sample of episodes. STUDY DESIGN: This retrospective subgroup analysis used a two-step approach called virtual twins. In Step 1, we trained a classification model that gives the probability of waiting (1\u2009day or more). In Step 2, we identified subgroups with a higher probability of differences due to race. We tested three classification models for Step 1 and identified the model with the best estimation. DATA COLLECTION: Client data were collected by states during personal interviews at admission and discharge. PRINCIPAL FINDINGS: Random forest was the most accurate model for the first step of subgroup analysis. We found large variation across states in racial disparities. Stratified analysis of two states with the largest samples showed critical factors that augmented disparities beyond race. In California, factors such as service setting, referral source, and homelessness defined the subgroup most vulnerable to racial disparities. In Maryland, service setting, prior episodes, receipt of medication-assisted opioid treatment, and primary drug use frequency augmented disparities beyond race. The identified subgroups had significantly larger racial disparities. CONCLUSIONS: The methodology used in this study enabled a nuanced understanding of the complexities in disparities research. We found state and service factors that intersected with race and augmented disparities in wait time. Findings can help decision makers target modifiable factors that make subgroups vulnerable to waiting longer to enter treatment.",
      "journal": "Health services research",
      "year": "2022",
      "doi": "10.1111/1475-6773.13896",
      "authors": "Kong Yinfei et al.",
      "keywords": "racial disparities; regression tree; subgroup analysis; virtual twins; wait time for opioid treatment",
      "mesh_terms": "Analgesics, Opioid; Healthcare Disparities; Humans; Machine Learning; Maryland; Retrospective Studies; Substance-Related Disorders; United States",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34657287/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8928038",
      "ft_text_length": 38988,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8928038)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "34893776",
      "title": "Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations.",
      "abstract": "Artificial intelligence (AI) systems have increasingly achieved expert-level performance in medical imaging applications. However, there is growing concern that such AI systems may reflect and amplify human bias, and reduce the quality of their performance in historically under-served populations such as female patients, Black patients, or patients of low socioeconomic status. Such biases are especially troubling in the context of underdiagnosis, whereby the AI algorithm would inaccurately label an individual with a disease as healthy, potentially delaying access to care. Here, we examine algorithmic underdiagnosis in chest X-ray pathology classification across three large chest X-ray datasets, as well as one multi-source dataset. We find that classifiers produced using state-of-the-art computer vision techniques consistently and selectively underdiagnosed under-served patient populations and that the underdiagnosis rate was higher for intersectional under-served subpopulations, for example, Hispanic female patients. Deployment of AI systems using medical imaging for disease diagnosis with such biases risks exacerbation of existing care biases and can potentially lead to unequal access to medical treatment, thereby raising ethical concerns for the use of these models in the clinic.",
      "journal": "Nature medicine",
      "year": "2021",
      "doi": "10.1038/s41591-021-01595-0",
      "authors": "Seyyed-Kalantari Laleh et al.",
      "keywords": "",
      "mesh_terms": "Adolescent; Algorithms; Artificial Intelligence; Child; Child, Preschool; Datasets as Topic; Female; Humans; Infant; Infant, Newborn; Male; Radiography, Thoracic; Vulnerable Populations; Young Adult",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34893776/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8674135",
      "ft_text_length": 38656,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC8674135)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "36017878",
      "title": "Statistical quantification of confounding bias in machine learning models.",
      "abstract": "BACKGROUND: The lack of nonparametric statistical tests for confounding bias significantly hampers the development of robust, valid, and generalizable predictive models in many fields of research. Here I propose the partial confounder test, which, for a given confounder variable, probes the null hypotheses of the model being unconfounded. RESULTS: The test provides a strict control for type I errors and high statistical power, even for nonnormally and nonlinearly dependent predictions, often seen in machine learning. Applying the proposed test on models trained on large-scale functional brain connectivity data (N= 1,865) (i) reveals previously unreported confounders and (ii) shows that state-of-the-art confound mitigation approaches may fail preventing confounder bias in several cases. CONCLUSIONS: The proposed test (implemented in the package mlconfound; https://mlconfound.readthedocs.io) can aid the assessment and improvement of the generalizability and validity of predictive models and, thereby, fosters the development of clinically useful machine learning biomarkers.",
      "journal": "GigaScience",
      "year": "2022",
      "doi": "10.1093/gigascience/giac082",
      "authors": "Spisak Tamas",
      "keywords": "conditional independence; conditional permutation; confounder test; confounding bias; machine learning; predictive modeling",
      "mesh_terms": "Bias; Brain; Machine Learning; Models, Statistical",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36017878/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9412867",
      "ft_text_length": 102983,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9412867)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "36339512",
      "title": "Considerations in the reliability and fairness audits of predictive models for advance care planning.",
      "abstract": "Multiple reporting guidelines for artificial intelligence (AI) models in healthcare recommend that models be audited for reliability and fairness. However, there is a gap of operational guidance for performing reliability and fairness audits in practice. Following guideline recommendations, we conducted a reliability audit of two models based on model performance and calibration as well as a fairness audit based on summary statistics, subgroup performance and subgroup calibration. We assessed the Epic End-of-Life (EOL) Index model and an internally developed Stanford Hospital Medicine (HM) Advance Care Planning (ACP) model in 3 practice settings: Primary Care, Inpatient Oncology and Hospital Medicine, using clinicians' answers to the surprise question (\"Would you be surprised if [patient X] passed away in [Y years]?\") as a surrogate outcome. For performance, the models had positive predictive value (PPV) at or above 0.76 in all settings. In Hospital Medicine and Inpatient Oncology, the Stanford HM ACP model had higher sensitivity (0.69, 0.89 respectively) than the EOL model (0.20, 0.27), and better calibration (O/E 1.5, 1.7) than the EOL model (O/E 2.5, 3.0). The Epic EOL model flagged fewer patients (11%, 21% respectively) than the Stanford HM ACP model (38%, 75%). There were no differences in performance and calibration by sex. Both models had lower sensitivity in Hispanic/Latino male patients with Race listed as \"Other.\" 10 clinicians were surveyed after a presentation summarizing the audit. 10/10 reported that summary statistics, overall performance, and subgroup performance would affect their decision to use the model to guide care; 9/10 said the same for overall and subgroup calibration. The most commonly identified barriers for routinely conducting such reliability and fairness audits were poor demographic data quality and lack of data access. This audit required 115 person-hours across 8-10 months. Our recommendations for performing reliability and fairness audits include verifying data validity, analyzing model performance on intersectional subgroups, and collecting clinician-patient linkages as necessary for label generation by clinicians. Those responsible for AI models should require such audits before model deployment and mediate between model auditors and impacted stakeholders.",
      "journal": "Frontiers in digital health",
      "year": "2022",
      "doi": "10.3389/fdgth.2022.943768",
      "authors": "Lu Jonathan et al.",
      "keywords": "advance care planning; artificial intelligence; audit; electronic health record; fairness; model reporting guideline",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36339512/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9634737",
      "ft_text_length": 84513,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC9634737)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "36702767",
      "title": "Invited Commentary: Undiagnosed and Undertreated-the Suffocating Consequences of\u00a0the Use of Racially Biased Medical Devices During the COVID-19 Pandemic.",
      "abstract": "While medical technology is typically considered neutral, many devices rely upon racially biased algorithms that prioritize care for White patients over Black patients, who may require more urgent medical attention. In their accompanying article, Sudat et al. (Am J Epidemiol. 2023;XXX(XX):XXX-XXX) document striking inaccuracies in pulse oximeter readings among Black patients, with significant clinical implications. Their findings suggest that this resulted in racial differences in delivery of evidence-based care during the coronavirus disease 2019 (COVID-19) pandemic, affecting admissions and treatment protocols. Despite the medical community's growing awareness of the pulse oximeter's significant design flaw, the device is still in use. In this article, I contextualize Sudat et al.'s study results within the larger history of racial bias in medical devices by highlighting the consequences of the continued underrepresentation of diverse populations in clinical trials. I probe the implications of racially biased assessments within clinical practice and research and illustrate the disproportionate impact on patients of color by examining 2 medical tools, the pulse oximeter and pulmonary function tests. Both cases result in the undertreatment and underdiagnosis of Black patients. I also demonstrate how the social underpinnings of racial bias in medical technology contribute to poor health outcomes and reproduce health disparities, and propose several recommendations for the field to rectify the harms of racial bias in medical technology.",
      "journal": "American journal of epidemiology",
      "year": "2023",
      "doi": "10.1093/aje/kwad019",
      "authors": "Plaisime Marie V",
      "keywords": "COVID-19; Food and Drug Administration; clinical trials; hypoxia; pulse oximetry; racial bias; racism; skin pigmentation",
      "mesh_terms": "Humans; Black or African American; COVID-19; Oximetry; Pandemics; Racism; Equipment and Supplies",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36702767/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10160765",
      "ft_text_length": 19191,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10160765)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37001995",
      "title": "Evaluating equity in performance of an electronic health record-based 6-month mortality risk model to trigger palliative care consultation: a retrospective model validation analysis.",
      "abstract": "OBJECTIVE: Evaluate predictive performance of an electronic health record (EHR)-based, inpatient 6-month mortality risk model developed to trigger palliative care consultation among patient groups stratified by age, race, ethnicity, insurance and socioeconomic status (SES), which may vary due to social forces (eg, racism) that shape health, healthcare and health data. DESIGN: Retrospective evaluation of prediction model. SETTING: Three urban hospitals within a single health system. PARTICIPANTS: All patients \u226518 years admitted between 1 January and 31 December 2017, excluding observation, obstetric, rehabilitation and hospice (n=58\u2009464 encounters, 41\u2009327 patients). MAIN OUTCOME MEASURES: General performance metrics (c-statistic, integrated calibration index (ICI), Brier Score) and additional measures relevant to health equity (accuracy, false positive rate (FPR), false negative rate (FNR)). RESULTS: For black versus non-Hispanic white patients, the model's accuracy was higher (0.051, 95%\u2009CI 0.044 to 0.059), FPR lower (-0.060, 95%\u2009CI -0.067 to -0.052) and FNR higher (0.049, 95%\u2009CI 0.023 to 0.078). A similar pattern was observed among patients who were Hispanic, younger, with Medicaid/missing insurance, or living in low SES zip codes. No consistent differences emerged in c-statistic, ICI or Brier Score. Younger age had the second-largest effect size in the mortality prediction model, and there were large standardised group differences in age (eg, 0.32 for non-Hispanic white versus black patients), suggesting age may contribute to systematic differences in the predicted probabilities between groups. CONCLUSIONS: An EHR-based mortality risk model was less likely to identify some marginalised patients as potentially benefiting from palliative care, with younger age pinpointed as a possible mechanism. Evaluating predictive performance is a critical preliminary step in addressing algorithmic inequities in healthcare, which must also include evaluating clinical impact, and governance and regulatory structures for oversight, monitoring and accountability.",
      "journal": "BMJ quality & safety",
      "year": "2023",
      "doi": "10.1136/bmjqs-2022-015173",
      "authors": "Teeple Stephanie et al.",
      "keywords": "decision support, computerized; evaluation methodology; information technology",
      "mesh_terms": "Pregnancy; Female; United States; Humans; Palliative Care; Retrospective Studies; Electronic Health Records; Ethnicity; Referral and Consultation",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37001995/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10898860",
      "ft_text_length": 2075,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10898860)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "37195986",
      "title": "Assessing racial bias in type 2 diabetes risk prediction algorithms.",
      "abstract": "Risk prediction models for type 2 diabetes can be useful for the early detection of individuals at high risk. However, models may also bias clinical decision-making processes, for instance by differential risk miscalibration across racial groups. We investigated whether the Prediabetes Risk Test (PRT) issued by the National Diabetes Prevention Program, and two prognostic models, the Framingham Offspring Risk Score, and the ARIC Model, demonstrate racial bias between non-Hispanic Whites and non-Hispanic Blacks. We used National Health and Nutrition Examination Survey (NHANES) data, sampled in six independent two-year batches between 1999 and 2010. A total of 9,987 adults without a prior diagnosis of diabetes and with fasting blood samples available were included. We calculated race- and year-specific average predicted risks of type 2 diabetes according to the risk models. We compared the predicted risks with observed ones extracted from the US Diabetes Surveillance System across racial groups (summary calibration). All investigated models were found to be miscalibrated with regard to race, consistently across the survey years. The Framingham Offspring Risk Score overestimated type 2 diabetes risk for non-Hispanic Whites and underestimated risk for non-Hispanic Blacks. The PRT and the ARIC models overestimated risk for both races, but more so for non-Hispanic Whites. These landmark models overestimated the risk of type 2 diabetes for non-Hispanic Whites more severely than for non-Hispanic Blacks. This may result in a larger proportion of non-Hispanic Whites being prioritized for preventive interventions, but it also increases the risk of overdiagnosis and overtreatment in this group. On the other hand, a larger proportion of non-Hispanic Blacks may be potentially underprioritized and undertreated.",
      "journal": "PLOS global public health",
      "year": "2023",
      "doi": "10.1371/journal.pgph.0001556",
      "authors": "Cronj\u00e9 H\u00e9l\u00e9ne T et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37195986/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10191313",
      "ft_text_length": 35770,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10191313)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "37266959",
      "title": "Awareness of Racial and Ethnic Bias and Potential Solutions to Address Bias With Use of Health Care Algorithms.",
      "abstract": "IMPORTANCE: Algorithms are commonly incorporated into health care decision tools used by health systems and payers and thus affect quality of care, access, and health outcomes. Some algorithms include a patient's race or ethnicity among their inputs and can lead clinicians and decision-makers to make choices that vary by race and potentially affect inequities. OBJECTIVE: To inform an evidence review on the use of race- and ethnicity-based algorithms in health care by gathering public and stakeholder perspectives about the repercussions of and efforts to address algorithm-related bias. DESIGN, SETTING, AND PARTICIPANTS: Qualitative methods were used to analyze responses. Responses were initially open coded and then consolidated to create a codebook, with themes and subthemes identified and finalized by consensus. This qualitative study was conducted from May 4, 2021, through December 7, 2022. Forty-two organization representatives (eg, clinical professional societies, universities, government agencies, payers, and health technology organizations) and individuals responded to the request for information. MAIN OUTCOMES AND MEASURES: Identification of algorithms with the potential for race- and ethnicity-based biases and qualitative themes. RESULTS: Forty-two respondents identified 18 algorithms currently in use with the potential for bias, including, for example, the Simple Calculated Osteoporosis Risk Estimation risk prediction tool and the risk calculator for vaginal birth after cesarean section. The 7 qualitative themes, with 31 subthemes, included the following: (1) algorithms are in widespread use and have significant repercussions, (2) bias can result from algorithms whether or not they explicitly include race, (3) clinicians and patients are often unaware of the use of algorithms and potential for bias, (4) race is a social construct used as a proxy for clinical variables, (5) there is a lack of standardization in how race and social determinants of health are collected and defined, (6) bias can be introduced at all stages of algorithm development, and (7) algorithms should be discussed as part of shared decision-making between the patient and clinician. CONCLUSIONS AND RELEVANCE: This qualitative study found that participants perceived widespread and increasing use of algorithms in health care and lack of oversight, potentially exacerbating racial and ethnic inequities. Increasing awareness for clinicians and patients and standardized, transparent approaches for algorithm development and implementation may be needed to address racial and ethnic biases related to algorithms.",
      "journal": "JAMA health forum",
      "year": "2023",
      "doi": "10.1001/jamahealthforum.2023.1197",
      "authors": "Jain Anjali et al.",
      "keywords": "",
      "mesh_terms": "Pregnancy; Humans; Female; Cesarean Section; Delivery of Health Care; Ethnicity; Health Facilities; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37266959/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10238944",
      "ft_text_length": 48488,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10238944)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "37842820",
      "title": "Development and Validation of the US Diabetes, Obesity, Cardiovascular Disease Microsimulation (DOC-M) Model: Health Disparity and Economic Impact Model.",
      "abstract": "BACKGROUND: Few simulation models have incorporated the interplay of diabetes, obesity, and cardiovascular disease (CVD); their upstream lifestyle and biological risk factors; and their downstream effects on health disparities and economic consequences. METHODS: We developed and validated a US Diabetes, Obesity, Cardiovascular Disease Microsimulation (DOC-M) model that incorporates demographic, clinical, and lifestyle risk factors to jointly predict overall and racial-ethnic groups-specific obesity, diabetes, CVD, and cause-specific mortality for the US adult population aged 40 to 79 y at baseline. An individualized health care cost prediction model was further developed and integrated. This model incorporates nationally representative data on baseline demographics, lifestyle, health, and cause-specific mortality; dynamic changes in modifiable risk factors over time; and parameter uncertainty using probabilistic distributions. Validation analyses included assessment of 1) population-level risk calibration and 2) individual-level risk discrimination. To illustrate the application of the DOC-M model, we evaluated the long-term cost-effectiveness of a national produce prescription program. RESULTS: Comparing the 15-y model-predicted population risk of primary outcomes among the 2001-2002 National Health and Nutrition Examination Survey (NHANES) cohort with the observed prevalence from age-matched cross-sectional 2003-2016 NHANES cohorts, calibration performance was strong based on observed-to-expected ratio and calibration plot analysis. In most cases, Brier scores fell below 0.0004, indicating a low overall prediction error. Using the Multi-Ethnic Study of Atherosclerosis cohorts, the c-statistics for assessing individual-level risk discrimination were 0.85 to 0.88 for diabetes, 0.93 to 0.95 for obesity, 0.74 to 0.76 for CVD history, and 0.78 to 0.81 for all-cause mortality, both overall and in three racial-ethnic groups. Open-source code for the model was posted at https://github.com/food-price/DOC-M-Model-Development-and-Validation. CONCLUSIONS: The validated DOC-M model can be used to examine health, equity, and the economic impact of health policies and interventions on behavioral and clinical risk factors for obesity, diabetes, and CVD. HIGHLIGHTS: We developed a novel microsimula'tion model for obesity, diabetes, and CVD, which intersect together and - critically for prevention and treatment interventions - share common lifestyle, biologic, and demographic risk factors.Validation analyses, including assessment of (1) population-level risk calibration and (2) individual-level risk discrimination, showed strong performance across the overall population and three major racial-ethnic groups for 6 outcomes (obesity, diabetes, CVD, and all-cause mortality, CVD- and DM-cause mortality)This paper provides a thorough explanation and documentation of the development and validation process of a novel microsimulation model, along with the open-source code (https://github.com/food-price/ DOCM_validation) for public use, to serve as a guide for future simulation model assessments, validation, and implementation.",
      "journal": "Medical decision making : an international journal of the Society for Medical Decision Making",
      "year": "2023",
      "doi": "10.1177/0272989X231196916",
      "authors": "Kim David D et al.",
      "keywords": "calibration; cardiovascular disease; diabetes; microsimulation model; obesity; validation",
      "mesh_terms": "Adult; Humans; Cardiovascular Diseases; Nutrition Surveys; Cross-Sectional Studies; Diabetes Mellitus; Risk Factors; Obesity",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37842820/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10625721",
      "ft_text_length": 55627,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10625721)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "38106012",
      "title": "A Fair Individualized Polysocial Risk Score for Identifying Increased Social Risk in Type 2 Diabetes.",
      "abstract": "BACKGROUND: Racial and ethnic minority groups and individuals facing social disadvantages, which often stem from their social determinants of health (SDoH), bear a disproportionate burden of type 2 diabetes (T2D) and its complications. It is crucial to implement effective social risk management strategies at the point of care. OBJECTIVE: To develop an electronic health records (EHR)-based machine learning (ML) analytical pipeline to address unmet social needs associated with hospitalization risk in patients with T2D. METHODS: We identified real-world patients with T2D from the EHR data from University of Florida (UF) Health Integrated Data Repository (IDR), incorporating both contextual SDoH (e.g., neighborhood deprivation) and individual-level SDoH (e.g., housing instability). The 2015-2020 data were used for training and validation and 2021-2022 data for independent testing. We developed a machine learning analytic pipeline, namely individualized polysocial risk score (iPsRS), to identify high social risk associated with hospitalizations in T2D patients, along with explainable AI (XAI) and fairness optimization. RESULTS: The study cohort included 10,192 real-world patients with T2D, with a mean age of 59 years and 58% female. Of the cohort, 50% were non-Hispanic White, 39% were non-Hispanic Black, 6% were Hispanic, and 5% were other races/ethnicities. Our iPsRS, including both contextual and individual-level SDoH as input factors, achieved a C statistic of 0.72 in predicting 1-year hospitalization after fairness optimization across racial and ethnic groups. The iPsRS showed excellent utility for capturing individuals at high hospitalization risk because of SDoH, that is, the actual 1-year hospitalization rate in the top 5% of iPsRS was 28.1%, ~13 times as high as the bottom decile (2.2% for 1-year hospitalization rate). CONCLUSION: Our ML pipeline iPsRS can fairly and accurately screen for patients who have increased social risk leading to hospitalization in real word patients with T2D.",
      "journal": "Research square",
      "year": "2023",
      "doi": "10.21203/rs.3.rs-3684698/v1",
      "authors": "Huang Yu et al.",
      "keywords": "Fairness; Machine Learning; Machine learning; Prediction; Type 2 diabetes",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38106012/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10723535",
      "ft_text_length": 27620,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC10723535)",
      "ft_reason": "Included: bias central + approach content (14 indicators)"
    },
    {
      "pmid": "38875699",
      "title": "Equity-Driven Sensing System for Measuring Skin Tone-Calibrated Peripheral Blood Oxygen Saturation (OptoBeat): Development, Design, and Evaluation Study.",
      "abstract": "BACKGROUND: Many commodity pulse oximeters are insufficiently calibrated for patients with darker skin. We demonstrate a quantitative measurement of this disparity in peripheral blood oxygen saturation (SpO2) with a controlled experiment. To mitigate this, we present OptoBeat, an ultra-low-cost smartphone-based optical sensing system that captures SpO2 and heart rate while calibrating for differences in skin tone. Our sensing system can be constructed from commodity components and 3D-printed clips for approximately US $1. In our experiments, we demonstrate the efficacy of the OptoBeat system, which can measure SpO2 within 1% of the ground truth in levels as low as 75%. OBJECTIVE: The objective of this work is to test the following hypotheses and implement an ultra-low-cost smartphone adapter to measure SpO2: skin tone has a significant effect on pulse oximeter measurements (hypothesis 1), images of skin tone can be used to calibrate pulse oximeter error (hypothesis 2), and SpO2 can be measured with a smartphone camera using the screen as a light source (hypothesis 3). METHODS: Synthetic skin with the same optical properties as human skin was used in ex vivo experiments. A skin tone scale was placed in images for calibration and ground truth. To achieve a wide range of SpO2 for measurement, we reoxygenated sheep blood and pumped it through synthetic arteries. A custom optical system was connected from the smartphone screen (flashing red and blue) to the analyte and into the phone's camera for measurement. RESULTS: The 3 skin tones were accurately classified according to the Fitzpatrick scale as types 2, 3, and 5. Classification was performed using the Euclidean distance between the measured red, green, and blue values. Traditional pulse oximeter measurements (n=2000) showed significant differences between skin tones in both alternating current and direct current measurements using ANOVA (direct current: F2,5997=3.1170 \u00d7 105, P<.01; alternating current: F2,5997=8.07 \u00d7 106, P<.01). Continuous SpO2 measurements (n=400; 10-second samples, 67 minutes total) from 95% to 75% were captured using OptoBeat in an ex vivo experiment. The accuracy was measured to be within 1% of the ground truth via quadratic support vector machine regression and 10-fold cross-validation (R2=0.97, root mean square error=0.7, mean square error=0.49, and mean absolute error=0.5). In the human-participant proof-of-concept experiment (N=3; samples=3 \u00d7 N, duration=20-30 seconds per sample), SpO2 measurements were accurate to within 0.5% of the ground truth, and pulse rate measurements were accurate to within 1.7% of the ground truth. CONCLUSIONS: In this work, we demonstrate that skin tone has a significant effect on SpO2 measurements and the design and evaluation of OptoBeat. The ultra-low-cost OptoBeat system enables smartphones to classify skin tone for calibration, reliably measure SpO2 as low as 75%, and normalize to avoid skin tone-based bias.",
      "journal": "JMIR biomedical engineering",
      "year": "2022",
      "doi": "10.2196/34934",
      "authors": "Adams Alexander T et al.",
      "keywords": "bias; health app; health equity; heart rate; mHealth; mobile health; mobile phone; oximeter; oximetry; oxygen level; oxygen saturation; pulse; pulse oximetry; sensor; skin tone; smartphone; ubiquitous health",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38875699/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11041433",
      "ft_text_length": 34934,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11041433)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "39334279",
      "title": "Predicting the risk of diabetes complications using machine learning and social administrative data in a country with ethnic inequities in health: Aotearoa New Zealand.",
      "abstract": "BACKGROUND: In the age of big data, linked social and administrative health data in combination with machine learning (ML) is being increasingly used to improve prediction in chronic disease, e.g., cardiovascular diseases (CVD). In this study we aimed to apply ML methods on extensive national-level health and social administrative datasets to assess the utility of these for predicting future diabetes complications, including by ethnicity. METHODS: Five ML models were used to predict CVD events among all people with known diabetes in the population of New Zealand, utilizing nationwide individual-level administrative data. RESULTS: The Xgboost ML model had the best predictive power for predicting CVD events three years into the future among the population with diabetes (N\u2009=\u2009145,600). The optimization procedure also found limited improvement in prediction by ethnicity (using area under the receiver operating curve, [AUC]). The results indicated no trade-off between model predictive performance and equity gap of prediction by ethnicity (that is improving model prediction and reducing performance gaps by ethnicity can be achieved simultaneously). The list of variables of importance was different among different models/ethnic groups, for example: age, deprivation (neighborhood-level), having had a hospitalization event, and the number of years living with diabetes. DISCUSSION AND CONCLUSIONS: We provide further evidence that ML with administrative health data can be used for meaningful future prediction of health outcomes. As such, it could be utilized to inform health planning and healthcare resource allocation for diabetes management and the prevention of CVD events. Our results may suggest limited scope for developing prediction models by ethnic group and that the major ways to reduce inequitable health outcomes is probably via improved delivery of prevention and management to those groups with diabetes at highest need.",
      "journal": "BMC medical informatics and decision making",
      "year": "2024",
      "doi": "10.1186/s12911-024-02678-x",
      "authors": "Nghiem Nhung et al.",
      "keywords": "Cardiovascular disease; Diabetes complications; Health and social administrative data; Machine learning; Risk prediction",
      "mesh_terms": "Adult; Aged; Female; Humans; Male; Middle Aged; Cardiovascular Diseases; Diabetes Complications; Diabetes Mellitus; Ethnicity; Health Status Disparities; Machine Learning; New Zealand; Risk Assessment",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39334279/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11438423",
      "ft_text_length": 41434,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11438423)",
      "ft_reason": "Included: bias central + approach content (2 indicators)"
    },
    {
      "pmid": "40005359",
      "title": "Operational Advantages of Novel Strategies Supported by Portability and Artificial Intelligence for Breast Cancer Screening in Low-Resource Rural Areas: Opportunities to Address Health Inequities and Vulnerability.",
      "abstract": "Early detection of breast cancer plays a crucial role in reducing the number of cases diagnosed at advanced stages, thereby lowering the high healthcare costs required to achieve disease-free survival and helping to prevent avoidable premature deaths. However, women living in rural and low-income areas face multiple obstacles that limit their access to conventional screening methods, such as mammography, which has been widely proven effective, particularly in high-income countries. The main barriers include a lack of healthcare infrastructure, long distances to medical facilities, high costs associated with large-scale screening programs, and shortages of specialized personnel. In this context, emerging technologies offer innovative solutions with the potential to mitigate these challenges. The development of strategies supported by artificial intelligence and the use of portable devices capable of overcoming geographical and sociocultural barriers represent valuable complementary tools for strengthening community-driven screening programs and expanding the reach of large-scale initiatives. However, to date, no comprehensive analysis has been conducted on the availability of evidence assessing the outcomes of breast cancer screening programs in vulnerable and underserved communities. This manuscript outlines the benefits of emerging portable technologies powered by artificial intelligence for detecting significant breast lesions in low-resource rural areas, where traditional screening methods are often difficult to implement. It also highlights gaps in the current knowledge, drawing on the available evidence. A search using PubMed yielded 7629 articles on breast cancer screening, of which only 59 (0.77%) addressed resource-limited settings and rural populations. Further filtering identified 29 original studies (0.38%) with specific epidemiological designs involving humans as the unit of analysis. The findings revealed significant disparities in evidence availability: nine studies originated from high-income countries, while fewer than half were from low-income or lower middle-income countries. Only two studies were conducted in Latin America, specifically in Peru and Argentina. This limited evidence poses challenges for generalizing and replicating recommendations for unexplored settings.",
      "journal": "Medicina (Kaunas, Lithuania)",
      "year": "2025",
      "doi": "10.3390/medicina61020242",
      "authors": "Xiques-Molina Wolmark et al.",
      "keywords": "breast neoplasms; early detection of cancer; mass screening; poverty areas; rural population; socio-economic factors",
      "mesh_terms": "Female; Humans; Artificial Intelligence; Breast Neoplasms; Early Detection of Cancer; Healthcare Disparities; Mammography; Mass Screening; Rural Population",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40005359/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11857370",
      "ft_text_length": 17637,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11857370)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40216860",
      "title": "Detecting implicit biases of large language models with Bayesian hypothesis testing.",
      "abstract": "Despite the remarkable performance of large language models (LLMs), such as generative pre-trained Transformers (GPTs), across various tasks, they often perpetuate social biases and stereotypes embedded in their training data. In this paper, we introduce a novel framework that reformulates bias detection in LLMs as a hypothesis testing problem, where the null hypothesis [Formula: see text] represents the absence of implicit bias. Our framework leverages binary-choice questions to measure social bias in both open-source and proprietary LLMs accessible via APIs. We demonstrate the flexibility of our approach by integrating classical statistical methods, such as the exact binomial test, with Bayesian inference using Bayes factors for bias detection and quantification. Extensive experiments are conducted on prominent models, including ChatGPT (GPT-3.5-Turbo), DeepSeek-V3, and Llama-3.1-70B, utilizing publicly available datasets such as BBQ, CrowS-Pairs (in both English and French), and Winogender. While the exact Binomial test fails to distinguish between no evidence of bias and evidence of no bias, our results underscore the advantages of Bayes factors, particularly their capacity to quantify evidence for both competing hypotheses and their robustness to small sample size. Additionally, our experiments reveal that the bias behavior of LLMs is largely consistent across the English and French versions of the CrowS-Pairs dataset, with subtle differences likely arising from variations in social norms across linguistic and cultural contexts.",
      "journal": "Scientific reports",
      "year": "2025",
      "doi": "10.1038/s41598-025-95825-x",
      "authors": "Si Shijing et al.",
      "keywords": "Bayes factor; Fairness; Group bias; Large language models",
      "mesh_terms": "Bayes Theorem; Humans; Bias; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40216860/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11992123",
      "ft_text_length": 81404,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC11992123)",
      "ft_reason": "Included: bias central + approach content (5 indicators)"
    },
    {
      "pmid": "40354423",
      "title": "Automatic large-scale political bias detection of news outlets.",
      "abstract": "Political bias is an inescapable characteristic in news and media reporting, and understanding what political biases people are exposed to when interacting with online news is of crucial import. However, quantifying political bias is problematic. To systematically study the political biases of online news, much of previous research has used human-labelled databases. Yet, these databases tend to be costly, and cover only a few thousand instances at most. Additionally, despite the wide recognition that bias can be expressed in a multitude of ways, many have only examined narrow expressions of bias. For example, most have focused on biased wording in news articles, but ignore bias expressed when an outlet avoids reporting on certain topics or events. In this article, we introduce a data-driven approach that uses machine learning techniques to analyse multiple forms of bias, and that can estimate the political leaning of hundreds of thousands of Web domains with high accuracy. Crucially, this approach also allows us to provide detailed explanations for why a news outlet is assigned a particular political bias. Our work thereby presents a scalable and comprehensive approach to studying political bias in news on a larger scale than ever before.",
      "journal": "PloS one",
      "year": "2025",
      "doi": "10.1371/journal.pone.0321418",
      "authors": "R\u00f6nnback Ronja et al.",
      "keywords": "",
      "mesh_terms": "Politics; Humans; Machine Learning; Mass Media; Bias; Internet",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40354423/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12068563",
      "ft_text_length": 50564,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12068563)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "40554779",
      "title": "Large Language Model\u2013Assisted Risk-of-Bias Assessment in Randomized Controlled Trials Using the Revised Risk-of-Bias Tool: Evaluation Study.",
      "abstract": "BACKGROUND: The revised Risk-of-Bias tool (RoB2) overcomes the limitations of its predecessor but introduces new implementation challenges. Studies demonstrate low interrater reliability and substantial time requirements for RoB2 implementation. Large language models (LLMs) may assist in RoB2 implementation, although their effectiveness remains uncertain. OBJECTIVE: This study aims to evaluate the accuracy of LLMs in RoB2 assessments to explore their potential as research assistants for bias evaluation. METHODS: We systematically searched the Cochrane Library (through October 2023) for reviews using RoB2, categorized by interest in adhering or assignment. From 86 eligible reviews of randomized controlled trials (covering 1399 RCTs), we randomly selected 46 RCTs (23 per category). In addition, 3 experienced reviewers independently assessed all 46 RCTs using RoB2, recording assessment time for each trial. Reviewer judgments were reconciled through consensus. Furthermore, 6 RCTs (3 from each category) were randomly selected for prompt development and optimization. The remaining 40 trials established the internal validation standard, while Cochrane Reviews judgments served as external validation. Primary outcomes were extracted as reported in corresponding Cochrane Reviews. We calculated accuracy rates, Cohen \u03ba, and time differentials. RESULTS: We identified significant differences between Cochrane and reviewer judgments, particularly in domains 1, 4, and 5, likely due to different standards in assessing randomization and blinding. Among the 20 articles focusing on adhering, 18 Cochrane Reviews and 19 reviewer judgments classified them as \"High risk,\" while assignment-focused RCTs showed more heterogeneous risk distribution. Compared with Cochrane Reviews, LLMs demonstrated accuracy rates of 57.5% and 70% for overall (assignment) and overall (adhering), respectively. When compared with reviewer judgments, LLMs' accuracy rates were 65% and 70% for these domains. The average accuracy rates for the remaining 6 domains were 65.2% (95% CI 57.6-72.7) against Cochrane Reviews and 74.2% (95% CI 64.7-83.9) against reviewers. At the signaling question level, LLMs achieved 83.2% average accuracy (95% CI 77.5-88.9), with accuracy exceeding 70% for most questions except 2.4 (assignment), 2.5 (assignment), 3.3, and 3.4. When domain judgments were derived from LLM-generated signaling questions using the RoB2 algorithm rather than direct LLM domain judgments, accuracy improved substantially for Domain 2 (adhering; 55-95) and overall (adhering; 70-90). LLMs demonstrated high consistency between iterations (average 85.2%, 95% CI 85.15-88.79) and completed assessments in 1.9 minutes versus 31.5 minutes for human reviewers (mean difference 29.6, 95% CI 25.6-33.6 minutes). CONCLUSIONS: LLMs achieved commendable accuracy when guided by structured prompts, particularly through processing methodological details through structured reasoning. While not replacing human assessment, LLMs demonstrate strong potential for assisting RoB2 evaluations. Larger studies with improved prompting could enhance performance.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/70450",
      "authors": "Huang Jiajie et al.",
      "keywords": "artificial intelligence; efficiency; large language models; risk of bias 2; systematic review",
      "mesh_terms": "Randomized Controlled Trials as Topic; Humans; Language; Bias; Reproducibility of Results; Risk Assessment; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40554779/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12238788",
      "ft_text_length": 26887,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12238788)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "40784946",
      "title": "Evaluating gender bias in large language models in long-term care.",
      "abstract": "BACKGROUND: Large language models (LLMs) are being used to reduce the administrative burden in long-term care by automatically generating and summarising case notes. However, LLMs can reproduce bias in their training data. This study evaluates gender bias in summaries of long-term care records generated with two state-of-the-art, open-source LLMs released in 2024: Meta's Llama 3 and Google Gemma. METHODS: Gender-swapped versions were created of long-term care records for 617 older people from a London local authority. Summaries of male and female versions were generated with Llama 3 and Gemma, as well as benchmark models from Meta and Google released in 2019: T5 and BART. Counterfactual bias was quantified through sentiment analysis alongside an evaluation of word frequency and thematic patterns. RESULTS: The benchmark models exhibited some variation in output on the basis of gender. Llama 3 showed no gender-based differences across any metrics. Gemma displayed the most significant gender-based differences. Male summaries focus more on physical and mental health issues. Language used for men was more direct, with women's needs downplayed more often than men's. CONCLUSION: Care services are allocated on the basis of need. If women's health issues are underemphasised, this may lead to gender-based disparities in service receipt. LLMs may offer substantial benefits in easing administrative burden. However, the findings highlight the variation in state-of-the-art LLMs, and the need for evaluation of bias. The methods in this paper provide a practical framework for quantitative evaluation of gender bias in LLMs. The code is available on GitHub.",
      "journal": "BMC medical informatics and decision making",
      "year": "2025",
      "doi": "10.1186/s12911-025-03118-0",
      "authors": "Rickman Sam",
      "keywords": "Bias; Gender; LLMs; Long-term care",
      "mesh_terms": "Humans; Male; Female; Sexism; Long-Term Care; Aged; Language; Electronic Health Records; Aged, 80 and over; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40784946/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12337462",
      "ft_text_length": 98663,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12337462)",
      "ft_reason": "Included: bias central + approach content (6 indicators)"
    },
    {
      "pmid": "41044148",
      "title": "Validity of two subjective skin tone scales and its implications on healthcare model fairness.",
      "abstract": "Skin tone assessments are critical for fairness evaluation in healthcare algorithms (e.g., pulse oximetry) but lack validation. Using prospectively collected facial images from 90 hospitalized adults at the San Francisco VA, three independent annotators rated facial regions in triplicate using Fitzpatrick (I-VI) and Monk (1-10) skin tone scales. Patients also self-identified their skin tone. Annotator confidence was recorded using 5-point Likert scales. Across 810 images in 90 patients (9 images each), within-rater agreement was high, but inter-annotator agreement was moderate to low. Annotators frequently rated patients as darker when patients self-identified as lighter, and lighter when patients self-identified as darker. In linear mixed-effects models controlling for facial region and annotator confidence, darker self-reported skin tones were associated with lighter annotator scores. These findings highlight challenges in consistent skin tone labeling and suggest that current methods for assessing representation in biosensor-based algorithm studies may be influenced by labeling bias.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01975-7",
      "authors": "Cu Cassandra W et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41044148/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12494915",
      "ft_text_length": 25085,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12494915)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "41102216",
      "title": "Exploring biases related to the use of large language models in a multilingual depression corpus.",
      "abstract": "Recent advancements in Large Language Models (LLMs) present promising opportunities for applying these technologies to aid the detection and monitoring of Major Depressive Disorder. However, demographic biases in LLMs may present challenges in the extraction of key information, where concerns persist about whether these models perform equally well across diverse populations. This study investigates how demographic factors, specifically age and gender affect the performance of LLMs in classifying depression symptom severity across multilingual datasets. By systematically balancing and evaluating datasets in English, Spanish, and Dutch, we aim to uncover performance disparities linked to demographic representation and linguistic diversity. The findings from this work can directly inform the design and deployment of more equitable LLM-based screening systems. Gender had varying effects across models, whereas age consistently produced more pronounced differences in performance. Additionally, model accuracy varied noticeably across languages. This study emphasizes the need to incorporate demographic-aware models in health-related analyses. It raises awareness of the biases that may affect their application in mental health and suggests further research on methods to mitigate these biases and enhance model generalization.",
      "journal": "Scientific reports",
      "year": "2025",
      "doi": "10.1038/s41598-025-19980-x",
      "authors": "Perez-Toro Paula Andrea et al.",
      "keywords": "Biases in AI; Depression; Large language models; Text analysis",
      "mesh_terms": "Humans; Female; Male; Multilingualism; Adult; Middle Aged; Language; Major Depressive Disorder; Young Adult; Aged; Depression; Bias; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41102216/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12533096",
      "ft_text_length": 38381,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12533096)",
      "ft_reason": "Included: bias central + approach content (3 indicators)"
    },
    {
      "pmid": "41259327",
      "title": "Reinforcing intensive motherhood: A study of gender bias in parental responsibilities allocation by large language models.",
      "abstract": "This study investigated gender bias in Large Language Models (LLMs) within the context of parenting responsibility attribution, focusing on whether LLMs implicitly reinforce the ideology of \"intensive mothering\" by assigning caregiving duties predominantly to mothers. Using GPT-4.1 and DeepSeek-V3 as case studies, we used a 3-factor experimental design involving model type, caregiver role (mother, father, or neutral parent), and responsibility framing (prescriptive vs. descriptive). Results revealed an obvious gender bias across both models: mothers were consistently assigned highest caregiving responsibility scores, while fathers received the lowest. Moreover, LLMs produced higher responsibility scores in prescriptive contexts than in descriptive ones, suggesting a tendency to reflect normative social expectations. Mediation analysis showed that gender equality attitudes did not significantly explain these biases, indicating that LLMs' outputs were likely driven by contextual associations in training data rather than consistent ideological positioning. This study extended LLMs bias research into the domestic domain of childrearing, highlighting that even in private contexts, advanced language models tend to reproduce and amplify traditional gender norms. The findings underscored the urgency of incorporating gender sensitivity in LLMs design and training processes. Interventions such as fine-tuning and dataset balancing are essential to prevent these models from reinforcing gendered divisions of labor in parenting.",
      "journal": "PloS one",
      "year": "2025",
      "doi": "10.1371/journal.pone.0335706",
      "authors": "Xiu Jiaxing et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Male; Sexism; Parenting; Mothers; Language; Adult; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41259327/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12629417",
      "ft_text_length": 41966,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12629417)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    },
    {
      "pmid": "41353223",
      "title": "An investigation of race bias in deep learning-based segmentation of prostate MRI images.",
      "abstract": "Deep learning (DL) has been proposed for magnetic resonance imaging (MRI) prostate segmentation for various clinical tasks, including radiotherapy treatment planning. In other applications, DL models have exhibited performance bias by protected attributes such as race. To investigate possible race bias in prostate MRI segmentation, DL models were trained on five clinical T2-weighted MRI datasets with varying White/Black race imbalance, plus one public dataset with unknown races, and evaluated on 32 White/Black matched clinical subjects. For the models trained with differing levels of race imbalance, the best performance for both races was when the training set was race-balanced. A linear mixed-effects model analysis showed that Dice Similarity Coefficient (DSC) differences between Black and White subjects depended on race representation in the training data, with a slight reduction in White-Black performance gap as Black representation increased (p\u2009<\u20090.05). The model trained on public data showed no difference in performance between races for DSC. The findings reveal the potential for race bias in DL prostate MRI segmentation performance when training sets are highly imbalanced. We argue for transparency in race reporting in DL prostate segmentation training data and reporting of test performance across demographic groups, with appropriate ethical/legal safeguards.",
      "journal": "Scientific reports",
      "year": "2025",
      "doi": "10.1038/s41598-025-26189-5",
      "authors": "Alqarni Maram et al.",
      "keywords": "Bias; Deep learning; MRI; Prostate; Race; Radiotherapy; Segmentation",
      "mesh_terms": "Humans; Male; Middle Aged; Black or African American; Deep Learning; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Prostate; Prostatic Neoplasms; Racism; White",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41353223/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12689646",
      "ft_text_length": 30190,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12689646)",
      "ft_reason": "Included: bias is central topic (1 indicators)"
    },
    {
      "pmid": "41392298",
      "title": "Cutting-edge bayesian deep learning and statistical strategies for bias mitigation in COVID-19 detection via chest x-ray imaging.",
      "abstract": "Chest radiography (CXR is widely used for triage and follow-up of pulmonary disease, yet COVID-19 classification remains vulnerable to bias, label noise, and domain shift. We propose a multi-stage Bayesian deep learning framework that combines lung segmentation, segmentation-guided classification, calibrated ensembling, and uncertainty estimation to classify four classes (COVID-19, normal, viral pneumonia, bacterial pneumonia) and to grade COVID-19 severity. Models are trained and tested on 1,531 CXRs (100 COVID-19 images from 70 patients; 1,431 non-COVID images from ChestX-ray14) with patient-wise splits. The final ensemble achieves 98.33% test accuracy; COVID-19 sensitivity reaches 100% on this split. Robustness is quantified by stress-testing five image degradations (Gaussian noise, motion/defocus blur, JPEG compression, and downsampling), with macro AUC drops remaining small at moderate severities and larger under strong blur or heavy downsampling. Saliency and context-relevance analyses are used to identify spurious cues. The study is limited by dataset size and lack of external multi-site validation; a planned evaluation on COVIDx and BIMCV-COVID19\u2009+\u2009is outlined.",
      "journal": "Scientific reports",
      "year": "2025",
      "doi": "10.1038/s41598-025-28723-x",
      "authors": "Chen Yuanyuan et al.",
      "keywords": "Baysian deep learning; COVID-19 detection; Chest X-ray images; Pneumonia classification; Statisical analysis",
      "mesh_terms": "Humans; COVID-19; Deep Learning; Bayes Theorem; Radiography, Thoracic; SARS-CoV-2; Lung; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41392298/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12764913",
      "ft_text_length": 60644,
      "ft_status": "Full text screened \u2014 INCLUDED (PMC12764913)",
      "ft_reason": "Included: bias central + approach content (4 indicators)"
    }
  ],
  "ft_excluded": [
    {
      "pmid": "16943441",
      "title": "Configurational-bias sampling technique for predicting side-chain conformations in proteins.",
      "abstract": "Prediction of side-chain conformations is an important component of several biological modeling applications. In this work, we have developed and tested an advanced Monte Carlo sampling strategy for predicting side-chain conformations. Our method is based on a cooperative rearrangement of atoms that belong to a group of neighboring side-chains. This rearrangement is accomplished by deleting groups of atoms from the side-chains in a particular region, and regrowing them with the generation of trial positions that depends on both a rotamer library and a molecular mechanics potential function. This method allows us to incorporate flexibility about the rotamers in the library and explore phase space in a continuous fashion about the primary rotamers. We have tested our algorithm on a set of 76 proteins using the all-atom AMBER99 force field and electrostatics that are governed by a distance-dependent dielectric function. When the tolerance for correct prediction of the dihedral angles is a <20 degrees deviation from the native state, our prediction accuracies for chi1 are 83.3% and for chi1 and chi2 are 65.4%. The accuracies of our predictions are comparable to the best results in the literature that often used Hamiltonians that have been specifically optimized for side-chain packing. We believe that the continuous exploration of phase space enables our method to overcome limitations inherent with using discrete rotamers as trials.",
      "journal": "Protein science : a publication of the Protein Society",
      "year": "2006",
      "doi": "10.1110/ps.062165906",
      "authors": "Jain Tushar et al.",
      "keywords": "",
      "mesh_terms": "Bias; Computer Simulation; Databases, Factual; Models, Molecular; Monte Carlo Method; Predictive Value of Tests; Protein Conformation; Reproducibility of Results",
      "pub_types": "Evaluation Study; Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/16943441/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC2242598",
      "ft_text_length": 1447,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC2242598)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "17511519",
      "title": "A method to address differential bias in genotyping in large-scale association studies.",
      "abstract": "In a previous paper we have shown that, when DNA samples for cases and controls are prepared in different laboratories prior to high-throughput genotyping, scoring inaccuracies can lead to differential misclassification and, consequently, to increased false-positive rates. Different DNA sourcing is often unavoidable in large-scale disease association studies of multiple case and control sets. Here, we describe methodological improvements to minimise such biases. These fall into two categories: improvements to the basic clustering methods for identifying genotypes from fluorescence intensities, and use of \"fuzzy\" calls in association tests in order to make appropriate allowance for call uncertainty. We find that the main improvement is a modification of the calling algorithm that links the clustering of cases and controls while allowing for different DNA sourcing. We also find that, in the presence of different DNA sourcing, biases associated with missing data can increase the false-positive rate. Therefore, we propose the use of \"fuzzy\" calls to deal with uncertain genotypes that would otherwise be labeled as missing.",
      "journal": "PLoS genetics",
      "year": "2007",
      "doi": "10.1371/journal.pgen.0030074",
      "authors": "Plagnol Vincent et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Bias; Case-Control Studies; Computer Simulation; Databases, Genetic; Epidemiologic Methods; Female; Gene Frequency; Genetic Predisposition to Disease; Genotype; Humans; Male; Polymorphism, Single Nucleotide; United Kingdom",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/17511519/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC1868951",
      "ft_text_length": 34292,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC1868951)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "18407896",
      "title": "Validation of image segmentation by estimating rater bias and variance.",
      "abstract": "The accuracy and precision of segmentations of medical images has been difficult to quantify in the absence of a 'ground truth' or reference standard segmentation for clinical data. Although physical or digital phantoms can help by providing a reference standard, they do not allow the reproduction of the full range of imaging and anatomical characteristics observed in clinical data. An alternative assessment approach is to compare with segmentations generated by domain experts. Segmentations may be generated by raters who are trained experts or by automated image analysis algorithms. Typically, these segmentations differ due to intra-rater and inter-rater variability. The most appropriate way to compare such segmentations has been unclear. We present here a new algorithm to enable the estimation of performance characteristics, and a true labelling, from observations of segmentations of imaging data where segmentation labels may be ordered or continuous measures. This approach may be used with, among others, surface, distance transform or level-set representations of segmentations, and can be used to assess whether or not a rater consistently overestimates or underestimates the position of a boundary.",
      "journal": "Philosophical transactions. Series A, Mathematical, physical, and engineering sciences",
      "year": "2008",
      "doi": "10.1098/rsta.2008.0040",
      "authors": "Warfield Simon K et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Analysis of Variance; Bayes Theorem; Bias; Biometry; Brain Neoplasms; Expert Testimony; Humans; Image Processing, Computer-Assisted; Likelihood Functions; Magnetic Resonance Imaging; Models, Statistical; Phantoms, Imaging",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/18407896/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC3227147",
      "ft_text_length": 22646,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC3227147)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "18602242",
      "title": "Communicating side effect risks in a tamoxifen prophylaxis decision aid: the debiasing influence of pictographs.",
      "abstract": "OBJECTIVE: To experimentally test whether using pictographs (image matrices), incremental risk formats, and varied risk denominators would influence perceptions and comprehension of side effect risks in an online decision aid about prophylactic use of tamoxifen to prevent primary breast cancers. METHODS: We recruited 631 women with elevated breast cancer risk from two healthcare organizations. Participants saw tailored estimates of the risks of 5 side effects: endometrial cancer, blood clotting, cataracts, hormonal symptoms, and sexual problems. Presentation format was randomly varied in a three factor design: (A) risk information was displayed either in pictographs or numeric text; (B) presentations either reported total risks with and without tamoxifen or highlighted the incremental risk most relevant for decision making; and (C) risk estimates used 100 or 1000 person denominators. Primary outcome measures included risk perceptions and gist knowledge. RESULTS: Incremental risk formats consistently lowered perceived risk of side effects but resulted in low knowledge when displayed by numeric text only. Adding pictographs, however, produced significantly higher comprehension levels. CONCLUSIONS: Pictographs make risk statistics easier to interpret, reducing biases associated with incremental risk presentations. PRACTICE IMPLICATIONS: Including graphs in risk communications is essential to support an informed treatment decision-making process.",
      "journal": "Patient education and counseling",
      "year": "2008",
      "doi": "10.1016/j.pec.2008.05.010",
      "authors": "Zikmund-Fisher Brian J et al.",
      "keywords": "",
      "mesh_terms": "Adult; Aged; Audiovisual Aids; Breast Neoplasms; Decision Support Techniques; Female; Health Knowledge, Attitudes, Practice; Humans; Internet; Michigan; Middle Aged; Multivariate Analysis; Patient Education as Topic; Risk Assessment; Tamoxifen; Washington",
      "pub_types": "Journal Article; Multicenter Study; Randomized Controlled Trial; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/18602242/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC2649664",
      "ft_text_length": 1469,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC2649664)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "21204122",
      "title": "Using latent variable modeling and multiple imputation to calibrate rater bias in diagnosis assessment.",
      "abstract": "We present an approach that uses latent variable modeling and multiple imputation to correct rater bias when one group of raters tends to be more lenient in assigning a diagnosis than another. Our method assumes that there exists an unobserved moderate category of patient who is assigned a positive diagnosis by one type of rater and a negative diagnosis by the other type. We present a Bayesian random effects censored ordinal probit model that allows us to calibrate the diagnoses across rater types by identifying and multiply imputing 'case' or 'non-case' status for patients in the moderate category. A Markov chain Monte Carlo algorithm is presented to estimate the posterior distribution of the model parameters and generate multiple imputations. Our method enables the calibrated diagnosis variable to be used in subsequent analyses while also preserving uncertainty in true diagnosis. We apply our model to diagnoses of posttraumatic stress disorder (PTSD) from a depression study where nurse practitioners were twice as likely as clinical psychologists to diagnose PTSD despite the fact that participants were randomly assigned to either a nurse or a psychologist. Our model appears to balance PTSD rates across raters, provides a good fit to the data, and preserves between-rater variability. After calibrating the diagnoses of PTSD across rater types, we perform an analysis looking at the effects of comorbid PTSD on changes in depression scores over time. Results are compared with an analysis that uses the original diagnoses and show that calibrating the PTSD diagnoses can yield different inferences.",
      "journal": "Statistics in medicine",
      "year": "2011",
      "doi": "10.1002/sim.4109",
      "authors": "Siddique Juned et al.",
      "keywords": "",
      "mesh_terms": "Adult; Algorithms; Anxiety; Depression; Diagnosis, Differential; Female; Humans; Markov Chains; Models, Statistical; Nurse Practitioners; Observer Variation; Stress Disorders, Post-Traumatic",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/21204122/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC3058328",
      "ft_text_length": 1613,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC3058328)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "21385160",
      "title": "Buckley-James-type estimator with right-censored and length-biased data.",
      "abstract": "We present a natural generalization of the Buckley-James-type estimator for traditional survival data to right-censored length-biased data under the accelerated failure time (AFT) model. Length-biased data are often encountered in prevalent cohort studies and cancer screening trials. Informative right censoring induced by length-biased sampling creates additional challenges in modeling the effects of risk factors on the unbiased failure times for the target population. In this article, we evaluate covariate effects on the failure times of the target population under the AFT model given the observed length-biased data. We construct a Buckley-James-type estimating equation, develop an iterative computing algorithm, and establish the asymptotic properties of the estimators. We assess the finite-sample properties of the proposed estimators against the estimators obtained from the existing methods. Data from a prevalent cohort study of patients with dementia are used to illustrate the proposed methodology.",
      "journal": "Biometrics",
      "year": "2011",
      "doi": "10.1111/j.1541-0420.2011.01568.x",
      "authors": "Ning Jing et al.",
      "keywords": "",
      "mesh_terms": "Bias; Canada; Data Interpretation, Statistical; Dementia; Humans; Proportional Hazards Models; Survival Analysis; Survival Rate",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/21385160/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC3137763",
      "ft_text_length": 1037,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC3137763)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "22719749",
      "title": "Reducing bias of allele frequency estimates by modeling SNP genotype data with informative missingness.",
      "abstract": "The presence of missing single-nucleotide polymorphism (SNP) genotypes is common in genetic studies. For studies with low-density SNPs, the most commonly used approach to dealing with genotype missingness is to simply remove the observations with missing genotypes from the analyses. This na\u00efve method is straightforward but is valid only when the missingness is random. However, a given assay often has a different capability in genotyping heterozygotes and homozygotes, causing the phenomenon of \"differential dropout\" in the sense that the missing rates of heterozygotes and homozygotes are different. In practice, differential dropout among genotypes exists in even carefully designed studies, such as the data from the HapMap project and the Wellcome Trust Case Control Consortium. Under the assumption of Hardy-Weinberg equilibrium and no genotyping error, we here propose a statistical method to model the differential dropout among different genotypes. Compared with the na\u00efve method, our method provides more accurate allele frequency estimates when the differential dropout is present. To demonstrate its practical use, we further apply our method to the HapMap data and a scleroderma data set.",
      "journal": "Frontiers in genetics",
      "year": "2012",
      "doi": "10.3389/fgene.2012.00107",
      "authors": "Lin Wan-Yu et al.",
      "keywords": "EM algorithm; allele frequency; genotype; informative missingness; missing at random; single-nucleotide polymorphism",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/22719749/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC3376470",
      "ft_text_length": 22275,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC3376470)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "23997761",
      "title": "Localized FCM Clustering with Spatial Information for Medical Image Segmentation and Bias Field Estimation.",
      "abstract": "This paper presents a novel fuzzy energy minimization method for simultaneous segmentation and bias field estimation of medical images. We first define an objective function based on a localized fuzzy c-means (FCM) clustering for the image intensities in a neighborhood around each point. Then, this objective function is integrated with respect to the neighborhood center over the entire image domain to formulate a global fuzzy energy, which depends on membership functions, a bias field that accounts for the intensity inhomogeneity, and the constants that approximate the true intensities of the corresponding tissues. Therefore, segmentation and bias field estimation are simultaneously achieved by minimizing the global fuzzy energy. Besides, to reduce the impact of noise, the proposed algorithm incorporates spatial information into the membership function using the spatial function which is the summation of the membership functions in the neighborhood of each pixel under consideration. Experimental results on synthetic and real images are given to demonstrate the desirable performance of the proposed algorithm.",
      "journal": "International journal of biomedical imaging",
      "year": "2013",
      "doi": "10.1155/2013/930301",
      "authors": "Cui Wenchao et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/23997761/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC3749607",
      "ft_text_length": 24283,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC3749607)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "24320536",
      "title": "Breast density quantification using magnetic resonance imaging (MRI) with bias field correction: a postmortem study.",
      "abstract": "PURPOSE: Quantification of breast density based on three-dimensional breast MRI may provide useful information for the early detection of breast cancer. However, the field inhomogeneity can severely challenge the computerized image segmentation process. In this work, the effect of the bias field in breast density quantification has been investigated with a postmortem study. METHODS: T1-weighted images of 20 pairs of postmortem breasts were acquired on a 1.5 T breast MRI scanner. Two computer-assisted algorithms were used to quantify the volumetric breast density. First, standard fuzzy c-means (FCM) clustering was used on raw images with the bias field present. Then, the coherent local intensity clustering (CLIC) method estimated and corrected the bias field during the iterative tissue segmentation process. Finally, FCM clustering was performed on the bias-field-corrected images produced by CLIC method. The left-right correlation for breasts in the same pair was studied for both segmentation algorithms to evaluate the precision of the tissue classification. Finally, the breast densities measured with the three methods were compared to the gold standard tissue compositions obtained from chemical analysis. The linear correlation coefficient, Pearson's r, was used to evaluate the two image segmentation algorithms and the effect of bias field. RESULTS: The CLIC method successfully corrected the intensity inhomogeneity induced by the bias field. In left-right comparisons, the CLIC method significantly improved the slope and the correlation coefficient of the linear fitting for the glandular volume estimation. The left-right breast density correlation was also increased from 0.93 to 0.98. When compared with the percent fibroglandular volume (%FGV) from chemical analysis, results after bias field correction from both the CLIC the FCM algorithms showed improved linear correlation. As a result, the Pearson's r increased from 0.86 to 0.92 with the bias field correction. CONCLUSIONS: The investigated CLIC method significantly increased the precision and accuracy of breast density quantification using breast MRI images by effectively correcting the bias field. It is expected that a fully automated computerized algorithm for breast density quantification may have great potential in clinical MRI applications.",
      "journal": "Medical physics",
      "year": "2013",
      "doi": "10.1118/1.4831967",
      "authors": "Ding Huanjun et al.",
      "keywords": "",
      "mesh_terms": "Autopsy; Breast; Female; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Organ Size",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/24320536/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC3862600",
      "ft_text_length": 2343,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC3862600)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "24361666",
      "title": "Cortical surface-based analysis reduces bias and variance in kinetic modeling of brain PET data.",
      "abstract": "Exploratory (i.e., voxelwise) spatial methods are commonly used in neuroimaging to identify areas that show an effect when a region-of-interest (ROI) analysis cannot be performed because no strong a priori anatomical hypothesis exists. However, noise at a single voxel is much higher than noise in a ROI making noise management critical to successful exploratory analysis. This work explores how preprocessing choices affect the bias and variability of voxelwise kinetic modeling analysis of brain positron emission tomography (PET) data. These choices include the use of volume- or cortical surface-based smoothing, level of smoothing, use of voxelwise partial volume correction (PVC), and PVC masking threshold. PVC was implemented using the Muller-Gartner method with the masking out of voxels with low gray matter (GM) partial volume fraction. Dynamic PET scans of an antagonist serotonin-4 receptor radioligand ([(11)C]SB207145) were collected on sixteen healthy subjects using a Siemens HRRT PET scanner. Kinetic modeling was used to compute maps of non-displaceable binding potential (BPND) after preprocessing. The results showed a complicated interaction between smoothing, PVC, and masking on BPND estimates. Volume-based smoothing resulted in large bias and intersubject variance because it smears signal across tissue types. In some cases, PVC with volume smoothing paradoxically caused the estimated BPND to be less than when no PVC was used at all. When applied in the absence of PVC, cortical surface-based smoothing resulted in dramatically less bias and the least variance of the methods tested for smoothing levels 5mm and higher. When used in combination with PVC, surface-based smoothing minimized the bias without significantly increasing the variance. Surface-based smoothing resulted in 2-4 times less intersubject variance than when volume smoothing was used. This translates into more than 4 times fewer subjects needed in a group analysis to achieve similarly powered statistical tests. Surface-based smoothing has less bias and variance because it respects cortical geometry by smoothing the PET data only along the cortical ribbon and so does not contaminate the GM signal with that of white matter and cerebrospinal fluid. The use of surface-based analysis in PET should result in substantial improvements in the reliability and detectability of effects in exploratory PET analysis, with or without PVC.",
      "journal": "NeuroImage",
      "year": "2014",
      "doi": "10.1016/j.neuroimage.2013.12.021",
      "authors": "Greve Douglas N et al.",
      "keywords": "",
      "mesh_terms": "Adult; Algorithms; Artifacts; Cerebral Cortex; Computer Simulation; Female; Humans; Image Enhancement; Kinetics; Male; Metabolic Clearance Rate; Models, Biological; Piperidines; Positron-Emission Tomography; Radiopharmaceuticals; Receptors, Serotonin, 5-HT4; Reproducibility of Results; Sensitivity and Specificity; Tissue Distribution; Young Adult",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/24361666/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC4008670",
      "ft_text_length": 2441,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC4008670)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "25316533",
      "title": "Posttreatment attrition and its predictors, attrition bias, and treatment efficacy of the anxiety online programs.",
      "abstract": "BACKGROUND: Although relatively new, the field of e-mental health is becoming more popular with more attention given to researching its various aspects. However, there are many areas that still need further research, especially identifying attrition predictors at various phases of assessment and treatment delivery. OBJECTIVE: The present study identified the predictors of posttreatment assessment completers based on 24 pre- and posttreatment demographic and personal variables and 1 treatment variable, their impact on attrition bias, and the efficacy of the 5 fully automated self-help anxiety treatment programs for generalized anxiety disorder (GAD), social anxiety disorder (SAD), panic disorder with or without agoraphobia (PD/A), obsessive-compulsive disorder (OCD), and posttraumatic stress disorder (PTSD). METHODS: A complex algorithm was used to diagnose participants' mental disorders based on the criteria of the Diagnostic and Statistical Manual of Mental Disorders (Fourth Edition, Text Revision; DSM-IV-TR). Those who received a primary or secondary diagnosis of 1 of 5 anxiety disorders were offered an online 12-week disorder-specific treatment program. A total of 3199 individuals did not formally drop out of the 12-week treatment cycle, whereas 142 individuals formally dropped out. However, only 347 participants who completed their treatment cycle also completed the posttreatment assessment measures. Based on these measures, predictors of attrition were identified and attrition bias was examined. The efficacy of the 5 treatment programs was assessed based on anxiety-specific severity scores and 5 additional treatment outcome measures. RESULTS: On average, completers of posttreatment assessment measures were more likely to be seeking self-help online programs; have heard about the program from traditional media or from family and friends; were receiving mental health assistance; were more likely to learn best by reading, hearing and doing; had a lower pretreatment Kessler-6 total score; and were older in age. Predicted probabilities resulting from these attrition variables displayed no significant attrition bias using Heckman's method and thus allowing for the use of completer analysis. Six treatment outcome measures (Kessler-6 total score, number of diagnosed disorders, self-confidence in managing mental health issues, quality of life, and the corresponding pre- and posttreatment severity for each program-specific anxiety disorder and for major depressive episode) were used to assess the efficacy of the 5 anxiety treatment programs. Repeated measures MANOVA revealed a significant multivariate time effect for all treatment outcome measures for each treatment program. Follow-up repeated measures ANOVAs revealed significant improvements on all 6 treatment outcome measures for GAD and PTSD, 5 treatment outcome measures were significant for SAD and PD/A, and 4 treatment outcome measures were significant for OCD. CONCLUSIONS: Results identified predictors of posttreatment assessment completers and provided further support for the efficacy of self-help online treatment programs for the 5 anxiety disorders. TRIAL REGISTRATION: Australian and New Zealand Clinical Trials Registry ACTRN121611000704998; http://www.anzctr.org.au/trial_view.aspx?ID=336143 (Archived by WebCite at http://www.webcitation.org/618r3wvOG).",
      "journal": "Journal of medical Internet research",
      "year": "2014",
      "doi": "10.2196/jmir.3513",
      "authors": "Al-Asadi Ali M et al.",
      "keywords": "Internet interventions; Web treatment; cognitive behavioral therapy; e-mental health; fully automated; generalized anxiety disorder; obsessive compulsive disorder; online therapy; posttreatment attrition; posttreatment predictors; self-help; treatment efficacy",
      "mesh_terms": "Adolescent; Adult; Aged; Aged, 80 and over; Anxiety; Bias; Female; Humans; Internet; Male; Middle Aged; Models, Psychological; Outcome Assessment, Health Care; Quality of Life; Randomized Controlled Trials as Topic; Treatment Outcome; Young Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/25316533/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC4211028",
      "ft_text_length": 48180,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC4211028)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "25609791",
      "title": "Bias in microRNA functional enrichment analysis.",
      "abstract": "MOTIVATION: Many studies have investigated the differential expression of microRNAs (miRNAs) in disease states and between different treatments, tissues and developmental stages. Given a list of perturbed miRNAs, it is common to predict the shared pathways on which they act. The standard test for functional enrichment typically yields dozens of significantly enriched functional categories, many of which appear frequently in the analysis of apparently unrelated diseases and conditions. RESULTS: We show that the most commonly used functional enrichment test is inappropriate for the analysis of sets of genes targeted by miRNAs. The hypergeometric distribution used by the standard method consistently results in significant P-values for functional enrichment for targets of randomly selected miRNAs, reflecting an underlying bias in the predicted gene targets of miRNAs as a whole. We developed an algorithm to measure enrichment using an empirical sampling approach, and applied this in a reanalysis of the gene ontology classes of targets of miRNA lists from 44 published studies. The vast majority of the miRNA target sets were not significantly enriched in any functional category after correction for bias. We therefore argue against continued use of the standard functional enrichment method for miRNA targets.",
      "journal": "Bioinformatics (Oxford, England)",
      "year": "2015",
      "doi": "10.1093/bioinformatics/btv023",
      "authors": "Bleazard Thomas et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Computational Biology; Gene Expression Regulation; Humans; MicroRNAs; Molecular Sequence Annotation; Sequence Analysis, RNA",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/25609791/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC4426843",
      "ft_text_length": 28700,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC4426843)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "26232237",
      "title": "Diagnostic biases in translational bioinformatics.",
      "abstract": "BACKGROUND: With the surge of translational medicine and computational omics research, complex disease diagnosis is more and more relying on massive omics data-driven molecular signature detection. However, how to detect and prevent possible diagnostic biases in translational bioinformatics remains an unsolved problem despite its importance in the coming era of personalized medicine. METHODS: In this study, we comprehensively investigate the diagnostic bias problem by analyzing benchmark gene array, protein array, RNA-Seq and miRNA-Seq data under the framework of support vector machines for different model selection methods. We further categorize the diagnostic biases into different types by conducting rigorous kernel matrix analysis and provide effective machine learning methods to conquer the diagnostic biases. RESULTS: In this study, we comprehensively investigate the diagnostic bias problem by analyzing benchmark gene array, protein array, RNA-Seq and miRNA-Seq data under the framework of support vector machines. We have found that the diagnostic biases happen for data with different distributions and SVM with different kernels. Moreover, we identify total three types of diagnostic biases: overfitting bias, label skewness bias, and underfitting bias in SVM diagnostics, and present corresponding reasons through rigorous analysis. Compared with the overfitting and underfitting biases, the label skewness bias is more challenging to detect and conquer because it can be easily confused as a normal diagnostic case from its deceptive accuracy. To tackle this problem, we propose a derivative component analysis based support vector machines to conquer the label skewness bias by achieving the rivaling clinical diagnostic results. CONCLUSIONS: Our studies demonstrate that the diagnostic biases are mainly caused by the three major factors, i.e. kernel selection, signal amplification mechanism in high-throughput profiling, and training data label distribution. Moreover, the proposed DCA-SVM diagnosis provides a generic solution for the label skewness bias overcome due to the powerful feature extraction capability from derivative component analysis. Our work identifies and solves an important but less addressed problem in translational research. It also has a positive impact on machine learning for adding new results to kernel-based learning for omics data.",
      "journal": "BMC medical genomics",
      "year": "2015",
      "doi": "10.1186/s12920-015-0116-y",
      "authors": "Han Henry",
      "keywords": "",
      "mesh_terms": "Algorithms; Bias; Brain Neoplasms; Computational Biology; Diagnosis; Glioma; Humans; MicroRNAs; Monte Carlo Method; Neoplasm Grading; Oligonucleotide Array Sequence Analysis; Phenotype; Protein Array Analysis; Sequence Analysis, RNA; Support Vector Machine; Translational Research, Biomedical",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/26232237/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC4522082",
      "ft_text_length": 77337,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC4522082)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "26256455",
      "title": "A joint latent class analysis for adjusting survival bias with application to a trauma transfusion study.",
      "abstract": "There is no clear classification rule to rapidly identify trauma patients who are severely hemorrhaging and may need substantial blood transfusions. Massive transfusion (MT), defined as the transfusion of at least 10 units of red blood cells within 24\u00a0h of hospital admission, has served as a conventional surrogate that has been used to develop early predictive algorithms and establish criteria for ordering an MT protocol from the blood bank. However, the conventional MT rule is a poor proxy, because it is likely to misclassify many severely hemorrhaging trauma patients as they could die before receiving the 10th red blood cells transfusion. In this article, we propose to use a latent class model to obtain a more accurate and complete metric in the presence of early death. Our new approach incorporates baseline patient information from the time of hospital admission, by combining respective models for survival time and usage of blood products transfused within the framework of latent class analysis. To account for statistical challenges, caused by induced dependent censoring inherent in 24-h sums of transfusions, we propose to estimate an improved standard via a pseudo-likelihood function using an expectation-maximization algorithm with the inverse weighting principle. We evaluated the performance of our new standard in simulation studies and compared with the conventional MT definition using actual patient data from the Prospective Observational Multicenter Major Trauma Transfusion study. Copyright \u00a9 2015 John Wiley & Sons, Ltd.",
      "journal": "Statistics in medicine",
      "year": "2016",
      "doi": "10.1002/sim.6615",
      "authors": "Ning Jing et al.",
      "keywords": "EM algorithm; induced dependent censoring; inverse weighting principle; latent class model; massive transfusion",
      "mesh_terms": "Algorithms; Bias; Biostatistics; Blood Transfusion; Computer Simulation; Hemorrhage; Humans; Kaplan-Meier Estimate; Likelihood Functions; Logistic Models; Survival Analysis; Wounds and Injuries",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/26256455/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC4715697",
      "ft_text_length": 1513,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC4715697)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "26639183",
      "title": "Modeling X Chromosome Data Using Random Forests: Conquering Sex Bias.",
      "abstract": "Machine learning methods, including Random Forests (RF), are increasingly used for genetic data analysis. However, the standard RF algorithm does not correctly model the effects of X chromosome single nucleotide polymorphisms (SNPs), leading to biased estimates of variable importance. We propose extensions of RF to correctly model X SNPs, including a stratified approach and an approach based on the process of X chromosome inactivation. We applied the new and standard RF approaches to case-control alcohol dependence data from the Study of Addiction: Genes and Environment (SAGE), and compared the performance of the alternative approaches via a simulation study. Standard RF applied to a case-control study of alcohol dependence yielded inflated variable importance estimates for X SNPs, even when sex was included as a variable, but the results of the new RF methods were consistent with univariate regression-based approaches that correctly model X chromosome data. Simulations showed that the new RF methods eliminate the bias in standard RF variable importance for X SNPs when sex is associated with the trait, and are able to detect causal autosomal and X SNPs. Even in the absence of sex effects, the new extensions perform similarly to standard RF. Thus, we provide a powerful multimarker approach for genetic analysis that accommodates X chromosome data in an unbiased way. This method is implemented in the freely available R package \"snpRF\" (http://www.cran.r-project.org/web/packages/snpRF/).",
      "journal": "Genetic epidemiology",
      "year": "2016",
      "doi": "10.1002/gepi.21946",
      "authors": "Winham Stacey J et al.",
      "keywords": "Random Forest; X chromosome; bias; sex differences; variable importance",
      "mesh_terms": "Alcoholism; Algorithms; Bias; Case-Control Studies; Chromosomes, Human, X; Computer Simulation; Data Interpretation, Statistical; Decision Trees; Genetic Markers; Genetic Predisposition to Disease; Humans; Models, Genetic; Phenotype; Polymorphism, Single Nucleotide; Sex Factors",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/26639183/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC4724236",
      "ft_text_length": 1511,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC4724236)",
      "ft_reason": "Not health-related in full text"
    },
    {
      "pmid": "27092241",
      "title": "ve-SEQ: Robust, unbiased enrichment for streamlined detection and whole-genome sequencing of HCV and other highly diverse pathogens.",
      "abstract": "The routine availability of high-depth virus sequence data would allow the sensitive detection of resistance-associated variants that can jeopardize HIV or hepatitis C virus (HCV) treatment. We introduce ve-SEQ, a high-throughput method for sequence-specific enrichment and characterization of whole-virus genomes at up to 20% divergence from a reference sequence and 1,000-fold greater sensitivity than direct sequencing. The extreme genetic diversity of HCV led us to implement an algorithm for the efficient design of panels of oligonucleotide probes to capture any sequence among a defined set of targets without detectable bias. ve-SEQ enables efficient detection and sequencing of any HCV genome, including mixtures and intra-host variants, in a single experiment, with greater tolerance of sequence diversity than standard amplification methods and greater sensitivity than metagenomic sequencing, features that are directly applicable to other pathogens or arbitrary groups of target organisms, allowing the combination of sensitive detection with sequencing in many settings.",
      "journal": "F1000Research",
      "year": "2015",
      "doi": "10.12688/f1000research.7111.1",
      "authors": "Bonsall David et al.",
      "keywords": "Anti-viral resistance; Hepatitis C virus; Sequence capture and enrichment; Virus genome sequencing",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/27092241/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC4821293",
      "ft_text_length": 28856,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC4821293)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "28525542",
      "title": "Semiparametric model and inference for spontaneous abortion data with a cured proportion and biased sampling.",
      "abstract": "Evaluating and understanding the risk and safety of using medications for autoimmune disease in a woman during her pregnancy will help both clinicians and pregnant women to make better treatment decisions. However, utilizing spontaneous abortion (SAB) data collected in observational studies of pregnancy to derive valid inference poses two major challenges. First, the data from the observational cohort are not random samples of the target population due to the sampling mechanism. Pregnant women with early SAB are more likely to be excluded from the cohort, and there may be substantial differences between the observed SAB time and those in the target population. Second, the observed data are heterogeneous and contain a \"cured\" proportion. In this article, we consider semiparametric models to simultaneously estimate the probability of being cured and the distribution of time to SAB for the uncured subgroup. To derive the maximum likelihood estimators, we appropriately adjust the sampling bias in the likelihood function and develop an expectation-maximization algorithm to overcome the computational challenge. We apply the empirical process theory to prove the consistency and asymptotic normality of the estimators. We examine the finite sample performance of the proposed estimators in simulation studies and illustrate the proposed method through an application to SAB data from pregnant women.",
      "journal": "Biostatistics (Oxford, England)",
      "year": "2018",
      "doi": "10.1093/biostatistics/kxx024",
      "authors": "Piao Jin et al.",
      "keywords": "Biased sampling; Cure rate model; EM algorithm; Left truncation",
      "mesh_terms": "Abortion, Spontaneous; Adult; Algorithms; Female; Humans; Likelihood Functions; Models, Statistical; Pregnancy; Selection Bias",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/28525542/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC5862342",
      "ft_text_length": 1418,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC5862342)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "29040596",
      "title": "High-fidelity phenotyping: richness and freedom from bias.",
      "abstract": "Electronic health record phenotyping is the use of raw electronic health record data to assert characterizations about patients. Researchers have been doing it since the beginning of biomedical informatics, under different names. Phenotyping will benefit from an increasing focus on fidelity, both in the sense of increasing richness, such as measured levels, degree or severity, timing, probability, or conceptual relationships, and in the sense of reducing bias. Research agendas should shift from merely improving binary assignment to studying and improving richer representations. The field is actively researching new temporal directions and abstract representations, including deep learning. The field would benefit from research in nonlinear dynamics, in combining mechanistic models with empirical data, including data assimilation, and in topology. The health care process produces substantial bias, and studying that bias explicitly rather than treating it as merely another source of noise would facilitate addressing it.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2018",
      "doi": "10.1093/jamia/ocx110",
      "authors": "Hripcsak George et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29040596/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7282504",
      "ft_text_length": 19325,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7282504)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "29104450",
      "title": "Optimizing Variance-Bias Trade-off in the TWANG Package for Estimation of Propensity Scores.",
      "abstract": "While propensity score weighting has been shown to reduce bias in treatment effect estimation when selection bias is present, it has also been shown that such weighting can perform poorly if the estimated propensity score weights are highly variable. Various approaches have been proposed which can reduce the variability of the weights and the risk of poor performance, particularly those based on machine learning methods. In this study, we closely examine approaches to fine-tune one machine learning technique (generalized boosted models [GBM]) to select propensity scores that seek to optimize the variance-bias trade-off that is inherent in most propensity score analyses. Specifically, we propose and evaluate three approaches for selecting the optimal number of trees for the GBM in the twang package in R. Normally, the twang package in R iteratively selects the optimal number of trees as that which maximizes balance between the treatment groups being considered. Because the selected number of trees may lead to highly variable propensity score weights, we examine alternative ways to tune the number of trees used in the estimation of propensity score weights such that we sacrifice some balance on the pre-treatment covariates in exchange for less variable weights. We use simulation studies to illustrate these methods and to describe the potential advantages and disadvantages of each method. We apply these methods to two case studies: one examining the effect of dog ownership on the owner's general health using data from a large, population-based survey in California, and a second investigating the relationship between abstinence and a long-term economic outcome among a sample of high-risk youth.",
      "journal": "Health services & outcomes research methodology",
      "year": "2017",
      "doi": "10.1007/s10742-016-0168-2",
      "authors": "Parast Layla et al.",
      "keywords": "causal inference; machine learning; propensity score",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29104450/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC5667923",
      "ft_text_length": 1723,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC5667923)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "30267539",
      "title": "Semiparametric regression analysis of length-biased interval-censored data.",
      "abstract": "In prevalent cohort design, subjects who have experienced an initial event but not the failure event are preferentially enrolled and the observed failure times are often length-biased. Moreover, the prospective follow-up may not be continuously monitored and failure times are subject to interval censoring. We study the nonparametric maximum likelihood estimation for the proportional hazards model with length-biased interval-censored data. Direct maximization of likelihood function is intractable, thus we develop a computationally simple and stable expectation-maximization algorithm through introducing two layers of data augmentation. We establish the strong consistency, asymptotic normality and efficiency of the proposed estimator and provide an inferential procedure through profile likelihood. We assess the performance of the proposed methods through extensive simulations and apply the proposed methods to the Massachusetts Health Care Panel Study.",
      "journal": "Biometrics",
      "year": "2019",
      "doi": "10.1111/biom.12970",
      "authors": "Gao Fei et al.",
      "keywords": "Nonparametric maximum likelihood estimation; left truncation; proportional hazards model; semiparametric efficiency",
      "mesh_terms": "Activities of Daily Living; Aged; Aged, 80 and over; Algorithms; Bias; Computer Simulation; Data Interpretation, Statistical; Humans; Likelihood Functions; Proportional Hazards Models; Regression Analysis; Survival Analysis",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30267539/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8614128",
      "ft_text_length": 962,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8614128)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "30362919",
      "title": "Evaluating the Presence of Cognitive Biases in Health Care Decision Making: A Survey of U.S. Formulary Decision Makers.",
      "abstract": "BACKGROUND: Behavioral economics is a field of economics that draws on insights from psychology to understand and identify patterns of decision making. Cognitive biases are psychological tendencies to process information in predictable patterns that result in deviations from rational decision making. Previous research has not evaluated the influence of cognitive biases on decision making in a managed care setting. OBJECTIVE: To assess the presence of cognitive biases in formulary decision making. METHODS: An online survey was conducted with a panel of U.S. pharmacy and medical directors who worked at managed care organizations and served on pharmacy and therapeutics committees. Survey questions assessed 4 cognitive biases: relative versus absolute framing effect, risk aversion, zero-risk bias, and delay discounting. Simulated data were presented in various scenarios related to adverse event profiles, drug safety and efficacy, and drug pricing for new hypothetical oncology products. Survey questions prompted participants to select a preferred drug based on the information provided. Survey answers were analyzed to identify decision patterns that could be explained by the cognitive biases. Likelihood of bias was analyzed via chi-square tests for framing effect, risk aversion, and zero-risk bias. The delay discounting section used a published algorithm to characterize discounting patterns. RESULTS: A total of 35 pharmacy directors and 19 medical directors completed the survey. In the framing effect section, 80% of participants selected the suboptimal choice in the relative risk frame, compared with 38.9% in the absolute risk frame (P < 0.0001). When assessing risk aversion, 42.6% and 61.1% of participants displayed risk aversion in the cost- and efficacy-based scenarios, respectively, but these were not statistically significant (P = 0.27 and P = 0.10, respectively). In the zero-risk bias section, results from each scenario diverged. In the first zero-risk bias scenario, 90.7% of participants selected the drug with zero risk (P < 0.001), but in the second scenario, only 32.1% chose the zero-risk option (P < 0.01). In the section assessing delay discounting, 54% of survey participants favored a larger delayed rebate over a smaller immediate discount. A shallow delay discounting curve was produced, which indicated participants discounted delayed rewards to a minimal degree. CONCLUSIONS: Pharmacy and medical directors, like other decision makers, appear to be susceptible to some cognitive biases. Directors demonstrated a tendency to underestimate risks when they were presented in relative risk terms but made more accurate appraisals when information was presented in absolute risk terms. Delay discounting also may be applicable to directors when choosing immediate discounts over delayed rebates. However, directors neither displayed a statistically significant bias for risk aversion when assessing scenarios related to drug pricing or clinical efficacy nor were there significant conclusions for zero-risk biases. Further research with larger samples using real-world health care decisions is necessary to validate these findings. DISCLOSURES: This research was funded by Xcenda. Mezzio, Nguyen, and O'Day are employees of Xcenda. Kiselica was employed by Xcenda at the time the study was conducted. The authors have nothing to disclose. A portion of the preliminary data was presented as posters at the 2017 AMCP Managed Care & Specialty Pharmacy Annual Meeting; March 27-30, 2017; in Denver, CO, and the 2017 International Society for Pharmacoeconomics and Outcomes Research 22nd Annual International Meeting; May 20-24, 2017; in Boston, MA.",
      "journal": "Journal of managed care & specialty pharmacy",
      "year": "2018",
      "doi": "10.18553/jmcp.2018.24.11.1173",
      "authors": "Mezzio Dylan J et al.",
      "keywords": "",
      "mesh_terms": "Cognition; Decision Making; Economics, Pharmaceutical; Humans; Likelihood Functions; Managed Care Programs; Outcome Assessment, Health Care; Pharmacy; Physician Executives; Prejudice; Risk Assessment; Surveys and Questionnaires",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30362919/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10397589",
      "ft_text_length": 3184,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10397589)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "30508424",
      "title": "Ensuring Fairness in Machine Learning to Advance Health Equity.",
      "abstract": "Machine learning is used increasingly in clinical care to improve diagnosis, treatment selection, and health system efficiency. Because machine-learning models learn from historically collected data, populations that have experienced human and structural biases in the past-called protected groups-are vulnerable to harm by incorrect predictions or withholding of resources. This article describes how model design, biases in data, and the interactions of model predictions with clinicians and patients may exacerbate health care disparities. Rather than simply guarding against these harms passively, machine-learning systems should be used proactively to advance health equity. For that goal to be achieved, principles of distributive justice must be incorporated into model design, deployment, and evaluation. The article describes several technical implementations of distributive justice-specifically those that ensure equality in patient outcomes, performance, and resource allocation-and guides clinicians as to when they should prioritize each principle. Machine learning is providing increasingly sophisticated decision support and population-level monitoring, and it should encode principles of justice to ensure that models benefit all patients.",
      "journal": "Annals of internal medicine",
      "year": "2018",
      "doi": "10.7326/M18-1990",
      "authors": "Rajkomar Alvin et al.",
      "keywords": "",
      "mesh_terms": "Critical Care; Health Care Rationing; Health Equity; Healthcare Disparities; Humans; Length of Stay; Machine Learning; Patient Outcome Assessment; Social Justice",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30508424/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC6594166",
      "ft_text_length": 1258,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC6594166)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "30652884",
      "title": "Association between negative cognitive bias and depression: A symptom-level approach.",
      "abstract": "Cognitive models of depression posit that negatively biased self-referent processing and attention have important roles in the disorder. However, depression is a heterogeneous collection of symptoms and all symptoms are unlikely to be associated with these negative cognitive biases. The current study involved 218 community adults whose depression ranged from no symptoms to clinical levels of depression. Random forest machine learning was used to identify the most important depression symptom predictors of each negative cognitive bias. Depression symptoms were measured with the Beck Depression Inventory-II. Model performance was evaluated using predictive R-squared (Rpred2), the expected variance explained in data not used to train the algorithm, estimated by 10 repetitions of 10-fold cross-validation. Using the self-referent encoding task (SRET), depression symptoms explained 34% to 45% of the variance in negative self-referent processing. The symptoms of sadness, self-dislike, pessimism, feelings of punishment, and indecision were most important. Notably, many depression symptoms made virtually no contribution to this prediction. In contrast, for attention bias for sad stimuli, measured with the dot-probe task using behavioral reaction time (RT) and eye gaze metrics, no reliable symptom predictors were identified. Findings indicate that a symptom-level approach may provide new insights into which symptoms, if any, are associated with negative cognitive biases in depression. (PsycINFO Database Record (c) 2019 APA, all rights reserved).",
      "journal": "Journal of abnormal psychology",
      "year": "2019",
      "doi": "10.1037/abn0000405",
      "authors": "Beevers Christopher G et al.",
      "keywords": "",
      "mesh_terms": "Adolescent; Adult; Attention; Attentional Bias; Cognition Disorders; Depression; Depressive Disorder; Emotions; Female; Fixation, Ocular; Humans; Male; Personality Inventory; Reaction Time; Research Design; Young Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30652884/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC6449499",
      "ft_text_length": 1503,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC6449499)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "30892656",
      "title": "Fair compute loads enabled by blockchain: sharing models by alternating client and server roles.",
      "abstract": "OBJECTIVE: Decentralized privacy-preserving predictive modeling enables multiple institutions to learn a more generalizable model on healthcare or genomic data by sharing the partially trained models instead of patient-level data, while avoiding risks such as single point of control. State-of-the-art blockchain-based methods remove the \"server\" role but can be less accurate than models that rely on a server. Therefore, we aim at developing a general model sharing framework to preserve predictive correctness, mitigate the risks of a centralized architecture, and compute the models in a fair way. MATERIALS AND METHODS: We propose a framework that includes both server and \"client\" roles to preserve correctness. We adopt a blockchain network to obtain the benefits of decentralization, by alternating the roles for each site to ensure computational fairness. Also, we developed GloreChain (Grid Binary LOgistic REgression on Permissioned BlockChain) as a concrete example, and compared it to a centralized algorithm on 3 healthcare or genomic datasets to evaluate predictive correctness, number of learning iterations and execution time. RESULTS: GloreChain performs exactly the same as the centralized method in terms of correctness and number of iterations. It inherits the advantages of blockchain, at the cost of increased time to reach a consensus model. DISCUSSION: Our framework is general or flexible and can also address intrinsic challenges of blockchain networks. Further investigations will focus on higher-dimensional datasets, additional use cases, privacy-preserving quality concerns, and ethical, legal, and social implications. CONCLUSIONS: Our framework provides a promising potential for institutions to learn a predictive model based on healthcare or genomic data in a privacy-preserving and decentralized way.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2019",
      "doi": "10.1093/jamia/ocy180",
      "authors": "Kuo Tsung-Ting et al.",
      "keywords": "batch machine learning; blockchain distributed ledger technology; clinical information systems; decision support systems; privacy-preserving predictive modeling",
      "mesh_terms": "Algorithms; Blockchain; Computer Communication Networks; Confidentiality; Decision Support Systems, Clinical; Genomics; Health Information Systems; Humans; Information Dissemination; Information Systems; Machine Learning; Models, Theoretical",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30892656/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7787356",
      "ft_text_length": 38800,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7787356)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "30906163",
      "title": "Strategic Opportunities for Leveraging Low-cost, High-impact Technological Innovations to Promote Cardiovascular Health in India.",
      "abstract": "Accelerated epidemiological transition in India over the last 40 years has resulted in a dramatic increase in the burden of cardiovascular diseases and the related risk factors of diabetes and hypertension. This increase in disease burden has been accompanied by pervasive health disparities associated with low disease detection rates, inadequate awareness, poor use of evidence-based interventions, and low adherence rates among patients in rural regions in India and those with low socioeconomic status. Several research groups in India have developed innovative technologies and care-delivery models for screening, diagnosis, clinical management, remote-monitoring, self-management, and rehabilitation for a range of chronic conditions. These innovations can leverage advances in sensor technology, genomic tools, artificial intelligence, big-data analytics, and so on, for improving access to and delivering quality and affordable personalized medicine in primary care. In addition, several health technology start-ups are entering this booming market that is set to grow rapidly. Innovations outside biomedical space (eg, protection of traditional wisdom in diet, lifestyle, yoga) are equally important and are part of a comprehensive solution. Such low-cost, culturally tailored, robust innovations to promote health and reduce disparities require partnership among multi-sectors including academia, industry, civil society, and health systems operating in a conducive policy environment that fosters adequate public and private investments. In this article, we present the unique opportunity for India to use culturally tailored, low-cost, high-impact technological innovations and strategies to ameliorate the perennial challenges of social, policy, and environmental challenges including poverty, low educational attainment, culture, and other socioeconomic factors to promote cardiovascular health and advance health equity.",
      "journal": "Ethnicity & disease",
      "year": "2019",
      "doi": "10.18865/ed.29.S1.145",
      "authors": "Prabhakaran Dorairaj et al.",
      "keywords": "Cardiovascular Diseases; Diabetes; India; Innovations; Multi-disciplinary; Policy; Technology",
      "mesh_terms": "Cardiovascular Diseases; Chronic Disease; Diabetes Mellitus; Health Promotion; Humans; India; Inventions; Primary Health Care; Risk Factors; Socioeconomic Factors",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30906163/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC6428188",
      "ft_text_length": 1938,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC6428188)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "30924225",
      "title": "Investigating systematic bias in brain age estimation with application to post-traumatic stress disorders.",
      "abstract": "Brain age prediction using machine-learning techniques has recently attracted growing attention, as it has the potential to serve as a biomarker for characterizing the typical brain development and neuropsychiatric disorders. Yet one long-standing problem is that the predicted brain age is overestimated in younger subjects and underestimated in older. There is a plethora of claims as to the bias origins, both methodologically and in data itself. With a large neuroanatomical dataset (N\u2009=\u20092,026; 6-89 years of age) from multiple shared datasets, we show this bias is neither data-dependent nor specific to particular method including deep neural network. We present an alternative account that offers a statistical explanation for the bias and describe a simple, yet efficient, method using general linear model to adjust the bias. We demonstrate the effectiveness of bias adjustment with a large multi-modal neuroimaging data (N\u2009=\u2009804; 8-21 years of age) for both healthy controls and post-traumatic stress disorders patients obtained from the Philadelphia Neurodevelopmental Cohort.",
      "journal": "Human brain mapping",
      "year": "2019",
      "doi": "10.1002/hbm.24588",
      "authors": "Liang Hualou et al.",
      "keywords": "PTSD; bias; brain age prediction; machine-learning; regression to the mean",
      "mesh_terms": "Adolescent; Adult; Aged; Aged, 80 and over; Aging; Brain; Child; Humans; Machine Learning; Magnetic Resonance Imaging; Middle Aged; Neuroimaging; Stress Disorders, Post-Traumatic; Young Adult",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30924225/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC6865701",
      "ft_text_length": 1100,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC6865701)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "31409293",
      "title": "Guide for library design and bias correction for large-scale transcriptome studies using highly multiplexed RNAseq methods.",
      "abstract": "BACKGROUND: Standard RNAseq methods using bulk RNA and recent single-cell RNAseq methods use DNA barcodes to identify samples and cells, and the barcoded cDNAs are pooled into a library pool before high throughput sequencing. In cases of single-cell and low-input RNAseq methods, the library is further amplified by PCR after the pooling. Preparation of hundreds or more samples for a large study often requires multiple library pools. However, sometimes correlation between expression profiles among the libraries is low and batch effect biases make integration of data between library pools difficult. RESULTS: We investigated 166 technical replicates in 14 RNAseq libraries made using the STRT method. The patterns of the library biases differed by genes, and uneven library yields were associated with library biases. The former bias was corrected using the NBGLM-LBC algorithm, which we present in the current study. The latter bias could not be corrected directly, but could be solved by omitting libraries with particularly low yields. A simulation experiment suggested that the library bias correction using NBGLM-LBC requires a consistent sample layout. The NBGLM-LBC correction method was applied to an expression profile for a cohort study of childhood acute respiratory illness, and the library biases were resolved. CONCLUSIONS: The R source code for the library bias correction named NBGLM-LBC is available at https://shka.github.io/NBGLM-LBC and https://shka.bitbucket.io/NBGLM-LBC . This method is applicable to correct the library biases in various studies that use highly multiplexed sequencing-based profiling methods with a consistent sample layout with samples to be compared (e.g., \"cases\" and \"controls\") equally distributed in each library.",
      "journal": "BMC bioinformatics",
      "year": "2019",
      "doi": "10.1186/s12859-019-3017-9",
      "authors": "Katayama Shintaro et al.",
      "keywords": "Gene expression; Library bias correction; Next-generation sequencing; Transcriptome",
      "mesh_terms": "Cell Line; Cluster Analysis; Gene Library; High-Throughput Nucleotide Sequencing; Humans; Principal Component Analysis; RNA; Sequence Analysis, RNA; Transcriptome; User-Computer Interface",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31409293/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC6693229",
      "ft_text_length": 23028,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC6693229)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "31441044",
      "title": "Investigation of bias in an epilepsy machine learning algorithm trained on physician notes.",
      "abstract": "Racial disparities in the utilization of epilepsy surgery are well documented, but it is unknown whether a natural language processing (NLP) algorithm trained on physician notes would produce biased recommendations for epilepsy presurgical evaluations. To assess this, an NLP algorithm was trained to identify potential surgical candidates using 1097 notes from 175 epilepsy patients with a history of resective epilepsy surgery and 268 patients who achieved seizure freedom without surgery (total N\u00a0=\u00a0443 patients). The model was tested on 8340 notes from 3776 patients with epilepsy whose surgical candidacy status was unknown (2029 male, 1747 female, median age = 9\u00a0years; age range = 0-60\u00a0years). Multiple linear regression using demographic variables as covariates was used to test for correlations between patient race and surgical candidacy scores. After accounting for other demographic and socioeconomic variables, patient race, gender, and primary language did not influence surgical candidacy scores (P\u00a0>\u00a0.35 for all). Higher scores were given to patients >18\u00a0years old who traveled farther to receive care, and those who had a higher family income and public insurance (P\u00a0<\u00a0.001, .001, .001, and .01, respectively). Demographic effects on surgical candidacy scores appeared to reflect patterns in patient referrals.",
      "journal": "Epilepsia",
      "year": "2019",
      "doi": "10.1111/epi.16320",
      "authors": "Wissel Benjamin D et al.",
      "keywords": "clinical decision support; epilepsy surgery; machine learning; natural language processing",
      "mesh_terms": "Adolescent; Adult; Age Factors; Algorithms; Child; Child, Preschool; Electroencephalography; Epilepsy; Healthcare Disparities; Humans; Infant; Machine Learning; Middle Aged; Patient Selection; Prejudice; Referral and Consultation; Young Adult",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31441044/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC6731998",
      "ft_text_length": 1339,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC6731998)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "31665002",
      "title": "Key challenges for delivering clinical impact with artificial intelligence.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) research in healthcare is accelerating rapidly, with potential applications being demonstrated across various domains of medicine. However, there are currently limited examples of such techniques being successfully deployed into clinical practice. This article explores the main challenges and limitations of AI in healthcare, and considers the steps required to translate these potentially transformative technologies from research to clinical practice. MAIN BODY: Key challenges for the translation of AI systems in healthcare include those intrinsic to the science of machine learning, logistical difficulties in implementation, and consideration of the barriers to adoption as well as of the necessary sociocultural or pathway changes. Robust peer-reviewed clinical evaluation as part of randomised controlled trials should be viewed as the gold standard for evidence generation, but conducting these in practice may not always be appropriate or feasible. Performance metrics should aim to capture real clinical applicability and be understandable to intended users. Regulation that balances the pace of innovation with the potential for harm, alongside thoughtful post-market surveillance, is required to ensure that patients are not exposed to dangerous interventions nor deprived of access to beneficial innovations. Mechanisms to enable direct comparisons of AI systems must be developed, including the use of independent, local and representative test sets. Developers of AI algorithms must be vigilant to potential dangers, including dataset shift, accidental fitting of confounders, unintended discriminatory bias, the challenges of generalisation to new populations, and the unintended negative consequences of new algorithms on health outcomes. CONCLUSION: The safe and timely translation of AI research into clinically validated and appropriately regulated systems that can benefit everyone is challenging. Robust clinical evaluation, using metrics that are intuitive to clinicians and ideally go beyond measures of technical accuracy to include quality of care and patient outcomes, is essential. Further work is required (1) to identify themes of algorithmic bias and unfairness while developing mitigations to address these, (2) to reduce brittleness and improve generalisability, and (3) to develop methods for improved interpretability of machine learning predictions. If these goals can be achieved, the benefits for patients are likely to be transformational.",
      "journal": "BMC medicine",
      "year": "2019",
      "doi": "10.1186/s12916-019-1426-2",
      "authors": "Kelly Christopher J et al.",
      "keywords": "Algorithms; Artificial intelligence; Evaluation; Machine learning; Regulation; Translation",
      "mesh_terms": "Algorithms; Artificial Intelligence; Delivery of Health Care; Humans; Peer Review",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31665002/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC6821018",
      "ft_text_length": 27268,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC6821018)",
      "ft_reason": "Excluded: insufficient approach content (1 indicators)"
    },
    {
      "pmid": "31722599",
      "title": "Predicting 15O-Water PET cerebral blood flow maps from multi-contrast MRI using a deep convolutional neural network with evaluation of training cohort bias.",
      "abstract": "To improve the quality of MRI-based cerebral blood flow (CBF) measurements, a deep convolutional neural network (dCNN) was trained to combine single- and multi-delay arterial spin labeling (ASL) and structural images to predict gold-standard 15O-water PET CBF images obtained on a simultaneous PET/MRI scanner. The dCNN was trained and tested on 64 scans in 16 healthy controls (HC) and 16 cerebrovascular disease patients (PT) with 4-fold cross-validation. Fidelity to the PET CBF images and the effects of bias due to training on different cohorts were examined. The dCNN significantly improved CBF image quality compared with ASL alone (mean\u2009\u00b1\u2009standard deviation): structural similarity index (0.854\u2009\u00b1\u20090.036 vs. 0.743\u2009\u00b1\u20090.045 [single-delay] and 0.732\u2009\u00b1\u20090.041 [multi-delay], P\u2009<\u20090.0001); normalized root mean squared error (0.209\u2009\u00b1\u20090.039 vs. 0.326\u2009\u00b1\u20090.050 [single-delay] and 0.344\u2009\u00b1\u20090.055 [multi-delay], P\u2009<\u20090.0001). The dCNN also yielded mean CBF with reduced estimation error in both HC and PT (P\u2009<\u20090.001), and demonstrated better correlation with PET. The dCNN trained with the mixed HC and PT cohort performed the best. The results also suggested that models should be trained on cases representative of the target population.",
      "journal": "Journal of cerebral blood flow and metabolism : official journal of the International Society of Cerebral Blood Flow and Metabolism",
      "year": "2020",
      "doi": "10.1177/0271678X19888123",
      "authors": "Guo Jia et al.",
      "keywords": "Cerebral blood flow; arterial spin labeling; deep convolutional neural network; magnetic resonance imaging; positron emission tomography",
      "mesh_terms": "Adolescent; Adult; Aged; Algorithms; Brain; Cerebrovascular Circulation; Data Analysis; Female; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Middle Aged; Models, Biological; Neural Networks, Computer; Oxygen Radioisotopes; Positron-Emission Tomography; Reproducibility of Results; Water; Young Adult",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31722599/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7585922",
      "ft_text_length": 1246,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7585922)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "31840093",
      "title": "Eliminating biasing signals in lung cancer images for prognosis predictions with deep learning.",
      "abstract": "Deep learning has shown remarkable results for image analysis and is expected to aid individual treatment decisions in health care. Treatment recommendations are predictions with an inherently causal interpretation. To use deep learning for these applications in the setting of observational data, deep learning methods must be made compatible with the required causal assumptions. We present a scenario with real-world medical images (CT-scans of lung cancer) and simulated outcome data. Through the data simulation scheme, the images contain two distinct factors of variation that are associated with survival, but represent a collider (tumor size) and a prognostic factor (tumor heterogeneity), respectively. When a deep network would use all the information available in the image to predict survival, it would condition on the collider and thereby introduce bias in the estimation of the treatment effect. We show that when this collider can be quantified, unbiased individual prognosis predictions are attainable with deep learning. This is achieved by (1) setting a dual task for the network to predict both the outcome and the collider and (2) enforcing a form of linear independence of the activation distributions of the last layer. Our method provides an example of combining deep learning and structural causal models to achieve unbiased individual prognosis predictions. Extensions of machine learning methods for applications to causal questions are required to attain the long-standing goal of personalized medicine supported by artificial intelligence.",
      "journal": "NPJ digital medicine",
      "year": "2019",
      "doi": "10.1038/s41746-019-0194-x",
      "authors": "van Amsterdam W A C et al.",
      "keywords": "Computed tomography; Computer science; Epidemiology; Prognosis",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31840093/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC6904461",
      "ft_text_length": 75696,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC6904461)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "32064914",
      "title": "Assessing and Mitigating Bias in Medical Artificial Intelligence: The Effects of Race and Ethnicity on a Deep Learning Model for ECG Analysis.",
      "abstract": "BACKGROUND: Deep learning algorithms derived in homogeneous populations may be poorly generalizable and have the potential to reflect, perpetuate, and even exacerbate racial/ethnic disparities in health and health care. In this study, we aimed to (1) assess whether the performance of a deep learning algorithm designed to detect low left ventricular ejection fraction using the 12-lead ECG varies by race/ethnicity and to (2) determine whether its performance is determined by the derivation population or by racial variation in the ECG. METHODS: We performed a retrospective cohort analysis that included 97 829 patients with paired ECGs and echocardiograms. We tested the model performance by race/ethnicity for convolutional neural network designed to identify patients with a left ventricular ejection fraction \u226435% from the 12-lead ECG. RESULTS: The convolutional neural network that was previously derived in a homogeneous population (derivation cohort, n=44 959; 96.2% non-Hispanic white) demonstrated consistent performance to detect low left ventricular ejection fraction across a range of racial/ethnic subgroups in a separate testing cohort (n=52 870): non-Hispanic white (n=44 524; area under the curve [AUC], 0.931), Asian (n=557; AUC, 0.961), black/African American (n=651; AUC, 0.937), Hispanic/Latino (n=331; AUC, 0.937), and American Indian/Native Alaskan (n=223; AUC, 0.938). In secondary analyses, a separate neural network was able to discern racial subgroup category (black/African American [AUC, 0.84], and white, non-Hispanic [AUC, 0.76] in a 5-class classifier), and a network trained only in non-Hispanic whites from the original derivation cohort performed similarly well across a range of racial/ethnic subgroups in the testing cohort with an AUC of at least 0.930 in all racial/ethnic subgroups. CONCLUSIONS: Our study demonstrates that while ECG characteristics vary by race, this did not impact the ability of a convolutional neural network to predict low left ventricular ejection fraction from the ECG. We recommend reporting of performance among diverse ethnic, racial, age, and sex groups for all new artificial intelligence tools to ensure responsible use of artificial intelligence in medicine.",
      "journal": "Circulation. Arrhythmia and electrophysiology",
      "year": "2020",
      "doi": "10.1161/CIRCEP.119.007988",
      "authors": "Noseworthy Peter A et al.",
      "keywords": "United States; artificial intelligence; electrocardiography; humans; machine learning",
      "mesh_terms": "Artificial Intelligence; Deep Learning; Electrocardiography; Ethnicity; Female; Follow-Up Studies; Heart Ventricles; Humans; Male; Middle Aged; Racial Groups; Retrospective Studies; Ventricular Function, Left",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32064914/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7158877",
      "ft_text_length": 2062,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7158877)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "32284656",
      "title": "Four equity considerations for the use of artificial intelligence in public health.",
      "abstract": "",
      "journal": "Bulletin of the World Health Organization",
      "year": "2020",
      "doi": "10.2471/BLT.19.237503",
      "authors": "Smith Maxwell J et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artificial Intelligence; Decision Making; Digital Divide; Health Equity; Public Health",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32284656/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7133473",
      "ft_text_length": 10764,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7133473)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "32457147",
      "title": "Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis.",
      "abstract": "Artificial intelligence (AI) systems for computer-aided diagnosis and image-based screening are being adopted worldwide by medical institutions. In such a context, generating fair and unbiased classifiers becomes of paramount importance. The research community of medical image computing is making great efforts in developing more accurate algorithms to assist medical doctors in the difficult task of disease diagnosis. However, little attention is paid to the way databases are collected and how this may influence the performance of AI systems. Our study sheds light on the importance of gender balance in medical imaging datasets used to train AI systems for computer-assisted diagnosis. We provide empirical evidence supported by a large-scale study, based on three deep neural network architectures and two well-known publicly available X-ray image datasets used to diagnose various thoracic diseases under different gender imbalance conditions. We found a consistent decrease in performance for underrepresented genders when a minimum balance is not fulfilled. This raises the alarm for national agencies in charge of regulating and approving computer-assisted diagnosis systems, which should include explicit gender balance and diversity recommendations. We also establish an open problem for the academic medical image computing community which needs to be addressed by novel algorithms endowed with robustness to gender imbalance.",
      "journal": "Proceedings of the National Academy of Sciences of the United States of America",
      "year": "2020",
      "doi": "10.1073/pnas.1919012117",
      "authors": "Larrazabal Agostina J et al.",
      "keywords": "computer-aided diagnosis; deep learning; gender bias; gendered innovations; medical image analysis",
      "mesh_terms": "Bias; Datasets as Topic; Deep Learning; Female; Humans; Male; Radiographic Image Interpretation, Computer-Assisted; Radiography, Thoracic; Reference Standards; Sex Factors",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32457147/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7293650",
      "ft_text_length": 12472,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7293650)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "32585698",
      "title": "Patient safety and quality improvement: Ethical principles for a regulatory approach to bias in healthcare machine learning.",
      "abstract": "Accumulating evidence demonstrates the impact of bias that reflects social inequality on the performance of machine learning (ML) models in health care. Given their intended placement within healthcare decision making more broadly, ML tools require attention to adequately quantify the impact of bias and reduce its potential to exacerbate inequalities. We suggest that taking a patient safety and quality improvement approach to bias can support the quantification of bias-related effects on ML. Drawing from the ethical principles underpinning these approaches, we argue that patient safety and quality improvement lenses support the quantification of relevant performance metrics, in order to minimize harm while promoting accountability, justice, and transparency. We identify specific methods for operationalizing these principles with the goal of attending to bias to support better decision making in light of controllable and uncontrollable factors.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2020",
      "doi": "10.1093/jamia/ocaa085",
      "authors": "McCradden Melissa D et al.",
      "keywords": "healthcare delivery; machine learning; patient safety; quality improvement; systematic bias",
      "mesh_terms": "Artificial Intelligence; Data Collection; Government Regulation; Healthcare Disparities; Humans; Patient Safety; Prejudice; Quality Improvement; Social Determinants of Health",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32585698/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7727331",
      "ft_text_length": 966,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7727331)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "32880609",
      "title": "Low-Shot Deep Learning of Diabetic Retinopathy With Potential Applications to Address Artificial Intelligence Bias in Retinal Diagnostics and Rare Ophthalmic Diseases.",
      "abstract": "IMPORTANCE: Recent studies have demonstrated the successful application of artificial intelligence (AI) for automated retinal disease diagnostics but have not addressed a fundamental challenge for deep learning systems: the current need for large, criterion standard-annotated retinal data sets for training. Low-shot learning algorithms, aiming to learn from a relatively low number of training data, may be beneficial for clinical situations involving rare retinal diseases or when addressing potential bias resulting from data that may not adequately represent certain groups for training, such as individuals older than 85 years. OBJECTIVE: To evaluate whether low-shot deep learning methods are beneficial when using small training data sets for automated retinal diagnostics. DESIGN, SETTING, AND PARTICIPANTS: This cross-sectional study, conducted from July 1, 2019, to June 21, 2020, compared different diabetic retinopathy classification algorithms, traditional and low-shot, for 2-class designations (diabetic retinopathy warranting referral vs not warranting referral). The public domain EyePACS data set was used, which originally included 88\u202f692 fundi from 44\u202f346 individuals. Statistical analysis was performed from February 1 to June 21, 2020. MAIN OUTCOMES AND MEASURES: The performance (95% CIs) of the various AI algorithms was measured via receiver operating curves and their area under the curve (AUC), precision recall curves, accuracy, and F1 score, evaluated for different training data sizes, ranging from 5120 to 10 samples per class. RESULTS: Deep learning algorithms, when trained with sufficiently large data sets (5120 samples per class), yielded comparable performance, with an AUC of 0.8330 (95% CI, 0.8140-0.8520) for a traditional approach (eg, fined-tuned ResNet), compared with low-shot methods (AUC, 0.8348 [95% CI, 0.8159-0.8537]) (using self-supervised Deep InfoMax [our method denoted as DIM]). However, when far fewer training images were available (n\u2009=\u2009160), the traditional deep learning approach had an AUC decreasing to 0.6585 (95% CI, 0.6332-0.6838) and was outperformed by a low-shot method using self-supervision with an AUC of 0.7467 (95% CI, 0.7239-0.7695). At very low shots (n\u2009=\u200910), the traditional approach had performance close to chance, with an AUC of 0.5178 (95% CI, 0.4909-0.5447) compared with the best low-shot method (AUC, 0.5778 [95% CI, 0.5512-0.6044]). CONCLUSIONS AND RELEVANCE: These findings suggest the potential benefits of using low-shot methods for AI retinal diagnostics when a limited number of annotated training retinal images are available (eg, with rare ophthalmic diseases or when addressing potential AI bias).",
      "journal": "JAMA ophthalmology",
      "year": "2020",
      "doi": "10.1001/jamaophthalmol.2020.3269",
      "authors": "Burlina Philippe et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artificial Intelligence; Cross-Sectional Studies; Deep Learning; Diabetic Retinopathy; Female; Humans; Male; Neural Networks, Computer; ROC Curve; Rare Diseases; Retrospective Studies",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32880609/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7489388",
      "ft_text_length": 838,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7489388)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "32971486",
      "title": "An atlas of health inequalities and health disparities research: \"How is this all getting done in silos, and why?\".",
      "abstract": "Research on health inequalities and health disparities has grown exponentially since the 1960s, but this expansion has not been matched by an associated sense of progress. Criticisms include claims that too much research addresses well-trodden questions and that the field has failed to gain public and policy traction. Qualitative studies have found researchers partly attribute these challenges to fragmentation resulting from disciplinary and methodological differences. Yet, empirical investigation ('research on research') is limited. This study addresses this gap, employing mixed-methods to examine, at scale, how and why this field is defined by insular research clusters. First, bibliometric analysis identifies and visualizes the 250 most-connected authors. Next, an algorithm was used to identify clustering via citation links between authors. We used researcher profiling to ascertain authors' geographical and institutional locations and disciplinary training, examining how this mapped onto clusters. Finally, causes of siloing were investigated via semi-structured interviews with 45 researchers. The resulting 'atlas' of health inequalities and health disparities research identifies eight clusters of authors with varying degrees of connectedness. No single factor neatly describes observed fragmentation, health equity scholars exhibit a diverse disciplinary backgrounds, and geographical, institutional, and historical factors appear to intersect to explain siloed citation patterns. While the configuration of research activity within clusters potentially helps render questions scientifically manageable, it affirms perceptions of the field as fragmented. We draw on Thomas Kuhn and Sheila Jasanoff to position results within theoretical pictures of scientific progress. Newcomers to the field can use our findings to orient themselves within the many streams of health equity scholarship, and existing health equity scholars can use the atlas to move beyond existing geo-disciplinary networks. However, although stronger cross-cluster engagement would be likely to improve insights, the complex nexus of factors underlying the field's structure will likely make this challenging in practice.",
      "journal": "Social science & medicine (1982)",
      "year": "2020",
      "doi": "10.1016/j.socscimed.2020.113330",
      "authors": "Collyer Taya A et al.",
      "keywords": "Bibliometric analysis; Disciplines; Health disparities; Health equity; Health inequalities; Interviews; Scientific paradigms; Sociology of science",
      "mesh_terms": "Bibliometrics; Health Equity; Health Status Disparities; Humans; Qualitative Research; Research Personnel",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32971486/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7449896",
      "ft_text_length": 53425,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7449896)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "33313606",
      "title": "Frequent Causal Pattern Mining: A Computationally Efficient Framework For Estimating Bias-Corrected Effects.",
      "abstract": "Our aging population increasingly suffers from multiple chronic diseases simultaneously, necessitating the comprehensive treatment of these conditions. Finding the optimal set of drugs for a combinatorial set of diseases is a combinatorial pattern exploration problem. Association rule mining is a popular tool for such problems, but the requirement of health care for finding causal, rather than associative, patterns renders association rule mining unsuitable. To address this issue, we propose a novel framework based on the Rubin-Neyman causal model for extracting causal rules from observational data, correcting for a number of common biases. Specifically, given a set of interventions and a set of items that define subpopulations (e.g., diseases), we wish to find all subpopulations in which effective intervention combinations exist and in each such subpopulation, we wish to find all intervention combinations such that dropping any intervention from this combination will reduce the efficacy of the treatment. A key aspect of our framework is the concept of closed intervention sets which extend the concept of quantifying the effect of a single intervention to a set of concurrent interventions. Closed intervention sets also allow for a pruning strategy that is strictly more efficient than the traditional pruning strategy used by the Apriori algorithm. To implement our ideas, we introduce and compare five methods of estimating causal effect from observational data and rigorously evaluate them on synthetic data to mathematically prove (when possible) why they work. We also evaluated our causal rule mining framework on the Electronic Health Records (EHR) data of a large cohort of 152000 patients from Mayo Clinic and showed that the patterns we extracted are sufficiently rich to explain the controversial findings in the medical literature regarding the effect of a class of cholesterol drugs on Type-II Diabetes Mellitus (T2DM).",
      "journal": "Proceedings : ... IEEE International Conference on Big Data. IEEE International Conference on Big Data",
      "year": "2019",
      "doi": "10.1109/bigdata47090.2019.9005977",
      "authors": "Yadav Pranjul et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33313606/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7730315",
      "ft_text_length": 1950,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7730315)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "33422469",
      "title": "Cloud-Based Functional Magnetic Resonance Imaging Neurofeedback to Reduce the Negative Attentional Bias in Depression: A Proof-of-Concept Study.",
      "abstract": "Individuals with depression show an attentional bias toward negatively valenced stimuli and thoughts. In this proof-of-concept study, we present a novel closed-loop neurofeedback procedure intended to remediate this bias. Internal attentional states were detected in real time by applying machine learning techniques to functional magnetic resonance imaging data on a cloud server; these attentional states were externalized using a visual stimulus that the participant could learn to control. We trained 15 participants with major depressive disorder and 12 healthy control participants over 3 functional magnetic resonance imaging sessions. Exploratory analysis showed that participants with major depressive disorder were initially more likely than healthy control participants to get stuck in negative attentional states, but this diminished with neurofeedback training relative to controls. Depression severity also decreased from pre- to posttraining. These results demonstrate that our method is sensitive to the negative attentional bias in major depressive disorder and showcase the potential of this novel technique as a treatment that can be evaluated in future clinical trials.",
      "journal": "Biological psychiatry. Cognitive neuroscience and neuroimaging",
      "year": "2021",
      "doi": "10.1016/j.bpsc.2020.10.006",
      "authors": "Mennen Anne C et al.",
      "keywords": "Attentional bias; Brain-machine interface; Cloud computing; Cognitive training; Depression; Real-time fMRI",
      "mesh_terms": "Attentional Bias; Cloud Computing; Depression; Major Depressive Disorder; Humans; Magnetic Resonance Imaging; Neurofeedback",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33422469/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8035170",
      "ft_text_length": 1039,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8035170)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "33484133",
      "title": "The risk of racial bias while tracking influenza-related content on social media using machine learning.",
      "abstract": "OBJECTIVE: Machine learning is used to understand and track influenza-related content on social media. Because these systems are used at scale, they have the potential to adversely impact the people they are built to help. In this study, we explore the biases of different machine learning methods for the specific task of detecting influenza-related content. We compare the performance of each model on tweets written in Standard American English (SAE) vs African American English (AAE). MATERIALS AND METHODS: Two influenza-related datasets are used to train 3 text classification models (support vector machine, convolutional neural network, bidirectional long short-term memory) with different feature sets. The datasets match real-world scenarios in which there is a large imbalance between SAE and AAE examples. The number of AAE examples for each class ranges from 2% to 5% in both datasets. We also evaluate each model's performance using a balanced dataset via undersampling. RESULTS: We find that all of the tested machine learning methods are biased on both datasets. The difference in false positive rates between SAE and AAE examples ranges from 0.01 to 0.35. The difference in the false negative rates ranges from 0.01 to 0.23. We also find that the neural network methods generally has more unfair results than the linear support vector machine on the chosen datasets. CONCLUSIONS: The models that result in the most unfair predictions may vary from dataset to dataset. Practitioners should be aware of the potential harms related to applying machine learning to health-related social media data. At a minimum, we recommend evaluating fairness along with traditional evaluation metrics.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2021",
      "doi": "10.1093/jamia/ocaa326",
      "authors": "Lwowski Brandon et al.",
      "keywords": "classification; deep learning; fairness; machine learning; social network",
      "mesh_terms": "Black or African American; Datasets as Topic; Humans; Influenza Vaccines; Influenza, Human; Machine Learning; Neural Networks, Computer; Racism; Social Media; Support Vector Machine",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33484133/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7973478",
      "ft_text_length": 1706,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7973478)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "33609456",
      "title": "RadTranslate: An Artificial Intelligence-Powered Intervention for Urgent Imaging to Enhance Care Equity for Patients With Limited English Proficiency During the COVID-19 Pandemic.",
      "abstract": "PURPOSE: Disproportionally high rates of coronavirus disease 2019 (COVID-19) have been noted among communities with limited English proficiency, resulting in an unmet need for improved multilingual care and interpreter services. To enhance multilingual care, the authors created a freely available web application, RadTranslate, that provides multilingual radiology examination instructions. The purpose of this study was to evaluate the implementation of this intervention in radiology. METHODS: The device-agnostic web application leverages artificial intelligence text-to-speech technology to provide standardized, human-like spoken examination instructions in the patient's preferred language. Standardized phrases were collected from a consensus group consisting of technologists, radiologists, and ancillary staff members. RadTranslate was piloted in Spanish for chest radiography performed at a COVID-19 triage outpatient center that served a predominantly Spanish-speaking Latino community. Implementation included a tablet displaying the application in the chest radiography room. Imaging appointment duration was measured and compared between pre- and postimplementation groups. RESULTS: In the 63-day test period after launch, there were 1,267 application uses, with technologists voluntarily switching exclusively to RadTranslate for Spanish-speaking patients. The most used phrases were a general explanation of the examination (30% of total), followed by instructions to disrobe and remove any jewelry (12%). There was no significant difference in imaging appointment duration (11 \u00b1 7 and 12 \u00b1 3 min for standard of care versus RadTranslate, respectively), but variability was significantly lower when RadTranslate was used (P\u00a0= .003). CONCLUSIONS: Artificial intelligence-aided multilingual audio instructions were successfully integrated into imaging workflows, reducing strain on medical interpreters and variance in throughput and resulting in more reliable average examination length.",
      "journal": "Journal of the American College of Radiology : JACR",
      "year": "2021",
      "doi": "10.1016/j.jacr.2021.01.013",
      "authors": "Chonde Daniel B et al.",
      "keywords": "English proficiency; Innovation; disparities; operations",
      "mesh_terms": "Artificial Intelligence; COVID-19; Humans; Limited English Proficiency; Pandemics; SARS-CoV-2",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33609456/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7847389",
      "ft_text_length": 24524,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7847389)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "33620624",
      "title": "Predicting Self-Rated Health Across the Life Course: Health Equity Insights from Machine Learning Models.",
      "abstract": "BACKGROUND: Self-rated health is a strong predictor of mortality and morbidity. Machine learning techniques may provide insights into which of the multifaceted contributors to self-rated health are key drivers in diverse groups. OBJECTIVE: We used machine learning algorithms to predict self-rated health in diverse groups in the Behavioral Risk Factor Surveillance System (BRFSS), to understand how machine learning algorithms might be used explicitly to examine drivers of self-rated health in diverse populations. DESIGN: We applied three common machine learning algorithms to predict self-rated health in the 2017 BRFSS survey, stratified by age, race/ethnicity, and sex. We replicated our process in the 2016 BRFSS survey. PARTICIPANTS: We analyzed data from 449,492 adult participants of the 2017 BRFSS survey. MAIN MEASURES: We examined area under the curve (AUC) statistics to examine model fit within each group. We used traditional logistic regression to predict self-rated health associated with features identified by machine learning models. KEY RESULTS: Each algorithm, regularized logistic regression (AUC: 0.81), random forest (AUC: 0.80), and support vector machine (AUC: 0.81), provided good model fit in the BRFSS. Predictors of self-rated health were similar by sex and race/ethnicity but differed by age. Socioeconomic features were prominent predictors of self-rated health in mid-life age groups. Income [OR: 1.70 (95% CI: 1.62-1.80)], education [OR: 2.02 (95% CI: 1.89, 2.16)], physical activity [OR: 1.52 (95% CI: 1.46-1.58)], depression [OR: 0.66 (95% CI: 0.63-0.68)], difficulty concentrating [OR: 0.62 (95% CI: 0.58-0.66)], and hypertension [OR: 0.59 (95% CI: 0.57-0.61)] all predicted the odds of excellent or very good self-rated health. CONCLUSIONS: Our analysis of BRFSS data show social determinants of health are prominent predictors of self-rated health in mid-life. Our work may demonstrate promising practices for using machine learning to advance health equity.",
      "journal": "Journal of general internal medicine",
      "year": "2021",
      "doi": "10.1007/s11606-020-06438-1",
      "authors": "Clark Cheryl R et al.",
      "keywords": "healthcare disparities; machine learning; self-rated health; social determinants of health; socioeconomic factors",
      "mesh_terms": "Adult; Algorithms; Behavioral Risk Factor Surveillance System; Health Equity; Humans; Logistic Models; Machine Learning",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33620624/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8131482",
      "ft_text_length": 2110,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8131482)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "33678038",
      "title": "A case study of ascertainment bias for the primary outcome in the Strategies to Reduce Injuries and Develop Confidence in Elders (STRIDE) trial.",
      "abstract": "BACKGROUND/AIM: In clinical trials, there is potential for bias from unblinded observers that may influence ascertainment of outcomes. This issue arose in the Strategies to Reduce Injuries and Develop Confidence in Elders trial, a cluster randomized trial to test a multicomponent intervention versus enhanced usual care (control) to prevent serious fall injuries, originally defined as a fall injury leading to medical attention. An unblinded nurse falls care manager administered the intervention, while the usual care arm did not involve contact with a falls care manager. Thus, there was an opportunity for falls care managers to refer participants reporting falls to seek medical attention. Since this type of observer bias could not occur in the usual care arm, there was potential for additional falls to be reported in the intervention arm, leading to dilution of the intervention effect and a reduction in study power. We describe the clinical basis for ascertainment bias, the statistical approach used to assess it, and its effect on study power. METHODS: The prespecified interim monitoring plan included a decision algorithm for assessing ascertainment bias and adapting (revising) the primary outcome definition, if necessary. The original definition categorized serious fall injuries requiring medical attention into Type 1 (fracture other than thoracic/lumbar vertebral, joint dislocation, cut requiring closure) and Type 2 (head injury, sprain or strain, bruising or swelling, other). The revised definition, proposed by the monitoring plan, excluded Type 2 injuries that did not necessarily require an overnight hospitalization since these would be most subject to bias. These injuries were categorized into those with (Type 2b) and without (Type 2c) medical attention. The remaining Type 2a injuries required medical attention and an overnight hospitalization. We used the ratio of 2b/(2b\u2009+\u20092c) in intervention versus control as a measure of ascertainment bias; ratios\u2009>\u20091 indicated the likelihood of falls care manager bias. We determined the effect of ascertainment bias on study power for the revised (Types 1 and 2a) versus original definition (Types 1, 2a, and 2b). RESULTS: The estimate of ascertainment bias was 1.14 (95% confidence interval: 0.98, 1.30), providing evidence of the likelihood of falls care manager bias. We estimated that this bias diluted the hazard ratio from the hypothesized 0.80 to 0.86 and reduced power to under 80% for the original primary outcome definition. In contrast, adapting the revised definition maintained study power at nearly 90%. CONCLUSION: There was evidence of ascertainment bias in the Strategies to Reduce Injuries and Develop Confidence in Elders trial. The decision to adapt the primary outcome definition reduced the likelihood of this bias while preserving the intervention effect and study power.",
      "journal": "Clinical trials (London, England)",
      "year": "2021",
      "doi": "10.1177/1740774520980070",
      "authors": "Esserman Denise A et al.",
      "keywords": "Cluster randomized trial; adjudication; ascertainment bias; hazard ratio; power",
      "mesh_terms": "Accidental Falls; Aged; Bias; Fractures, Bone; Hospitalization; Humans; Randomized Controlled Trials as Topic",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33678038/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8009806",
      "ft_text_length": 2787,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8009806)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "33679476",
      "title": "Correlation Constraints for Regression Models: Controlling Bias in Brain Age Prediction.",
      "abstract": "In neuroimaging, the difference between chronological age and predicted brain age, also known as brain age delta, has been proposed as a pathology marker linked to a range of phenotypes. Brain age delta is estimated using regression, which involves a frequently observed bias due to a negative correlation between chronological age and brain age delta. In brain age prediction models, this correlation can manifest as an overprediction of the age of young brains and an underprediction for elderly ones. We show that this bias can be controlled for by adding correlation constraints to the model training procedure. We develop an analytical solution to this constrained optimization problem for Linear, Ridge, and Kernel Ridge regression. The solution is optimal in the least-squares sense i.e., there is no other model that satisfies the correlation constraints and has a better fit. Analyses on the PAC2019 competition data demonstrate that this approach produces optimal unbiased predictive models with a number of advantages over existing approaches. Finally, we introduce regression toolboxes for Python and MATLAB that implement our algorithm.",
      "journal": "Frontiers in psychiatry",
      "year": "2021",
      "doi": "10.3389/fpsyt.2021.615754",
      "authors": "Treder Matthias S et al.",
      "keywords": "age; brain; correlation; optimization; prediction; regression",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33679476/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7930839",
      "ft_text_length": 53988,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7930839)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "33746859",
      "title": "A Pictorial Dot Probe Task to Assess Food-Related Attentional Bias in Youth With and Without Obesity: Overview of Indices and Evaluation of Their Reliability.",
      "abstract": "Several versions of the dot probe detection task are frequently used to assess maladaptive attentional processes associated with a broad range of psychopathology and health behavior, including eating behavior and weight. However, there are serious concerns about the reliability of the indices derived from the paradigm as measurement of attentional bias toward or away from salient stimuli. The present paper gives an overview of different attentional bias indices used in psychopathology research and scrutinizes three types of indices (the traditional attentional bias score, the dynamic trial-level base scores, and the probability index) calculated from a pictorial version of the dot probe task to assess food-related attentional biases in children and youngsters with and without obesity. Correlational analyses reveal that dynamic scores (but not the traditional and probability indices) are dependent on general response speed. Reliability estimates are low for the traditional and probability indices. The higher reliability for the dynamic indices is at least partially explained by general response speed. No significant group differences between youth with and without obesity are found, and correlations with weight are also non-significant. Taken together, results cast doubt on the applicability of this specific task for both experimental and individual differences research on food-related attentional biases in youth. However, researchers are encouraged to make and test adaptations to the procedure or computational algorithm in an effort to increase psychometric quality of the task and to report psychometric characteristics of their version of the task for their specific sample.",
      "journal": "Frontiers in psychology",
      "year": "2021",
      "doi": "10.3389/fpsyg.2021.644512",
      "authors": "Vervoort Leentje et al.",
      "keywords": "attentional bias; children and adolescent; dot probe paradigm; obesity; reliability",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33746859/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7965983",
      "ft_text_length": 43502,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7965983)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "33947843",
      "title": "Interrogation of gender disparity uncovers androgen receptor as the transcriptional activator for oncogenic miR-125b in gastric cancer.",
      "abstract": "There is a male preponderance in gastric cancer (GC), which suggests a role of androgen and androgen receptor (AR). However, the mechanism of AR signaling in GC especially in female patients remains obscure. We sought to identify the AR signaling pathway that might be related to prognosis and examine the potential clinical utility of the AR antagonist for treatment. Deep learning and gene set enrichment analysis was used to identify potential critical factors associated with gender bias in GC (n\u2009=\u20091390). Gene expression profile analysis was performed to screen differentially expressed genes associated with AR expression in the Tianjin discovery set (n\u2009=\u200990) and TCGA validation set (n\u2009=\u2009341). Predictors of survival were identified via lasso regression analyses and validated in the expanded Tianjin cohort (n\u2009=\u2009373). In vitro and in vivo experiments were established to determine the drug effect. The GC gender bias was attributable to sex chromosome abnormalities and AR signaling dysregulation. The candidates for AR-related gene sets were screened, and AR combined with miR-125b was associated with poor prognosis, particularly among female patients. AR was confirmed to directly regulate miR-125b expression. AR-miR-125b signaling pathway inhibited apoptosis and promoted proliferation. AR antagonist, bicalutamide, exerted anti-tumor activities and induced apoptosis both in vitro and in vivo, using GC cell lines and female patient-derived xenograft (PDX) model. We have shed light on gender differences by revealing a hormone-regulated oncogenic signaling pathway in GC. Our preclinical studies suggest that AR is a potential therapeutic target for this deadly cancer type, especially in female patients.",
      "journal": "Cell death & disease",
      "year": "2021",
      "doi": "10.1038/s41419-021-03727-3",
      "authors": "Liu Ben et al.",
      "keywords": "",
      "mesh_terms": "Animals; Female; Heterografts; Humans; Male; Mice; Mice, Inbred BALB C; Mice, Nude; MicroRNAs; Receptors, Androgen; Sex Factors; Stomach Neoplasms; Transcriptome",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33947843/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8096848",
      "ft_text_length": 71964,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8096848)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "34027610",
      "title": "Health Care Equity in the Use of Advanced Analytics and Artificial Intelligence Technologies in Primary Care.",
      "abstract": "The integration of advanced analytics and artificial intelligence (AI) technologies into the practice of medicine holds much promise. Yet, the opportunity to leverage these tools carries with it an equal responsibility to ensure that principles of equity are incorporated into their implementation and use. Without such efforts, tools will potentially reflect the myriad of ways in which data, algorithmic, and analytic biases can be produced, with the potential to widen inequities by race, ethnicity, gender, and other sociodemographic factors implicated in disparate health outcomes. We propose a set of strategic assertions to examine before, during, and after adoption of these technologies in order to facilitate healthcare equity across all patient population groups. The purpose is to enable generalists to promote engagement with technology companies and co-create, promote, or support innovation and insights that can potentially inform decision-making and health care equity.",
      "journal": "Journal of general internal medicine",
      "year": "2021",
      "doi": "10.1007/s11606-021-06846-x",
      "authors": "Clark Cheryl R et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Delivery of Health Care; Humans; Medicine; Primary Health Care; Technology",
      "pub_types": "Editorial; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34027610/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8481410",
      "ft_text_length": 17217,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8481410)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "34151976",
      "title": "Towards gender equity in artificial intelligence and machine learning applications in dermatology.",
      "abstract": "There has been increased excitement around the use of machine learning (ML) and artificial intelligence (AI) in dermatology for the diagnosis of skin cancers and assessment of other dermatologic conditions. As these technologies continue to expand, it is essential to ensure they do not create or widen sex- and gender-based disparities in care. While desirable bias may result from the explicit inclusion of sex or gender in diagnostic criteria of diseases with gender-based differences, undesirable biases can result from usage of datasets with an underrepresentation of certain groups. We believe that sex and gender differences should be taken into consideration in ML/AI algorithms in dermatology because there are important differences in the epidemiology and clinical presentation of dermatologic conditions including skin cancers, sex-specific cancers, and autoimmune conditions. We present recommendations for ensuring sex and gender equity in the development of ML/AI tools in dermatology to increase desirable bias and avoid undesirable bias.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2022",
      "doi": "10.1093/jamia/ocab113",
      "authors": "Lee Michelle S et al.",
      "keywords": "artificial intelligence; dermatology; disparities; equity; gender; machine learning",
      "mesh_terms": "Algorithms; Artificial Intelligence; Dermatology; Gender Equity; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34151976/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8757299",
      "ft_text_length": 1062,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8757299)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "34169495",
      "title": "The COVID-19 Pandemic and the Need for an Integrated and Equitable Approach: An International Expert Consensus Paper.",
      "abstract": "BACKGROUND: One year after the declaration of the coronavirus disease 2019 (COVID-19) pandemic by the World Health Organization (WHO) and despite the implementation of mandatory physical barriers and social distancing, humanity remains challenged by a long-lasting and devastating public health crisis. MANAGEMENT: Non-pharmacological interventions (NPIs) are efficient mitigation strategies. The success of these NPIs is dependent on the approval and commitment of the population. The launch of a mass vaccination program in many countries in late December 2020 with mRNA vaccines, adenovirus-based vaccines, and inactivated virus vaccines has generated hope for the end of the pandemic. CURRENT ISSUES: The continuous appearance of new pathogenic viral strains and the ability of vaccines to prevent infection and transmission raise important concerns as we try to achieve community immunity against severe acute respiratory syndrome coronavirus type 2 (SARS-CoV-2) and its variants. The need of a second and even third generation of vaccines has already been acknowledged by the WHO and governments. PERSPECTIVES: There is a critical and urgent need for a balanced and integrated strategy for the management of the COVID-19 outbreaks organized on three axes: (1) Prevention of the SARS-CoV-2 infection, (2) Detection and early diagnosis of patients at risk of disease worsening, and (3) Anticipation of medical care (PDA). CONCLUSION: The \"PDA strategy\" integrated into state policy for the support and expansion of health systems and introduction of digital organizations (i.e., telemedicine, e-Health, artificial intelligence, and machine-learning technology) is of major importance for the preservation of citizens' health and life world-wide.",
      "journal": "Thrombosis and haemostasis",
      "year": "2021",
      "doi": "10.1055/a-1535-8807",
      "authors": "Gerotziafas Grigoris T et al.",
      "keywords": "",
      "mesh_terms": "COVID-19; COVID-19 Testing; COVID-19 Vaccines; Disease Management; Humans; Immunization Programs; Pandemics; Public Health; Risk Assessment; SARS-CoV-2",
      "pub_types": "Journal Article; Consensus Statement",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34169495/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8322591",
      "ft_text_length": 50499,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8322591)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "34522832",
      "title": "Representation Learning with Statistical Independence to Mitigate Bias.",
      "abstract": "Presence of bias (in datasets or tasks) is inarguably one of the most critical challenges in machine learning applications that has alluded to pivotal debates in recent years. Such challenges range from spurious associations between variables in medical studies to the bias of race in gender or face recognition systems. Controlling for all types of biases in the dataset curation stage is cumbersome and sometimes impossible. The alternative is to use the available data and build models incorporating fair representation learning. In this paper, we propose such a model based on adversarial training with two competing objectives to learn features that have (1) maximum discriminative power with respect to the task and (2) minimal statistical mean dependence with the protected (bias) variable(s). Our approach does so by incorporating a new adversarial loss function that encourages a vanished correlation between the bias and the learned features. We apply our method to synthetic data, medical images (containing task bias), and a dataset for gender classification (containing dataset bias). Our results show that the learned features by our method not only result in superior prediction performance but also are unbiased.",
      "journal": "IEEE Winter Conference on Applications of Computer Vision. IEEE Winter Conference on Applications of Computer Vision",
      "year": "2021",
      "doi": "10.1109/wacv48630.2021.00256",
      "authors": "Adeli Ehsan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34522832/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8436589",
      "ft_text_length": 1228,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8436589)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "34544367",
      "title": "Minimizing bias in massive multi-arm observational studies with BCAUS: balancing covariates automatically using supervision.",
      "abstract": "BACKGROUND: Observational studies are increasingly being used to provide supplementary evidence in addition to Randomized Control Trials (RCTs) because they provide a scale and diversity of participants and outcomes that would be infeasible in an RCT. Additionally, they more closely reflect the settings in which the studied interventions will be applied in the future. Well-established propensity-score-based methods exist to overcome the challenges of working with observational data to estimate causal effects. These methods also provide quality assurance diagnostics to evaluate the degree to which bias has been removed and the estimates can be trusted. In large medical datasets it is common to find the same underlying health condition being treated with a variety of distinct drugs or drug combinations. Conventional methods require a manual iterative workflow, making them scale poorly to studies with many intervention arms. In such situations, automated causal inference methods that are compatible with traditional propensity-score-based workflows are highly desirable. METHODS: We introduce an automated causal inference method BCAUS, that features a deep-neural-network-based propensity model that is trained with a loss which penalizes both the incorrect prediction of the assigned treatment as well as the degree of imbalance between the inverse probability weighted covariates. The network is trained end-to-end by dynamically adjusting the loss term for each training batch such that the relative contributions from the two loss components are held fixed. Trained BCAUS models can be used in conjunction with traditional propensity-score-based methods to estimate causal treatment effects. RESULTS: We tested BCAUS on the semi-synthetic Infant Health & Development Program dataset with a single intervention arm, and a real-world observational study of diabetes interventions with over 100,000 individuals spread across more than a hundred intervention arms. When compared against other recently proposed automated causal inference methods, BCAUS had competitive accuracy for estimating synthetic treatment effects and provided highly concordant estimates on the real-world dataset but was an order-of-magnitude faster. CONCLUSIONS: BCAUS is directly compatible with trusted protocols to estimate treatment effects and diagnose the quality of those estimates, while making the established approaches automatically scalable to an arbitrary number of simultaneous intervention arms without any need for manual iteration.",
      "journal": "BMC medical research methodology",
      "year": "2021",
      "doi": "10.1186/s12874-021-01383-x",
      "authors": "Belthangady Chinmay et al.",
      "keywords": "Causal inference; Deep learning; Neural networks; Observational studies",
      "mesh_terms": "Bias; Causality; Humans; Propensity Score",
      "pub_types": "Journal Article; Observational Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34544367/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8454087",
      "ft_text_length": 42152,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8454087)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "34567966",
      "title": "Ethics of artificial intelligence in global health: Explainability, algorithmic bias and trust.",
      "abstract": "AI has the potential to disrupt and transform the way we deliver care globally. It is reputed to be able to improve the accuracy of diagnoses and treatments, and make the provision of services more efficient and effective. In surgery, AI systems could lead to more accurate diagnoses of health problems and help surgeons better care for their patients. In the context of lower-and-middle-income-countries (LMICs), where access to healthcare still remains a global problem, AI could facilitate access to healthcare professionals and services, even specialist services, for millions of people. The ability of AI to deliver on its promises, however, depends on successfully resolving the ethical and practical issues identified, including that of explainability and algorithmic bias. Even though such issues might appear as being merely practical or technical ones, their closer examination uncovers questions of value, fairness and trust. It should not be left to AI developers, being research institutions or global tech companies, to decide how to resolve these ethical questions. Particularly, relying only on the trustworthiness of companies and institutions to address ethical issues relating to justice, fairness and health equality would be unsuitable and unwise. The pathway to a fair, appropriate and relevant AI necessitates the development, and critically, successful implementation of national and international rules and regulations that define the parameters and set the boundaries of operation and engagement.",
      "journal": "Journal of oral biology and craniofacial research",
      "year": "2021",
      "doi": "10.1016/j.jobcr.2021.09.004",
      "authors": "Kerasidou Angeliki",
      "keywords": "Algorithmic bias; Artificial intelligence; Explainability; Global health; Lower-middle-income-countries (LMICS); Trust",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34567966/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8449079",
      "ft_text_length": 1522,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8449079)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "34580088",
      "title": "Influence of social determinants of health and county vaccination rates on machine learning models to predict COVID-19 case growth in Tennessee.",
      "abstract": "INTRODUCTION: The SARS-CoV-2 (COVID-19) pandemic has exposed health disparities throughout the USA, particularly among racial and ethnic minorities. As a result, there is a need for data-driven approaches to pinpoint the unique constellation of clinical and social determinants of health (SDOH) risk factors that give rise to poor patient outcomes following infection in US communities. METHODS: We combined county-level COVID-19 testing data, COVID-19 vaccination rates and SDOH information in Tennessee. Between February and May 2021, we trained machine learning models on a semimonthly basis using these datasets to predict COVID-19 incidence in Tennessee counties. We then analyzed SDOH data features at each time point to rank the impact of each feature on model performance. RESULTS: Our results indicate that COVID-19 vaccination rates play a crucial role in determining future COVID-19 disease risk. Beginning in mid-March 2021, higher vaccination rates significantly correlated with lower COVID-19 case growth predictions. Further, as the relative importance of COVID-19 vaccination data features grew, demographic SDOH features such as age, race and ethnicity decreased while the impact of socioeconomic and environmental factors, including access to healthcare and transportation, increased. CONCLUSION: Incorporating a data framework to track the evolving patterns of community-level SDOH risk factors could provide policy-makers with additional data resources to improve health equity and resilience to future public health emergencies.",
      "journal": "BMJ health & care informatics",
      "year": "2021",
      "doi": "10.1136/bmjhci-2021-100439",
      "authors": "Wylezinski Lukasz S et al.",
      "keywords": "COVID-19; artificial intelligence; health equity; machine learning; public health",
      "mesh_terms": "COVID-19; COVID-19 Testing; COVID-19 Vaccines; Humans; Machine Learning; Models, Theoretical; Social Determinants of Health; Tennessee; Vaccination",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34580088/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8478575",
      "ft_text_length": 5624,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8478575)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "34868674",
      "title": "Racial and Socioeconomic Disparities in Out-Of-Hospital Cardiac Arrest Outcomes: Artificial Intelligence-Augmented Propensity Score and Geospatial Cohort Analysis of 3,952 Patients.",
      "abstract": "INTRODUCTION: Social disparities in out-of-hospital cardiac arrest (OHCA) outcomes are preventable, costly, and unjust. We sought to perform the first large artificial intelligence- (AI-) guided statistical and geographic information system (GIS) analysis of a multiyear and multisite cohort for OHCA outcomes (incidence and poor neurological disposition). METHOD: We conducted a retrospective cohort analysis of a prospectively collected multicenter dataset of adult patients who sequentially presented to Houston metro area hospitals from 01/01/07-01/01/16. Then AI-based machine learning (backward propagation neural network) augmented multivariable regression and GIS heat mapping were performed. RESULTS: Of 3,952 OHCA patients across 38 hospitals, African Americans were the most likely to suffer OHCA despite representing a significantly lower percentage of the population (42.6 versus 22.8%; p < 0.001). Compared to Caucasians, they were significantly more likely to have poor neurological disposition (OR 2.21, 95%CI 1.25-3.92; p=0.006) and be discharged to a facility instead of home (OR 1.39, 95%CI 1.05-1.85; p=0.023). Compared to the safety net hospital system primarily serving poorer African Americans, the university hospital serving primarily higher income commercially and Medicare insured patients had the lowest odds of death (OR 0.45, p < 0.001). Each additional $10,000 above median household income was associated with a decrease in the total number of cardiac arrests per zip code by 2.86 (95%CI -4.26- -1.46; p < 0.001); zip codes with a median income above $54,600 versus the federal poverty level had 14.62 fewer arrests (p < 0.001). GIS maps showed convergence of the greater density of poor neurologic outcome cases and greater density of poorer African American residences. CONCLUSION: This large, longitudinal AI-guided analysis statistically and geographically identifies racial and socioeconomic disparities in OHCA outcomes in a way that may allow targeted medical and public health coordinated efforts to improve clinical, cost, and social equity outcomes.",
      "journal": "Cardiology research and practice",
      "year": "2021",
      "doi": "10.1155/2021/3180987",
      "authors": "Monlezun Dominique J et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34868674/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8635948",
      "ft_text_length": 17502,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8635948)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "34895784",
      "title": "Willingness to vaccinate against SARS-CoV-2: The role of reasoning biases and conspiracist ideation.",
      "abstract": "UNLABELLED: BACKGR1OUND: Widespread vaccine hesitancy and refusal complicate containment of the SARS-CoV-2 pandemic. Extant research indicates that biased reasoning and conspiracist ideation discourage vaccination. However, causal pathways from these constructs to vaccine hesitancy and refusal remain underspecified, impeding efforts to intervene and increase vaccine uptake. METHOD: 554 participants who denied prior SARS-CoV-2 vaccination completed self-report measures of SARS-CoV-2 vaccine intentions, conspiracist ideation, and constructs from the Health Belief Model of medical decision-making (such as perceived vaccine dangerousness) along with tasks measuring reasoning biases (such as those concerning data gathering behavior). Cutting-edge machine learning algorithms (Greedy Fast Causal Inference) and psychometric network analysis were used to elucidate causal pathways to (and from) vaccine intentions. RESULTS: Results indicated that a bias toward reduced data gathering during reasoning may cause paranoia, increasing the perceived dangerousness of vaccines and thereby reducing willingness to vaccinate. Existing interventions that target data gathering and paranoia therefore hold promise for encouraging vaccination. Additionally, reduced willingness to vaccinate was identified as a likely cause of belief in conspiracy theories, subverting the common assumption that the opposite causal relation exists. Finally, perceived severity of SARS-CoV-2 infection and perceived vaccine dangerousness (but not effectiveness) were potential direct causes of willingness to vaccinate, providing partial support for the Health Belief Model's applicability to SARS-CoV-2 vaccine decisions. CONCLUSIONS: These insights significantly advance our understanding of the underpinnings of vaccine intentions and should scaffold efforts to prepare more effective interventions on hesitancy for deployment during future pandemics.",
      "journal": "Vaccine",
      "year": "2022",
      "doi": "10.1016/j.vaccine.2021.11.079",
      "authors": "Bronstein Michael V et al.",
      "keywords": "COVID-19; Conspiracy theories; GFCI; Reasoning; SARS-CoV-2; Vaccines",
      "mesh_terms": "Bias; COVID-19; COVID-19 Vaccines; Humans; SARS-CoV-2; Vaccination; Vaccination Hesitancy",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34895784/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8642163",
      "ft_text_length": 49988,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8642163)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "34912242",
      "title": "Gender Bias in Artificial Intelligence: Severity Prediction at an Early Stage of COVID-19.",
      "abstract": "Artificial intelligence (AI) technologies have been applied in various medical domains to predict patient outcomes with high accuracy. As AI becomes more widely adopted, the problem of model bias is increasingly apparent. In this study, we investigate the model bias that can occur when training a model using datasets for only one particular gender and aim to present new insights into the bias issue. For the investigation, we considered an AI model that predicts severity at an early stage based on the medical records of coronavirus disease (COVID-19) patients. For 5,601 confirmed COVID-19 patients, we used 37 medical records, namely, basic patient information, physical index, initial examination findings, clinical findings, comorbidity diseases, and general blood test results at an early stage. To investigate the gender-based AI model bias, we trained and evaluated two separate models-one that was trained using only the male group, and the other using only the female group. When the model trained by the male-group data was applied to the female testing data, the overall accuracy decreased-sensitivity from 0.93 to 0.86, specificity from 0.92 to 0.86, accuracy from 0.92 to 0.86, balanced accuracy from 0.93 to 0.86, and area under the curve (AUC) from 0.97 to 0.94. Similarly, when the model trained by the female-group data was applied to the male testing data, once again, the overall accuracy decreased-sensitivity from 0.97 to 0.90, specificity from 0.96 to 0.91, accuracy from 0.96 to 0.91, balanced accuracy from 0.96 to 0.90, and AUC from 0.97 to 0.95. Furthermore, when we evaluated each gender-dependent model with the test data from the same gender used for training, the resultant accuracy was also lower than that from the unbiased model.",
      "journal": "Frontiers in physiology",
      "year": "2021",
      "doi": "10.3389/fphys.2021.778720",
      "authors": "Chung Heewon et al.",
      "keywords": "COVID-19; artificial intelligence bias; feature importance; gender dependent bias; severity prediction",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34912242/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8667070",
      "ft_text_length": 28005,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8667070)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "34918101",
      "title": "Gender-sensitive word embeddings for healthcare.",
      "abstract": "OBJECTIVE: To analyze gender bias in clinical trials, to design an algorithm that mitigates the effects of biases of gender representation on natural-language (NLP) systems trained on text drawn from clinical trials, and to evaluate its performance. MATERIALS AND METHODS: We analyze gender bias in clinical trials described by 16\u00a0772 PubMed abstracts (2008-2018). We present a method to augment word embeddings, the core building block of NLP-centric representations, by weighting abstracts by the number of women participants in the trial. We evaluate the resulting gender-sensitive embeddings performance on several clinical prediction tasks: comorbidity classification, hospital length of stay prediction, and intensive care unit (ICU) readmission prediction. RESULTS: For female patients, the gender-sensitive model area under the receiver-operator characteristic (AUROC) is 0.86 versus the baseline of 0.81 for comorbidity classification, mean absolute error 4.59 versus the baseline of 4.66 for length of stay prediction, and AUROC 0.69 versus 0.67 for ICU readmission. All results are statistically significant. DISCUSSION: Women have been underrepresented in clinical trials. Thus, using the broad clinical trials literature as training data for statistical language models could result in biased models, with deficits in knowledge about women. The method presented enables gender-sensitive use of publications as training data for word embeddings. In experiments, the gender-sensitive embeddings show better performance than baseline embeddings for the clinical tasks studied. The results highlight opportunities for recognizing and addressing gender and other representational biases in the clinical trials literature. CONCLUSION: Addressing representational biases in data for training NLP embeddings can lead to better results on downstream tasks for underrepresented populations.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2022",
      "doi": "10.1093/jamia/ocab279",
      "authors": "Agmon Shunit et al.",
      "keywords": "algorithms; bias; gender; statistical models; word embeddings",
      "mesh_terms": "Clinical Trials as Topic; Delivery of Health Care; Female; Humans; Male; Natural Language Processing; PubMed; Sexism",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34918101/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8800511",
      "ft_text_length": 1897,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8800511)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "34951033",
      "title": "Automatic coronavirus disease 2019 diagnosis based on chest radiography and deep learning - Success story or dataset bias?",
      "abstract": "PURPOSE: Over the last 2 years, the artificial intelligence (AI) community has presented several automatic screening tools for coronavirus disease 2019 (COVID-19) based on chest radiography (CXR), with reported accuracies often well over 90%. However, it has been noted that many of these studies have likely suffered from dataset bias, leading to overly optimistic results. The purpose of this study was to thoroughly investigate to what extent biases have influenced the performance of a range of previously proposed and promising convolutional neural networks (CNNs), and to determine what performance can be expected with current CNNs on a realistic and unbiased dataset. METHODS: Five CNNs for COVID-19 positive/negative classification were implemented for evaluation, namely VGG19, ResNet50, InceptionV3, DenseNet201, and COVID-Net. To perform both internal and cross-dataset evaluations, four datasets were created. The first dataset Valencian Region Medical Image Bank (BIMCV) followed strict reverse transcriptase-polymerase chain reaction (RT-PCR) test criteria and was created from a single reliable open access databank, while the second dataset (COVIDxB8) was created through a combination of six online CXR repositories. The third and fourth datasets were created by combining the opposing classes from the BIMCV and COVIDxB8 datasets. To decrease inter-dataset variability, a pre-processing workflow of resizing, normalization, and histogram equalization were applied to all datasets. Classification performance was evaluated on unseen test sets using precision and recall. A qualitative sanity check was performed by evaluating saliency maps displaying the top 5%, 10%, and 20% most salient segments in the input CXRs, to evaluate whether the CNNs were using relevant information for decision making. In an additional experiment and to further investigate the origin of potential dataset bias, all pixel values outside the lungs were set to zero through automatic lung segmentation before training and testing. RESULTS: When trained and evaluated on the single online source dataset (BIMCV), the performance of all CNNs is relatively low (precision: 0.65-0.72, recall: 0.59-0.71), but remains relatively consistent during external evaluation (precision: 0.58-0.82, recall: 0.57-0.72). On the contrary, when trained and internally evaluated on the combinatory datasets, all CNNs performed well across all metrics (precision: 0.94-1.00, recall: 0.77-1.00). However, when subsequently evaluated cross-dataset, results dropped substantially (precision: 0.10-0.61, recall: 0.04-0.80). For all datasets, saliency maps revealed the CNNs rarely focus on areas inside the lungs for their decision-making. However, even when setting all pixel values outside the lungs to zero, classification performance does not change and dataset bias remains. CONCLUSIONS: Results in this study confirm that when trained on a combinatory dataset, CNNs tend to learn the origin of the CXRs rather than the presence or absence of disease, a behavior known as short-cut learning. The bias is shown to originate from differences in overall pixel values rather than embedded text or symbols, despite consistent image pre-processing. When trained on a reliable, and realistic single-source dataset in which non-lung pixels have been masked, CNNs currently show limited sensitivity\u00a0(<70%) for COVID-19 infection in CXR, questioning their use as a reliable automatic screening tool.",
      "journal": "Medical physics",
      "year": "2022",
      "doi": "10.1002/mp.15419",
      "authors": "Dhont Jennifer et al.",
      "keywords": "COVID-19; X-ray imaging; artificial intelligence; dataset bias",
      "mesh_terms": "Artificial Intelligence; Bias; COVID-19; Deep Learning; Humans; Radiography; SARS-CoV-2",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34951033/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9015341",
      "ft_text_length": 26318,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9015341)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35044842",
      "title": "Negative Patient Descriptors: Documenting Racial Bias In The Electronic Health Record.",
      "abstract": "Little is known about how racism and bias may be communicated in the medical record. This study used machine learning to analyze electronic health records (EHRs) from an urban academic medical center and to investigate whether providers' use of negative patient descriptors varied by patient race or ethnicity. We analyzed a sample of 40,113 history and physical notes (January 2019-October 2020) from 18,459 patients for sentences containing a negative descriptor (for example, resistant or noncompliant) of the patient or the patient's behavior. We used mixed effects logistic regression to determine the odds of finding at least one negative descriptor as a function of the patient's race or ethnicity, controlling for sociodemographic and health characteristics. Compared with White patients, Black patients had 2.54 times the odds of having at least one negative descriptor in the history and physical notes. Our findings raise concerns about stigmatizing language in the EHR and its potential to exacerbate racial and ethnic health care disparities.",
      "journal": "Health affairs (Project Hope)",
      "year": "2022",
      "doi": "10.1377/hlthaff.2021.01423",
      "authors": "Sun Michael et al.",
      "keywords": "",
      "mesh_terms": "Black People; Electronic Health Records; Ethnicity; Healthcare Disparities; Humans; Racism",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35044842/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8973827",
      "ft_text_length": 28367,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8973827)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35048111",
      "title": "Digital Ageism: Challenges and Opportunities in Artificial Intelligence for Older Adults.",
      "abstract": "Artificial intelligence (AI) and machine learning are changing our world through their impact on sectors including health care, education, employment, finance, and law. AI systems are developed using data that reflect the implicit and explicit biases of society, and there are significant concerns about how the predictive models in AI systems amplify inequity, privilege, and power in society. The widespread applications of AI have led to mainstream discourse about how AI systems are perpetuating racism, sexism, and classism; yet, concerns about ageism have been largely absent in the AI bias literature. Given the globally aging population and proliferation of AI, there is a need to critically examine the presence of age-related bias in AI systems. This forum article discusses ageism in AI systems and introduces a conceptual model that outlines intersecting pathways of technology development that can produce and reinforce digital ageism in AI systems. We also describe the broader ethical and legal implications and considerations for future directions in digital ageism research to advance knowledge in the field and deepen our understanding of how ageism in AI is fostered by broader cycles of injustice.",
      "journal": "The Gerontologist",
      "year": "2022",
      "doi": "10.1093/geront/gnab167",
      "authors": "Chu Charlene H et al.",
      "keywords": "Bias; Gerontology; Machine learning; Technology",
      "mesh_terms": "Aged; Ageism; Artificial Intelligence; Delivery of Health Care; Humans; Machine Learning; Racism",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35048111/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9372891",
      "ft_text_length": 30929,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9372891)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35089868",
      "title": "Precision Public Health and Structural Racism in the United States: Promoting Health Equity in the COVID-19 Pandemic Response.",
      "abstract": "The COVID-19 pandemic has revealed deeply entrenched structural inequalities that resulted in an excess of mortality and morbidity in certain racial and ethnic groups in the United States. Therefore, this paper examines from the US perspective how structural racism and defective data collection on racial and ethnic minorities can negatively influence the development of precision public health (PPH) approaches to tackle the ongoing COVID-19 pandemic. Importantly, the effects of structural and data racism on the development of fair and inclusive data-driven components of PPH interventions are discussed, such as with the use of machine learning algorithms to predict public health risks. The objective of this viewpoint is thus to inform public health policymaking with regard to the development of ethically sound PPH interventions against COVID-19. Particular attention is given to components of structural racism (eg, hospital segregation, implicit and organizational bias, digital divide, and sociopolitical influences) that are likely to hinder such approaches from achieving their social justice and health equity goals.",
      "journal": "JMIR public health and surveillance",
      "year": "2022",
      "doi": "10.2196/33277",
      "authors": "Genevi\u00e8ve Lester Darryl et al.",
      "keywords": "COVID-19; SARS-CoV-2; discrimination; disparity; equity; health equity; inequality; morbidity; mortality; pandemic; precision health; precision public health; public health; racism; social justice; stigma; structural racism",
      "mesh_terms": "COVID-19; Data Collection; Health Equity; Humans; Pandemics; Public Health; SARS-CoV-2; Systemic Racism; United States",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35089868/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8900917",
      "ft_text_length": 22924,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8900917)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35142367",
      "title": "A joint fairness model with applications to risk predictions for underrepresented populations.",
      "abstract": "In data collection for predictive modeling, underrepresentation of certain groups, based on gender, race/ethnicity, or age, may yield less accurate predictions for these groups. Recently, this issue of fairness in predictions has attracted significant attention, as data-driven models are increasingly utilized to perform crucial decision-making tasks. Existing methods to achieve fairness in the machine learning literature typically build a single prediction model in a manner that encourages fair prediction performance for all groups. These approaches have two major limitations: (i) fairness is often achieved by compromising accuracy for some groups; (ii) the underlying relationship between dependent and independent variables may not be the same across groups. We propose a joint fairness model (JFM) approach for logistic regression models for binary outcomes that estimates group-specific classifiers using a joint modeling objective function that incorporates fairness criteria for prediction. We introduce an accelerated smoothing proximal gradient algorithm to solve the convex objective function, and present the key asymptotic properties of the JFM estimates. Through simulations, we demonstrate the efficacy of the JFM in achieving good prediction performance and across-group parity, in comparison with the single fairness model, group-separate model, and group-ignorant model, especially when the minority group's sample size is small. Finally, we demonstrate the utility of the JFM method in a real-world example to obtain fair risk predictions for underrepresented older patients diagnosed with coronavirus disease 2019 (COVID-19).",
      "journal": "Biometrics",
      "year": "2023",
      "doi": "10.1111/biom.13632",
      "authors": "Do Hyungrok et al.",
      "keywords": "algorithmic bias; algorithmic fairness; joint estimation; underrepresented population",
      "mesh_terms": "Humans; COVID-19; Logistic Models; Algorithms",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35142367/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9363518",
      "ft_text_length": 1662,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9363518)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35184750",
      "title": "Systematic comparison of published host gene expression signatures for bacterial/viral discrimination.",
      "abstract": "BACKGROUND: Measuring host gene expression is a promising diagnostic strategy to discriminate bacterial and viral infections. Multiple signatures of varying size, complexity, and target populations have been described. However, there is little information to indicate how the performance of various published signatures compare to one another. METHODS: This systematic comparison of host gene expression signatures evaluated the performance of 28 signatures, validating them in 4589 subjects from 51 publicly available datasets. Thirteen COVID-specific datasets with 1416 subjects were included in a separate analysis. Individual signature performance was evaluated using the area under the receiving operating characteristic curve (AUC) value. Overall signature performance was evaluated using median AUCs and accuracies. RESULTS: Signature performance varied widely, with median AUCs ranging from 0.55 to 0.96 for bacterial classification and 0.69-0.97 for viral classification. Signature size varied (1-398 genes), with smaller signatures generally performing more poorly (P < 0.04). Viral infection was easier to diagnose than bacterial infection (84% vs. 79% overall accuracy, respectively; P < .001). Host gene expression classifiers performed more poorly in some pediatric populations (3 months-1 year and 2-11 years) compared to the adult population for both bacterial infection (73% and 70% vs. 82%, respectively; P < .001) and viral infection (80% and 79% vs. 88%, respectively; P < .001). We did not observe classification differences based on illness severity as defined by ICU admission for bacterial or viral infections. The median AUC across all signatures for COVID-19 classification was 0.80 compared to 0.83 for viral classification in the same datasets. CONCLUSIONS: In this systematic comparison of 28 host gene expression signatures, we observed differences based on a signature's size and characteristics of the validation population, including age and infection type. However, populations used for signature discovery did not impact performance, underscoring the redundancy among many of these signatures. Furthermore, differential performance in specific populations may only be observable through this type of large-scale validation.",
      "journal": "Genome medicine",
      "year": "2022",
      "doi": "10.1186/s13073-022-01025-x",
      "authors": "Bodkin Nicholas et al.",
      "keywords": "Biomarkers; Diagnostics; Gene expression; Infectious disease; Machine learning",
      "mesh_terms": "Adult; Bacterial Infections; Biomarkers; COVID-19; Child; Cohort Studies; Datasets as Topic; Diagnosis, Differential; Gene Expression Profiling; Genetic Association Studies; Host-Pathogen Interactions; Humans; Publications; SARS-CoV-2; Transcriptome; Validation Studies as Topic; Virus Diseases",
      "pub_types": "Comparative Study; Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35184750/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8858657",
      "ft_text_length": 47641,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8858657)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35185011",
      "title": "Use of a community advisory board to build equitable algorithms for participation in clinical trials: a protocol paper for HoPeNET.",
      "abstract": "INTRODUCTION: Participation from racial and ethnic minorities in clinical trials has been burdened by issues surrounding mistrust and access to healthcare. There is emerging use of machine learning (ML) in clinical trial recruitment and evaluation. However, for individuals from groups who are recipients of societal biases, utilisation of ML can lead to the creation and use of biased algorithms. To minimise bias, the design of equitable ML tools that advance health equity could be guided by community engagement processes. The Howard University Partnership with the National Institutes of Health for Equitable Clinical Trial Participation for Racial/Ethnic Communities Underrepresented in Research (HoPeNET) seeks to create an ML-based infrastructure from community advisory board (CAB) experiences to enhance participation of African-Americans/Blacks in clinical trials. METHODS AND ANALYSIS: This triphased cross-sectional study (24 months, n=56) will create a CAB of community members and research investigators. The three phases of the study include: (1) identification of perceived barriers/facilitators to clinical trial engagement through qualitative/quantitative methods and systems-based model building participation; (2) operation of CAB meetings and (3) development of a predictive ML tool and outcome evaluation. Identified predictors from the participant-derived systems-based map will be used for the ML tool development. ETHICS AND DISSEMINATION: We anticipate minimum risk for participants. Institutional review board approval and informed consent has been obtained and patient confidentiality ensured.",
      "journal": "BMJ health & care informatics",
      "year": "2022",
      "doi": "10.1136/bmjhci-2021-100453",
      "authors": "Farmer Nicole et al.",
      "keywords": "BMJ health informatics; artificial intelligence; health equity",
      "mesh_terms": "Algorithms; Clinical Trials as Topic; Cross-Sectional Studies; Humans; Patient Selection",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35185011/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8860013",
      "ft_text_length": 13380,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8860013)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35211742",
      "title": "Observability and its impact on differential bias for clinical prediction models.",
      "abstract": "OBJECTIVE: Electronic health records have incomplete capture of patient outcomes. We consider the case when observability is differential across a predictor. Including such a predictor (sensitive variable) can lead to algorithmic bias, potentially exacerbating health inequities. MATERIALS AND METHODS: We define bias for a clinical prediction model (CPM) as the difference between the true and estimated risk, and differential bias as bias that differs across a sensitive variable. We illustrate the genesis of differential bias via a 2-stage process, where conditional on having the outcome of interest, the outcome is differentially observed. We use simulations and a real-data example to demonstrate the possible impact of including a sensitive variable in a CPM. RESULTS: If there is differential observability based on a sensitive variable, including it in a CPM can induce differential bias. However, if the sensitive variable impacts the outcome but not observability, it is better to include it. When a sensitive variable impacts both observability and the outcome no simple recommendation can be provided. We show that one cannot use observed data to detect differential bias. DISCUSSION: Our study furthers the literature on observability, showing that differential observability can lead to algorithmic bias. This highlights the importance of considering whether to include sensitive variables in CPMs. CONCLUSION: Including a sensitive variable in a CPM depends on whether it truly affects the outcome or just the observability of the outcome. Since this cannot be distinguished with observed data, observability is an implicit assumption of CPMs.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2022",
      "doi": "10.1093/jamia/ocac019",
      "authors": "Yan Mengying et al.",
      "keywords": "algorithmic bias; clinical prediction models; electronic health record; health equity; observability",
      "mesh_terms": "Bias; Humans; Models, Statistical; Prognosis",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35211742/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9006687",
      "ft_text_length": 1666,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9006687)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35239737",
      "title": "A test of affect processing bias in response to affect regulation.",
      "abstract": "In this study we merged methods from machine learning and human neuroimaging to test the role of self-induced affect processing states in biasing the affect processing of subsequent image stimuli. To test this relationship we developed a novel paradigm in which (n = 40) healthy adult participants observed affective neural decodings of their real-time functional magnetic resonance image (rtfMRI) responses as feedback to guide explicit regulation of their brain (and corollary affect processing) state towards a positive valence goal state. By this method individual differences in affect regulation ability were controlled. Attaining this brain-affect goal state triggered the presentation of pseudo-randomly selected affectively congruent (positive valence) or incongruent (negative valence) image stimuli drawn from the International Affective Picture Set. Separately, subjects passively viewed randomly triggered positively and negatively valent image stimuli during fMRI acquisition. Multivariate neural decodings of the affect processing induced by these stimuli were modeled using the task trial type (state- versus randomly-triggered) as the fixed-effect of a general linear mixed-effects model. Random effects were modeled subject-wise. We found that self-induction of a positive valence brain state significantly positively biased valence processing of subsequent stimuli. As a manipulation check, we validated affect processing state induction achieved by the image stimuli using independent psychophysiological response measures of hedonic valence and autonomic arousal. We also validated the predictive fidelity of the trained neural decoding models using brain states induced by an out-of-sample set of image stimuli. Beyond its contribution to our understanding of the neural mechanisms that bias affect processing, this work demonstrated the viability of novel experimental paradigms triggered by pre-defined cognitive states. This line of individual differences research potentially provides neuroimaging scientists with a valuable tool for exploring the roles and identities of intrinsic cognitive processing mechanisms that shape our perceptual processing of sensory stimuli.",
      "journal": "PloS one",
      "year": "2022",
      "doi": "10.1371/journal.pone.0264758",
      "authors": "Bush Keith A et al.",
      "keywords": "",
      "mesh_terms": "Adult; Affect; Arousal; Brain; Brain Mapping; Emotions; Humans; Magnetic Resonance Imaging; Neuroimaging",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35239737/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8893671",
      "ft_text_length": 55299,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8893671)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35243993",
      "title": "Mitigating Racial Bias in Machine Learning.",
      "abstract": "When applied in the health sector, AI-based applications raise not only ethical but legal and safety concerns, where algorithms trained on data from majority populations can generate less accurate or reliable results for minorities and other disadvantaged groups.",
      "journal": "The Journal of law, medicine & ethics : a journal of the American Society of Law, Medicine & Ethics",
      "year": "2022",
      "doi": "10.1017/jme.2022.13",
      "authors": "Kostick-Quenet Kristin M et al.",
      "keywords": "Algorithmic Bias; Artificial Intelligence; Ethics; Machine Learning; Racial Bias",
      "mesh_terms": "Artificial Intelligence; Humans; Machine Learning; Racism",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35243993/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12140104",
      "ft_text_length": 263,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12140104)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35308985",
      "title": "Data and Model Biases in Social Media Analyses: A Case Study of COVID-19 Tweets.",
      "abstract": "During the coronavirus disease pandemic (COVID-19), social media platforms such as Twitter have become a venue for individuals, health professionals, and government agencies to share COVID-19 information. Twitter has been a popular source of data for researchers, especially for public health studies. However, the use of Twitter data for research also has drawbacks and barriers. Biases appear everywhere from data collection methods to modeling approaches, and those biases have not been systematically assessed. In this study, we examined six different data collection methods and three different machine learning (ML) models-commonly used in social media analysis-to assess data collection bias and measure ML models' sensitivity to data collection bias. We showed that (1) publicly available Twitter data collection endpoints with appropriate strategies can collect data that is reasonably representative of the Twitter universe; and (2) careful examinations of ML models' sensitivity to data collection bias are critical.",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "year": "2021",
      "doi": "10.1145/3400806.3400839",
      "authors": "Zhao Yunpeng et al.",
      "keywords": "",
      "mesh_terms": "Bias; COVID-19; Data Collection; Humans; Machine Learning; Social Media",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35308985/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8861742",
      "ft_text_length": 1027,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8861742)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35353048",
      "title": "Leveraging Machine Learning to Understand How Emotions Influence Equity Related Education: Quasi-Experimental Study.",
      "abstract": "BACKGROUND: Teaching and learning about topics such as bias are challenging due to the emotional nature of bias-related discourse. However, emotions can be challenging to study in health professions education for numerous reasons. With the emergence of machine learning and natural language processing, sentiment analysis (SA) has the potential to bridge the gap. OBJECTIVE: To improve our understanding of the role of emotions in bias-related discourse, we developed and conducted a SA of bias-related discourse among health professionals. METHODS: We conducted a 2-stage quasi-experimental study. First, we developed a SA (algorithm) within an existing archive of interviews with health professionals about bias. SA refers to a mechanism of analysis that evaluates the sentiment of textual data by assigning scores to textual components and calculating and assigning a sentiment value to the text. Next, we applied our SA algorithm to an archive of social media discourse on Twitter that contained equity-related hashtags to compare sentiment among health professionals and the general population. RESULTS: When tested on the initial archive, our SA algorithm was highly accurate compared to human scoring of sentiment. An analysis of bias-related social media discourse demonstrated that health professional tweets (n=555) were less neutral than the general population (n=6680) when discussing social issues on professionally associated accounts (\u03c72 [2, n=555)]=35.455; P<.001), suggesting that health professionals attach more sentiment to their posts on Twitter than seen in the general population. CONCLUSIONS: The finding that health professionals are more likely to show and convey emotions regarding equity-related issues on social media has implications for teaching and learning about sensitive topics related to health professions education. Such emotions must therefore be considered in the design, delivery, and evaluation of equity and bias-related education.",
      "journal": "JMIR medical education",
      "year": "2022",
      "doi": "10.2196/33934",
      "authors": "Sukhera Javeed et al.",
      "keywords": "bias; education; emotion; equity; medical education; sentiment analysis",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35353048/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9008524",
      "ft_text_length": 26642,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9008524)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35396996",
      "title": "Assessing socioeconomic bias in machine learning algorithms in health care: a case study of the HOUSES index.",
      "abstract": "OBJECTIVE: Artificial intelligence (AI) models may propagate harmful biases in performance and hence negatively affect the underserved. We aimed to assess the degree to which data quality of electronic health records (EHRs) affected by inequities related to low socioeconomic status (SES), results in differential performance of AI models across SES. MATERIALS AND METHODS: This study utilized existing machine learning models for predicting asthma exacerbation in children with asthma. We compared balanced error rate (BER) against different SES levels measured by HOUsing-based SocioEconomic Status measure (HOUSES) index. As a possible mechanism for differential performance, we also compared incompleteness of EHR information relevant to asthma care by SES. RESULTS: Asthmatic children with lower SES had larger BER than those with higher SES (eg, ratio = 1.35 for HOUSES Q1 vs Q2-Q4) and had a higher proportion of missing information relevant to asthma care (eg, 41% vs 24% for missing asthma severity and 12% vs 9.8% for undiagnosed asthma despite meeting asthma criteria). DISCUSSION: Our study suggests that lower SES is associated with worse predictive model performance. It also highlights the potential role of incomplete EHR data in this differential performance and suggests a way to mitigate this bias. CONCLUSION: The HOUSES index allows AI researchers to assess bias in predictive model performance by SES. Although our case study was based on a small sample size and a single-site study, the study results highlight a potential strategy for identifying bias by using an innovative SES measure.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2022",
      "doi": "10.1093/jamia/ocac052",
      "authors": "Juhn Young J et al.",
      "keywords": "HOUSES; algorithmic bias; artificial intelligence; electronic health records; social determinants of health",
      "mesh_terms": "Artificial Intelligence; Asthma; Bias; Child; Delivery of Health Care; Humans; Machine Learning; Social Class",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35396996/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9196683",
      "ft_text_length": 1615,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9196683)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35434676",
      "title": "Establishing an interdisciplinary research team for cardio-oncology artificial intelligence informatics precision and health equity.",
      "abstract": "STUDY OBJECTIVE: A multi-institutional interdisciplinary team was created to develop a research group focused on leveraging artificial intelligence and informatics for cardio-oncology patients. Cardio-oncology is an emerging medical field dedicated to prevention, screening, and management of adverse cardiovascular effects of cancer/ cancer therapies. Cardiovascular disease is a leading cause of death in cancer survivors. Cardiovascular risk in these patients is higher than in the general population. However, prediction and prevention of adverse cardiovascular events in individuals with a history of cancer/cancer treatment is challenging. Thus, establishing an interdisciplinary team to create cardiovascular risk stratification clinical decision aids for integration into electronic health records for oncology patients was considered crucial. DESIGN/SETTING/PARTICIPANTS: Core team members from the Medical College of Wisconsin (MCW), University of Wisconsin-Milwaukee (UWM), and Milwaukee School of Engineering (MSOE), and additional members from Cleveland Clinic, Mayo Clinic, and other institutions have joined forces to apply high-performance computing in cardio-oncology. RESULTS: The team is comprised of clinicians and researchers from relevant complementary and synergistic fields relevant to this work. The team has built an epidemiological cohort of ~5000 cancer survivors that will serve as a database for interdisciplinary multi-institutional artificial intelligence projects. CONCLUSION: Lessons learned from establishing this team, as well as initial findings from the epidemiology cohort, are presented. Barriers have been broken down to form a multi-institutional interdisciplinary team for health informatics research in cardio-oncology. A database of cancer survivors has been created collaboratively by the team and provides initial insight into cardiovascular outcomes and comorbidities in this population.",
      "journal": "American heart journal plus : cardiology research and practice",
      "year": "2022",
      "doi": "10.1016/j.ahjo.2022.100094",
      "authors": "Brown Sherry-Ann et al.",
      "keywords": "Artificial intelligence; Cancer survivorship; Cardio-oncology; Informatics; Team science",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35434676/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9012235",
      "ft_text_length": 55557,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9012235)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35471566",
      "title": "Analysis of Race and Sex Bias in the Autism Diagnostic Observation Schedule (ADOS-2).",
      "abstract": "IMPORTANCE: There are long-standing disparities in the prevalence of autism spectrum disorder (ASD) across race and sex. Surprisingly, few studies have examined whether these disparities arise partially out of systematic biases in the Autism Diagnostic Observation Schedule, Second Edition (ADOS-2), the reference standard measure of ASD. OBJECTIVE: To examine differential item functioning (DIF) of ADOS-2 items across sex and race. DESIGN, SETTING, AND PARTICIPANTS: This is a cross-sectional study of children who were evaluated for ASD between 2014 and 2020 at a specialty outpatient clinic located in the Mid-Atlantic region of the US. Data were analyzed from July 2021 to February 2022. EXPOSURES: Child race (Black/African American vs White) and sex (female vs male). MAIN OUTCOMES AND MEASURES: Item-level biases across ADOS-2 harmonized algorithm items, including social affect (SA; 10 items) and repetitive/restricted behaviors (RRBs; 4 items), were evaluated across 3 modules. Measurement bias was identified by examining DIF and differential test functioning (DTF), within a graded response, item response theory framework. Statistical significance was determined by a likelihood ratio \u03c72 test, and a series of metrics was used to examine the magnitude of DIF and DTF. RESULTS: A total of 6269 children (mean [SD] age, 6.77 [3.27] years; 1619 Black/African American [25.9%], 3151 White [50.3%], and 4970 male [79.4%]), were included in this study. Overall, 16 of 140 ADOS-2 diagnostic items (11%) had a significant DIF. For race, 8 items had a significant DIF, 6 of which involved SA. No single item showed DIF consistently across all modules. Most items with DIF had greater difficulty and poorer discrimination in Black/African American children compared with White children. For sex, 5 items showed significant DIF. DIF was split across SA and RRB. However, hand mannerisms evidenced DIF across all 5 algorithms, with generally greater difficulty. The magnitude of DIF was only moderate to large for 2 items: hand mannerisms (among female children) and repetitive interests (among Black/African American children). The overall estimated effect of DIF on total DTF was not large. CONCLUSIONS AND RELEVANCE: These findings suggest that the ADOS-2 does not have widespread systematic measurement bias across race or sex. However, the findings raise some concerns around underdetection that warrant further research.",
      "journal": "JAMA network open",
      "year": "2022",
      "doi": "10.1001/jamanetworkopen.2022.9498",
      "authors": "Kalb Luther G et al.",
      "keywords": "",
      "mesh_terms": "Autism Spectrum Disorder; Autistic Disorder; Child; Cross-Sectional Studies; Female; Humans; Male; Racial Groups; Sexism",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35471566/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9044110",
      "ft_text_length": 30297,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9044110)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35477689",
      "title": "Operationalising fairness in medical AI adoption: detection of early Alzheimer's disease with 2D CNN.",
      "abstract": "OBJECTIVES: To operationalise fairness in the adoption of medical artificial intelligence (AI) algorithms in terms of access to computational resources, the proposed approach is based on a two-dimensional (2D) convolutional neural networks (CNN), which provides a faster, cheaper and accurate-enough detection of early Alzheimer's disease (AD) and mild cognitive impairment (MCI), without the need for use of large training data sets or costly high-performance computing (HPC) infrastructures. METHODS: The standardised Alzheimer's Disease Neuroimaging Initiative (ADNI) data sets are used for the proposed model, with additional skull stripping, using the Brain Extraction Tool V.2approach. The 2D CNN architecture is based on LeNet-5, the Leaky Rectified Linear Unit activation function and a Sigmoid function were used, and batch normalisation was added after every convolutional layer to stabilise the learning process. The model was optimised by manually tuning all its hyperparameters. RESULTS: The model was evaluated in terms of accuracy, recall, precision and f1-score. The results demonstrate that the model predicted MCI with an accuracy of 0.735, passing the random guessing baseline of 0.521 and predicted AD with an accuracy of 0.837, passing the random guessing baseline of 0.536. DISCUSSION: The proposed approach can assist clinicians in the early diagnosis of AD and MCI, with high-enough accuracy, based on relatively smaller data sets, and without the need of HPC infrastructures. Such an approach can alleviate disparities and operationalise fairness in the adoption of medical algorithms. CONCLUSION: Medical AI algorithms should not be focused solely on accuracy but should also be evaluated with respect to how they might impact disparities and operationalise fairness in their adoption.",
      "journal": "BMJ health & care informatics",
      "year": "2022",
      "doi": "10.1136/bmjhci-2021-100485",
      "authors": "Heising Luca et al.",
      "keywords": "artificial intelligence; medical informatics applications; neural networks, computer",
      "mesh_terms": "Alzheimer Disease; Artificial Intelligence; Cognitive Dysfunction; Humans; Magnetic Resonance Imaging; Neural Networks, Computer",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35477689/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9047889",
      "ft_text_length": 22657,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9047889)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35493533",
      "title": "Personalized Digital Health Communications to Increase COVID-19 Vaccination in Underserved Populations: A Double Diamond Approach to Behavioral Design.",
      "abstract": "The COVID-19 pandemic exacerbated pre-existing health disparities. People of historically underserved communities, including racial and ethnic minority groups and people with lower incomes and educational attainments, experienced disproportionate premature mortality, access to healthcare, and vaccination acceptance and adoption. At the same time, the pandemic increased reliance on digital devices, offering a unique opportunity to leverage digital communication channels to address health inequities, particularly related to COVID-19 vaccination. We offer a real-world, systematic approach to designing personalized behavior change email and text messaging interventions that address individual barriers with evidence-based behavioral science inclusive of underserved populations. Integrating design processes such as the Double Diamond model with evidence-based behavioral science intervention development offers a unique opportunity to create equitable interventions. Further, leveraging behavior change artificial intelligence (AI) capabilities allows for both personalizing and automating that personalization to address barriers to COVID-19 vaccination at scale. The result is an intervention whose broad component library meets the needs of a diverse population and whose technology can deliver the right components for each individual.",
      "journal": "Frontiers in digital health",
      "year": "2022",
      "doi": "10.3389/fdgth.2022.831093",
      "authors": "Ford Kelsey Lynett et al.",
      "keywords": "behavioral design; behavioral science; digital health (eHealth); health communication (MESH); health equity (MeSH); personalization",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35493533/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9051039",
      "ft_text_length": 23152,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9051039)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35496981",
      "title": "Ethics-by-design: efficient, fair and inclusive resource allocation using machine learning.",
      "abstract": "The distribution of crucial medical goods and services in conditions of scarcity is among the most important, albeit contested, areas of public policy development. Policymakers must strike a balance between multiple efficiency and fairness objectives, while reconciling disparate value judgments from a diverse set of stakeholders. We present a general framework for combining ethical theory, data modeling, and stakeholder input in this process and illustrate through a case study on designing organ transplant allocation policies. We develop a novel analytical tool, based on machine learning and optimization, designed to facilitate efficient and wide-ranging exploration of policy outcomes across multiple objectives. Such a tool enables all stakeholders, regardless of their technical expertise, to more effectively engage in the policymaking process by developing evidence-based value judgments based on relevant tradeoffs.",
      "journal": "Journal of law and the biosciences",
      "year": "2022",
      "doi": "10.1093/jlb/lsac012",
      "authors": "Papalexopoulos Theodore P et al.",
      "keywords": "Analytics; ethics by design; machine learning; organ allocation; organ transplantation; resource allocation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35496981/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9050238",
      "ft_text_length": 32068,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9050238)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35506520",
      "title": "Bias field correction in hyperpolarized 129 Xe ventilation MRI using templates derived by RF-depolarization mapping.",
      "abstract": "PURPOSE: To correct for RF inhomogeneity for in vivo 129 Xe ventilation MRI using flip-angle mapping enabled by randomized 3D radial acquisitions. To extend this RF-depolarization mapping approach to create a flip-angle map template applicable to arbitrary acquisition strategies, and to compare these approaches to conventional bias field correction. METHODS: RF-depolarization mapping was evaluated first in digital simulations and then in 51 subjects who had undergone radial 129 Xe ventilation MRI in the supine position at 3T (views\u00a0=\u00a03600; samples/view\u00a0=\u00a0128; TR/TE\u00a0=\u00a04.5/0.45\u2009ms; flip angle\u00a0=\u00a01.5; FOV\u00a0=\u00a040\u2009cm). The images were corrected using newly developed RF-depolarization and templated-based methods and the resulting quantitative ventilation metrics (mean, coefficient of variation, and gradient) were compared to those resulting from N4ITK correction. RESULTS: RF-depolarization and template-based mapping methods yielded a pattern of RF-inhomogeneity consistent with the expected variation based on coil architecture. The resulting corrected images were visually similar, but meaningfully distinct from those generated using standard N4ITK correction. The N4ITK algorithm eliminated the physiologically expected anterior-posterior gradient (-0.04\u2009\u00b1\u20091.56%/cm, P\u2009<\u20090.001). These 2 newly introduced methods of RF-depolarization and template correction retained the physiologically expected anterior-posterior ventilation gradient in healthy subjects (2.77\u2009\u00b1\u20092.09%/cm and 2.01\u2009\u00b1\u20092.73%/cm, respectively). CONCLUSIONS: Randomized 3D 129 Xe MRI ventilation acquisitions can inherently be corrected for bias field, and this technique can be extended to create flip angle templates capable of correcting images from a given coil regardless of acquisition strategy. These methods may be more favorable than the de facto standard N4ITK because they can remove undesirable heterogeneity caused by RF effects while retaining results from known physiology.",
      "journal": "Magnetic resonance in medicine",
      "year": "2022",
      "doi": "10.1002/mrm.29254",
      "authors": "Lu Junlan et al.",
      "keywords": "bias field correction; hyperpolarized 129Xe MRI; ventilation defect percentage",
      "mesh_terms": "Algorithms; Humans; Lung; Magnetic Resonance Imaging; Respiration; Xenon Isotopes",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35506520/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9248357",
      "ft_text_length": 1963,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9248357)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35592084",
      "title": "Operationalizing \"One Health\" as \"One Digital Health\" Through a Global Framework That Emphasizes Fair and Equitable Sharing of Benefits From the Use of Artificial Intelligence and Related Digital Technologies.",
      "abstract": "The operationalization of One Health (OH) through digitalization is a means to deploy digital technologies (including Artificial Intelligence (AI), big data and related digital technologies) to better capacitate us to deal with growing climate exigency and related threats to human, animal and plant health. With reference to the concept of One Digital Health (ODH), this paper considers how digital capabilities can help to overcome 'operational brakes' in OH through new and deeper insights, better predictions, and more targeted or precise preventive strategies and public health countermeasures. However, the data landscape is fragmented and access to certain types of data is increasingly restrictive as individuals, communities and countries seek to assert greater control over data taken from them. This paper proposes for a dedicated global ODH framework-centered on fairness and equity-to be established to promote data-sharing across all the key knowledge domains of OH and to devise data-driven solutions to challenges in the human-animal-ecosystems interface. It first considers the data landscape in relation to: (1) Human and population health; (2) Pathogens; (3) Animal and plant health; and (4) Ecosystems and biodiversity. The complexification from the application of advance genetic sequencing technology is then considered, with focus on current debates over whether certain types of data like digital (genetic) sequencing information (DSI) should remain openly and freely accessible. The proposed ODH framework must augment the existing access and benefit sharing (ABS) framework currently prescribed under the Nagoya Protocol to the Convention on Biological Diversity (CBD) in at least three different ways. First, the ODH framework should apply to all genetic resources and data, including DSI, whether from humans or non-humans. Second, the FAIRER principles should be implemented, with focus on fair and equitable benefit-sharing. Third, the ODH framework should adopt multilateral approaches to data sharing (such as through federated data systems) and to ABS. By operationalizing OH as ODH, we are more likely to be able to protect and restore natural habitats, secure the health and well-being of all living things, and thereby realize the goals set out in the post-2020 Global Biodiversity Framework under the CBD.",
      "journal": "Frontiers in public health",
      "year": "2022",
      "doi": "10.3389/fpubh.2022.768977",
      "authors": "Ho Calvin Wai-Loon",
      "keywords": "Convention on Biological Diversity; FAIR; International Health Regulations; One Health; artificial intelligence; benefit sharing; data sharing; digital",
      "mesh_terms": "Artificial Intelligence; Biodiversity; Digital Technology; Ecosystem; One Health",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35592084/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9110679",
      "ft_text_length": 85625,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9110679)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35801177",
      "title": "Transcranial Direct Current Stimulation Over the Left Dorsolateral Prefrontal Cortex Reduced Attention Bias Toward Negative Facial Expression: A Pilot Study in Healthy Subjects.",
      "abstract": "Research in the cognitive neuroscience field has shown that individuals with a stronger attention bias for negative information had higher depression risk, which may be the underlying pathogenesis of depression. This dysfunction of affect-biased attention also represents a decline in emotion regulation ability. Clinical studies have suggested that transcranial direct current stimulation (tDCS) treatment can improve the symptoms of depression, yet the neural mechanism behind this improvement is still veiled. This study aims to investigate the effects of tDCS on affect-biased attention. A sample of healthy participants received 20 min active (n = 22) or sham tDCS (n = 19) over the left dorsolateral prefrontal cortex (DLPFC) for 7 consecutive days. Electroencephalographic (EEG) signals were recorded while performing the rest task and emotional oddball task. The oddball task required response to pictures of the target (positive or negative) emotional facial stimuli and neglecting distracter (negative or positive) or standard (neutral) stimuli. Welch power spectrum estimation algorithm was applied to calculate frontal alpha asymmetry (FAA) in the rest task, and the overlapping averaging method was used to extract event-related potentials (ERP) components in the oddball task. Compared to sham tDCS, active tDCS caused an obvious increment in FAA in connection with emotion regulation (p < 0.05). Also, participants in the active tDCS group show greater P3 amplitudes following positive targets (p < 0.05) and greater N2 amplitudes following negative distracters (p < 0.05), reflecting emotion-related attention biases. These results offer valuable insights into the relationship between affect-biased attention and the effects of tDCS, which may be of assistance in exploring the neuropathological mechanism of depression and anxiety and new treatment strategies for tDCS.",
      "journal": "Frontiers in neuroscience",
      "year": "2022",
      "doi": "10.3389/fnins.2022.894798",
      "authors": "Liu Shuang et al.",
      "keywords": "DLPFC; ERP; attention bias; emotion regulation; tDCS",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35801177/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9256464",
      "ft_text_length": 44109,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9256464)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35878534",
      "title": "Age, sex and race bias in automated arrhythmia detectors.",
      "abstract": "Despite the recent explosion of machine learning applied to medical data, very few studies have examined algorithmic bias in any meaningful manner, comparing across algorithms, databases, and assessment metrics. In this study, we compared the biases in sex, age, and race of 56 algorithms on over 130,000 electrocardiograms (ECGs) using several metrics and propose a machine learning model design to reduce bias. Participants of the 2021 PhysioNet Challenge designed and implemented working, open-source algorithms to identify clinical diagnosis from 2- lead ECG recordings. We grouped the data from the training, validation, and test datasets by sex (male vs female), age (binned by decade), and race (Asian, Black, White, and Other) whenever possible. We computed recording-wise accuracy, area under the receiver operating characteristic curve (AUROC), area under the precision recall curve (AUPRC), F-measure, and the Challenge Score for each of the 56 algorithms. The Mann-Whitney U and the Kruskal-Wallis tests assessed the performance differences of algorithms across these demographic groups. Group trends revealed similar values for the AUROC, AUPRC, and F-measure for both male and female groups across the training, validation, and test sets. However, recording-wise accuracies were 20% higher (p\u00a0<\u00a00.01) and the Challenge Score 12% lower (p\u00a0=\u00a00.02) for female subjects on the test set. AUPRC, F-measure, and the Challenge Score increased with age, while recording-wise accuracy and AUROC decreased with age. The results were similar for the training and test sets, but only recording-wise accuracy (12% decrease per decade, p\u00a0<\u00a00.01), Challenge Score (1% increase per decade, p\u00a0<\u00a00.01), and AUROC (1% decrease per decade, p\u00a0<\u00a00.01) were statistically different on the test set. We observed similar AUROC, AUPRC, Challenge Score, and F-measure values across the different race categories. But, recording-wise accuracies were significantly lower for Black subjects and higher for Asian subjects on the training (31% difference, p\u00a0<\u00a00.01) and test (39% difference, p\u00a0<\u00a00.01) sets. A top performing model was then retrained using an additional constraint which simultaneously minimized differences in performance across sex, race and age. This resulted in a modest reduction in performance, with a significant reduction in bias. This work provides a demonstration that biases manifest as a function of model architecture, population, cost function and optimization metric, all of which should be closely examined in any model.",
      "journal": "Journal of electrocardiology",
      "year": "2022",
      "doi": "10.1016/j.jelectrocard.2022.07.007",
      "authors": "Perez Alday Erick A et al.",
      "keywords": "Age; Bias; Healthcare; Machine learning; Race; Sex",
      "mesh_terms": "Female; Humans; Male; Electrocardiography; Arrhythmias, Cardiac; Sex Factors; Age Factors",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35878534/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11486543",
      "ft_text_length": 2547,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11486543)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "35920547",
      "title": "Using Cg05575921 methylation to predict lung cancer risk: a potentially bias-free precision epigenetics approach.",
      "abstract": "The decision to engage in lung cancer screening (LCS) necessitates weighing benefits versus harms. Previously, clinicians in the United States have used the PLCOM2012 algorithm to guide LCS decision-making. However, that formula contains race and gender-based variables. Previously, using data from a European study, Bojesen and colleagues have suggested that cg05575921 methylation could guide decision-making. To test this hypothesis in a more diverse American population, we examined DNA and clinical data from 3081 subjects from the National Lung Screening Trial (NLST) study. Using survival analysis, we found a simple linear predictor consisting of age, pack-year consumption and cg05575921, to have the best predictive power among several alternatives (AUC\u00a0=\u00a00.66). Results showed that the highest quartile of risk was more than 2-fold more likely to develop lung cancer than those in the lowest quartile. Race, ethnicity, and gender had no effect on prediction with both cg05575921 and pack years contributing equally (both p <\u00a00.003) to risk prediction. Current smokers had considerably lower methylation than former smokers (46% vs 67%; p <\u00a00.001) with the average methylation of those who quit approaching 80% after 25\u00a0years of cessation. Finally, current male smokers had lower mean cg05575921 percentage than female smokers (46% vs 49%; p <\u00a00.001). We conclude that cg05575921 (along with age and pack years) can be used to guide LCS decision-making, and additional studies might focus on how best to use methylation to inform decision-making.",
      "journal": "Epigenetics",
      "year": "2022",
      "doi": "10.1080/15592294.2022.2108082",
      "authors": "Philibert Rob et al.",
      "keywords": "AHRR; DNA methylation; cg05575921; lung cancer; smoking",
      "mesh_terms": "Humans; Male; Female; United States; Lung Neoplasms; Early Detection of Cancer; DNA Methylation; Smoking; Epigenesis, Genetic; Lung",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35920547/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9665144",
      "ft_text_length": 1567,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9665144)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36010243",
      "title": "A Fair Performance Comparison between Complex-Valued and Real-Valued Neural Networks for Disease Detection.",
      "abstract": "Our aim is to contribute to the classification of anomalous patterns in biosignals using this novel approach. We specifically focus on melanoma and heart murmurs. We use a comparative study of two convolution networks in the Complex and Real numerical domains. The idea is to obtain a powerful approach for building portable systems for early disease detection. Two similar algorithmic structures were chosen so that there is no bias determined by the number of parameters to train. Three clinical data sets, ISIC2017, PH2, and Pascal, were used to carry out the experiments. Mean comparison hypothesis tests were performed to ensure statistical objectivity in the conclusions. In all cases, complex-valued networks presented a superior performance for the Precision, Recall, F1 Score, Accuracy, and Specificity metrics in the detection of associated anomalies. The best complex number-based classifier obtained in the Receiving Operating Characteristic (ROC) space presents a Euclidean distance of 0.26127 with respect to the ideal classifier, as opposed to the best real number-based classifier, whose Euclidean distance to the ideal is 0.36022 for the same task of melanoma detection. The 27.46% superiority in this metric, as in the others reported in this work, suggests that complex-valued networks have a greater ability to extract features for more efficient discrimination in the dataset.",
      "journal": "Diagnostics (Basel, Switzerland)",
      "year": "2022",
      "doi": "10.3390/diagnostics12081893",
      "authors": "Jojoa Mario et al.",
      "keywords": "complex numbers; complex-valued convolution neural networks; complex-valued deep learning; fair performance comparison; real-valued neural networks",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36010243/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9406326",
      "ft_text_length": 30352,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9406326)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36046104",
      "title": "Fairness-related performance and explainability effects in deep learning models for brain image analysis.",
      "abstract": "Purpose: Explainability and fairness are two key factors for the effective and ethical clinical implementation of deep learning-based machine learning models in healthcare settings. However, there has been limited work on investigating how unfair performance manifests in explainable artificial intelligence (XAI) methods, and how XAI can be used to investigate potential reasons for unfairness. Thus, the aim of this work was to analyze the effects of previously established sociodemographic-related confounders on classifier performance and explainability methods. Approach: A convolutional neural network (CNN) was trained to predict biological sex from T1-weighted brain MRI datasets of 4547 9- to 10-year-old adolescents from the Adolescent Brain Cognitive Development study. Performance disparities of the trained CNN between White and Black subjects were analyzed and saliency maps were generated for each subgroup at the intersection of sex and race. Results: The classification model demonstrated a significant difference in the percentage of correctly classified White male ( 90.3 % \u00b1 1.7 %  ) and Black male ( 81.1 % \u00b1 4.5 %  ) children. Conversely, slightly higher performance was found for Black female ( 89.3 % \u00b1 4.8 %  ) compared with White female ( 86.5 % \u00b1 2.0 %  ) children. Saliency maps showed subgroup-specific differences, corresponding to brain regions previously associated with pubertal development. In line with this finding, average pubertal development scores of subjects used in this study were significantly different between Black and White females ( p < 0.001  ) and males ( p < 0.001  ). Conclusions: We demonstrate that a CNN with significantly different sex classification performance between Black and White adolescents can identify different important brain regions when comparing subgroup saliency maps. Importance scores vary substantially between subgroups within brain structures associated with pubertal development, a race-associated confounder for predicting sex. We illustrate that unfair models can produce different XAI results between subgroups and that these results may explain potential reasons for biased performance.",
      "journal": "Journal of medical imaging (Bellingham, Wash.)",
      "year": "2022",
      "doi": "10.1117/1.JMI.9.6.061102",
      "authors": "Stanley Emma A M et al.",
      "keywords": "adolescent brain cognitive development study; bias; explainable artificial intelligence; fairness; machine learning; magnetic resonance imaging",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36046104/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9412191",
      "ft_text_length": 2169,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9412191)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36048021",
      "title": "Picture a data scientist: a call to action for increasing diversity, equity, and inclusion in the age of AI.",
      "abstract": "The lack of diversity, equity, and inclusion continues to hamper the artificial intelligence (AI) field and is especially problematic for healthcare applications. In this article, we expand on the need for diversity, equity, and inclusion, specifically focusing on the composition of AI teams. We call to action leaders at all levels to make team inclusivity and diversity the centerpieces of AI development, not the afterthought. These recommendations take into consideration mitigation at several levels, including outreach programs at the local level, diversity statements at the academic level, and regulatory steps at the federal level.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2022",
      "doi": "10.1093/jamia/ocac156",
      "authors": "de Hond Anne A H et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Physicians; Delivery of Health Care",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36048021/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9667164",
      "ft_text_length": 650,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9667164)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36054492",
      "title": "A Machine Learning-Based Approach to Discrimination of Tauopathies Using [18 F]PM-PBB3 PET Images.",
      "abstract": "BACKGROUND: We recently developed a positron emission tomography (PET) probe, [18 F]PM-PBB3, to detect tau lesions in diverse tauopathies, including mixed three-repeat and four-repeat (3R\u2009+\u20094R) tau fibrils in Alzheimer's disease (AD) and 4R tau aggregates in progressive supranuclear palsy (PSP). For wider availability of this technology for clinical settings, bias-free quantitative evaluation of tau images without a priori disease information is needed. OBJECTIVE: We aimed to establish tau PET pathology indices to characterize PSP and AD using a machine learning approach and test their validity and tracer capabilities. METHODS: Data were obtained from 50 healthy control subjects, 46 patients with PSP Richardson syndrome, and 37 patients on the AD continuum. Tau PET data from 114 regions of interest were subjected to Elastic Net cross-validation linear classification analysis with a one-versus-the-rest multiclass strategy to obtain a linear function that discriminates diseases by maximizing the area under the receiver operating characteristic curve. We defined PSP- and AD-tau scores for each participant as values of the functions optimized for differentiating PSP (4R) and AD (3R\u2009+\u20094R), respectively, from others. RESULTS: The discriminatory ability of PSP- and AD-tau scores assessed as the area under the receiver operating characteristic curve was 0.98 and 1.00, respectively. PSP-tau scores correlated with the PSP rating scale in patients with PSP, and AD-tau scores correlated with Mini-Mental State Examination scores in healthy control-AD continuum patients. The globus pallidus and amygdala were highlighted as regions with high weight coefficients for determining PSP- and AD-tau scores, respectively. CONCLUSIONS: These findings highlight our technology's unbiased capability to identify topologies of 3R\u2009+\u20094R versus 4R tau deposits. \u00a9 2022 The Authors. Movement Disorders published by Wiley Periodicals LLC on behalf of International Parkinson and Movement Disorder Society.",
      "journal": "Movement disorders : official journal of the Movement Disorder Society",
      "year": "2022",
      "doi": "10.1002/mds.29173",
      "authors": "Endo Hironobu et al.",
      "keywords": "Alzheimer's disease; machine learning; progressive supranuclear palsy; tau PET; tauopathy",
      "mesh_terms": "Humans; tau Proteins; Brain; Tauopathies; Supranuclear Palsy, Progressive; Positron-Emission Tomography; Alzheimer Disease; Movement Disorders; Machine Learning",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36054492/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9805085",
      "ft_text_length": 34486,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9805085)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36101652",
      "title": "Validity of a Computational Linguistics-Derived Automated Health Literacy Measure Across Race/Ethnicity: Findings from The ECLIPPSE Project.",
      "abstract": "Limited health literacy (HL) partially mediates health disparities. Measurement constraints, including lack of validity assessment across racial/ethnic groups and administration challenges, have undermined the field and impeded scaling of HL interventions. We employed computational linguistics to develop an automated and novel HL measure, analyzing >300,000 messages sent by >9,000 diabetes patients via a patient portal to create a Literacy Profiles. We carried out stratified analyses among White/non-Hispanics, Black/non-Hispanics, Hispanics, and Asian/Pacific Islanders to determine if the Literacy Profile has comparable criterion and predictive validities. We discovered that criterion validity was consistently high across all groups (c-statistics 0.82-0.89). We observed consistent relationships across racial/ethnic groups between HL and outcomes, including communication, adherence, hypoglycemia, diabetes control, and ED utilization. While concerns have arisen regarding bias in AI, the automated Literacy Profile appears sufficiently valid across race/ethnicity, enabling HL measurement at a scale that could improve clinical care and population health among diverse populations.",
      "journal": "Journal of health care for the poor and underserved",
      "year": "2021",
      "doi": "10.1353/hpu.2021.0067",
      "authors": "Schillinger Dean et al.",
      "keywords": "Health literacy; artificial intelligence; communication; computational linguistics; diabetes; health disparities; machine learning; validation study",
      "mesh_terms": "Diabetes Mellitus; Ethnicity; Health Literacy; Humans; Linguistics; Racial Groups",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36101652/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9467454",
      "ft_text_length": 1193,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9467454)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "36204532",
      "title": "Mitigating Bias in Radiology Machine Learning: 2. Model Development.",
      "abstract": "There are increasing concerns about the bias and fairness of artificial intelligence (AI) models as they are put into clinical practice. Among the steps for implementing machine learning tools into clinical workflow, model development is an important stage where different types of biases can occur. This report focuses on four aspects of model development where such bias may arise: data augmentation, model and loss function, optimizers, and transfer learning. This report emphasizes appropriate considerations and practices that can mitigate biases in radiology AI studies. Keywords: Model, Bias, Machine Learning, Deep Learning, Radiology \u00a9 RSNA, 2022.",
      "journal": "Radiology. Artificial intelligence",
      "year": "2022",
      "doi": "10.1148/ryai.220010",
      "authors": "Zhang Kuan et al.",
      "keywords": "Bias; Deep Learning; Machine Learning; Model; Radiology",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36204532/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9530765",
      "ft_text_length": 656,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9530765)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36204539",
      "title": "Mitigating Bias in Radiology Machine Learning: 3. Performance Metrics.",
      "abstract": "The increasing use of machine learning (ML) algorithms in clinical settings raises concerns about bias in ML models. Bias can arise at any step of ML creation, including data handling, model development, and performance evaluation. Potential biases in the ML model can be minimized by implementing these steps correctly. This report focuses on performance evaluation and discusses model fitness, as well as a set of performance evaluation toolboxes: namely, performance metrics, performance interpretation maps, and uncertainty quantification. By discussing the strengths and limitations of each toolbox, our report highlights strategies and considerations to mitigate and detect biases during performance evaluations of radiology artificial intelligence models. Keywords: Segmentation, Diagnosis, Convolutional Neural Network (CNN) \u00a9 RSNA, 2022.",
      "journal": "Radiology. Artificial intelligence",
      "year": "2022",
      "doi": "10.1148/ryai.220061",
      "authors": "Faghani Shahriar et al.",
      "keywords": "Convolutional Neural Network (CNN); Diagnosis; Segmentation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36204539/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9530766",
      "ft_text_length": 846,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9530766)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36204544",
      "title": "Mitigating Bias in Radiology Machine Learning: 1. Data Handling.",
      "abstract": "Minimizing bias is critical to adoption and implementation of machine learning (ML) in clinical practice. Systematic mathematical biases produce consistent and reproducible differences between the observed and expected performance of ML systems, resulting in suboptimal performance. Such biases can be traced back to various phases of ML development: data handling, model development, and performance evaluation. This report presents 12 suboptimal practices during data handling of an ML study, explains how those practices can lead to biases, and describes what may be done to mitigate them. Authors employ an arbitrary and simplified framework that splits ML data handling into four steps: data collection, data investigation, data splitting, and feature engineering. Examples from the available research literature are provided. A Google Colaboratory Jupyter notebook includes code examples to demonstrate the suboptimal practices and steps to prevent them. Keywords: Data Handling, Bias, Machine Learning, Deep Learning, Convolutional Neural Network (CNN), Computer-aided Diagnosis (CAD) \u00a9 RSNA, 2022.",
      "journal": "Radiology. Artificial intelligence",
      "year": "2022",
      "doi": "10.1148/ryai.210290",
      "authors": "Rouzrokh Pouria et al.",
      "keywords": "Bias; Computer-aided Diagnosis (CAD); Convolutional Neural Network (CNN); Data Handling; Deep Learning; Machine Learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36204544/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9533091",
      "ft_text_length": 1105,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9533091)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36247476",
      "title": "Leveraging innovation, education, and technology for prevention and health equity: Proceedings from the cardiology oncology innovation ThinkTank 2021.",
      "abstract": "",
      "journal": "Frontiers in cardiovascular medicine",
      "year": "2022",
      "doi": "10.3389/fcvm.2022.982021",
      "authors": "Brown Sherry-Ann et al.",
      "keywords": "artificial intelligence; cardiology; digital health; digital transformation; health equity; innovation; oncology; prevention",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36247476/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9557098",
      "ft_text_length": 38810,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9557098)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36301408",
      "title": "Equity in AgeTech for Ageing Well in Technology-Driven Places: The Role of Social Determinants in Designing AI-based Assistive Technologies.",
      "abstract": "AgeTech involves the use of emerging technologies to support the health, well-being and independent living of older adults. In this paper we focus on how AgeTech based on artificial intelligence (AI) may better support older adults to remain in their own living environment for longer, provide social connectedness, support wellbeing and mental health, and enable social participation. In order to assess and better understand the positive as well as negative outcomes of AI-based AgeTech, a critical analysis of ethical design, digital equity, and policy pathways is required. A crucial question is how AI-based AgeTech may drive practical, equitable, and inclusive multilevel solutions to support healthy, active ageing.In our paper, we aim to show that a focus on equity is key for AI-based AgeTech if it is to realize its full potential. We propose that equity should not just be an extra benefit or minimum requirement, but the explicit aim of designing AI-based health tech. This means that social determinants that affect the use of or access to these technologies have to be addressed. We will explore how complexity management as a crucial element of AI-based AgeTech may potentially create and exacerbate social inequities by marginalising or ignoring social determinants. We identify bias, standardization, and access as main ethical issues in this context and subsequently, make recommendations as to how inequities that stem form AI-based AgeTech can be addressed.",
      "journal": "Science and engineering ethics",
      "year": "2022",
      "doi": "10.1007/s11948-022-00397-y",
      "authors": "Rubeis Giovanni et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Social Determinants of Health; Healthy Aging; Self-Help Devices; Technology",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36301408/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9613787",
      "ft_text_length": 37903,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9613787)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36313215",
      "title": "Turing test-inspired method for analysis of biases prevalent in artificial intelligence-based medical imaging.",
      "abstract": "Due to the growing need to provide better global healthcare, computer-based and robotic healthcare equipment that depend on artificial intelligence has seen an increase in development. In order to evaluate artificial intelligence (AI) in computer technology, the Turing test was created. For evaluating the future generation of medical diagnostics and medical robots, it remains an essential qualitative instrument. We propose a novel methodology to assess AI-based healthcare technology that provided verifiable diagnostic accuracy and statistical robustness. In order to run our test, we used a state-of-the-art AI model and compared it to radiologists for checking how generalized the model is and if any biases are prevalent. We achieved results that can evaluate the performance of our chosen model for this study in a clinical setting and we also applied a quantifiable method for evaluating our modified Turing test results using a meta-analytical evaluation framework. His test provides a translational standard for upcoming AI modalities. Our modified Turing test is a notably strong standard to measure the actual performance of the AI model on a variety of edge cases and normal cases and also helps in detecting if the algorithm is biased towards any one type of case. This method extends the flexibility to detect any prevalent biases and also classify the type of bias.",
      "journal": "AI and ethics",
      "year": "2022",
      "doi": "10.1007/s43681-022-00227-8",
      "authors": "Tripathi Satvik et al.",
      "keywords": "Artificial intelligence; Diagnostic tests; Fairness; Healthcare; Turing test",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36313215/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9590390",
      "ft_text_length": 23362,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9590390)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36378761",
      "title": "Improving Fairness in the Prediction of Heart Failure Length of Stay and Mortality by Integrating Social Determinants of Health.",
      "abstract": "BACKGROUND: Machine learning (ML) approaches have been broadly applied to the prediction of length of stay and mortality in hospitalized patients. ML may also reduce societal health burdens, assist in health resources planning and improve health outcomes. However, the fairness of these ML models across ethnoracial or socioeconomic subgroups is rarely assessed or discussed. In this study, we aim (1) to quantify the algorithmic bias of ML models when predicting the probability of long-term hospitalization or in-hospital mortality for different heart failure (HF) subpopulations, and (2) to propose a novel method that can improve the fairness of our models without compromising predictive power. METHODS: We built 5 ML classifiers to predict the composite outcome of hospitalization length-of-stay and in-hospital mortality for 210\u2009368 HF patients extracted from the Get With The Guidelines-Heart Failure registry data set. We integrated 15 social determinants of health variables, including the Social Deprivation Index and the Area Deprivation Index, into the feature space of ML models based on patients' geographies to mitigate the algorithmic bias. RESULTS: The best-performing random forest model demonstrated modest predictive power but selectively underdiagnosed underserved subpopulations, for example, female, Black, and socioeconomically disadvantaged patients. The integration of social determinants of health variables can significantly improve fairness without compromising model performance. CONCLUSIONS: We quantified algorithmic bias against underserved subpopulations in the prediction of the composite outcome for HF patients. We provide a potential direction to reduce disparities of ML-based predictive models by integrating social determinants of health variables. We urge fellow researchers to strongly consider ML fairness when developing predictive models for HF patients.",
      "journal": "Circulation. Heart failure",
      "year": "2022",
      "doi": "10.1161/CIRCHEARTFAILURE.122.009473",
      "authors": "Li Yikuan et al.",
      "keywords": "bias; healthcare disparities; heart failure; machine learning; social determinants of health",
      "mesh_terms": "Humans; Female; Heart Failure; Length of Stay; Social Determinants of Health; Hospitalization; Hospital Mortality",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36378761/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9673161",
      "ft_text_length": 1878,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9673161)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36387013",
      "title": "Application of convex hull analysis for the evaluation of data heterogeneity between patient populations of different origin and implications of hospital bias in downstream machine-learning-based data processing: A comparison of 4 critical-care patient datasets.",
      "abstract": "Machine learning (ML) models are developed on a learning dataset covering only a small part of the data of interest. If model predictions are accurate for the learning dataset but fail for unseen data then generalization error is considered high. This problem manifests itself within all major sub-fields of ML but is especially relevant in medical applications. Clinical data structures, patient cohorts, and clinical protocols may be highly biased among hospitals such that sampling of representative learning datasets to learn ML models remains a challenge. As ML models exhibit poor predictive performance over data ranges sparsely or not covered by the learning dataset, in this study, we propose a novel method to assess their generalization capability among different hospitals based on the convex hull (CH) overlap between multivariate datasets. To reduce dimensionality effects, we used a two-step approach. First, CH analysis was applied to find mean CH coverage between each of the two datasets, resulting in an upper bound of the prediction range. Second, 4 types of ML models were trained to classify the origin of a dataset (i.e., from which hospital) and to estimate differences in datasets with respect to underlying distributions. To demonstrate the applicability of our method, we used 4 critical-care patient datasets from different hospitals in Germany and USA. We estimated the similarity of these populations and investigated whether ML models developed on one dataset can be reliably applied to another one. We show that the strongest drop in performance was associated with the poor intersection of convex hulls in the corresponding hospitals' datasets and with a high performance of ML methods for dataset discrimination. Hence, we suggest the application of our pipeline as a first tool to assess the transferability of trained models. We emphasize that datasets from different hospitals represent heterogeneous data sources, and the transfer from one database to another should be performed with utmost care to avoid implications during real-world applications of the developed models. Further research is needed to develop methods for the adaptation of ML models to new hospitals. In addition, more work should be aimed at the creation of gold-standard datasets that are large and diverse with data from varied application sites.",
      "journal": "Frontiers in big data",
      "year": "2022",
      "doi": "10.3389/fdata.2022.603429",
      "authors": "Sharafutdinov Konstantin et al.",
      "keywords": "ARDS; convex hull (CH); data pooling; dataset-bias; generalization error",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36387013/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9659720",
      "ft_text_length": 41194,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9659720)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36463866",
      "title": "Special Section on Inclusive Digital Health: Notable Papers on Addressing Bias, Equity, and Literacy to Strengthen Health Systems.",
      "abstract": "OBJECTIVE: To summarize significant research contributions on addressing bias, equity, and literacy in health delivery systems published in 2021. METHODS: An extensive search using PubMed and Scopus was conducted to identify peer-reviewed articles published in 2021 that examined ways that informatics methods, approaches, and tools could address bias, equity, and literacy in health systems and care delivery processes. The selection process comprised three steps: (1) 15 candidate best papers were first selected by the two section editors; (2) external reviewers from internationally renowned research teams reviewed each candidate best paper; and (3) the final selection of three best papers was conducted by the editorial committee of the Yearbook. RESULTS: Selected best papers represent studies that characterized significant challenges facing biomedical informatics with respect to equity and practices that support equity and literacy in the design of health information systems. Selected papers represent the full spectrum of this year's yearbook theme. In general, papers identified in the search fell into one of the following categories: (1) descriptive accounts of algorithmic bias in medical software or machine learning approaches; (2) enabling health information systems to appropriately encode for gender identity and sex; (3) approaches to support health literacy among individuals who interact with information systems and mobile applications; and (4) approaches to engage diverse populations in the use of health information systems and the biomedical informatics workforce CONCLUSIONS: : Although the selected papers are notable, our collective efforts as a biomedical informatics community to address equity, literacy, and bias remain nascent. More work is needed to ensure health information systems are just in their use of advanced computing approaches and all persons have equal access to health care and informatics tools.",
      "journal": "Yearbook of medical informatics",
      "year": "2022",
      "doi": "10.1055/s-0042-1742536",
      "authors": "Dixon Brian E et al.",
      "keywords": "",
      "mesh_terms": "Female; Humans; Male; Gender Identity; Bias; Health Literacy; Health Information Systems; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36463866/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9719755",
      "ft_text_length": 13816,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9719755)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36508020",
      "title": "Racial disparities in dermatology.",
      "abstract": "Significant racial/ethnic disparities in dermatologic care and their subsequent impact on dermatologic conditions were recently reported. Contributing factors include socioeconomic factors, gaps in educational exposure, and underrepresentation of minority groups in the dermatologic workforce. In 2021, the American Academy of Dermatology (AAD) announced its three-year plan to expand diversity, equity, and inclusion in dermatology. One way to reduce disparities in dermatology is for every dermatologist, regardless of race or ethnicity, to receive adequate education in diseases, treatments, health equity, and tailored approaches to delivering dermatologic care with cultural humility. In addition, a diverse dermatologic workforce-especially at the level of residency program educators and organizational leaders-will contribute to improved cross-cultural understanding, more inclusive research efforts, and improved treatment approaches for conditions that are more prevalent or nuanced in certain racial/ethnic populations.\u00a0Finally, the\u00a0dermatology and broader healthcare community needs to acknowledge\u00a0and educate\u00a0ourselves on the health impacts of racism.",
      "journal": "Archives of dermatological research",
      "year": "2023",
      "doi": "10.1007/s00403-022-02507-z",
      "authors": "Narla Shanthi et al.",
      "keywords": "Atopic dermatitis; Clinical trials; Dermatology; Disparities; Hidradenitis suppurativa; Machine learning; Mycosis fungoides; Psoriasis; Race; Skin cancer; Structural racism",
      "mesh_terms": "Humans; United States; Dermatology; Delivery of Health Care; Ethnicity; Minority Groups; Socioeconomic Factors",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36508020/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9743121",
      "ft_text_length": 30353,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9743121)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36540977",
      "title": "FairPRS: adjusting for admixed populations in polygenic risk scores using invariant risk minimization.",
      "abstract": "Polygenic risk scores (PRS) are increasingly used to estimate the personal risk of a trait based on genetics. However, most genomic cohorts are of European populations, with a strong under-representation of non-European groups. Given that PRS poorly transport across racial groups, this has the potential to exacerbate health disparities if used in clinical care. Hence there is a need to generate PRS that perform comparably across ethnic groups. Borrowing from recent advancements in the domain adaption field of machine learning, we propose FairPRS - an Invariant Risk Minimization (IRM) approach for estimating fair PRS or debiasing a pre-computed PRS. We test our method on both a diverse set of synthetic data and real data from the UK Biobank. We show our method can create ancestry-invariant PRS distributions that are both racially unbiased and largely improve phenotype prediction. We hope that FairPRS will contribute to a fairer characterization of patients by genetics rather than by race.",
      "journal": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing",
      "year": "2023",
      "doi": "",
      "authors": "Machado Reyes Diego et al.",
      "keywords": "",
      "mesh_terms": "Humans; Genetic Predisposition to Disease; Genome-Wide Association Study; Computational Biology; Risk Factors; Phenotype; Multifactorial Inheritance",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36540977/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10804441",
      "ft_text_length": 19521,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10804441)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36601036",
      "title": "Imputation Strategies Under Clinical Presence: Impact on Algorithmic Fairness.",
      "abstract": "Biases have marked medical history, leading to unequal care affecting marginalised groups. The patterns of missingness in observational data often reflect these group discrepancies, but the algorithmic fairness implications of group-specific missingness are not well understood. Despite its potential impact, imputation is too often an overlooked preprocessing step. When explicitly considered, attention is placed on overall performance, ignoring how this preprocessing can reinforce groupspecific inequities. Our work questions this choice by studying how imputation affects downstream algorithmic fairness. First, we provide a structured view of the relationship between clinical presence mechanisms and groupspecific missingness patterns. Then, through simulations and real-world experiments, we demonstrate that the imputation choice influences marginalised group performance and that no imputation strategy consistently reduces disparities. Importantly, our results show that current practices may endanger health equity as similarly performing imputation strategies at the population level can affect marginalised groups differently. Finally, we propose recommendations for mitigating inequities that may stem from a neglected step of the machine learning pipeline.",
      "journal": "Proceedings of machine learning research",
      "year": "2022",
      "doi": "",
      "authors": "Jeanselme Vincent et al.",
      "keywords": "Clinical Presence; Fairness; Imputation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36601036/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7614014",
      "ft_text_length": 27566,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7614014)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36611079",
      "title": "Fast, accurate, and racially unbiased pan-cancer tumor-only variant calling with tabular machine learning.",
      "abstract": "Accurately identifying somatic mutations is essential for precision oncology and crucial for calculating tumor-mutational burden (TMB), an important predictor of response to immunotherapy. For tumor-only variant calling (i.e., when the cancer biopsy but not the patient's normal tissue sample is sequenced), accurately distinguishing somatic mutations from germline variants is a challenging problem that, when unaddressed, results in unreliable, biased, and inflated TMB estimates. Here, we apply machine learning to the task of somatic vs germline classification in tumor-only solid tumor samples using TabNet, XGBoost, and LightGBM, three machine-learning models for tabular data. We constructed a training set for supervised classification using features derived exclusively from tumor-only variant calling and drawing somatic and germline truth labels from an independent pipeline using the patient-matched normal samples. All three trained models achieved state-of-the-art performance on two holdout test datasets: a TCGA dataset including sarcoma, breast adenocarcinoma, and endometrial carcinoma samples (AUC\u2009>\u200994%), and a metastatic melanoma dataset (AUC\u2009>\u200985%). Concordance between matched-normal and tumor-only TMB improves from R2\u2009=\u20090.006 to 0.71-0.76 with the addition of a machine-learning classifier, with LightGBM performing best. Notably, these machine-learning models generalize across cancer subtypes and capture kits with a call rate of 100%. We reproduce the recent finding that tumor-only TMB estimates for Black patients are extremely inflated relative to that of white patients due to the racial biases of germline databases. We show that our approach with XGBoost and LightGBM eliminates this significant racial bias in tumor-only variant calling.",
      "journal": "NPJ precision oncology",
      "year": "2023",
      "doi": "10.1038/s41698-022-00340-1",
      "authors": "McLaughlin R Tyler et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36611079/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9825621",
      "ft_text_length": 55258,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9825621)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36635084",
      "title": "Primary Care Physicians' and Patients' Perspectives on Equity and Health Security of Infectious Disease Digital Surveillance.",
      "abstract": "PURPOSE: The coronavirus disease 2019 (COVID-19) pandemic facilitated the rapid development of digital detection surveillance (DDS) for outbreaks. This qualitative study examined how DDS for infectious diseases (ID) was perceived and experienced by primary care physicians and patients in order to highlight ethical considerations for promoting patients' autonomy and health care rights. METHODS: In-depth interviews were conducted with a purposefully selected group of 16 primary care physicians and 24 of their patients. The group was reflective of a range of ages, educational attainment, and clinical experiences from urban areas in northern and southern China. Interviews were audio recorded, transcribed, and translated. Two researchers coded data and organized it into themes. A third researcher reviewed 15% of the data and discussed findings with the other researchers to assure accuracy. RESULTS: Five themes were identified: ambiguity around the need for informed consent with usage of DDS; importance of autonomous decision making; potential for discrimination against vulnerable users of DDS for ID; risk of social inequity and disparate care outcomes; and authoritarian institutions' responsibility for maintaining health data security. The adoption of DDS meant some patients would be reluctant to go to the hospital for fear of either being discriminated against or forced into quarantine. Certain groups (older people and children) were thought to be vulnerable to DDS misappropriation. CONCLUSIONS: These findings indicate the paramount importance of establishing national and international ethical frameworks for DDS implementation. Frameworks should guide all aspects of ID surveillance, addressing privacy protection and health security, and underscored by principles of social equity and accountability.Annals \"Online First\" article.",
      "journal": "Annals of family medicine",
      "year": "2023",
      "doi": "10.1370/afm.2895",
      "authors": "Wai Wong William Chi et al.",
      "keywords": "AI; artificial intelligence; disease outbreaks; disease survelillances; ethical issue",
      "mesh_terms": "Child; Humans; Aged; COVID-19; Physicians, Primary Care; Communicable Diseases; Informed Consent; Qualitative Research",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36635084/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9870645",
      "ft_text_length": 1854,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9870645)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "36639799",
      "title": "Ethics and governance of trustworthy medical artificial intelligence.",
      "abstract": "BACKGROUND: The growing application of artificial intelligence (AI) in healthcare has brought technological breakthroughs to traditional diagnosis and treatment, but it is accompanied by many risks and challenges. These adverse effects are also seen as ethical issues and affect trustworthiness in medical AI and need to be managed through identification, prognosis and monitoring. METHODS: We adopted a multidisciplinary approach and summarized five subjects that influence the trustworthiness of medical AI: data quality, algorithmic bias, opacity, safety and security, and responsibility attribution, and discussed these factors from the perspectives of technology, law, and healthcare stakeholders and institutions. The ethical framework of ethical values-ethical principles-ethical norms is used to propose corresponding ethical governance countermeasures for trustworthy medical AI from the ethical, legal, and regulatory aspects. RESULTS: Medical data are primarily unstructured, lacking uniform and standardized annotation, and data quality will directly affect the quality of medical AI algorithm models. Algorithmic bias can affect AI clinical predictions and exacerbate health disparities. The opacity of algorithms affects patients' and doctors' trust in medical AI, and algorithmic errors or security vulnerabilities can pose significant risks and harm to patients. The involvement of medical AI in clinical practices may threaten doctors 'and patients' autonomy and dignity. When accidents occur with medical AI, the responsibility attribution is not clear. All these factors affect people's trust in medical AI. CONCLUSIONS: In order to make medical AI trustworthy, at the ethical level, the ethical value orientation of promoting human health should first and foremost be considered as the top-level design. At the legal level, current medical AI does not have moral status and humans remain the duty bearers. At the regulatory level, strengthening data quality management, improving algorithm transparency and traceability to reduce algorithm bias, and regulating and reviewing the whole process of the AI industry to control risks are proposed. It is also necessary to encourage multiple parties to discuss and assess AI risks and social impacts, and to strengthen international cooperation and communication.",
      "journal": "BMC medical informatics and decision making",
      "year": "2023",
      "doi": "10.1186/s12911-023-02103-9",
      "authors": "Zhang Jie et al.",
      "keywords": "Algorithms; Artificial intelligence; Data; Ethics; Governance; Healthcare; Regulation; Responsibility attribution",
      "mesh_terms": "Humans; Artificial Intelligence; Algorithms; Delivery of Health Care; Prognosis; Data Management",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36639799/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9840286",
      "ft_text_length": 61306,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9840286)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36653067",
      "title": "Evaluation of race/ethnicity-specific survival machine learning models for Hispanic and Black patients with breast cancer.",
      "abstract": "OBJECTIVES: Survival machine learning (ML) has been suggested as a useful approach for forecasting future events, but a growing concern exists that ML models have the potential to cause racial disparities through the data used to train them. This study aims to develop race/ethnicity-specific survival ML models for Hispanic and black women diagnosed with breast cancer to examine whether race/ethnicity-specific ML models outperform the general models trained with all races/ethnicity data. METHODS: We used the data from the US National Cancer Institute's Surveillance, Epidemiology and End Results programme registries. We developed the Hispanic-specific and black-specific models and compared them with the general model using the Cox proportional-hazards model, Gradient Boost Tree, survival tree and survival support vector machine. RESULTS: A total of 322\u2009348 female patients who had breast cancer diagnoses between 1 January 2000 and 31 December 2017 were identified. The race/ethnicity-specific models for Hispanic and black women consistently outperformed the general model when predicting the outcomes of specific race/ethnicity. DISCUSSION: Accurately predicting the survival outcome of a patient is critical in determining treatment options and providing appropriate cancer care. The high-performing models developed in this study can contribute to providing individualised oncology care and improving the survival outcome of black and Hispanic women. CONCLUSION: Predicting the individualised survival outcome of breast cancer can provide the evidence necessary for determining treatment options and high-quality, patient-centred cancer care delivery for under-represented populations. Also, the race/ethnicity-specific ML models can mitigate representation bias and contribute to addressing health disparities.",
      "journal": "BMJ health & care informatics",
      "year": "2023",
      "doi": "10.1136/bmjhci-2022-100666",
      "authors": "Park Jung In et al.",
      "keywords": "artificial intelligence; health equity; informatics; machine learning",
      "mesh_terms": "Humans; Female; Ethnicity; Breast Neoplasms; Hispanic or Latino; Black People; Proportional Hazards Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36653067/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9853120",
      "ft_text_length": 27594,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9853120)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36713099",
      "title": "Spectrum bias in algorithms derived by artificial intelligence: a case study in detecting aortic stenosis using electrocardiograms.",
      "abstract": "AIMS: Spectrum bias can arise when a diagnostic test is derived from study populations with different disease spectra than the target population, resulting in poor generalizability. We used a real-world artificial intelligence (AI)-derived algorithm to detect severe aortic stenosis (AS) to experimentally assess the effect of spectrum bias on test performance. METHODS AND RESULTS: All adult patients at the Mayo Clinic between 1 January 1989 and 30 September 2019 with transthoracic echocardiograms within 180 days after electrocardiogram (ECG) were identified. Two models were developed from two distinct patient cohorts: a whole-spectrum cohort comparing severe AS to any non-severe AS and an extreme-spectrum cohort comparing severe AS to no AS at all. Model performance was assessed. Overall, 258 607 patients had valid ECG and echocardiograms pairs. The area under the receiver operator curve was 0.87 and 0.91 for the whole-spectrum and extreme-spectrum models, respectively. Sensitivity and specificity for the whole-spectrum model was 80% and 81%, respectively, while for the extreme-spectrum model it was 84% and 84%, respectively. When applying the AI-ECG derived from the extreme-spectrum cohort to patients in the whole-spectrum cohort, the sensitivity, specificity, and area under the curve dropped to 83%, 73%, and 0.86, respectively. CONCLUSION: While the algorithm performed robustly in identifying severe AS, this study shows that limiting datasets to clearly positive or negative labels leads to overestimation of test performance when testing an AI algorithm in the setting of classifying severe AS using ECG data. While the effect of the bias may be modest in this example, clinicians should be aware of the existence of such a bias in AI-derived algorithms.",
      "journal": "European heart journal. Digital health",
      "year": "2021",
      "doi": "10.1093/ehjdh/ztab061",
      "authors": "Tseng Andrew S et al.",
      "keywords": "Aortic stenosis; Artificial intelligence; Electrocardiogram; Spectrum bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36713099/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9707965",
      "ft_text_length": 23564,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9707965)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36743404",
      "title": "A machine learning approach to quantify gender bias in collaboration practices of mathematicians.",
      "abstract": "Collaboration practices have been shown to be crucial determinants of scientific careers. We examine the effect of gender on coauthorship-based collaboration in mathematics, a discipline in which women continue to be underrepresented, especially in higher academic positions. We focus on two key aspects of scientific collaboration-the number of different coauthors and the number of single authorships. A higher number of coauthors has a positive effect on, e.g., the number of citations and productivity, while single authorships, for example, serve as evidence of scientific maturity and help to send a clear signal of one's proficiency to the community. Using machine learning-based methods, we show that collaboration networks of female mathematicians are slightly larger than those of their male colleagues when potential confounders such as seniority or total number of publications are controlled, while they author significantly fewer papers on their own. This confirms previous descriptive explorations and provides more precise models for the role of gender in collaboration in mathematics.",
      "journal": "Frontiers in big data",
      "year": "2022",
      "doi": "10.3389/fdata.2022.989469",
      "authors": "Steinfeldt Christian et al.",
      "keywords": "authorship; coauthorship; collaboration networks; gender in mathematics; machine learning; regression-based analysis; scientific publishing; single-authored publications",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36743404/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9889827",
      "ft_text_length": 46481,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9889827)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36755564",
      "title": "AI revolution in healthcare and medicine and the (re-)emergence of inequalities and disadvantages for ageing population.",
      "abstract": "AI systems in medicine and healthcare are being extensively explored in prevention, diagnosis, novel drug designs and after-care. The application of AI technology in healthcare systems promises impressive outcomes such as equalising healthcare, reducing mortality rate and human error, reducing medical costs, as well as reducing reliance on social services. In the light of the WHO \"Decade of Healthy Ageing\", AI applications are designed as digital innovations to support the quality of life for older persons. However, the emergence of evidence of different types of algorithmic bias in AI applications, ageism in the use of digital devices and platforms, as well as age bias in digital data suggests that the use of AI might have discriminatory effects on older population or even cause harm. This paper addresses the issue of age biases and age discrimination in AI applications in medicine and healthcare systems and try to identify main challenges in this area. It will reflect on the potential of AI applications to amplify the already existing health inequalities by discussing two levels where potential negative impact of AI on age inequalities might be observed. Firstly, we will address the technical level of age bias in algorithms and digital datasets (especially health data). Secondly, we will discuss the potential disparate outcomes of automatic decision-making systems (ADMs) used in healthcare on the older population. These examples will demonstrate, although only partially, how AI systems may create new structures of age inequalities and novel dimensions of exclusion in healthcare and medicine.",
      "journal": "Frontiers in sociology",
      "year": "2022",
      "doi": "10.3389/fsoc.2022.1038854",
      "authors": "Stypi\u0144ska Justyna et al.",
      "keywords": "ageing population; ageism; artificial intelligence; automatic decision making; health care",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36755564/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9899925",
      "ft_text_length": 25704,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9899925)",
      "ft_reason": "Excluded: insufficient approach content (1 indicators)"
    },
    {
      "pmid": "36755853",
      "title": "High-resolution synthesis of high-density breast mammograms: Application to improved fairness in deep learning based mass detection.",
      "abstract": "Computer-aided detection systems based on deep learning have shown good performance in breast cancer detection. However, high-density breasts show poorer detection performance since dense tissues can mask or even simulate masses. Therefore, the sensitivity of mammography for breast cancer detection can be reduced by more than 20% in dense breasts. Additionally, extremely dense cases reported an increased risk of cancer compared to low-density breasts. This study aims to improve the mass detection performance in high-density breasts using synthetic high-density full-field digital mammograms (FFDM) as data augmentation during breast mass detection model training. To this end, a total of five cycle-consistent GAN (CycleGAN) models using three FFDM datasets were trained for low-to-high-density image translation in high-resolution mammograms. The training images were split by breast density BI-RADS categories, being BI-RADS A almost entirely fatty and BI-RADS D extremely dense breasts. Our results showed that the proposed data augmentation technique improved the sensitivity and precision of mass detection in models trained with small datasets and improved the domain generalization of the models trained with large databases. In addition, the clinical realism of the synthetic images was evaluated in a reader study involving two expert radiologists and one surgical oncologist.",
      "journal": "Frontiers in oncology",
      "year": "2022",
      "doi": "10.3389/fonc.2022.1044496",
      "authors": "Garrucho Lidia et al.",
      "keywords": "breast cancer; data augmentation (DA); data synthesis; full-field digital mammograms; generative adversarial networks (GANs); mass detection; reader study",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36755853/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9899892",
      "ft_text_length": 52070,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9899892)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36768092",
      "title": "Evaluation of AIML + HDR-A Course to Enhance Data Science Workforce Capacity for Hispanic Biomedical Researchers.",
      "abstract": "Artificial intelligence (AI) and machine learning (ML) facilitate the creation of revolutionary medical techniques. Unfortunately, biases in current AI and ML approaches are perpetuating minority health inequity. One of the strategies to solve this problem is training a diverse workforce. For this reason, we created the course \"Artificial Intelligence and Machine Learning applied to Health Disparities Research (AIML + HDR)\" which applied general Data Science (DS) approaches to health disparities research with an emphasis on Hispanic populations. Some technical topics covered included the Jupyter Notebook Framework, coding with R and Python to manipulate data, and ML libraries to create predictive models. Some health disparities topics covered included Electronic Health Records, Social Determinants of Health, and Bias in Data. As a result, the course was taught to 34 selected Hispanic participants and evaluated by a survey on a Likert scale (0-4). The surveys showed high satisfaction (more than 80% of participants agreed) regarding the course organization, activities, and covered topics. The students strongly agreed that the activities were relevant to the course and promoted their learning (3.71 \u00b1 0.21). The students strongly agreed that the course was helpful for their professional development (3.76 \u00b1 0.18). The open question was quantitatively analyzed and showed that seventy-five percent of the comments received from the participants confirmed their great satisfaction.",
      "journal": "International journal of environmental research and public health",
      "year": "2023",
      "doi": "10.3390/ijerph20032726",
      "authors": "Heredia-Negron Frances et al.",
      "keywords": "artificial intelligence; data science; health disparities; hispanic biomedical research; machine learning",
      "mesh_terms": "Humans; Artificial Intelligence; Data Science; Hispanic or Latino; Machine Learning; Workforce; Biomedical Research",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36768092/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9914971",
      "ft_text_length": 24294,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC9914971)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36842070",
      "title": "Inequities in Mental Health Care Facing Racialized Immigrant Older Adults With Mental Disorders Despite Universal Coverage: A Population-Based Study in Canada.",
      "abstract": "OBJECTIVES: Contemporary immigration scholarship has typically treated immigrants with diverse racial backgrounds as a monolithic population. Knowledge gaps remain in understanding how racial and nativity inequities in mental health care intersect and unfold in midlife and old age. This study aims to examine the joint impact of race, migration, and old age in shaping mental health treatment. METHODS: Pooled data were obtained from the Canadian Community Health Survey (2015-2018) and restricted to respondents (aged \u226545 years) with mood or anxiety disorders (n = 9,099). Multivariable logistic regression was performed to estimate associations between race-migration nexus and past-year mental health consultations (MHC). Classification and regression tree (CART) analysis was applied to identify intersecting determinants of MHC. RESULTS: Compared to Canadian-born Whites, racialized immigrants had greater mental health needs: poor/fair self-rated mental health (odds ratio [OR] = 2.23, 99% confidence interval [CI]: 1.67-2.99), perceived life stressful (OR = 1.49, 99% CI: 1.14-1.95), psychiatric comorbidity (OR = 1.42, 99% CI: 1.06-1.89), and unmet needs for care (OR = 2.02, 99% CI: 1.36-3.02); in sharp contrast, they were less likely to access mental health services across most indicators: overall past-year MHC (OR = 0.54, 99% CI: 0.41-0.71) and consultations with family doctors (OR = 0.67, 99% CI: 0.50-0.89), psychologists (OR = 0.54, 99% CI: 0.33-0.87), and social workers (OR = 0.37, 99% CI: 0.21-0.65), with the exception of psychiatrist visits (p = .324). The CART algorithm identifies three groups at risk of MHC service underuse: racialized immigrants aged \u226555 years, immigrants without high school diplomas, and linguistic minorities who were home renters. DISCUSSION: To safeguard health care equity for medically underserved communities in Canada, multisectoral efforts need to guarantee culturally responsive mental health care, multilingual services, and affordable housing for racialized immigrant older adults with mental disorders.",
      "journal": "The journals of gerontology. Series B, Psychological sciences and social sciences",
      "year": "2023",
      "doi": "10.1093/geronb/gbad036",
      "authors": "Lin Shen Lamson",
      "keywords": "Geriatric psychiatry; Machine learning; Mental health treatment; Migration; Minority aging",
      "mesh_terms": "Humans; Aged; Canada; Mental Health; Universal Health Insurance; Mental Disorders; Emigrants and Immigrants",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36842070/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10461535",
      "ft_text_length": 49985,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10461535)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36947701",
      "title": "Intelligent decision support in medical triage: are people robust to biased advice?",
      "abstract": "BACKGROUND: Intelligent artificial agents ('agents') have emerged in various domains of human society (healthcare, legal, social). Since using intelligent agents can lead to biases, a common proposed solution is to keep the human in the loop. Will this be enough to ensure unbiased decision making? METHODS: To address this question, an experimental testbed was developed in which a human participant and an agent collaboratively conduct triage on patients during a pandemic crisis. The agent uses data to support the human by providing advice and extra information about the patients. In one condition, the agent provided sound advice; the agent in the other condition gave biased advice. The research question was whether participants neutralized bias from the biased artificial agent. RESULTS: Although it was an exploratory study, the data suggest that human participants may not be sufficiently in control to correct the agent's bias. CONCLUSIONS: This research shows how important it is to design and test for human control in concrete human-machine collaboration contexts. It suggests that insufficient human control can potentially result in people being unable to detect biases in machines and thus unable to prevent machine biases from affecting decisions.",
      "journal": "Journal of public health (Oxford, England)",
      "year": "2023",
      "doi": "10.1093/pubmed/fdad005",
      "authors": "van der Stigchel Birgit et al.",
      "keywords": "emergency care; ethics; health intelligence",
      "mesh_terms": "Humans; Triage; Decision Support Systems, Clinical; Artificial Intelligence",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36947701/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10470333",
      "ft_text_length": 19301,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10470333)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36969013",
      "title": "Detection of missed deaths in cancer registry data to reduce bias in long-term survival estimation.",
      "abstract": "BACKGROUND: Population-based cancer survival estimates can provide insight into the real-world impacts of healthcare interventions and preventive services. However, estimation of survival rates obtained from population-based cancer registries can be biased due to missed incidence or incomplete vital status data. Long-term survival estimates in particular are prone to overestimation, since the proportion of deaths that are missed, for example through unregistered emigration, increases with follow-up time. This also applies to registry-based long-term prevalence estimates. The aim of this report is to introduce a method to detect missed deaths within cancer registry data such that long-term survival of cancer patients does not exceed survival in the general population. METHODS: We analyzed data from 15 German epidemiologic cancer registries covering the years 1970-2016 and from Surveillance, Epidemiology, and End Results (SEER)-18 registries covering 1975-2015. The method is based on comparing survival times until exit (death or follow-up end) and ages at exit between deceased patients and surviving patients, stratified by diagnosis group, sex, age group and stage. Deceased patients with both follow-up time and age at exit in the highest percentile were regarded as outliers and used to fit a logistic regression. The regression was then used to classify each surviving patient as a survivor or a missed death. The procedure was repeated for lower percentile thresholds regarding deceased persons until long-term survival rates no longer exceeded the survival rates in the general population. RESULTS: For the German cancer registry data, 0.9% of total deaths were classified as having been missed. Excluding these missed deaths reduced 20-year relative survival estimates for all cancers combined from 140% to 51%. For the whites in SEER data, classified missed deaths amounted to 0.02% of total deaths, resulting in 0.4 percent points lower 20-year relative survival rate for all cancers combined. CONCLUSION: The method described here classified a relatively small proportion of missed deaths yet reduced long-term survival estimates to more plausible levels. The effects of missed deaths should be considered when calculating long-term survival or prevalence estimates.",
      "journal": "Frontiers in oncology",
      "year": "2023",
      "doi": "10.3389/fonc.2023.1088657",
      "authors": "Dahm Stefan et al.",
      "keywords": "cancer registry data; classification algorithm; long-term survival; missed deaths; relative survival",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36969013/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10034313",
      "ft_text_length": 41755,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10034313)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36978286",
      "title": "Disaggregating Latino nativity in equity research using electronic health records.",
      "abstract": "OBJECTIVE: To develop and validate prediction models for inference of Latino nativity to advance health equity research. DATA SOURCES/STUDY SETTING: This study used electronic health records (EHRs) from 19,985 Latino children with self-reported country of birth seeking care from January 1, 2012 to December 31, 2018 at 456 community health centers (CHCs) across 15 states along with census-tract geocoded neighborhood composition and surname data. STUDY DESIGN: We constructed and evaluated the performance of prediction models within a broad machine learning framework (Super Learner) for the estimation of Latino nativity. Outcomes included binary indicators denoting nativity (US vs. foreign-born) and Latino country of birth (Mexican, Cuban, Guatemalan). The performance of these models was compared using the area under the receiver operating characteristics curve (AUC) from an externally withheld patient sample. DATA COLLECTION/EXTRACTION METHODS: Census surname lists, census neighborhood composition, and Forebears administrative data were linked to EHR data. PRINCIPAL FINDINGS: Of the 19,985 Latino patients, 10.7% reported a non-US country of birth (5.1% Mexican, 4.7% Guatemalan, 0.8% Cuban). Overall, prediction models for nativity showed outstanding performance with external validation (US-born vs. foreign: AUC\u2009=\u20090.90; Mexican vs. non-Mexican: AUC\u2009=\u20090.89; Guatemalan vs. non-Guatemalan: AUC\u2009=\u20090.95; Cuban vs. non-Cuban: AUC\u2009=\u20090.99). CONCLUSIONS: Among challenges facing health equity researchers in health services is the absence of methods for data disaggregation, and the specific ability to determine Latino country of birth (nativity) to inform disparities. Recent interest in more robust health equity research has called attention to the importance of data disaggregation. In a multistate network of CHCs using multilevel inputs from EHR data linked to surname and community data, we developed and validated novel prediction models for the use of available EHR data to infer Latino nativity for health disparities research in primary care and health services research, which is a significant potential methodologic advance in studying this population.",
      "journal": "Health services research",
      "year": "2023",
      "doi": "10.1111/1475-6773.14154",
      "authors": "Marino Miguel et al.",
      "keywords": "U.S. Census location; ethnicity; health disparities; machine learning; surname data",
      "mesh_terms": "Humans; Electronic Health Records; Health Equity; Hispanic or Latino; Residence Characteristics",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36978286/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10480087",
      "ft_text_length": 2179,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10480087)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36992801",
      "title": "Disparities in health condition diagnoses among aging transgender and cisgender medicare beneficiaries, 2008-2017.",
      "abstract": "INTRODUCTION: The objective of this research is to provide national estimates of the prevalence of health condition diagnoses among age-entitled transgender and cisgender Medicare beneficiaries. Quantification of the health burden across sex assigned at birth and gender can inform prevention, research, and allocation of funding for modifiable risk factors. METHODS: Using 2009-2017 Medicare fee-for-service data, we implemented an algorithm that leverages diagnosis, procedure, and pharmacy claims to identify age-entitled transgender Medicare beneficiaries and stratify the sample by inferred gender: trans feminine and nonbinary (TFN), trans masculine and nonbinary (TMN), and unclassified. We selected a 5% random sample of cisgender individuals for comparison. We descriptively analyzed (means and frequencies) demographic characteristics (age, race/ethnicity, US census region, months of enrollment) and used chi-square and t-tests to determine between- (transgender vs. cisgender) and within-group gender differences (e.g., TMN, TFN, unclassified) difference in demographics (p<0.05). We then used logistic regression to estimate and examine within- and between-group gender differences in the predicted probability of 25 health conditions, controlling for age, race/ethnicity, enrollment length, and census region. RESULTS: The analytic sample included 9,975 transgender (TFN n=4,198; TMN n=2,762; unclassified n=3,015) and 2,961,636 cisgender (male n=1,294,690, female n=1,666,946) beneficiaries. The majority of the transgender and cisgender samples were between the ages of 65 and 69 and White, non-Hispanic. The largest proportion of transgender and cisgender beneficiaries were from the South. On average, transgender individuals had more months of enrollment than cisgender individuals. In adjusted models, aging TFN or TMN Medicare beneficiaries had the highest probability of each of the 25 health diagnoses studied relative to cisgender males or females. TFN beneficiaries had the highest burden of health diagnoses relative to all other groups. DISCUSSION: These findings document disparities in key health condition diagnoses among transgender Medicare beneficiaries relative to cisgender individuals. Future application of these methods will enable the study of rare and anatomy-specific conditions among hard-to-reach aging transgender populations and inform interventions and policies to address documented disparities.",
      "journal": "Frontiers in endocrinology",
      "year": "2023",
      "doi": "10.3389/fendo.2023.1102348",
      "authors": "Hughto Jaclyn M W et al.",
      "keywords": "Medicare; aging; chronic conditions; diagnoses; health disparities; insurance claims; older adults; transgender",
      "mesh_terms": "Aged; Female; Humans; Male; Aging; Ethnicity; Gender Identity; Medicare; Transgender Persons; United States; Aged, 80 and over",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36992801/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10040837",
      "ft_text_length": 49524,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10040837)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36998311",
      "title": "A fair and interpretable network for clinical risk prediction: a regularized multi-view multi-task learning approach.",
      "abstract": "In healthcare domain, complication risk profiling which can be seen as multiple clinical risk prediction tasks is challenging due to the complex interaction between heterogeneous clinical entities. With the availability of real-world data, many deep learning methods are proposed for complication risk profiling. However, the existing methods face three open challenges. First, they leverage clinical data from a single view and then lead to suboptimal models. Second, most existing methods lack an effective mechanism to interpret predictions. Third, models learned from clinical data may have inherent pre-existing biases and exhibit discrimination against certain social groups. We then propose a multi-view multi-task network (MuViTaNet) to tackle these issues. MuViTaNet complements patient representation by using a multi-view encoder to exploit more information. Moreover, it uses a multi-task learning to generate more generalized representations using both labeled and unlabeled datasets. Last, a fairness variant (F-MuViTaNet) is proposed to mitigate the unfairness issues and promote healthcare equity. The experiments show that MuViTaNet outperforms existing methods for cardiac complication profiling. Its architecture also provides an effective mechanism for interpreting the predictions, which helps clinicians discover the underlying mechanism triggering the complication onsets. F-MuViTaNet can also effectively mitigate the unfairness with only negligible impact on accuracy.",
      "journal": "Knowledge and information systems",
      "year": "2023",
      "doi": "10.1007/s10115-022-01813-2",
      "authors": "Pham Thai-Hoang et al.",
      "keywords": "Attention; Complication risk profiling; Contrastive learning; Equal opportunity; Fairness; Multi-task; Multi-view; Regularization",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36998311/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10046420",
      "ft_text_length": 1497,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10046420)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37063491",
      "title": "Multi-energy CT material decomposition using Bayesian deep convolutional neural network with explicit penalty of uncertainty and bias.",
      "abstract": "Convolutional neural network (CNN)-based material decomposition has the potential to improve image quality (visual appearance) and quantitative accuracy of material maps. Most methods use deterministic CNNs with mean-square-error loss to provide point-estimates of mass densities. Point estimates can be over-confident as the reliability of CNNs is frequently compromised by bias and two major uncertainties - data and model uncertainties originating from noise in inputs and train-test data dissimilarity, respectively. Also, mean-square-error lacks explicit control of uncertainty and bias. To tackle these problems, a Bayesian dual-task CNN (BDT-CNN) with explicit penalization of uncertainty and bias was developed. It is a probabilistic CNN that concurrently conducts material classification and quantification and allows for pixel-wise modeling of bias, data uncertainty, and model uncertainty. CNN was trained with images of physical and simulated tissue-mimicking inserts at varying mass densities. Hydroxyapatite (nominal density 400mg/cc) and blood (nominal density 1095mg/cc) inserts were placed in different-sized body phantoms (30 - 45cm) and used to evaluate mean-absolute-bias (MAB) in predicted mass densities across different images at routine- and half-routine-dose. Patient CT exams were collected to assess generalizability of BDT-CNN in the presence of anatomical background. Noise insertion was used to simulate patient exams at half- and quarter-routine-dose. The deterministic dual-task CNN was used as baseline. In phantoms, BDT-CNN improved consistency of insert delineation, especially edges, and reduced overall bias (average MAB for hydroxyapatite: BDT-CNN 5.4mgHA/cc, baseline 11.0mgHA/cc and blood: BDT-CNN 8.9mgBlood/cc, baseline 14.0mgBlood/cc). In patient images, BDT-CNN improved detail preservation, lesion conspicuity, and structural consistency across different dose levels.",
      "journal": "Proceedings of SPIE--the International Society for Optical Engineering",
      "year": "2023",
      "doi": "10.1117/12.2654317",
      "authors": "Gong Hao et al.",
      "keywords": "Bayesian neural network; Multi-energy CT; bias; deep learning; material decomposition; uncertainty",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37063491/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10099768",
      "ft_text_length": 1912,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10099768)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37081528",
      "title": "Higher performance for women than men in MRI-based Alzheimer's disease detection.",
      "abstract": "INTRODUCTION: Although machine learning classifiers have been frequently used to detect Alzheimer's disease (AD) based on structural brain MRI data, potential bias with respect to sex and age has not yet been addressed. Here, we examine a state-of-the-art AD classifier for potential sex and age bias even in the case of balanced training data. METHODS: Based on an age- and sex-balanced cohort of 432 subjects (306 healthy controls, 126 subjects with AD) extracted from the ADNI data base, we trained a convolutional neural network to detect AD in MRI brain scans and performed ten different random training-validation-test splits to increase robustness of the results. Classifier decisions for single subjects were explained using layer-wise relevance propagation. RESULTS: The classifier performed significantly better for women (balanced accuracy [Formula: see text]) than for men ([Formula: see text]). No significant differences were found in clinical AD scores, ruling out a disparity in disease severity as a cause for the performance difference. Analysis of the explanations revealed a larger variance in regional brain areas for male subjects compared to female subjects. DISCUSSION: The identified sex differences cannot be attributed to an imbalanced training dataset and therefore point to the importance of examining and reporting classifier performance across population subgroups to increase transparency and algorithmic fairness. Collecting more data especially among underrepresented subgroups and balancing the dataset are important but do not always guarantee a fair outcome.",
      "journal": "Alzheimer's research & therapy",
      "year": "2023",
      "doi": "10.1186/s13195-023-01225-6",
      "authors": "Klingenberg Malte et al.",
      "keywords": "Alzheimer\u2019s disease; Bias; Deep learning; MRI; Sex",
      "mesh_terms": "Humans; Male; Female; Alzheimer Disease; Cognitive Dysfunction; Magnetic Resonance Imaging; Neuroimaging; Machine Learning",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37081528/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10116672",
      "ft_text_length": 52108,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10116672)",
      "ft_reason": "Excluded: insufficient approach content (1 indicators)"
    },
    {
      "pmid": "37198691",
      "title": "Biased data, biased AI: deep networks predict the acquisition site of TCGA images.",
      "abstract": "BACKGROUND: Deep learning models applied to healthcare applications including digital pathology have been increasing their scope and importance in recent years. Many of these models have been trained on The Cancer Genome Atlas (TCGA) atlas of digital images, or use it as a validation source. One crucial factor that seems to have been widely ignored is the internal bias that originates from the institutions that contributed WSIs to the TCGA dataset, and its effects on models trained on this dataset. METHODS: 8,579 paraffin-embedded, hematoxylin and eosin stained, digital slides were selected from the TCGA dataset. More than 140 medical institutions (acquisition sites) contributed to this dataset. Two deep neural networks (DenseNet121 and KimiaNet were used to extract deep features at 20\u00d7 magnification. DenseNet was pre-trained on non-medical objects. KimiaNet has the same structure but trained for cancer type classification on TCGA images. The extracted deep features were later used to detect each slide's acquisition site, and also for slide representation in image search. RESULTS: DenseNet's deep features could distinguish acquisition sites with 70% accuracy whereas KimiaNet's deep features could reveal acquisition sites with more than 86% accuracy. These findings suggest that there are acquisition site specific patterns that could be picked up by deep neural networks. It has also been shown that these medically irrelevant patterns can interfere with other applications of deep learning in digital pathology, namely image search. This study shows that there are acquisition site specific patterns that can be used to identify tissue acquisition sites without any explicit training. Furthermore, it was observed that a model trained for cancer subtype classification has exploited such medically irrelevant patterns to classify cancer types. Digital scanner configuration and noise, tissue stain variation and artifacts, and source site patient demographics are among factors that likely account for the observed bias. Therefore, researchers should be cautious of such bias when using histopathology datasets for developing and training deep networks.",
      "journal": "Diagnostic pathology",
      "year": "2023",
      "doi": "10.1186/s13000-023-01355-3",
      "authors": "Dehkharghanian Taher et al.",
      "keywords": "AI bias; AI ethics; Cancer; Deep Learning; Digital pathology; TCGA",
      "mesh_terms": "Humans; Neoplasms; Neural Networks, Computer; Coloring Agents; Hematoxylin; Eosine Yellowish-(YS)",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37198691/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10189924",
      "ft_text_length": 34784,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10189924)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37218934",
      "title": "Can Machine Learning Be Better than Biased Readers?",
      "abstract": "Background: Training machine learning (ML) models in medical imaging requires large amounts of labeled data. To minimize labeling workload, it is common to divide training data among multiple readers for separate annotation without consensus and then combine the labeled data for training a ML model. This can lead to a biased training dataset and poor ML algorithm prediction performance. The purpose of this study is to determine if ML algorithms can overcome biases caused by multiple readers' labeling without consensus. Methods: This study used a publicly available chest X-ray dataset of pediatric pneumonia. As an analogy to a practical dataset without labeling consensus among multiple readers, random and systematic errors were artificially added to the dataset to generate biased data for a binary-class classification task. The Resnet18-based convolutional neural network (CNN) was used as a baseline model. A Resnet18 model with a regularization term added as a loss function was utilized to examine for improvement in the baseline model. Results: The effects of false positive labels, false negative labels, and random errors (5-25%) resulted in a loss of AUC (0-14%) when training a binary CNN classifier. The model with a regularized loss function improved the AUC (75-84%) over that of the baseline model (65-79%). Conclusion: This study indicated that it is possible for ML algorithms to overcome individual readers' biases when consensus is not available. It is recommended to use regularized loss functions when allocating annotation tasks to multiple readers as they are easy to implement and effective in mitigating biased labels.",
      "journal": "Tomography (Ann Arbor, Mich.)",
      "year": "2023",
      "doi": "10.3390/tomography9030074",
      "authors": "Hibi Atsuhiro et al.",
      "keywords": "annotation bias; chest X-ray; convolutional neural network; labeling consensus; machine learning",
      "mesh_terms": "Humans; Child; Machine Learning; Neural Networks, Computer; Algorithms; Bias",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37218934/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10204355",
      "ft_text_length": 18215,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10204355)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37312214",
      "title": "An equitable and sustainable community of practice framework to address the use of artificial intelligence for global health workforce training.",
      "abstract": "Artificial Intelligence (AI) technologies and data science models may hold potential for enabling an understanding of global health inequities and support decision-making related toward possible interventions. However, AI inputs should not perpetuate the biases and structural issues within our global societies that have created various health inequities. We need AI to\u00a0be able to 'see' the full context of what it is meant to learn. AI trained with biased data produces biased outputs and providing health workforce training with such outputs further contributes to the buildup of biases and structural inequities. The accelerating and intricately evolving technology and digitalization will influence the education and practice of health care workers. Before we invest in utilizing AI in health workforce training globally, it is important to make sure that multiple stakeholders from the global arena are included in the conversation to address the need for training in 'AI and the role of AI in training'. This is a daunting task for any one entity and a multi-sectorial interactions and solutions are needed. We believe that partnerships among various national, regional, and global stakeholders involved directly or indirectly with health workforce training ranging to name a few, from public health & clinical science training institutions, computer science, learning design, data science, technology companies, social scientists, law, and AI ethicists, need to be developed in ways that enable the formation of an equitable and sustainable Communities of Practice (CoP) to address the use of AI for global health workforce training. This paper has laid out a framework for such CoP.",
      "journal": "Human resources for health",
      "year": "2023",
      "doi": "10.1186/s12960-023-00833-5",
      "authors": "Frehywot Seble et al.",
      "keywords": "Artificial Intelligence; Capacity-building; Community of practice; Equity; Machine Learning; Health workforce training",
      "mesh_terms": "Humans; Health Workforce; Artificial Intelligence; Workforce; Educational Status; Learning",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37312214/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10262492",
      "ft_text_length": 20231,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10262492)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37318857",
      "title": "Algorithmic bias and research integrity; the role of nonhuman authors in shaping scientific knowledge with respect to artificial intelligence: a perspective.",
      "abstract": "Artificial intelligence technologies were developed to assist authors in bettering the organization and caliber of their published papers, which are both growing in quantity and sophistication. Even though the usage of artificial intelligence tools in particular ChatGPT's natural language processing systems has been shown to be beneficial in research, there are still concerns about accuracy, responsibility, and transparency when it comes to the norms regarding authorship credit and contributions. Genomic algorithms quickly examine large amounts of genetic data to identify potential disease-causing mutations. By analyzing millions of medications for potential therapeutic benefits, they can quickly and relatively economically find novel approaches to treatment. Researchers from several fields can collaborate on difficult tasks with the assistance of nonhuman writers, promoting interdisciplinary research. Sadly, there are a number of significant disadvantages associated with employing nonhuman authors, including the potential for algorithmic prejudice. Biased data may be reinforced by the algorithm since machine learning algorithms can only be as objective as the data they are trained on. It is overdue that scholars bring forth basic moral concerns in the fight against algorithmic prejudice. Overall, even if the use of nonhuman authors has the potential to significantly improve scientific research, it is crucial for scientists to be aware of these drawbacks and take precautions to avoid bias and limits. To provide accurate and objective results, algorithms must be carefully designed and implemented, and researchers need to be mindful of the larger ethical ramifications of their usage.",
      "journal": "International journal of surgery (London, England)",
      "year": "2023",
      "doi": "10.1097/JS9.0000000000000552",
      "authors": "Oduoye Malik Olatunde et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Algorithms; Authorship; Awareness; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37318857/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10583945",
      "ft_text_length": 14514,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10583945)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37350910",
      "title": "Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served populations.",
      "abstract": "In the United States, primary open-angle glaucoma (POAG) is the leading cause of blindness, especially among African American and Hispanic individuals. Deep learning has been widely used to detect POAG using fundus images as its performance is comparable to or even surpasses diagnosis by clinicians. However, human bias in clinical diagnosis may be reflected and amplified in the widely-used deep learning models, thus impacting their performance. Biases may cause (1) underdiagnosis, increasing the risks of delayed or inadequate treatment, and (2) overdiagnosis, which may increase individuals' stress, fear, well-being, and unnecessary/costly treatment. In this study, we examined the underdiagnosis and overdiagnosis when applying deep learning in POAG detection based on the Ocular Hypertension Treatment Study (OHTS) from 22 centers across 16 states in the United States. Our results show that the widely-used deep learning model can underdiagnose or overdiagnose under-served populations. The most underdiagnosed group is female younger (< 60 yrs) group, and the most overdiagnosed group is Black older (\u2265 60 yrs) group. Biased diagnosis through traditional deep learning methods may delay disease detection, treatment and create burdens among under-served populations, thereby, raising ethical concerns about using deep learning models in ophthalmology clinics.",
      "journal": "AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science",
      "year": "2023",
      "doi": "",
      "authors": "Lin Mingquan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37350910/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10283103",
      "ft_text_length": 1370,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10283103)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37450282",
      "title": "Retinal Vessel Caliber Measurement Bias in Fundus Images in the Presence of the Central Light Reflex.",
      "abstract": "PURPOSE: To investigate the agreement between a fundus camera and a scanning laser ophthalmoscope in retinal vessel caliber measurements and to identify whether the presence of the central light reflex (CLR) explains potential discrepancies. METHODS: For this cross-sectional study, we obtained fundus camera and scanning laser ophthalmoscope images from 85 eyes of 85 healthy individuals (aged 50-65 years) with different blood pressure status. We measured the central retinal artery equivalent (CRAE) and central retinal artery vein equivalent (CRVE) with the Knudtson-Parr-Hubbard algorithm and assessed the CLR using a semiautomatic grading method. We used Bland-Altman plots, 95% limits of agreement, and the two-way mixed effects intraclass correlation coefficient for consistency [ICC(3,1)] to describe interdevice agreement. We used multivariable regression to identify factors associated with differences in between-device measurements. RESULTS: The between-device difference in CRAE (9.5 \u00a0\u00b5m; 95% confidence interval, 8.0-11.1 \u00a0\u00b5m) was larger than the between-device difference in CRVE (2.9 \u00a0\u00b5m; 95% confidence interval, 1.3-4.5 \u00a0\u00b5m), with the fundus camera yielding higher measurements (both P < 0.001). The 95% fundus camera-scanning laser ophthalmoscope limits of agreement were -4.8 to 23.9 \u00a0\u00b5m for CRAE and -12.0 to 17.8 \u00a0\u00b5m for CRVE. The corresponding ICCs(3,1) were 0.89 (95% confidence interval, 0.83-0.92) and 0.91 (95% confidence interval, 0.86-0.94). The between-device CRAE difference was positively associated with the presence of a CLR (P = 0.002). CONCLUSIONS: Fundus cameras and scanning laser ophthalmoscopes yield correlated but not interchangeable caliber measurements. The CLR induces bias in arteriolar caliber in fundus camera images, compared with scanning laser ophthalmoscope images. TRANSLATIONAL RELEVANCE: Refined measurements could yield better estimates of the association between retinal vessel caliber and ophthalmic or systemic disease.",
      "journal": "Translational vision science & technology",
      "year": "2023",
      "doi": "10.1167/tvst.12.7.16",
      "authors": "Pappelis Konstantinos et al.",
      "keywords": "",
      "mesh_terms": "Humans; Cross-Sectional Studies; Retinal Vessels; Retinal Vein; Retinal Artery; Reflex",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37450282/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10353742",
      "ft_text_length": 28351,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10353742)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37480515",
      "title": "Non-response Bias in Social Risk Factor Screening Among Adult Emergency Department Patients.",
      "abstract": "Healthcare organizations increasingly use screening questionnaires to assess patients' social factors, but non-response may contribute to selection bias. This study assessed differences between respondents and those refusing participation in a social factor screening. We used a cross-sectional approach with logistic regression models to measure the association between subject characteristics and social factor screening questionnaire participation. The study subjects were patients from a mid-western state safety-net hospital's emergency department. Subjects' inclusion criteria were: (1) \u2265 18 years old, (2) spoke English or Spanish, and (3) able to complete a self-administered questionnaire. We classified subjects that consented and answered the screening questionnaire in full as respondents. All others were non-respondents. Using natural language processing, we linked all subjects' participation status to demographic characteristics, clinical data, an area-level deprivation measure, and social risk factors extracted from clinical notes. We found that nearly 6 out of every 10 subjects approached (59.9%), consented, and completed the questionnaire. Subjects with prior documentation of financial insecurity were 22% less likely to respond to the screening questionnaire (marginal effect = -22.40; 95% confidence interval (CI) = -41.16, -3.63; p\u2009=\u20090.019). No other factors were significantly associated with response. This study uniquely contributes to the growing social determinants of health literature by confirming that selection bias may exist within social factor screening practices and research studies.",
      "journal": "Journal of medical systems",
      "year": "2023",
      "doi": "10.1007/s10916-023-01975-8",
      "authors": "Vest Joshua R et al.",
      "keywords": "Bias; Emergency department; Social determinants of health; Surveys",
      "mesh_terms": "Humans; Adult; Adolescent; Documentation; Emergency Service, Hospital; Language; Logistic Models; Natural Language Processing",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37480515/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10439727",
      "ft_text_length": 1623,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10439727)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37546907",
      "title": "Equitable machine learning counteracts ancestral bias in precision medicine, improving outcomes for all.",
      "abstract": "Gold standard genomic datasets severely under-represent non-European populations, leading to inequities and a limited understanding of human disease [1-8]. Therapeutics and outcomes remain hidden because we lack insights that we could gain from analyzing ancestry-unbiased genomic data. To address this significant gap, we present PhyloFrame, the first-ever machine learning method for equitable genomic precision medicine. PhyloFrame corrects for ancestral bias by integrating big data tissue-specific functional interaction networks, global population variation data, and disease-relevant transcriptomic data. Application of PhyloFrame to breast, thyroid, and uterine cancers shows marked improvements in predictive power across all ancestries, less model overfitting, and a higher likelihood of identifying known cancer-related genes. The ability to provide accurate predictions for underrepresented groups, in particular, is substantially increased. These results demonstrate how AI can mitigate ancestral bias in training data and contribute to equitable representation in medical research.",
      "journal": "Research square",
      "year": "2023",
      "doi": "10.21203/rs.3.rs-3168446/v1",
      "authors": "Smith Leslie A et al.",
      "keywords": "ancestry; artificial intelligence; cancer; equitable AI; genomics",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37546907/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10402189",
      "ft_text_length": 37811,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10402189)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37561535",
      "title": "Artificial intelligence suppression as a strategy to mitigate artificial intelligence automation bias.",
      "abstract": "BACKGROUND: Incorporating artificial intelligence (AI) into clinics brings the risk of automation bias, which potentially misleads the clinician's decision-making. The purpose of this study was to propose a potential strategy to mitigate automation bias. METHODS: This was a laboratory study with a randomized cross-over design. The diagnosis of anterior cruciate ligament (ACL) rupture, a common injury, on magnetic resonance imaging (MRI) was used as an example. Forty clinicians were invited to diagnose 200 ACLs with and without AI assistance. The AI's correcting and misleading (automation bias) effects on the clinicians' decision-making processes were analyzed. An ordinal logistic regression model was employed to predict the correcting and misleading probabilities of the AI. We further proposed an AI suppression strategy that retracted AI diagnoses with a higher misleading probability and provided AI diagnoses with a higher correcting probability. RESULTS: The AI significantly increased clinicians' accuracy from 87.2%\u00b113.1% to 96.4%\u00b11.9% (P\u2009<\u2009.001). However, the clinicians' errors in the AI-assisted round were associated with automation bias, accounting for 45.5% of the total mistakes. The automation bias was found to affect clinicians of all levels of expertise. Using a logistic regression model, we identified an AI output zone with higher probability to generate misleading diagnoses. The proposed AI suppression strategy was estimated to decrease clinicians' automation bias by 41.7%. CONCLUSION: Although AI improved clinicians' diagnostic performance, automation bias was a serious problem that should be addressed in clinical practice. The proposed AI suppression strategy is a practical method for decreasing automation bias.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2023",
      "doi": "10.1093/jamia/ocad118",
      "authors": "Wang Ding-Yu et al.",
      "keywords": "AI suppression; automation bias; clinician-AI interaction; deep learning",
      "mesh_terms": "Artificial Intelligence; Magnetic Resonance Imaging; Clinical Decision-Making; Humans; Anterior Cruciate Ligament Injuries; Diagnosis, Computer-Assisted",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37561535/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10531198",
      "ft_text_length": 1760,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10531198)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37566454",
      "title": "Ethical Considerations of Using ChatGPT in Health Care.",
      "abstract": "ChatGPT has promising applications in health care, but potential ethical issues need to be addressed proactively to prevent harm. ChatGPT presents potential ethical challenges from legal, humanistic, algorithmic, and informational perspectives. Legal ethics concerns arise from the unclear allocation of responsibility when patient harm occurs and from potential breaches of patient privacy due to data collection. Clear rules and legal boundaries are needed to properly allocate liability and protect users. Humanistic ethics concerns arise from the potential disruption of the physician-patient relationship, humanistic care, and issues of integrity. Overreliance on artificial intelligence (AI) can undermine compassion and erode trust. Transparency and disclosure of AI-generated content are critical to maintaining integrity. Algorithmic ethics raise concerns about algorithmic bias, responsibility, transparency and explainability, as well as validation and evaluation. Information ethics include data bias, validity, and effectiveness. Biased training data can lead to biased output, and overreliance on ChatGPT can reduce patient adherence and encourage self-diagnosis. Ensuring the accuracy, reliability, and validity of ChatGPT-generated content requires rigorous validation and ongoing updates based on clinical practice. To navigate the evolving ethical landscape of AI, AI in health care must adhere to the strictest ethical standards. Through comprehensive ethical guidelines, health care professionals can ensure the responsible use of ChatGPT, promote accurate and reliable information exchange, protect patient privacy, and empower patients to make informed decisions about their health care.",
      "journal": "Journal of medical Internet research",
      "year": "2023",
      "doi": "10.2196/48009",
      "authors": "Wang Changyu et al.",
      "keywords": "AI; ChatGPT; algorithm; artificial intelligence; artificial intelligence development; development; ethics; health care; large language models; patient privacy; patient safety; privacy; safety",
      "mesh_terms": "Humans; Artificial Intelligence; Reproducibility of Results; Data Collection; Disclosure; Patient Compliance",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37566454/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10457697",
      "ft_text_length": 24078,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10457697)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37605208",
      "title": "Fairness and generalizability of OCT normative databases: a comparative analysis.",
      "abstract": "PURPOSE: In supervised Machine Learning algorithms, labels and reports are important in model development. To provide a normality assessment, the OCT has an in-built normative database that provides a color base scale from the measurement database comparison. This article aims to evaluate and compare normative databases of different OCT machines, analyzing patient demographic, contrast inclusion and exclusion criteria, diversity index, and statistical approach to assess their fairness and generalizability. METHODS: Data were retrieved from Cirrus, Avanti, Spectralis, and Triton's FDA-approval and equipment manual. The following variables were compared: number of eyes and patients, inclusion and exclusion criteria, statistical approach, sex, race and ethnicity, age, participant country, and diversity index. RESULTS: Avanti OCT has the largest normative database (640 eyes). In every database, the inclusion and exclusion criteria were similar, including adult patients and excluding pathological eyes. Spectralis has the largest White (79.7%) proportionately representation, Cirrus has the largest Asian (24%), and Triton has the largest Black (22%) patient representation. In all databases, the statistical analysis applied was Regression models. The sex diversity index is similar in all datasets, and comparable to the ten most populous contries. Avanti dataset has the highest diversity index in terms of race, followed by Cirrus, Triton, and Spectralis. CONCLUSION: In all analyzed databases, the data framework is static, with limited upgrade options and lacking normative databases for new modules. As a result, caution in OCT normality interpretation is warranted. To address these limitations, there is a need for more diverse, representative, and open-access datasets that take into account patient demographics, especially considering the development of supervised Machine Learning algorithms in healthcare.",
      "journal": "International journal of retina and vitreous",
      "year": "2023",
      "doi": "10.1186/s40942-023-00459-8",
      "authors": "Nakayama Luis Filipe et al.",
      "keywords": "Database; Fairness; Generalizability; Optical coherence tomography; Supervised machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37605208/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10440930",
      "ft_text_length": 19368,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10440930)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37610808",
      "title": "Can AI Mitigate Bias in Writing Letters of Recommendation?",
      "abstract": "Letters of recommendation play a significant role in higher education and career progression, particularly for women and underrepresented groups in medicine and science. Already, there is evidence to suggest that written letters of recommendation contain language that expresses implicit biases, or unconscious biases, and that these biases occur for all recommenders regardless of the recommender's sex. Given that all individuals have implicit biases that may influence language use, there may be opportunities to apply contemporary technologies, such as large language models or other forms of generative artificial intelligence (AI), to augment and potentially reduce implicit biases in the written language of letters of recommendation. In this editorial, we provide a brief overview of existing literature on the manifestations of implicit bias in letters of recommendation, with a focus on academia and medical education. We then highlight potential opportunities and drawbacks of applying this emerging technology in augmenting the focused, professional task of writing letters of recommendation. We also offer best practices for integrating their use into the routine writing of letters of recommendation and conclude with our outlook for the future of generative AI applications in supporting this task.",
      "journal": "JMIR medical education",
      "year": "2023",
      "doi": "10.2196/51494",
      "authors": "Leung Tiffany I et al.",
      "keywords": "artificial intelligence; bias; career advancement; gender bias; implicit bias; large language models; leadership; letters of recommendation; medical education; promotion; sponsorship; tenure and promotion",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37610808/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10483302",
      "ft_text_length": 17092,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10483302)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37615359",
      "title": "Toward Advancing Precision Environmental Health: Developing a Customized Exposure Burden Score to PFAS Mixtures to Enable Equitable Comparisons Across Population Subgroups, Using Mixture Item Response Theory.",
      "abstract": "Quantifying a person's cumulative exposure burden to per- and polyfluoroalkyl substances (PFAS) mixtures is important for risk assessment, biomonitoring, and reporting of results to participants. However, different people may be exposed to different sets of PFASs due to heterogeneity in the exposure sources and patterns. Applying a single measurement model for the entire population (e.g., by summing concentrations of all PFAS analytes) assumes that each PFAS analyte is equally informative to PFAS exposure burden for all individuals. This assumption may not hold if PFAS exposure sources systematically differ within the population. However, the sociodemographic, dietary, and behavioral characteristics that underlie systematic exposure differences may not be known, or may be due to a combination of these factors. Therefore, we used mixture item response theory, an unsupervised psychometrics and data science method, to develop a customized PFAS exposure burden scoring algorithm. This scoring algorithm ensures that PFAS burden scores can be equitably compared across population subgroups. We applied our methods to PFAS biomonitoring data from the United States National Health and Nutrition Examination Survey (2013-2018). Using mixture item response theory, we found that participants with higher household incomes had higher PFAS burden scores. Asian Americans had significantly higher PFAS burden compared with non-Hispanic Whites and other race/ethnicity groups. However, some disparities were masked when using summed PFAS concentrations as the exposure metric. This work demonstrates that our summary PFAS burden metric, accounting for sources of exposure variation, may be a more fair and informative estimate of PFAS exposure.",
      "journal": "Environmental science & technology",
      "year": "2023",
      "doi": "10.1021/acs.est.3c00343",
      "authors": "Liu Shelley H et al.",
      "keywords": "algorithmic bias; chemical mixtures; fairness; item response theory; latent variables; per- and polyfluoroalkyl substances; precision environmental health; psychometrics",
      "mesh_terms": "Humans; United States; Environmental Pollutants; Alkanesulfonic Acids; Nutrition Surveys; Fluorocarbons; Environmental Health",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37615359/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11106720",
      "ft_text_length": 1693,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11106720)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37649910",
      "title": "Performance of Off-the-Shelf Machine Learning Architectures and Biases in Detection of Low Left Ventricular Ejection Fraction.",
      "abstract": "Artificial intelligence - machine learning (AI-ML) is a computational technique that has been demonstrated to be able to extract meaningful clinical information from diagnostic data that are not available using either human interpretation or more simple analysis methods. Recent developments have shown that AI-ML approaches applied to ECGs can accurately predict different patient characteristics and pathologies not detectable by expert physician readers. There is an extensive body of literature surrounding the use of AI-ML in other fields, which has given rise to an array of predefined open-source AI-ML architectures which can be translated to new problems in an \"off-the-shelf\" manner. Applying \"off-the-shelf\" AI-ML architectures to ECG-based datasets opens the door for rapid development and identification of previously unknown disease biomarkers. Despite the excellent opportunity, the ideal open-source AI-ML architecture for ECG related problems is not known. Furthermore, there has been limited investigation on how and when these AI-ML approaches fail and possible bias or disparities associated with particular network architectures. In this study, we aimed to: (1) determine if open-source, \"off-the-shelf\" AI-ML architectures could be trained to classify low LVEF from ECGs, (2) assess the accuracy of different AI-ML architectures compared to each other, and (3) to identify which, if any, patient characteristics are associated with poor AI-ML performance.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2023",
      "doi": "10.1101/2023.06.10.23291237",
      "authors": "Bergquist Jake A et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37649910/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10465010",
      "ft_text_length": 1488,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10465010)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37695082",
      "title": "Artificial intelligence-based clinical decision support for liver transplant evaluation and considerations about fairness: A qualitative study.",
      "abstract": "BACKGROUND: The use of large-scale data and artificial intelligence (AI) to support complex transplantation decisions is in its infancy. Transplant candidate decision-making, which relies heavily on subjective assessment (ie, high variability), provides a ripe opportunity for AI-based clinical decision support (CDS). However, AI-CDS for transplant applications must consider important concerns regarding fairness (ie, health equity). The objective of this study was to use human-centered design methods to elicit providers' perceptions of AI-CDS for liver transplant listing decisions. METHODS: In this multicenter qualitative study conducted from December 2020 to July 2021, we performed semistructured interviews with 53 multidisciplinary liver transplant providers from 2 transplant centers. We used inductive coding and constant comparison analysis of interview data. RESULTS: Analysis yielded 6 themes important for the design of fair AI-CDS for liver transplant listing decisions: (1) transparency in the creators behind the AI-CDS and their motivations; (2) understanding how the AI-CDS uses data to support recommendations (ie, interpretability); (3) acknowledgment that AI-CDS could mitigate emotions and biases; (4) AI-CDS as a member of the transplant team, not a replacement; (5) identifying patient resource needs; and (6) including the patient's role in the AI-CDS. CONCLUSIONS: Overall, providers interviewed were cautiously optimistic about the potential for AI-CDS to improve clinical and equitable outcomes for patients. These findings can guide multidisciplinary developers in the design and implementation of AI-CDS that deliberately considers health equity.",
      "journal": "Hepatology communications",
      "year": "2023",
      "doi": "10.1097/HC9.0000000000000239",
      "authors": "Strauss Alexandra T et al.",
      "keywords": "",
      "mesh_terms": "Humans; Liver Transplantation; Artificial Intelligence; Decision Support Systems, Clinical; Qualitative Research",
      "pub_types": "Multicenter Study; Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37695082/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10497243",
      "ft_text_length": 36666,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10497243)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37789032",
      "title": "Humans inherit artificial intelligence biases.",
      "abstract": "Artificial intelligence recommendations are sometimes erroneous and biased. In our research, we hypothesized that people who perform a (simulated) medical diagnostic task assisted by a biased AI system will reproduce the model's bias in their own decisions, even when they move to a context without AI support. In three experiments, participants completed a medical-themed classification task with or without the help of a biased AI system. The biased recommendations by the AI influenced participants' decisions. Moreover, when those participants, assisted by the AI, moved on to perform the task without assistance, they made the same errors as the AI had made during the previous phase. Thus, participants' responses mimicked AI bias even when the AI was no longer making suggestions. These results provide evidence of human inheritance of AI bias.",
      "journal": "Scientific reports",
      "year": "2023",
      "doi": "10.1038/s41598-023-42384-8",
      "authors": "Vicente Luc\u00eda et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Bias; Inheritance Patterns; Suggestion",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37789032/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10547752",
      "ft_text_length": 49054,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10547752)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37790341",
      "title": "Evaluating and Improving Health Equity and Fairness of Polygenic Scores.",
      "abstract": "Polygenic scores (PGS) are quantitative metrics for predicting phenotypic values, such as human height or disease status. Some PGS methods require only summary statistics of a relevant genome-wide association study (GWAS) for their score. One such method is Lassosum, which inherits the model selection advantages of Lasso to select a meaningful subset of the GWAS single nucleotide polymorphisms as predictors from their association statistics. However, even efficient scores like Lassosum, when derived from European-based GWAS, are poor predictors of phenotype for subjects of non-European ancestry; that is, they have limited portability to other ancestries. To increase the portability of Lassosum, when GWAS information and estimates of linkage disequilibrium are available for both ancestries, we propose Joint-Lassosum. In the simulation settings we explore, Joint-Lassosum provides more accurate PGS compared with other methods, especially when measured in terms of fairness. Like all PGS methods, Joint-Lassosum requires selection of predictors, which are determined by data-driven tuning parameters. We describe a new approach to selecting tuning parameters and note its relevance for model selection for any PGS. We also draw connections to the literature on algorithmic fairness and discuss how Joint-Lassosum can help mitigate fairness-related harms that might result from the use of PGS scores in clinical settings. While no PGS method is likely to be universally portable, due to the diversity of human populations and unequal information content of GWAS for different ancestries, Joint-Lassosum is an effective approach for enhancing portability and reducing predictive bias.",
      "journal": "bioRxiv : the preprint server for biology",
      "year": "2023",
      "doi": "10.1101/2023.09.22.559051",
      "authors": "Zhang Tianyu et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37790341/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10542523",
      "ft_text_length": 1701,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10542523)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37790442",
      "title": "Disparities in seizure outcomes revealed by large language models.",
      "abstract": "OBJECTIVE: Large-language models (LLMs) in healthcare have the potential to propagate existing biases or introduce new ones. For people with epilepsy, social determinants of health are associated with disparities in access to care, but their impact on seizure outcomes among those with access to specialty care remains unclear. Here we (1) evaluated our validated, epilepsy-specific LLM for intrinsic bias, and (2) used LLM-extracted seizure outcomes to test the hypothesis that different demographic groups have different seizure outcomes. METHODS: First, we tested our LLM for intrinsic bias in the form of differential performance in demographic groups by race, ethnicity, sex, income, and health insurance in manually annotated notes. Next, we used LLM-classified seizure freedom at each office visit to test for outcome disparities in the same demographic groups, using univariable and multivariable analyses. RESULTS: We analyzed 84,675 clinic visits from 25,612 patients seen at our epilepsy center 2005-2022. We found no differences in the accuracy, or positive or negative class balance of outcome classifications across demographic groups. Multivariable analysis indicated worse seizure outcomes for female patients (OR 1.33, p = 3\u00d710-8), those with public insurance (OR 1.53, p = 2\u00d710-13), and those from lower-income zip codes (OR \u2265 1.22, p \u2264 6.6\u00d710-3). Black patients had worse outcomes than White patients in univariable but not multivariable analysis (OR 1.03, p = 0.66). SIGNIFICANCE: We found no evidence that our LLM was intrinsically biased against any demographic group. Seizure freedom extracted by LLM revealed disparities in seizure outcomes across several demographic groups. These findings highlight the critical need to reduce disparities in the care of people with epilepsy.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2023",
      "doi": "10.1101/2023.09.20.23295842",
      "authors": "Xie Kevin et al.",
      "keywords": "Clinical Informatics; Electronic Health Record; Health Disparities; Natural Language Processing",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37790442/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10543059",
      "ft_text_length": 16516,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10543059)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37873343",
      "title": "BOLD: Blood-gas and Oximetry Linked Dataset - Open Source Research.",
      "abstract": "Pulse oximeters measure peripheral arterial oxygen saturation (SpO 2 ) noninvasively, while the gold standard (SaO 2 ) involves arterial blood gas measurement. There are known racial and ethnic disparities in their performance. BOLD is a new comprehensive dataset that aims to underscore the importance of addressing biases in pulse oximetry accuracy, which disproportionately affect darker-skinned patients. The dataset was created by harmonizing three Electronic Health Record databases (MIMIC-III, MIMIC-IV, eICU-CRD) comprising Intensive Care Unit stays of US patients. Paired SpO 2 and SaO 2 measurements were time-aligned and combined with various other sociodemographic and parameters to provide a detailed representation of each patient. BOLD includes 49,099 paired measurements, within a 5-minute window and with oxygen saturation levels between 70-100%. Minority racial and ethnic groups account for \u223c25% of the data - a proportion seldom achieved in previous studies. The codebase is publicly available. Given the prevalent use of pulse oximeters in the hospital and at home, we hope that BOLD will be leveraged to develop debiasing algorithms that can result in more equitable healthcare solutions.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2023",
      "doi": "10.1101/2023.10.03.23296485",
      "authors": "Matos Jo\u00e3o et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37873343/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10593048",
      "ft_text_length": 1231,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10593048)",
      "ft_reason": "Excluded: insufficient approach content (2 indicators)"
    },
    {
      "pmid": "37879386",
      "title": "Development of a prediction model of postpartum hospital use using an equity-focused approach.",
      "abstract": "BACKGROUND: Racial inequities in maternal morbidity and mortality persist into the postpartum period, leading to a higher rate of postpartum hospital use among Black and Hispanic people. Delivery hospitalizations provide an opportunity to screen and identify people at high risk to prevent adverse postpartum outcomes. Current models do not adequately incorporate social and structural determinants of health, and some include race, which may result in biased risk stratification. OBJECTIVE: This study aimed to develop a risk prediction model of postpartum hospital use while incorporating social and structural determinants of health and using an equity approach. STUDY DESIGN: We conducted a retrospective cohort study using 2016-2018 linked birth certificate and hospital discharge data for live-born infants in New York City. We included deliveries from 2016 to 2017 in model development, randomly assigning 70%/30% of deliveries as training/test data. We used deliveries in 2018 for temporal model validation. We defined \"Composite postpartum hospital use\" as at least 1 readmission or emergency department visit within 30 days of the delivery discharge. We categorized diagnosis at first hospital use into 14 categories based on International Classification of Diseases-Tenth Revision diagnosis codes. We tested 72 candidate variables, including social determinants of health, demographics, comorbidities, obstetrical complications, and severe maternal morbidity. Structural determinants of health were the Index of Concentration at the Extremes, which is an indicator of racial-economic segregation at the zip code level, and publicly available indices of the neighborhood built/natural and social/economic environment of the Child Opportunity Index. We used 4 statistical and machine learning algorithms to predict \"Composite postpartum hospital use\", and an ensemble approach to predict \"Cause-specific postpartum hospital use\". We simulated the impact of each risk stratification method paired with an effective intervention on race-ethnic equity in postpartum hospital use. RESULTS: The overall incidence of postpartum hospital use was 5.7%; the incidences among Black, Hispanic, and White people were 8.8%, 7.4%, and 3.3%, respectively. The most common diagnoses for hospital use were general perinatal complications (17.5%), hypertension/eclampsia (12.0%), nongynecologic infections (10.7%), and wound infections (8.4%). Logistic regression with least absolute shrinkage and selection operator selection retained 22 predictor variables and achieved an area under the receiver operating curve of 0.69 in the training, 0.69 in test, and 0.69 in validation data. Other machine learning algorithms performed similarly. Selected social and structural determinants of health features included the Index of Concentration at the Extremes, insurance payor, depressive symptoms, and trimester entering prenatal care. The \"Cause-specific postpartum hospital use\" model selected 6 of the 14 outcome diagnoses (acute cardiovascular disease, gastrointestinal disease, hypertension/eclampsia, psychiatric disease, sepsis, and wound infection), achieving an area under the receiver operating curve of 0.75 in training, 0.77 in test, and 0.75 in validation data using a cross-validation approach. Models had slightly lower performance in Black and Hispanic subgroups. When simulating use of the risk stratification models with a postpartum intervention, identifying high-risk individuals with the \"Composite postpartum hospital use\" model resulted in the greatest reduction in racial-ethnic disparities in postpartum hospital use, compared with the \"Cause-specific postpartum hospital use\" model or a standard approach to identifying high-risk individuals with common pregnancy complications. CONCLUSION: The \"Composite postpartum hospital use\" prediction model incorporating social and structural determinants of health can be used at delivery discharge to identify persons at risk for postpartum hospital use.",
      "journal": "American journal of obstetrics and gynecology",
      "year": "2024",
      "doi": "10.1016/j.ajog.2023.10.033",
      "authors": "Janevic Teresa et al.",
      "keywords": "birth; delivery; diabetes; disparities; emergency department; equity; ethnicity; hypertension; inequity; maternal morbidity; maternal mortality; postpartum; prediction; preeclampsia; race; readmission; social determinants of health; structural determinants of health",
      "mesh_terms": "Humans; Female; Retrospective Studies; Adult; Pregnancy; New York City; Postpartum Period; Social Determinants of Health; Hospitalization; Risk Assessment; Young Adult; Hispanic or Latino; Patient Readmission; Cohort Studies; Black or African American",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37879386/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11035486",
      "ft_text_length": 3410,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11035486)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37885556",
      "title": "Bias and Inaccuracy in AI Chatbot Ophthalmologist Recommendations.",
      "abstract": "PURPOSE AND DESIGN: To evaluate the accuracy and bias of ophthalmologist recommendations made by three AI chatbots, namely\u00a0ChatGPT 3.5 (OpenAI, San Francisco, CA, USA),\u00a0Bing Chat (Microsoft Corp., Redmond, WA, USA), and\u00a0Google Bard (Alphabet Inc., Mountain View, CA, USA). This study analyzed chatbot recommendations for the 20 most populous U.S. cities. METHODS: Each chatbot returned 80 total recommendations when given the prompt\u00a0\"Find me four good ophthalmologists in (city).\"\u00a0Characteristics of the physicians, including specialty, location, gender, practice type, and fellowship, were collected. A one-proportion z-test was performed to compare the proportion of female ophthalmologists recommended by each chatbot to the national average (27.2% per the\u00a0Association of American Medical Colleges\u00a0(AAMC)). Pearson's chi-squared test was performed to determine differences between the three chatbots in male versus female recommendations and recommendation accuracy. RESULTS: Female ophthalmologists recommended by Bing Chat (1.61%) and Bard (8.0%) were significantly less than the national proportion of 27.2% practicing female ophthalmologists (p<0.001, p<0.01, respectively). ChatGPT recommended fewer female (29.5%) than male ophthalmologists (p<0.722). ChatGPT (73.8%), Bing Chat (67.5%), and Bard (62.5%) gave high rates of inaccurate recommendations. Compared to the national average of academic ophthalmologists (17%), the proportion of recommended ophthalmologists in academic medicine or in combined academic and private practice was significantly greater for all three chatbots. CONCLUSION: This study revealed substantial bias and inaccuracy in the AI chatbots' recommendations. They struggled to recommend ophthalmologists reliably and accurately, with most recommendations being physicians in specialties other than ophthalmology or not in or near the desired city. Bing Chat and Google Bard showed a significant tendency against recommending female ophthalmologists, and all chatbots favored recommending ophthalmologists in academic medicine.",
      "journal": "Cureus",
      "year": "2023",
      "doi": "10.7759/cureus.45911",
      "authors": "Oca Michael C et al.",
      "keywords": "ai chatbot; artificial intelligence (ai) in medicine; artificial intelligence in health care; artificial intelligence in medicine; gender bias; patient education",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37885556/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10599183",
      "ft_text_length": 21145,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10599183)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37904073",
      "title": "Chatting Beyond ChatGPT: Advancing Equity Through AI-Driven Language Interpretation.",
      "abstract": "Medical interpretation is an underutilized resource, despite its legal mandate and proven efficacy in improving health outcomes for populations with low English proficiency. This disconnect can often be attributed to the costs and wait-times associated with traditional means of interpretation, making the service inaccessible and burdensome. Technology has improved access to translation through phone and video interpretation; with the acceleration of artificial intelligence (AI) large language models, we have an opportunity to further improve interpreter access through real-time, automated translation. The impetus to utilize this burgeoning tool for improved health equity must be combined with a critical view of the safety, privacy, and clinical decision-making risks involved. Physicians must be active participants and collaborators in both the mobilization of AI tools to improve clinical care and the development of regulations to mitigate harm.",
      "journal": "Journal of general internal medicine",
      "year": "2024",
      "doi": "10.1007/s11606-023-08497-6",
      "authors": "Bakdash Leen et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Allied Health Personnel; Clinical Decision-Making; Health Equity; Language",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37904073/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10897100",
      "ft_text_length": 958,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10897100)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37904983",
      "title": "Machine Learning Strategies for Improved Phenotype Prediction in Underrepresented Populations.",
      "abstract": "Precision medicine models often perform better for populations of European ancestry due to the over-representation of this group in the genomic datasets and large-scale biobanks from which the models are constructed. As a result, prediction models may misrepresent or provide less accurate treatment recommendations for underrepresented populations, contributing to health disparities. This study introduces an adaptable machine learning toolkit that integrates multiple existing methodologies and novel techniques to enhance the prediction accuracy for underrepresented populations in genomic datasets. By leveraging machine learning techniques, including gradient boosting and automated methods, coupled with novel population-conditional re-sampling techniques, our method significantly improves the phenotypic prediction from single nucleotide polymorphism (SNP) data for diverse populations. We evaluate our approach using the UK Biobank, which is composed primarily of British individuals with European ancestry, and a minority representation of groups with Asian and African ancestry. Performance metrics demonstrate substantial improvements in phenotype prediction for underrepresented groups, achieving prediction accuracy comparable to that of the majority group. This approach represents a significant step towards improving prediction accuracy amidst current dataset diversity challenges. By integrating a tailored pipeline, our approach fosters more equitable validity and utility of statistical genetics methods, paving the way for more inclusive models and outcomes.",
      "journal": "bioRxiv : the preprint server for biology",
      "year": "2023",
      "doi": "10.1101/2023.10.12.561949",
      "authors": "Bonet David et al.",
      "keywords": "Bioinformatics; Genetics; Machine Learning; Phenotype Prediction; Precision Medicine",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37904983/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10614800",
      "ft_text_length": 25665,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10614800)",
      "ft_reason": "Excluded: insufficient approach content (1 indicators)"
    },
    {
      "pmid": "37917120",
      "title": "The Accuracy and Potential Racial and Ethnic Biases of GPT-4 in the Diagnosis and Triage of Health Conditions: Evaluation Study.",
      "abstract": "BACKGROUND: Whether GPT-4, the conversational artificial intelligence, can accurately diagnose and triage health conditions and whether it presents racial and ethnic biases in its decisions remain unclear. OBJECTIVE: We aim to assess the accuracy of GPT-4 in the diagnosis and triage of health conditions and whether its performance varies by patient race and ethnicity. METHODS: We compared the performance of GPT-4 and physicians, using 45 typical clinical vignettes, each with a correct diagnosis and triage level, in February and March 2023. For each of the 45 clinical vignettes, GPT-4 and 3 board-certified physicians provided the most likely primary diagnosis and triage level (emergency, nonemergency, or self-care). Independent reviewers evaluated the diagnoses as \"correct\" or \"incorrect.\" Physician diagnosis was defined as the consensus of the 3 physicians. We evaluated whether the performance of GPT-4 varies by patient race and ethnicity, by adding the information on patient race and ethnicity to the clinical vignettes. RESULTS: The accuracy of diagnosis was comparable between GPT-4 and physicians (the percentage of correct diagnosis was 97.8% (44/45; 95% CI 88.2%-99.9%) for GPT-4 and 91.1% (41/45; 95% CI 78.8%-97.5%) for physicians; P=.38). GPT-4 provided appropriate reasoning for 97.8% (44/45) of the vignettes. The appropriateness of triage was comparable between GPT-4 and physicians (GPT-4: 30/45, 66.7%; 95% CI 51.0%-80.0%; physicians: 30/45, 66.7%; 95% CI 51.0%-80.0%; P=.99). The performance of GPT-4 in diagnosing health conditions did not vary among different races and ethnicities (Black, White, Asian, and Hispanic), with an accuracy of 100% (95% CI 78.2%-100%). P values, compared to the GPT-4 output without incorporating race and ethnicity information, were all .99. The accuracy of triage was not significantly different even if patients' race and ethnicity information was added. The accuracy of triage was 62.2% (95% CI 46.5%-76.2%; P=.50) for Black patients; 66.7% (95% CI 51.0%-80.0%; P=.99) for White patients; 66.7% (95% CI 51.0%-80.0%; P=.99) for Asian patients, and 62.2% (95% CI 46.5%-76.2%; P=.69) for Hispanic patients. P values were calculated by comparing the outputs with and without conditioning on race and ethnicity. CONCLUSIONS: GPT-4's ability to diagnose and triage typical clinical vignettes was comparable to that of board-certified physicians. The performance of GPT-4 did not vary by patient race and ethnicity. These findings should be informative for health systems looking to introduce conversational artificial intelligence to improve the efficiency of patient diagnosis and triage.",
      "journal": "JMIR medical education",
      "year": "2023",
      "doi": "10.2196/47532",
      "authors": "Ito Naoki et al.",
      "keywords": "AI; GPT; GPT-4; artificial intelligence; bias; clinical vignettes; decision-making; diagnosis; efficiency; physician; race; racial and ethnic bias; triage; typical clinical vignettes",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37917120/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10654908",
      "ft_text_length": 24644,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10654908)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37963683",
      "title": "Algorithmic fairness in cardiovascular disease risk prediction: overcoming inequalities.",
      "abstract": "The main purpose of prognostic risk prediction models is to identify individuals who are at risk of disease, to enable early intervention. Current prognostic cardiovascular risk prediction models, such as the Systematic COronary Risk Evaluation (SCORE2) and the SCORE2-Older Persons (SCORE2-OP) models, which represent the clinically used gold standard in assessing patient risk for major cardiovascular events in the European Union (EU), generally overlook socioeconomic determinants, leading to disparities in risk prediction and resource allocation. A central recommendation of this article is the explicit inclusion of individual-level socioeconomic determinants of cardiovascular disease in risk prediction models. The question of whether prognostic risk prediction models can promote health equity remains to be answered through experimental research, potential clinical implementation and public health analysis. This paper introduces four distinct fairness concepts in cardiovascular disease prediction and their potential to narrow existing disparities in cardiometabolic health.",
      "journal": "Open heart",
      "year": "2023",
      "doi": "10.1136/openhrt-2023-002395",
      "authors": "Varga Tibor V",
      "keywords": "biomarkers; delivery of health care; epidemiology; ethics, medical",
      "mesh_terms": "Humans; Aged; Aged, 80 and over; Health Promotion; Cardiovascular Diseases; Socioeconomic Factors; Prognosis",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37963683/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10649900",
      "ft_text_length": 14683,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10649900)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37998503",
      "title": "Percutaneous Coronary Intervention Mortality, Cost, Complications, and Disparities after Radiation Therapy: Artificial Intelligence-Augmented, Cost Effectiveness, and Computational Ethical Analysis.",
      "abstract": "The optimal cardio-oncology management of radiation therapy and its complications are unknown despite the high patient and societal costs. This study is the first known nationally representative, multi-year, artificial intelligence and propensity score-augmented causal clinical inference and computational ethical and policy analysis of percutaneous coronary intervention (PCI) mortality, cost, and disparities including by primary malignancy following radiation therapy. Bayesian Machine learning-augmented Propensity Score translational (BAM-PS) statistics were conducted in the 2016-2020 National Inpatient Sample. Of the 148,755,036 adult hospitalizations, 2,229,285 (1.50%) had a history of radiation therapy, of whom, 67,450 (3.00%) had an inpatient AMI, and of whom, 18,400 (28.69%) underwent PCI. Post-AMI mortality, costs, and complications were comparable with and without radiation across cancers in general and across the 30 primary malignancies tested, except for breast cancer, in which PCI significantly increased mortality (OR 3.70, 95%CI 1.10-12.43, p = 0.035). In addition to significant sex, race, and insurance disparities, significant regional disparities were associated with nearly 50 extra inpatient deaths and over USD 500 million lost. This large clinical, cost, and pluralistic ethical analysis suggests PCI when clinically indicated should be provided to patients regardless of sex, race, insurance, or region to generate significant improvements in population health, cost savings, and social equity.",
      "journal": "Journal of cardiovascular development and disease",
      "year": "2023",
      "doi": "10.3390/jcdd10110445",
      "authors": "Monlezun Dominique J",
      "keywords": "PCI; artificial intelligence; cardio-oncology; cost; equity; ethics; radiation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37998503/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10672341",
      "ft_text_length": 35254,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10672341)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38020160",
      "title": "Artificial intelligence in global health equity: an evaluation and discussion on the application of ChatGPT, in the Chinese National Medical Licensing Examination.",
      "abstract": "BACKGROUND: The demand for healthcare is increasing globally, with notable disparities in access to resources, especially in Asia, Africa, and Latin America. The rapid development of Artificial Intelligence (AI) technologies, such as OpenAI's ChatGPT, has shown promise in revolutionizing healthcare. However, potential challenges, including the need for specialized medical training, privacy concerns, and language bias, require attention. METHODS: To assess the applicability and limitations of ChatGPT in Chinese and English settings, we designed an experiment evaluating its performance in the 2022 National Medical Licensing Examination (NMLE) in China. For a standardized evaluation, we used the comprehensive written part of the NMLE, translated into English by a bilingual expert. All questions were input into ChatGPT, which provided answers and reasons for choosing them. Responses were evaluated for \"information quality\" using the Likert scale. RESULTS: ChatGPT demonstrated a correct response rate of 81.25% for Chinese and 86.25% for English questions. Logistic regression analysis showed that neither the difficulty nor the subject matter of the questions was a significant factor in AI errors. The Brier Scores, indicating predictive accuracy, were 0.19 for Chinese and 0.14 for English, indicating good predictive performance. The average quality score for English responses was excellent (4.43 point), slightly higher than for Chinese (4.34 point). CONCLUSION: While AI language models like ChatGPT show promise for global healthcare, language bias is a key challenge. Ensuring that such technologies are robustly trained and sensitive to multiple languages and cultures is vital. Further research into AI's role in healthcare, particularly in areas with limited resources, is warranted.",
      "journal": "Frontiers in medicine",
      "year": "2023",
      "doi": "10.3389/fmed.2023.1237432",
      "authors": "Tong Wenting et al.",
      "keywords": "ChatGPT; artificial intelligence; equity; global healthcare; language bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38020160/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10656681",
      "ft_text_length": 23496,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10656681)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38031481",
      "title": "Translating ethical and quality principles for the effective, safe and fair development, deployment and use of artificial intelligence technologies in healthcare.",
      "abstract": "OBJECTIVE: The complexity and rapid pace of development of algorithmic technologies pose challenges for their regulation and oversight in healthcare settings. We sought to improve our institution's approach to evaluation and governance of algorithmic technologies used in clinical care and operations by creating an Implementation Guide that standardizes evaluation criteria so that local oversight is performed in an objective fashion. MATERIALS AND METHODS: Building on a framework that applies key ethical and quality principles (clinical value and safety, fairness and equity, usability and adoption, transparency and accountability, and regulatory compliance), we created concrete guidelines for evaluating algorithmic technologies at our institution. RESULTS: An Implementation Guide articulates evaluation criteria used during review of algorithmic technologies and details what evidence supports the implementation of ethical and quality principles for trustworthy health AI. Application of the processes described in the Implementation Guide can lead to algorithms that are safer as well as more effective, fair, and equitable upon implementation, as illustrated through 4 examples of technologies at different phases of the algorithmic lifecycle that underwent evaluation at our academic medical center. DISCUSSION: By providing clear descriptions/definitions of evaluation criteria and embedding them within standardized processes, we streamlined oversight processes and educated communities using and developing algorithmic technologies within our institution. CONCLUSIONS: We developed a scalable, adaptable framework for translating principles into evaluation criteria and specific requirements that support trustworthy implementation of algorithmic technologies in patient care and healthcare operations.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2024",
      "doi": "10.1093/jamia/ocad221",
      "authors": "Economou-Zavlanos Nicoleta J et al.",
      "keywords": "algorithms; artificial intelligence; health equity; quality assurance healthcare; technology assessment",
      "mesh_terms": "Humans; Artificial Intelligence; Health Facilities; Algorithms; Academic Medical Centers; Patient Compliance",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38031481/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10873841",
      "ft_text_length": 1823,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10873841)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38043883",
      "title": "Efficient adversarial debiasing with concept activation vector - Medical image case-studies.",
      "abstract": "BACKGROUND: A major hurdle for the real time deployment of the AI models is ensuring trustworthiness of these models for the unseen population. More often than not, these complex models are black boxes in which promising results are generated. However, when scrutinized, these models begin to reveal implicit biases during the decision making, particularly for the minority subgroups. METHOD: We develop an efficient adversarial de-biasing approach with partial learning by incorporating the existing concept activation vectors (CAV) methodology, to reduce racial disparities while preserving the performance of the targeted task. CAV is originally a model interpretability technique which we adopted to identify convolution layers responsible for learning race and only fine-tune up to that layer instead of fine-tuning the complete network, limiting the drop in performance RESULTS:: The methodology has been evaluated on two independent medical image case-studies - chest X-ray and mammograms, and we also performed external validation on a different racial population. On the external datasets for the chest X-ray use-case, debiased models (averaged AUC 0.87 ) outperformed the baseline convolution models (averaged AUC 0.57 ) as well as the models trained with the popular fine-tuning strategy (averaged AUC 0.81). Moreover, the mammogram models is debiased using a single dataset (white, black and Asian) and improved the performance on an external datasets (averaged AUC 0.8 to 0.86 ) with completely different population (primarily Hispanic patients). CONCLUSION: In this study, we demonstrated that the adversarial models trained only with internal data performed equally or often outperformed the standard fine-tuning strategy with data from an external setting. The adversarial training approach described can be applied regardless of predictor's model architecture, as long as the convolution model is trained using a gradient-based method. We release the training code with academic open-source license - https://github.com/ramon349/JBI2023_TCAV_debiasing.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2023.104548",
      "authors": "Correa Ramon et al.",
      "keywords": "Adversarial fairness; Concept activation vector; Debiasing; Mammogram images; X-ray images",
      "mesh_terms": "Humans; Mammography; Minority Groups; Artificial Intelligence; Clinical Decision-Making; Bias; Healthcare Disparities; Racial Groups; Diagnostic Imaging",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38043883/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11192465",
      "ft_text_length": 2074,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11192465)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "38062039",
      "title": "Accelerating African neuroscience to provide an equitable framework using perspectives from West and Southern Africa.",
      "abstract": "Drawing on perspectives from West and Southern Africa, this Comment critically examines the current state of neuroscience progress in Africa, describing the unique landscape and ongoing challenges as embedded within wider socio-political realities. Distinct research opportunities in the African context are explored to include genetic and bio-diversity, multilingual and multicultural populations, life-course development, clinical neuroscience and neuropsychology, with applications to machine learning models, in light of complex post-colonial legacies that often impede research progress. Key determinants needed to accelerate African neuroscience are then discussed, as well as cautionary underpinnings that together create an equitable neuroscience framework.",
      "journal": "Nature communications",
      "year": "2023",
      "doi": "10.1038/s41467-023-43943-3",
      "authors": "Besharati Sahba et al.",
      "keywords": "",
      "mesh_terms": "Africa, Southern; Africa, Western; Neurosciences",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38062039/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10703764",
      "ft_text_length": 12895,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10703764)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38064003",
      "title": "Development of a Novel Telemedicine Tool to Reduce Disparities Related to the Identification of Preschool Children with Autism.",
      "abstract": "The wait for ASD evaluation dramatically increases with age, with wait times of a year or more common as children reach preschool. Even when appointments become available, families from traditionally underserved groups struggle to access care. Addressing care disparities requires designing identification tools and processes specifically for and with individuals most at-risk for health inequities. This work describes the development of a novel telemedicine-based ASD assessment tool, the TELE-ASD-PEDS-Preschool (TAP-Preschool). We applied machine learning models to a clinical data set of preschoolers with ASD and other developmental concerns (n\u2009=\u2009914) to generate behavioral targets that best distinguish ASD and non-ASD features. We conducted focus groups with clinicians, early interventionists, and parents of children with ASD from traditionally underrepresented racial/ethnic and linguistic groups. Focus group themes and machine learning analyses were used to generate a play-based instrument with assessment tasks and scoring procedures based on the child's language (i.e., TAP-P Verbal, TAP-P Non-verbal). TAP-P procedures were piloted with 30 families. Use of the instrument in isolation (i.e., without history or collateral information) yielded accurate diagnostic classification in 63% of cases. Children with existing ASD diagnoses received higher TAP-P scores, relative to children with other developmental concerns. Clinician diagnostic accuracy and certainty were higher when confirming existing ASD diagnoses (80% agreement) than when ruling out ASD in children with other developmental concerns (30% agreement). Utilizing an equity approach to understand the functionality and impact of tele-assessment for preschool children has potential to transform the ASD evaluation process and improve care access.",
      "journal": "Journal of autism and developmental disorders",
      "year": "2025",
      "doi": "10.1007/s10803-023-06176-3",
      "authors": "Wagner Liliana et al.",
      "keywords": "Autism spectrum disorder; Preschool children; Tele-assessment; Telemedicine",
      "mesh_terms": "Humans; Child, Preschool; Telemedicine; Male; Female; Healthcare Disparities; Autism Spectrum Disorder; Focus Groups; Machine Learning; Parents",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38064003/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11161552",
      "ft_text_length": 1827,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11161552)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38070817",
      "title": "Trans-Balance: Reducing demographic disparity for prediction models in the presence of class imbalance.",
      "abstract": "INTRODUCTION: Risk prediction, including early disease detection, prevention, and intervention, is essential to precision medicine. However, systematic bias in risk estimation caused by heterogeneity across different demographic groups can lead to inappropriate or misinformed treatment decisions. In addition, low incidence (class-imbalance) outcomes negatively impact the classification performance of many standard learning algorithms which further exacerbates the racial disparity issues. Therefore, it is crucial to improve the performance of statistical and machine learning models in underrepresented populations in the presence of heavy class imbalance. METHOD: To address demographic disparity in the presence of class imbalance, we develop a novel framework, Trans-Balance, by leveraging recent advances in imbalance learning, transfer learning, and federated learning. We consider a practical setting where data from multiple sites are stored locally under privacy constraints. RESULTS: We show that the proposed Trans-Balance framework improves upon existing approaches by explicitly accounting for heterogeneity across demographic subgroups and cohorts. We demonstrate the feasibility and validity of our methods through numerical experiments and a real application to a multi-cohort study with data from participants of four large, NIH-funded cohorts for stroke risk prediction. CONCLUSION: Our findings indicate that the Trans-Balance approach significantly improves predictive performance, especially in scenarios marked by severe class imbalance and demographic disparity. Given its versatility and effectiveness, Trans-Balance offers a valuable contribution to enhancing risk prediction in biomedical research and related fields.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2023.104532",
      "authors": "Hong Chuan et al.",
      "keywords": "Class imbalance; Demographic disparity; Federated learning; Model fairness; Predictive modeling; Transfer learning",
      "mesh_terms": "Humans; Cohort Studies; Algorithms; Machine Learning; Biomedical Research; Demography",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38070817/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10850917",
      "ft_text_length": 1751,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10850917)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38073717",
      "title": "Allyship to Advance Diversity, Equity, and Inclusion in Otolaryngology: What We Can All Do.",
      "abstract": "PURPOSE OF REVIEW: To summarize the current literature on allyship, providing a historical perspective, concept analysis, and practical steps to advance equity, diversity, and inclusion. This review also provides evidence-based tools to foster allyship and identifies potential pitfalls. RECENT FINDINGS: Allies in healthcare advocate for inclusive and equitable practices that benefit patients, coworkers, and learners. Allyship requires working in solidarity with individuals from underrepresented or historically marginalized groups to promote a sense of belonging and opportunity. New technologies present possibilities and perils in paving the pathway to diversity. SUMMARY: Unlocking the power of allyship requires that allies confront unconscious biases, engage in self-reflection, and act as effective partners. Using an allyship toolbox, allies can foster psychological safety in personal and professional spaces while avoiding missteps. Allyship incorporates goals, metrics, and transparent data reporting to promote accountability and to sustain improvements. Implementing these allyship strategies in solidarity holds promise for increasing diversity and inclusion in the specialty.",
      "journal": "Current otorhinolaryngology reports",
      "year": "2023",
      "doi": "10.1007/s40136-023-00467-0",
      "authors": "Balakrishnan Karthik et al.",
      "keywords": "Allyship; Artificial intelligence; Diversity equity, and inclusion; Healthcare disparities; Mentorship, coaching, and sponsorship; Psychological safety",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38073717/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10707492",
      "ft_text_length": 1191,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10707492)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "38074628",
      "title": "Deep learning based tomosynthesis denoising: a bias investigation across different breast types.",
      "abstract": "PURPOSE: High noise levels due to low X-ray dose are a challenge in digital breast tomosynthesis (DBT) reconstruction. Deep learning algorithms show promise in reducing this noise. However, these algorithms can be complex and biased toward certain patient groups if the training data are not representative. It is important to thoroughly evaluate deep learning-based denoising algorithms before they are applied in the medical field to ensure their effectiveness and fairness. In this work, we present a deep learning-based denoising algorithm and examine potential biases with respect to breast density, thickness, and noise level. APPROACH: We use physics-driven data augmentation to generate low-dose images from full field digital mammography and train an encoder-decoder network. The rectified linear unit (ReLU)-loss, specifically designed for mammographic denoising, is utilized as the objective function. To evaluate our algorithm for potential biases, we tested it on both clinical and simulated data generated with the virtual imaging clinical trial for regulatory evaluation pipeline. Simulated data allowed us to generate X-ray dose distributions not present in clinical data, enabling us to separate the influence of breast types and X-ray dose on the denoising performance. RESULTS: Our results show that the denoising performance is proportional to the noise level. We found a bias toward certain breast groups on simulated data; however, on clinical data, our algorithm denoises different breast types equally well with respect to structural similarity index. CONCLUSIONS: We propose a robust deep learning-based denoising algorithm that reduces DBT projection noise levels and subject it to an extensive test that provides information about its strengths and weaknesses.",
      "journal": "Journal of medical imaging (Bellingham, Wash.)",
      "year": "2023",
      "doi": "10.1117/1.JMI.10.6.064003",
      "authors": "Eckert Dominik et al.",
      "keywords": "deep learning; denoising; mammography; noise simulation; tomosynthesis",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38074628/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10704268",
      "ft_text_length": 1793,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10704268)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38076222",
      "title": "Race Correction and Algorithmic Bias in Atrial Fibrillation Wearable Technologies.",
      "abstract": "Stakeholders in biomedicine are evaluating how race corrections in clinical algorithms inequitably allocate health care resources on the basis of a misunderstanding of race-as-genetic difference. Ostensibly used to intervene on persistent disparities in health outcomes across different racial groups, these troubling corrections in risk assessments embed essentialist ideas of race as a biological reality, rather than a social and political construct that reproduces a racial hierarchy, into practice guidelines. This article explores the harms of such race corrections by considering how the technologies we use to account for disparities in health outcomes can actually innovate and amplify these harms. Focusing on the design of wearable digital health technologies that use photoplethysmographic sensors to detect atrial fibrillation, we argue that these devices, which are notoriously poor in accurately functioning on users with darker skin tones, embed a subtle form of race correction that presupposes the need for explicit adjustments in the clinical interpretation of their data outputs. We point to research on responsible innovation in health, and its commitment to being responsive in addressing inequities and harms, as a way forward for those invested in the elimination of race correction.",
      "journal": "Health equity",
      "year": "2023",
      "doi": "10.1089/heq.2023.0034",
      "authors": "Merid Beza et al.",
      "keywords": "African American; cardiovascular health; health disparities; technology",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38076222/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10698766",
      "ft_text_length": 26879,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10698766)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38106815",
      "title": "Validating racial and ethnic non-bias of artificial intelligence decision support for diagnostic breast ultrasound evaluation.",
      "abstract": "PURPOSE: Breast ultrasound suffers from low positive predictive value and specificity. Artificial intelligence (AI) proposes to improve accuracy, reduce false negatives, reduce inter- and intra-observer variability and decrease the rate of benign biopsies. Perpetuating racial/ethnic disparities in healthcare and patient outcome is a potential risk when incorporating AI-based models into clinical practice; therefore, it is necessary to validate its non-bias before clinical use. APPROACH: Our retrospective review assesses whether our AI decision support (DS) system demonstrates racial/ethnic bias by evaluating its performance on 1810 biopsy proven cases from nine breast imaging facilities within our health system from January 1, 2018 to October 28, 2021. Patient age, gender, race/ethnicity, AI DS output, and pathology results were obtained. RESULTS: Significant differences in breast pathology incidence were seen across different racial and ethnic groups. Stratified analysis showed that the difference in output by our AI DS system was due to underlying differences in pathology incidence for our specific cohort and did not demonstrate statistically significant bias in output among race/ethnic groups, suggesting similar effectiveness of our AI DS system among different races (p>0.05 for all). CONCLUSIONS: Our study shows promise that an AI DS system may serve as a valuable second opinion in the detection of breast cancer on diagnostic ultrasound without significant racial or ethnic bias. AI tools are not meant to replace the radiologist, but rather to aid in screening and diagnosis without perpetuating racial/ethnic disparities.",
      "journal": "Journal of medical imaging (Bellingham, Wash.)",
      "year": "2023",
      "doi": "10.1117/1.JMI.10.6.061108",
      "authors": "Koo Clara et al.",
      "keywords": "artificial intelligence; breast cancer; breast ultrasound; deep learning; racial and ethnicity bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38106815/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10721939",
      "ft_text_length": 1661,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10721939)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38123951",
      "title": "Understanding Bias in Artificial Intelligence: A Practice Perspective.",
      "abstract": "In the fall of 2021, several experts in this space delivered a Webinar hosted by the American Society of Neuroradiology (ASNR) Diversity and Inclusion Committee, focused on expanding the understanding of bias in artificial intelligence, with a health equity lens, and provided key concepts for neuroradiologists to approach the evaluation of these tools. In this perspective, we distill key parts of this discussion, including understanding why this topic is important to neuroradiologists and lending insight on how neuroradiologists can develop a framework to assess health equity-related bias in artificial intelligence tools. In addition, we provide examples of clinical workflow implementation of these tools so that we can begin to see how artificial intelligence tools will impact discourse on equitable radiologic care. As continuous learners, we must be engaged in new and rapidly evolving technologies that emerge in our field. The Diversity and Inclusion Committee of the ASNR has addressed this subject matter through its programming content revolving around health equity in neuroradiologic advances.",
      "journal": "AJNR. American journal of neuroradiology",
      "year": "2024",
      "doi": "10.3174/ajnr.A8070",
      "authors": "Davis Melissa A et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Radiologists; Radiology; Workflow",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38123951/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11288570",
      "ft_text_length": 1122,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11288570)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38126168",
      "title": "Race, Sex, and Age Disparities in the Performance of ECG Deep Learning Models Predicting Heart Failure.",
      "abstract": "BACKGROUND: Deep learning models may combat widening racial disparities in heart failure outcomes through early identification of individuals at high risk. However, demographic biases in the performance of these models have not been well-studied. METHODS: This retrospective analysis used 12-lead ECGs taken between 2008 and 2018 from 326\u2005518 patient encounters referred for standard clinical indications to Stanford Hospital. The primary model was a convolutional neural network model trained to predict incident heart failure within 5 years. Biases were evaluated on the testing set (160\u2005312 ECGs) using the area under the receiver operating characteristic curve, stratified across the protected attributes of race, ethnicity, age, and sex. RESULTS: There were 59 817 cases of incident heart failure observed within 5 years of ECG collection. The performance of the primary model declined with age. There were no significant differences observed between racial groups overall. However, the primary model performed significantly worse in Black patients aged 0 to 40 years compared with all other racial groups in this age group, with differences most pronounced among young Black women. Disparities in model performance did not improve with the integration of race, ethnicity, sex, and age into model architecture, by training separate models for each racial group, or by providing the model with a data set of equal racial representation. Using probability thresholds individualized for race, age, and sex offered substantial improvements in F1 scores. CONCLUSIONS: The biases found in this study warrant caution against perpetuating disparities through the development of machine learning tools for the prognosis and management of heart failure. Customizing the application of these models by using probability thresholds individualized by race, ethnicity, age, and sex may offer an avenue to mitigate existing algorithmic disparities.",
      "journal": "Circulation. Heart failure",
      "year": "2024",
      "doi": "10.1161/CIRCHEARTFAILURE.123.010879",
      "authors": "Kaur Dhamanpreet et al.",
      "keywords": "deep learning; electrocardiography; healthcare disparities; heart failure",
      "mesh_terms": "Humans; Female; Heart Failure; Retrospective Studies; Deep Learning; Ethnicity; Electrocardiography",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38126168/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10984643",
      "ft_text_length": 1913,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10984643)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38160295",
      "title": "Machine Learning Strategies for Improved Phenotype Prediction in Underrepresented Populations.",
      "abstract": "Precision medicine models often perform better for populations of European ancestry due to the over-representation of this group in the genomic datasets and large-scale biobanks from which the models are constructed. As a result, prediction models may misrepresent or provide less accurate treatment recommendations for underrepresented populations, contributing to health disparities. This study introduces an adaptable machine learning toolkit that integrates multiple existing methodologies and novel techniques to enhance the prediction accuracy for underrepresented populations in genomic datasets. By leveraging machine learning techniques, including gradient boosting and automated methods, coupled with novel population-conditional re-sampling techniques, our method significantly improves the phenotypic prediction from single nucleotide polymorphism (SNP) data for diverse populations. We evaluate our approach using the UK Biobank, which is composed primarily of British individuals with European ancestry, and a minority representation of groups with Asian and African ancestry. Performance metrics demonstrate substantial improvements in phenotype prediction for underrepresented groups, achieving prediction accuracy comparable to that of the majority group. This approach represents a significant step towards improving prediction accuracy amidst current dataset diversity challenges. By integrating a tailored pipeline, our approach fosters more equitable validity and utility of statistical genetics methods, paving the way for more inclusive models and outcomes.",
      "journal": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing",
      "year": "2024",
      "doi": "",
      "authors": "Bonet David et al.",
      "keywords": "",
      "mesh_terms": "Humans; Computational Biology; Machine Learning; Minority Groups; Phenotype; White People; UK Biobank",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38160295/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10799683",
      "ft_text_length": 25663,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10799683)",
      "ft_reason": "Excluded: insufficient approach content (1 indicators)"
    },
    {
      "pmid": "38165430",
      "title": "Reader bias in breast cancer screening related to cancer prevalence and artificial intelligence decision support-a reader study.",
      "abstract": "OBJECTIVES: The aim of our study was to examine how breast radiologists would be affected by high cancer prevalence and the use of artificial intelligence (AI) for decision support. MATERIALS AND METHOD: This reader study was based on selection of screening mammograms, including the original radiologist assessment, acquired in 2010 to 2013 at the Karolinska University Hospital, with a ratio of 1:1 cancer versus healthy based on a 2-year follow-up. A commercial AI system generated an exam-level positive or negative read, and image markers. Double-reading and consensus discussions were first performed without AI and later with AI, with a 6-week wash-out period in between. The chi-squared test was used to test for differences in contingency tables. RESULTS: Mammograms of 758 women were included, half with cancer and half healthy. 52% were 40-55\u00a0years; 48% were 56-75\u00a0years. In the original non-enriched screening setting, the sensitivity was 61% (232/379) at specificity 98% (323/379). In the reader study, the sensitivity without and with AI was 81% (307/379) and 75% (284/379) respectively (p\u2009<\u20090.001). The specificity without and with AI was 67% (255/379) and 86% (326/379) respectively (p\u2009<\u20090.001). The tendency to change assessment from positive to negative based on erroneous AI information differed between readers and was affected by type and number of image signs of malignancy. CONCLUSION: Breast radiologists reading a list with high cancer prevalence performed at considerably higher sensitivity and lower specificity than the original screen-readers. Adding AI information, calibrated to a screening setting, decreased sensitivity and increased specificity. CLINICAL RELEVANCE STATEMENT: Radiologist screening mammography assessments will be biased towards higher sensitivity and lower specificity by high-risk triaging and nudged towards the sensitivity and specificity setting of AI reads. After AI implementation in clinical practice, there is reason to carefully follow screening metrics to ensure the impact is desired. KEY POINTS: \u2022 Breast radiologists' sensitivity and specificity will be affected by changes brought by artificial intelligence. \u2022 Reading in a high cancer prevalence setting markedly increased sensitivity and decreased specificity. \u2022 Reviewing the binary reads by AI, negative or positive, biased screening radiologists towards the sensitivity and specificity of the AI system.",
      "journal": "European radiology",
      "year": "2024",
      "doi": "10.1007/s00330-023-10514-5",
      "authors": "Al-Bazzaz Hanen et al.",
      "keywords": "Artificial intelligence; Bias; Breast; Cancer screening; Mammography",
      "mesh_terms": "Humans; Female; Breast Neoplasms; Artificial Intelligence; Middle Aged; Mammography; Aged; Adult; Prevalence; Early Detection of Cancer; Sensitivity and Specificity; Observer Variation; Decision Support Systems, Clinical",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38165430/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11255031",
      "ft_text_length": 25317,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11255031)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38222354",
      "title": "Real-world Application of Racial and Ethnic Imputation and Cohort Balancing Techniques to Deliver Equitable Clinical Trial Recruitment.",
      "abstract": "Enhancing diversity and inclusion in clinical trial recruitment, especially for historically marginalized populations including Black, Indigenous, and People of Color individuals, is essential. This practice ensures that generalizable trial results are achieved to deliver safe, effective, and equitable health and healthcare. However, recruitment is limited by two inextricably linked barriers - the inability to recruit and retain enough trial participants, and the lack of diversity amongst trial populations whereby racial and ethnic groups are underrepresented when compared to national composition. To overcome these barriers, this study describes and evaluates a framework that combines 1) probabilistic and machine learning models to accurately impute missing race and ethnicity fields in real-world data including medical and pharmacy claims for the identification of eligible trial participants, 2) randomized controlled trial experimentation to deliver an optimal patient outreach strategy, and 3) stratified sampling techniques to effectively balance cohorts to continuously improve engagement and recruitment metrics.",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "year": "2023",
      "doi": "",
      "authors": "Craig Kelly J et al.",
      "keywords": "",
      "mesh_terms": "Humans; Patient Selection; Ethnicity; Research Design; Minority Groups",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38222354/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10785904",
      "ft_text_length": 1130,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10785904)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38276870",
      "title": "Real-world Trends, Rural-urban Differences, and Socioeconomic Disparities in Utilization of Narrow versus Broad Next-generation Sequencing Panels.",
      "abstract": "UNLABELLED: Advances in genetic technology have led to the increasing use of genomic panels in precision oncology practice, with panels ranging from a couple to hundreds of genes. However, the clinical utilization and utility of oncology genomic panels, especially among vulnerable populations, is unclear. We examined the association of panel size with socioeconomic status and clinical trial matching. We retrospectively identified 9,886 eligible adult subjects in the Mayo Clinic Health System who underwent genomic testing between January 1, 2016 and June 30, 2020. Patient data were retrieved from structured and unstructured data sources of institutional collections, including cancer registries, clinical data warehouses, and clinical notes. Socioeconomic surrogates were approximated using the Area Deprivation Index (ADI) corresponding to primary residence addresses. Logistic regression was performed to analyze relationships between ADI or rural/urban status and (i) use of genomic test by panel size; (ii) clinical trial matching status. Compared with patients from the most affluent areas, patients had a lower odds of receiving a panel test (vs. a single-gene test) if from areas of higher socioeconomic deprivation [OR (95% confidence interval (CI): 0.71 (0.61-0.83), P < 0.01] or a rural area [OR (95% CI): 0.85 (0.76-0.96), P < 0.01]. Patients in areas of higher socioeconomic deprivation were less likely to be matched to clinical trials if receiving medium panel tests [(OR) (95% CI): 0.69 (0.49-0.97), P = 0.03]; however, there was no difference among patients receiving large panel tests (P > 0.05) and rural patients were almost 2x greater odds of being matched if receiving a large panel test [(OR) (95% CI): 1.76 (1.21-2.55), P < 0.01]. SIGNIFICANCE: We identified socioeconomic and rurality disparities in the use of genomic tests and trial matching by panel size, which may have implications for equal access to targeted therapies. The lack of association between large panel tests and clinical trial matching by socioeconomic status, suggests a potential health equity impact, while removing barriers in access to large panels for rural patients may improve access to trials. However, further research is needed.",
      "journal": "Cancer research communications",
      "year": "2024",
      "doi": "10.1158/2767-9764.CRC-23-0190",
      "authors": "Zhao Yiqing et al.",
      "keywords": "",
      "mesh_terms": "Adult; Humans; Neoplasms; Socioeconomic Disparities in Health; Retrospective Studies; Socioeconomic Factors; Precision Medicine; High-Throughput Nucleotide Sequencing",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38276870/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10840454",
      "ft_text_length": 32737,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10840454)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38354967",
      "title": "Hierarchical Clustering Applied to Chronic Pain Drawings Identifies Undiagnosed Fibromyalgia: Implications for Busy Clinical Practice.",
      "abstract": "Currently-used assessments for fibromyalgia require clinicians to suspect a fibromyalgia diagnosis, a process susceptible to unintentional bias. Automated assessments of standard patient-reported outcomes (PROs) could be used to prompt formal assessments, potentially reducing bias. We sought to determine whether hierarchical clustering of patient-reported pain distribution on digital body map drawings predicted fibromyalgia diagnosis. Using an observational cohort from the University of Pittsburgh's Patient Outcomes Repository for Treatment registry, which contains PROs and electronic medical record data from 21,423 patients (March 17, 2016-June 25, 2019) presenting to pain management clinics, we tested the hypothesis that hierarchical clustering subgroup was associated with fibromyalgia diagnosis, as determined by ICD-10 code. Logistic regression revealed a significant relationship between the body map cluster subgroup and fibromyalgia diagnosis. The cluster subgroup with the most body areas selected was the most likely to receive a diagnosis of fibromyalgia when controlling for age, gender, anxiety, and depression. Despite this, more than two-thirds of patients in this cluster lacked a clinical fibromyalgia diagnosis. In an exploratory analysis to better understand this apparent underdiagnosis, we developed and applied proxies of fibromyalgia diagnostic criteria. We found that proxy diagnoses were more common than ICD-10 diagnoses, which may be due to less frequent clinical fibromyalgia diagnosis in men. Overall, we find evidence of fibromyalgia underdiagnosis, likely due to gender bias. Coupling PROs that take seconds to complete, such as a digital pain body map, with machine learning is a promising strategy to reduce bias in fibromyalgia diagnosis and improve patient outcomes. PERSPECTIVE: This investigation applies hierarchical clustering to patient-reported, digital pain body maps, finding an association between body map responses and clinical fibromyalgia diagnosis. Rapid, computer-assisted interpretation of pain body maps would be clinically useful in prompting more detailed assessments for fibromyalgia, potentially reducing gender bias.",
      "journal": "The journal of pain",
      "year": "2024",
      "doi": "10.1016/j.jpain.2024.02.003",
      "authors": "Alter Benedict J et al.",
      "keywords": "Chronic pain; Cluster analysis; Fibromyalgia; Machine learning; Pain measurement",
      "mesh_terms": "Humans; Fibromyalgia; Male; Female; Middle Aged; Chronic Pain; Adult; Cluster Analysis; Aged; Patient Reported Outcome Measures; Cohort Studies",
      "pub_types": "Journal Article; Observational Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38354967/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11180596",
      "ft_text_length": 1798,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11180596)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38384497",
      "title": "Traditional, complementary, and integrative medicine and artificial intelligence: Novel opportunities in healthcare.",
      "abstract": "The convergence of traditional, complementary, and integrative medicine (TCIM) with artificial intelligence (AI) is a promising frontier in healthcare. TCIM is a patient-centric approach that combines conventional medicine with complementary therapies, emphasizing holistic well-being. AI can revolutionize healthcare through data-driven decision-making and personalized treatment plans. This article explores how AI technologies can complement and enhance TCIM, aligning with the shared objectives of researchers from both fields in improving patient outcomes, enhancing care quality, and promoting holistic wellness. This integration of TCIM and AI introduces exciting opportunities but also noteworthy challenges. AI may augment TCIM by assisting in early disease detection, providing personalized treatment plans, predicting health trends, and enhancing patient engagement. Challenges at the intersection of AI and TCIM include data privacy and security, regulatory complexities, maintaining the human touch in patient-provider relationships, and mitigating bias in AI algorithms. Patients' trust, informed consent, and legal accountability are all essential considerations. Future directions in AI-enhanced TCIM include advanced personalized medicine, understanding the efficacy of herbal remedies, and studying patient-provider interactions. Research on bias mitigation, patient acceptance, and trust in AI-driven TCIM healthcare is crucial. In this article, we outlined that the merging of TCIM and AI holds great promise in enhancing healthcare delivery, personalizing treatment plans, preventive care, and patient engagement. Addressing challenges and fostering collaboration between AI experts, TCIM practitioners, and policymakers, however, is vital to harnessing the full potential of this integration.",
      "journal": "Integrative medicine research",
      "year": "2024",
      "doi": "10.1016/j.imr.2024.101024",
      "authors": "Ng Jeremy Y et al.",
      "keywords": "Artificial intelligence; Complementary and integrative medicine; Novel opportunities in healthcare; Traditional medicine",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38384497/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10879672",
      "ft_text_length": 22106,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10879672)",
      "ft_reason": "Excluded: insufficient approach content (1 indicators)"
    },
    {
      "pmid": "38476785",
      "title": "The Digital Lifeline: Telemedicine and Artificial Intelligence Synergy as a Catalyst for Healthcare Equity in Pakistan.",
      "abstract": "Telemedicine emerges as a critical innovation in Pakistan, aiming to overcome the nation's unique healthcare delivery challenges, including inadequate facilities, professional scarcity, and access disparities. Examining telemedicine's potential to bridge the healthcare gap, particularly in rural and underserved regions plagued by a digital divide and infrastructural deficits, is crucial. There is a critical need for robust digital infrastructure, regulatory frameworks, and digital literacy to facilitate telemedicine adoption. By highlighting the socio-economic and logistical obstacles alongside proposed strategic interventions, the analysis suggests that telemedicine can significantly enhance healthcare accessibility, efficiency, and equity across Pakistan, offering a pragmatic solution to its pressing healthcare needs while also opening room for artificial intelligence in the landscape.",
      "journal": "Cureus",
      "year": "2024",
      "doi": "10.7759/cureus.54017",
      "authors": "Irfan Bilal et al.",
      "keywords": "artificial intelligence in telemedicine; health-care equity; healthcare accessibility; rural healthcare delivery; telemedicine",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38476785/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10930096",
      "ft_text_length": 6828,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10930096)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38478455",
      "title": "Harvard Glaucoma Fairness: A Retinal Nerve Disease Dataset for Fairness Learning and Fair Identity Normalization.",
      "abstract": "Fairness (also known as equity interchangeably) in machine learning is important for societal well-being, but limited public datasets hinder its progress. Currently, no dedicated public medical datasets with imaging data for fairness learning are available, though underrepresented groups suffer from more health issues. To address this gap, we introduce Harvard Glaucoma Fairness (Harvard-GF), a retinal nerve disease dataset including 3,300 subjects with both 2D and 3D imaging data and balanced racial groups for glaucoma detection. Glaucoma is the leading cause of irreversible blindness globally with Blacks having doubled glaucoma prevalence than other races. We also propose a fair identity normalization (FIN) approach to equalize the feature importance between different identity groups. Our FIN approach is compared with various state-of-the-art fairness learning methods with superior performance in the racial, gender, and ethnicity fairness tasks with 2D and 3D imaging data, demonstrating the utilities of our dataset Harvard-GF for fairness learning. To facilitate fairness comparisons between different models, we propose an equity-scaled performance measure, which can be flexibly used to compare all kinds of performance metrics in the context of fairness. The dataset and code are publicly accessible via https://ophai.hms.harvard.edu/datasets/harvard-gf3300/.",
      "journal": "IEEE transactions on medical imaging",
      "year": "2024",
      "doi": "10.1109/TMI.2024.3377552",
      "authors": "Luo Yan et al.",
      "keywords": "",
      "mesh_terms": "Humans; Glaucoma; Machine Learning; Male; Databases, Factual; Female; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Middle Aged; Aged",
      "pub_types": "Journal Article; Dataset; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38478455/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11251413",
      "ft_text_length": 1371,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11251413)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38481027",
      "title": "Disparities in seizure outcomes revealed by large language models.",
      "abstract": "OBJECTIVE: Large-language models (LLMs) can potentially revolutionize health care delivery and research, but risk propagating existing biases or introducing new ones. In epilepsy, social determinants of health are associated with disparities in care access, but their impact on seizure outcomes among those with access remains unclear. Here we (1) evaluated our validated, epilepsy-specific LLM for intrinsic bias, and (2) used LLM-extracted seizure outcomes to determine if different demographic groups have different seizure outcomes. MATERIALS AND METHODS: We tested our LLM for differences and equivalences in prediction accuracy and confidence across demographic groups defined by race, ethnicity, sex, income, and health insurance, using manually annotated notes. Next, we used LLM-classified seizure freedom at each office visit to test for demographic outcome disparities, using univariable and multivariable analyses. RESULTS: We analyzed 84\u00a0675 clinic visits from 25\u00a0612 unique patients seen at our epilepsy center. We found little evidence of bias in the prediction accuracy or confidence of outcome classifications across demographic groups. Multivariable analysis indicated worse seizure outcomes for female patients (OR 1.33, P \u2264 .001), those with public insurance (OR 1.53, P \u2264 .001), and those from lower-income zip codes (OR \u22651.22, P \u2009\u2264\u2009.007). Black patients had worse outcomes than White patients in univariable but not multivariable analysis (OR 1.03, P\u2009=\u2009.66). CONCLUSION: We found little evidence that our LLM was intrinsically biased against any demographic group. Seizure freedom extracted by LLM revealed disparities in seizure outcomes across several demographic groups. These findings quantify the critical need to reduce disparities in the care of people with epilepsy.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2024",
      "doi": "10.1093/jamia/ocae047",
      "authors": "Xie Kevin et al.",
      "keywords": "clinical informatics; electronic health record; health disparities; natural language processing",
      "mesh_terms": "Humans; Female; Male; Adult; Healthcare Disparities; Seizures; Epilepsy; Middle Aged; Natural Language Processing; Social Determinants of Health; Adolescent; Young Adult; Language",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38481027/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11105138",
      "ft_text_length": 1809,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11105138)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38488828",
      "title": "New Approach to Equitable Intervention Planning to Improve Engagement and Outcomes in a Digital Health Program: Simulation Study.",
      "abstract": "BACKGROUND: Digital health programs provide individualized support to patients with chronic diseases and their effectiveness is measured by the extent to which patients achieve target individual clinical outcomes and the program's ability to sustain patient engagement. However, patient dropout and inequitable intervention delivery strategies, which may unintentionally penalize certain patient subgroups, represent challenges to maximizing effectiveness. Therefore, methodologies that optimize the balance between success factors (achievement of target clinical outcomes and sustained engagement) equitably would be desirable, particularly when there are resource constraints. OBJECTIVE: Our objectives were to propose a model for digital health program resource management that accounts jointly for the interaction between individual clinical outcomes and patient engagement, ensures equitable allocation as well as allows for capacity planning, and conducts extensive simulations using publicly available data on type 2 diabetes, a chronic disease. METHODS: We propose a restless multiarmed bandit (RMAB) model to plan interventions that jointly optimize long-term engagement and individual clinical outcomes (in this case measured as the achievement of target healthy glucose levels). To mitigate the tendency of RMAB to achieve good aggregate performance by exacerbating disparities between groups, we propose new equitable objectives for RMAB and apply bilevel optimization algorithms to solve them. We formulated a model for the joint evolution of patient engagement and individual clinical outcome trajectory to capture the key dynamics of interest in digital chronic disease management programs. RESULTS: In simulation exercises, our optimized intervention policies lead to up to 10% more patients reaching healthy glucose levels after 12 months, with a 10% reduction in dropout compared to standard-of-care baselines. Further, our new equitable policies reduce the mean absolute difference of engagement and health outcomes across 6 demographic groups by up to 85% compared to the state-of-the-art. CONCLUSIONS: Planning digital health interventions with individual clinical outcome objectives and long-term engagement dynamics as considerations can be both feasible and effective. We propose using an RMAB sequential decision-making framework, which may offer additional capabilities in capacity planning as well. The integration of an equitable RMAB algorithm further enhances the potential for reaching equitable solutions. This approach provides program designers with the flexibility to switch between different priorities and balance trade-offs across various objectives according to their preferences.",
      "journal": "JMIR diabetes",
      "year": "2024",
      "doi": "10.2196/52688",
      "authors": "Killian Jackson A et al.",
      "keywords": "T2D; chronic disease; digital health; equity; machine learning; multi-armed bandit; multi-armed bandits; resource allocation; restless multiarmed bandits; type-2 diabetes",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38488828/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10980993",
      "ft_text_length": 39712,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10980993)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38510858",
      "title": "What Goes In, Must Come Out: Generative Artificial Intelligence Does Not Present Algorithmic Bias Across Race and Gender in Medical Residency Specialties.",
      "abstract": "Objective Artificial Intelligence (AI) has made significant inroads into various domains, including medicine, raising concerns about algorithmic bias. This study investigates the presence of biases in generative AI programs, with a specific focus on gender and racial representations across 19 medical residency specialties. Methodology This comparative study utilized DALL-E2 to generate faces representing 19 distinct residency training specialties, as identified by the Association of American Medical Colleges (AAMC), which were then compared to the AAMC's residency specialty breakdown with respect to race and gender. Results Our findings reveal an alignment between OpenAI's DALL-E2's predictions and the current demographic landscape of medical residents, suggesting an absence of algorithmic bias in this AI model. Conclusion This revelation gives rise to important ethical considerations. While AI excels at pattern recognition, it inherits and mirrors the biases present in its training data. To combat AI bias, addressing real-world disparities is imperative. Initiatives to promote inclusivity and diversity within medicine are commendable and contribute to reshaping medical education. This study underscores the need for ongoing efforts to dismantle barriers and foster inclusivity in historically male-dominated medical fields, particularly for underrepresented populations. Ultimately, our findings underscore the crucial role of real-world data quality in mitigating AI bias. As AI continues to shape healthcare and education, the pursuit of equitable, unbiased AI applications should remain at the forefront of these transformative endeavors.",
      "journal": "Cureus",
      "year": "2024",
      "doi": "10.7759/cureus.54448",
      "authors": "Lin Shu et al.",
      "keywords": "artificial intelligence; bias identification; diversity; healthcare; medical education",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38510858/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10951939",
      "ft_text_length": 20316,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10951939)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38526701",
      "title": "DeepN4: Learning N4ITK Bias Field Correction for T1-weighted Images.",
      "abstract": "T1-weighted (T1w) MRI has low frequency intensity artifacts due to magnetic field inhomogeneities. Removal of these biases in T1w MRI images is a critical preprocessing step to ensure spatially consistent image interpretation. N4ITK bias field correction, the current state-of-the-art, is implemented in such a way that makes it difficult to port between different pipelines and workflows, thus making it hard to reimplement and reproduce results across local, cloud, and edge platforms. Moreover, N4ITK is opaque to optimization before and after its application, meaning that methodological development must work around the inhomogeneity correction step. Given the importance of bias fields correction in structural preprocessing and flexible implementation, we pursue a deep learning approximation / reinterpretation of the N4ITK bias fields correction to create a method which is portable, flexible, and fully differentiable. In this paper, we trained a deep learning network \"DeepN4\" on eight independent cohorts from 72 different scanners and age ranges with N4ITK-corrected T1w MRI and bias field for supervision in log space. We found that we can closely approximate N4ITK bias fields correction with na\u00efve networks. We evaluate the peak signal to noise ratio (PSNR) in test dataset against the N4ITK corrected images. The median PSNR of corrected images between N4ITK and DeepN4 was 47.96\u00a0dB. In addition, we assess the DeepN4 model on eight additional external datasets and show the generalizability of the approach. This study establishes that incompatible N4ITK preprocessing steps can be closely approximated by na\u00efve deep neural networks, facilitating more flexibility. All code and models are released at https://github.com/MASILab/DeepN4 .",
      "journal": "Neuroinformatics",
      "year": "2024",
      "doi": "10.1007/s12021-024-09655-9",
      "authors": "Kanakaraj Praitayini et al.",
      "keywords": "3D U-Net; Bias field correction; Inhomogeneity; N4ITK; T1-weighted images",
      "mesh_terms": "Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Algorithms; Neural Networks, Computer; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38526701/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11182041",
      "ft_text_length": 1755,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11182041)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38540608",
      "title": "Reducing Sample Size While Improving Equity in Vaccine Clinical Trials: A Machine Learning-Based Recruitment Methodology with Application to Improving Trials of Hepatitis C Virus Vaccines in People Who Inject Drugs.",
      "abstract": "Despite the availability of direct-acting antivirals that cure individuals infected with the hepatitis C virus (HCV), developing a vaccine is critically needed in achieving HCV elimination. HCV vaccine trials have been performed in populations with high incidence of new HCV infection such as people who inject drugs (PWID). Developing strategies of optimal recruitment of PWID for HCV vaccine trials could reduce sample size, follow-up costs and disparities in enrollment. We investigate trial recruitment informed by machine learning and evaluate a strategy for HCV vaccine trials termed PREDICTEE-Predictive Recruitment and Enrichment method balancing Demographics and Incidence for Clinical Trial Equity and Efficiency. PREDICTEE utilizes a survival analysis model applied to trial candidates, considering their demographic and injection characteristics to predict the candidate's probability of HCV infection during the trial. The decision to recruit considers both the candidate's predicted incidence and demographic characteristics such as age, sex, and race. We evaluated PREDICTEE using in silico methods, in which we first generated a synthetic candidate pool and their respective HCV infection events using HepCEP, a validated agent-based simulation model of HCV transmission among PWID in metropolitan Chicago. We then compared PREDICTEE to conventional recruitment of high-risk PWID who share drugs or injection equipment in terms of sample size and recruitment equity, with the latter measured by participation-to-prevalence ratio (PPR) across age, sex, and race. Comparing conventional recruitment to PREDICTEE found a reduction in sample size from 802 (95%: 642-1010) to 278 (95%: 264-294) with PREDICTEE, while also reducing screening requirements by 30%. Simultaneously, PPR increased from 0.475 (95%: 0.356-0.568) to 0.754 (95%: 0.685-0.834). Even when targeting a dissimilar maximally balanced population in which achieving recruitment equity would be more difficult, PREDICTEE is able to reduce sample size from 802 (95%: 642-1010) to 304 (95%: 288-322) while improving PPR to 0.807 (95%: 0.792-0.821). PREDICTEE presents a promising strategy for HCV clinical trial recruitment, achieving sample size reduction while improving recruitment equity.",
      "journal": "Healthcare (Basel, Switzerland)",
      "year": "2024",
      "doi": "10.3390/healthcare12060644",
      "authors": "Chiu Richard et al.",
      "keywords": "equity; hepatitis C; machine learning; people who inject drugs; randomized clinical trial; vaccine trial recruitment",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38540608/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10970332",
      "ft_text_length": 34076,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10970332)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38544748",
      "title": "Identifying incarceration status in the electronic health record using large language models in emergency department settings.",
      "abstract": "BACKGROUND: Incarceration is a significant social determinant of health, contributing to high morbidity, mortality, and racialized health inequities. However, incarceration status is largely invisible to health services research due to inadequate clinical electronic health record (EHR) capture. This study aims to develop, train, and validate natural language processing (NLP) techniques to more effectively identify incarceration status in the EHR. METHODS: The study population consisted of adult patients (\u2265 18 y.o.) who presented to the emergency department between June 2013 and August 2021. The EHR database was filtered for notes for specific incarceration-related terms, and then a random selection of 1,000 notes was annotated for incarceration and further stratified into specific statuses of prior history, recent, and current incarceration. For NLP model development, 80% of the notes were used to train the Longformer-based and RoBERTa algorithms. The remaining 20% of the notes underwent analysis with GPT-4. RESULTS: There were 849 unique patients across 989 visits in the 1000 annotated notes. Manual annotation revealed that 559 of 1000 notes (55.9%) contained evidence of incarceration history. ICD-10 code (sensitivity: 4.8%, specificity: 99.1%, F1-score: 0.09) demonstrated inferior performance to RoBERTa NLP (sensitivity: 78.6%, specificity: 73.3%, F1-score: 0.79), Longformer NLP (sensitivity: 94.6%, specificity: 87.5%, F1-score: 0.93), and GPT-4 (sensitivity: 100%, specificity: 61.1%, F1-score: 0.86). CONCLUSIONS: Our advanced NLP models demonstrate a high degree of accuracy in identifying incarceration status from clinical notes. Further research is needed to explore their scaled implementation in population health initiatives and assess their potential to mitigate health disparities through tailored system interventions.",
      "journal": "Journal of clinical and translational science",
      "year": "2024",
      "doi": "10.1017/cts.2024.496",
      "authors": "Huang Thomas et al.",
      "keywords": "ChatGPT; emergency department; incarceration; justice involvement; large language models; machine learning; natural language processing",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38544748/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10966832",
      "ft_text_length": 40017,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10966832)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38559222",
      "title": "Autonomous Artificial Intelligence Increases Access and Health Equity in Underserved Populations with Diabetes.",
      "abstract": "Diabetic eye disease (DED) is a leading cause of blindness in the world. Early detection and treatment of DED have been shown to be both sight-saving and cost-effective. As such, annual testing for DED is recommended for adults with diabetes and is a Healthcare Effectiveness Data and Information Set (HEDIS) measure. However, adherence to this guideline has historically been low, and access to this sight-saving intervention has particularly been limited for specific populations, such as Black or African American patients. In 2018, the US Food and Drug Agency (FDA) De Novo cleared autonomous artificial intelligence (AI) for diagnosing DED in a primary care setting. In 2020, Johns Hopkins Medicine (JHM), an integrated healthcare system with over 30 primary care sites, began deploying autonomous AI for DED testing in some of its primary care clinics. In this retrospective study, we aimed to determine whether autonomous AI implementation was associated with increased adherence to annual DED testing, and whether this was different for specific populations. JHM primary care sites were categorized as \"non-AI\" sites (sites with no autonomous AI deployment over the study period and where patients are referred to eyecare for DED testing) or \"AI-switched\" sites (sites that did not have autonomous AI testing in 2019 but did by 2021). We conducted a difference-in-difference analysis using a logistic regression model to compare change in adherence rates from 2019 to 2021 between non-AI and AI-switched sites. Our study included all adult patients with diabetes managed within our health system (17,674 patients for the 2019 cohort and 17,590 patients for the 2021 cohort) and has three major findings. First, after controlling for a wide range of potential confounders, our regression analysis demonstrated that the odds ratio of adherence at AI-switched sites was 36% higher than that of non-AI sites, suggesting that there was a higher increase in DED testing between 2019 and 2021 at AI-switched sites than at non-AI sites. Second, our data suggested autonomous AI improved access for historically disadvantaged populations. The adherence rate for Black/African Americans increased by 11.9% within AI-switched sites whereas it decreased by 1.2% within non-AI sites over the same time frame. Third, the data suggest that autonomous AI improved health equity by closing care gaps. For example, in 2019, a large adherence rate gap existed between Asian Americans and Black/African Americans (61.1% vs. 45.5%). This 15.6% gap shrank to 3.5% by 2021. In summary, our real-world deployment results in a large integrated healthcare system suggest that autonomous AI improves adherence to a HEDIS measure, patient access, and health equity for patients with diabetes - particularly in historically disadvantaged patient groups. While our findings are encouraging, they will need to be replicated and validated in a prospective manner across more diverse settings.",
      "journal": "Research square",
      "year": "2024",
      "doi": "10.21203/rs.3.rs-3979992/v1",
      "authors": "Liu T Y Alvin et al.",
      "keywords": "Autonomous artificial intelligence; adherence; diabetic eye disease; diabetic retinopathy; health access; health equity",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38559222/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10980149",
      "ft_text_length": 15949,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10980149)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38560436",
      "title": "Sex-related disparities in vehicle crash injury and hemodynamics.",
      "abstract": "OBJECTIVE: Multiple studies evaluate relative risk of female vs. male crash injury; clinical data may offer a more direct injury-specific evaluation of sex disparity in vehicle safety. This study sought to evaluate trauma injury patterns in a large trauma database to identify sex-related differences in crash injury victims. METHODS: Data on lap and shoulder belt wearing patients age 16 and up with abdominal and pelvic injuries from 2018 to 2021 were extracted from the National Trauma Data Bank for descriptive analysis using injuries, vital signs, International Classification of Disease (ICD) coding, age, and injury severity using AIS (Abbreviated Injury Scale) and ISS (Injury Severity Score). Multiple linear regression was used to assess the relationship of shock index (SI) and ISS, sex, age, and sex*age interaction. Regression analysis was performed on multiple injury regions to assess patient characteristics related to increased shock index. RESULTS: Sex, age, and ISS are strongly related to shock index for most injury regions. Women had greater overall SI than men, even in less severe injuries; women had greater numbers of pelvis and liver injuries across severity categories; men had greater numbers of injury in other abdominal/pelvis injury regions. CONCLUSIONS: Female crash injury victims' tendency for higher (AIS) severity of pelvis and liver injuries may relate to how their bodies interact with safety equipment. Females are entering shock states (SI > 1.0) with lesser injury severity (ISS) than male crash injury victims, which may suggest that female crash patients are somehow more susceptible to compromised hemodynamics than males. These findings indicate an urgent need to conduct vehicle crash injury research within a sex-equity framework; evaluating sex-related clinical data may hold the key to reducing disparities in vehicle crash injury.",
      "journal": "Frontiers in public health",
      "year": "2024",
      "doi": "10.3389/fpubh.2024.1331313",
      "authors": "Cronn Susan et al.",
      "keywords": "crash safety; equity in research; sex differences; shock index; traumatic injury; vehicle crash",
      "mesh_terms": "Humans; Male; Female; Adolescent; Accidents, Traffic; Liver; Injury Severity Score; Protective Devices; Hemodynamics",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38560436/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10978633",
      "ft_text_length": 34850,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10978633)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "38567296",
      "title": "Muffin: A Framework Toward Multi-Dimension AI Fairness by Uniting Off-the-Shelf Models.",
      "abstract": "Model fairness (a.k.a., bias) has become one of the most critical problems in a wide range of AI applications. An unfair model in autonomous driving may cause a traffic accident if corner cases (e.g., extreme weather) cannot be fairly regarded; or it will incur healthcare disparities if the AI model misdiagnoses a certain group of people (e.g., brown and black skin). In recent years, there are emerging research works on addressing unfairness, and they mainly focus on a single unfair attribute, like skin tone; however, real-world data commonly have multiple attributes, among which unfairness can exist in more than one attribute, called \"multi-dimensional fairness\". In this paper, we first reveal a strong correlation between the different unfair attributes, i.e., optimizing fairness on one attribute will lead to the collapse of others. Then, we propose a novel Multi-Dimension Fairness framework, namely Muffin, which includes an automatic tool to unite off-the-shelf models to improve the fairness on multiple attributes simultaneously. Case studies on dermatology datasets with two unfair attributes show that the existing approach can achieve 21.05% fairness improvement on the first attribute while it makes the second attribute unfair by 1.85%. On the other hand, the proposed Muffin can unite multiple models to achieve simultaneously 26.32% and 20.37% fairness improvement on both attributes; meanwhile, it obtains 5.58% accuracy gain.",
      "journal": "Proceedings. Design Automation Conference",
      "year": "2023",
      "doi": "10.1109/dac56929.2023.10247765",
      "authors": "Sheng Yi et al.",
      "keywords": "model fusing; multi-dimensional fairness; parameters; reinforcement learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38567296/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10987014",
      "ft_text_length": 1464,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10987014)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "38621641",
      "title": "Identifying social determinants of health from clinical narratives: A study of performance, documentation ratio, and potential bias.",
      "abstract": "OBJECTIVE: To develop a natural language processing (NLP) package to extract social determinants of health (SDoH) from clinical narratives, examine the bias among race and gender groups, test the generalizability of extracting SDoH for different disease groups, and examine population-level extraction ratio. METHODS: We developed SDoH corpora using clinical notes identified at the University of Florida (UF) Health. We systematically compared 7 transformer-based large language models (LLMs) and developed an open-source package - SODA (i.e., SOcial DeterminAnts) to facilitate SDoH extraction from clinical narratives. We examined the performance and potential bias of SODA for different race and gender groups, tested the generalizability of SODA using two disease domains including cancer and opioid use, and explored strategies for improvement. We applied SODA to extract 19 categories of SDoH from the breast (n\u00a0=\u00a07,971), lung (n\u00a0=\u00a011,804), and colorectal cancer (n\u00a0=\u00a06,240) cohorts to assess patient-level extraction ratio and examine the differences among race and gender groups. RESULTS: We developed an SDoH corpus using 629 clinical notes of cancer patients with annotations of 13,193 SDoH concepts/attributes from 19 categories of SDoH, and another cross-disease validation corpus using 200 notes from opioid use patients with 4,342 SDoH concepts/attributes. We compared 7 transformer models and the GatorTron model achieved the best mean average strict/lenient F1 scores of 0.9122 and 0.9367 for SDoH concept extraction and 0.9584 and 0.9593 for linking attributes to SDoH concepts. There is a small performance gap (\u223c4%) between Males and Females, but a large performance gap (>16\u00a0%) among race groups. The performance dropped when we applied the cancer SDoH model to the opioid cohort; fine-tuning using a smaller opioid SDoH corpus improved the performance. The extraction ratio varied in the three cancer cohorts, in which 10 SDoH could be extracted from over 70\u00a0% of cancer patients, but 9 SDoH could be extracted from less than 70\u00a0% of cancer patients. Individuals from the White and Black groups have a higher extraction ratio than other minority race groups. CONCLUSIONS: Our SODA package achieved good performance in extracting 19 categories of SDoH from clinical narratives. The SODA package with pre-trained transformer models is available at https://github.com/uf-hobi-informatics-lab/SODA_Docker.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104642",
      "authors": "Yu Zehao et al.",
      "keywords": "Cancer; Clinical concept extraction; Large language model; Natural language processing; Social determinants of health; Transformer",
      "mesh_terms": "Humans; Natural Language Processing; Social Determinants of Health; Female; Male; Narration; Bias; Electronic Health Records; Documentation; Data Mining",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38621641/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11141428",
      "ft_text_length": 2412,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11141428)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38639979",
      "title": "Harnessing Artificial Intelligence to Address Oral Health Disparities.",
      "abstract": "This Viewpoint explores the unique attributes of dentistry that could leverage artificial intelligence for many improvements including greater health equity.",
      "journal": "JAMA health forum",
      "year": "2024",
      "doi": "10.1001/jamahealthforum.2024.0642",
      "authors": "Elani Hawazin W et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Health Inequities",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38639979/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11909795",
      "ft_text_length": 6720,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11909795)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38645091",
      "title": "A deep learning-based approach for unbiased kinematic analysis in CNS injury.",
      "abstract": "UNLABELLED: Traumatic spinal cord injury (SCI) is a devastating condition that impacts over 300,000 individuals in the US alone. Depending on the severity of the injury, SCI can lead to varying degrees of sensorimotor deficits and paralysis. Despite advances in our understanding of the underlying pathological mechanisms of SCI and the identification of promising molecular targets for repair and functional restoration, few therapies have made it into clinical use. To improve the success rate of clinical translation, more robust, sensitive, and reproducible means of functional assessment are required. The gold standards for the evaluation of locomotion in rodents with SCI are the Basso Beattie Bresnahan (BBB) and Basso Mouse Scale (BMS) tests. To overcome the shortcomings of current methods, we developed two separate marker-less kinematic analysis paradigms in mice, MotorBox and MotoRater, based on deep-learning algorithms generated with the DeepLabCut open-source toolbox. The MotorBox system uses an originally designed, custom-made chamber, and the MotoRater system was implemented on a commercially available MotoRater device. We validated the MotorBox and MotoRater systems by comparing them with the traditional BMS test and extracted metrics of movement and gait that can provide an accurate and sensitive representation of mouse locomotor function post-injury, while eliminating investigator bias and variability. The integration of MotorBox and/or MotoRater assessments with BMS scoring will provide a much wider range of information on specific aspects of locomotion, ensuring the accuracy, rigor, and reproducibility of behavioral outcomes after SCI. HIGHLIGHTS: MotorBox and MotoRater systems are two novel marker-less kinematic analysis paradigms in mice, based on deep-learning algorithms generated with DeepLabCut.MotorBox and MotoRater systems are highly sensitive, accurate and unbiased in analyzing locomotor behavior in mice.MotorBox and MotoRater systems allow for sensitive detection of SCI-induced changes in movement metrics, including range of motion, gait, coordination, and speed.MotorBox and MotoRater systems allow for detection of movement metrics not measurable with the BMS.",
      "journal": "bioRxiv : the preprint server for biology",
      "year": "2024",
      "doi": "10.1101/2024.04.08.588606",
      "authors": "Ascona Maureen et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38645091/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11030365",
      "ft_text_length": 2216,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11030365)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38681759",
      "title": "Analyzing the Impact of Personalization on Fairness in Federated Learning for Healthcare.",
      "abstract": "As machine learning (ML) usage becomes more popular in the healthcare sector, there are also increasing concerns about potential biases and risks such as privacy. One countermeasure is to use federated learning (FL) to support collaborative learning without the need for patient data sharing across different organizations. However, the inherent heterogeneity of data distributions among participating FL parties poses challenges for exploring group fairness in FL. While personalization within FL can handle performance degradation caused by data heterogeneity, its influence on group fairness is not fully investigated. Therefore, the primary focus of this study is to rigorously assess the impact of personalized FL on group fairness in the healthcare domain, offering a comprehensive understanding of how personalized FL affects group fairness in clinical outcomes. We conduct an empirical analysis using two prominent real-world Electronic Health Records (EHR) datasets, namely eICU and MIMIC-IV. Our methodology involves a thorough comparison between personalized FL and two baselines: standalone training, where models are developed independently without FL collaboration, and standard FL, which aims to learn a global model via the FedAvg algorithm. We adopt Ditto as our personalized FL approach, which enables each client in FL to develop its own personalized model through multi-task learning. Our assessment is achieved through a series of evaluations, comparing the predictive performance (i.e., AUROC and AUPRC) and fairness gaps (i.e., EOPP, EOD, and DP) of these methods. Personalized FL demonstrates superior predictive accuracy and fairness over standalone training across both datasets. Nevertheless, in comparison with standard FL, personalized FL shows improved predictive accuracy but does not consistently offer better fairness outcomes. For instance, in the 24-h in-hospital mortality prediction task, personalized FL achieves an average EOD of 27.4% across racial groups in the eICU dataset and 47.8% in MIMIC-IV. In comparison, standard FL records a better EOD of 26.2% for eICU and 42.0% for MIMIC-IV, while standalone training yields significantly worse EOD of 69.4% and 54.7% on these datasets, respectively. Our analysis reveals that personalized FL has the potential to enhance fairness in comparison to standalone training, yet it does not consistently ensure fairness improvements compared to standard FL. Our findings also show that while personalization can improve fairness for more biased hospitals (i.e., hospitals having larger fairness gaps in standalone training), it can exacerbate fairness issues for less biased ones. These insights suggest that the integration of personalized FL with additional strategic designs could be key to simultaneously boosting prediction accuracy and reducing fairness disparities. The findings and opportunities outlined in this paper can inform the research agenda for future studies, to overcome the limitations and further advance health equity research.",
      "journal": "Journal of healthcare informatics research",
      "year": "2024",
      "doi": "10.1007/s41666-024-00164-7",
      "authors": "Wang Tongnian et al.",
      "keywords": "Federated learning; Group fairness; Health disparities; Personalization; Privacy",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38681759/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11052754",
      "ft_text_length": 3030,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11052754)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38698620",
      "title": "Investigating disparities in smoking cessation treatment for veterans with multiple sclerosis: A national analysis.",
      "abstract": "BACKGROUND AND AIMS: Smoking is a risk factor for multiple sclerosis (MS) development, symptom burden, decreased medication efficacy, and increased disease-related mortality. Veterans with MS (VwMS) smoke at critically high rates; however, treatment rates and possible disparities are unknown. To promote equitable treatment, we aim to investigate smoking cessation prescription practices for VwMS across social determinant factors. METHODS: We extracted data from the national Veterans Health Administration electronic health records between October 1, 2017, and September 30, 2018. To derive marginal estimates of the association of MS with receipt of smoking-cessation pharmacotherapy, we used propensity score matching through the extreme gradient boosting machine learning model. VwMS who smoke were matched with veterans without MS who smoke on factors including age, race, depression, and healthcare visits. To assess the marginal association of MS with different cessation treatments, we used logistic regression and conducted stratified analyses by sex, race, and ethnicity. RESULTS: The matched sample achieved a good balance across most covariates, compared to the pre-match sample. VwMS (n\u00a0=\u00a03320) had decreased odds of receiving prescriptions for nicotine patches ([Odds Ratio]OR\u00a0=\u00a00.86, p\u00a0<\u00a0.01), non-patch nicotine replacement therapy (NRT; OR\u00a0=\u00a00.81, p\u00a0<\u00a0.001), and standard practice dual NRT (OR\u00a0=\u00a00.77, p\u00a0<\u00a0.01), compared to matches without MS (n\u00a0=\u00a013,280). Men with MS had lower odds of receiving prescriptions for nicotine patches (OR\u00a0=\u00a00.88, p\u00a0=\u00a0.05), non-patch NRT (OR\u00a0=\u00a00.77, p\u00a0<\u00a0.001), and dual NRT (OR\u00a0=\u00a00.72, p\u00a0<\u00a0.001). Similarly, Black VwMS had lower odds of receiving prescriptions for patches (OR\u00a0=\u00a00.62, p\u00a0<\u00a0.001), non-patch NRT (OR\u00a0=\u00a00.75, p\u00a0<\u00a0.05), and dual NRT (OR\u00a0=\u00a00.52, p\u00a0<\u00a0.01). The odds of receiving prescriptions for bupropion or varenicline did not differ between VwMS and matches without MS. CONCLUSION: VwMS received significantly less smoking cessation treatment, compared to matched controls without MS, showing a critical gap in health services as VwMS are not receiving dual NRT as the standard of care. Prescription rates were especially lower for male and Black VwMS, suggesting that under-represented demographic groups outside of the white female category, most often considered as the \"traditional MS\" group, could be under-treated regarding smoking cessation support. This foundational work will help inform future work to promote equitable treatment and implementation of cessation interventions for people living with MS.",
      "journal": "Brain and behavior",
      "year": "2024",
      "doi": "10.1002/brb3.3513",
      "authors": "Polick Carri S et al.",
      "keywords": "Health equity; Multiple Sclerosis; Smoking cessation; Veterans",
      "mesh_terms": "Humans; Male; Female; Veterans; Smoking Cessation; Multiple Sclerosis; Middle Aged; United States; Tobacco Use Cessation Devices; Healthcare Disparities; Adult; United States Department of Veterans Affairs; Smoking Cessation Agents; Aged; Bupropion; Varenicline",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38698620/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11066415",
      "ft_text_length": 24224,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11066415)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38740316",
      "title": "A roadmap to artificial intelligence (AI): Methods for designing and building AI ready data to promote fairness.",
      "abstract": "OBJECTIVES: We evaluated methods for preparing electronic health record data to reduce bias before applying artificial intelligence (AI). METHODS: We created methods for transforming raw data into a data framework for applying machine learning and natural language processing techniques for predicting falls and fractures. Strategies such as inclusion and reporting for multiple races, mixed data sources such as outpatient, inpatient, structured codes, and unstructured notes, and addressing missingness were applied to raw data to promote a reduction in bias. The raw data was carefully curated using validated definitions to create data variables such as age, race, gender, and healthcare utilization. For the formation of these variables, clinical, statistical, and data expertise were used. The research team included a variety of experts with diverse professional and demographic backgrounds to include diverse perspectives. RESULTS: For the prediction of falls, information extracted from radiology reports was converted to a matrix for applying machine learning. The processing of the data resulted in an input of 5,377,673 reports to the machine learning algorithm, out of which 45,304 were flagged as positive and 5,332,369 as negative for falls. Processed data resulted in lower missingness and a better representation of race and diagnosis codes. For fractures, specialized algorithms extracted snippets of text around keywork \"femoral\" from dual x-ray absorptiometry (DXA) scans to identify femoral neck T-scores that are important for predicting fracture risk. The natural language processing algorithms yielded 98% accuracy and 2% error rate The methods to prepare data for input to artificial intelligence processes are reproducible and can be applied to other studies. CONCLUSION: The life cycle of data from raw to analytic form includes data governance, cleaning, management, and analysis. When applying artificial intelligence methods, input data must be prepared optimally to reduce algorithmic bias, as biased output is harmful. Building AI-ready data frameworks that improve efficiency can contribute to transparency and reproducibility. The roadmap for the application of AI involves applying specialized techniques to input data, some of which are suggested here. This study highlights data curation aspects to be considered when preparing data for the application of artificial intelligence to reduce bias.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104654",
      "authors": "Kidwai-Khan Farah et al.",
      "keywords": "Algorithms; Artificial Intelligence; Data preparation; Diversity; Fairness; Inclusion",
      "mesh_terms": "Humans; Electronic Health Records; Artificial Intelligence; Natural Language Processing; Accidental Falls; Algorithms; Machine Learning; Fractures, Bone; Female",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38740316/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11144439",
      "ft_text_length": 2432,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11144439)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38746448",
      "title": "Measuring algorithmic bias to analyze the reliability of AI tools that predict depression risk using smartphone sensed-behavioral data.",
      "abstract": "AI tools intend to transform mental healthcare by providing remote estimates of depression risk using behavioral data collected by sensors embedded in smartphones. While these tools accurately predict elevated symptoms in small, homogenous populations, recent studies show that these tools are less accurate in larger, more diverse populations. In this work, we show that accuracy is reduced because sensed-behaviors are unreliable predictors of depression across individuals; specifically the sensed-behaviors that predict depression risk are inconsistent across demographic and socioeconomic subgroups. We first identified subgroups where a developed AI tool underperformed by measuring algorithmic bias, where subgroups with depression were incorrectly predicted to be at lower risk than healthier subgroups. We then found inconsistencies between sensed-behaviors predictive of depression across these subgroups. Our findings suggest that researchers developing AI tools predicting mental health from behavior should think critically about the generalizability of these tools, and consider tailored solutions for targeted populations.",
      "journal": "Research square",
      "year": "2024",
      "doi": "10.21203/rs.3.rs-3044613/v1",
      "authors": "Adler Daniel A et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38746448/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11092819",
      "ft_text_length": 1146,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11092819)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38789452",
      "title": "BOLD: Blood-gas and Oximetry Linked Dataset.",
      "abstract": "Pulse oximeters measure peripheral arterial oxygen saturation (SpO2) noninvasively, while the gold standard (SaO2) involves arterial blood gas measurement. There are known racial and ethnic disparities in their performance. BOLD is a dataset that aims to underscore the importance of addressing biases in pulse oximetry accuracy, which disproportionately affect darker-skinned patients. The dataset was created by harmonizing three Electronic Health Record databases (MIMIC-III, MIMIC-IV, eICU-CRD) comprising Intensive Care Unit stays of US patients. Paired SpO2 and SaO2 measurements were time-aligned and combined with various other sociodemographic and parameters to provide a detailed representation of each patient. BOLD includes 49,099 paired measurements, within a 5-minute window and with oxygen saturation levels between 70-100%. Minority racial and ethnic groups account for ~25% of the data - a proportion seldom achieved in previous studies. The codebase is publicly available. Given the prevalent use of pulse oximeters in the hospital and at home, we hope that BOLD will be leveraged to develop debiasing algorithms that can result in more equitable healthcare solutions.",
      "journal": "Scientific data",
      "year": "2024",
      "doi": "10.1038/s41597-024-03225-z",
      "authors": "Matos Jo\u00e3o et al.",
      "keywords": "",
      "mesh_terms": "Humans; Oximetry; Blood Gas Analysis; Oxygen Saturation; Intensive Care Units; Ethnicity; Oxygen",
      "pub_types": "Dataset; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38789452/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11126612",
      "ft_text_length": 40558,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11126612)",
      "ft_reason": "Excluded: insufficient approach content (1 indicators)"
    },
    {
      "pmid": "38825845",
      "title": "Scoring facial attractiveness with deep convolutional neural networks: How training on standardized images reduces the bias of facial expressions.",
      "abstract": "OBJECTIVE: In many medical disciplines, facial attractiveness is part of the diagnosis, yet its scoring might be confounded by facial expressions. The intent was to apply deep convolutional neural networks (CNN) to identify how facial expressions affect facial attractiveness and to explore whether a dedicated training of the CNN is able to reduce the bias of facial expressions. MATERIALS AND METHODS: Frontal facial images (n\u2009=\u2009840) of 40 female participants (mean age 24.5\u2009years) were taken adapting a neutral facial expression and the six universal facial expressions. Facial attractiveness was computed by means of a face detector, deep convolutional neural networks, standard support vector regression for facial beauty, visual regularized collaborative filtering and a regression technique for handling visual queries without rating history. CNN was first trained on random facial photographs from a dating website and then further trained on the Chicago Face Database (CFD) to increase its suitability to medical conditions. Both algorithms scored every image for attractiveness. RESULTS: Facial expressions affect facial attractiveness scores significantly. Scores from CNN additionally trained on CFD had less variability between the expressions (range 54.3-60.9 compared to range: 32.6-49.5) and less variance within the scores (P\u2009\u2264\u2009.05), but also caused a shift in the ranking of the expressions' facial attractiveness. CONCLUSION: Facial expressions confound attractiveness scores. Training on norming images generated scores less susceptible to distortion, but more difficult to interpret. Scoring facial attractiveness based on CNN seems promising, but AI solutions must be developed on CNN trained to recognize facial expressions as distractors.",
      "journal": "Orthodontics & craniofacial research",
      "year": "2024",
      "doi": "10.1111/ocr.12820",
      "authors": "Obwegeser Dorothea et al.",
      "keywords": "artificial intelligence; deep convolutional neural networks; emotions; facial attractiveness; facial expressions",
      "mesh_terms": "Humans; Female; Facial Expression; Neural Networks, Computer; Beauty; Young Adult; Adult; Face; Photography; Algorithms; Image Processing, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38825845/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11654357",
      "ft_text_length": 24721,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11654357)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38858343",
      "title": "Equity in Using Artificial Intelligence Mortality Predictions to Target Goals of Care Documentation.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) algorithms are increasingly used to target patients with elevated mortality risk scores for goals-of-care (GOC) conversations. OBJECTIVE: To evaluate the association between the presence or absence of AI-generated mortality risk scores with GOC documentation. DESIGN: Retrospective cross-sectional study at one large academic medical center between July 2021 and December 2022. PARTICIPANTS: Hospitalized adult patients with AI-defined Serious Illness Risk Indicator (SIRI) scores indicating\u2009>\u200930% 90-day mortality risk (defined as \"elevated\" SIRI) or no SIRI scores due to insufficient data. INTERVENTION: A targeted intervention to increase GOC documentation for patients with AI-generated scores predicting elevated risk of mortality. MAIN MEASURES: Odds ratios comparing GOC documentation for patients with elevated or no SIRI scores with similar severity of illness using propensity score matching and risk-adjusted mixed-effects logistic regression. KEY RESULTS: Among 13,710 patients with elevated (n\u2009=\u20093643, 27%) or no (n\u2009=\u200910,067, 73%) SIRI scores, the median age was 64\u00a0years (SD 18). Twenty-five percent were non-White, 18% had Medicaid, 43% were admitted to an intensive care unit, and 11% died during admission. Patients lacking SIRI scores were more likely to be younger (median 60 vs. 72\u00a0years, p\u2009<\u20090.0001), be non-White (29% vs. 13%, p\u2009<\u20090.0001), and have Medicaid (22% vs. 9%, p\u2009<\u20090.0001). Patients with elevated versus no SIRI scores were more likely to have GOC documentation in the unmatched (aOR 2.5, p\u2009<\u20090.0001) and propensity-matched cohorts (aOR 2.1, p\u2009<\u20090.0001). CONCLUSIONS: Using AI predictions of mortality to target GOC documentation may create differences in documentation prevalence between patients with and without AI mortality prediction scores with similar severity of illness. These finding suggest using AI to target GOC documentation may have the unintended consequence of disadvantaging severely ill patients lacking AI-generated scores from receiving targeted GOC documentation, including patients who are more likely to be non-White and have Medicaid insurance.",
      "journal": "Journal of general internal medicine",
      "year": "2024",
      "doi": "10.1007/s11606-024-08849-w",
      "authors": "Piscitello Gina M et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Male; Middle Aged; Cross-Sectional Studies; Artificial Intelligence; Retrospective Studies; Aged; Documentation; Patient Care Planning; Adult; Aged, 80 and over; Risk Assessment; Hospital Mortality",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38858343/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11576666",
      "ft_text_length": 2267,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11576666)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38876558",
      "title": "Assessing and attenuating the impact of selection bias on spatial cluster detection studies.",
      "abstract": "Spatial cluster analyses are commonly used in epidemiologic studies of case-control data to detect whether certain areas in a study region have an excess of disease risk. Case-control studies are susceptible to potential biases including selection bias, which can result from non-participation of eligible subjects in the study. However, there has been no systematic evaluation of the effects of non-participation on the findings of spatial cluster analyses. In this paper, we perform a simulation study assessing the effect of non-participation on spatial cluster analysis using the local spatial scan statistic under a variety of scenarios that vary the location and rates of study non-participation and the presence and intensity of a zone of elevated risk for disease for simulated case-control studies. We find that geographic areas of lower participation among controls than cases can greatly inflate false-positive rates for identification of artificial spatial clusters. Additionally, we find that even modest non-participation outside of a true zone of elevated risk can decrease spatial power to identify the true zone. We propose a spatial algorithm to correct for potentially spatially structured non-participation that compares the spatial distributions of the observed sample and underlying population. We demonstrate its ability to markedly decrease false positive rates in the absence of elevated risk and resist decreasing spatial sensitivity to detect true zones of elevated risk. We apply our method to a case-control study of non-Hodgkin lymphoma. Our findings suggest that greater attention should be paid to the potential effects of non-participation in spatial cluster studies.",
      "journal": "Spatial and spatio-temporal epidemiology",
      "year": "2024",
      "doi": "10.1016/j.sste.2024.100659",
      "authors": "Boyle Joseph et al.",
      "keywords": "Case-control study; Epidemiology; Local spatial scan; Non-participation; Selection bias; Spatial cluster",
      "mesh_terms": "Humans; Cluster Analysis; Case-Control Studies; Spatial Analysis; Selection Bias; Computer Simulation; Algorithms; Lymphoma, Non-Hodgkin",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, N.I.H., Intramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38876558/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11180222",
      "ft_text_length": 1700,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11180222)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38931955",
      "title": "Single-Cell Transcriptomic and Targeted Genomic Profiling Adjusted for Inflammation and Therapy Bias Reveal CRTAM and PLCB1 as Novel Hub Genes for Anti-Tumor Necrosis Factor Alpha Therapy Response in Crohn's Disease.",
      "abstract": "The lack of reliable biomarkers in response to anti-TNF\u03b1 biologicals hinders personalized therapy for Crohn's disease (CD) patients. The motivation behind our study is to shift the paradigm of anti-TNF\u03b1 biomarker discovery toward specific immune cell sub-populations using single-cell RNA sequencing and an innovative approach designed to uncover PBMCs gene expression signals, which may be masked due to the treatment or ongoing inflammation; Methods: The single-cell RNA sequencing was performed on PBMC samples from CD patients either na\u00efve to biological therapy, in remission while on adalimumab, or while on ustekinumab but previously non-responsive to adalimumab. Sieves for stringent downstream gene selection consisted of gene ontology and independent cohort genomic profiling. Replication and meta-analyses were performed using publicly available raw RNA sequencing files of sorted immune cells and an association analysis summary. Machine learning, Mendelian randomization, and oligogenic risk score methods were deployed to validate DEGs highly relevant to anti-TNF\u03b1 therapy response; Results: This study found PLCB1 in CD4+ T cells and CRTAM in double-negative T cells, which met the stringent statistical thresholds throughout the analyses. An additional assessment proved causal inference of both genes in response to anti-TNF\u03b1 therapy; Conclusions: This study, jointly with an innovative design, uncovered novel candidate genes in the anti-TNF\u03b1 response landscape of CD, potentially obscured by therapy or inflammation.",
      "journal": "Pharmaceutics",
      "year": "2024",
      "doi": "10.3390/pharmaceutics16060835",
      "authors": "Gorenjak Mario et al.",
      "keywords": "Crohn\u2019s disease; adalimumab; inflammatory bowel diseases; single-cell gene expression analysis; tumor necrosis factor alpha",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38931955/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11207411",
      "ft_text_length": 47455,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11207411)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38955389",
      "title": "Advancing equity in breast cancer care: natural language processing for analysing treatment outcomes in under-represented populations.",
      "abstract": "OBJECTIVE: The study aimed to develop natural language processing (NLP) algorithms to automate extracting patient-centred breast cancer treatment outcomes from clinical notes in electronic health records (EHRs), particularly for women from under-represented populations. METHODS: The study used clinical notes from 2010 to 2021 from a tertiary hospital in the USA. The notes were processed through various NLP techniques, including vectorisation methods (term frequency-inverse document frequency (TF-IDF), Word2Vec, Doc2Vec) and classification models (support vector classification, K-nearest neighbours (KNN), random forest (RF)). Feature selection and optimisation through random search and fivefold cross-validation were also conducted. RESULTS: The study annotated 100 out of 1000 clinical notes, using 970 notes to build the text corpus. TF-IDF and Doc2Vec combined with RF showed the highest performance, while Word2Vec was less effective. RF classifier demonstrated the best performance, although with lower recall rates, suggesting more false negatives. KNN showed lower recall due to its sensitivity to data noise. DISCUSSION: The study highlights the significance of using NLP in analysing clinical notes to understand breast cancer treatment outcomes in under-represented populations. The TF-IDF and Doc2Vec models were more effective in capturing relevant information than Word2Vec. The study observed lower recall rates in RF models, attributed to the dataset's imbalanced nature and the complexity of clinical notes. CONCLUSION: The study developed high-performing NLP pipeline to capture treatment outcomes for breast cancer in under-represented populations, demonstrating the importance of document-level vectorisation and ensemble methods in clinical notes analysis. The findings provide insights for more equitable healthcare strategies and show the potential for broader NLP applications in clinical settings.",
      "journal": "BMJ health & care informatics",
      "year": "2024",
      "doi": "10.1136/bmjhci-2023-100966",
      "authors": "Park Jung In et al.",
      "keywords": "BMJ health informatics; artificial intelligence; health equity; machine learning; nursing informatics",
      "mesh_terms": "Humans; Natural Language Processing; Breast Neoplasms; Female; Electronic Health Records; Algorithms; Treatment Outcome; United States",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38955389/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11218025",
      "ft_text_length": 28475,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11218025)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38960729",
      "title": "Fair prediction of 2-year stroke risk in patients with atrial fibrillation.",
      "abstract": "OBJECTIVE: This study aims to develop machine learning models that provide both accurate and equitable predictions of 2-year stroke risk for patients with atrial fibrillation across diverse racial groups. MATERIALS AND METHODS: Our study utilized structured electronic health records (EHR) data from the All of Us Research Program. Machine learning models (LightGBM) were utilized to capture the relations between stroke risks and the predictors used by the widely recognized CHADS2 and CHA2DS2-VASc scores. We mitigated the racial disparity by creating a representative tuning set, customizing tuning criteria, and setting binary thresholds separately for subgroups. We constructed a hold-out test set that not only supports temporal validation but also includes a larger proportion of Black/African Americans for fairness validation. RESULTS: Compared to the original CHADS2 and CHA2DS2-VASc scores, significant improvements were achieved by modeling their predictors using machine learning models (Area Under the Receiver Operating Characteristic curve from near 0.70 to above 0.80). Furthermore, applying our disparity mitigation strategies can effectively enhance model fairness compared to the conventional cross-validation approach. DISCUSSION: Modeling CHADS2 and CHA2DS2-VASc risk factors with LightGBM and our disparity mitigation strategies achieved decent discriminative performance and excellent fairness performance. In addition, this approach can provide a complete interpretation of each predictor. These highlight its potential utility in clinical practice. CONCLUSIONS: Our research presents a practical example of addressing clinical challenges through the All of Us Research Program data. The disparity mitigation framework we proposed is adaptable across various models and data modalities, demonstrating broad potential in clinical informatics.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2024",
      "doi": "10.1093/jamia/ocae170",
      "authors": "Gao Jifan et al.",
      "keywords": "atrial fibrillation; bias; fairness; machine learning; stroke",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Atrial Fibrillation; Black or African American; Electronic Health Records; Machine Learning; Risk Assessment; Risk Factors; ROC Curve; Stroke; Racial Groups",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38960729/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11631105",
      "ft_text_length": 1892,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11631105)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38961501",
      "title": "Harnessing health information technology to promote equitable care for patients with limited English proficiency and complex care needs.",
      "abstract": "BACKGROUND: Patients with language barriers encounter healthcare disparities, which may be alleviated by leveraging interpreter skills to reduce cultural, language, and literacy barriers through improved bidirectional communication. Evidence supports the use of in-person interpreters, especially for interactions involving patients with complex care needs. Unfortunately, due to interpreter shortages and clinician underuse of interpreters, patients with language barriers frequently do not get the language services they need or are entitled to. Health information technologies (HIT), including artificial intelligence (AI), have the potential to streamline processes, prompt clinicians to utilize in-person interpreters, and support prioritization. METHODS: From May 1, 2023, to June 21, 2024, a single-center stepped wedge cluster randomized trial will be conducted within 35 units of Saint Marys Hospital & Methodist Hospital at Mayo Clinic in Rochester, Minnesota. The units include medical, surgical, trauma, and mixed ICUs and hospital floors that admit acute medical and surgical care patients as well as the emergency department (ED). The transitions between study phases will be initiated at 60-day intervals resulting in a 12-month study period. Units in the control group will receive standard care and rely on clinician initiative to request interpreter services. In the intervention group, the study team will generate a daily list of adult inpatients with language barriers, order the list based on their complexity scores (from highest to lowest), and share it with interpreter services, who will send a secure chat message to the bedside nurse. This engagement will be triggered by a predictive machine-learning algorithm based on a palliative care score, supplemented by other predictors of complexity including length of stay and level of care as well as procedures, events, and clinical notes. DISCUSSION: This pragmatic clinical trial approach will integrate a predictive machine-learning algorithm into a workflow process and evaluate the effectiveness of the intervention. We will compare the use of in-person interpreters and time to first interpreter use between the control and intervention groups. TRIAL REGISTRATION: NCT05860777. May 16, 2023.",
      "journal": "Trials",
      "year": "2024",
      "doi": "10.1186/s13063-024-08254-y",
      "authors": "Strechen Inna et al.",
      "keywords": "AI; Complex care needs; Complexity score; Healthcare disparities; In-person interpreter; Language services; Non-English language preference (NELP)",
      "mesh_terms": "Humans; Limited English Proficiency; Healthcare Disparities; Medical Informatics; Translating; Artificial Intelligence; Randomized Controlled Trials as Topic; Communication Barriers; Pragmatic Clinical Trials as Topic",
      "pub_types": "Journal Article; Clinical Trial Protocol",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38961501/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11223355",
      "ft_text_length": 31246,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11223355)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39019301",
      "title": "Recommendations to promote fairness and inclusion in biomedical AI research and clinical use.",
      "abstract": "OBJECTIVE: Understanding and quantifying biases when designing and implementing actionable approaches to increase fairness and inclusion is critical for artificial intelligence (AI) in biomedical applications. METHODS: In this Special Communication, we discuss how bias is introduced at different stages of the development and use of AI applications in biomedical sciences and health care. We describe various AI applications and their implications for fairness and inclusion in sections on 1) Bias in Data Source Landscapes, 2) Algorithmic Fairness, 3) Uncertainty in AI Predictions, 4) Explainable AI for Fairness and Equity, and 5) Sociological/Ethnographic Issues in Data and Results Representation. RESULTS: We provide recommendations to address biases when developing and using AI in clinical applications. CONCLUSION: These recommendations can be applied to informatics research and practice to foster more equitable and inclusive health care systems and research discoveries.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104693",
      "authors": "Griffin Ashley C et al.",
      "keywords": "Artificial Intelligence; Bias; Digital Divide; Equity; Fairness; Inclusion; Uncertainty Quantification",
      "mesh_terms": "Artificial Intelligence; Humans; Biomedical Research; Algorithms; Bias; Medical Informatics; Delivery of Health Care",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39019301/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11402591",
      "ft_text_length": 983,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11402591)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39039218",
      "title": "Autonomous artificial intelligence for diabetic eye disease increases access and health equity in underserved populations.",
      "abstract": "Diabetic eye disease (DED) is a leading cause of blindness in the world. Annual DED testing is recommended for adults with diabetes, but adherence to this guideline has historically been low. In 2020, Johns Hopkins Medicine (JHM) began deploying autonomous AI for DED testing. In this study, we aimed to determine whether autonomous AI implementation was associated with increased adherence to annual DED testing, and how this differed across patient populations. JHM primary care sites were categorized as \"non-AI\" (no autonomous AI deployment) or \"AI-switched\" (autonomous AI deployment by 2021). We conducted a propensity score weighting analysis to compare change in adherence rates from 2019 to 2021 between non-AI and AI-switched sites. Our study included all adult patients with diabetes (>17,000) managed within JHM and has three major findings. First, AI-switched sites experienced a 7.6 percentage point greater increase in DED testing than non-AI sites from 2019 to 2021 (p\u2009<\u20090.001). Second, the adherence rate for Black/African Americans increased by 12.2 percentage points within AI-switched sites but decreased by 0.6% points within non-AI sites (p\u2009<\u20090.001), suggesting that autonomous AI deployment improved access to retinal evaluation for historically disadvantaged populations. Third, autonomous AI is associated with improved health equity, e.g. the adherence rate gap between Asian Americans and Black/African Americans shrank from 15.6% in 2019 to 3.5% in 2021. In summary, our results from real-world deployment in a large integrated healthcare system suggest that autonomous AI is associated with improvement in overall DED testing adherence, patient access, and health equity.",
      "journal": "NPJ digital medicine",
      "year": "2024",
      "doi": "10.1038/s41746-024-01197-3",
      "authors": "Huang Jane J et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39039218/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11263546",
      "ft_text_length": 21307,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11263546)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39100239",
      "title": "From national and regional commitments to global impact: artificial intelligence for equitable public health at the G20.",
      "abstract": "",
      "journal": "Revista panamericana de salud publica = Pan American journal of public health",
      "year": "2024",
      "doi": "10.26633/RPSP.2024.73",
      "authors": "da Silva Jarbas Barbosa et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39100239/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11292443",
      "ft_text_length": 12675,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11292443)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39112910",
      "title": "How do deep-learning models generalize across populations? Cross-ethnicity generalization of COPD detection.",
      "abstract": "OBJECTIVES: To evaluate the performance and potential biases of deep-learning models in detecting chronic obstructive pulmonary disease (COPD) on chest CT scans across different ethnic groups, specifically non-Hispanic White (NHW) and African American (AA) populations. MATERIALS AND METHODS: Inspiratory chest CT and clinical data from 7549 Genetic epidemiology of COPD individuals (mean age 62 years old, 56-69 interquartile range), including 5240 NHW and 2309 AA individuals, were retrospectively analyzed. Several factors influencing COPD binary classification performance on different ethnic populations were examined: (1) effects of training population: NHW-only, AA-only, balanced set (half NHW, half AA) and the entire set (NHW\u2009+\u2009AA all); (2) learning strategy: three supervised learning (SL) vs. three self-supervised learning (SSL) methods. Distribution shifts across ethnicity were further assessed for the top-performing methods. RESULTS: The learning strategy significantly influenced model performance, with SSL methods achieving higher performances compared to SL methods (p\u2009<\u20090.001), across all training configurations. Training on balanced datasets containing NHW and AA individuals resulted in improved model performance compared to population-specific datasets. Distribution shifts were found between ethnicities for the same health status, particularly when models were trained on nearest-neighbor contrastive SSL. Training on a balanced dataset resulted in fewer distribution shifts across ethnicity and health status, highlighting its efficacy in reducing biases. CONCLUSION: Our findings demonstrate that utilizing SSL methods and training on large and balanced datasets can enhance COPD detection model performance and reduce biases across diverse ethnic populations. These findings emphasize the importance of equitable AI-driven healthcare solutions for COPD diagnosis. CRITICAL RELEVANCE STATEMENT: Self-supervised learning coupled with balanced datasets significantly improves COPD detection model performance, addressing biases across diverse ethnic populations and emphasizing the crucial role of equitable AI-driven healthcare solutions. KEY POINTS: Self-supervised learning methods outperform supervised learning methods, showing higher AUC values (p\u2009<\u20090.001). Balanced datasets with non-Hispanic White and African American individuals improve model performance. Training on diverse datasets enhances COPD detection accuracy. Ethnically diverse datasets reduce bias in COPD detection models. SimCLR models mitigate biases in COPD detection across ethnicities.",
      "journal": "Insights into imaging",
      "year": "2024",
      "doi": "10.1186/s13244-024-01781-x",
      "authors": "D Almeida Silvia et al.",
      "keywords": "Artificial intelligence; Chronic obstructive pulmonary disease; Computed tomography; Deep learning; Ethnicity",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39112910/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11306482",
      "ft_text_length": 32570,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11306482)",
      "ft_reason": "Excluded: insufficient approach content (1 indicators)"
    },
    {
      "pmid": "39119159",
      "title": "Mobilizing health equity through Computable Biomedical Knowledge (CBK): a call to action to the library, information sciences, and health informatics communities.",
      "abstract": "The twin pandemics of COVID-19 and structural racism brought into focus health disparities and disproportionate impacts of disease on communities of color. Health equity has subsequently emerged as a priority. Recognizing that the future of health care will be informed by advanced information technologies including artificial intelligence (AI), machine learning, and algorithmic applications, the authors argue that to advance towards states of improved health equity, health information professionals need to engage in and encourage the conduct of research at the intersections of health equity, health disparities, and computational biomedical knowledge (CBK) applications. Recommendations are provided with a means to engage in this mobilization effort.",
      "journal": "Journal of the Medical Library Association : JMLA",
      "year": "2024",
      "doi": "10.5195/jmla.2024.1836",
      "authors": "Allee Nancy J et al.",
      "keywords": "Algorithms; Artificial Intelligence; Computing Methodologies; Health Equity; Health Inequalities; Health Status Disparities; Information Science; Library Science; Machine Learning",
      "mesh_terms": "Humans; Health Equity; COVID-19; Medical Informatics; SARS-CoV-2; Libraries, Medical; Artificial Intelligence",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39119159/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11305479",
      "ft_text_length": 14102,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11305479)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39121467",
      "title": "Twenty-Five Years of Progress-Lessons Learned From JMIR Publications to Address Gender Parity in Digital Health Authorships: Bibliometric Analysis.",
      "abstract": "BACKGROUND: Digital health research plays a vital role in advancing equitable health care. The diversity of research teams is thereby instrumental in capturing societal challenges, increasing productivity, and reducing bias in algorithms. Despite its importance, the gender distribution within digital health authorship remains largely unexplored. OBJECTIVE: This study aimed to investigate the gender distribution among first and last authors in digital health research, thereby identifying predicting factors of female authorship. METHODS: This bibliometric analysis examined the gender distribution across 59,980 publications from 1999 to 2023, spanning 42 digital health journals indexed in the Web of Science. To identify strategies ensuring equality in research, a detailed comparison of gender representation in JMIR journals was conducted within the field, as well as against a matched sample. Two-tailed Welch 2-sample t tests, Wilcoxon rank sum tests, and chi-square tests were used to assess differences. In addition, odds ratios were calculated to identify predictors of female authorship. RESULTS: The analysis revealed that 37% of first authors and 30% of last authors in digital health were female. JMIR journals demonstrated a higher representation, with 49% of first authors and 38% of last authors being female, yielding odds ratios of 1.96 (95% CI 1.90-2.03; P<.001) and 1.78 (95% CI 1.71-1.84; P<.001), respectively. Since 2008, JMIR journals have consistently featured a greater proportion of female first authors than male counterparts. Other factors that predicted female authorship included having female authors in other relevant positions and gender discordance, given the higher rate of male last authors in the field. CONCLUSIONS: There was an evident shift toward gender parity across publications in digital health, particularly from the publisher JMIR Publications. The specialized focus of its sister journals, equitable editorial policies, and transparency in the review process might contribute to these achievements. Further research is imperative to establish causality, enabling the replication of these successful strategies across other scientific fields to bridge the gender gap in digital health effectively.",
      "journal": "Journal of medical Internet research",
      "year": "2024",
      "doi": "10.2196/58950",
      "authors": "Meyer Annika et al.",
      "keywords": "JMIR Publications; Web of Science; algorithmic bias reduction; article; articles; author; authors; authorships; bibliometric; bibliometric analysis; comparative analysis; comparison; control group; digital health; diversity; equality; gender; gender distribution; gender gap; gender representation",
      "mesh_terms": "Authorship; Bibliometrics; Humans; Female; Male; Periodicals as Topic; Sex Factors; Digital Health",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39121467/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11344179",
      "ft_text_length": 22701,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11344179)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39127820",
      "title": "Disparities in clinical studies of AI enabled applications from a global perspective.",
      "abstract": "Artificial intelligence (AI) has been extensively researched in medicine, but its practical application remains limited. Meanwhile, there are various disparities in existing AI-enabled clinical studies, which pose a challenge to global health equity. In this study, we conducted an in-depth analysis of the geo-economic distribution of 159 AI-enabled clinical studies, as well as the gender disparities among these studies. We aim to reveal these disparities from a global literature perspective, thus highlighting the need for equitable access to medical AI technologies.",
      "journal": "NPJ digital medicine",
      "year": "2024",
      "doi": "10.1038/s41746-024-01212-7",
      "authors": "Yang Rui et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39127820/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11316833",
      "ft_text_length": 7350,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11316833)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39132618",
      "title": "It Is Time for a Simplified Approach to Hepatitis B Elimination.",
      "abstract": "BACKGROUND AND AIMS: Hepatitis B virus (HBV) infection continues to threaten millions of lives across the globe, despite universal vaccination efforts. Current guidelines for screening, vaccination, and treatment are complex and have left too many people undiagnosed or improperly managed. Antiviral therapy has been shown to significantly reduce the incidence of liver-related complications, including liver cancer. However, the complexity of existing guidelines can make it difficult to identify which patients to target for treatment, and recommendations that are difficult to implement in real-world settings pose a barrier to eligible patients to receive therapy and contribute to health disparities in HBV care. The goal of this global expert panel was to gain consensus on a streamlined approach to HBV care to facilitate implementation of HBV intervention and treatment, especially in the primary care setting. METHODS: A group of 8 liver and infectious disease specialists attended a meeting in January 2021 with the objective of gaining consensus on a streamlined algorithm for HBV care that would encourage implementation of HBV intervention and treatment. RESULTS: We have created a comprehensive perspective highlighting screening optimization, diagnostic workup, treatment, and monitoring. This treatment algorithm is designed to provide a streamlined visual pathway for risk stratification and management of patients with HBV that can be adapted in various care settings. CONCLUSION: Simplification of guidelines will be critical to achieving health equity to address this public health threat and achieve HBV elimination.",
      "journal": "Gastro hep advances",
      "year": "2023",
      "doi": "10.1016/j.gastha.2022.10.004",
      "authors": "Dieterich Douglas et al.",
      "keywords": "HBV algorithm; Hepatitis B; Simplified treatment",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39132618/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11307636",
      "ft_text_length": 32629,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11307636)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39134999",
      "title": "The impact of health disparities on peripheral vascular access outcomes in hospitalized patients: an observational study.",
      "abstract": "BACKGROUND: Placement of peripheral intravenous catheters (PIVC) is a routine procedure in hospital settings. The primary objective is to explore the relationship between healthcare inequities and PIVC outcomes. METHODS: This study was a multicenter, observational analysis of adults with PIVC access established in the emergency department requiring inpatient admission between January 1st, 2021, and January 31st, 2023, in metro Detroit, Michigan, United States. Epidemiological, demographic, therapeutic, clinical, and outcomes data were collected. Health disparities were defined by the National Institute on Minority Health and Health Disparities. The primary outcome was the proportion of PIVC dwell time to hospitalization length of stay, expressed as the proportion of dwell time (hours) to hospital stay (hours) x 100%. Multivariable linear regression and a machine learning model were used for variable selection. Subsequently, a multivariate linear regression analysis was utilized to adjust for confounders and best estimate the true effect of each variable. RESULTS: Between January 1st, 2021, and January 31st, 2023, our study analyzed 144,524 ED encounters, with an average patient age of 65.7 years and 53.4% female. Racial demographics showed 67.2% White, and 27.0% Black, with the remaining identifying as Asian, American Indian Alaska Native, or other races. The median proportion of PIVC dwell time to hospital length of stay was 0.88, with individuals identifying as Asian having the highest ratio (0.94) and Black individuals the lowest (0.82). Black females had a median dwell time to stay ratio of 0.76, significantly lower than White males at 0.93 (p\u2009<\u20090.001). After controlling for confounder variables, a multivariable linear regression demonstrated that Black males and White males had a 10.0% and 19.6% greater proportion of dwell to stay, respectively, compared to Black females (p\u2009<\u20090.001). CONCLUSIONS: Black females face the highest risk of compromised PIVC functionality, resulting in approximately one full day of less reliable PIVC access than White males. To comprehensively address and rectify these disparities, further research is imperative to improve understanding of the clinical impact of healthcare inequities on PIVC access. Moreover, it is essential to formulate effective strategies to mitigate these disparities and ensure equitable healthcare outcomes for all individuals.",
      "journal": "International journal for equity in health",
      "year": "2024",
      "doi": "10.1186/s12939-024-02213-4",
      "authors": "Mielke Nicholas et al.",
      "keywords": "Healthcare inequities; Hospitalized patients; IV functionality; Peripheral intravenous catheters; Racial disparities",
      "mesh_terms": "Humans; Female; Male; Aged; Middle Aged; Healthcare Disparities; Michigan; Catheterization, Peripheral; Length of Stay; Hospitalization; Adult; Emergency Service, Hospital",
      "pub_types": "Journal Article; Observational Study; Multicenter Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39134999/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11318308",
      "ft_text_length": 33331,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11318308)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39137028",
      "title": "Ameliorating Racial Disparities in HIV Prevention via a Nurse-Led, AI-Enhanced Program for Pre-Exposure Prophylaxis Utilization Among Black Cisgender Women: Protocol for a Mixed Methods Study.",
      "abstract": "BACKGROUND: HIV pre-exposure prophylaxis (PrEP) is a critical biomedical strategy to prevent HIV transmission among cisgender women. Despite its proven effectiveness, Black cisgender women remain significantly underrepresented throughout the PrEP care continuum, facing barriers such as limited access to care, medical mistrust, and intersectional racial or HIV stigma. Addressing these disparities is vital to improving HIV prevention outcomes within this community. On the other hand, nurse practitioners (NPs) play a pivotal role in PrEP utilization but are underrepresented due to a lack of awareness, a lack of human resources, and insufficient support. Equipped with the rapid evolution of artificial intelligence (AI) and advanced large language models, chatbots effectively facilitate health care communication and linkage to care in various domains, including HIV prevention and PrEP care. OBJECTIVE: Our study harnesses NPs' holistic care capabilities and the power of AI through natural language processing algorithms, providing targeted, patient-centered facilitation for PrEP care. Our overarching goal is to create a nurse-led, stakeholder-inclusive, and AI-powered program to facilitate PrEP utilization among Black cisgender women, ultimately enhancing HIV prevention efforts in this vulnerable group in 3 phases. This project aims to mitigate health disparities and advance innovative, technology-based solutions. METHODS: The study uses a mixed methods design involving semistructured interviews with key stakeholders, including 50 PrEP-eligible Black women, 10 NPs, and a community advisory board representing various socioeconomic backgrounds. The AI-powered chatbot is developed using HumanX technology and SmartBot360's Health Insurance Portability and Accountability Act-compliant framework to ensure data privacy and security. The study spans 18 months and consists of 3 phases: exploration, development, and evaluation. RESULTS: As of May 2024, the institutional review board protocol for phase 1 has been approved. We plan to start recruitment for Black cisgender women and NPs in September 2024, with the aim to collect information to understand their preferences regarding chatbot development. While institutional review board approval for phases 2 and 3 is still in progress, we have made significant strides in networking for participant recruitment. We plan to conduct data collection soon, and further updates on the recruitment and data collection progress will be provided as the study advances. CONCLUSIONS: The AI-powered chatbot offers a novel approach to improving PrEP care utilization among Black cisgender women, with opportunities to reduce barriers to care and facilitate a stigma-free environment. However, challenges remain regarding health equity and the digital divide, emphasizing the need for culturally competent design and robust data privacy protocols. The implications of this study extend beyond PrEP care, presenting a scalable model that can address broader health disparities. INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): PRR1-10.2196/59975.",
      "journal": "JMIR research protocols",
      "year": "2024",
      "doi": "10.2196/59975",
      "authors": "Zhang Chen et al.",
      "keywords": "AI; AIDS; Black; Black cisgender women; Black women; HIV; HIV pre-exposure prophylaxis; HIV prevention; HumanX technology; PrEP; PrEP care; artificial intelligence; biomedical; chatbot; cisgender; effectiveness; health care interventions; medical mistrust; nurse; nurse practitioners",
      "mesh_terms": "Humans; Female; HIV Infections; Pre-Exposure Prophylaxis; Artificial Intelligence; Black or African American; Healthcare Disparities; Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39137028/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11350295",
      "ft_text_length": 27052,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11350295)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39166973",
      "title": "Improving Fairness of Automated Chest Radiograph Diagnosis by Contrastive Learning.",
      "abstract": "Purpose To develop an artificial intelligence model that uses supervised contrastive learning (SCL) to minimize bias in chest radiograph diagnosis. Materials and Methods In this retrospective study, the proposed method was evaluated on two datasets: the Medical Imaging and Data Resource Center (MIDRC) dataset with 77\u2009887 chest radiographs in 27\u2009796 patients collected as of April 20, 2023, for COVID-19 diagnosis and the National Institutes of Health ChestX-ray14 dataset with 112\u2009120 chest radiographs in 30\u2009805 patients collected between 1992 and 2015. In the ChestX-ray14 dataset, thoracic abnormalities included atelectasis, cardiomegaly, effusion, infiltration, mass, nodule, pneumonia, pneumothorax, consolidation, edema, emphysema, fibrosis, pleural thickening, and hernia. The proposed method used SCL with carefully selected positive and negative samples to generate fair image embeddings, which were fine-tuned for subsequent tasks to reduce bias in chest radiograph diagnosis. The method was evaluated using the marginal area under the receiver operating characteristic curve difference (\u2206mAUC). Results The proposed model showed a significant decrease in bias across all subgroups compared with the baseline models, as evidenced by a paired t test (P < .001). The \u2206mAUCs obtained by the proposed method were 0.01 (95% CI: 0.01, 0.01), 0.21 (95% CI: 0.21, 0.21), and 0.10 (95% CI: 0.10, 0.10) for sex, race, and age subgroups, respectively, on the MIDRC dataset and 0.01 (95% CI: 0.01, 0.01) and 0.05 (95% CI: 0.05, 0.05) for sex and age subgroups, respectively, on the ChestX-ray14 dataset. Conclusion Employing SCL can mitigate bias in chest radiograph diagnosis, addressing concerns of fairness and reliability in deep learning-based diagnostic methods. Keywords: Thorax, Diagnosis, Supervised Learning, Convolutional Neural Network (CNN), Computer-aided Diagnosis (CAD) Supplemental material is available for this article. \u00a9 RSNA, 2024 See also the commentary by Johnson in this issue.",
      "journal": "Radiology. Artificial intelligence",
      "year": "2024",
      "doi": "10.1148/ryai.230342",
      "authors": "Lin Mingquan et al.",
      "keywords": "Computer-aided Diagnosis (CAD); Convolutional Neural Network (CNN); Diagnosis; Supervised Learning; Thorax",
      "mesh_terms": "Humans; Radiography, Thoracic; Retrospective Studies; Female; Male; Middle Aged; Aged; COVID-19; Adult; Artificial Intelligence; SARS-CoV-2; Radiographic Image Interpretation, Computer-Assisted; Supervised Machine Learning; Adolescent; Young Adult",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39166973/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11449211",
      "ft_text_length": 2017,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11449211)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39199522",
      "title": "Leveraging Artificial Intelligence to Optimize Transcranial Direct Current Stimulation for Long COVID Management: A Forward-Looking Perspective.",
      "abstract": "Long COVID (Coronavirus disease), affecting millions globally, presents unprecedented challenges to healthcare systems due to its complex, multifaceted nature and the lack of effective treatments. This perspective review explores the potential of artificial intelligence (AI)-guided transcranial direct current stimulation (tDCS) as an innovative approach to address the urgent need for effective Long COVID management. The authors examine how AI could optimize tDCS protocols, enhance clinical trial design, and facilitate personalized treatment for the heterogeneous manifestations of Long COVID. Key areas discussed include AI-driven personalization of tDCS parameters based on individual patient characteristics and real-time symptom fluctuations, the use of machine learning for patient stratification, and the development of more sensitive outcome measures in clinical trials. This perspective addresses ethical considerations surrounding data privacy, algorithmic bias, and equitable access to AI-enhanced treatments. It also explores challenges and opportunities for implementing AI-guided tDCS across diverse healthcare settings globally. Future research directions are outlined, including the need for large-scale validation studies and investigations of long-term efficacy and safety. The authors argue that while AI-guided tDCS shows promise for addressing the complex nature of Long COVID, significant technical, ethical, and practical challenges remain. They emphasize the importance of interdisciplinary collaboration, patient-centered approaches, and a commitment to global health equity in realizing the potential of this technology. This perspective article provides a roadmap for researchers, clinicians, and policymakers involved in developing and implementing AI-guided neuromodulation therapies for Long COVID and potentially other neurological and psychiatric conditions.",
      "journal": "Brain sciences",
      "year": "2024",
      "doi": "10.3390/brainsci14080831",
      "authors": "Rudroff Thorsten et al.",
      "keywords": "artificial intelligence; brain stimulation; long COVID; neuroimaging",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39199522/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11353063",
      "ft_text_length": 88544,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11353063)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39222370",
      "title": "Socioeconomic disparities in kidney transplant access for patients with end-stage kidney disease within the All of Us Research Program.",
      "abstract": "OBJECTIVES: Disparity in kidney transplant access has been demonstrated by a disproportionately low rate of kidney transplantation in socioeconomically disadvantaged patients. However, the information is not from national representative populations with end-stage kidney disease (ESKD). We aim to examine whether socioeconomic disparity for kidney transplant access exists by utilizing data from the All of Us Research Program. MATERIALS AND METHODS: We analyzed data of adult ESKD patients using the All of Us Researcher Workbench. The association of socioeconomic data including types of health insurance, levels of education, and household incomes with kidney transplant access was evaluated by multivariable logistic regression analysis adjusted by baseline demographic, medical comorbidities, and behavioral information. RESULTS: Among 4078 adults with ESKD, mean diagnosis age was 54 and 51.64% were male. The majority had Medicare (39.6%), were non-graduate college (75.79%), and earned $10\u00a0000-24\u00a0999 annual income (20.16%). After adjusting for potential confounders, insurance status emerged as a significant predictor of kidney transplant access. Individuals covered by Medicaid (adjusted odds ratio [AOR] 0.45; 95% confidence interval [CI], 0.35-0.58; P-value < .001) or uninsured (AOR 0.21; 95% CI, 0.12-0.37; P-value < .001) exhibited lower odds of transplantation compared to those with private insurance. DISCUSSION/CONCLUSION: Our findings reveal the influence of insurance status and socioeconomic factors on access to kidney transplantation among ESKD patients. Addressing these disparities through expanded insurance coverage and improved healthcare access is vital for promoting equitable treatment and enhancing health outcomes in vulnerable populations.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2024",
      "doi": "10.1093/jamia/ocae178",
      "authors": "Wang Jiayuan et al.",
      "keywords": "disparities; end-stage kidney disease; healthcare access; inequity; kidney transplantation; machine learning; socioeconomic factors",
      "mesh_terms": "Humans; Kidney Transplantation; Kidney Failure, Chronic; Male; United States; Female; Middle Aged; Health Services Accessibility; Healthcare Disparities; Socioeconomic Factors; Adult; Aged; Insurance Coverage; Medicaid; Logistic Models; Socioeconomic Disparities in Health",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39222370/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11631050",
      "ft_text_length": 1788,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11631050)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39229549",
      "title": "Artificial intelligence-driven surgical innovation: A catalyst for medical equity.",
      "abstract": "",
      "journal": "Annals of gastroenterological surgery",
      "year": "2024",
      "doi": "10.1002/ags3.12827",
      "authors": "Chiu Si-Wai Vivian et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39229549/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11368485",
      "ft_text_length": 4139,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11368485)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39242068",
      "title": "A deep learning-based approach for unbiased kinematic analysis in CNS injury.",
      "abstract": "Traumatic spinal cord injury (SCI) is a devastating condition that impacts over 300,000 individuals in the US alone. Depending on the severity of the injury, SCI can lead to varying degrees of sensorimotor deficits and paralysis. Despite advances in our understanding of the underlying pathological mechanisms of SCI and the identification of promising molecular targets for repair and functional restoration, few therapies have made it into clinical use. To improve the success rate of clinical translation, more robust, sensitive, and reproducible means of functional assessment are required. The gold standards for the evaluation of locomotion in rodents with SCI are the Basso Beattie Bresnahan (BBB) scale and Basso Mouse Scale (BMS). To overcome the shortcomings of current methods, we developed two separate markerless kinematic analysis paradigms in mice, MotorBox and MotoRater, based on deep-learning algorithms generated with the DeepLabCut open-source toolbox. The MotorBox system uses an originally designed, custom-made chamber, and the MotoRater system was implemented on a commercially available MotoRater device. We validated the MotorBox and MotoRater systems by comparing them with the traditional BMS test and extracted metrics of movement and gait that can provide an accurate and sensitive representation of mouse locomotor function post-injury, while eliminating investigator bias and variability. The integration of MotorBox and/or MotoRater assessments with BMS scoring will provide a much wider range of information on specific aspects of locomotion, ensuring the accuracy, rigor, and reproducibility of behavioral outcomes after SCI.",
      "journal": "Experimental neurology",
      "year": "2024",
      "doi": "10.1016/j.expneurol.2024.114944",
      "authors": "Ascona Maureen C et al.",
      "keywords": "DeepLabCut; Kinematics; Machine learning; Spinal cord injury; Traumatic brain injury",
      "mesh_terms": "Animals; Deep Learning; Mice; Biomechanical Phenomena; Spinal Cord Injuries; Mice, Inbred C57BL; Female; Locomotion",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39242068/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12576851",
      "ft_text_length": 1654,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12576851)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39312289",
      "title": "Evaluating the Bias in Hospital Data: Automatic Preprocessing of Patient Pathways Algorithm Development and Validation Study.",
      "abstract": "BACKGROUND: The optimization of patient care pathways is crucial for hospital managers in the context of a scarcity of medical resources. Assuming unlimited capacities, the pathway of a patient would only be governed by pure medical logic to meet at best the patient's needs. However, logistical limitations (eg, resources such as inpatient beds) are often associated with delayed treatments and may ultimately affect patient pathways. This is especially true for unscheduled patients-when a patient in the emergency department needs to be admitted to another medical unit without disturbing the flow of planned hospitalizations. OBJECTIVE: In this study, we proposed a new framework to automatically detect activities in patient pathways that may be unrelated to patients' needs but rather induced by logistical limitations. METHODS: The scientific contribution lies in a method that transforms a database of historical pathways with bias into 2 databases: a labeled pathway database where each activity is labeled as relevant (related to a patient's needs) or irrelevant (induced by logistical limitations) and a corrected pathway database where each activity corresponds to the activity that would occur assuming unlimited resources. The labeling algorithm was assessed through medical expertise. In total, 2 case studies quantified the impact of our method of preprocessing health care data using process mining and discrete event simulation. RESULTS: Focusing on unscheduled patient pathways, we collected data covering 12 months of activity at the Groupe Hospitalier Bretagne Sud in France. Our algorithm had 87% accuracy and demonstrated its usefulness for preprocessing traces and obtaining a clean database. The 2 case studies showed the importance of our preprocessing step before any analysis. The process graphs of the processed data had, on average, 40% (SD 10%) fewer variants than the raw data. The simulation revealed that 30% of the medical units had >1 bed difference in capacity between the processed and raw data. CONCLUSIONS: Patient pathway data reflect the actual activity of hospitals that is governed by medical requirements and logistical limitations. Before using these data, these limitations should be identified and corrected. We anticipate that our approach can be generalized to obtain unbiased analyses of patient pathways for other hospitals.",
      "journal": "JMIR medical informatics",
      "year": "2024",
      "doi": "10.2196/58978",
      "authors": "Uhl Laura et al.",
      "keywords": "bed management; framework; health care data; patient pathway; preprocessing",
      "mesh_terms": "Humans; Algorithms; Critical Pathways; Data Mining; Bias; Emergency Service, Hospital; Databases, Factual",
      "pub_types": "Journal Article; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39312289/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11459108",
      "ft_text_length": 71244,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11459108)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39331661",
      "title": "Trust as moral currency: Perspectives of health researchers in sub-Saharan Africa on strategies to promote equitable data sharing.",
      "abstract": "Groundbreaking data-sharing techniques and quick access to stored research data from the African continent are highly beneficial to create diverse unbiased datasets to inform digital health technologies and artificial intelligence in healthcare. Yet health researchers in sub-Saharan Africa (SSA) experience individual and collective challenges that render them cautious and even hesitant to share data despite acknowledging the public health benefits of sharing. This qualitative study reports on the perspectives of health researchers regarding strategies to mitigate these challenges. In-depth interviews were conducted via Microsoft Teams with 16 researchers from 16 different countries across SSA between July 2022 and April 2023. Purposive and snowball sampling techniques were used to invite participants via email. Recorded interviews were transcribed, cleaned, coded and managed through Atlas.ti.22. Thematic Analysis was used to analyse the data. Three recurrent themes and several subthemes emerged around strategies to improve governance of data sharing. The main themes identified were (1) Strategies for change at a policy level: guideline development, (2) Strengthening data governance to improve data quality and (3) Reciprocity: towards equitable data sharing. Building trust is central to the promotion of data sharing amongst researchers on the African continent and with global partners. This can be achieved by enhancing research integrity and strengthening micro and macro level governance. Substantial resources are required from funders and governments to enhance data governance practices, to improve data literacy and to enhance data quality. High quality data from Africa will afford diversity to global data sets, reducing bias in algorithms built for artificial intelligence technologies in healthcare. Engagement with multiple stakeholders including researchers and research communities is necessary to establish an equitable data sharing approach based on reciprocity and mutual benefit.",
      "journal": "PLOS digital health",
      "year": "2024",
      "doi": "10.1371/journal.pdig.0000551",
      "authors": "Brown Qunita et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39331661/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11432837",
      "ft_text_length": 55866,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11432837)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39364424",
      "title": "Developing SysteMatic: Prevention, precision and equity by design for people living with multiple long-term conditions.",
      "abstract": "BACKGROUND: The number of individuals living with multiple (\u22652) long term conditions (MLTCs) is a growing global challenge. People with MLTCs experience reduced life expectancy, complex healthcare needs, higher healthcare utilisation, increased burden of treatment, poorer quality of life and higher mortality. Evolving technologies including artificial intelligence (AI) could address some of these challenges by enabling more preventive and better integrated care, however, they may also exacerbate inequities. OBJECTIVE: We aim to deliver an equity focused, action-ready plan for transforming MLTC prevention and care, co-designed by people with lived experience of MLTCs and delivered through an Innovation Hub: SysteMatic. DESIGN: Our Hub is being co-designed by people with lived experience of MLTCs, practitioners, academics and industry partners in Liverpool and Glasgow, UK. This work builds on research into mental-physical health interdependence across the life-course, and on mobilisation of large-scale quantitative data and technology validation in health and care systems serving deprived populations in Glasgow and Liverpool. We work with 3 population segments: 1) Children & Families: facing psychosocial and environmental challenges with lifetime impacts; 2). Working Life: people with poorly integrated mental, physical and social care; and 3) Pre-Frailty: older people with MLTCs. We aim to understand their experiences and in parallel look at routinely collected health data on people with MLTCs to help us identify targets for intervention. We are co-identifying opportunities for systems transformation with our patient partners, healthcare professionals and through discussion with companies and public-sector organisations. We are co-defining 3/5/7-year MLTC innovation/transition targets and sustainable learning approaches. DISCUSSION: SysteMatic will deliver an actionable MLTC Innovation Hub strategic plan, with investment from the UK National Health Service, civic health and care partners, universities, and industry, enabling feedback of well-translated, patient and public prioritised problems into the engineering, physical, health and social sciences to underpin future equitable innovation delivery.",
      "journal": "Journal of multimorbidity and comorbidity",
      "year": "2024",
      "doi": "10.1177/26335565241272682",
      "authors": "Mair Frances S et al.",
      "keywords": "Multimorbidity; digital health; health technology; wearable devices",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39364424/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11447698",
      "ft_text_length": 20864,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11447698)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39372453",
      "title": "Artificial Intelligence and Health Equity: Opportunities and Obstacles.",
      "abstract": "",
      "journal": "JACC. Advances",
      "year": "2024",
      "doi": "10.1016/j.jacadv.2024.101045",
      "authors": "Deo Rahul C",
      "keywords": "artificial intelligence; diagnosis; health equity; treatment",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39372453/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11450931",
      "ft_text_length": 13119,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11450931)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39376670",
      "title": "Analyzing Racial Differences in Imaging Joint Replacement Registries Using Generative Artificial Intelligence: Advancing Orthopaedic Data Equity.",
      "abstract": "BACKGROUND: Discrepancies in medical data sets can perpetuate bias, especially when training deep learning models, potentially leading to biased outcomes in clinical applications. Understanding these biases is crucial for the development of equitable healthcare technologies. This study employs generative deep learning technology to explore and understand radiographic differences based on race among patients undergoing total hip arthroplasty. METHODS: Utilizing a large institutional registry, we retrospectively analyzed pelvic radiographs from total hip arthroplasty patients, characterized by demographics and image features. Denoising diffusion probabilistic models generated radiographs conditioned on demographic and imaging characteristics. Fr\u00e9chet Inception Distance assessed the generated image quality, showing the diversity and realism of the generated images. Sixty transition videos were generated that showed transforming White pelvises to their closest African American counterparts and vice versa while controlling for patients' sex, age, and body mass index. Two expert surgeons and 2 radiologists carefully studied these videos to understand the systematic differences that are present in the 2 races' radiographs. RESULTS: Our data set included 480,407 pelvic radiographs, with a predominance of White patients over African Americans. The generative denoising diffusion probabilistic model created high-quality images and reached an Fr\u00e9chet Inception Distance of 6.8. Experts identified 6 characteristics differentiating races, including interacetabular distance, osteoarthritis degree, obturator foramina shape, femoral neck-shaft angle, pelvic ring shape, and femoral cortical thickness. CONCLUSIONS: This study demonstrates the potential of generative models for understanding disparities in medical imaging data sets. By visualizing race-based differences, this method aids in identifying bias in downstream tasks, fostering the development of fairer healthcare practices.",
      "journal": "Arthroplasty today",
      "year": "2024",
      "doi": "10.1016/j.artd.2024.101503",
      "authors": "Khosravi Bardia et al.",
      "keywords": "Bias; Dataset curation; Equity; Explainability; Generative AI",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39376670/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11456877",
      "ft_text_length": 22374,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11456877)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39435256",
      "title": "Ethicara for Responsible AI in Healthcare: A System for Bias Detection and AI Risk Management.",
      "abstract": "The increasing torrents of health AI innovations hold promise for facilitating the delivery of patient-centered care. Yet the enablement and adoption of AI innovations in the healthcare and life science industries can be challenging with the rising concerns of AI risks and the potential harms to health equity. This paper describes Ethicara, a system that enables health AI risk assessment for responsible AI model development. Ethicara works by orchestrating a collection of self-analytics services that detect and mitigate bias and increase model transparency from harmonized data models. For the lack of risk controls currently in the health AI development and deployment process, the self-analytics tools enhanced by Ethicara are expected to provide repeatable and measurable controls to operationalize voluntary risk management frameworks and guidelines (e.g., NIST RMF, FDA GMLP) and regulatory requirements emerging from the upcoming AI regulations (e.g., EU AI Act, US Blueprint for an AI Bill of Rights). In addition, Ethicara provides plug-ins via which analytics results are incorporated into healthcare applications. This paper provides an overview of Ethicara's architecture, pipeline, and technical components and showcases the system's capability to facilitate responsible AI use, and exemplifies the types of AI risk controls it enables in the healthcare and life science industry.",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "year": "2023",
      "doi": "",
      "authors": "Kritharidou Maria et al.",
      "keywords": "",
      "mesh_terms": "Risk Management; Artificial Intelligence; Humans; Delivery of Health Care; Risk Assessment; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39435256/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11492113",
      "ft_text_length": 1398,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11492113)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "39436298",
      "title": "Evaluating the Performance and Bias of Natural Language Processing Tools in Labeling Chest Radiograph Reports.",
      "abstract": "Background Natural language processing (NLP) is commonly used to annotate radiology datasets for training deep learning (DL) models. However, the accuracy and potential biases of these NLP methods have not been thoroughly investigated, particularly across different demographic groups. Purpose To evaluate the accuracy and demographic bias of four NLP radiology report labeling tools on two chest radiograph datasets. Materials and Methods This retrospective study, performed between April 2022 and April 2024, evaluated chest radiograph report labeling using four NLP tools (CheXpert [rule-based], RadReportAnnotator [RRA; DL-based], OpenAI's GPT-4 [DL-based], cTAKES [hybrid]) on a subset of the Medical Information Mart for Intensive Care (MIMIC) chest radiograph dataset balanced for representation of age, sex, and race and ethnicity (n = 692) and the entire Indiana University (IU) chest radiograph dataset (n = 3665). Three board-certified radiologists annotated the chest radiograph reports for 14 thoracic disease labels. NLP tool performance was evaluated using several metrics, including accuracy and error rate. Bias was evaluated by comparing performance between demographic subgroups using the Pearson \u03c72 test. Results The IU dataset included 3665 patients (mean age, 49.7 years \u00b1 17 [SD]; 1963 female), while the MIMIC dataset included 692 patients (mean age, 54.1 years \u00b1 23.1; 357 female). All four NLP tools demonstrated high accuracy across findings in the IU and MIMIC datasets, as follows: CheXpert (92.6% [47\u2009516 of 51\u2009310], 90.2% [8742 of 9688]), RRA (82.9% [19\u2009746 of 23\u2009829], 92.2% [2870 of 3114]), GPT-4 (94.3% [45\u2009586 of 48\u2009342], 91.6% [6721 of 7336]), and cTAKES (84.7% [43\u2009436 of 51\u2009310], 88.7% [8597 of 9688]). RRA and cTAKES had higher accuracy (P < .001) on the MIMIC dataset, while CheXpert and GPT-4 had higher accuracy on the IU dataset. Differences (P < .001) in error rates were observed across age groups for all NLP tools except RRA on the MIMIC dataset, with the highest error rates for CheXpert, RRA, and cTAKES in patients older than 80 years (mean, 15.8% \u00b1 5.0) and the highest error rate for GPT-4 in patients 60-80 years of age (8.3%). Conclusion Although commonly used NLP tools for chest radiograph report annotation are accurate when evaluating reports in aggregate, demographic subanalyses showed significant bias, with poorer performance in older patients. \u00a9 RSNA, 2024 Supplemental material is available for this article. See also the editorial by Cai in this issue.",
      "journal": "Radiology",
      "year": "2024",
      "doi": "10.1148/radiol.232746",
      "authors": "Santomartino Samantha M et al.",
      "keywords": "",
      "mesh_terms": "Natural Language Processing; Humans; Radiography, Thoracic; Male; Female; Retrospective Studies; Middle Aged; Aged; Adult; Deep Learning; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39436298/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11535863",
      "ft_text_length": 2537,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11535863)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39446671",
      "title": "Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support.",
      "abstract": "The integration of large language models (LLMs) into healthcare highlights the need to ensure their efficacy while mitigating potential harms, such as the perpetuation of biases. Current evidence on the existence of bias within LLMs remains inconclusive. In this study, we present an approach to investigate the presence of bias within an LLM designed for mental health support. We simulated physician-patient conversations by using a communication loop between an LLM-based conversational agent and digital standardized patients (DSPs) that engaged the agent in dialogue while remaining agnostic to sociodemographic characteristics. In contrast, the conversational agent was made aware of each DSP's characteristics, including age, sex, race/ethnicity, and annual income. The agent's responses were analyzed to discern potential systematic biases using the Linguistic Inquiry and Word Count tool. Multivariate regression analysis, trend analysis, and group-based trajectory models were used to quantify potential biases. Among 449 conversations, there was no evidence of bias in both descriptive assessments and multivariable linear regression analyses. Moreover, when evaluating changes in mean tone scores throughout a dialogue, the conversational agent exhibited a capacity to show understanding of the DSPs' chief complaints and to elevate the tone scores of the DSPs throughout conversations. This finding did not vary by any sociodemographic characteristics of the DSP. Using an objective methodology, our study did not uncover significant evidence of bias within an LLM-enabled mental health conversational agent. These findings offer a complementary approach to examining bias in LLM-based conversational agents for mental health support.",
      "journal": "Cyberpsychology, behavior and social networking",
      "year": "2025",
      "doi": "10.1089/cyber.2024.0199",
      "authors": "Yeo Yee Hui et al.",
      "keywords": "artificial intelligence; bias; disparity; large language model; linguistic inquiry and word count; mental health",
      "mesh_terms": "Humans; Male; Female; Communication; Adult; Middle Aged; Mental Health; Physician-Patient Relations; Language; Mental Health Services; Sociodemographic Factors",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39446671/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11807910",
      "ft_text_length": 1756,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11807910)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39476380",
      "title": "Ensuring Accuracy and Equity in Vaccination Information From ChatGPT and CDC: Mixed-Methods Cross-Language Evaluation.",
      "abstract": "BACKGROUND: In the digital age, large language models (LLMs) like ChatGPT have emerged as important sources of health care information. Their interactive capabilities offer promise for enhancing health access, particularly for groups facing traditional barriers such as insurance and language constraints. Despite their growing public health use, with millions of medical queries processed weekly, the quality of LLM-provided information remains inconsistent. Previous studies have predominantly assessed ChatGPT's English responses, overlooking the needs of non-English speakers in the United States. This study addresses this gap by evaluating the quality and linguistic parity of vaccination information from ChatGPT and the Centers for Disease Control and Prevention (CDC), emphasizing health equity. OBJECTIVE: This study aims to assess the quality and language equity of vaccination information provided by ChatGPT and the CDC in English and Spanish. It highlights the critical need for cross-language evaluation to ensure equitable health information access for all linguistic groups. METHODS: We conducted a comparative analysis of ChatGPT's and CDC's responses to frequently asked vaccination-related questions in both languages. The evaluation encompassed quantitative and qualitative assessments of accuracy, readability, and understandability. Accuracy was gauged by the perceived level of misinformation; readability, by the Flesch-Kincaid grade level and readability score; and understandability, by items from the National Institutes of Health's Patient\u00a0Education\u00a0Materials Assessment Tool (PEMAT) instrument. RESULTS: The study found that both ChatGPT and CDC provided mostly accurate and understandable (eg, scores over 95 out of 100) responses. However, Flesch-Kincaid grade levels often exceeded the American Medical Association's recommended levels, particularly in English (eg, average grade level in English for ChatGPT=12.84, Spanish=7.93, recommended=6). CDC responses outperformed ChatGPT in readability across both languages. Notably, some Spanish responses appeared to be direct translations from English, leading to unnatural phrasing. The findings underscore the potential and challenges of using ChatGPT for health care access. CONCLUSIONS: ChatGPT holds potential as a health information resource but requires improvements in readability and linguistic equity to be truly effective for diverse populations. Crucially, the default user experience with ChatGPT, typically encountered by those without advanced language and prompting skills, can significantly shape health perceptions. This is vital from a public health standpoint, as the majority of users will interact with LLMs in their most accessible form. Ensuring that default responses are accurate, understandable, and equitable is imperative for fostering informed health decisions across diverse communities.",
      "journal": "JMIR formative research",
      "year": "2024",
      "doi": "10.2196/60939",
      "authors": "Joshi Saubhagya et al.",
      "keywords": "artificial intelligence; conversational agents; health equity; health information; health literacy; language equity; large language models; multilingualism; online health information; public health; vaccination",
      "mesh_terms": "Humans; United States; Centers for Disease Control and Prevention, U.S.; Vaccination; Language; Consumer Health Information; Health Literacy; Health Equity",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39476380/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11561424",
      "ft_text_length": 24144,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11561424)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39493911",
      "title": "Performance of off-the-shelf machine learning architectures and biases in low left ventricular ejection fraction detection.",
      "abstract": "BACKGROUND: Artificial intelligence-machine learning (AI-ML) has demonstrated the ability to extract clinically useful information from electrocardiograms (ECGs) not available using traditional interpretation methods. There exists an extensive body of AI-ML research in fields outside of cardiology including several open-source AI-ML architectures that can be translated to new problems in an \"off-the-shelf\" manner. OBJECTIVE: We sought to address the limited investigation of which if any of these off-the-shelf architectures could be useful in ECG analysis as well as how and when these AI-ML approaches fail. METHODS: We applied 6 off-the-shelf AI-ML architectures to detect low left ventricular ejection fraction (LVEF) in a cohort of ECGs from 24,868 patients. We assessed LVEF classification and explored patient characteristics associated with inaccurate (false positive or false negative) LVEF prediction. RESULTS: We found that all of these network architectures produced LVEF detection area under the receiver-operating characteristic curve values above 0.9 (averaged over 5 instances per network), with the ResNet 18 network performing the highest (average area under the receiver-operating characteristic curve of 0.917). We also observed that some patient-specific characteristics such as race, sex, and presence of several comorbidities were associated with lower LVEF prediction performance. CONCLUSIONS: This demonstrates the ability of off-the-shelf AI-ML architectures to detect clinically useful information from ECGs with performance matching contemporary custom-build AI-ML architectures. We also highlighted the presence of possible biases in these AI-ML approaches in the context of patient characteristics. These findings should be considered in the pursuit of efficient and equitable deployment of AI-ML technologies moving forward.",
      "journal": "Heart rhythm O2",
      "year": "2024",
      "doi": "10.1016/j.hroo.2024.07.009",
      "authors": "Bergquist Jake A et al.",
      "keywords": "Artificial intelligence; Electrocardiogram; Explainability; Heart failure; Machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39493911/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11524967",
      "ft_text_length": 37534,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11524967)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39500908",
      "title": "Artificial intelligence tools trained on human-labeled data reflect human biases: a case study in a large clinical consecutive knee osteoarthritis cohort.",
      "abstract": "Humans have been shown to have biases when reading medical images, raising questions about whether humans are uniform in their disease gradings. Artificial intelligence (AI) tools trained on human-labeled data may have inherent human non-uniformity. In this study, we used a radiographic knee osteoarthritis external validation dataset of 50 patients and a six-year retrospective consecutive clinical cohort of 8,273 patients. An FDA-approved and CE-marked AI tool was tested for potential non-uniformity in Kellgren-Lawrence grades between the right and left sides of the images. We flipped the images horizontally so that a left knee looked like a right knee and vice versa. According to human review, the AI tool showed non-uniformity with 20-22% disagreements on the external validation dataset and 13.6% on the cohort. However, we found no evidence of a significant difference in the accuracy compared to senior radiologists on the external validation dataset, or age bias or sex bias on the cohort. AI non-uniformity can boost the evaluated performance against humans, but image areas with inferior performance should be investigated.",
      "journal": "Scientific reports",
      "year": "2024",
      "doi": "10.1038/s41598-024-75752-z",
      "authors": "Lenskjold Anders et al.",
      "keywords": "Laterality; Artificial intelligence; Bias; Clinical data; Knee osteoarthritis; Uniform performance",
      "mesh_terms": "Humans; Osteoarthritis, Knee; Artificial Intelligence; Male; Female; Aged; Middle Aged; Retrospective Studies; Cohort Studies; Bias; Radiography",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39500908/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11538298",
      "ft_text_length": 27808,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11538298)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39511117",
      "title": "Bias Sensitivity in Diagnostic Decision-Making: Comparing ChatGPT with Residents.",
      "abstract": "BACKGROUND: Diagnostic errors, often due to biases in clinical reasoning, significantly affect patient care. While artificial intelligence chatbots like ChatGPT could help mitigate such biases, their potential susceptibility to biases is unknown. METHODS: This study evaluated diagnostic accuracy of ChatGPT against the performance of 265 medical residents in five previously published experiments aimed at inducing bias. The residents worked in several major teaching hospitals in the Netherlands. The biases studied were case-intrinsic (presence of salient distracting findings in the patient history, effects of disruptive patient behaviors) and situational (prior availability of a look-alike patient). ChatGPT's accuracy in identifying the most-likely diagnosis was measured. RESULTS: Diagnostic accuracy of residents and ChatGPT was equivalent. For clinical cases involving case-intrinsic bias, both ChatGPT and the residents exhibited a decline in diagnostic accuracy. Residents' accuracy decreased on average 12%, while the accuracy of ChatGPT 4.0 decreased 21%. Accuracy of ChatGPT 3.5 decreased 9%. These findings suggest that, like human diagnosticians, ChatGPT is sensitive to bias when the biasing information is part of the patient history. When the biasing information was extrinsic to the case in the form of the prior availability of a look-alike case, residents' accuracy decreased by 15%. By contrast, ChatGPT's performance was not affected by the biasing information. Chi-square goodness-of-fit tests corroborated these outcomes. CONCLUSIONS: It seems that, while ChatGPT is not sensitive to bias when biasing information is situational, it is sensitive to bias when the biasing information is part of the patient's disease history. Its utility in diagnostic support has potential, but caution is advised. Future research should enhance AI's bias detection and mitigation to make it truly useful for diagnostic support.",
      "journal": "Journal of general internal medicine",
      "year": "2025",
      "doi": "10.1007/s11606-024-09177-9",
      "authors": "Schmidt Henk G et al.",
      "keywords": "",
      "mesh_terms": "Humans; Internship and Residency; Diagnostic Errors; Clinical Decision-Making; Bias; Male; Female; Clinical Competence; Artificial Intelligence; Netherlands; Adult; Generative Artificial Intelligence",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39511117/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11914423",
      "ft_text_length": 21775,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11914423)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39535532",
      "title": "Mitigating bias in radiology: The promise of topological data analysis and simplicial complexes.",
      "abstract": "Topological Data Analysis (TDA) and simplicial complexes offer a novel approach to address biases in AI-assisted radiology. By capturing complex structures, n-way interactions, and geometric relationships in medical images, TDA enhances feature extraction, improves representation robustness, and increases interpretability. This mathematical framework has the potential to significantly improve the accuracy and fairness of radiological assessments, paving the way for more equitable patient care.",
      "journal": "Oncotarget",
      "year": "2024",
      "doi": "10.18632/oncotarget.28668",
      "authors": "Singh Yashbir et al.",
      "keywords": "medical imaging; radiology; simplicial complexes; topological data analysis",
      "mesh_terms": "Humans; Radiology; Bias; Data Analysis; Artificial Intelligence; Algorithms",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39535532/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11559658",
      "ft_text_length": 5228,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11559658)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39535539",
      "title": "Visualizing radiological data bias through persistence images.",
      "abstract": "Persistence images, derived from topological data analysis, emerge as a powerful tool for visualizing and mitigating biases in radiological data interpretation and AI model development. This technique transforms complex topological features into stable, interpretable representations, offering unique insights into medical imaging data structure. By providing intuitive visualizations, persistence images enable the identification of subtle structural differences and potential biases in data acquisition, interpretation, and AI model training. Persistence images can also facilitate stratified sampling, matching statistics, and noise filtration, enhancing the accuracy and equity of radiological analysis. Despite challenges in computational complexity and workflow integration, persistence images show promise in developing more accurate, equitable, and trustworthy AI systems in radiology, potentially improving patient outcomes and personalized healthcare delivery.",
      "journal": "Oncotarget",
      "year": "2024",
      "doi": "10.18632/oncotarget.28670",
      "authors": "Singh Yashbir et al.",
      "keywords": "data interpretation; persistence images; radiology; topology",
      "mesh_terms": "Humans; Artificial Intelligence; Radiology; Bias; Diagnostic Imaging",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39535539/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11559657",
      "ft_text_length": 7794,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11559657)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39561363",
      "title": "Mitigating Cognitive Biases in Clinical Decision-Making Through Multi-Agent Conversations Using Large Language Models: Simulation Study.",
      "abstract": "BACKGROUND: Cognitive biases in clinical decision-making significantly contribute to errors in diagnosis and suboptimal patient outcomes. Addressing these biases presents a formidable challenge in the medical field. OBJECTIVE: This study aimed to explore the role of large language models (LLMs) in mitigating these biases through the use of the multi-agent framework. We simulate the clinical decision-making processes through multi-agent conversation and evaluate its efficacy in improving diagnostic accuracy compared with humans. METHODS: A total of 16 published and unpublished case reports where cognitive biases have resulted in misdiagnoses were identified from the literature. In the multi-agent framework, we leveraged GPT-4 (OpenAI) to facilitate interactions among different simulated agents to replicate clinical team dynamics. Each agent was assigned a distinct role: (1) making the final diagnosis after considering the discussions, (2) acting as a devil's advocate to correct confirmation and anchoring biases, (3) serving as a field expert in the required medical subspecialty, (4) facilitating discussions to mitigate premature closure bias, and (5) recording and summarizing findings. We tested varying combinations of these agents within the framework to determine which configuration yielded the highest rate of correct final diagnoses. Each scenario was repeated 5 times for consistency. The accuracy of the initial diagnoses and the final differential diagnoses were evaluated, and comparisons with human-generated answers were made using the Fisher exact test. RESULTS: A total of 240 responses were evaluated (3 different multi-agent frameworks). The initial diagnosis had an accuracy of 0% (0/80). However, following multi-agent discussions, the accuracy for the top 2 differential diagnoses increased to 76% (61/80) for the best-performing multi-agent framework (Framework 4-C). This was significantly higher compared with the accuracy achieved by human evaluators (odds ratio 3.49; P=.002). CONCLUSIONS: The multi-agent framework demonstrated an ability to re-evaluate and correct misconceptions, even in scenarios with misleading initial investigations. In addition, the LLM-driven, multi-agent conversation framework shows promise in enhancing diagnostic accuracy in diagnostically challenging medical scenarios.",
      "journal": "Journal of medical Internet research",
      "year": "2024",
      "doi": "10.2196/59439",
      "authors": "Ke Yuhe et al.",
      "keywords": "clinical decision-making; cognitive bias; generative artificial intelligence; large language model; multi-agent",
      "mesh_terms": "Humans; Clinical Decision-Making; Cognition; Bias; Language; Computer Simulation; Diagnostic Errors; Communication",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39561363/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11615553",
      "ft_text_length": 25782,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11615553)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39565825",
      "title": "Learning patterns of HIV-1 resistance to broadly neutralizing antibodies with reduced subtype bias using multi-task learning.",
      "abstract": "The ability to predict HIV-1 resistance to broadly neutralizing antibodies (bnAbs) will increase bnAb therapeutic benefits. Machine learning is a powerful approach for such prediction. One challenge is that some HIV-1 subtypes in currently available training datasets are underrepresented, which likely affects models' generalizability across subtypes. A second challenge is that combinations of bnAbs are required to avoid the inevitable resistance to a single bnAb, and computationally determining optimal combinations of bnAbs is an unsolved problem. Recently, machine learning models trained using resistance outcomes for multiple antibodies at once, a strategy called multi-task learning (MTL), have been shown to improve predictions. We develop a new model and show that, beyond the boost in performance, MTL also helps address the previous two challenges. Specifically, we demonstrate empirically that MTL can mitigate bias from underrepresented subtypes, and that MTL allows the model to learn patterns of co-resistance to combinations of antibodies, thus providing tools to predict antibodies' epitopes and to potentially select optimal bnAb combinations. Our analyses, publicly available at https://github.com/iaime/LBUM, can be adapted to other infectious diseases that are treated with antibody therapy.",
      "journal": "PLoS computational biology",
      "year": "2024",
      "doi": "10.1371/journal.pcbi.1012618",
      "authors": "Igiraneza Aime Bienfait et al.",
      "keywords": "",
      "mesh_terms": "HIV-1; Humans; HIV Antibodies; HIV Infections; Machine Learning; Computational Biology; Antibodies, Neutralizing; Broadly Neutralizing Antibodies; Epitopes",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39565825/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11616810",
      "ft_text_length": 46861,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11616810)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39569464",
      "title": "Using human factors methods to mitigate bias in artificial intelligence-based clinical decision support.",
      "abstract": "OBJECTIVES: To highlight the often overlooked role of user interface (UI) design in mitigating bias in artificial intelligence (AI)-based clinical decision support (CDS). MATERIALS AND METHODS: This perspective paper discusses the interdependency between AI-based algorithm development and UI design and proposes strategies for increasing the safety and efficacy of CDS. RESULTS: The role of design in biasing user behavior is well documented in behavioral economics and other disciplines. We offer an example of how UI designs play a role in how bias manifests in our machine learning-based CDS development. DISCUSSION: Much discussion on bias in AI revolves around data quality and algorithm design; less attention is given to how UI design can exacerbate or mitigate limitations of AI-based applications. CONCLUSION: This work highlights important considerations including the role of UI design in reinforcing/mitigating bias, human factors methods for identifying issues before an application is released, and risk communication strategies.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2025",
      "doi": "10.1093/jamia/ocae291",
      "authors": "Militello Laura G et al.",
      "keywords": "artificial intelligence; bias mitigation; clinical decision support; human factors methods",
      "mesh_terms": "Decision Support Systems, Clinical; Humans; Artificial Intelligence; Algorithms; User-Computer Interface; Bias; Ergonomics; Machine Learning",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39569464/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11756570",
      "ft_text_length": 1050,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11756570)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39583943",
      "title": "Ethnic disparities in lung cancer incidence and differences in diagnostic characteristics: a population-based cohort study in England.",
      "abstract": "BACKGROUND: Lung cancer is a leading cause of mortality, yet disparities in lung cancer across different sociodemographic groups in the UK remain unclear. This study investigates ethnicity and sociodemographic disparities and differences in lung cancer in a nationally representative English cohort, aiming to highlight inequalities and promote equitable access to diagnostic advancements. METHODS: We conducted a population-based cohort study using health care records from QResearch, a large primary care database in England. The study included adults aged 25 and over, spanning the period of 2005-2019. Lung cancer incidence rates were calculated using age-standardized methods. Multinomial logistic regression was applied to assess associations between ethnicity/sociodemographic factors and diagnostic characteristics (histological type, stage, and cancer grade), adjusting for confounders. FINDINGS: From a cohort of over 17.5 million people, we identified disparities in incidence rates across ethnic groups from 2005 to 2019. Analysis of 84,253 lung cancer cases revealed that younger woman and Individuals of Indian, other Asian, Black African, Caribbean and Chinese backgrounds had a significantly higher risks of adenocarcinoma compared with squamous cell carcinoma than their White counterparts (relative risk ratios [RRR] spanning from 1.52 (95% CI 1.18-1.94) to 2.69 (95% CI 1.43-5.05). Men and current smokers were more likely to be diagnosed at an advanced stage than women and never smokers (RRR: 1.72 [95% CI 1.56-1.90]-2.45 [95% CI 2.16-2.78]). Socioeconomic deprivation was associated with higher risks of moderate or poorly differentiated adenocarcinoma compared with well differentiated (RRRs between 1.35 [CI: 1.02-1.79] and 1.37 [1.05-1.80]). INTERPRETATION: Our study highlights significant differences in lung cancer incidence and in lung cancer diagnostic characteristics related to ethnicity, deprivation and other demographic factors. These findings have important implications for the provision of equitable screening and prevention programmes to mitigate health inequalities. FUNDING: DART (The Integration and Analysis of Data using Artificial Intelligence to Improve Patient Outcomes with Thoracic Diseases) project, Innovate UK (UK Research and Innovation), QResearch\u00ae and grants from the NIHR Biomedical Research Centre (Oxford), John Fell Oxford University Press Research Fund, Cancer Research UK, and the Oxford Wellcome Institutional Strategic Support Fund.",
      "journal": "The Lancet regional health. Europe",
      "year": "2025",
      "doi": "10.1016/j.lanepe.2024.101124",
      "authors": "Tzu-Hsuan Chen Daniel et al.",
      "keywords": "Cancer epidemiology; Cancer prevention; Cohort study; Ethnic disparities; Health disparities; Lung cancer",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39583943/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11584601",
      "ft_text_length": 48203,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11584601)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39652394",
      "title": "Use of ChatGPT to Explore Gender and Geographic Disparities in Scientific Peer Review.",
      "abstract": "BACKGROUND: In the realm of scientific research, peer review serves as a cornerstone for ensuring the quality and integrity of scholarly papers. Recent trends in promoting transparency and accountability has led some journals to publish peer-review reports alongside papers. OBJECTIVE: ChatGPT-4 (OpenAI) was used to quantitatively assess sentiment and politeness in peer-review reports from high-impact medical journals. The objective was to explore gender and geographical disparities to enhance inclusivity within the peer-review process. METHODS: All 9 general medical journals with an impact factor >2 that publish peer-review reports were identified. A total of 12 research papers per journal were randomly selected, all published in 2023. The names of the first and last authors along with the first author's country of affiliation were collected, and the gender of both the first and last authors was determined. For each review, ChatGPT-4 was asked to evaluate the \"sentiment score,\" ranging from -100 (negative) to 0 (neutral) to +100 (positive), and the \"politeness score,\" ranging from -100 (rude) to 0 (neutral) to +100 (polite). The measurements were repeated 5 times and the minimum and maximum values were removed. The mean sentiment and politeness scores for each review were computed and then summarized using the median and interquartile range. Statistical analyses included Wilcoxon rank-sum tests, Kruskal-Wallis rank tests, and negative binomial regressions. RESULTS: Analysis of 291 peer-review reports corresponding to 108 papers unveiled notable regional disparities. Papers from the Middle East, Latin America, or Africa exhibited lower sentiment and politeness scores compared to those from North America, Europe, or Pacific and Asia (sentiment scores: 27 vs 60 and 62 respectively; politeness scores: 43.5 vs 67 and 65 respectively, adjusted P=.02). No significant differences based on authors' gender were observed (all P>.05). CONCLUSIONS: Notable regional disparities were found, with papers from the Middle East, Latin America, and Africa demonstrating significantly lower scores, while no discernible differences were observed based on authors' gender. The absence of gender-based differences suggests that gender biases may not manifest as prominently as other forms of bias within the context of peer review. The study underscores the need for targeted interventions to address regional disparities in peer review and advocates for ongoing efforts to promote equity and inclusivity in scholarly communication.",
      "journal": "Journal of medical Internet research",
      "year": "2024",
      "doi": "10.2196/57667",
      "authors": "Sebo Paul",
      "keywords": "Africa; ChatGPT; artificial intelligence; assessment; communication; consultation; discrimination; disparity; gender; gender bias; geographic; global south; inequality; peer review; researcher; sentiment analysis; woman",
      "mesh_terms": "Humans; Male; Female; Peer Review; Journal Impact Factor; Periodicals as Topic; Sex Factors; Peer Review, Research",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39652394/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11667125",
      "ft_text_length": 15701,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11667125)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39659993",
      "title": "Decoding disparities: evaluating automatic speech recognition system performance in transcribing Black and White patient verbal communication with nurses in home healthcare.",
      "abstract": "OBJECTIVES: As artificial intelligence evolves, integrating speech processing into home healthcare (HHC) workflows is increasingly feasible. Audio-recorded communications enhance risk identification models, with automatic speech recognition (ASR) systems as a key component. This study evaluates the transcription accuracy and equity of 4 ASR systems-Amazon Web Services (AWS) General, AWS Medical, Whisper, and Wave2Vec-in transcribing patient-nurse communication in US HHC, focusing on their ability in accurate transcription of speech from Black and White English-speaking patients. MATERIALS AND METHODS: We analyzed audio recordings of patient-nurse encounters from 35 patients (16 Black and 19 White) in a New York City-based HHC service. Overall, 860 utterances were available for study, including 475 drawn from Black patients and 385 from White patients. Automatic speech recognition performance was measured using word error rate (WER), benchmarked against a manual gold standard. Disparities were assessed by comparing ASR performance across racial groups using the linguistic inquiry and word count (LIWC) tool, focusing on 10 linguistic dimensions, as well as specific speech elements including repetition, filler words, and proper nouns (medical and nonmedical terms). RESULTS: The average age of participants was 67.8 years (SD\u2009=\u200914.4). Communication lasted an average of 15 minutes (range: 11-21 minutes) with a median of 1186 words per patient. Of 860 total utterances, 475 were from Black patients and 385 from White patients. Amazon Web Services General had the highest accuracy, with a median WER of 39%. However, all systems showed reduced accuracy for Black patients, with significant discrepancies in LIWC dimensions such as \"Affect,\" \"Social,\" and \"Drives.\" Amazon Web Services Medical performed best for medical terms, though all systems have difficulties with filler words, repetition, and nonmedical terms, with AWS General showing the lowest error rates at 65%, 64%, and 53%, respectively. DISCUSSION: While AWS systems demonstrated superior accuracy, significant disparities by race highlight the need for more diverse training datasets and improved dialect sensitivity. Addressing these disparities is critical for ensuring equitable ASR performance in HHC settings and enhancing risk prediction models through audio-recorded communication.",
      "journal": "JAMIA open",
      "year": "2024",
      "doi": "10.1093/jamiaopen/ooae130",
      "authors": "Zolnoori Maryam et al.",
      "keywords": "automatic speech recognition (ASR); health disparities; home healthcare; linguistic inquiry and word count (LIWC); speech to text; word error rate (WER)",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39659993/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11631515",
      "ft_text_length": 45592,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11631515)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39671773",
      "title": "Multiple approaches to advance health equity in nursing science: Recruitment, data, and dissemination.",
      "abstract": "Ensuring equitable care and health outcomes for all populations is essential in nursing science. However, achieving health equity in nursing science necessitates a multifaceted approach to address the complex factors influencing health disparities. This paper presented the keynote address delivered at the Advanced Methods Conference hosted by the Council for Advancement of Nursing Science in 2023. We identified critical gaps in health equity within nursing science, emphasizing the need for innovative recruitment strategies, comprehensive data analysis, and targeted dissemination efforts. This paper underscores the importance of equity in artificial intelligence research, highlighting issues such as biases in machine learning models and the underrepresentation of minoritized groups. Bridging the current gaps in health equity research within nursing science requires a systematic and forward-thinking approach.",
      "journal": "Nursing outlook",
      "year": "2025",
      "doi": "10.1016/j.outlook.2024.102343",
      "authors": "Wu Bei et al.",
      "keywords": "Artificial intelligence; Health equity; Methodology; Nursing science",
      "mesh_terms": "Humans; Health Equity; Nursing Research; Information Dissemination; Artificial Intelligence",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39671773/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11810570",
      "ft_text_length": 1145,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11810570)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39674855",
      "title": "Automating Racism: Is Use of the Vaginal Birth After Cesarean Calculator Associated with Inequity in Perinatal Service Delivery?",
      "abstract": "OBJECTIVE: The clinical application of race-adjusted algorithms may perpetuate health inequities. We assessed the impact of the vaginal birth after cesarean (VBAC) calculator, which was revised in 2021 to address concerns about equity. The original algorithm factored race and ethnicity and gave lower VBAC probabilities to Black and Hispanic patients. METHODS: From 2019 to 2020, we conducted a multi-site, ethnographic study consisting of interviews and audio recordings of 14 prenatal visits. We used grounded theory to describe the social processes of racialization. FINDINGS: Across 4 sites, 12 obstetricians, 5 midwives, and 31 pregnant/postpartum patients participated. Seventy-four percent (N\u2009=\u200923) of the pregnant/postpartum individuals identified as racially minoritized, and the remaining 24% (N\u2009=\u20098) identified as White. We identified four processes that facilitated the \"automation\" of racism: adhering to strict cutoffs; the routine adoption of calculators; obfuscating the calculator; and the reflexive categorization of race and ethnicity. When clinicians adhered to strict cutoffs, they steered low-scoring Black and Hispanic patients toward repeat cesareans. If clinicians obfuscated the calculator, Black and Hispanic patients had to work to decode the role of race and ethnicity in their probabilities in order to pursue a VBAC. By reflexively categorizing race and ethnicity, the use of the calculator forced patients to choose a singular identity, even if it obscured the truth about their multi-faceted race or ethnicity. CONCLUSION: The VBAC calculator's inclusion of race and ethnicity helped to automate racism by coding race into institutional practices and care interactions. This resulted in some clinicians discouraging or prohibiting Black and Hispanic patients from attempting a VBAC. SIGNIFICANCE: To date, no empiric study has examined whether the VBAC calculator produced inequities in access to VBAC services and reproduced racism in care. The VBAC calculator resulted in fewer VBAC attempts among racially minoritized patients, denying them the opportunity to undergo labor and a vaginal birthing experience.",
      "journal": "Journal of racial and ethnic health disparities",
      "year": "2026",
      "doi": "10.1007/s40615-024-02233-4",
      "authors": "Rubashkin Nicholas et al.",
      "keywords": "Algorithms; Cesarean birth; Health equity; Racism",
      "mesh_terms": "Adult; Female; Humans; Pregnancy; Algorithms; Anthropology, Cultural; Black or African American; Grounded Theory; Healthcare Disparities; Hispanic or Latino; Perinatal Care; Racism; Vaginal Birth after Cesarean; Qualitative Research",
      "pub_types": "Journal Article; Multicenter Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39674855/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12548435",
      "ft_text_length": 24908,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12548435)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39678690",
      "title": "Utilization of Artificial Intelligence to Improve Equitable Healthcare Access for Breast Implant Patients.",
      "abstract": "Recently, mandated FDA patient decision checklists were developed with the goal of improving the informed decision-making process for patients considering breast implants. However, these checklists are written at reading levels far higher than recommended by the National Institutes of Health and the American Medical Association. This study aims to improve the accessibility, and therefore, the utility of the mandated FDA patient literature for the average breast implant patient using the assistance of artificial intelligence (AI). Patient decision checklists were obtained from the 3 most utilized breast implant manufacturers in the United States-Allergan, Mentor, and Sientra. A novel patient decision checklist was synthesized by AI, written at the sixth grade reading level, using these checklists as source material. The AI-assisted checklist was edited by plastic surgeons for both formatting and content. The overall readability of Allergan, Mentor, and Sientra patient checklists correlated with the college reading level. These documents were of a statistically significantly higher reading level than the AI-assisted checklist, which was written at the recommended sixth grade level. Text composition analysis similarly demonstrated substantial differences between the AI-assisted and FDA-mandated literature. The currently mandated breast implant patient checklists are written at a college reading level and are inaccessible to the average patient. The authors propose a new patient decision checklist, generated with the assistance of AI, to improve healthcare access within plastic surgery. This simplified material can be used as an adjunct to the current checklists to improve shared decision making.",
      "journal": "Aesthetic surgery journal. Open forum",
      "year": "2024",
      "doi": "10.1093/asjof/ojae093",
      "authors": "Ragsdale Louisa B et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39678690/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11646116",
      "ft_text_length": 14693,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11646116)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39679140",
      "title": "Evaluating Large Language Model-Supported Instructions for Medication Use: First Steps Toward a Comprehensive Model.",
      "abstract": "OBJECTIVE: To assess the support of large language models (LLMs) in generating clearer and more personalized medication instructions to enhance e-prescription. PATIENTS AND METHODS: We established patient-centered guidelines for adequate, acceptable, and personalized directions to enhance e-prescription. A dataset comprising 104 outpatient scenarios, with an array of medications, administration routes, and patient conditions, was developed following the Brazilian national e-prescribing standard. Three prompts were submitted to a closed-source LLM. The first prompt involved a generic command, the second one was calibrated for content enhancement and personalization, and the third one requested bias mitigation. The third prompt was submitted to an open-source LLM. Outputs were assessed using automated metrics and human evaluation. We conducted the study between March 1, 2024 and September 10, 2024. RESULTS: Adequacy scores of our closed-source LLM's output showed the third prompt outperforming the first and second one. Full and partial acceptability was achieved in 94.3% of texts with the third prompt. Personalization was rated highly, especially with the second and third prompts. The 2 LLMs showed similar adequacy results. Lack of scientific evidence and factual errors were infrequent and unrelated to a particular prompt or LLM. The frequency of hallucinations was different for each LLM and concerned prescriptions issued upon symptom manifestation and medications requiring dosage adjustment or involving intermittent use. Gender bias was found in our closed-source LLM's output for the first and second prompts, with the third one being bias-free. The second LLM's output was bias-free. CONCLUSION: This study demonstrates the potential of LLM-supported generation to produce prescription directions and improve communication between health professionals and patients within the e-prescribing system.",
      "journal": "Mayo Clinic proceedings. Digital health",
      "year": "2024",
      "doi": "10.1016/j.mcpdig.2024.09.006",
      "authors": "Reis Zilma Silveira Nogueira et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39679140/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11638470",
      "ft_text_length": 32617,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11638470)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39707939",
      "title": "Accountability for Reasonableness as a Framework for the Promotion of Fair and Equitable Research.",
      "abstract": "Despite increased efforts to ensure diversity in genomic research, the exclusion of minority groups from data analyses and publications remains a critical issue. This paper addresses the ethical implications of these exclusions and proposes accountability for reasonableness (A4R) as a framework to promote fairness and equity in research. Originally conceived by Norman Daniels and James Sabin to guide resource allocation in the context of health policy, A4R emphasizes publicity, relevance of reasons, enforcement, and revision as essential for legitimacy and trust in the decision-making process. The authors argue that A4R is also relevant to resource allocation in research and that, if adequately informed and incentivized by funding agencies, institutional review boards, and scientific journals, researchers are well-positioned to assess data-selection justifications. The A4R framework provides a promising foundation for fostering accountability in genomics and other fields, including artificial intelligence, where lack of diversity and pervasive biases threaten equitable benefit sharing.",
      "journal": "The Hastings Center report",
      "year": "2024",
      "doi": "10.1002/hast.4931",
      "authors": "Dupras Charles et al.",
      "keywords": "A4R; accountability for reasonableness; diversity; equity and fairness; exclusion; genomic research; procedural justice; research ethics",
      "mesh_terms": "Humans; Social Responsibility; Minority Groups; Genomics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39707939/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11662771",
      "ft_text_length": 29570,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11662771)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39841529",
      "title": "AI Can Be a Powerful Social Innovation for Public Health if Community Engagement Is at the Core.",
      "abstract": "There is a critical need for community engagement in the process of adopting artificial intelligence (AI) technologies in public health. Public health practitioners and researchers have historically innovated in areas like vaccination and sanitation but have been slower in adopting emerging technologies such as generative AI. However, with increasingly complex funding, programming, and research requirements, the field now faces a pivotal moment to enhance its agility and responsiveness to evolving health challenges. Participatory methods and community engagement are key components of many current public health programs and research. The field of public health is well positioned to ensure community engagement is part of AI technologies applied to population health issues. Without such engagement, the adoption of these technologies in public health may exclude significant portions of the population, particularly those with the fewest resources, with the potential to exacerbate health inequities. Risks to privacy and perpetuation of bias are more likely to be avoided if AI technologies in public health are designed with knowledge of community engagement, existing health disparities, and strategies for improving equity. This viewpoint proposes a multifaceted approach to ensure safer and more effective integration of AI in public health with the following call to action: (1) include the basics of AI technology in public health training and professional development; (2) use a community engagement approach to co-design AI technologies in public health; and (3) introduce governance and best practice mechanisms that can guide the use of AI in public health to prevent or mitigate potential harms. These actions will support the application of AI to varied public health domains through a framework for more transparent, responsive, and equitable use of this evolving technology, augmenting the work of public health practitioners and researchers to improve health outcomes while minimizing risks and unintended consequences.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/68198",
      "authors": "Bazzano Alessandra N et al.",
      "keywords": "Artificial Intelligence; Citizen Science; Community Participation; Generative Artificial Intelligence; Innovation Diffusion",
      "mesh_terms": "Artificial Intelligence; Public Health; Humans; Community Participation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39841529/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11799803",
      "ft_text_length": 11649,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11799803)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39850650",
      "title": "Bioethics Principles in Machine Learning-Healthcare Application Design: Achieving Health Justice and Health Equity.",
      "abstract": "Health technologies featuring artificial intelligence (AI) are becoming more common. Some healthcare AIs are exhibiting bias towards underrepresented persons and populations. Although many computer scientists and healthcare professionals agree that eliminating or mitigating bias in healthcare AIs is needed, little information exists regarding how to operationalize bioethics principles like autonomy in product design and implementation. This short course is framed with a Social Determinants of Health lens and a health justice and health equity stance to support computer scientists and healthcare professionals in building and deploying ethical healthcare AI. In this short course we introduce the bioethics principle of autonomy in the context of human-centered design (Module 1) and share options for design thinking models, suggesting four activities to embed ethics principles during design (Module 2). We then discuss the importance of gaining the perspectives of diverse groups to minimize harm and support the fundamental human values of underrepresented persons in support of health equity and health justice ideals (Module 3).",
      "journal": "Harvard public health review (Cambridge, Mass.)",
      "year": "2024",
      "doi": "10.54111/0001/aaaa1",
      "authors": "Fritz Roschelle L et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39850650/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11756589",
      "ft_text_length": 1140,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11756589)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39914854",
      "title": "Biodesign in the generative AI era: enhancing innovation and equity with NLP and LLM tools.",
      "abstract": "",
      "journal": "BMJ health & care informatics",
      "year": "2025",
      "doi": "10.1136/bmjhci-2024-101409",
      "authors": "Tani Jowy",
      "keywords": "Artificial intelligence; BMJ Health Informatics; Health Equity; Software Design; User-Centred Design",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39914854/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11800217",
      "ft_text_length": 5616,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11800217)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39931300",
      "title": "Advancing health equity: evaluating AI translations of kidney donor information for Spanish speakers.",
      "abstract": "BACKGROUND: Health equity and access to essential medical information remain significant challenges, especially for the Spanish-speaking Hispanic population, which faces barriers in accessing living kidney donation opportunities. ChatGPT, an AI language model with sophisticated natural language processing capabilities, has been identified as a promising tool for translating critical health information into Spanish. This study aims to assess ChatGPT's translation efficacy to ensure the information provided is accurate and culturally relevant. METHODS: This study utilized ChatGPT versions 3.5 and 4.0 to translate 27 frequently asked questions (FAQs) from English to Spanish, sourced from Donate Life America's website. The translated content was reviewed by native Spanish-speaking nephrologists using a standard rubric scale (1-5). The assessment focused on linguistic accuracy and cultural sensitivity, emphasizing retention of the original message, appropriate vocabulary and grammar, and cultural relevance. RESULTS: The mean linguistic accuracy scores were 4.89\u202f\u00b1\u202f0.32 for GPT-3.5 and 5.00\u202f\u00b1\u202f0.00 for GPT-4.0 (p =\u202f0.08). The percentage of excellent-quality translations (score\u202f=\u202f5) in linguistic accuracy was 89% for GPT-3.5 and 100% for GPT-4.0 (p =\u202f0.24). The mean cultural sensitivity scores were 4.89\u202f\u00b1\u202f0.32 for both GPT-3.5 and GPT-4.0 (p =\u202f1.00). Similarly, excellent-quality translations in cultural sensitivity were achieved in 89% of cases for both versions (p =\u202f1.00). CONCLUSION: ChatGPT 4.0 demonstrates strong potential to enhance health equity by improving Spanish-speaking Hispanic patients' access to LKD information through accurate and culturally sensitive translations. These findings highlight the role of AI in mitigating healthcare disparities and underscore the need for integrating AI-driven tools into healthcare systems. Future efforts should focus on developing accessible platforms and establishing guidelines to maximize AI's impact on equitable healthcare delivery and patient education.",
      "journal": "Frontiers in public health",
      "year": "2025",
      "doi": "10.3389/fpubh.2025.1484790",
      "authors": "Garcia Valencia Oscar A et al.",
      "keywords": "ChatGPT; Spanish-speaking populations; artificial intelligence; cultural competency; health equity; healthcare communication barriers; language translation models; living kidney donation",
      "mesh_terms": "Adult; Female; Humans; Male; Middle Aged; Artificial Intelligence; Health Equity; Hispanic or Latino; Kidney Transplantation; Language; Living Donors; Translating; Translations",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39931300/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11808013",
      "ft_text_length": 24111,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11808013)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39947693",
      "title": "Hospital Artificial Intelligence/Machine Learning Adoption by Neighborhood Deprivation.",
      "abstract": "OBJECTIVE: To understand the variation in artificial intelligence/machine learning (AI/ML) adoption across different hospital characteristics and explore how AI/ML is utilized, particularly in relation to neighborhood deprivation. BACKGROUND: AI/ML-assisted care coordination has the potential to reduce health disparities, but there is a lack of empirical evidence on AI's impact on health equity. METHODS: We used linked datasets from the 2022 American Hospital Association Annual Survey and the 2023 American Hospital Association Information Technology Supplement. The data were further linked to the 2022 Area Deprivation Index (ADI) for each hospital's service area. State fixed-effect regressions were employed. A decomposition model was also used to quantify predictors of AI/ML implementation, comparing hospitals in higher versus lower ADI areas. RESULTS: Hospitals serving the most vulnerable areas (ADI Q4) were significantly less likely to apply ML or other predictive models (coef = -0.10, P = 0.01) and provided fewer AI/ML-related workforce applications (coef = -0.40, P = 0.01), compared with those in the least vulnerable areas. Decomposition results showed that our model specifications explained 79% of the variation in AI/ML adoption between hospitals in ADI Q4 versus ADI Q1-Q3. In addition, Accountable Care Organization affiliation accounted for 12%-25% of differences in AI/ML utilization across various measures. CONCLUSIONS: The underuse of AI/ML in economically disadvantaged and rural areas, particularly in workforce management and electronic health record implementation, suggests that these communities may not fully benefit from advancements in AI-enabled health care. Our results further indicate that value-based payment models could be strategically used to support AI integration.",
      "journal": "Medical care",
      "year": "2025",
      "doi": "10.1097/MLR.0000000000002110",
      "authors": "Chen Jie et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Machine Learning; United States; Hospitals; Neighborhood Characteristics; Residence Characteristics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39947693/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11809723",
      "ft_text_length": 26413,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11809723)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39949826",
      "title": "Artificial intelligence in global health: An unfair future for health in Sub-Saharan Africa?",
      "abstract": "Artificial intelligence (AI) holds transformative potential for global health, particularly in underdeveloped regions like Africa. However, the integration of AI into healthcare systems raises significant concerns regarding equity and fairness. This debate paper explores the challenges and risks associated with implementing AI in healthcare in Africa, focusing on the lack of infrastructure, data quality issues, and inadequate governance frameworks. It also explores the geopolitical and economic dynamics that exacerbate these disparities, including the impact of global competition and weakened international institutions. While highlighting the risks, the paper acknowledges the potential benefits of AI, including improved healthcare access, standardization of care, and enhanced health communication. To ensure equitable outcomes, it advocates for targeted policy measures, including infrastructure investment, capacity building, regulatory frameworks, and international collaboration. This comprehensive approach is essential to mitigate risks, harness the benefits of AI, and promote social justice in global health.",
      "journal": "Health affairs scholar",
      "year": "2025",
      "doi": "10.1093/haschl/qxaf023",
      "authors": "Victor Aud\u00eancio",
      "keywords": "Africa; artificial intelligence; global governance; health inequalities; social justice in health",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39949826/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11823112",
      "ft_text_length": 10804,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11823112)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39975902",
      "title": "Potential source of bias in AI models: Lactate measurement in the ICU as a template.",
      "abstract": "OBJECTIVE: Health inequities may be driven by demographics such as sex, language proficiency, and race-ethnicity. These disparities may manifest through likelihood of testing, which in turn can bias artificial intelligence models. The goal of this study is to evaluate variation in serum lactate measurements in the Intensive Care Unit (ICU). METHODS: Utilizing MIMIC-IV (2008-2019), we identified adults fulfilling sepsis-3 criteria. Exclusion criteria were ICU stay <1-day, unknown race-ethnicity, <18 years of age, and recurrent stays. Employing targeted maximum likelihood estimation analysis, we assessed the likelihood of a lactate measurement on day 1. For patients with a measurement on day 1, we evaluated the predictors of subsequent readings. RESULTS: We studied 15,601 patients (19.5% racial-ethnic minority, 42.4% female, and 10.0% limited English proficiency). After adjusting for confounders, Black patients had a slightly higher likelihood of receiving a lactate measurement on day 1 (odds ratio 1.19, 95% confidence interval (CI) 1.06-1.34), but not the other minority groups. Subsequent frequency was similar across race-ethnicities, but women had a lower incidence rate ratio (IRR) 0.94 (95% CI 0.90-0.98). Interestingly, patients with elective admission and private insurance also had a higher frequency of repeated serum lactate measurements (IRR 1.70, 95% CI 1.61-1.81, and 1.07, 95% CI, 1.02-1.12, respectively). CONCLUSION: We found no disparities in the likelihood of a lactate measurement among patients with sepsis across demographics, except for a small increase for Black patients, and a reduced frequency for women. Variation in biomarker monitoring can be a source of data bias when modeling patient outcomes, and thus should be accounted for in every analysis.",
      "journal": "Research square",
      "year": "2025",
      "doi": "10.21203/rs.3.rs-5836145/v1",
      "authors": "Hussein Nebal S Abu et al.",
      "keywords": "Critical Care; Health Equity; MIMIC-IV; Sepsis; lactate",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39975902/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11838763",
      "ft_text_length": 15145,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11838763)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40003519",
      "title": "Social Determinants and Health Equity Activities: Are They Connected with the Adaptation of AI and Telehealth Services in the U.S. Hospitals?",
      "abstract": "In recent decades, technological shifts within the healthcare sector have significantly transformed healthcare management and utilization, introducing unprecedented possibilities that elevate quality of life. Organizational factors are recognized as key drivers in technology adoption, but involvement of hospitals in community-oriented activities and promotion of health equity are underexplored. This study investigated the impact of community social determinant activities and health equity activities on the adoption of AI and telehealth services within U.S. hospitals. The data were collected from the 2021 American Hospital Association (AHA) annual survey and were analyzed using multiple logistic and linear regression models to examine activities related to addressing population health, particularly social determinants and health equity, and their impacts on the adoption of AI and telehealth among U.S. hospitals. The results indicate a significant positive association between the community social determinant indicator and health equity indicator in adopting AI and telehealth services. Organizational factors were also major drivers of AI and telehealth adoption. The active incorporation of hospital strategies that address social determinants and promote health equity leads to the integration of advanced technologies and improves hospital conditions, enabling more adaptability to the changing healthcare landscape, which enhances healthcare services and accessibility.",
      "journal": "International journal of environmental research and public health",
      "year": "2025",
      "doi": "10.3390/ijerph22020294",
      "authors": "Pinera Pearl A et al.",
      "keywords": "artificial intelligence; health equity; hospital; social determinants; telehealth",
      "mesh_terms": "Telemedicine; Social Determinants of Health; United States; Health Equity; Humans; Hospitals; Artificial Intelligence",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40003519/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11854943",
      "ft_text_length": 30017,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11854943)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40063843",
      "title": "Gamified Adaptive Approach Bias Modification in Individuals With Methamphetamine Use History From Communities in Sichuan: Pilot Randomized Controlled Trial.",
      "abstract": "BACKGROUND: Cognitive bias modification (CBM) programs have shown promise in treating psychiatric conditions, but they can be perceived as boring and repetitive. Incorporating gamified designs and adaptive algorithms in CBM training may address this issue and enhance engagement and effectiveness. OBJECTIVES: This study aims to gather preliminary data and assess the preliminary efficacy of an adaptive approach bias modification (A-ApBM) paradigm in reducing cue-induced craving in individuals with methamphetamine use history. METHODS: A randomized controlled trial with 3 arms was conducted. Individuals aged 18-60 years with methamphetamine dependence and at least 1 year of methamphetamine use were recruited from 12 community-based rehabilitation centers in Sichuan, China. Individuals with the inability to fluently operate a smartphone and the presence of mental health conditions other than methamphetamine use disorder were excluded. The A-ApBM group engaged in ApBM training using a smartphone app for 4 weeks. The A-ApBM used an adaptive algorithm to dynamically adjust the difficulty level based on individual performance. Cue-induced craving scores and relapses were assessed using a visual analogue scale at baseline, postintervention, and at week-16 follow-up. RESULTS: A total of 136 participants were recruited and randomized: 48 were randomized to the A-ApBM group, 48 were randomized to the static approach bias modification (S-ApBM) group, and 40 were randomized to the no-intervention control group. The A-ApBM group showed a significant reduction in cue-induced craving scores at postintervention compared with baseline (Cohen d=0.34; P<.01; 95% CI 0.03-0.54). The reduction remained significant at the week-16 follow-up (Cohen d=0.40; P=.01; 95% CI 0.18-0.57). No significant changes were observed in the S-ApBM and control groups. CONCLUSIONS: The A-ApBM paradigm with gamified designs and dynamic difficulty adjustments may be an effective intervention for reducing cue-induced craving in individuals with methamphetamine use history. This approach improves engagement and personalization, potentially enhancing the effectiveness of CBM programs. Further research is needed to validate these findings and explore the application of A-ApBM in other psychiatric conditions.",
      "journal": "JMIR serious games",
      "year": "2025",
      "doi": "10.2196/56978",
      "authors": "Shen Danlin et al.",
      "keywords": "cognitive bias modification; digital therapeutics; effectiveness; engagement; game; gamified design; methamphetamine; pilot RCT; psychiatric; randomized controlled trial; smartphone app; substance use disorder",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40063843/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11931399",
      "ft_text_length": 35803,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11931399)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40064867",
      "title": "Equitable machine learning counteracts ancestral bias in precision medicine.",
      "abstract": "Gold standard genomic datasets severely under-represent non-European populations, leading to inequities and a limited understanding of human disease. Therapeutics and outcomes remain hidden because we lack insights that could be gained from analyzing ancestrally diverse genomic data. To address this significant gap, we present PhyloFrame, a machine learning method for equitable genomic precision medicine. PhyloFrame corrects for ancestral bias by integrating functional interaction networks and population genomics data with transcriptomic training data. Application of PhyloFrame to breast, thyroid, and uterine cancers shows marked improvements in predictive power across all ancestries, less model overfitting, and a higher likelihood of identifying known cancer-related genes. Validation in fourteen ancestrally diverse datasets demonstrates that PhyloFrame is better able to adjust for ancestry bias across all populations. The ability to provide accurate predictions for underrepresented groups, in particular, is substantially increased. Analysis of performance in the most diverse continental ancestry group, African, illustrates how phylogenetic distance from training data negatively impacts model performance, as well as PhyloFrame's capacity to mitigate these effects. These results demonstrate how equitable artificial intelligence (AI) approaches can mitigate ancestral bias in training data and contribute to equitable representation in medical research.",
      "journal": "Nature communications",
      "year": "2025",
      "doi": "10.1038/s41467-025-57216-8",
      "authors": "Smith Leslie A et al.",
      "keywords": "",
      "mesh_terms": "Humans; Machine Learning; Precision Medicine; Genomics; Phylogeny; Female; Breast Neoplasms; Neoplasms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40064867/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11894161",
      "ft_text_length": 88050,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11894161)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40106555",
      "title": "Reducing hepatitis C diagnostic disparities with a fully automated deep learning-enabled microfluidic system for HCV antigen detection.",
      "abstract": "Viral hepatitis remains a major global health issue, with chronic hepatitis B (HBV) and hepatitis C (HCV) causing approximately 1 million deaths annually, primarily due to liver cancer and cirrhosis. More than 1.5 million people contract HCV each year, disproportionately affecting vulnerable populations, including American Indians and Alaska Natives (AI/AN). While direct-acting antivirals (DAAs) are highly effective, timely and accurate HCV diagnosis remains a challenge, particularly in resource-limited settings. The current two-step HCV testing process is costly and time-intensive, often leading to patient loss before treatment. Point-of-care (POC) HCV antigen (Ag) testing offers a promising alternative, but no FDA-approved test meets the required sensitivity and specificity. To address this, we developed a fully automated, smartphone-based POC HCV Ag assay using platinum nanoparticles, deep learning image processing, and microfluidics. With an overall accuracy of 94.59%, this cost-effective, portable device has the potential to reduce HCV-related health disparities, particularly among AI/AN populations, improving accessibility and equity in care.",
      "journal": "Science advances",
      "year": "2025",
      "doi": "10.1126/sciadv.adt3803",
      "authors": "Chen Hui et al.",
      "keywords": "",
      "mesh_terms": "Humans; Deep Learning; Hepatitis C; Hepacivirus; Hepatitis C Antigens; Microfluidics; Smartphone; Point-of-Care Systems",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40106555/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11922049",
      "ft_text_length": 52831,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11922049)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40124859",
      "title": "AI Horizons in Indian Healthcare: A Vision for Transformation and Equity.",
      "abstract": "Artificial intelligence (AI) is poised to revolutionize healthcare delivery in India, offering solutions to address the nation's unique healthcare challenges. This position paper, presented by the Indian Association of Preventive and Social Medicine, examines the integration of AI in Indian healthcare, exploring its applications across diagnostic imaging, patient care, medical research, rehabilitation, and administrative processes. Notable implementations include AI-driven disease detection systems, telemedicine platforms, and public health surveillance tools, with successful applications in tuberculosis screening, breast cancer detection, and ophthalmological care. While these advancements show promise, significant challenges persist, related to data privacy concerns and interoperability issues, including the need for robust ethical frameworks. The paper highlights key stakeholder collaborations, including government initiatives and international partnerships, which are driving innovation in this space. Based on this analysis, we propose policy recommendations emphasizing research investment, professional training, and regulatory frameworks to ensure responsible AI adoption. Our vision advocates for an approach that balances technological advancement with accessibility and equity in healthcare delivery.",
      "journal": "Indian journal of community medicine : official publication of Indian Association of Preventive & Social Medicine",
      "year": "2024",
      "doi": "10.4103/ijcm.ijcm_806_24",
      "authors": "Kapoor Neelesh et al.",
      "keywords": "Artificial intelligence; India; digital health; healthcare; medical innovation; public health",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40124859/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11927818",
      "ft_text_length": 23406,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11927818)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40182428",
      "title": "Unveiling patient profiles associated with elevated Lp(a) through an unbiased clustering analysis.",
      "abstract": "INTRODUCTION: Lipoprotein(a) [Lp(a)] has been recognized as key factor in cardiovascular research. This study aimed to identify key patient profiles based on the characteristics of a Portuguese cohort of adults who were referred for Lp(a) measurement. METHOD: An unsupervised clustering analysis was performed on 661 Portuguese adults to identify patient profiles associated with lipoprotein a [Lp(a)] based on a range of demographic and clinical indicators. Lp(a) levels were deliberately excluded from the algorithm, to ensure an unbiased cluster formation. RESULTS: The analysis revealed two distinct clusters based on Lp(a) levels. Cluster 1 (n\u2009=\u2009336) exhibited significantly higher median Lp(a) levels than Cluster 2 (n\u2009=\u2009325; p\u2009=\u20090.004), with 46.4% of individuals exceeding the 75\u2005nmol/L (30\u2005mg/dl) risk threshold (p\u2009<\u20090.001). This group was characterized by older age (median 57 vs. 45 years), lower body mass index (27.17 vs. 29.40), and a majority male composition (73.8% vs. 26.5%). Additionally, Cluster 1 displayed a higher prevalence of hypertension (56.5% vs. 31.1%), diabetes mellitus (38.7% vs. 17.2%), and dyslipidemia (88.7% vs. 55.4%). These data suggest that the Cluster 1 profile has a potential increased risk for cardiovascular complications and underscore the importance of considering specific patient profiles for Lp(a) screening and cardiovascular risk assessment. CONCLUSION: Despite the study limitations, including single-institution data and potential selection bias, this study highlights the utility of cluster analysis in identifying clinically meaningful patient profiles and suggests that proactive screening and management of Lp(a) levels, particularly in patients with characteristics resembling those of Cluster 1, may be beneficial.",
      "journal": "Frontiers in cardiovascular medicine",
      "year": "2025",
      "doi": "10.3389/fcvm.2025.1546351",
      "authors": "Saraiva Miguel et al.",
      "keywords": "cardiovascular risk assessment; clustering analysis; lipoprotein(a) levels; patient profiling; unsupervised learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40182428/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11965613",
      "ft_text_length": 19025,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11965613)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40209572",
      "title": "Global disparities in artificial intelligence-based mammogram interpretation for breast cancer: A scientometric analysis of representation, trends, and equity.",
      "abstract": "BACKGROUND: Breast cancer (BC) is the most frequently diagnosed cancer and the leading cause of cancer death among women worldwide. Artificial intelligence (AI) shows promise for improving mammogram interpretation, especially in resource-limited settings. However, concerns remain regarding the diversity of datasets and the representation of researchers in AI model development, which may affect the models' generalizability, fairness, and equity. METHODS: We performed a scientometric analysis of studies published in 2017, 2018, 2022, and 2023 that used screening or diagnostic mammograms for BC detection to train or validate AI algorithms. PubMed (MEDLINE) and EMBASE were searched in July 2024. Data extraction focused on patient cohort sociodemographics (including age and race/ethnicity), geographic distribution (categorized by World Bank country income levels and regions), and author profiles (sex, affiliation, and funding sources). RESULTS: Of 5774 studies identified, 264 met the inclusion criteria. The number of studies increased from 28 in 2017 to 115 in 2023-a 311 % increase. Despite this growth, only 0-25\u202f% of studies reported race/ethnicity, with most patients identified as Caucasian. Moreover, nearly all patient cohorts originated from high-income countries, with no studies from low-income settings. Author affiliations were predominantly from high-income regions, and gender imbalance was observed among first and last authors. CONCLUSION: The lack of racial, ethnic, and geographic diversity in both datasets and researcher representation could undermine the generalizability and fairness of AI-based mammogram interpretation. Addressing these disparities through diverse dataset collection and inclusive international collaborations is critical to ensuring equitable improvements in breast cancer care.",
      "journal": "European journal of cancer (Oxford, England : 1990)",
      "year": "2025",
      "doi": "10.1016/j.ejca.2025.115394",
      "authors": "Miyawaki Isabele A et al.",
      "keywords": "Artificial intelligence; Breast cancer; Mammogram; Mammography; Radiology; Scientometric",
      "mesh_terms": "Humans; Breast Neoplasms; Artificial Intelligence; Female; Mammography; Healthcare Disparities; Early Detection of Cancer; Global Health; Health Equity",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40209572/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12185668",
      "ft_text_length": 14303,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12185668)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40225881",
      "title": "I-SIRch: AI-powered concept annotation tool for equitable extraction and analysis of safety insights from maternity investigations.",
      "abstract": "BACKGROUND: Maternity care is a complex system involving treatments and interactions between patients, healthcare providers, and the care environment. To enhance patient safety and outcomes, it is crucial to understand the human factors (e.g. individuals' decisions, local facilities) influencing healthcare. However, most current tools for analysing healthcare data focus only on biomedical concepts (e.g. health conditions, procedures and tests), overlooking the importance of human factors. METHODS: We developed a new approach called I-SIRch, using artificial intelligence to automatically identify and label human factors concepts in maternity investigation reports describing adverse maternity incidents produced by England's Healthcare Safety Investigation Branch (HSIB). These incident investigation reports aim to identify opportunities for learning and improving maternal safety across the entire healthcare system. Unlike existing clinical annotation tools that extract solely biomedical insights, I-SIRch is uniquely designed to capture the socio-technical dimensions of patient safety incidents. This innovation enables a more comprehensive analysis of the complex systemic issues underlying adverse events in maternity care, providing insights that were previously difficult to obtain at scale. Importantly, I-SIRch employs a hybrid approach, incorporating human expertise to validate and refine the AI-generated annotations, ensuring the highest quality of analysis. FINDINGS: I-SIRch was trained using real data and tested on both real and synthetic data to evaluate its performance in identifying human factors concepts. When applied to real reports, the model achieved a high level of accuracy, correctly identifying relevant concepts in 90% of the sentences from 97 reports (Balanced Accuracy of 90% \u00b1 18% (Recall 93% \u00b1 18%, Precision 87% \u00b1 34%, F-score 96% \u00b1 10%). Applying I-SIRch to analyse these reports revealed that certain human factors disproportionately affected mothers from different ethnic groups. In particular, gaps in risk assessment were more prevalent for minority mothers, whilst communication issues were common across all groups but potentially more for minorities. INTERPRETATION: Our work demonstrates the potential of using automated tools to identify human factors concepts in maternity incident investigation reports, rather than focusing solely on biomedical concepts. This approach opens up new possibilities for understanding the complex interplay between social, technical and organisational factors influencing maternal safety and population health outcomes. By taking a more comprehensive view of maternal healthcare delivery, we can develop targeted interventions to address disparities and improve maternal outcomes. Targeted interventions to address these disparities could include culturally sensitive risk assessment protocols, enhanced language support, and specialised training for healthcare providers on recognising and mitigating biases. These findings highlight the need for tailored approaches to improve equitable care delivery and outcomes in maternity services. The I-SIRch framework thus represents a significant advancement in our ability to extract actionable intelligence from healthcare incident reports, moving beyond traditional clinical factors to encompass the broader systemic issues that impact patient safety.",
      "journal": "International journal of population data science",
      "year": "2024",
      "doi": "10.23889/ijpds.v9i2.2439",
      "authors": "Singh Mohit Kumar et al.",
      "keywords": "maternal care",
      "mesh_terms": "Humans; Female; Patient Safety; Artificial Intelligence; Pregnancy; England; Maternal Health Services",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40225881/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11986904",
      "ft_text_length": 49322,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11986904)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40269225",
      "title": "Author Correction: Transforming healthcare through just, equitable and quality driven artificial intelligence solutions in South Asia.",
      "abstract": "",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01639-6",
      "authors": "Adhikari Sushmita et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Published Erratum",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40269225/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12019597",
      "ft_text_length": 445,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12019597)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40276990",
      "title": "Component Associations of the Healthy Worker Survivor Bias in Medical Radiation Workers.",
      "abstract": "BACKGROUND: The healthy worker survivor bias may vary by sex. This study investigated three component associations necessary for this bias to determine the origins of sex differences in this bias among male and female workers. METHODS: We analyzed a data set of 93,918 South Korean diagnostic medical radiation workers registered in the National Dose Registry from 1996 to 2011, linked with mortality and cancer incidence data. Component associations were assessed using Cox regression to estimate hazard ratios (HRs) and logistic regression with generalized estimating equations to estimate odds ratios (ORs). RESULTS: A significant association between prior cumulative exposure and employment status was observed for all-cause mortality in male (HR 1.06, 95% CI 1.02-1.10), whereas an inverse association was noted in female workers (HR 0.82, 95% CI 0.78-0.87). Adjusted ORs for employment status and subsequent exposure for all-cause mortality, as well as HRs for employment status and survival time, demonstrated associations in the same direction in both males and females. CONCLUSIONS: Our findings demonstrate that sex-specific differences in healthy worker survivor bias were primarily driven by the association between prior exposure and employment status. To improve bias mitigation in occupational cohort studies, sex-specific components should be incorporated.",
      "journal": "American journal of industrial medicine",
      "year": "2025",
      "doi": "10.1002/ajim.23727",
      "authors": "Lee Won Jin et al.",
      "keywords": "bias; cohort; health professionals; ionizing radiation; occupational exposure",
      "mesh_terms": "Humans; Male; Female; Middle Aged; Occupational Exposure; Adult; Republic of Korea; Healthy Worker Effect; Employment; Sex Factors; Registries; Bias; Survivors; Proportional Hazards Models; Aged; Logistic Models; Odds Ratio",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40276990/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12070152",
      "ft_text_length": 15424,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12070152)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40291888",
      "title": "Magnetic resonance imaging bias field correction improves tumor prognostic evaluation after transcatheter arterial chemoembolization for liver cancer.",
      "abstract": "BACKGROUND: Transcatheter arterial chemoembolization (TACE) is a key treatment approach for advanced invasive liver cancer (infiltrative hepatocellular carcinoma). However, its therapeutic response can be difficult to evaluate accurately using conventional two-dimensional imaging criteria due to the tumor's diffuse and multifocal growth pattern. Volumetric imaging, especially enhanced tumor volume (ETV), offers a more comprehensive assessment. Nonetheless, bias field inhomogeneity in magnetic resonance imaging (MRI) poses challenges, potentially skewing volumetric measurements and undermining prognostic evaluation. AIM: To investigate whether MRI bias field correction enhances the accuracy of volumetric assessment of infiltrative hepatocellular carcinoma treated with TACE, and to analyze how this improved measurement impacts prognostic prediction. METHODS: We retrospectively collected data from 105 patients with invasive liver cancer who underwent TACE treatment at the Affiliated Hospital of Xuzhou Medical University from January 2020 to January 2024. The improved N4 bias field correction algorithm was applied to process MRI images, and the ETV before and after treatment was calculated. The ETV measurements before and after correction were compared, and their relationship with patient prognosis was analyzed. A Cox proportional hazards model was used to evaluate prognostic factors, with Martingale residual analysis determining the optimal cutoff value, followed by survival analysis. RESULTS: Bias field correction significantly affected ETV measurements, with the corrected baseline ETV mean (505.235 cm\u00b3) being significantly lower than before correction (825.632 cm\u00b3, P < 0.001). Cox analysis showed that the hazard ratio (HR) for corrected baseline ETV (HR = 1.165, 95%CI: 1.069-1.268) was higher than before correction (HR = 1.063, 95%CI: 1.031-1.095). Using 412 cm\u00b3 as the cutoff, the group with baseline ETV < 415 cm\u00b3 had a longer median survival time compared to the \u2265 415 cm\u00b3 group (18.523 months vs 8.926 months, P < 0.001). The group with an ETV reduction rate \u2265 41% had better prognosis than the < 41% group (17.862 months vs 9.235 months, P = 0.006). Multivariate analysis confirmed that ETV reduction rate (HR = 0.412, P < 0.001), Child-Pugh classification (HR = 0.298, P < 0.001), and Barcelona Clinic Liver Cancer stage (HR = 0.578, P = 0.045) were independent prognostic factors. CONCLUSION: Volume imaging based on MRI bias field correction can improve the accuracy of evaluating the efficacy of TACE treatment for invasive liver cancer. The corrected ETV and its reduction rate can serve as independent indicators for predicting patient prognosis, providing important reference for developing individualized treatment strategies.",
      "journal": "World journal of gastrointestinal surgery",
      "year": "2025",
      "doi": "10.4240/wjgs.v17.i4.104187",
      "authors": "Liu Ke et al.",
      "keywords": "Bias field correction; Invasive liver cancer; Magnetic resonance imaging; Transcatheter arterial chemoembolization; Volume imaging",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40291888/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12019036",
      "ft_text_length": 41250,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12019036)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40344545",
      "title": "Assessing Algorithmic Fairness With a Multimodal Artificial Intelligence Model in Men of African and Non-African Origin on NRG Oncology Prostate Cancer Phase III Trials.",
      "abstract": "PURPOSE: Artificial intelligence (AI) tools could improve clinical decision making or exacerbate inequities because of bias. African American (AA) men reportedly have a worse prognosis for prostate cancer (PCa) and are underrepresented in the development genomic biomarkers. We assess the generalizability of tools developed using a multimodal AI (MMAI) deep learning system using digital histopathology and clinical data from NRG/Radiation Therapy Oncology Group PCa trials across racial subgroups. METHODS: In total, 5,708 patients from five randomized phase III trials were included. Two MMAI algorithms were evaluated: (1) the distant metastasis (DM) MMAI model optimized to predict risk of DM, and (2) the PCa-specific mortality (PCSM) MMAI model optimized to focus on prediction death in the presence of DM (DDM). The prognostic performance of the MMAI algorithms was evaluated in AA and non-AA subgroups using time to DM (primary end point) and time to DDM (secondary end point). Exploratory end points included time to biochemical failure and overall survival with Fine-Gray or Cox proportional hazards models. Cumulative incidence estimates were computed for time-to-event end points and compared using Gray's test. RESULTS: There were 948 (16.6%) AA patients, 4,731 non-AA patients (82.9%), and 29 (0.5%) patients with unknown or missing race status. The DM-MMAI algorithm showed a strong prognostic signal for DM in the AA (subdistribution hazard ratio [sHR], 1.2 [95% CI, 1.0 to 1.3]; P = .007) and non-AA subgroups (sHR, 1.4 [95% CI, 1.3 to 1.5]; P < .001). Similarly, the PCSM-MMAI score showed a strong prognostic signal for DDM in both AA (sHR, 1.3 [95% CI, 1.1 to 1.5]; P = .001) and non-AA subgroups (sHR, 1.5 [95% CI, 1.4 to 1.6]; P < .001), with similar distributions of risk. CONCLUSION: Using cooperative group data sets with a racially diverse population, the MMAI algorithm performed well across racial subgroups without evidence of algorithmic bias.",
      "journal": "JCO clinical cancer informatics",
      "year": "2025",
      "doi": "10.1200/CCI-24-00284",
      "authors": "Roach Mack et al.",
      "keywords": "",
      "mesh_terms": "Aged; Humans; Male; Middle Aged; Algorithms; Artificial Intelligence; Black or African American; Clinical Trials, Phase III as Topic; Prognosis; Prostatic Neoplasms",
      "pub_types": "Clinical Trial, Phase III; Journal Article; Randomized Controlled Trial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40344545/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12335010",
      "ft_text_length": 1896,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12335010)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40354171",
      "title": "Mitigating Bias in Machine Learning Models with Ethics-Based Initiatives: The Case of Sepsis.",
      "abstract": "This paper discusses ethics-based strategies for mitigating bias in machine learning models used to predict sepsis onset. The first part discusses how various kinds of bias and their potential synergies can reduce predictive accuracy, especially as those biases derive from social determinants of health (SDOHs) and from the design and construction of the predictive model. The second part of the essay discusses how certain ethically-based strategies might mitigate the potential for disparate or unfair treatment produced by these models, not only as they might apply to sepsis but to any syndrome that witnesses the impact of adverse SDOHs on socioeconomically disadvantaged or marginalized populations.",
      "journal": "The American journal of bioethics : AJOB",
      "year": "2026",
      "doi": "10.1080/15265161.2025.2497971",
      "authors": "D Banja John et al.",
      "keywords": "Bias; equity; ethics; machine learning; sepsis; social determinants of health",
      "mesh_terms": "Humans; Machine Learning; Sepsis; Social Determinants of Health; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40354171/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12353398",
      "ft_text_length": 706,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12353398)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40387721",
      "title": "Predictive models for low birth weight: a comparative analysis of algorithmic fairness-improving approaches.",
      "abstract": "OBJECTIVE: Evaluating whether common algorithmic fairness-improving approaches can improve low-birth-weight predictive model performance can provide important implications for population health management and health equity. This study aimed to evaluate alternative approaches for improving algorithmic fairness for low-birth-weight predictive models. STUDY DESIGN: Retrospective, cross-sectional study of birth certificates linked with medical insurance claims. METHODS: Birth certificates (n\u2009=\u2009191,943; 2014-2022) were linked with insurance claims (2013-2021) from the Arkansas All-Payer Claims Database to assess alternative approaches for algorithmic fairness in predictive models for low birth weight (<\u20092500 g). We fit an original model and compared 6 fairness-improving approaches using elastic net models trained and tested with 70/30 balanced random split samples and 10-fold cross validation. RESULTS: The original model had lower accuracy (percent predicted correctly) in predicting low birth weight among Black, Native Hawaiian/Other Pacific Islander, Asian, and unknown racial/ethnic populations relative to White individuals. For Black individuals, accuracy increased with all 6 fairness-improving approaches relative to the original model; however, sensitivity (true-positives correctly predicted as low birth weight) significantly declined, as much as 31% (from 0.824 to 0.565), in 5 of 6 approaches. CONCLUSIONS: When developing and implementing decision-making algorithms, it is critical that model performance metrics align with management goals for the predictive tool. In our study, fairness-improving models improved accuracy and area under the curve scores for Black individuals but decreased sensitivity and negative predictive value, suggesting that the original model, although unfair, was not improved. Implementation of unfair models for allocating preventive services could perpetuate racial/ethnic inequities by failing to identify individuals most at risk for a low-birth-weight delivery.",
      "journal": "The American journal of managed care",
      "year": "2025",
      "doi": "10.37765/ajmc.2025.89737",
      "authors": "Brown Clare C et al.",
      "keywords": "",
      "mesh_terms": "Humans; Retrospective Studies; Infant, Newborn; Cross-Sectional Studies; Female; Algorithms; Infant, Low Birth Weight; Male; Birth Certificates; Arkansas; Adult",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40387721/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12109546",
      "ft_text_length": 1891,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12109546)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40446336",
      "title": "The Role of Digital Health Equity Audits in Preventing Harmful Infodemiology.",
      "abstract": "BACKGROUND: Health disparities persist and are influenced by digital transformation. Although digital tools offer opportunities, they can also exacerbate existing inequalities, a problem amplified by the COVID-19 pandemic and the related infodemic. Health equity audit (HEA) tools, such as those developed in the United Kingdom, provide a framework to assess equity but require adaptation for the digital context. Digital determinants of health (DDoH) are increasingly recognized as crucial factors influencing health outcomes in the digital era. OBJECTIVE: This editorial proposes an approach to extend HEA principles to create a specific framework, the digital health equity audit (DHEA), designed to systematically assess and address health inequities within the design, implementation, and evaluation of digital health technologies, with a focus on DDoH. METHODS: We propose a cyclical DHEA model based on existing HEA principles, integrating them with digital health equity frameworks. The DHEA cycle comprises six phases: (1) scoping the audit and mobilizing the team (including community members); (2) developing the digital health equity profile and identifying inequities (assessing DDoH at individual, interpersonal, community, and societal levels); (3) identifying high-impact actions to address DDoH and inequities; (4) prioritizing actions for maximum equity impact; (5) implementing and supporting change; and (6) evaluating progress and impact, and refining. This method emphasizes multilevel interventions and stakeholder engagement. RESULTS: The main result is the articulation of the DHEA framework: a structured, 6-phase cyclical model to guide organizations in the analysis and proactive mitigation of digital health-related disparities. The framework explicitly integrates the assessment of DDoH across multiple levels (individual, interpersonal, community, societal) and promotes the development of targeted interventions to ensure digital solutions promote equity. CONCLUSIONS: The DHEA model offers an integrated approach to consider social, epidemiological, health, and technological variables, aiming to reduce health inequities through the conscious use of new technologies. It is emphasized that digital technologies can be the cause or the solution to inequalities; DHEAs are proposed as a tool to foster equity. Its systematic adoption, along with a collaborative approach (co-design) and trust building, can help ensure that the benefits of health digitization are equitably distributed while strengthening trust in institutions. Continued attention is needed to manage emerging challenges such as infodemiology in the era of big data and artificial intelligence.",
      "journal": "JMIR infodemiology",
      "year": "2025",
      "doi": "10.2196/75495",
      "authors": "Biondi Massimiliano et al.",
      "keywords": "audit; digital; equity; infodemiology; quality of health care",
      "mesh_terms": "Humans; Health Equity; COVID-19; United Kingdom; SARS-CoV-2; Digital Technology; Pandemics; Social Determinants of Health; Digital Health",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40446336/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12143845",
      "ft_text_length": 14717,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12143845)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40494422",
      "title": "Tailoring task arithmetic to address bias in models trained on multi-institutional datasets.",
      "abstract": "OBJECTIVE: Multi-institutional datasets are widely used for machine learning from clinical data, to increase dataset size and improve generalization. However, deep learning models in particular may learn to recognize the source of a data element, leading to biased predictions. For example, deep learning models for image recognition trained on chest radiographs with COVID-19 positive and negative examples drawn from different data sources can respond to indicators of provenance (e.g., radiological annotations outside the lung area per institution-specific practices) rather than pathology, generalizing poorly beyond their training data. Bias of this sort, called confounding by provenance, is of concern in natural language processing (NLP) because provenance indicators (e.g., institution-specific section headers, or region-specific dialects) are pervasive in language data. Prior work on addressing such bias has focused on statistical methods, without providing a solution for deep learning models for NLP. METHODS: Recent work in representation learning has shown that representing the weights of a trained deep network as task vectors allows for their arithmetic composition to govern model capabilities towards desired behaviors. In this work, we evaluate the extent to which reducing a model's ability to distinguish between contributing sites with such task arithmetic can mitigate confounding by provenance. To do so, we propose two model-agnostic methods, Task Arithmetic for Provenance Effect Reduction (TAPER) and Dominance-Aligned Polarized Provenance Effect Reduction (DAPPER), extending the task vectors approach to a novel problem domain. RESULTS: Evaluation on three datasets shows improved robustness to confounding by provenance for both RoBERTa and Llama-2 models with the task vector approach, with improved performance at the extremes of distribution shift. CONCLUSION: This work emphasizes the importance of adjusting for confounding by provenance, especially in extreme cases of the shift. In use of deep learning models, DAPPER and TAPER show efficiency in mitigating such bias. They provide a novel mitigation strategy for confounding by provenance, with broad applicability to address other sources of bias in composite clinical data sets. Source code is available within the DeconDTN toolkit: https://github.com/LinguisticAnomalies/DeconDTN-toolkit.",
      "journal": "Journal of biomedical informatics",
      "year": "2025",
      "doi": "10.1016/j.jbi.2025.104858",
      "authors": "Ding Xiruo et al.",
      "keywords": "Confounding shift; Large language models; Low-rank adaptation (LoRA); Multi-institutional datasets; Robustness; Task arithmetic",
      "mesh_terms": "Humans; COVID-19; Natural Language Processing; Deep Learning; SARS-CoV-2; Bias; Machine Learning; Datasets as Topic; Databases, Factual",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40494422/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12282651",
      "ft_text_length": 2277,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12282651)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40494563",
      "title": "Evaluating Equity in Usage and Effectiveness of the CONCERN Early Warning System.",
      "abstract": "The CONCERN Early Warning System (CONCERN EWS) is an artificial intelligence-based clinical decision support system (AI-CDSS) for the prediction of clinical deterioration, leveraging signals from nursing documentation patterns. While a recent multisite randomized controlled trial (RCT) demonstrated its effectiveness in reducing inpatient mortality and length of stay, evaluating implementation outcomes is essential to ensure equitable results across patient populations.This study aims to (1) assess whether clinicians' usage of the CONCERN EWS, as measured by CONCERN Detailed Prediction Screen launches, varied by patient demographic characteristics, including sex, race, ethnicity, and primary language; (2) evaluate whether CONCERN EWS's effectiveness in reducing the risk of in-hospital mortality varied across patient demographic groups.We conducted a retrospective observational analysis of electronic health record log files and clinical outcomes from a multisite, pragmatic, cluster-RCT involving four hospitals across two health care systems. Equity in usage was assessed by comparing CONCERN Detailed Prediction Screen launches across demographic groups, and effectiveness was examined by comparing the risk of in-hospital mortality between intervention and usual care groups using Cox proportional hazards models adjusted for patient characteristics.Clinicians' CONCERN Detailed Prediction Screen launches did not significantly differ by patients' demographic characteristics, suggesting equitable usage. The CONCERN EWS was significantly associated with reduced risk of in-hospital mortality overall (adjusted hazard ratio [HR]\u2009=\u20090.644, 95% CI: 0.532-0.778, p\u2009<\u20090.0001), with consistent effectiveness across most groups. Notably, patients whose primary language was not English experienced a greater reduction of mortality risk compared to patients whose primary language was English (adjusted HR\u2009=\u20090.419, 95% CI: 0.287-0.610, p\u2009=\u20090.0082).This study presents a case of evaluating equity in AI-CDSS usage and effectiveness, contributing to the limited literature. While findings suggest equitable engagement and effectiveness, ongoing evaluations are needed to understand the observed variability and ensure responsible implementation.",
      "journal": "Applied clinical informatics",
      "year": "2025",
      "doi": "10.1055/a-2630-4192",
      "authors": "Lee Rachel Y et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Male; Hospital Mortality; Decision Support Systems, Clinical; Middle Aged; Aged; Artificial Intelligence; Electronic Health Records",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40494563/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12349966",
      "ft_text_length": 2311,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12349966)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40521229",
      "title": "Socio-Ecological Determinants of Food Pantry Use Among Food Insecure Racial and Ethnic Diverse College Students.",
      "abstract": "College students are disproportionately more food insecure as compared to the general U.S. population with racial and ethnic minority students at greater risk. The purpose of this study is to explore socio-ecological characteristics of food pantry utilization among food insecure Black/African American non-Hispanic and Hispanic college students employing secondary data analyses of a larger cross-sectional study of food insecurity among college students. The current study sample (n = 460) was comprised of 174 self-identified Black/African American non-Hispanic, 26 Black/African American & Hispanic, and 260 Hispanic participants. Food pantry use served as the dependent variable. Multi-level independent variables included Individual, Interpersonal, and Community Level factors. A multivariate logistic regression model analyzed the relationship between food pantry use and independent variables found to be significant in earlier independent sample t-tests and chi-square analyses. Statistical significance was set at P < .05 and a confidence interval (CI) of 95%. \"Saving\" coping mechanisms (OR = 1.15, 95% CI [1.082-1.231] and discrimination experiences of day-to-day unfair treatment (OR = 1.04, 95% CI [.1.000-1.077]) were predictive of food pantry utilization. This study suggests food pantry use among food insecure racial and ethnic minority college students may be influenced by socio-cultural influences at both interpersonal and community levels. Implications include interventions developed with a health equity framework.",
      "journal": "American journal of lifestyle medicine",
      "year": "2025",
      "doi": "10.1177/15598276251351585",
      "authors": "DeBate Rita et al.",
      "keywords": "college students; food insecurity; food pantry",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40521229/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12158980",
      "ft_text_length": 1541,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12158980)",
      "ft_reason": "Excluded: insufficient approach content (1 indicators)"
    },
    {
      "pmid": "40532198",
      "title": "Applications and Outcomes of Telehealth and Integrated Care in Men's Health Urology.",
      "abstract": "Men's health, particularly in the domain of urology, faces significant challenges in access to care, patient outcomes, and cost efficiency. Despite advances in medical treatment, conditions such as prostate cancer remain a leading cause of cancer-related death among men, with African American men disproportionately affected at twice the mortality rate of other groups. Compounding these challenges is a critical shortage of urologists, with 62% of US counties lacking a practicing urologist and only 1 new urologist entering the field for every 10 retiring. This shortage results in delayed diagnoses, increased rates of advanced-stage conditions, and significant health disparities. To address these pressing issues, telehealth and technology-based integrated care models present a promising solution. Telehealth expands access to specialized urological care by overcoming geographical barriers and offering virtual consultations, at-home diagnostics, and continuous patient engagement. Artificial intelligence-driven tools further enhance the efficiency and accuracy of care delivery, improving provider experience by automating administrative tasks and facilitating early intervention through predictive analytics. Furthermore, remote patient monitoring devices provide accurate, cost-effective, and highly accessible alternatives. These innovations reduce provider burnout, lower health care costs, and, critically, improve patient outcomes. This paper explores the potential of telehealth and integrated care in men's health urology as a practical pathway to bridging access gaps, enhancing care quality, and achieving cost savings. By leveraging digital health solutions, health care systems and employers can promote health equity, increase engagement, and ensure that all men receive timely and effective urological care.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/69095",
      "authors": "Bisset Bryce et al.",
      "keywords": "integrated care; men's health urology; quadruple aim; telehealth; trends in urology",
      "mesh_terms": "Humans; Telemedicine; Male; Men's Health; Delivery of Health Care, Integrated; Urology; Prostatic Neoplasms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40532198/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12192914",
      "ft_text_length": 26105,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12192914)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40534902",
      "title": "Proceedings of the Association for Pathology Informatics Bootcamp 2023: Ethics, equity, and regulations.",
      "abstract": "The Pathology Informatics Bootcamp is held annually at the Pathology Informatics Summit and provides pathology trainees with knowledge about both core and emerging topics in the rapidly evolving field of Pathology Informatics. In 2023, the Bootcamp focused on the applications of ethics, equity, and regulations pertinent to pathology informatics, with emphasis on the importance of these topics in the rapidly evolving landscape of artificial intelligence in pathology and lab medicine practice. Session topics are mapped to Pathology Informatics Essentials for Residents outlines to highlight the significance of these topics in pathology practice overall, and more so within informatics practice. The curriculum included lectures on data use in the clinical lab and in digital pathology, equitable use of lab data in daily practice and downstream use, and practical application of regulations for data with clinical decision-support, accreditation, and management of patient results.",
      "journal": "Journal of pathology informatics",
      "year": "2025",
      "doi": "10.1016/j.jpi.2025.100452",
      "authors": "Stoffel Michelle R et al.",
      "keywords": "Data; Equity and regulations; Ethics; Pathology informatics",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40534902/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12174555",
      "ft_text_length": 29233,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12174555)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40547595",
      "title": "Artificial Intelligence-Based Methods: The Path Forward in Achieving Equity in Lung Cancer Screening and Evaluation.",
      "abstract": "Although lung cancer remains a global threat to public health, evidenced based advances in screening and prevention hold promise for reducing its impact on mortality. An ongoing challenge facing the clinical and research community are the glaring disparities in access to preventive services faced by ethnically and socioeconomically marginalized groups. In this context, novel approaches are needed to improve research methods and thus bolster our ability to improve outcomes. Artificial intelligence (AI) applications such as machine learning and natural language processing hold promise as catalysts in this process, enhancing speed, accuracy and capability. This perspective will highlight the potential of AI methods as essential tool for growth across the lung cancer diagnostic continuum from screening to diagnosis.",
      "journal": "Cancer innovation",
      "year": "2025",
      "doi": "10.1002/cai2.70019",
      "authors": "Kuperberg Stephen J et al.",
      "keywords": "artificial intelligence; barriers; disparities; equity; lung cancer; machine learning; natural language processing; socioeconomic",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40547595/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12179435",
      "ft_text_length": 8887,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12179435)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40558623",
      "title": "From Data to Decisions: Leveraging Retrieval-Augmented Generation to Balance Citation Bias in Burn Management Literature.",
      "abstract": "(1) Burn injuries demand multidisciplinary, evidence-based care, yet the extensive literature complicates timely decision making. Retrieval-augmented generation (RAG) synthesizes research while addressing inaccuracies in pretrained models. However, citation bias in sourcing for RAG often prioritizes highly cited studies, overlooking less-cited but valuable research. This study examines RAG's performance in burn management, comparing citation levels to enhance evidence synthesis, reduce selection bias, and guide decisions. (2) Two burn management datasets were assembled: 30 highly cited (mean: 303) and 30 less-cited (mean: 21). The Gemini-1.0-Pro-002 RAG model addressed 30 questions, ranging from foundational principles to advanced surgical approaches. Responses were evaluated for accuracy (5-point scale), readability (Flesch-Kincaid metrics), and response time with Wilcoxon rank sum tests (p < 0.05). (3) RAG achieved comparable accuracy (4.6 vs. 4.2, p = 0.49), readability (Flesch Reading Ease: 42.8 vs. 46.5, p = 0.26; Grade Level: 9.9 vs. 9.5, p = 0.29), and response time (2.8 vs. 2.5 s, p = 0.39) for the highly and less-cited datasets. (4) Less-cited research performed similarly to highly cited sources. This equivalence broadens clinicians' access to novel, diverse insights without sacrificing quality. As plastic surgery evolves, RAG's inclusive approach fosters innovation, improves patient care, and reduces cognitive burden by integrating underutilized studies. Embracing RAG could propel the field toward dynamic, forward-thinking care.",
      "journal": "European burn journal",
      "year": "2025",
      "doi": "10.3390/ebj6020028",
      "authors": "Genovese Ariana et al.",
      "keywords": "AI (artificial intelligence); RAG (retrieval-augmented generation); burn; clinical decision support; large language model; plastic surgery",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40558623/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12191722",
      "ft_text_length": 37841,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12191722)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40587474",
      "title": "Racial disparities in continuous glucose monitoring-based 60-min glucose predictions among people with type 1 diabetes.",
      "abstract": "Non-Hispanic white (White) populations are overrepresented in medical studies. Potential healthcare disparities can happen when machine learning models, used in diabetes technologies, are trained on data from primarily White patients. We aimed to evaluate algorithmic fairness in glucose predictions. This study utilized continuous glucose monitoring (CGM) data from 101 White and 104 Black participants with type 1 diabetes collected by the JAEB Center for Health Research, US. Long short-term memory (LSTM) deep learning models were trained on 11 datasets of different proportions of White and Black participants and tailored to each individual using transfer learning to predict glucose 60 minutes ahead based on 60-minute windows. Root mean squared errors (RMSE) were calculated for each participant. Linear mixed-effect models were used to investigate the association between racial composition and RMSE while accounting for age, sex, and training data size. A median of 9 weeks (IQR: 7, 10) of CGM data was available per participant. The divergence in performance (RMSE slope by proportion) was not statistically significant for either group. However, the slope difference (from 0% White and 100% Black to 100% White and 0% Black) between groups was statistically significant (p\u2009=\u20090.02), meaning the RMSE increased 0.04 [0.01, 0.08] mmol/L more for Black participants compared to White participants when the proportion of White participants increased from 0 to 100% in the training data. This difference was attenuated in the transfer learned models (RMSE: 0.02 [-0.01, 0.05] mmol/L, p\u2009=\u20090.20). The racial composition of training data created a small statistically significant difference in the performance of the models, which was not present after using transfer learning. This demonstrates the importance of diversity in datasets and the potential value of transfer learning for developing more fair prediction models.",
      "journal": "PLOS digital health",
      "year": "2025",
      "doi": "10.1371/journal.pdig.0000918",
      "authors": "Thomsen Helene Bei et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40587474/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12208448",
      "ft_text_length": 27079,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12208448)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40598240",
      "title": "Implicit bias in ICU electronic health record data: measurement frequencies and missing data rates of clinical variables.",
      "abstract": "BACKGROUND: Systematic disparities in data collection within electronic health records (EHRs), defined as non-random patterns in the measurement and recording of clinical variables across demographic groups, can be reflective of underlying implicit bias and may affect patient outcome. Identifying and mitigating these biases is critical for ensuring equitable healthcare. This study aims to develop an analytical framework for measurement patterns, defined as the combination of measurement frequency (how often variables are collected) and missing data rates (the frequency of missing recordings), evaluate the association between them and demographic factors, and assess their impact on in-hospital mortality prediction. METHODS: We conducted a retrospective cohort study using the Medical Information Mart for Intensive Care III (MIMIC-III) database, which includes data on over 40,000 ICU patients from Beth Israel Deaconess Medical Center (2001-2012). Adult patients with ICU stays longer than 24\u00a0h were included. Measurement patterns, including missing data rates and measurement frequencies, were derived from EHR data and analyzed. Targeted Machine Learning (TML) methods were used to assess potential systematic disparities in measurement patterns across demographic factors (age, gender, race/ethnicity) while controlling for confounders such as other demographics and disease severity. The predictive power of measurement patterns on in-hospital mortality was evaluated. RESULTS: Among 23,426 patients, significant demographic systematic disparities were observed in the first 24\u00a0h of ICU stays. Elderly patients (\u2265\u200965 years) had more frequent temperature measurements compared to younger patients, while males had slightly fewer missing temperature measurements than females. Racial disparities were notable: White patients had more frequent blood pressure and oxygen saturation (SpO2) measurements compared to Black and Hispanic patients. Measurement patterns were associated with ICU mortality, with models based solely on these patterns achieving an area under the receiver operating characteristic curve (AUC) of 0.76 (95% CI: 0.74-0.77). CONCLUSIONS: This study underscores the significance of measurement patterns in ICU EHR data, which are associated with patient demographics and ICU mortality. Analyzing patterns of missing data and measurement frequencies provides valuable insights into patient monitoring practices and potential systemic disparities in healthcare delivery. Understanding these disparities is critical for improving the fairness of healthcare delivery and developing more accurate predictive models in critical care settings. CLINICAL TRIAL NUMBER: Not applicable.",
      "journal": "BMC medical informatics and decision making",
      "year": "2025",
      "doi": "10.1186/s12911-025-03058-9",
      "authors": "Shi Junming et al.",
      "keywords": "Bias detection; Critical care; Data completeness; Electronic health records (EHRs); Health equity; Healthcare applications; Implicit bias; MIMIC-III dataset; Measurement frequency; Systematic disparities",
      "mesh_terms": "Humans; Electronic Health Records; Female; Male; Intensive Care Units; Middle Aged; Retrospective Studies; Aged; Hospital Mortality; Adult; Bias; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40598240/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12220764",
      "ft_text_length": 31102,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12220764)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40605778",
      "title": "Striking a Balance: Innovation, Equity, and Consistency in AI Health Technologies.",
      "abstract": "With the explosion of innovation driven by generative and traditional artificial intelligence (AI), comes the necessity to understand and regulate products that often defy current regulatory classification. Tradition, and lack of regulatory expediency, imposes the notion of force-fitting novel innovations into pre-existing product classifications or into the essentially unregulated domains of wellness or consumer electronics. Further, regulatory requirements, levels of risk tolerance, and capabilities vary greatly across the spectrum of technology innovators. For example, currently unregulated information and consumer electronic suppliers set their own editorial and communication standards without extensive federal regulation. However, industries like biopharma companies are held to a higher standard in the same space, given current direct-to-consumer regulations like the Sunshine Act (also known as Open Payments), the federal Anti-Kickback Statute, the federal False Claims Act, and others. Clear and well-defined regulations not only reduce ambiguity but facilitate scale, showcasing the importance of regulatory clarity in fostering innovation and growth. To avoid highly regulated industries like health care and biopharma from being discouraged from developing AI to improve patient care, there is a need for a specialized framework to establish regulatory evidence for AI-based medical solutions. In this paper, we review the current regulatory environment considering current innovations but also pre-existing legal and regulatory responsibilities of the biopharma industry and propose a novel, hybridized approach for the assessment of novel AI-based patient solutions. Further, we will elaborate the proposed concepts via case studies. This paper explores the challenges posed by the current regulatory environment, emphasizing the need for a specialized framework for AI medical devices. By reviewing existing regulations and proposing a hybridized approach, we aim to ensure that the potential of AI in biopharmaceutical innovation is not hindered by uneven regulatory landscapes.",
      "journal": "JMIR AI",
      "year": "2025",
      "doi": "10.2196/57421",
      "authors": "Perakslis Eric et al.",
      "keywords": "algorithm; artificial intelligence; deep learning; digital health; health technology; large language model; machine learning; natural language processing; practical model; predictive analytics; predictive model; predictive system; regulatory; regulatory landscape",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40605778/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12223681",
      "ft_text_length": 27182,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12223681)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40620096",
      "title": "A practical guide for nephrologist peer reviewers: evaluating artificial intelligence and machine learning research in nephrology.",
      "abstract": "Artificial intelligence (AI) and machine learning (ML) are transforming nephrology by enhancing diagnosis, risk prediction, and treatment optimization for conditions such as acute kidney injury (AKI) and chronic kidney disease (CKD). AI-driven models utilize diverse datasets-including electronic health records, imaging, and biomarkers-to improve clinical decision-making. Applications such as convolutional neural networks for kidney biopsy interpretation, and predictive modeling for renal replacement therapies underscore AI's potential. Nonetheless, challenges including data quality, limited external validation, algorithmic bias, and poor interpretability constrain the clinical reliability of AI/ML models. To address these issues, this article offers a structured framework for nephrologist peer reviewers, integrating the TRIPOD-AI (Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis-AI Extension) checklist. Key evaluation criteria include dataset integrity, feature selection, model validation, reporting transparency, ethics, and real-world applicability. This framework promotes rigorous peer review and enhances the reproducibility, clinical relevance, and fairness of AI research in nephrology. Moreover, AI/ML studies must confront biases-data, selection, and algorithmic-that adversely affect model performance. Mitigation strategies such as data diversification, multi-center validation, and fairness-aware algorithms are essential. Overfitting in AI is driven by small patient cohorts faced with thousands of candidate features; our framework spotlights this imbalance and offers concrete remedies. Future directions in AI-driven nephrology include multimodal data fusion for improved predictive modeling, deep learning for automated imaging analysis, wearable-based monitoring, and clinical decision support systems (CDSS) that integrate comprehensive patient data. A visual summary of key manuscript sections is included.",
      "journal": "Renal failure",
      "year": "2025",
      "doi": "10.1080/0886022X.2025.2513002",
      "authors": "Wang Yanni et al.",
      "keywords": "Artificial intelligence; kidney diseases; machine learning; nephrology; peer review; personalized treatment",
      "mesh_terms": "Humans; Acute Kidney Injury; Artificial Intelligence; Machine Learning; Nephrologists; Nephrology; Peer Review, Research; Renal Insufficiency, Chronic; Reproducibility of Results",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40620096/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12239107",
      "ft_text_length": 56278,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12239107)",
      "ft_reason": "Excluded: insufficient approach content (2 indicators)"
    },
    {
      "pmid": "40631724",
      "title": "Disaggregating Health Differences and Disparities With Machine Learning and Observed-to-expected Ratios: Application to Major Lower Limb Amputation.",
      "abstract": "BACKGROUND: Major lower limb amputation is a devastating but preventable complication of peripheral artery disease. It is unclear whether racial and ethnic and rural differences in amputation rates are due to clinical, hospital, or structural factors. METHODS: We included all peripheral artery disease hospitalizations of patients \u226540 years old between 2017 and 2019 in Florida, Georgia, Maryland, Mississippi, or New York (HCUP State Inpatient Databases). We estimated the expected number of amputations using three models: (1) unadjusted, (2) adjusted for clinical factors, and (3) adjusted for clinical factors, hospital factors, and social determinants of health using least absolute shrinkage and selection operator (LASSO). We calculated and compared observed-to-expected ratios and quantified the role of these factors in amputation rates. RESULTS: Overall, 1,577,061 hospitalizations (990,152 unique patients) and 21,233 major lower limb amputations (1.4%) were included. After accounting for clinical differences, we observed amputation disparities among rural Black, Hispanic, Native American, and White patients and nonrural Black and Native American patients. After accounting for hospital factors and social determinants of health, disparities were no longer present among rural White adults (0.93, 95% confidence interval [CI]: 0.77, 1.09); however, disparities persisted among rural Black (1.26, 95% CI: 1.01, 1.51), Hispanic (1.50, 95% CI: 0.89, 2.12), and Native American patients (1.13, 95% CI: 0.68, 1.58) and nonrural Black (1.12, 95% CI: 1.09, 1.15) and Native American (1.15, 95% CI: 0.86, 1.44) patients. CONCLUSION: Clinical factors did not fully explain differences in amputation rates, and hospital factors and social determinants of health did not fully explain disparities. These findings provide additional evidence that implicit bias is associated with amputation disparities.",
      "journal": "Epidemiology (Cambridge, Mass.)",
      "year": "2025",
      "doi": "10.1097/EDE.0000000000001892",
      "authors": "Strassle Paula D et al.",
      "keywords": "Healthcare disparities; Machine learning; Major lower limb amputation; Peripheral artery disease; Rural health; Social determinants of health; Surgical",
      "mesh_terms": "Humans; Amputation, Surgical; Male; Female; Middle Aged; Aged; Peripheral Arterial Disease; Lower Extremity; Machine Learning; Healthcare Disparities; Health Status Disparities; Adult; United States; Social Determinants of Health; Hospitalization; Rural Population",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40631724/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12400468",
      "ft_text_length": 1810,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12400468)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40640192",
      "title": "Advancing equitable access to innovation in breast cancer.",
      "abstract": "This manuscript critically examines the challenges associated with the design and conduct of academic global breast cancer trials outside the influence of pharmaceutical companies, leveraging insights from the Breast International Group (BIG). In the past 4 decades significant declines in breast cancer mortality have occurred, partly related to industry-academic clinical and translational partnerships with long term study follow up. However, in the past decade these partnerships have largely uncoupled. The increasing complexity and non-alignment of trials, funding constraints, regulatory complexity, declining academic freedom, lack of transparency, and lack of affordability of new agents have become key barriers to equitably improving cancer outcomes. Industry research expenditure in the United States is now 5 fold greater than publically funded academic research. To address these challenges, we advocate for patient centred systemic reforms, with trials balancing commercial interests with public health imperatives. These reforms should include equitable research funding models, streamlined international clinical trial regulatory processes, and increased collaboration across diverse stakeholders. Practical solutions to enhance global trial accessibility and efficacy include leveraging digital technologies, artificial intelligence, real world data, decentralizing clinical trial infrastructure, and embedding translational research frameworks across countries.",
      "journal": "NPJ breast cancer",
      "year": "2025",
      "doi": "10.1038/s41523-025-00768-1",
      "authors": "O'Reilly Seamus et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40640192/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12246153",
      "ft_text_length": 30466,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12246153)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40703300",
      "title": "Potential source of bias in AI models: lactate measurement in the ICU in sepsis patients as a template.",
      "abstract": "OBJECTIVE: Health inequities may be driven by demographics such as sex, language proficiency, and race-ethnicity. These disparities may manifest through likelihood of testing, which in turn can bias artificial intelligence models. We aimed to evaluate variation in serum lactate measurements in the intensive care unit (ICU) in sepsis. METHODS: Utilizing MIMIC-IV (2008-2019), we identified adults fulfilling sepsis-3 criteria. Exclusion criteria were ICU stay < 1-day, unknown race-ethnicity, < 18 years of age, and recurrent ICU-stays. Employing targeted maximum likelihood estimation analysis, we assessed the likelihood of a lactate measurement on day 1. For patients with a measurement on day 1, we evaluated the predictors of subsequent readings. RESULTS: We studied 15,601 patients (19.5% racial-ethnic minority, 42.4% female, and 10.0% limited English proficiency). After adjusting for confounders, Black patients had a slightly higher likelihood of receiving a lactate measurement on day 1 [odds ratio 1.19, 95% confidence interval (CI) 1.06-1.34], but not the other minority groups. Subsequent frequency was similar across race-ethnicities, but women had a lower incidence rate ratio (IRR) 0.94 (95% CI 0.90-0.98). Patients with elective admission and private insurance also had a higher frequency of repeated serum lactate measurements (IRR 1.70, 95% CI 1.61-1.81 and 1.07, 95% CI, 1.02-1.12, respectively). CONCLUSION: We found no disparities in the likelihood of a lactate measurement among patients with sepsis across demographics, except for a small increase for Black patients, and a reduced frequency for women. Subsequent analyses should account for the variation in biomarker monitoring being present in MIMIC-IV.",
      "journal": "Frontiers in medicine",
      "year": "2025",
      "doi": "10.3389/fmed.2025.1606254",
      "authors": "Pradhan Pratiksha et al.",
      "keywords": "MIMIC-IV; critical care; health equity; lactate; sepsis",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40703300/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12283994",
      "ft_text_length": 24270,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12283994)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40767217",
      "title": "Co-production of Diagnostic Excellence - Patients, Clinicians, and Artificial Intelligence Comment on \"Achieving Diagnostic Excellence: Roadmaps to Develop and Use Patient-Reported Measures With an Equity Lens\".",
      "abstract": "Patients often experience long journeys within the healthcare system before obtaining a diagnosis. Though progress has been made in measuring the quality of diagnosis, existing measures largely fail to capture the diagnostic process from the patient's perspective. McDonald and colleagues' paper presents 7 overarching goals for the use of patient-reported measures (PRMs) in diagnostic excellence and presents visual roadmaps to guide the development, implementation, and evaluation of these measures. To accelerate the real-world use of PRMs, organizations should initially prioritize the use of patient-reported metrics that are already in development, such as patient-reported experience measures. Pairing PRMs with artificial intelligence (AI) techniques, such as \"diagnostic wayfinding\" (a dynamic diagnostic refinement process that also includes analysis of electronic health record data and metadata to characterize the diagnostic journey), should also improve diagnostic performance. Ultimately, combining PRMs with technological advancements holds the potential to achieve true co-production of diagnostic excellence.",
      "journal": "International journal of health policy and management",
      "year": "2025",
      "doi": "10.34172/ijhpm.8973",
      "authors": "Ranji Sumant R et al.",
      "keywords": "Diagnostic Excellence; Patient-Reported Measures; Roadmaps",
      "mesh_terms": "Artificial Intelligence; Humans; Patient Reported Outcome Measures; Electronic Health Records; Diagnosis; Quality of Health Care",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40767217/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12337162",
      "ft_text_length": 9160,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12337162)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40768261",
      "title": "Stakeholder Perspectives on Trustworthy AI for Parkinson Disease Management Using a Cocreation Approach: Qualitative Exploratory Study.",
      "abstract": "BACKGROUND: Parkinson disease (PD) is the fastest-growing neurodegenerative disorder in the world, with prevalence expected to exceed 12 million by 2040, which poses significant health care and societal challenges. Artificial intelligence (AI) systems and wearable sensors hold potential for PD diagnosis, personalized symptom monitoring, and progression prediction. Nonetheless, ethical AI adoption requires several core principles, including user trust, transparency, fairness, and human oversight. OBJECTIVE: This study aims to explore and synthesize the perspectives of diverse stakeholders, such as individuals living with PD, health care professionals, AI experts, and bioethicists. The aim was to guide the development of AI-driven digital health solutions, emphasizing transparency, data security, fairness, and bias mitigation while ensuring robust human oversight. These efforts are part of the broader Artificial Intelligence-Based Parkinson's Disease Risk Assessment and Prognosis (AI-PROGNOSIS) European project, dedicated to advancing ethical and effective AI applications in PD diagnosis and management. METHODS: An exploratory qualitative approach, based on 2 datasets constructed from cocreation workshops, engaged key stakeholders with diverse expertise to gather insights, ensuring a broad range of perspectives and enriching the thematic analysis. A total of 24 participants participated in the cocreation workshops, including 11 (46%) people with PD, 6 (25%) health care professionals, 3 (13%) AI technical experts, 1 (4%) bioethics expert, and 3 (13%) facilitators. Using a semistructured guide, key aspects of the discussion centered on trust, fairness, explainability, autonomy, and the psychological impact of AI in PD care. RESULTS: Thematic analysis of the cocreation workshop transcripts identified 5 key main themes, each explored through various corresponding subthemes. AI trust and security (theme 1) was highlighted, focusing on data safety and the accuracy and reliability of the AI systems. AI transparency and education (theme 2) emphasized the need for educational initiatives and the importance of transparency and explainability of AI technologies. AI bias (theme 3) was identified as a critical theme, addressing issues of bias and fairness and ensuring equitable access to AI-driven health care solutions. Human oversight (theme 4) stressed the significance of AI-human collaboration and the essential role of human review in AI processes. Finally, AI's psychological impact (theme 5) examined the emotional impact of AI on patients and how AI is perceived in the context of PD care. CONCLUSIONS: Our findings underline the importance of implementing robust security measures, developing transparent and explainable AI models, reinforcing bias mitigation and reduction strategies and equitable access to treatment, integrating human oversight, and considering the psychological impact of AI-assisted health care. These insights provide actionable guidance for developing trustworthy and effective AI-driven digital PD diagnosis and management solutions.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/73710",
      "authors": "Alves Beatriz et al.",
      "keywords": "Parkinson disease management; advanced care strategies; artificial intelligence; assessment; cocreation; digital health care solutions; disease risk; prognosis; stakeholder insights; trust in AI systems",
      "mesh_terms": "Parkinson Disease; Humans; Artificial Intelligence; Qualitative Research; Trust; Stakeholder Participation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40768261/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12368464",
      "ft_text_length": 60430,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12368464)",
      "ft_reason": "Excluded: insufficient approach content (2 indicators)"
    },
    {
      "pmid": "40773740",
      "title": "Developing an Equitable Machine Learning-Based Music Intervention for Older Adults At Risk for Alzheimer Disease: Protocol for Algorithm Development and Validation.",
      "abstract": "BACKGROUND: Given the high prevalence and cost of Alzheimer disease (AD), it is crucial to develop equitable interventions to address lifestyle factors associated with AD incidence (eg, depression). While lifestyle interventions show promise for reducing cognitive decline, culturally sensitive interventions are needed to ensure acceptability and engagement. Given the increased risk for AD and health care barriers among rural-residing older adults, tailoring interventions to align with rural culture and distinct needs is important to improve accessibility and adherence. OBJECTIVE: This protocol aims to develop an intelligent recommendation system capable of identifying the optimal therapeutic music components to elicit engagement and resonate with diverse rural-residing older adults at risk for AD. Aim 1 is to develop culturally inclusive user personas for rural-residing older adults to understand their goals and challenges for music-based digital health intervention. Aim 2 is to develop knowledge embedding-based machine learning (ML) models that use music metadata and survey response data to identify optimal therapeutic music components for enhancing engagement and emotional resonance for depression among rural-residing older adults at risk for AD. Aim 3 is to assess acceptability for personalized therapeutic music sessions and ML-based music recommendations with a separate sample. METHODS: Participants (N=1200) will be aged 55 years or older and residing in the United States. In phase 1, participants (n=1000) will receive 5 randomized songs and complete a survey to understand the sentiment, cultural relevance, and perceived benefit for each song. Brief, researcher-created Likert surveys will be used. In phase 2, survey data will be used to develop ML algorithms in collaboration with the University of Massachusetts Amherst Center for Data Science and Artificial Intelligence. These ML models will be integrated into the digital music intervention and tested with a separate sample of 200 participants. Similar to phase 1, participants will be provided with sets of songs generated by the recommendation system based on the target goal (ie, to reduce depression). The recommendation accuracy of the ML algorithm will be assessed using multiple performance metrics, including root-mean-square error and normalized discounted cumulative gain as well as the mean acceptability score with a goal of 85% user acceptability. RESULTS: Participant recruitment is complete for phases 1 and 2 as of June 2025. Data analysis for the results of aims 1, 2, and 3 are underway and results are expected to be published in the fall of 2025. CONCLUSIONS: This protocol seeks to use ML to improve the equitability and accessibility of a digital lifestyle intervention for AD. INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): DERR1-10.2196/73711.",
      "journal": "JMIR research protocols",
      "year": "2025",
      "doi": "10.2196/73711",
      "authors": "Brown Chelsea S et al.",
      "keywords": "Alzheimer disease; digital health; lifestyle; machine learning; music",
      "mesh_terms": "Humans; Alzheimer Disease; Machine Learning; Aged; Music Therapy; Algorithms; Male; Female; Rural Population; Music; Aged, 80 and over",
      "pub_types": "Journal Article; Clinical Trial Protocol",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40773740/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12371280",
      "ft_text_length": 26186,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12371280)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40795063",
      "title": "Enhancing end-stage renal disease outcome prediction: a multisourced data-driven approach.",
      "abstract": "OBJECTIVES: To improve prediction of chronic kidney disease (CKD) progression to end-stage renal disease (ESRD) using machine learning (ML) and deep learning (DL) models applied to integrated clinical and claims data with varying observation windows, supported by explainable artificial intelligence (AI) to enhance interpretability and reduce bias. MATERIALS AND METHODS: We utilized data from 10\u00a0326 CKD patients, combining clinical and claims information from 2009 to 2018. After preprocessing, cohort identification, and feature engineering, we evaluated multiple statistical, ML and DL models using 5 distinct observation windows. Feature importance and SHapley Additive exPlanations (SHAP) analysis were employed to understand key predictors. Models were tested for robustness, clinical relevance, misclassification patterns, and bias. RESULTS: Integrated data models outperformed single data source models, with long short-term memory achieving the highest area under the receiver operating characteristic curve (AUROC) (0.93) and F1 score (0.65). A 24-month observation window optimally balanced early detection and prediction accuracy. The 2021 estimated glomerular filtration rate (eGFR)\u00a0equation improved prediction accuracy and reduced racial bias, particularly for African American patients. DISCUSSION: Improved prediction accuracy, interpretability, and bias mitigation strategies have the potential to enhance CKD management, support targeted interventions, and reduce health-care disparities. CONCLUSION: This study presents a robust framework for predicting ESRD outcomes, improving clinical decision-making through integrated multisourced data and advanced analytics. Future research will expand data integration and extend this framework to other chronic diseases.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2026",
      "doi": "10.1093/jamia/ocaf118",
      "authors": "Li Yubo et al.",
      "keywords": "chronic kidney disease; clinical and claims data integration; end-stage renal disease; machine learning; predictive modeling",
      "mesh_terms": "Humans; Kidney Failure, Chronic; Machine Learning; Disease Progression; Male; Female; Deep Learning; Middle Aged; Renal Insufficiency, Chronic; Aged; ROC Curve",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40795063/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12758457",
      "ft_text_length": 45089,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12758457)",
      "ft_reason": "Excluded: insufficient approach content (1 indicators)"
    },
    {
      "pmid": "40802585",
      "title": "Racial bias in clinician assessment of patient credibility: Evidence from electronic health records.",
      "abstract": "OBJECTIVE: Black patients disproportionately report feeling disbelieved or having concerns dismissed in medical encounters, suggesting potential racial bias in clinicians' assessment of patient credibility. Because this bias may be evident in the language used by clinicians when writing notes about patients, we sought to assess racial differences in use of language either undermining or supporting patient credibility within the electronic health record (EHR). METHODS: We analyzed 13,065,081 notes written between 2016-2023 about 1,537,587 patients by 12,027 clinicians at a large health system with 5 hospitals and an extensive network of ambulatory practices in the mid-Atlantic region of the United States. We developed and applied natural language processing models to identify whether or not a note contained terms undermining or supporting patient credibility, and used logistic regression with generalized estimating equations to estimate the association of credibility language with patient race/ethnicity. RESULTS: The mean patient age was 43.3 years and 55.9% were female; 57.6% were non-Hispanic White, 28.0% non-Hispanic Black, 8.3% Hispanic/Latino, and 6.1% Asian. Clinician-authors were attending physicians (44.9%), physicians-in-training (40.1%) and advanced practice providers (15.0%). Terms specifically related to patient credibility were relatively uncommon, with 106,523 (0.82%) notes containing terms undermining patient credibility, and 33,706 (0.26%) supporting credibility. In adjusted analyses, notes written about non-Hispanic Black vs. White patients had higher odds of containing terms undermining credibility (aOR 1.29, 95% CI 1.27-1.32), and lower odds of supporting credibility (aOR 0.82; 95% CI 0.79-0.85). Notes written about Hispanic/Latino vs. White patients had similar odds of language undermining (aOR 0.99, 95% CI 0.95-1.03) and supporting credibility (aOR 0.95, 95% CI 0.89-1.02). Notes written about Asian vs. White patients had lower odds of language undermining credibility (aOR 0.85, 95% CI 0.81-0.89), and higher odds of supporting credibility (aOR 1.30, 95% CI 1.23-1.38). CONCLUSIONS: Clinician documentation undermining patient credibility may disproportionately stigmatize Black individuals and favor Asian individuals. As stigmatizing language in medical records has been shown to negatively influence clinician attitudes and decision making, these racial differences in documentation may influence patient care and outcomes and exacerbate health inequities.",
      "journal": "PloS one",
      "year": "2025",
      "doi": "10.1371/journal.pone.0328134",
      "authors": "Beach Mary Catherine et al.",
      "keywords": "",
      "mesh_terms": "Humans; Electronic Health Records; Female; Male; Racism; Adult; Middle Aged; Black or African American; White People; Physician-Patient Relations; United States; Language; Hispanic or Latino; White",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40802585/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12349006",
      "ft_text_length": 27998,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12349006)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40811096",
      "title": "Constructing a binary prediction model with incomplete data: Variable selection to balance fairness and precision.",
      "abstract": "The statistical and pragmatic tension between explanation and prediction is well recognized in psychology. Yarkoni and Westfall (2017) suggested focusing more on predictions, which will ultimately produce better calibrated interpretations. Variable selection methods, such as regularization, are strongly recommended because it will help construct interpretable models while optimizing prediction accuracy. However, when the data contain a nonignorable proportion of missingness, variable selection and model building via penalized regression methods are not straightforward. What further complicates the analysis protocol is when the model performance is evaluated on both prediction accuracy and fairness, the latter is of increasing attention when the predictive outcome has societal implications. This study explored two methods for variable selection with incomplete data: the bootstrap imputation-stability selection (BI-SS) method and the stacked elastic net (SENET) method. Both methods work with multiply imputed data sets but in different ways. BI-SS implements variable selection separately on each imputed bootstrap data set and aggregates the results via stability selection, while SENET stacks all imputed data sets and fits a single pooled model. We thoroughly evaluated their performance using a suite of metrics (including area under the curve, F1 score, and fairness criteria) via three increasingly complex simulation studies. Results reveal that while BI-SS and SENET methods perform almost equally well in settings with generalized linear models, only BI-SS fares well with nested data design because of high computation demand in fitting the regularized generalized linear mixed effects models. Finally, we demonstrated both methods with an example using rich electronic health data. (PsycInfo Database Record (c) 2025 APA, all rights reserved).",
      "journal": "Psychological methods",
      "year": "2025",
      "doi": "10.1037/met0000786",
      "authors": "Ren He et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40811096/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12356495",
      "ft_text_length": 2098,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12356495)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "40831669",
      "title": "Fair Text to Medical Image Diffusion Model with Subgroup Distribution Aligned Tuning.",
      "abstract": "The Text to Medical Image (T2MedI) approach using latent diffusion models holds significant promise for addressing the scarcity of medical imaging data and elucidating the appearance distribution of lesions corresponding to specific patient status descriptions. Like natural image synthesis models, our investigations reveal that the T2MedI model may exhibit biases towards certain subgroups, potentially neglecting minority groups present in the training dataset. In this study, we initially developed a T2MedI model adapted from the pre-trained Imagen framework. This model employs a fixed Contrastive Language-Image Pre-training (CLIP) text encoder, with its decoder fine-tuned using medical images from the Radiology Objects in Context (ROCO) dataset. We conduct both qualitative and quantitative analyses to examine its gender bias. To address this issue, we propose a subgroup distribution alignment method during fine-tuning on a target application dataset. Specifically, this process involves an alignment loss, guided by an off-the-shelf sensitivity-subgroup classifier, which aims to synchronize the classification probabilities between the generated images and those expected in the target dataset. Additionally, we preserve image quality through a CLIP-consistency regularization term, based on a knowledge distillation framework. For evaluation purposes, we designated the BraTS18 dataset as the target, and developed a gender classifier based on brain magnetic resonance (MR) imaging slices derived from it. Our methodology significantly mitigates gender representation inconsistencies in the generated MR images, aligning them more closely with the gender distribution in the BraTS18 dataset.",
      "journal": "Proceedings of SPIE--the International Society for Optical Engineering",
      "year": "2025",
      "doi": "10.1117/12.3046450",
      "authors": "Han Xu et al.",
      "keywords": "AI Fairness; Diffusion Model; Text to Medical Image",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40831669/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12360154",
      "ft_text_length": 1707,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12360154)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40847466",
      "title": "Artificial Intelligence and Health Equity for People with Disabilities: An Integrated Framework for Disability-Inclusive AI Design.",
      "abstract": "People with disabilities (PWD) face persistent health and rehabilitation disparities, including poorer health outcomes often driven by non-inclusive healthcare and technologies that overlook their unique needs and values. Artificial intelligence (AI) holds opportunities to transform health and rehabilitation services; however, without inclusive, participatory, and disability-centered design efforts, AI tools risk perpetuating existing health and rehabilitation disparities and inequalities. This paper introduces an integrated framework for disability-inclusive AI design grounded in Self-Determination Theory (SDT) and Self-Efficacy Theory (SET). The framework aims to guide the design, development, and implementation of inclusive AI tools for PWD. It also outlines implications for public health, workforce, training, and policy, supporting the integration of disability-centered AI in health and rehabilitation.",
      "journal": "Inquiry : a journal of medical care organization, provision and financing",
      "year": "2025",
      "doi": "10.1177/00469580251365472",
      "authors": "Umucu Emre",
      "keywords": "AI; disability; health disparities; health equity; inclusive technology",
      "mesh_terms": "Humans; Artificial Intelligence; Health Equity; Persons with Disabilities; Self Efficacy",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40847466/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12374087",
      "ft_text_length": 20813,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12374087)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40853469",
      "title": "Bias in deep learning-based image quality assessments of T2-weighted imaging in prostate MRI.",
      "abstract": "OBJECTIVES: To determine whether deep learning (DL)-based image quality (IQ) assessment of T2-weighted images (T2WI) could be biased by the presence of clinically significant prostate cancer (csPCa). METHODS: In this three-center retrospective study, five abdominal radiologists categorized IQ of 2,105 transverse T2WI series into optimal, mild, moderate, and severe degradation. An IQ classification model was developed using 1,719 series (development set). The agreement between the model and radiologists was assessed using the remaining 386 series with a quadratic weighted kappa. The model was applied to 11,723 examinations that were not included in the development set and without documented prostate cancer at the time of MRI (patient age, 65.5\u2009\u00b1\u20098.3 years [mean \u00b1 standard deviation]). Examinations categorized as mild to severe degradation were used as target groups, whereas those as optimal were used to construct matched control groups. Case-control matching was performed to mitigate the effects of pre-MRI confounding factors, such as age and prostate-specific antigen value. The proportion of patients with csPCa was compared between the target and matched control groups using the chi-squared test. RESULTS: The agreement between the model and radiologists was moderate with a quadratic weighted kappa of 0.53. The mild-moderate IQ-degraded groups had significantly higher csPCa proportions than the matched control groups with optimal IQ: moderate (N\u2009=\u2009126) vs. optimal (N\u2009=\u2009504), 26.3% vs. 22.7%, respectively, difference\u2009=\u20093.6% [95% confidence interval: 0.4%, 6.8%], p\u2009=\u20090.03; mild (N\u2009=\u20091,399) vs. optimal (N\u2009=\u20091,399), 22.9% vs. 20.2%, respectively, difference\u2009=\u20092.7% [0.7%, 4.7%], p\u2009=\u20090.008. CONCLUSION: The DL-based IQ tended to be worse in patients with csPCa, raising concerns about its clinical application.",
      "journal": "Abdominal radiology (New York)",
      "year": "2025",
      "doi": "10.1007/s00261-025-05163-9",
      "authors": "Nakai Hirotsugu et al.",
      "keywords": "Cancer screening; Deep learning; Image quality enhancements; Magnetic resonance imaging; Prostate cancer",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40853469/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12713002",
      "ft_text_length": 1840,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12713002)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40857554",
      "title": "Comparing Multiple Imputation Methods to Address Missing Patient Demographics in Immunization Information Systems: Retrospective Cohort Study.",
      "abstract": "BACKGROUND: Immunization Information Systems (IIS) and surveillance data are essential for public health interventions and programming; however, missing data are often a challenge, potentially introducing bias and impacting the accuracy of vaccine coverage assessments, particularly in addressing disparities. OBJECTIVE: This study aimed to evaluate the performance of 3 multiple imputation methods, Stata's (StataCorp LLC) multiple imputation using chained equations (MICE), scikit-learn's Iterative-Imputer, and Python's miceforest package, in managing missing race and ethnicity data in large-scale surveillance datasets. We compared these methodologies in their ability to preserve demographic distribution, computational efficiency, and performed G-tests on contingency tables to obtain likelihood ratio statistics to assess the association between race and ethnicity and flu vaccination status. METHODS: In this retrospective cohort study, we analyzed 2021-2022 flu vaccination and demographic data from the West Virginia Immunization Information System (N=2,302,036), where race (15%) and ethnicity (34%) were missing. MICE, Iterative Imputer, and miceforest were used to impute missing variables, generating 15 datasets each. Computational efficiency, demographic distribution preservation, and spatial clustering patterns were assessed using G-statistics. RESULTS: After imputation, an additional 780,339 observations were obtained compared with complete case analysis. All imputation methods exhibited significant spatial clustering for race imputation (G-statistics: MICE=26,452.7, Iterative-Imputer=128,280.3, Miceforest=26,891.5; P<.001), while ethnicity imputation showed variable clustering patterns (G-statistics: MICE=1142.2, Iterative-Imputer=1.7, Miceforest=2185.0; P: MICE<.001, Iterative-Imputer=1.7, Miceforest<.001). MICE and miceforest best preserved the proportional distribution of demographics. Computational efficiency varied, with MICE requiring 14 hours, Iterative Imputer 2 minutes, and miceforest 10 minutes for 15 imputations. Postimputation estimates indicated a 0.87%-18% reduction in stratified flu vaccination coverage rates. Overall estimated flu vaccination rates decreased from 26% to 19% after imputations. CONCLUSIONS: Both MICE and Miceforest offer flexible and reliable approaches for imputing missing demographic data while mitigating bias compared with Iterative-Imputer. Our results also highlight that the imputation method can profoundly affect research findings. Though MICE and Miceforest had better effect sizes and reliability, MICE was much more computationally and time-expensive, limiting its use in large, surveillance datasets. Miceforest can use cloud-based computing, which further enhances efficiency by offloading resource-intensive tasks, enabling parallel execution, and minimizing processing delays. The significant decrease in vaccination coverage estimates validates how incomplete or missing data can eclipse real disparities. Our findings support regular application of imputation methods in immunization surveillance to improve health equity evaluations and shape targeted public health interventions and programming.",
      "journal": "JMIR public health and surveillance",
      "year": "2025",
      "doi": "10.2196/73916",
      "authors": "Brown Sara et al.",
      "keywords": "data science; immunization information system; imputation methods; machine learning; missing data; multiple imputation; statistical modeling",
      "mesh_terms": "Retrospective Studies; Humans; Female; Male; Information Systems; Demography; Child, Preschool; Cohort Studies; Adolescent; Child; Immunization; Adult; Infant; Middle Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40857554/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12380239",
      "ft_text_length": 35331,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12380239)",
      "ft_reason": "Excluded: insufficient approach content (1 indicators)"
    },
    {
      "pmid": "40876870",
      "title": "Digital Health: An Opportunity to Advance Health Equity for People With Disabilities.",
      "abstract": "Policy Points Universal Design and Inclusion: Mandate all digital health platforms, devices, and services be built on universal design principles and codeveloped with people with disabilities, ensuring compatibility with assistive technology and emergency response features. Standardized Disability Data Collection: Implement mandatory, standardized disability data collection in electronic health records with robust privacy protections, addressing the Patient Protection and Affordable Care Act Section 4302 gaps while enabling personalized care and research. Accessibility as Civil Rights: Treat accessibility as a civil rights issue with strict enforcement of Section 508, Americans With Disabilities Act, and Section 1557, including the patient interoperability mandate, penalties for noncompliance, and legal recourse for patients. Funding and Incentives: Establish funding incentives prioritizing disability equity, digital literacy programs, value-based payment models, and workforce training for healthcare professionals using disability-inclusive digital health tools.",
      "journal": "The Milbank quarterly",
      "year": "2025",
      "doi": "10.1111/1468-0009.70049",
      "authors": "Jain Pankaj et al.",
      "keywords": "accessibility; artificial intelligence; disability health",
      "mesh_terms": "Humans; Persons with Disabilities; Health Equity; United States; Electronic Health Records; Patient Protection and Affordable Care Act; Health Services Accessibility; Digital Health",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40876870/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12863998",
      "ft_text_length": 1086,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12863998)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "40880105",
      "title": "Algorithms to Improve Fairness in Medicare Risk Adjustment.",
      "abstract": "IMPORTANCE: Payment system design creates incentives that affect health care spending, access, and outcomes. With Medicare Advantage accounting for more than half of Medicare spending, changes to its risk adjustment algorithm have the potential for broad consequences. OBJECTIVE: To assess the potential for algorithmic tools to achieve more equitable plan payment for Medicare risk adjustment while maintaining current levels of performance, flexibility, feasibility, transparency, and interpretability. DESIGN, SETTING, AND PARTICIPANTS: This diagnostic study included a retrospective analysis of traditional Medicare enrollment and claims data generated between January 1, 2017, and December 31, 2020, from a random 20% sample of non-dual-eligible Medicare beneficiaries with documented residence in the US or Puerto Rico. Race and ethnicity were designated using the Research Triangle Institute enhanced indicator. Diagnoses in claims were mapped to hierarchical condition categories. Algorithms used demographic indicators and hierarchical condition categories from 1 calendar year to predict Medicare spending in the subsequent year. Data analysis was conducted between August 16, 2023, and January 27, 2025. MAIN OUTCOMES AND MEASURES: The main outcome was prospective health care spending by Medicare. Overall performance was measured by payment system fit and mean absolute error. Net compensation was used to assess group-level fairness. RESULTS: The main analysis of Medicare risk adjustment algorithms included 4\u202f398\u202f035 Medicare beneficiaries with a mean (SD) age of 75.2 (7.4) years and mean (SD) annual Medicare spending of $8345 ($18\u202f581); 44% were men; fewer than 1% were American Indian or Alaska Native, 2% were Asian or Other Pacific Islander, 6% were Black, 3% were Hispanic, 86% were non-Hispanic White, and 1% were part of an additional group (termed as other in the Centers for Medicare & Medicaid Services data). Out-of-sample payment system fit for the baseline regression was 12.7%. Constrained regression and postprocessing both achieved fair spending targets while maintaining payment system fit (constrained regression, 12.6%; postprocessing, 12.7%). Whereas postprocessing increased mean payments for beneficiaries in minoritized racial and ethnic groups (American Indian or Alaska Native, Asian or Other Pacific Islander, Black, and Hispanic individuals) only, constrained regression increased mean payments for beneficiaries in minoritized racial and ethnic groups and beneficiaries in other groups residing in counties with greater exposure to socioeconomic factors that can adversely affect health outcomes. CONCLUSIONS AND RELEVANCE: Results of this study suggest that constrained regression and postprocessing can incorporate fairness objectives into the Medicare risk adjustment algorithm with minimal reduction in overall fit. These feasible changes to the Medicare risk adjustment algorithm could be considered by policymakers aiming to address health care disparities through payment system reform.",
      "journal": "JAMA health forum",
      "year": "2025",
      "doi": "10.1001/jamahealthforum.2025.2640",
      "authors": "Reitsma Marissa B et al.",
      "keywords": "",
      "mesh_terms": "Humans; United States; Algorithms; Risk Adjustment; Medicare; Male; Retrospective Studies; Female; Aged; Health Expenditures; Aged, 80 and over; Medicare Part C",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40880105/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12397885",
      "ft_text_length": 27555,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12397885)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40895087",
      "title": "Improving the FAIRness and Sustainability of the NHGRI Resources Ecosystem.",
      "abstract": "In 2024, individuals funded by NHGRI to support genomic community resources completed a Self-Assessment Tool (SAT) to evaluate their application of the FAIR (Findable, Accessible, Interoperable, and Reusable) principles and assess their sustainability. By collecting insights from the self-administered questionnaires and conducting personal interviews, a valuable perspective was gained on the FAIRness and sustainability of the NHGRI resources. The results highlighted several challenges and key areas the NHGRI resource community could improve by working together to form recommendations to address these challenges. The next step was the formation of an Organizing Committee to identify which challenges could lead to best practices or guidelines for the community. The workshop's Organizing Committee comprised four members from the NHGRI resource community: Carol Bult, PhD, Chris Mungall, PhD, Heidi Rehm, PhD, and Michael Schatz, PhD. In December 2024, the Organizing Committee engaged with the NHGRI resource community to refine these challenges further, inviting feedback on potential focus areas for a future workshop. This collaborative approach led to two informative webinars in December 2024, highlighting specific challenges in data curation, data processing, metadata tools, and variant identifiers within the NHGRI resources. Throughout the workshop planning process, the four Organizing Committee members worked together to create and develop themes, design breakout sessions, and create a detailed agenda. The workshop's agenda was intentionally structured to ensure participants could generate implementable recommendations for the NHGRI resource community. The two-day workshop was held in Bethesda, MD, on March 3-4, 2025. The challenges received from NHGRI resources were classified into four key categories, forming the basis of the workshop. The four key categories are variant identifiers, data processing, data curation, and metadata tools. They are briefly described below, with greater details on their challenges and recommendations in subsequent sections. Metadata Tools:While metadata is vital for capturing context in genomic datasets, its usage and relevance can vary by domain, making it difficult to standardize usage. While various methods exist for annotating and extracting metadata, incomplete or inconsistent annotations often result in ineffective data sharing and interoperability, further reducing data usability and reproducibility.Data Curation:Curation of annotations for genomics data is critical for FAIR-ness. Scalable curation solutions are challenging because of the multiple components for curation, including harmonizing data sets, data cleaning, and annotation. The workshop focused on identifying which aspects of data curation could be streamlined using computational methods while considering the barriers to increased automation.Variant Identifiers:Variant identifiers are standardized representations of genetic variants, crucial for sharing and interpreting genomic data in research and clinical work. They ensure consistent referencing and enable data aggregation. Standardizing variant identifiers is difficult due to varied formats, complex data, and distinct environments for generating and disseminating data.Data Processing:Data processing is a necessary first step in a FAIR environment. As there are many variant workflows, streamlining this process will ensure greater accuracy, reproducibility, interoperability, and FAIRness, driving advancements in clinical research. The workshop focused on addressing these aspects with a key focus on improvements and best practices around data processing for an NHGRI resource. Several recommendations were made throughout the workshop's interactive sessions with the resources' participants. While many recommendations were specific to data processing, data curation, metadata tools, or variant identifiers, they can be grouped into core recommendations addressing common challenges within the NHGRI resource community. These core recommendations highlight the key themes that emerged across sessions and are listed in the nine recommendations below. Increase transparency to enable effective sharing/reproducibility (documenting, benchmarking, publishing, mapping)Develop entity schema and ontology mapping tools (between models, identifiers, etc.)Annotate tools using resources to increase findability and reuse (Examples: EDAM Ontology of Bioscientific data analysis and data management)Use standard nomenclature and identifiersMake workflows usable by researchers with limited programming expertiseImplement APIs to improve data connectivityPresent data in an interpretable manner, along with machine readabilityDevelop artificial intelligence/machine learning (AI/ML) methods for scaling curation processesAssess the impact of resources using an independent group that can assess return on investment and impact to health and scientific advancement. An additional key collaborative outcome was the development of Appendix A, which outlines ongoing and future efforts, including additional workshops, webinars, and meetings through the listed events provided by the NHGRI resource community. We hope that these activities will enable further advances in the implementation of FAIR standards and continue to foster collaboration and exchange across NHGRI resources and the global community.",
      "journal": "ArXiv",
      "year": "2025",
      "doi": "",
      "authors": "Babb Larry et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40895087/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12393232",
      "ft_text_length": 52375,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12393232)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40898192",
      "title": "Exploring nurse perspectives on AI-based shift scheduling for fairness, transparency, and work-life balance.",
      "abstract": "INTRODUCTION: Work-life balance (WLB) is critical to nurse retention and job satisfaction in healthcare. Traditional shift scheduling, characterised by inflexible hours and limited employee control, often leads to stress and perceptions of unfairness, contributing to high turnover rates. AI-based scheduling systems are promoted as a promising solution by enabling fairer and more transparent shift distribution. This study explored the perspectives of nurse leaders, permanent nurses, and temporary nurses on the perceived fairness, transparency, and impact on WLB of AI-based shift scheduling systems, which they had not yet used. METHODS: A qualitative study design was used, with focus group (FG) interviews conducted between May and June 2024. FG interviews were conducted with 21 participants from acute hospitals, home care services, and nursing homes between May and June 2024. The interviews were analyzed using the knowledge mapping method, which allowed for a visual representation of key discussion points and highlighted consensus among participants. The discussions centered on five main themes: (1) experiences with current scheduling systems, (2) requirements for work scheduling, (3) fair and participatory work scheduling, (4) requirements for AI in work scheduling, and (5) perceived advantages and disadvantages of AI-based work scheduling. RESULTS: Participants reported that current scheduling practices often lacked fairness and transparency, leading to dissatisfaction, particularly among permanent nurses. While temporary staff appreciated the flexibility in their schedules, permanent nurses expressed a desire for more autonomy and fairness in shift allocation. AI-based scheduling has the potential to improve shift equity by objectively managing shifts based on pre-defined criteria, thereby reducing bias and administrative burden. However, participants raised concerns about the depersonalisation of scheduling, emphasising the need for human oversight to consider the emotional and contextual factors that AI systems may overlook. CONCLUSION: AI-based scheduling systems were perceived as having the potential to be beneficial in improving fairness, transparency and WLB for nurses. However, the integration of these systems must be accompanied by careful consideration of the human element and ongoing collaboration with healthcare professionals to ensure that the technology is aligned with organisational needs. By striking a balance between AI-driven efficiency and human judgement, healthcare organisations can improve nurse satisfaction and retention, ultimately benefiting patient care and organisational efficiency.",
      "journal": "BMC nursing",
      "year": "2025",
      "doi": "10.1186/s12912-025-03808-0",
      "authors": "Gerlach Maisa et al.",
      "keywords": "Artificial intelligence; Co-creation; Fairness; Nurse retention; Participation; Shift scheduling; Transparency; Work-life balance",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40898192/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12406402",
      "ft_text_length": 49042,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12406402)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40978155",
      "title": "Can AI Bridge or Widen Maternal Health Inequities?",
      "abstract": "Artificial intelligence (AI) is rapidly transforming maternal healthcare through tools like risk prediction algorithms, telemedicine platforms, and postpartum support chatbots. Although these innovations offer promise, particularly in low- and middle-income countries (LMICs), their impact on health equity remains contested. This commentary explores how AI can either bridge or widen maternal health inequities, depending on how it is designed, governed, and implemented. We introduce a conceptual framework comprising four interdependent domains that shape equity outcomes in maternal health: inclusive data practices, equitable governance, participatory design, and local capacity-building. Drawing from interdisciplinary literature, we situate AI within broader health and social systems and argue for equity-oriented approaches that foreground representation, accountability, and community engagement. By examining both opportunities and risks, this commentary offers practical, context-sensitive recommendations for LMICs to ensure AI serves as a tool for justice in maternal healthcare.",
      "journal": "Public health challenges",
      "year": "2025",
      "doi": "10.1002/puh2.70119",
      "authors": "Laguitan Reuben Victor M et al.",
      "keywords": "algorithmic bias; artificial intelligence (AI) in healthcare; health equity; low\u2010 and middle\u2010income countries (LMICs); maternal health",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40978155/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12445195",
      "ft_text_length": 32114,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12445195)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41001459",
      "title": "Racial and Ethnic Disparities in Brain Age Algorithm Performance: Investigating Bias Across Six Popular Methods.",
      "abstract": "Brain age algorithms, which estimate biological aging from neuroimaging data, are increasingly used as biomarkers for health and disease. However, most algorithms are trained on datasets with limited racial and ethnic diversity, raising concerns about potential algorithmic bias that could exacerbate health disparities. To probe this potential, we evaluated six popular brain age algorithms using data from the Health and Aging Brain Study-Health Disparities (HABS-HD), comprising 1,123 White American, 1,107 Hispanic American, and 678 African American participants, ages \u226550. Comparing correlations between brain age and chronological age across racial/ethnic groups, relations were consistently weaker for African American participants compared to White and Hispanic American participants across most algorithms (ranging from r=0.51-0.85 for African Americans vs. r=0.57-0.89 for other groups). We also examined error for brain age v. chronological age and found significant differences in median errors across racial/ethnic groups, though specific patterns varied by algorithm. Sensitivity models weighting for age, sex, and scan quality noted similar patterns, with all algorithms maintaining significant differences in correlation or median prediction error between groups. Our findings reveal systematic performance differences in brain age algorithms across racial and ethnic groups, with most algorithms consistently showing reduced algorithm accuracy for African American and/or Hispanic-American participants. These biases, which are likely introduced at multiple stages of algorithm development, could impact clinical utility and diagnostic accuracy. Results highlight the urgent need for more inclusive algorithm development and validation to ensure equitable healthcare applications of neuroimaging biomarkers.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.09.18.25336117",
      "authors": "Adkins Dorthea J et al.",
      "keywords": "algorithm; brain age; chronological age; correlation; ethnicity; median error; race",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41001459/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12458499",
      "ft_text_length": 38886,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12458499)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41040699",
      "title": "Privacy-Enhancing Sequential Learning under Heterogeneous Selection Bias in Multi-Site EHR Data.",
      "abstract": "OBJECTIVE: To develop privacy-enhancing statistical methods for estimation of binary disease risk model association parameters across multiple electronic health record (EHR) sites with heterogeneous selection mechanisms, without sharing raw individual-level data. We illustrate their utility through a cross-biobank analysis of smoking and 97 cancer subtypes using data from the NIH All of Us (AOU) and the Michigan Genomics Initiative (MGI). MATERIALS AND METHODS: Large-scale biobanks often follow heterogeneous recruitment strategies and store data in separate cloud-based platforms, making centralized algorithms infeasible. To address this, we propose two decentralized sequential estimators namely, Sequential Pseudo-likelihood (SPL) and Sequential Augmented Inverse Probability Weighting (SAIPW) that leverage external population-level information to adjust for selection bias, with valid variance estimation. SAIPW additionally protects against misspecification of the selection model using flexible machine learning based auxiliary outcome models. We compare SPL and SAIPW with the existing Sequential Unweighted (SUW) estimator and with centralized and meta learning extensions of IPW and AIPW in simulations under both correctly specified and misspecified selection mechanisms. We apply the methods to harmonized data from MGI ( n = 50,935) and AOU ( n = 241,563) to estimate smoking-cancer associations. RESULTS: In simulations, SUW exhibited substantial bias and poor coverage. SPL and SAIPW yielded unbiased estimates with valid coverage probabilities under correct model specification, with SAIPW remaining robust under selection model misspecification. Both approaches showed no notable efficiency loss relative to centralized methods. Meta-learning methods were efficient for large sites but failed in settings with small cohort sizes and rare outcome prevalence. In real-data analysis, strong associations were consistently identified between smoking and cancers of the lung, bladder, and larynx, aligning with established epidemiological evidence. CONCLUSION: Our framework enables valid, privacy-enhancing inference across EHR cohorts with heterogeneous selection, supporting scalable, decentralized research using real-world data.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.09.26.25336642",
      "authors": "Kundu Ritoban et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41040699/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12486029",
      "ft_text_length": 2262,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12486029)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41040729",
      "title": "Reducing Inequalities Using an Unbiased Machine Learning Approach to Identify Births with the Highest Risk of Preventable Neonatal Deaths.",
      "abstract": "BACKGROUND: Despite contemporaneous declines in neonatal mortality, recent studies show the existence of left-behind populations that continue to have higher mortality rates than the national averages. Additionally, many of these deaths are from preventable causes. This reality creates the need for more precise methods to identify high-risk births, allowing policymakers to target them more effectively. This study fills this gap by developing unbiased machine-learning approaches to more accurately identify births with a high risk of neonatal deaths from preventable causes. METHODS: We link administrative databases from the Brazilian health ministry to obtain birth and death records in the country from 2015 to 2017. The final dataset comprises 8,797,968 births, of which 59,615 newborns died before reaching 28 days alive (neonatal deaths). These neonatal deaths are categorized into preventable deaths (42,290) and non-preventable deaths (17,325). Our analysis identifies the death risk of the former group, as they are amenable to policy interventions. We train six machine-learning algorithms, test their performance on unseen data, and evaluate them using a new policy-oriented metric. To avoid biased policy recommendations, we also investigate how our approach impacts disadvantaged populations. RESULTS: XGBoost was the best-performing algorithm for our task, with the 5% of births identified as highest risk by the model accounting for over 85% of the observed deaths. Furthermore, the risk predictions exhibit no statistical differences in the proportion of actual preventable deaths from disadvantaged populations, defined by race, education, marital status, and maternal age. These results are similar for other threshold levels. CONCLUSIONS: We show that, by using publicly available administrative data sets and ML methods, it is possible to identify the births with the highest risk of preventable deaths with a high degree of accuracy. This is useful for policymakers as they can target health interventions to those who need them the most and where they can be effective without producing bias against disadvantaged populations. Overall, our approach can guide policymakers in reducing neonatal mortality rates and their health inequalities. Finally, it can be adapted for use in other developing countries.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2024.01.12.24301163",
      "authors": "Ramos Antonio P et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41040729/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12486022",
      "ft_text_length": 2336,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12486022)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41049571",
      "title": "Hypertension control in resource-constrained settings: Bridging socioeconomic gaps with predictive insights.",
      "abstract": "BACKGROUND: Hypertension continues to be a pivotal driver of global cardiovascular disease burden and adverse health outcomes, particularly in resource-constrained settings where disparities in socioeconomic status and clinical infrastructure hinder effective management. Despite medical advancements, achieving optimal blood pressure (BP) control remains a formidable challenge, necessitating a nuanced understanding of multifactorial risk determinants. METHODS: A cross-sectional analysis was conducted on 1,000 hypertensive patients from a larger dataset comprising 100,000 population size. Three hundred patients were examined for personalised BP control predictors who met the inclusion criteria of being treated for at least one year at the Hypertension and Research Centre in Rangpur, Bangladesh, between January 2020 and January 2021. BP control was assessed using World Health Organisation (WHO) and National Institute for Clinical Excellence (NICE) guidelines, and a comprehensive analysis of the sociodemographic and clinical variables was performed using multivariate logistic regression. Machine learning models such as K-Nearest Neighbours (KNN) were utilised to predict BP control with good performance using cross-validation techniques compared to other models. Explainable AI tools like Shapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) provide interpretations of key variables with predictive qualities. RESULTS: The mean age of participants was 49.37 \u00b1 12.81 years, with 54.7% aged 40-59 years and 57.7% male. The overall BP control rate among the study population was 28%. Among those with controlled hypertension, 42% were rural residents ( p = 0 . 005  ) and 37% were homemakers ( p < 0.001), indicating better control in these subgroups. Key facilitators of BP control included higher education levels (e.g., post-graduate OR = 1.17, p < 0 . 001  ), lower cholesterol levels (SHAP value = 0.097), and adherence to combination therapy (75% of controlled cases). Conversely, diabetes mellitus (SHAP value = 0.069) and ischemic heart disease (OR = 0.95, p = 0 . 004  ) emerged as significant impediments to BP control. Advanced machine learning models, including KNN, achieved an unparallelled predictive accuracy of 99%, underscoring precision-based interventions' transformative potential. SHAP analysis revealed dietary habits (SHAP value = 0.077) and physical activity (SHAP value = 0.079) as modifiable predictors, highlighting the efficacy of personalised lifestyle strategies. Simulation-based interventions grounded in machine learning insights reduced high-risk classifications by 15%, further reinforcing predictive analytics' value in hypertension management. Sensitivity analysis highlighted the dominance of socioeconomic factors, with income level (sensitivity: 0.85) and healthcare accessibility (sensitivity: 0.78) emerging as critical predictors, reinforcing the importance of addressing health inequities in hypertension management. CONCLUSION: The study elucidates critical gaps in hypertension management, emphasising the urgent need to address modifiable risk factors, tailor therapeutic regimens, and integrate socioeconomic considerations into public health frameworks. The findings advocate for scalable, data-driven interventions to bridge the hypertension care gap, thereby mitigating cardiovascular disease risks and enhancing health equity in underserved regions.",
      "journal": "International journal of cardiology. Cardiovascular risk and prevention",
      "year": "2025",
      "doi": "10.1016/j.ijcrp.2025.200472",
      "authors": "Azad Md Abul Kalam et al.",
      "keywords": "Blood pressure optimisation; Cardiovascular health equity; Explainable AI; Hypertension; Lifestyle interventions; Machine learning models; Precision medicine; Predictive analytics; Risk stratification; Socioeconomic disparities",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41049571/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12495085",
      "ft_text_length": 88529,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12495085)",
      "ft_reason": "Excluded: insufficient approach content (1 indicators)"
    },
    {
      "pmid": "41056616",
      "title": "Quantitative assessment of impact of technical and population-based factors on fairness of AI models for chest X-ray scans.",
      "abstract": "Ensuring fairness in diagnostic AI models is essential for their safe deployment in clinical practice. This study investigates fairness by jointly analyzing population-based factors (sex and race) and technical factors (imaging site and X-ray energy) using chest X-ray data. A total of 49 datasets covering over 321,000 patients and 960,000 images were used. Six experiments were conducted to evaluate the effect of these factors on model performance across classification scores, class activation maps (CAMs), and deep features (DFs). Fairness was assessed using effect sizes derived from Kolmogorov-Smirnov statistics. Within single datasets, performance differences between demographic groups were generally small, with effect sizes below 0.1 for classification scores and CAMs, and up to 0.2 for deep features by sex. However, much larger discrepancies were observed when comparing the same patient group across different imaging sites, with effect sizes ranging from 0.1 to 0.6 across all metrics. Our findings suggest that technical variability has a greater impact on model behavior than population-based factors. Notably, deep features revealed more substantial group differences than surface-level outputs like diagnostic probability scores or CAMs. The findings emphasize the need to evaluate fairness not only within datasets but also across institutions, comparing model performance on training versus external populations, thereby helping to identify fairness limitations that might not be visible through single-cohort analyses.",
      "journal": "Computers in biology and medicine",
      "year": "2025",
      "doi": "10.1016/j.compbiomed.2025.111147",
      "authors": "Cherezov Dmitry et al.",
      "keywords": "",
      "mesh_terms": "Humans; Male; Female; Radiography, Thoracic; Middle Aged; Adult; Aged; Artificial Intelligence",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41056616/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12614815",
      "ft_text_length": 1542,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12614815)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "41060023",
      "title": "The Role of Data in Public Health and Health Innovation: Perspectives on Social Determinants of Health, Community-Based Data Approaches, and AI.",
      "abstract": "Public health is undergoing profound transformation driven by data from the global health sector and related fields. To address systemic health disparities, scholars and health practitioners are increasingly applying a data equity lens, an approach that has become even more urgent as the United States faces the erosion of public health data infrastructure. This paper summarizes insights from an April 2024 convening by the Yale School of Public Health-The Role of Data in Public Health Equity and Innovation-with intersectoral stakeholders from academia, government (local, state, and federal), health care, and private industry. The convening included keynote presentations and roundtables regarding the depiction of social determinants of health in data; effects of artificial intelligence (AI) on health data equity; and community-based models for data, providing a framework for cross-cutting discussions. Through a narrative synthesis, themes were identified and synthesized from systematically gathered information from presentations and roundtables. This process led to a set of actionable, cross-cutting recommendations to guide inclusive and impactful data practices for policymakers, public health professionals, and health innovators across diverse contexts: (1) enable big data and interoperability connecting social determinants of health and health outcomes; (2) include diverse, nontechnical voices in AI and health discussions; (3) fund research on data equity and AI in health sciences; (4) modernize the Health Insurance Portability and Accountability Act (HIPAA) with new guidelines for AI and big data; and (5) research and conceptual frameworks are needed to elucidate interconnections between data equity and health equity.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/78794",
      "authors": "Yelpaala Kaakpema et al.",
      "keywords": "AI; HIPAA; Health Insurance Portability and Accountability Act; SDOH; artificial intelligence; community-based data approaches; data equity; health policy; social determinants of health",
      "mesh_terms": "Social Determinants of Health; Artificial Intelligence; Public Health; Humans; United States",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41060023/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12505398",
      "ft_text_length": 32906,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12505398)",
      "ft_reason": "Excluded: insufficient approach content (2 indicators)"
    },
    {
      "pmid": "41062130",
      "title": "Leveraging Machine Learning Models to Explore Disparities in Prostate Cancer Diagnosis, Treatment, and Survival.",
      "abstract": "IntroductionProstate cancer (CaP) is the second most common cancer in men globally and a leading cause of cancer-related mortality, particularly among older men. In the United States, disparities in incidence, stage at diagnosis, and outcomes persist across racial, socioeconomic, and geographic lines. Men in medically underserved regions like Appalachia experience higher mortality due to limited access to timely screening and treatment. Given the complex interplay of clinical, behavioral, and sociodemographic factors, machine learning (ML) offers promise in identifying survival predictors that traditional models may overlook. This study applies ML models to assess the impact of age at diagnosis, treatment modality, and other sociodemographic and clinical factors on CaP survival outcomes using data from the Kentucky Cancer Registry (KCR).MethodsWe retrospectively analyzed 37\u00a0893 CaP cases diagnosed from 2010 to 2022 using KCR data linked to mortality records. Kaplan-Meier (KM), Random Survival Forest (RSF), and Elastic Net regression were used to estimate survival, assess variable importance, and evaluate predictive performance. Missing data were handled via multiple imputation, and leave-one-out cross-validation minimized overfitting. Models were compared using out-of-bag error, continuous ranked probability scores (CRPS), and interpretability.ResultsML models identified age at diagnosis, treatment modality, and smoking status as the top survival predictors. RSF showed that age was approximately 2.5 times more influential than treatment type. Patients diagnosed before age 60 were more likely to undergo surgery and had lower mortality, while older men more often received non-surgical therapies and experienced worse outcomes. Insurance status, tumor grade, lymph node involvement, and marital status also affected survival, with pronounced disparities among uninsured and government-insured patients.ConclusionsML models highlighted age, smoking, and treatment type as key predictors of CaP survival. Findings support early screening, equitable treatment access, and behavioral health integration to reduce disparities in high-risk areas such as Kentucky and Appalachia.",
      "journal": "Cancer control : journal of the Moffitt Cancer Center",
      "year": "2025",
      "doi": "10.1177/10732748251384702",
      "authors": "Adatorwovor Reuben et al.",
      "keywords": "Kentucky cancer registry; early diagnosis; elastic net; machine learning; prostate cancer; random survival forest; smoking; survival disparities; treatment disparities",
      "mesh_terms": "Humans; Male; Prostatic Neoplasms; Machine Learning; Aged; Retrospective Studies; Middle Aged; Healthcare Disparities; Kentucky; Registries",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41062130/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12511736",
      "ft_text_length": 39940,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12511736)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41065617",
      "title": "Artificial Intelligence Methods Applied to Electronic Health Record Data for Health Equity in Clinical Trials.",
      "abstract": "",
      "journal": "JACC. Advances",
      "year": "2025",
      "doi": "10.1016/j.jacadv.2025.102201",
      "authors": "Breeze Jonathan et al.",
      "keywords": "clinical trial representation; coronary artery disease; natural language processing; patient diversity; socioeconomic factors",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41065617/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12717601",
      "ft_text_length": 8072,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12717601)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41088416",
      "title": "PURE: policy-guided unbiased REpresentations for structure-constrained molecular generation.",
      "abstract": "Structure-constrained molecular generation (SCMG) generates novel molecules that are structurally similar to a given molecule and have optimized properties. Deep learning solutions for SCMG are limited in that they are predisposed towards existing knowledge, and they suffer from a natural impedance mismatch problem due to the discrete nature of molecules, while deep learning methods for SCMG often operate in continuous space. Moreover, many task-specific evaluation metrics used during training often bias the model towards a particular metric -\"metric-leakage\". To overcome these shortcomings, we propose Policy-guided Unbiased REpresentations (PURE) for SCMG that learns within a framework simulating molecular transformations for drug synthesis. PURE combines self-supervised learning with a policy-based reinforcement\u00a0learning (RL) framework, thereby avoiding the need for external molecular metrics while learning high-quality representations that incorporate an inherent notion of similarity specific to the given task. Along with a semi-supervised training design, PURE utilizes template-based molecular simulations to better explore and navigate the discrete molecular search space. Despite the lack of metric biases, PURE achieves competitive or superior performance to state-of-the-art methods on multiple benchmarks. Our study emphasizes the importance of reevaluating current approaches for SCMG and developing strategies that naturally align with the problem. Finally, we illustrate how our methodology can be applied to combat drug resistance by identifying sorafenib-like compounds as a case study.",
      "journal": "Journal of cheminformatics",
      "year": "2025",
      "doi": "10.1186/s13321-025-01090-5",
      "authors": "Gupta Abhor et al.",
      "keywords": "Cancer drugs; Deep learning; Drug discovery; Drug resistance; Drug synthesis; Human health; Lead optimization; Machine learning; Product innovation; Reinforcement learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41088416/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12522651",
      "ft_text_length": 83358,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12522651)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41092928",
      "title": "Global burden of 292 causes of death in 204 countries and territories and 660 subnational locations, 1990-2023: a systematic analysis for the Global Burden of Disease Study 2023.",
      "abstract": "BACKGROUND: Timely and comprehensive analyses of causes of death stratified by age, sex, and location are essential for shaping effective health policies aimed at reducing global mortality. The Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) 2023 provides cause-specific mortality estimates measured in counts, rates, and years of life lost (YLLs). GBD 2023 aimed to enhance our understanding of the relationship between age and cause of death by quantifying the probability of dying before age 70 years (70q0) and the mean age at death by cause and sex. This study enables comparisons of the impact of causes of death over time, offering a deeper understanding of how these causes affect global populations. METHODS: GBD 2023 produced estimates for 292 causes of death disaggregated by age-sex-location-year in 204 countries and territories and 660 subnational locations for each year from 1990 until 2023. We used a modelling tool developed for GBD, the Cause of Death Ensemble model (CODEm), to estimate cause-specific death rates for most causes. We computed YLLs as the product of the number of deaths for each cause-age-sex-location-year and the standard life expectancy at each age. Probability of death was calculated as the chance of dying from a given cause in a specific age period, for a specific population. Mean age at death was calculated by first assigning the midpoint age of each age group for every death, followed by computing the mean of all midpoint ages across all deaths attributed to a given cause. We used GBD death estimates to calculate the observed mean age at death and to model the expected mean age across causes, sexes, years, and locations. The expected mean age reflects the expected mean age at death for individuals within a population, based on global mortality rates and the population's age structure. Comparatively, the observed mean age represents the actual mean age at death, influenced by all factors unique to a location-specific population, including its age structure. As part of the modelling process, uncertainty intervals (UIs) were generated using the 2\u00b75th and 97\u00b75th percentiles from a 250-draw distribution for each metric. Findings are reported as counts and age-standardised rates. Methodological improvements for cause-of-death estimates in GBD 2023 include a correction for the misclassification of deaths due to COVID-19, updates to the method used to estimate COVID-19, and updates to the CODEm modelling framework. This analysis used 55\u2008761 data sources, including vital registration and verbal autopsy data as well as data from surveys, censuses, surveillance systems, and cancer registries, among others. For GBD 2023, there were 312 new country-years of vital registration cause-of-death data, 3 country-years of surveillance data, 51 country-years of verbal autopsy data, and 144 country-years of other data types that were added to those used in previous GBD rounds. FINDINGS: The initial years of the COVID-19 pandemic caused shifts in long-standing rankings of the leading causes of global deaths: it ranked as the number one age-standardised cause of death at Level 3 of the GBD cause classification hierarchy in 2021. By 2023, COVID-19 dropped to the 20th place among the leading global causes, returning the rankings of the leading two causes to those typical across the time series (ie, ischaemic heart disease and stroke). While ischaemic heart disease and stroke persist as leading causes of death, there has been progress in reducing their age-standardised mortality rates globally. Four other leading causes have also shown large declines in global age-standardised mortality rates across the study period: diarrhoeal diseases, tuberculosis, stomach cancer, and measles. Other causes of death showed disparate patterns between sexes, notably for deaths from conflict and terrorism in some locations. A large reduction in age-standardised rates of YLLs occurred for neonatal disorders. Despite this, neonatal disorders remained the leading cause of global YLLs over the period studied, except in 2021, when COVID-19 was temporarily the leading cause. Compared to 1990, there has been a considerable reduction in total YLLs in many vaccine-preventable diseases, most notably diphtheria, pertussis, tetanus, and measles. In addition, this study quantified the mean age at death for all-cause mortality and cause-specific mortality and found noticeable variation by sex and location. The global all-cause mean age at death increased from 46\u00b78 years (95% UI 46\u00b76-47\u00b70) in 1990 to 63\u00b74 years (63\u00b71-63\u00b77) in 2023. For males, mean age increased from 45\u00b74 years (45\u00b71-45\u00b77) to 61\u00b72 years (60\u00b77-61\u00b76), and for females it increased from 48\u00b75 years (48\u00b71-48\u00b78) to 65\u00b79 years (65\u00b75-66\u00b73), from 1990 to 2023. The highest all-cause mean age at death in 2023 was found in the high-income super-region, where the mean age for females reached 80\u00b79 years (80\u00b79-81\u00b70) and for males 74\u00b78 years (74\u00b78-74\u00b79). By comparison, the lowest all-cause mean age at death occurred in sub-Saharan Africa, where it was 38\u00b70 years (37\u00b75-38\u00b74) for females and 35\u00b76 years (35\u00b72-35\u00b79) for males in 2023. Lastly, our study found that all-cause 70q0 decreased across each GBD super-region and region from 2000 to 2023, although with large variability between them. For females, we found that 70q0 notably increased from drug use disorders and conflict and terrorism. Leading causes that increased 70q0 for males also included drug use disorders, as well as diabetes. In sub-Saharan Africa, there was an increase in 70q0 for many non-communicable diseases (NCDs). Additionally, the mean age at death from NCDs was lower than the expected mean age at death for this super-region. By comparison, there was an increase in 70q0 for drug use disorders in the high-income super-region, which also had an observed mean age at death lower than the expected value. INTERPRETATION: We examined global mortality patterns over the past three decades, highlighting-with enhanced estimation methods-the impacts of major events such as the COVID-19 pandemic, in addition to broader trends such as increasing NCDs in low-income regions that reflect ongoing shifts in the global epidemiological transition. This study also delves into premature mortality patterns, exploring the interplay between age and causes of death and deepening our understanding of where targeted resources could be applied to further reduce preventable sources of mortality. We provide essential insights into global and regional health disparities, identifying locations in need of targeted interventions to address both communicable and non-communicable diseases. There is an ever-present need for strengthened health-care systems that are resilient to future pandemics and the shifting burden of disease, particularly among ageing populations in regions with high mortality rates. Robust estimates of causes of death are increasingly essential to inform health priorities and guide efforts toward achieving global health equity. The need for global collaboration to reduce preventable mortality is more important than ever, as shifting burdens of disease are affecting all nations, albeit at different paces and scales. FUNDING: Gates Foundation.",
      "journal": "Lancet (London, England)",
      "year": "2025",
      "doi": "10.1016/S0140-6736(25)01917-8",
      "authors": "N/A",
      "keywords": "",
      "mesh_terms": "Humans; Global Burden of Disease; Cause of Death; Male; Female; Aged; Middle Aged; Adult; Infant; Child; Child, Preschool; Adolescent; Life Expectancy; Global Health; Young Adult; Infant, Newborn; Aged, 80 and over; Risk Factors; Mortality",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41092928/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12535838",
      "ft_text_length": 228981,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12535838)",
      "ft_reason": "Excluded: insufficient approach content (1 indicators)"
    },
    {
      "pmid": "41191859",
      "title": "Designing for Patient-Centered Care and Equity in Virtual Hospital-at-Home Models: Quality Improvement Initiative Using Experience-Based Co-Design.",
      "abstract": "BACKGROUND: The rapid expansion of virtual care during COVID-19 accelerated the development of virtual hospital-at-home models, which deliver hospital-level care in patients' homes through remote monitoring, virtual communication, and in-person support when required. While the virtual hospital-at-home model offers the potential to improve patient-centered care and health equity, rapid implementation often overlooks culturally diverse and underserved populations, including South Asian communities who experience disproportionate chronic disease burden and barriers to accessing culturally relevant services. Strategies are needed to ensure equitable design and adoption of virtual hospital-at-home models. OBJECTIVE: This study used an experience-based co-design (EBCD) approach to engage patients, caregivers, clinicians, and community organizations in shaping a regional virtual hospital-at-home strategy within the Fraser Health Authority, British Columbia, Canada. The aim was to identify barriers, facilitators, and equity-focused solutions to inform future implementation. METHODS: We conducted a five-stage EBCD quality improvement process in the Fraser Health Authority, British Columbia, including (1) forming a multidisciplinary steering committee, (2) reviewing health care provider experiences, (3) interviewing South Asian patients and caregivers, (4) hosting a co-design workshop to develop solutions, and (5) sharing back findings. RESULTS: Participants identified barriers, including digital literacy, language, and trust in virtual care. The co-designed solutions focused on culturally tailored education, hybrid digital training, caregiver inclusion, and community-driven engagement strategies. CONCLUSIONS: EBCD enabled the development of inclusive and actionable strategies to improve virtual hospital-at-home services. The findings highlight the importance of ongoing community collaboration to ensure equity in virtual care innovation.",
      "journal": "JMIR human factors",
      "year": "2025",
      "doi": "10.2196/79679",
      "authors": "Kandola Mahabhir et al.",
      "keywords": "AI; South Asian people; artificial intelligence; community-based participatory research; health equity; telemedicine",
      "mesh_terms": "Humans; Patient-Centered Care; Quality Improvement; COVID-19; Telemedicine; Health Equity; British Columbia; SARS-CoV-2",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41191859/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12588588",
      "ft_text_length": 38905,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12588588)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41195697",
      "title": "International Expert Consensus on Defining Skin of Color and Delivering Equitable Dermatologic Care.",
      "abstract": "Dermatological practice faces significant challenges in meeting the needs of populations with skin of color (SOC). Patients with SOC face disparities, including misdiagnoses and inequitable treatment outcomes. This expert consensus sought to identify gaps in dermatologic care for these populations and to propose strategies to promote inclusivity. Classification systems for SOC and healthcare disparities were investigated by reviewing English language articles on the subject published in PubMed between 2019 and 2024. An international panel of multidisciplinary experts from four continents (America, Asia, Africa, and Europe) analyzed the findings and developed recommendations. There are limitations in the current skin type classification systems, and gaps persist for SOC populations in research, clinical trials, and education for both providers and patients. Proposed strategies to bridge these gaps include refining classification systems (Dermatology societies), advancing SOC-specific research, enhancing education, and integrating artificial intelligence. Key recommendations from the panel focused on four areas: (1) Research: Achieve SOC numbers in clinical trials and publications that would be a reflection of local and global populations, publish new guidelines on key SOC-related issues, and achieve representative authorship in the said clinical trials and publications; (2) Resources: Create a global library of SOC images for dermatological diseases; (3) Education: Provide training for healthcare professionals and scholarships for students worldwide to improve awareness and expertise; (4) Representation: Ensure SOC representation in images used for patient communication and educational materials.",
      "journal": "International journal of dermatology",
      "year": "2026",
      "doi": "10.1111/ijd.70100",
      "authors": "Lim Henry W et al.",
      "keywords": "SOC; classification; healthcare disparities; medical education; patient education as topic; skin of color; skin pigmentation; terminology",
      "mesh_terms": "Humans; Dermatology; Ethnic and Racial Minorities; Health Equity; Healthcare Disparities; Skin Diseases; Skin Pigmentation",
      "pub_types": "Consensus Statement; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41195697/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12712775",
      "ft_text_length": 32975,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12712775)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41201814",
      "title": "Proactive Bias Mitigation When Using Online Survey Panels for Self-Reported Use of Illicitly Manufactured Fentanyl in the General Adult Population.",
      "abstract": "IMPORTANCE: Illicitly manufactured fentanyl remains a public health threat and trustworthy measurements in prevalence are crucial to public health approaches. Low prevalence behaviors, such as route of administration of illicitly manufactured fentanyl, may have shifted over time, which changes community risk profiles. OBJECTIVE: To assess the impact of bias mitigation methods in an online survey sample and quantify changes in routes of administration in illicitly manufactured fentanyl use over time. DESIGN, SETTING, AND PARTICIPANTS: This repeated cross-sectional survey included US adults 18 years and older in an online, panel-based general population sample fielded twice yearly, in spring and autumn. Corrections for demographic and nondemographic composition bias using calibration weights and removal of misclassification from careless/inattentive responses were applied. Data were collected from April 2022 to October 2024, and data were analyzed in May 2025. MAIN OUTCOMES AND MEASURES: Self-reported use of illicitly manufactured fentanyl in the past 12 months and routes of administration, which included oral, injection, smoking, or snorting. Weighted frequency and percentages were calculated. RESULTS: In the full 2022-2024 sample of 175\u202f058 respondents where misclassification removal and calibration was applied, 50.6% (95% uncertainty interval [UI], 50.3-60.0) were female, 48.1% (95% UI, 47.8-48.4) were male, and 1.3% (95% UI, 1.2-1.3) were transgender, nonbinary, or something else, and the median (IQR) age was 47 (32-62) years. The bias-mitigated prevalence estimate of illicitly manufactured fentanyl use in the last 12 months increased from 0.7% (95% UI, 0.7-0.8) in 2022 to 1.1% (95% UI, 1.0-1.2) in 2024. Oral use of illicitly manufactured fentanyl increased from 35.9% (95% UI, 31.1-40.7) in 2022 to 44.4% (95% UI, 40.3-48.5) in 2024, which was the most common route of administration. In 2024, use by smoking was 37.9% (95% UI, 34.1-41.6), use by snorting was 27.1% (95% UI, 23.5-30.7), and use by injection was 24.5% (95% UI, 21.3-27.7). Importantly, bias mitigation cumulatively reduced the national estimate of illicitly manufactured fentanyl by 70.9% in 2024 (from 3.9% [95% UI, 3.8-4.1] when neither was applied to 1.1% [95% UI, 1.0-1.2]), an important factor when considering prevalence and change over time. CONCLUSIONS AND RELEVANCE: Results of this survey study suggest that fentanyl use has shifted toward oral use, which may contribute to observed lower mortality rates despite an increase in prevalence of use. Methods intended to reduce systematic bias have a strong influence on low prevalence behavior estimates and should be implemented for all survey-based drug use surveillance.",
      "journal": "JAMA health forum",
      "year": "2025",
      "doi": "10.1001/jamahealthforum.2025.4011",
      "authors": "Black Joshua C et al.",
      "keywords": "",
      "mesh_terms": "Humans; Fentanyl; Male; Female; Adult; Cross-Sectional Studies; Self Report; Middle Aged; Bias; United States; Illicit Drugs; Surveys and Questionnaires; Young Adult; Adolescent; Prevalence; Opioid-Related Disorders; Analgesics, Opioid",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41201814/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12595535",
      "ft_text_length": 13643,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12595535)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "41221373",
      "title": "Using mixture cure models to address algorithmic bias in diagnostic timing: autism as a test case.",
      "abstract": "OBJECTIVES: To address algorithmic bias in clinical prediction models related to the timing of diagnosis, we evaluated the efficacy of mixture cure models that integrate time-to-event and binary classification frameworks to predict diagnoses. MATERIALS AND METHODS: We conducted a simulation and analyzed real-world North Carolina Medicaid data for children born in 2014, followed until 2023. The study evaluated traditional time-to-event and classification models against mixture cure models under scenarios with varied diagnostic timing and censoring. RESULTS: Simulation results demonstrated that traditional models exhibit increased bias as diagnosis timing differences widened, whereas mixture cure models yielded unbiased estimates across varying censoring times. In real-world analyses, significant racial and ethnic variations in autism diagnosis rates were observed, with non-Hispanic White children having higher diagnosis rates compared to other groups. The mixture cure model effectively adjusted for these disparities, providing fairer and more accurate diagnostic predictions across varying levels of censoring. DISCUSSION: Mixture cure models effectively address algorithmic bias by providing unbiased estimates regardless of variations in diagnostic timing and censoring, making them particularly suitable for conditions like autism where not all individuals will receive a diagnosis. This approach shifts focus from when an event will occur to whether it will occur, aligning more closely with clinical needs in early detection of pediatric developmental conditions. CONCLUSION: Mixture cure models offer a promising tool to enhance accuracy and fairness in predictive modeling, especially when the outcome of interest is not uniformly observed across groups.",
      "journal": "JAMIA open",
      "year": "2025",
      "doi": "10.1093/jamiaopen/ooaf148",
      "authors": "Wu Peng et al.",
      "keywords": "algorithmic bias; autism spectrum disorder; clinical prediction models; electronic health records and claims data; mixture cure models",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41221373/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12598640",
      "ft_text_length": 26410,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12598640)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41228248",
      "title": "TILDA-X: Transcriptome-Informed Lung Cancer Disparities via Explainable AI.",
      "abstract": "BACKGROUND: Lung cancer is a leading cause of cancer-related mortality, with disparities in incidence and outcomes observed across different racial and sex groups. Identifying both patient-specific and cohort-specific disparity biomarkers is critical for developing targeted treatments. The lung cancer dataset is highly imbalanced across races, leading to biased results in disparity information if classification is based on race. METHOD: This study developed an explainable artificial intelligence-based framework, TILDA-X, which designs classification models based on disease conditions instead of races to mitigate racial imbalance in the dataset and applies explainable AI to delineate patient-specific disparity information. A lung cancer transcriptome dataset with three disease conditions-lung adenocarcinoma, lung squamous cell carcinoma, and healthy samples-was used to develop classification models. Applying a bottom-up approach from patient-specific disparity information, the cohort-specific disparity information is discovered for different racial and sex groups, African American males, European American males, African American females, and European American females. RESULTS: Classification based on disease conditions achieved accuracy between 88% and 100% for minority groups (African American males and females), whereas it was only between 0% and 16% for race-based classification, which underscores the significance of the proposed approach. Functional analysis of sub-cohort-specific biomarker genes revealed unique pathways associated with lung cancers in different races and sexes. Among the significant pathways identified, over ~63% overlapped with previously reported lung cancer-related studies, supporting the biological validity of our findings. Overall, combining disease conditions-based classification with explainable AI, this study provides a robust, interpretable framework for characterizing race- and sex-specific disparities in lung cancer, offering a foundation for precision oncology and equitable therapeutic development based on transcriptome profile only.",
      "journal": "Cancers",
      "year": "2025",
      "doi": "10.3390/cancers17213454",
      "authors": "Sobhan Masrur et al.",
      "keywords": "SHAP; explainable AI; health disparity; lung cancer; patient-specific biomarker",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41228248/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12607860",
      "ft_text_length": 48988,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12607860)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41230243",
      "title": "FairAlloc: Learning Fair Organ Allocation Policy for Liver Transplant.",
      "abstract": "UNLABELLED: Liver transplantation is a critical treatment for end-stage liver diseases, but ensuring fair and effective allocation of scarce donor organs remains a major challenge in healthcare. Existing allocation policies often struggle to balance clinical outcomes with fairness across patients and demographic groups. To address this, we propose FairAlloc, a learning-based framework that formulates organ allocation as a ranking problem and jointly optimizes for post-transplant outcomes and fairness. Specifically, FairAlloc integrates two fairness objectives-group fairness across sensitive attributes such as gender and race and individual fairness between similar patients-into the ranking process. We evaluate FairAlloc using real-world data from the Organ Procurement and Transplantation Network (OPTN) and compare its performance to six baseline methods. Results show that FairAlloc improves group and individual fairness metrics by up to 37.9% and 39.9%, respectively, while maintaining competitive performance on key post-transplant outcomes such as graft failure and survival rates. This work contributes a novel, fairness-aware decision-making framework to the healthcare informatics community, with the potential to improve equity and efficiency in organ allocation systems. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007/s41666-025-00206-8.",
      "journal": "Journal of healthcare informatics research",
      "year": "2025",
      "doi": "10.1007/s41666-025-00206-8",
      "authors": "Ding Sirui et al.",
      "keywords": "Fairness in healthcare; Multi-objective optimization; Organ allocation; Organ transplant; Reinforcement learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41230243/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12602758",
      "ft_text_length": 1397,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12602758)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "41255469",
      "title": "The Target Study: A Conceptual Model and Framework for Measuring Disparity.",
      "abstract": "We present a conceptual model to measure disparity-the target study-where social groups may be similarly situated (i.e., balanced) on allowable covariates. Our model, based on a sampling design, does not intervene to assign social group membership or alter allowable covariates. To address non-random sample selection, we extend our model to generalize or transport disparity or to assess disparity after an intervention on eligibility-related variables that eliminates forms of collider-stratification. To avoid bias from differential timing of enrollment, we aggregate time-specific study results by balancing calendar time of enrollment across social groups. To provide a framework for emulating our model, we discuss study designs, data structures, and G-computation and weighting estimators. We compare our sampling-based model to prominent decomposition-based models used in healthcare and algorithmic fairness. We provide R code for all estimators and apply our methods to measure health system disparities in hypertension control using electronic medical records.",
      "journal": "Sociological methods & research",
      "year": "2025",
      "doi": "10.1177/00491241251314037",
      "authors": "Jackson John W et al.",
      "keywords": "Conceptual Model; Disparity; Equity; Ethics; Fairness; Framework; Target Study Emulation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41255469/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12622519",
      "ft_text_length": 1071,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12622519)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41307800",
      "title": "Inequity in imaging: Why it matters? A statement from the Equity, Diversity and Inclusion Subcommittee of the European Society of Radiology.",
      "abstract": "The ESR equity, diversity and inclusivity (EDI) subcommittee is a part of the Young ESR committee created in 2024. This statement paper is the first in our series regarding EDI and radiology. In this paper, we examine and discuss issues which have been studied and reported regarding the inequity of imaging services. Inequity is prevalent in radiology and imaging circles in Europe. The variations observed in women, ethnic, age, disabled, non-binary, and gender groups are examined, as well as the variations in radiology research and in artificial intelligence-related imaging. Radiology departments need to be aware of the existing variations in radiology services. They need to educate their personnel on the etiquette and interaction with diverse populations. There should be versatile equipment to serve patients with disabilities. Radiologists should be aware of the lack of evidence-based knowledge with regard to female and non-white populations. Regarding clinical AI, departments need to actively audit and check for possible biases in AI in clinical use. CRITICAL RELEVANCE STATEMENT: Understanding how EDI affects patient care is vital to providing equitable service to all patients. Radiologists should be aware of the lack of evidence-based knowledge regarding female and non-white populations, and be sensibly critical of guidelines which lack proper evidence. KEY POINTS: The workflow of the department should be organised so that all patients are served equitably. Radiologists need to be aware of the lack of evidence-based knowledge about female and non-white populations, and be critical of guidelines which lack proper evidence. Regarding AI, radiologists must actively audit and check for possible biases in AI in clinical use.",
      "journal": "Insights into imaging",
      "year": "2025",
      "doi": "10.1186/s13244-025-02144-w",
      "authors": "Parkar Anagha P et al.",
      "keywords": "Diversity; Equity; Imaging (AI); Inclusivity; Radiology",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41307800/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12660587",
      "ft_text_length": 14432,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12660587)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41332851",
      "title": "Causal modeling of chronic kidney disease in a participatory framework for informing the inclusion of social drivers in health algorithms.",
      "abstract": "Incomplete or incorrect causal theories are a key source of bias in machine learning (ML) algorithms. Community-engaged methodologies provide an avenue for mitigating this bias through incorporating causal insights from community stakeholders into ML development. In health applications, community-engaged approaches can enable the study of social drivers of health (SDOH), which are known to shape health inequities. However, it remains challenging for SDOH to inform ML algorithms, partially because SDOH variables are known to be interrelated, yet it is difficult to elucidate the causal relationships between them. Community based system dynamics is a community-engaged methodology that can be used to co-create formal causal graphs, called causal loop diagrams, with patients. Here, we used community based system dynamics to create a causal graph representing the impacts of SDOH on the progression of chronic kidney disease, a chronic condition with SDOH-driven health disparities. We conducted focus groups with 42 participants and a day-long model building workshop with 11 participants, resulting in a final graph comprising 16 variables, 42 causal links, and 5 subsystems of semantically related SDOH variables. This final graph, representing the causal relationships between social variables relevant to chronic kidney disease, can inform the development of clinical ML algorithms and other technological interventions.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.11.19.25340498",
      "authors": "Foryciarz Agata et al.",
      "keywords": "causal loop diagram; chronic kidney disease; community based systems dynamics; community engaged research; group model building; health inequities; participatory AI; social drivers of health",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41332851/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12668084",
      "ft_text_length": 29806,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12668084)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41343855",
      "title": "Assessing Usefulness of the Dashboard Instrument to Review Equity (DIRE) Checklist to Evaluate Equity in Public Health Dashboards: Reliability Study.",
      "abstract": "BACKGROUND: The COVID-19 pandemic was a critical time for public health, and though dashboards remained a source of critical health information for decision-makers, key gaps in equity-based decision support were revealed. The DIRE (Dashboard Instrument to Review Equity) Framework and Checklist tool was developed to be a practical tool for public health departments to use in evaluating equity-based decision support mechanisms in their dashboards. OBJECTIVE: The objective of this agreement and reliability study was to validate the DIRE Checklist tool as a practical and reliable instrument for data practitioners to use in evaluating dashboards. METHODS: This study was divided into 5 steps to conduct the necessary analysis for agreement and reliability. Step 1 completed the development of the DIRE Checklist tool in Qualtrics (Qualtrics International Inc). Step 2 focused on the parameters required for the selection of the 26 US state-based dashboards. Step 3 was the user testing and assessment process during which each reviewer applied the DIRE tool to each dashboard. Step 4 involved conducting different assessment methods to specifically calculate the comparative analysis, interrater agreement, intraclass correlation coefficients, and the cosine similarity for the Qualtrics, reviewer, and categorical scores. Finally, Step 5 involved conducting any qualitative assessment required on the notes. RESULTS: A total of 26 dashboards were evaluated using the DIRE Checklist tool by 2 reviewers. The overall percentage comparison for the Qualtrics Score was 31.7% (28.24/89) for Reviewer 1 and 41.8% (37.16/89) for Reviewer 2, resulting in a relative percent agreement of 72.7%. Additionally, the categorical scores showed substantial to high agreement across most categories based on percent agreement within each category. The intraclass correlation coefficient scores indicated varying levels of agreement across different categories, with good agreement observed for the Qualtrics score. CONCLUSIONS: The reliability and agreement result of the study confirmed strong performance of the DIRE checklist tool. The scores calculated were evaluated consistently and reliably by both raters-demonstrating the DIRE Checklist tool's ability to robustly evaluate different dashboards across a number of different categories and parameters.",
      "journal": "JMIR public health and surveillance",
      "year": "2025",
      "doi": "10.2196/71094",
      "authors": "Sosa Paulina et al.",
      "keywords": "COVID-19; dashboards; decision support; equity; informatics; public health; public health data; public health informatics",
      "mesh_terms": "Checklist; Humans; COVID-19; Reproducibility of Results; Public Health; Health Equity; Pandemics; United States; Dashboard Systems",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41343855/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12677865",
      "ft_text_length": 48007,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12677865)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41349854",
      "title": "ChatGPT-Assisted Glaucoma Diagnosis: A Health-Equitable Multi-Ancestry Analysis Using Visual Field and Optical Coherence Tomography Data.",
      "abstract": "PURPOSE: Early glaucoma detection is challenging due to variable ocular anatomy, non-glaucomatous optic neuropathy impacting optical coherence tomography (OCT) results, and the subjective nature of visual field (VF) tests. Multimodal large language models may overcome these challenges to provide equitable and accurate screening diagnoses across ancestries and glaucoma genetic predispositions. We evaluated ChatGPT o1 Pro's accuracy in identifying glaucoma using circumpapillary retinal nerve fiber layer (RNFL) OCT and VF data, and its consistency across ancestries and glaucoma polygenic risk scores (PRS). DESIGN: Cross-sectional diagnostic accuracy study. SETTINGS AND PARTICIPANTS: We enrolled 204 participants from the Mount Sinai BioMe Biobank for a comprehensive ophthalmic examination from November 2022 to March 2025. This cross-sectional diagnostic accuracy study included 38% European (EUR) and 62% non-European (non-EUR) participants stratified by low/intermediate (n = 107) and high-risk glaucoma PRS (n = 97). Two glaucoma specialists masked to PRS status provided a consensus reference diagnosis. ChatGPT received only de-identified VFs and OCT-RNFL numerical outputs to determine glaucoma status. Performance metrics were compared with the reference diagnosis. Subgroup comparisons by ancestry (EUR versus non-EUR) and PRS (high versus low/intermediate) were conducted. We used logistic regression models to assess the impacts of ancestry, PRS and ocular parameters on classification accuracy. MAIN OUTCOME MEASURES: ChatGPT o1 Pro's diagnostic performance in detecting glaucoma compared to consensus specialist diagnoses, stratified by ancestry and genetic risk. RESULTS: ChatGPT o1 Pro exhibited 96.0% sensitivity (95% confidence interval (CI): 88.3%-100%), 83.7% specificity (95% CI: 78.3%-89.1%), 85.2% accuracy (95% CI: 80.3%-90.1%), an area under the receiver operator curve (AUC) of 0.899, a positive predictive value (PPV) of 45.3% (95% CI: 31.9%-58.7%), and a negative predictive value (NPV) of 99.3% (95% CI: 98.0%-100%); \u03ba for agreement with the consensus reference was 0.538. No significant differences were observed between EUR and non-EUR subgroups (AUC: 0.894 vs 0.906, P = .79; accuracy: 88.3% vs 83.3%, P = .44) or high and low/intermediate-PRS subgroups (AUC: 0.889 vs 0.922, P = .45; accuracy: 85.4% vs 85.0%, P = .50). Global RNFL was the only determinant of reference disease classification (OR = 1.1 per micron, P < .001). CONCLUSION: ChatGPT o1 Pro diagnosed glaucoma similarly to specialists using only VF and OCT data. The model performance was similar across ancestral groups and genetic predispositions to glaucoma.",
      "journal": "American journal of ophthalmology",
      "year": "2026",
      "doi": "10.1016/j.ajo.2025.11.046",
      "authors": "Huang Andy S et al.",
      "keywords": "",
      "mesh_terms": "Humans; Tomography, Optical Coherence; Cross-Sectional Studies; Visual Fields; Male; Female; Middle Aged; Retinal Ganglion Cells; Visual Field Tests; Nerve Fibers; Glaucoma; Aged; Intraocular Pressure; ROC Curve; Optic Disk; Reproducibility of Results; Adult; Generative Artificial Intelligence",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41349854/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12698116",
      "ft_text_length": 2605,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12698116)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41360694",
      "title": "Seeing Is Believing? Exploring Gender Bias in Artificial Intelligence Imagery of Specialty Doctors.",
      "abstract": "BACKGROUND: In medicine and medical education, women are disproportionately affected by gender bias. Artificial intelligence (AI) is increasingly being employed in medical education. As gender bias exists within AI, there is a risk of reinforcing gender stereotypes if AI is used to generate images of medical professionals. We examined whether the gender distribution of doctors seen in AI-generated images was representative of UK specialty trainee doctors. METHODS: Free-to-use AI text-to-image generators were used to create 1200 images across 30 specialties. NHS England recruitment data provided figures on gender. Specialties accounting for <\u20090.25% of overall recruitment were excluded as small numbers precluded meaningful analysis. Each image was independently reviewed by both authors and classified (male/female/not-classifiable). Any disagreement was resolved by discussion. 'Not-classifiable' images were removed from analysis. Gender distribution between the AI images and recruitment data was compared (chi-squared test, significance p\u2009<\u20090.05). FINDINGS: There was a significantly higher proportion of males in the AI-generated images compared to NHS specialty data (82% vs. 47%; p\u2009<\u20090.0001). Notably, both AI tools created no images of female general practitioners, orthopaedic surgeons or urologists. Conversely females were overrepresented as dermatologists, obstetricians and gynaecologists and plastic surgeons. CONCLUSION: The finding of representational and presentational gender bias in AI-generated images of doctors is consequential because 'visual culture' within medical school, and beyond, matters. We contend that healthcare educators ought to employ caution when using AI and consider developing guidance on responsible use of AI imagery; otherwise, they risk perpetuating, rather than challenging, harmful gender stereotypes about medical career pathways.",
      "journal": "The clinical teacher",
      "year": "2026",
      "doi": "10.1111/tct.70297",
      "authors": "Hartley Alice et al.",
      "keywords": "artificial intelligence; careers; gender bias; medicine; stereotypes; visual culture",
      "mesh_terms": "Humans; Artificial Intelligence; Female; Sexism; Male; United Kingdom; Physicians",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41360694/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12685619",
      "ft_text_length": 15832,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12685619)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41377245",
      "title": "Artificial intelligence for equitable remote patient monitoring in low-resource health systems.",
      "abstract": "",
      "journal": "Annals of medicine and surgery (2012)",
      "year": "2025",
      "doi": "10.1097/MS9.0000000000004193",
      "authors": "Alam Md Shafe Ul et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41377245/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12688776",
      "ft_text_length": 3877,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12688776)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41399713",
      "title": "Design and validation of a responsible artificial intelligence-based system for the referral of diabetic retinopathy patients.",
      "abstract": "Diabetic Retinopathy (DR) is a leading cause of vision loss among working-age individuals. Early detection can reduce the risk of vision loss by up to 95%, yet a shortage of retinologists and logistical challenges often delay the DR detection. Artificial Intelligence (AI) systems using Retinal Fundus Photographs (RFPs) present a promising solution. However, their clinical adoption is often hindered by issues such as low-quality data, model biases, learning of spurious features or lack of external validation. To address these challenges, we developed RAIS-DR, a Responsible AI System for DR screening that incorporates ethical principles across the AI lifecycle. RAIS-DR integrates efficient convolutional models for preprocessing, quality assessment, and three specialized DR classification models. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local dataset of 1046 patients, unseen by both systems. Results are reported for two clinically relevant referral criteria: Referable DR (RDR) and All-Cause Referable (ACR), the latter including low-quality or ungradable images. Evaluations were conducted both per patient and per image. RAIS-DR demonstrated performance improvements in patient-level referral: for RDR, F1-score, accuracy, and specificity increased by 12, 19, and 20%, respectively; for ACR, the corresponding increases were 5, 6, and 10%. RAIS-DR demonstrated equitable performance across demographic subgroups, with Disparate Impact (DI) values between 0.984 and 1.031 and Equal Opportunity Difference (EOD) values near zero. Model explainability helped identify a clinical limitation: false positives were linked to patients with a history of LASER treatment. These findings position RAIS-DR as a robust, reproducible, responsible, and clinically viable solution for DR screening.",
      "journal": "Health information science and systems",
      "year": "2026",
      "doi": "10.1007/s13755-025-00405-y",
      "authors": "Moya-S\u00e1nchez E Ulises et al.",
      "keywords": "Diabetic retinopathy; Fairness; Responsible AI system",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41399713/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12701887",
      "ft_text_length": 1820,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12701887)",
      "ft_reason": "Excluded: insufficient approach content (2 indicators)"
    },
    {
      "pmid": "41425665",
      "title": "Human reinforcement learning processes and biases: computational characterization and possible applications to behavioral public policy.",
      "abstract": "The reinforcement learning framework provides a computational and behavioral foundation for understanding how agents learn to maximize rewards and minimize punishments through interaction with their environment. This framework has been widely applied across disciplines, including artificial intelligence, animal psychology, and economics. Over the last decade, a growing body of research has shown that human reinforcement learning often deviates from normative standards, exhibiting systematic biases. The first aim of this paper is to propose a conceptual framework and a\u00a0taxonomy for evaluating computational biases within reinforcement learning. We specifically propose a distinction between praxic biases, characterized by a mismatch between internal representations and selected actions, and epistemic biases, characterized by a mismatch between past experiences and internal representations. Building on this foundation, we characterize\u00a0and discuss two primary types of epistemic biases: relative valuation and biased update. We describe their behavioral signatures\u00a0and discuss their potential adaptive roles. Finally, we eleborate on how these findings may shape future developments in both theoretical and applied domains. Notably, despite being widely used in clinical and educational settings, reinforcement-based interventions have been comparatively neglected in the domains of behavioral\u00a0public policy and decision-making\u00a0improvement, particularly when compared to more popular approaches such as nudges and boosts. In this review, we offer an explanation for this comparative neglect that we believe rooted in common historical and epistemological misconceptions, and advocate for a greater integration of reinforcement learning into the design of behavioral public policy.",
      "journal": "Mind & society",
      "year": "2025",
      "doi": "10.1007/s11299-025-00329-w",
      "authors": "Palminteri Stefano",
      "keywords": "Behavioral public policy; Cognitive biases; Context-dependent valuation; Positivity bias; Reinforcement learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41425665/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12713303",
      "ft_text_length": 57631,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12713303)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41445654",
      "title": "Is it possible to vaccinate AI against bias? An exploratory study in epilepsy.",
      "abstract": "IMPORTANCE: Large language models are increasingly used for clinical decision support yet may perpetuate socioeconomic biases. Whether simple prompt-based interventions can mitigate such biases remains unknown. OBJECTIVE: To determine whether a prompt-based 'inoculation' instructing large-language-models (LLMs) to disregard clinically irrelevant information can reduce bias and improve accuracy in recommendations. DESIGN: Experimental study conducted November 21 to December 11, 2025. Each clinical vignette was presented 10 times per condition to account for stochastic variance. SETTING: Publicly available web interfaces of six frontier LLMs with memory features disabled. PARTICIPANTS: No real patients were involved. Two fictional epilepsy vignettes (diagnostic and therapeutic) were created with identical clinical features but differing socioeconomic (SES) descriptors. MAIN OUTCOMES AND MEASURES: Accuracy (proportion of responses concordant with guidelines) and bias (accuracy difference between high and low SES vignettes), assessed via binary scoring based on evidence-based guidelines. RESULTS: A total of 480 LLM responses were analyzed. For diagnosis, base accuracy was 36% (43/120), with 45 percentage point bias gap (high SES 58% vs. low SES 13%); inoculation improved accuracy to 55% (66/120) and reduced bias to 27 percentage points. For treatment, base accuracy was 51% (61/120) with 25 percentage point bias gap; inoculation improved accuracy to 63% (75/120) and reduced bias to 8 percentage points. Responses to inoculation varied considerably: Gemini 3 Pro showed complete diagnostic bias elimination (low SES accuracy 0% \u2192 100%), while Sonnet 4.5 showed paradoxical worsening. CONCLUSIONS AND RELEVANCE: A simple prompt-based intervention overall reduced socioeconomic bias and improved accuracy in LLM clinical recommendations, though effects varied across models. Prompt engineering may offer a practical approach to mitigating specific AI bias in healthcare. KEY POINTS: Question: Can a simple prompt-based \"inoculation\" instructing large language models to ignore clinically irrelevant socioeconomic details reduce bias and improve accuracy in epilepsy diagnosis and treatment recommendations?Findings: In this experimental study of 480 responses from 6 large language models to paired high- vs low-socioeconomic status epilepsy vignettes, base diagnostic and treatment accuracies were 36% and 51%, respectively, with bias gaps of 45 and 25 percentage points, respectively; adding an inoculation prompt increased accuracy to 55% and 63% and reduced bias gaps to 27 and 8 percentage points, though effects varied by model, with some showing near-complete bias elimination and others demonstrating paradoxical worsening in certain conditions.Meaning: Prompt-based inoculation may offer a practical, low-cost strategy to partially mitigate socioeconomic bias and modestly improve the quality of large language model clinical recommendations, but model-specific behavior and residual disparities highlight the need for ongoing oversight and complementary bias-mitigation strategies.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.64898/2025.12.18.25342563",
      "authors": "Bhansali Rohan Manish et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41445654/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12723972",
      "ft_text_length": 3107,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12723972)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41472841",
      "title": "Using Under-Represented Subgroup Fine Tuning to Improve Fairness for Disease Prediction.",
      "abstract": "The role of artificial intelligence is growing in healthcare and disease prediction. Because of its potential impact and demographic disparities that have been identified in machine learning models for disease prediction, there are growing concerns about transparency, accountability and fairness of these predictive models. However, very little research has investigated methods for improving model fairness in disease prediction, particularly when the sensitive attribute is multivariate and when the distribution of sensitive attribute groups is highly skewed. In this work, we explore algorithmic fairness when predicting heart disease and Alzheimer's Disease and Related Dementias (ADRD). We propose a fine tuning approach to improve model fairness that takes advantage of observations from the majority groups to build a pre-trained model and uses observations from each underrepresented subgroup to fine tune the pre-trained model, thereby incorporating additional specific knowledge about each subgroup. We find that our fine tuning approach performs better than other algorithmic fairness fixing methods across all subgroups even if the subgroup distribution is very imbalanced and some subgroups are very small. This is an important step toward understanding approaches for improving fairness for healthcare and disease prediction.",
      "journal": "Biomedical engineering systems and technologies, international joint conference, BIOSTEC ... revised selected papers. BIOSTEC (Conference)",
      "year": "2025",
      "doi": "10.5220/0013318600003911",
      "authors": "Wang Yanchen et al.",
      "keywords": "Disease Prediction; Machine Learning Fairness; Model Fine Tuning; Multivariate Sensitive Attribute",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41472841/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12746336",
      "ft_text_length": 1341,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12746336)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41473602",
      "title": "ASL Consent in the Digital Informed Consent Process.",
      "abstract": "There is an estimated 500,000 people in the U.S. who are deaf and who use ASL and live in the U.S. Compared to the general population, deaf people are at greater risk of having chronic health problems and experience significant health disparities and inequities (Sanfacon, Leffers, Miller, Stabbe, DeWindt, Wagner, & Kushalnagar, 2020; Kushalnagar, Reesman, Holcomb, & Ryan, 2019; Kushalnagar & Miller, 2019). Much of the disparities are explained by the barriers in the environment, such as the unavailability of materials in ASL and lack of healthcare professionals who know how to provide deaf patient-centered care. Intersecting social determinants of health (e.g., intrinsic - low education; and extrinsic - barrier to healthcare services) create a mutually constituted vulnerability for health disparities when a person is deaf (Kushalnagar & Miller, 2019; Lesch, Brucher, Chapple, R., & Chapple, K., 2019; Smith & Chin, 2012). Moreover, the longstanding history of inequitable access to language and education, and a lack of printed information and materials, leave people who are deaf and use ASL unaware of opportunities to participate in cutting-edge research/clinical trials. An unintended consequence, therefore, is that PIs neglect to include people who are deaf and use ASL in their subject sample pools, and this marginalized population continues to be at disparity for health outcomes and also clinical research participation. One barrier is the unavailability of informed consent materials that are accessible in ASL. The current research study conducted by our team at the Center for Deaf Health Equity at Gallaudet University attempts to address the language barrier to the consent process through a careful reconsideration of its traditional English format and the development of an American Sign Language (ASL) informed consent app. This team successfully leveraged existing machine learning methods to develop a way to navigate and signature an informed consent process using ASL. We call this new method of navigation and signature \"ASL consent.\" In our findings, we found that deaf people who are primarily college educated were more likely to agree that the process for obtaining ASL consent through an accessible app is comparable to traditional English consent.",
      "journal": "Journal on technology and persons with disabilities : ... Annual International Technology and Persons with Disabilities Conference",
      "year": "2023",
      "doi": "10.1093/deafed/enz035",
      "authors": "Kosa Ben S et al.",
      "keywords": "American Sign Language; Consent; Deaf and hard of hearing; Machine Learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41473602/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12747571",
      "ft_text_length": 2308,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12747571)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41502557",
      "title": "Clinical practice, barriers to implementation, and priorities for equitable access of Stereotactic Body Radiation Therapy: An analysis of the global status by the ESTRO SBRT Focus Group.",
      "abstract": "BACKGROUND: Stereotactic Body Radiation Therapy (SBRT) has become an established treatment for several primary and metastatic malignancies; however, considerable heterogeneity remains in its definition, clinical indications, and technical delivery. METHODS: In May 2025, the SBRT Focus Group of the European Society for Radiotherapy and Oncology (ESTRO), in collaboration with International Stereotactic Radiosurgery Society (ISRS), the Radiosurgery Society (RSS), and the Japanese Society for Radiation Oncology (JASTRO), conducted a global survey. A 44-item questionnaire explored SBRT indications, technical aspects, dose/fractionation, and barriers to implementation. Descriptive statistics summarized the responses. RESULTS: Overall, 289 professionals from 59 countries participated. Routine use of SBRT was reported by 96.6\u00a0% of respondents, with lung, bone, liver and prostate as the most frequent indications. Pancreatic tumor (48.4\u00a0%), renal cell carcinoma (46.4\u00a0%), and ventricular tachycardia (12.4\u00a0%) represented emerging indications. C-arm linacs (89.2\u00a0%) and in-room Cone beam CT (CBCT) (92.0\u00a0%) were the dominant technologies. Motion management relied mainly on 4D-CT internal target volume (ITV) (88.9\u00a0%) and deep inspiration breath-hold (DIBH) (57.8\u00a0%). Fractionation was consistent for lung and prostate but heterogeneous for liver, and pancreas. Only 3.5\u00a0% reported routine use of online adaptive SBRT, while 61.5\u00a0% reported artificial intelligence (AI) use, mainly for organs-at-risk delineation. Key barriers included limited clinical trial funding (35.2\u00a0%), high equipment costs (34.2\u00a0%), insufficient reimbursement (27.7\u00a0%), and workforce shortages (33.9\u00a0%). CONCLUSIONS: This ESTRO international survey provides the first global overview of SBRT practices. It demonstrates broad adoption but also substantial variability, highlighting the need for consensus guidelines, greater trial access, and expanded education to harmonize SBRT delivery and ensure equitable care worldwide.",
      "journal": "Clinical and translational radiation oncology",
      "year": "2026",
      "doi": "10.1016/j.ctro.2025.101096",
      "authors": "Franzese Ciro et al.",
      "keywords": "Ablative radiotherapy; Advanced radiotherapy; Radiation oncology; SABR; SBRT; Stereotactic body radiation therapy; Stereotactic radiotherapy",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41502557/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12771324",
      "ft_text_length": 24763,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12771324)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41519721",
      "title": "Assessing nonresponse bias in a 30-year study of gulf war and gulf era veterans.",
      "abstract": "BACKGROUND: Cohort studies of veterans are critical for understanding the long-term health effects of deployment and toxic exposures. However, longitudinal research is susceptible to attrition and potential nonresponse bias. The Gulf War Era Cohort Study (GWECS) is the largest and longest-running longitudinal cohort study of 1990\u20131991 Gulf War veterans. In this paper, we identify demographic and military service characteristics associated with patterns of response over time and examine the extent to which accounting for nonresponse bias in Wave 4, conducted more than 30 years after the Gulf War, might impact the estimates of health conditions. METHOD: Multivariate multinomial logistic regression analysis was used to identify demographic and military service characteristics associated with response patterns over time (always responder, current responder, past responder, never responder). To adjust for nonresponse at Wave 4, a search algorithm was used to identify predictors of response and form weighting class cells. The effectiveness of nonresponse adjustments was evaluated by (1) comparing estimates of demographic and military characteristics before and after weighting, (2) examining the correlation between the weighting classes and health outcomes, and (3) comparing early and late responders on health outcomes. RESULTS: Wave 4 obtained a response rate of 47%, close to the 50% response rate obtained in Wave 3 over a decade earlier. Veterans most likely to respond to the survey over time and in Wave 4 were older, White, deployed, officers, and married in 1991. Weighting adjustments accounted for these differences and reduced bias in demographic and military characteristics, as well as in survey variables related to those characteristics. However, differences observed between early and late responders on alcohol and drug dependence suggest that these conditions may be underestimated. CONCLUSION: GWECS achieved a relatively high response rate in the most recent follow-up. Although differential response occurred, nonresponse adjustments effectively reduced bias in the key variables examined. This cohort continues to provide insight into the long-term health effects of Gulf War deployment, including cancer and chronic conditions, as the cohort ages. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1186/s12874-025-02761-5.",
      "journal": "BMC medical research methodology",
      "year": "2026",
      "doi": "10.1186/s12874-025-02761-5",
      "authors": "Gasper Joseph et al.",
      "keywords": "Cohort study; Longitudinal study; Nonresponse bias; Veterans; Weighting",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41519721/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12882365",
      "ft_text_length": 37931,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12882365)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41550542",
      "title": "2024 Healthcare Delivery Science: Innovation and Partnerships for Health Equity Research (DESCIPHER) Symposium.",
      "abstract": "BACKGROUND: The Southern California Healthcare Delivery Science Center organizes an annual symposium for a broad audience interested in health innovation. AIMS: The 2024 symposium convened healthcare professionals, researchers, policymakers, and advocates to explore innovative strategies for advancing health equity. MATERIALS & METHODS: Organizers assembled panels of patients, health system leaders, and researchers to present their perspectives on four primary themes: (1) conceptual frameworks for social determinants of health, (2) community-engaged healthcare delivery interventions, (3) healthcare system-based strategies and innovations, and (4) the ethical use of emerging technologies. The agenda also included poster presentations and interactive break-out sessions. RESULTS: Twenty presenters and facilitators engaged attendees in discussions throughout the day-long symposium. A common theme was understanding social determinants as fundamental, intermediate, and proximate drivers of health inequities. Strategies to bridge these gaps included interdisciplinary collaboration, engaging individuals with lived experience, healthcare system-based and community-centered interventions, ethical use of artificial intelligence, and policy reform. DISCUSSION: Presentations emphasized the importance of interdisciplinary collaboration, innovation, and policy reform in addressing social determinants of health and achieving equity. They also highlighted the significance of lived experience, community involvement, and data-driven strategies in advancing healthcare delivery science. CONCLUSION: The 2024 Healthcare Delivery Science Symposium successfully convened broad stakeholders to exchange ideas and proven strategies for advancing health equity.",
      "journal": "Learning health systems",
      "year": "2026",
      "doi": "10.1002/lrh2.70042",
      "authors": "Towfighi Amytis et al.",
      "keywords": "conference; engagement; healthcare delivery science; lived experience; symposium",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41550542/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12805802",
      "ft_text_length": 15773,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12805802)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41552743",
      "title": "Overcoming Normalcy Bias in Acute Myocardial Infarction: A Case Report of Generative AI as a Behavioral Catalyst for Emergency Care Seeking.",
      "abstract": "Pre-hospital delay remains a major determinant of outcomes in acute myocardial infarction (AMI), with normalcy bias playing a central role in patients' failure to interpret symptoms as signals of serious illness. This case report examines the role of generative artificial intelligence (AI) not as a diagnostic instrument\u00a0but as a behavioral catalyst that prompted timely emergency care seeking. A man in his early sixties presented with chest discomfort, neck radiation, bilateral lower molar pain, diaphoresis, and cold extremities. Although these symptoms are medically typical of AMI, myocardial infarction was not part of the patient's immediate cognitive framework, and they were initially interpreted as dental discomfort or nonspecific physical fatigue. After consulting a publicly available generative AI system that issued a clear imperative to contact emergency medical services, the patient activated emergency care. He was subsequently diagnosed with inferior ST-elevation myocardial infarction due to right coronary artery occlusion and underwent successful emergency percutaneous coronary intervention. This case suggests that AI-generated language can mitigate normalcy bias and accelerate patient decision-making in acute medical settings without functioning as a diagnostic tool.",
      "journal": "Cureus",
      "year": "2026",
      "doi": "10.7759/cureus.101199",
      "authors": "Ikeda Osamu et al.",
      "keywords": "acute myocardial infarction; artificial intelligence; behavioral catalyst; generative ai; large language models; normalcy bias; patient decision making; percutaneous coronary intervention; pre-hospital delay; stemi",
      "mesh_terms": "",
      "pub_types": "Case Reports; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41552743/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12811831",
      "ft_text_length": 9203,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12811831)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41635417",
      "title": "Airway quantifications of bronchitis patients with photon-counting and energy-integrating computed tomography.",
      "abstract": "PURPOSE: Accurate airway measurement is critical for bronchitis quantification with computed tomography (CT), yet optimal protocols and the added value of photon-counting CT (PCCT) over energy-integrating CT (EICT) for reducing bias remain unclear. We quantified biomarker accuracy across modalities and protocols and assessed strategies to reduce bias. APPROACH: A virtual imaging trial with 20 bronchitis anthropomorphic models was scanned using a validated simulator for two systems (EICT: SOMATOM Flash; PCCT: NAEOTOM Alpha) at 6.3 and 12.6\u00a0mGy. Reconstructions varied algorithm, kernel sharpness, slice thickness, and pixel size. Pi10 (square-root wall thickness at 10-mm perimeter) and WA% (wall-area percentage) were compared against ground-truth airway dimensions obtained from the 0.1-mm-precision anatomical models prior to CT simulation. External validation used clinical PCCT ( n = 22  ) and EICT ( n = 80  ). RESULTS: Simulated airway dimensions agreed with pathological references ( R = 0.89 - 0.93  ). PCCT had lower errors than EICT across segmented generations ( p < 0.05  ). Under optimal parameters, PCCT improved Pi10 and WA% accuracy by 26.3% and 64.9%. Across the tested PCCT and EICT imaging protocols, improvements were associated with sharper kernels (25.8% Pi10, 33.0% WA%), thinner slices (23.9% Pi10, 49.8% WA%), smaller pixels (17.0% Pi10, 23.1% WA%), and higher dose ( \u2264 3.9 %  ). Clinically, PCCT achieved higher maximum airway generation ( 8.8 \u00b1 0.5  versus 6.0 \u00b1 1.1  ) and lower variability, mirroring trends in virtual results. CONCLUSIONS: PCCT improves the accuracy and consistency of airway biomarker quantification relative to EICT, particularly with optimized protocols. The validated virtual platform enables modality-bias assessment and protocol optimization for accurate, reproducible bronchitis measurements.",
      "journal": "Journal of medical imaging (Bellingham, Wash.)",
      "year": "2026",
      "doi": "10.1117/1.JMI.13.1.013501",
      "authors": "Ho Fong Chi et al.",
      "keywords": "DukeSim; chronic obstructive pulmonary disease; computational phantoms; extended cardiac-torso; photon-counting computed tomography; virtual clinical trial",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41635417/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12863983",
      "ft_text_length": 1853,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12863983)",
      "ft_reason": "Excluded: insufficient approach content (1 indicators)"
    },
    {
      "pmid": "41642815",
      "title": "Does ChatGPT enhance equity for global health publications? Copyediting by ChatGPT compared to Grammarly and a human editor.",
      "abstract": "English language copyediting poses significant barriers to global health authors in academic publishing. Editing is too expensive for most researchers in low-income countries, and large language models (LLMs) like ChatGPT may offer a cost-effective alternative. The technology, however, has been criticized for its biases and inaccuracies. In a preliminary, in-depth case comparison, we compared the number and quality of corrections made by U-M GPT, a secure, University of Michigan-hosted generative AI tool, to those from Grammarly and a human editor to text from two draft papers written by Ugandan sexual and reproductive health researchers. Overall, U-M GPT made about three times as many corrections compared to the human editor and about ten times more than Grammarly. U-M GPT was the least discriminating in terms of quality: only 61% (51/83) of its corrections were judged as improvements. Despite this, U-M GPT has advantages, such as a broad scope of correction types, fast turnaround, and no cost. Its disadvantages, which reflect shortcomings of LLMs more broadly, include the need for prompt engineering skill, careful review of corrections, and high environmental costs due to energy consumption. Additional concerns involve data privacy and content moderation policies that restrict discussions on topics deemed as sensitive; these included words related to sexual and reproductive health. Although LLMs could improve equity, efficiency, and productivity, several important issues should be considered when using the technology. Larger follow-up investigations are needed to confirm our findings. Authors using LLMs should consult journal guidelines and disclose their use.",
      "journal": "PloS one",
      "year": "2026",
      "doi": "10.1371/journal.pone.0342170",
      "authors": "August Ella et al.",
      "keywords": "",
      "mesh_terms": "Humans; Global Health; Publishing; Language; Generative Artificial Intelligence",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41642815/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12875453",
      "ft_text_length": 34092,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12875453)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41646534",
      "title": "Patient perceptions of artificial intelligence integration in dermatology: a cross-sectional study of trust, comfort and equity across multiple care modalities.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) and telemedicine are rapidly changing the way dermatological care is delivered. As these tools are increasingly used in tandem, understanding how patients perceive the integration of AI across different care settings is important for responsible implementation. OBJECTIVES: To assess patient perceptions of AI in dermatology across five care modalities and examine how demographic factors influence acceptance. METHODS: A cross-sectional survey was conducted among 130 adults at a US academic dermatology clinic between December 2024 and April 2025. Participants rated trust, comfort, perceived quality, privacy and confidence in equitable performance across three AI-involved modalities: standalone AI apps, AI-assisted in-person visits and AI-assisted telemedicine visits. Differences in perception outcomes across the three care modalities were analysed using repeated measures Anova. Logistic and linear regressions analysed predictors of acceptance, including age, race, skin tone, socioeconomic status, rurality and technology experience. RESULTS: Patients strongly preferred dermatologist-involved care over standalone AI, with 73.8% trusting dermatologist-guided AI and only 1.5% trusting AI apps alone. Comfort and perceptions of equal performance across skin tones were significantly higher for telemedicine and AI-assisted visits compared with AI apps (P < 0.001). Darker skin tone and Black race predicted lower acceptance of AI-assisted care (P = 0.01 and P = 0.003, respectively), while greater technology familiarity predicted higher acceptance (P = 0.05). Comfort varied by clinical scenario, with in-person visits showing dramatically higher odds of patient comfort compared with AI apps alone [odds ratio (OR) 232.8 for new concerns, OR 137.3 for serious concerns, OR 18.4 for sensitive concerns]. AI-assisted in-person visits also showed significantly higher odds of comfort over AI apps (OR 18.4 for serious concerns, OR 3.6 for ongoing concerns). CONCLUSIONS: Patients strongly prefer AI as clinical support systems rather than autonomous decision-makers, especially for high-stakes and sensitive concerns. Differences in acceptance by race and skin tone point to the need for better representation in datasets and clearer communication about how these tools perform. Moving forward, development and implementation should emphasize clinician and patient involvement, fairness and patient choice to ensure AI is integrated into dermatology in a way that earns patient trust.",
      "journal": "Skin health and disease",
      "year": "2026",
      "doi": "10.1093/skinhd/vzaf086",
      "authors": "McRae Charlotte et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41646534/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12867938",
      "ft_text_length": 36104,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12867938)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41659839",
      "title": "Big data in healthcare and medicine revisited design and managerial challenges in the age of artificial intelligence.",
      "abstract": "A decade ago, we characterized big data in healthcare as a nascent field anchored in distributed computing paradigms. The intervening years have witnessed a transformation so profound that revisiting our original framework is essential. This paper critically examines the evolution of big data in healthcare and medicine, assessing the shift from Hadoop-centric architectures to cloud computing platforms and GPU-accelerated artificial intelligence, including large language models and the emerging paradigm of agentic AI. The landscape has been reshaped by landmark biobank initiatives, breakthrough applications such as AlphaFold's Nobel Prize-winning solution to protein structure prediction, and the rapid growth of FDA-cleared AI medical devices from fewer than ten in 2015 to over 1200 by mid-2025. AI has enabled advances across precision oncology, drug discovery, and public health surveillance. Yet new challenges have emerged: algorithmic bias perpetuating health disparities, opacity undermining clinical trust, environmental sustainability concerns, and unresolved questions of privacy, security, data ownership, and interoperability. We propose extending the original \"4Vs\" framework to accommodate veracity through explainability, validity through fairness, and viability through sustainability. The paper concludes with prescriptive implications for healthcare organizations, technology developers, policymakers, and researchers.",
      "journal": "Health information science and systems",
      "year": "2026",
      "doi": "10.1007/s13755-026-00433-2",
      "authors": "Raghupathi Wullianallur et al.",
      "keywords": "Agentic AI; Algorithmic bias; Artificial intelligence; Big data; Clinical trials; Cloud computing; Drug discovery; Electronic health records; Explainable AI,; Governance; Healthcare; Interoperability; Large language models; Medicine; Precision medicine; Privacy; Public health surveillance; Security",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41659839/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12881233",
      "ft_text_length": 1444,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12881233)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41694236",
      "title": "Data Fit for Health Equity: Learning Health Systems, AI, and the STANDING Together Recommendations.",
      "abstract": "INTRODUCTION: Artificial Intelligence (AI) tools may deliver significant improvements in healthcare and Learning Health Systems are well positioned to benefit. However, during the adoption of AI, Learning Health Systems should consider the potential for AI to exacerbate health inequity and perpetuate biases that exist in healthcare and its associated data. METHODS: The STANDING Together recommendations provide a method to identify and report potential bias during the curation of datasets for AI and the development of AI from those datasets. The recommendations could form a key learning cycle within a Learning Health System ensuring transparent reporting of healthcare data use and the implementation of AI healthcare technologies that promote health equity. RESULTS AND CONCLUSIONS: Learning Health Systems are well placed to adopt the STANDING Together best practice recommendations for using healthcare data as they are likely to have both the capabilities to implement the recommendations and the strategic goals that will realize the value of health data and AI that promotes health equity. The STANDING Together recommendations are available from www.datadiversity.org.",
      "journal": "Learning health systems",
      "year": "2026",
      "doi": "10.1002/lrh2.70053",
      "authors": "Laws Elinor et al.",
      "keywords": "artificial intelligence; clinical decision support; health data; health equity; learning health systems",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41694236/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12900613",
      "ft_text_length": 13312,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12900613)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "30128552",
      "title": "Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data.",
      "abstract": "A promise of machine learning in health care is the avoidance of biases in diagnosis and treatment; a computer algorithm could objectively synthesize and interpret the data in the medical record. Integration of machine learning with clinical decision support tools, such as computerized alerts or diagnostic support, may offer physicians and others who provide health care targeted and timely information that can improve clinical decisions. Machine learning algorithms, however, may also be subject to biases. The biases include those related to missing data and patients not identified by algorithms, sample size and underestimation, and misclassification and measurement error. There is concern that biases and deficiencies in the data used by machine learning algorithms may contribute to socioeconomic disparities in health care. This Special Communication outlines the potential biases that may be introduced into machine learning-based clinical decision support tools that use electronic health record data and proposes potential solutions to the problems of overreliance on automation, algorithms based on biased data, and algorithms that do not provide information that is clinically meaningful. Existing health care disparities should not be amplified by thoughtless or excessive reliance on machines.",
      "journal": "JAMA internal medicine",
      "year": "2018",
      "doi": "10.1001/jamainternmed.2018.3763",
      "authors": "Gianfrancesco Milena A et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Electronic Health Records; Healthcare Disparities; Humans; Machine Learning; Socioeconomic Factors",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30128552/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC6347576",
      "ft_text_length": 1311,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC6347576)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "31344123",
      "title": "Analysis of gene expression in rheumatoid arthritis and related conditions offers insights into sex-bias, gene biotypes and co-expression patterns.",
      "abstract": "The era of next-generation sequencing has mounted the foundation of many gene expression studies. In rheumatoid arthritis research, this has led to the discovery of important candidate genes which offered novel insights into mechanisms and their possible roles in the cure of the disease. In the last years, data generation has outstripped data analysis and while many studies focused on specific aspects of the disease, a global picture of the disease is not yet accomplished. Here, we analyzed and compared a collection of gene expression information from healthy individuals and from patients suffering under different arthritis conditions from published studies containing the following clinical conditions: early and established rheumatoid arthritis, osteoarthritis and arthralgia. We show comprehensive overviews of this data collection and give new insights specifically on gene expression in the early stage, into sex-dependent gene expression, and we describe general differences in expression of different biotypes of genes. Many genes that are related to cytoskeleton changes (actin filament related genes) are differently expressed in early rheumatoid arthritis in comparison to healthy subjects; interestingly, eight of these genes reverse their expression ratio significantly between men and women compared early rheumatoid arthritis and healthy subjects. There are some slighter changes between men and woman between the conditions early and established rheumatoid arthritis. Another aspect are miRNAs and other gene biotypes which are not only promising candidates for diagnoses but also change their expression grossly in average at rheumatoid arthritis and arthralgia compared to the healthy condition. With a selection of intersecting genes, we were able to generate simple classification models to distinguish between healthy and rheumatoid arthritis as well as between early rheumatoid arthritis to other arthritides based on gene expression.",
      "journal": "PloS one",
      "year": "2019",
      "doi": "10.1371/journal.pone.0219698",
      "authors": "Platzer Alexander et al.",
      "keywords": "",
      "mesh_terms": "Arthritis, Rheumatoid; Cluster Analysis; Female; Gene Expression Regulation; Gene Ontology; Humans; Male; Models, Biological; Osteoarthritis; Principal Component Analysis; Sexism",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31344123/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC6657850",
      "ft_text_length": 41478,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC6657850)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "31794334",
      "title": "Reducing Disparities in Receipt of Genetic Counseling for Underserved Women at Risk of Hereditary Breast and Ovarian Cancer.",
      "abstract": "Purpose: Genetic counseling (GC) provides critical risk prediction information to women at-risk of carrying a genetic alternation; yet racial/ethnic and socioeconomic disparities persist with regard to GC uptake. This study examined patterns of GC uptake after a referral in a racially diverse population. Materials and Methods: In an urban academic medical center, medical records were reviewed between January 2016 and December 2017 for women who were referred to a genetic counselor for hereditary breast and ovarian cancer. Study outcomes were making an appointment (yes/no) and keeping an appointment. We assessed sociodemographic factors and clinical factors. Associations between factors and the outcomes were analyzed using chi square, and logistic regression was used for multivariable analysis. Results: A total of 510 women were referred to GC and most made appointments. More than half were white (55.3%) and employed (53.1%). No significant associations were observed between sociodemographic factors and making an appointment. A total of 425 women made an appointment and 268 kept their appointment. Insurance status (p\u2009=\u20090.003), marital status (p\u2009=\u20090.000), and work status (p\u2009=\u20090.039) were associated with receiving GC. In the logistic model, being married (odds ratio [OR] 2.119 [95% confidence interval, CI 1.341-3.347] p\u2009=\u20090.001) and having insurance (OR 2.203 [95% CI 1.208-4.016] p\u2009=\u20090.021) increased the likelihood of receiving counseling. Conclusions: Racial disparities in GC uptake were not observed in this sample. Unmarried women may need additional support to obtain GC. Financial assistance or other options need to be discussed during navigation as a way to lessen the disparity between women with insurance and those without.",
      "journal": "Journal of women's health (2002)",
      "year": "2020",
      "doi": "10.1089/jwh.2019.7984",
      "authors": "Sutton Arnethea L et al.",
      "keywords": "BRCA 1/2; disparities; genetic counseling; hereditary breast and ovarian cancer; navigation",
      "mesh_terms": "Attitude to Health; Black People; Breast Neoplasms; Child; Female; Genes, BRCA1; Genes, BRCA2; Genetic Counseling; Genetic Testing; Health Services Accessibility; Healthcare Disparities; Humans; Ovarian Neoplasms; Retrospective Studies; Risk Factors; Socioeconomic Factors; Virginia; White People; Black or African American",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31794334/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7462013",
      "ft_text_length": 1769,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7462013)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "31890853",
      "title": "Racial disparities and temporal trends in dementia misdiagnosis risk in the United States.",
      "abstract": "INTRODUCTION: Systematic disparities in misdiagnosis of dementia across racial/ethnic groups have implications for health disparities. We compared the risk of dementia under- and overdiagnosis in clinical settings across racial/ethnic groups from 2000 to\u00a02010. METHODS: We linked fee-for-service Medicare claims to participants aged \u226570 from the nationally representative Health and Retirement Study. We classified dementia status using an algorithm with similar sensitivity and specificity across racial/ethnic groups and assigned clinical dementia diagnosis status using ICD-9-CM codes from Medicare claims. Multinomial logit models were used to estimate relative risks of clinical under- and overdiagnosis between groups and over time. RESULTS: Non-Hispanic blacks had roughly double the risk of underdiagnosis as non-Hispanic whites. While primary analyses suggested a shrinking disparity over time, this was not robust to sensitivity analyses or adjustment for covariates. Risk of overdiagnosis increased over time in both groups. DISCUSSION: Our results suggest that efforts to reduce racial disparities in underdiagnosis are warranted.",
      "journal": "Alzheimer's & dementia (New York, N. Y.)",
      "year": "2019",
      "doi": "10.1016/j.trci.2019.11.008",
      "authors": "Gianattasio Kan Z et al.",
      "keywords": "Alzheimer's disease; Clinical diagnosis; Dementia; Disparities; Health and retirement study",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31890853/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC6926355",
      "ft_text_length": 26459,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC6926355)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "32462314",
      "title": "Deep learning COVID-19 detection bias: accuracy through artificial intelligence.",
      "abstract": "BACKGROUND: Detection of COVID-19 cases' accuracy is posing a conundrum for scientists, physicians, and policy-makers. As of April 23, 2020, 2.7 million cases have been confirmed, over 190,000 people are dead, and about 750,000 people are reported recovered. Yet, there is no publicly available data on tests that could be missing infections. Complicating matters and furthering anxiety are specific instances of false-negative tests. METHODS: We developed a deep learning model to improve accuracy of reported cases and to precisely predict the disease from chest X-ray scans. Our model relied on convolutional neural networks (CNNs) to detect structural abnormalities and disease categorization that were keys to uncovering hidden patterns. To do so, a transfer learning approach was deployed to perform detections from the chest anterior-posterior radiographs of patients. We used publicly available datasets to achieve this. RESULTS: Our results offer very high accuracy (96.3%) and loss (0.151 binary cross-entropy) using the public dataset consisting of patients from different countries worldwide. As the confusion matrix indicates, our model is able to accurately identify true negatives (74) and true positives (32); this deep learning model identified three cases of false-positive and one false-negative finding from the healthy patient scans. CONCLUSIONS: Our COVID-19 detection model minimizes manual interaction dependent on radiologists as it automates identification of structural abnormalities in patient's CXRs, and our deep learning model is likely to detect true positives and true negatives and weed out false positive and false negatives with > 96.3% accuracy.",
      "journal": "International orthopaedics",
      "year": "2020",
      "doi": "10.1007/s00264-020-04609-7",
      "authors": "Vaid Shashank et al.",
      "keywords": "Artificial intelligence; COVID-19; Deep learning; Detection bias",
      "mesh_terms": "Adolescent; Adult; Aged; Aged, 80 and over; Betacoronavirus; Bias; COVID-19; Child; Coronavirus Infections; Deep Learning; Female; Humans; Male; Middle Aged; Neural Networks, Computer; Pandemics; Pneumonia, Viral; SARS-CoV-2; Young Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32462314/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7251557",
      "ft_text_length": 11909,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7251557)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "32574353",
      "title": "Latent bias and the implementation of artificial intelligence in medicine.",
      "abstract": "Increasing recognition of biases in artificial intelligence (AI) algorithms has motivated the quest to build fair models, free of biases. However, building fair models may be only half the challenge. A seemingly fair model could involve, directly or indirectly, what we call \"latent biases.\" Just as latent errors are generally described as errors \"waiting to happen\" in complex systems, latent biases are biases waiting to happen. Here we describe 3 major challenges related to bias in AI algorithms and propose several ways of managing them. There is an urgent need to address latent biases before the widespread implementation of AI algorithms in clinical practice.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2020",
      "doi": "10.1093/jamia/ocaa094",
      "authors": "DeCamp Matthew et al.",
      "keywords": "artificial intelligence; bias; clinical decision support; health informatics; machine learning",
      "mesh_terms": "Algorithms; Artificial Intelligence; Bias; Decision Support Systems, Clinical; Humans; Prejudice",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32574353/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7727353",
      "ft_text_length": 677,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7727353)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "32825349",
      "title": "Predicting Psychological Distress Amid the COVID-19 Pandemic by Machine Learning: Discrimination and Coping Mechanisms of Korean Immigrants in the U.S.",
      "abstract": "The current study examined the predictive ability of discrimination-related variables, coping mechanisms, and sociodemographic factors on the psychological distress level of Korean immigrants in the U.S. amid the COVID-19 pandemic. Korean immigrants (both foreign-born and U.S.-born) in the U.S. above the age of 18 were invited to participate in an online survey through purposive sampling. In order to verify the variables predicting the level of psychological distress on the final sample from 42 states (n = 790), the Artificial Neural Network (ANN) analysis, which is able to examine complex non-linear interactions among variables, was conducted. The most critical predicting variables in the neural network were a person's resilience, experiences of everyday discrimination, and perception that racial discrimination toward Asians has increased in the U.S. since the beginning of the COVID-19 pandemic.",
      "journal": "International journal of environmental research and public health",
      "year": "2020",
      "doi": "10.3390/ijerph17176057",
      "authors": "Choi Shinwoo et al.",
      "keywords": "Artificial Neural Network; COVID-19; Korean immigrants; United States; mental health; racism",
      "mesh_terms": "Adaptation, Psychological; Adult; Betacoronavirus; COVID-19; Coronavirus Infections; Emigrants and Immigrants; Female; Humans; Machine Learning; Male; Middle Aged; Pandemics; Pneumonia, Viral; Racism; Republic of Korea; SARS-CoV-2; Stress, Psychological; United States",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32825349/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7504344",
      "ft_text_length": 31370,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7504344)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "33046699",
      "title": "Deep transfer learning for reducing health care disparities arising from biomedical data inequality.",
      "abstract": "As artificial intelligence (AI) is increasingly applied to biomedical research and clinical decisions, developing unbiased AI models that work equally well for all ethnic groups is of crucial importance to health disparity prevention and reduction. However, the biomedical data inequality between different ethnic groups is set to generate new health care disparities through data-driven, algorithm-based biomedical research and clinical decisions. Using an extensive set of machine learning experiments on cancer omics data, we find that current prevalent schemes of multiethnic machine learning are prone to generating significant model performance disparities between ethnic groups. We show that these performance disparities are caused by data inequality and data distribution discrepancies between ethnic groups. We also find that transfer learning can improve machine learning model performance for data-disadvantaged ethnic groups, and thus provides an effective approach to reduce health care disparities arising from data inequality among ethnic groups.",
      "journal": "Nature communications",
      "year": "2020",
      "doi": "10.1038/s41467-020-18918-3",
      "authors": "Gao Yan et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artificial Intelligence; Biomedical Research; Ethnicity; Healthcare Disparities; Humans; Machine Learning; Neoplasms",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33046699/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7552387",
      "ft_text_length": 43577,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7552387)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "33506704",
      "title": "A Latent Disease Model to Reduce Detection Bias in Cancer Risk Prediction Studies.",
      "abstract": "In studies of cancer risk, detection bias arises when risk factors are associated with screening patterns, affecting the likelihood and timing of diagnosis. To eliminate detection bias in a screened cohort, we propose modeling the latent onset of cancer and estimating the association between risk factors and onset rather than diagnosis. We apply this framework to estimate the increase in prostate cancer risk associated with black race and family history using data from the SELECT prostate cancer prevention trial, in which men were screened and biopsied according to community practices. A positive family history was associated with a hazard ratio (HR) of prostate cancer onset of 1.8, lower than the corresponding HR of prostate cancer diagnosis (HR = 2.2). This result comports with a finding that men in SELECT with a family history were more likely to be biopsied following a positive PSA test than men with no family history. For black race, the HRs for onset and diagnosis were similar, consistent with similar patterns of screening and biopsy by race. If individual screening and diagnosis histories are available, latent disease modeling can be used to decouple risk of disease from risk of disease diagnosis and reduce detection bias.",
      "journal": "Evaluation & the health professions",
      "year": "2021",
      "doi": "10.1177/0163278720984203",
      "authors": "Aleshin-Guendel Serge et al.",
      "keywords": "cancer screening; detection bias; disease surveillance; latent model; risk prediction",
      "mesh_terms": "Early Detection of Cancer; Humans; Male; Mass Screening; Prostate-Specific Antigen; Prostatic Neoplasms; Risk Factors",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33506704/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8279086",
      "ft_text_length": 1247,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8279086)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "33647062",
      "title": "Convolutional neural network model based on radiological images to support COVID-19 diagnosis: Evaluating database biases.",
      "abstract": "As SARS-CoV-2 has spread quickly throughout the world, the scientific community has spent major efforts on better understanding the characteristics of the virus and possible means to prevent, diagnose, and treat COVID-19. A valid approach presented in the literature is to develop an image-based method to support COVID-19 diagnosis using convolutional neural networks (CNN). Because the availability of radiological data is rather limited due to the novelty of COVID-19, several methodologies consider reduced datasets, which may be inadequate, biasing the model. Here, we performed an analysis combining six different databases using chest X-ray images from open datasets to distinguish images of infected patients while differentiating COVID-19 and pneumonia from 'no-findings' images. In addition, the performance of models created from fewer databases, which may imperceptibly overestimate their results, is discussed. Two CNN-based architectures were created to process images of different sizes (512 \u00d7 512, 768 \u00d7 768, 1024 \u00d7 1024, and 1536 \u00d7 1536). Our best model achieved a balanced accuracy (BA) of 87.7% in predicting one of the three classes ('no-findings', 'COVID-19', and 'pneumonia') and a specific balanced precision of 97.0% for 'COVID-19' class. We also provided binary classification with a precision of 91.0% for detection of sick patients (i.e., with COVID-19 or pneumonia) and 98.4% for COVID-19 detection (i.e., differentiating from 'no-findings' or 'pneumonia'). Indeed, despite we achieved an unrealistic 97.2% BA performance for one specific case, the proposed methodology of using multiple databases achieved better and less inflated results than from models with specific image datasets for training. Thus, this framework is promising for a low-cost, fast, and noninvasive means to support the diagnosis of COVID-19.",
      "journal": "PloS one",
      "year": "2021",
      "doi": "10.1371/journal.pone.0247839",
      "authors": "Maior Caio B S et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Bias; COVID-19; Databases, Factual; Deep Learning; Humans; Image Interpretation, Computer-Assisted; Neural Networks, Computer; Pneumonia; Radiography, Thoracic",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33647062/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC7920391",
      "ft_text_length": 51821,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC7920391)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "33989163",
      "title": "Minimizing Selection and Classification Biases. Comment on \"Clinical Characteristics and Prognostic Factors for Intensive Care Unit Admission of Patients With COVID-19: Retrospective Study Using Machine Learning and Natural Language Processing\".",
      "abstract": "",
      "journal": "Journal of medical Internet research",
      "year": "2021",
      "doi": "10.2196/27142",
      "authors": "Martos P\u00e9rez Francisco et al.",
      "keywords": "COVID-19; SARS-CoV-2; artificial intelligence; big data; classification bias; critical care; electronic health records; predictive model; prognosis; tachypnea",
      "mesh_terms": "Bias; COVID-19; Humans; Intensive Care Units; Machine Learning; Natural Language Processing; Prognosis; Retrospective Studies; SARS-CoV-2",
      "pub_types": "Journal Article; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33989163/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8190647",
      "ft_text_length": 2641,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8190647)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "33989164",
      "title": "Authors' Reply to: Minimizing Selection and Classification Biases Comment on \"Clinical Characteristics and Prognostic Factors for Intensive Care Unit Admission of Patients With COVID-19: Retrospective Study Using Machine Learning and Natural Language Processing\".",
      "abstract": "",
      "journal": "Journal of medical Internet research",
      "year": "2021",
      "doi": "10.2196/29405",
      "authors": "Izquierdo Jose Luis et al.",
      "keywords": "COVID-19; SARS-CoV-2; artificial intelligence; big data; classification bias; critical care; electronic health records; predictive model; prognosis; tachypnea",
      "mesh_terms": "Bias; COVID-19; Humans; Intensive Care Units; Machine Learning; Natural Language Processing; Prognosis; Retrospective Studies; SARS-CoV-2",
      "pub_types": "Letter; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33989164/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8190644",
      "ft_text_length": 2481,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8190644)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "34591923",
      "title": "An analysis of unconscious gender bias in academic texts by means of a decision algorithm.",
      "abstract": "Inclusive language focuses on using the vocabulary to avoid exclusion or discrimination, specially referred to gender. The task of finding gender bias in written documents must be performed manually, and it is a time-consuming process. Consequently, studying the usage of non-inclusive language on a document, and the impact of different document properties (such as author gender, date of presentation, etc.) on how many non-inclusive instances are found, is quite difficult or even impossible for big datasets. This research analyzes the gender bias in academic texts by analyzing a study corpus of more than 12,000 million words obtained from more than one hundred thousand doctoral theses from Spanish universities. For this purpose, an automated algorithm was developed to evaluate the different characteristics of the document and look for interactions between age, year of publication, gender or the field of knowledge in which the doctoral thesis is framed. The algorithm identified information patterns using a CNN (convolutional neural network) by the creation of a vector representation of the sentences. The results showed evidence that there was a greater bias as the age of the authors increased, who were more likely to use non-inclusive terms; it was concluded that there is a greater awareness of inclusiveness in women than in men, and also that this awareness grows as the candidate is younger. The results showed evidence that the age of the authors increased discrimination, with men being more likely to use non-inclusive terms (up to an index of 23.12), showing that there is a greater awareness of inclusiveness in women than in men in all age ranges (with an average of 14.99), and also that this awareness grows as the candidate is younger (falling down to 13.07). In terms of field of knowledge, the humanities are the most biased (20.97), discarding the subgroup of Linguistics, which has the least bias at all levels (9.90), and the field of science and engineering, which also have the least influence (13.46). Those results support the assumption that the bias in academic texts (doctoral theses) is due to unconscious issues: otherwise, it would not depend on the field, age, gender, and would occur in any field in the same proportion. The innovation provided by this research lies mainly in the ability to detect, within a textual document in Spanish, whether the use of language can be considered non-inclusive, based on a CNN that has been trained in the context of the doctoral thesis. A significant number of documents have been used, using all accessible doctoral theses from Spanish universities of the last 40 years; this dataset is only manageable by data mining systems, so that the training allows identifying the terms within the context effectively and compiling them in a novel dictionary of non-inclusive terms.",
      "journal": "PloS one",
      "year": "2021",
      "doi": "10.1371/journal.pone.0257903",
      "authors": "Orgeira-Crespo Pedro et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Data Mining; Decision Making; Education, Graduate; Female; Humans; Language; Male; Neural Networks, Computer; Sexism; Spain; Unconscious, Psychology; Universities",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34591923/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8483299",
      "ft_text_length": 35775,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC8483299)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "36621750",
      "title": "Mining for equitable health: Assessing the impact of missing data in electronic health records.",
      "abstract": "Electronic health records (EHR) are collected as a routine part of healthcare delivery, and have great potential to be utilized to improve patient health outcomes. They contain multiple years of health information to be leveraged for risk prediction, disease detection, and treatment evaluation. However, they do not have a consistent, standardized format across institutions, particularly in the United States, and can present significant analytical challenges- they contain multi-scale data from heterogeneous domains and include both structured and unstructured data. Data for individual patients are collected at irregular time intervals and with varying frequencies. In addition to the analytical challenges, EHR can reflect inequity- patients belonging to different groups will have differing amounts of data in their health records. Many of these issues can contribute to biased data collection. The consequence is that the data for under-served groups may be less informative partly due to more fragmented care, which can be viewed as a type of missing data problem. For EHR data in this complex form, there is currently no framework for introducing realistic missing values. There has also been little to no work in assessing the impact of missing data in EHR. In this work, we first introduce a terminology to define three levels of EHR data and then propose a novel framework for simulating realistic missing data scenarios in EHR to adequately assess their impact on predictive modeling. We incorporate the use of a medical knowledge graph to capture dependencies between medical events to create a more realistic missing data framework. In an intensive care unit setting, we found that missing data have greater negative impact on the performance of disease prediction models in groups that tend to have less access to healthcare, or seek less healthcare. We also found that the impact of missing data on disease prediction models is stronger when using the knowledge graph framework to introduce realistic missing values as opposed to random event removal.",
      "journal": "Journal of biomedical informatics",
      "year": "2023",
      "doi": "10.1016/j.jbi.2022.104269",
      "authors": "Getzen Emily et al.",
      "keywords": "Electronic health records; Fairness; Health disparities; Knowledge graph; Missing data",
      "mesh_terms": "Humans; United States; Electronic Health Records; Delivery of Health Care; Intensive Care Units",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36621750/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10391553",
      "ft_text_length": 2070,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10391553)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37306245",
      "title": "Risk of cancer versus risk of cancer diagnosis? Accounting for diagnostic bias in predictions of breast cancer risk by race and ethnicity.",
      "abstract": "OBJECTIVES: Cancer risk prediction may be subject to detection bias if utilization of screening is related to cancer risk factors. We examine detection bias when predicting breast cancer risk by race/ethnicity. METHODS: We used screening and diagnosis histories from the Breast Cancer Surveillance Consortium to estimate risk of breast cancer onset and calculated relative risk of onset and diagnosis for each racial/ethnic group compared with non-Hispanic White women. RESULTS: Of 104,073 women aged 40-54 receiving their first screening mammogram at a Breast Cancer Surveillance Consortium facility between 2000 and 2018, 10.2% (n\u2009=\u200910,634) identified as Asian, 10.9% (n\u2009=\u200911,292) as Hispanic, and 8.4% (n\u2009=\u20098719) as non-Hispanic Black. Hispanic and non-Hispanic Black women had slightly lower screening frequencies but biopsy rates following a positive mammogram were similar across groups. Risk of cancer diagnosis was similar for non-Hispanic Black and White women (relative risk vs non-Hispanic White\u2009=\u20090.90, 95% CI 0.65 to 1.14) but was lower for Asian (relative risk\u2009=\u20090.70, 95% CI 0.56 to 0.97) and Hispanic women (relative risk\u2009=\u20090.82, 95% CI 0.62 to 1.08). Relative risks of disease onset were 0.78 (95% CI 0.68 to 0.88), 0.70 (95% CI 0.59 to 0.83), and 0.95 (95% CI 0.84 to 1.09) for Asian, Hispanic, and non-Hispanic Black women, respectively. CONCLUSIONS: Racial/ethnic differences in mammography and biopsy utilization did not induce substantial detection bias; relative risks of disease onset were similar to or modestly different than relative risks of diagnosis. Asian and Hispanic women have lower risks of developing breast cancer than non-Hispanic Black and White women, who have similar risks.",
      "journal": "Journal of medical screening",
      "year": "2023",
      "doi": "10.1177/09691413231180028",
      "authors": "Gard Charlotte C et al.",
      "keywords": "Breast Cancer Surveillance Consortium; Detection bias; cancer screening; disease surveillance; latent model; risk prediction",
      "mesh_terms": "Female; Humans; Breast Neoplasms; Early Detection of Cancer; Ethnicity; Risk Factors; White People; Adult; Middle Aged; Asian; Hispanic or Latino; Black or African American",
      "pub_types": "Comparative Study; Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37306245/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10713859",
      "ft_text_length": 1630,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10713859)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37393863",
      "title": "Disparities in adherence and emergency department utilization among people with epilepsy: A machine learning approach.",
      "abstract": "PURPOSE: We used a machine learning approach to identify the combinations of factors that contribute to lower adherence and high emergency department (ED) utilization. METHODS: Using Medicaid claims, we identified adherence to anti-seizure medications and the number of ED visits for people with epilepsy in a 2-year follow up period. We used three years of baseline data to identify demographics, disease severity and management, comorbidities, and county-level social factors. Using Classification and Regression Tree (CART) and random forest analyses we identified combinations of baseline factors that predicted lower adherence and ED visits. We further stratified these models by race and ethnicity. RESULTS: From 52,175 people with epilepsy, the CART model identified developmental disabilities, age, race and ethnicity, and utilization as top predictors of adherence. When stratified by race and ethnicity, there was variation in the combinations of comorbidities including developmental disabilities, hypertension, and psychiatric comorbidities. Our CART model for ED utilization included a primary split among those with previous injuries, followed by anxiety and mood disorders, headache, back problems, and urinary tract infections. When stratified by race and ethnicity we saw that for Black individuals headache was a top predictor of future ED utilization although this did not appear in other racial and ethnic groups. CONCLUSIONS: ASM adherence differed by race and ethnicity, with different combinations of comorbidities predicting lower adherence across racial and ethnic groups. While there were not differences in ED use across races and ethnicity, we observed different combinations of comorbidities that predicted high ED utilization.",
      "journal": "Seizure",
      "year": "2023",
      "doi": "10.1016/j.seizure.2023.06.021",
      "authors": "Bensken Wyatt P et al.",
      "keywords": "Adherence; Disparities; Emergency department utilization; Machine Learning",
      "mesh_terms": "United States; Humans; Ethnicity; Emergency Service, Hospital; Machine Learning; Epilepsy; Headache; Healthcare Disparities",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37393863/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10528555",
      "ft_text_length": 1749,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10528555)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37418377",
      "title": "Cite-seeing and reviewing: A study on citation bias in peer review.",
      "abstract": "Citations play an important role in researchers' careers as a key factor in evaluation of scientific impact. Many anecdotes advice authors to exploit this fact and cite prospective reviewers to try obtaining a more positive evaluation for their submission. In this work, we investigate if such a citation bias actually exists: Does the citation of a reviewer's own work in a submission cause them to be positively biased towards the submission? In conjunction with the review process of two flagship conferences in machine learning and algorithmic economics, we execute an observational study to test for citation bias in peer review. In our analysis, we carefully account for various confounding factors such as paper quality and reviewer expertise, and apply different modeling techniques to alleviate concerns regarding the model mismatch. Overall, our analysis involves 1,314 papers and 1,717 reviewers and detects citation bias in both venues we consider. In terms of the effect size, by citing a reviewer's work, a submission has a non-trivial chance of getting a higher score from the reviewer: an expected increase in the score is approximately 0.23 on a 5-point Likert item. For reference, a one-point increase of a score by a single reviewer improves the position of a submission by 11% on average.",
      "journal": "PloS one",
      "year": "2023",
      "doi": "10.1371/journal.pone.0283980",
      "authors": "Stelmakh Ivan et al.",
      "keywords": "",
      "mesh_terms": "Humans; Prospective Studies; Peer Review; Bias; Research Personnel; Machine Learning; Peer Review, Research",
      "pub_types": "Observational Study; Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37418377/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10328240",
      "ft_text_length": 49221,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10328240)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37610746",
      "title": "Development of a Machine Learning-Based Prescriptive Tool to Address Racial Disparities in Access to Care After Penetrating Trauma.",
      "abstract": "IMPORTANCE: The use of artificial intelligence (AI) in clinical medicine risks perpetuating existing bias in care, such as disparities in access to postinjury rehabilitation services. OBJECTIVE: To leverage a novel, interpretable AI-based technology to uncover racial disparities in access to postinjury rehabilitation care and create an AI-based prescriptive tool to address these disparities. DESIGN, SETTING, AND PARTICIPANTS: This cohort study used data from the 2010-2016 American College of Surgeons Trauma Quality Improvement Program database for Black and White patients with a penetrating mechanism of injury. An interpretable AI methodology called optimal classification trees (OCTs) was applied in an 80:20 derivation/validation split to predict discharge disposition (home vs postacute care [PAC]). The interpretable nature of OCTs allowed for examination of the AI logic to identify racial disparities. A prescriptive mixed-integer optimization model using age, injury, and gender data was allowed to \"fairness-flip\" the recommended discharge destination for a subset of patients while minimizing the ratio of imbalance between Black and White patients. Three OCTs were developed to predict discharge disposition: the first 2 trees used unadjusted data (one without and one with the race variable), and the third tree used fairness-adjusted data. MAIN OUTCOMES AND MEASURES: Disparities and the discriminative performance (C statistic) were compared among fairness-adjusted and unadjusted OCTs. RESULTS: A total of 52\u202f468 patients were included; the median (IQR) age was 29 (22-40) years, 46\u202f189 patients (88.0%) were male, 31\u202f470 (60.0%) were Black, and 20\u202f998 (40.0%) were White. A total of 3800 Black patients (12.1%) were discharged to PAC, compared with 4504 White patients (21.5%; P\u2009<\u2009.001). Examining the AI logic uncovered significant disparities in PAC discharge destination access, with race playing the second most important role. The prescriptive fairness adjustment recommended flipping the discharge destination of 4.5% of the patients, with the performance of the adjusted model increasing from a C statistic of 0.79 to 0.87. After fairness adjustment, disparities disappeared, and a similar percentage of Black and White patients (15.8% vs 15.8%; P\u2009=\u2009.87) had a recommended discharge to PAC. CONCLUSIONS AND RELEVANCE: In this study, we developed an accurate, machine learning-based, fairness-adjusted model that can identify barriers to discharge to postacute care. Instead of accidentally encoding bias, interpretable AI methodologies are powerful tools to diagnose and remedy system-related bias in care, such as disparities in access to postinjury rehabilitation care.",
      "journal": "JAMA surgery",
      "year": "2023",
      "doi": "10.1001/jamasurg.2023.2293",
      "authors": "Gebran Anthony et al.",
      "keywords": "",
      "mesh_terms": "Adult; Female; Humans; Male; Middle Aged; Black or African American; Health Services Accessibility; Healthcare Disparities; Machine Learning; Patient Discharge; United States; Wounds, Penetrating; White",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37610746/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10448365",
      "ft_text_length": 219,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10448365)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "37889504",
      "title": "Identifying, Understanding, and Addressing Disparities in Glaucoma Care in the United States.",
      "abstract": "UNLABELLED: Glaucoma is the leading cause of irreversible blindness worldwide, currently affecting around 80\u00a0million people. Glaucoma prevalence is rapidly rising in the United States due to an aging population. Despite recent advances in the diagnosis and treatment of glaucoma, significant disparities persist in disease detection, management, and outcomes among the diverse patient populations of the United States. Research on disparities is critical to identifying, understanding, and addressing societal and healthcare inequalities. Disparities research is especially important and impactful in the context of irreversible diseases such as glaucoma, where earlier detection and intervention are the primary approach to improving patient outcomes. In this article, we first review recent studies identifying disparities in glaucoma care that affect patient populations based on race, age, and gender. We then review studies elucidating and furthering our understanding of modifiable factors that contribute to these inequities, including socioeconomic status (particularly age and education), insurance product, and geographic region. Finally, we present work proposing potential strategies addressing disparities in glaucoma care, including teleophthalmology and artificial intelligence. We also discuss the presence of non-modifiable factors that contribute to differences in glaucoma burden and can confound the detection of glaucoma disparities. TRANSLATIONAL RELEVANCE: By recognizing underlying causes and proposing potential solutions, healthcare providers, policymakers, and other stakeholders can work collaboratively to reduce the burden of glaucoma and improve visual health and clinical outcomes in vulnerable patient populations.",
      "journal": "Translational vision science & technology",
      "year": "2023",
      "doi": "10.1167/tvst.12.10.18",
      "authors": "Davuluru Shaili S et al.",
      "keywords": "",
      "mesh_terms": "Humans; United States; Aged; Artificial Intelligence; Ophthalmology; Telemedicine; Glaucoma; Healthcare Disparities",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37889504/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10617640",
      "ft_text_length": 38416,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10617640)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38076216",
      "title": "Racial Disparities Among Predicted Bronchopulmonary Dysplasia Risk Outcomes in Premature Infants Born <30 Weeks Gestation.",
      "abstract": "BACKGROUND AND OBJECTIVE: There is extensive literature to support eliminating race-based risk stratification. The National Institute of Child Health and Human Development (NICHD) calculator, used to predict risk of bronchopulmonary dysplasia (BPD), includes race as a variable. We sought to investigate how utilizing race in determination of risk for BPD may lead to inequitable care. METHODS: The study included a retrospective cohort of infants born <30 weeks gestation between January 2016 and February 2022. The primary outcome was the difference in predictive risk of BPD for non-Hispanic Black compared to non-Hispanic White infants. The secondary outcome was the disparity in theoretical administration of post-natal corticosteroids when the calculator was applied to the cohort. Analysis included paired T-tests and Chi-Square. RESULTS: Of the 273 infants studied, 154 were non-Hispanic Black (56%). There was no difference between the groups in gestation or respiratory support on day of life (DOL) 14 or 28. The predicted risk of moderate or severe BPD in non-Hispanic White babies was greater than non-Hispanic Black babies on both DOL 14 and 28 (p<0.01). When applied retrospectively to the cohort, the calculator resulted in differences in corticosteroid administration (risk >40%-non-Hispanic White 51.3% vs. non-Hispanic Black 35.7%, p=0.010; risk >50%-non-Hispanic White 42.9% vs. non-Hispanic Black 29.9%, p=0.026). CONCLUSION: When applied to our study cohort, the calculator resulted in a reduction in the predicted risk of BPD in non-Hispanic Black infants. If utilized to guide treatment, the calculator can potentially lead to disparities in care for non-Hispanic Black infants.",
      "journal": "Health equity",
      "year": "2023",
      "doi": "10.1089/heq.2023.0042",
      "authors": "Patel Priyanka et al.",
      "keywords": "bronchopulmonary dysplasia; disparities; mortality; pre-maturity; risk prediction",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38076216/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10698790",
      "ft_text_length": 17902,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10698790)",
      "ft_reason": "No AI/ML component in full text"
    },
    {
      "pmid": "38147867",
      "title": "Enriching Real-world Data with Social Determinants of Health for Health Outcomes and Health Equity: Successes, Challenges, and Opportunities.",
      "abstract": "OBJECTIVE: To summarize the recent methods and applications that leverage real-world data such as electronic health records (EHRs) with social determinants of health (SDoH) for public and population health and health equity and identify successes, challenges, and possible solutions. METHODS: In this opinion review, grounded on a social-ecological-model-based conceptual framework, we surveyed data sources and recent informatics approaches that enable leveraging SDoH along with real-world data to support public health and clinical health applications including helping design public health intervention, enhancing risk stratification, and enabling the prediction of unmet social needs. RESULTS: Besides summarizing data sources, we identified gaps in capturing SDoH data in existing EHR systems and opportunities to leverage informatics approaches to collect SDoH information either from structured and unstructured EHR data or through linking with public surveys and environmental data. We also surveyed recently developed ontologies for standardizing SDoH information and approaches that incorporate SDoH for disease risk stratification, public health crisis prediction, and development of tailored interventions. CONCLUSIONS: To enable effective public health and clinical applications using real-world data with SDoH, it is necessary to develop both non-technical solutions involving incentives, policies, and training as well as technical solutions such as novel social risk management tools that are integrated into clinical workflow. Ultimately, SDoH-powered social risk management, disease risk prediction, and development of SDoH tailored interventions for disease prevention and management have the potential to improve population health, reduce disparities, and improve health equity.",
      "journal": "Yearbook of medical informatics",
      "year": "2023",
      "doi": "10.1055/s-0043-1768732",
      "authors": "He Zhe et al.",
      "keywords": "",
      "mesh_terms": "Humans; Health Equity; Social Determinants of Health; Electronic Health Records; Population Health; Outcome Assessment, Health Care",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38147867/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10751148",
      "ft_text_length": 36650,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10751148)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38216691",
      "title": "Modelling dataset bias in machine-learned theories of economic decision-making.",
      "abstract": "Normative and descriptive models have long vied to explain and predict human risky choices, such as those between goods or gambles. A recent study reported the discovery of a new, more accurate model of human decision-making by training neural networks on a new online large-scale dataset, choices13k. Here we systematically analyse the relationships between several models and datasets using machine-learning methods and find evidence for dataset bias. Because participants' choices in stochastically dominated gambles were consistently skewed towards equipreference in the choices13k dataset, we hypothesized that this reflected increased decision noise. Indeed, a probabilistic generative model adding structured decision noise to a neural network trained on data from a laboratory study transferred best, that is, outperformed all models apart from those trained on choices13k. We conclude that a careful combination of theory and data analysis is still required to understand the complex interactions of machine-learning models and data of human risky choices.",
      "journal": "Nature human behaviour",
      "year": "2024",
      "doi": "10.1038/s41562-023-01784-6",
      "authors": "Thomas Tobias et al.",
      "keywords": "",
      "mesh_terms": "Humans; Machine Learning; Decision Making; Neural Networks, Computer; Risk-Taking; Datasets as Topic; Models, Psychological; Choice Behavior; Adult; Bias",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38216691/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11045447",
      "ft_text_length": 70328,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11045447)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38326853",
      "title": "Modification of the PM2.5- and extreme heat-mortality relationships by historical redlining: a case-crossover study in thirteen U.S. states.",
      "abstract": "BACKGROUND: Redlining has been associated with worse health outcomes and various environmental disparities, separately, but little is known of the interaction between these two factors, if any. We aimed to estimate whether living in a historically-redlined area modifies the effects of exposures to ambient PM2.5 and extreme heat on mortality by non-external causes. METHODS: We merged 8,884,733 adult mortality records from thirteen state departments of public health with scanned and georeferenced Home Owners Loan Corporation (HOLC) maps from the University of Richmond, daily average PM2.5 from a sophisticated prediction model on a 1-km grid, and daily temperature and vapor pressure from the Daymet V4 1-km grid. A case-crossover approach was used to assess modification of the effects of ambient PM2.5 and extreme heat exposures by redlining and control for all fixed and slow-varying factors by design. Multiple moving averages of PM2.5 and duration-aware analyses of extreme heat were used to assess the most vulnerable time windows. RESULTS: We found significant statistical interactions between living in a redlined area and exposures to both ambient PM2.5 and extreme heat. Individuals who lived in redlined areas had an interaction odds ratio for mortality of 1.0093 (95% confidence interval [CI]: 1.0084, 1.0101) for each 10\u00a0\u00b5g\u00a0m-3 increase in same-day ambient PM2.5 compared to individuals who did not live in redlined areas. For extreme heat, the interaction odds ratio was 1.0218 (95% CI 1.0031, 1.0408). CONCLUSIONS: Living in areas that were historically-redlined in the 1930's increases the effects of exposures to both PM2.5 and extreme heat on mortality by non-external causes, suggesting that interventions to reduce environmental health disparities can be more effective by also considering the social context of an area and how to reduce disparities there. Further study is required to ascertain the specific pathways through which this effect modification operates and to develop interventions that can contribute to health equity for individuals living in these areas.",
      "journal": "Environmental health : a global access science source",
      "year": "2024",
      "doi": "10.1186/s12940-024-01055-5",
      "authors": "Castro Edgar et al.",
      "keywords": "Air pollution; Effect modification; Environmental justice; Extreme heat; Redlining; Temperature",
      "mesh_terms": "Humans; Adult; Cross-Over Studies; Extreme Heat; Particulate Matter; Air Pollutants",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38326853/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10851491",
      "ft_text_length": 38574,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10851491)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38349846",
      "title": "Estimation of racial and language disparities in pediatric emergency department triage using statistical modeling and natural language processing.",
      "abstract": "OBJECTIVES: The study aims to assess racial and language disparities in pediatric emergency department (ED) triage using analytical techniques and provide insights into the extent and nature of the disparities in the ED setting. MATERIALS AND METHODS: The study analyzed a cross-sectional dataset encompassing ED visits from January 2019 to April 2021. The study utilized analytical techniques, including K-mean clustering (KNN), multivariate adaptive regression splines (MARS), and natural language processing (NLP) embedding. NLP embedding and KNN were employed to handle the chief complaints and categorize them into clusters, while the MARS was used to identify significant interactions among the clinical features. The study also explored important variables, including age-adjusted vital signs. Multiple logistic regression models with varying specifications were developed to assess the robustness of analysis results. RESULTS: The study consistently found that non-White children, especially African American (AA) and Hispanic, were often under-triaged, with AA children having >2 times higher odds of receiving lower acuity scores compared to White children. While the results are generally consistent, incorporating relevant variables modified the results for specific patient groups (eg, Asians). DISCUSSION: By employing a comprehensive analysis methodology, the study checked the robustness of the analysis results on racial and language disparities in pediatric ED triage. The study also recognized the significance of analytical techniques in assessing pediatric health conditions and analyzing disparities. CONCLUSION: The study's findings highlight the significant need for equal and fair assessment and treatment in the pediatric ED, regardless of their patients' race and language.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2024",
      "doi": "10.1093/jamia/ocae018",
      "authors": "Lee Seung-Yup Joshua et al.",
      "keywords": "language triage disparity; natural language processing; pediatric emergency department; racial triage disparity",
      "mesh_terms": "Child; Humans; Cross-Sectional Studies; Emergency Service, Hospital; Healthcare Disparities; Hispanic or Latino; Natural Language Processing; Retrospective Studies; Triage; Black or African American; Models, Statistical",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38349846/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10990499",
      "ft_text_length": 1804,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC10990499)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "38383145",
      "title": "Modeling chronic disease risk across equity factors using a population-based prediction model: the Chronic Disease Population Risk Tool (CDPoRT).",
      "abstract": "BACKGROUND: Predicting chronic disease incidence at a population level can help inform overall future chronic disease burden and opportunities for prevention. This study aimed to estimate the future burden of chronic disease in Ontario, Canada, using a population-level risk prediction algorithm and model interventions for equity-deserving groups who experience barriers to services and resources due to disadvantages and discrimination. METHODS: The validated Chronic Disease Population Risk Tool (CDPoRT) estimates the 10-year risk and incidence of major chronic diseases. CDPoRT was applied to data from the 2017/2018 Canadian Community Health Survey to predict baseline 10-year chronic disease estimates to 2027/2028 in the adult population of Ontario, Canada, and among equity-deserving groups. CDPoRT was used to model prevention scenarios of 2% and 5% risk reductions over 10 years targeting high-risk equity-deserving groups. RESULTS: Baseline chronic disease risk was highest among those with less than secondary school education (37.5%), severe food insecurity (19.5%), low income (21.2%) and extreme workplace stress (15.0%). CDPoRT predicted 1.42\u2009million new chronic disease cases in Ontario from 2017/2018 to 2027/2028. Reducing chronic disease risk by 5% prevented 1500 cases among those with less than secondary school education, prevented 14\u2009900 cases among those with low household income and prevented 2800 cases among food-insecure populations. Large reductions of 57 100 cases were found by applying a 5% risk reduction in individuals with quite a bit workplace stress. CONCLUSION: Considerable reduction in chronic disease cases was predicted across equity-defined scenarios, suggesting the need for prevention strategies that consider upstream determinants affecting chronic disease risk.",
      "journal": "Journal of epidemiology and community health",
      "year": "2024",
      "doi": "10.1136/jech-2023-221080",
      "authors": "Chen Kitty et al.",
      "keywords": "CHRONIC DI; DECISION SUPPORT TECHNIQUES; EPIDEMIOLOGY; HEALTH IMPACT ASSESSMENT; HEALTHCARE DISPARITIES",
      "mesh_terms": "Adult; Humans; Risk Factors; Chronic Disease; Ontario; Poverty; Occupational Stress",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38383145/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11041567",
      "ft_text_length": 26882,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11041567)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39178002",
      "title": "Using AI and Social Media to Understand Health Disparities for Transgender Cancer Care.",
      "abstract": "This qualitative study used an artificial intelligence (AI) large language model and social media to investigate challenges encountered by transgender individuals during breast and gynecological cancer care.",
      "journal": "JAMA network open",
      "year": "2024",
      "doi": "10.1001/jamanetworkopen.2024.29792",
      "authors": "Annan Augustine et al.",
      "keywords": "",
      "mesh_terms": "Humans; Social Media; Transgender Persons; Neoplasms; Female; Male; Healthcare Disparities; Artificial Intelligence",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39178002/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11344235",
      "ft_text_length": 9188,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11344235)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39283333",
      "title": "Racial, ethnic, and sex bias in large language model opioid recommendations for pain management.",
      "abstract": "Understanding how large language model (LLM) recommendations vary with patient race/ethnicity provides insight into how LLMs may counter or compound bias in opioid prescription. Forty real-world patient cases were sourced from the MIMIC-IV Note dataset with chief complaints of abdominal pain, back pain, headache, or musculoskeletal pain and amended to include all combinations of race/ethnicity and sex. Large language models were instructed to provide a subjective pain rating and comprehensive pain management recommendation. Univariate analyses were performed to evaluate the association between racial/ethnic group or sex and the specified outcome measures-subjective pain rating, opioid name, order, and dosage recommendations-suggested by 2 LLMs (GPT-4 and Gemini). Four hundred eighty real-world patient cases were provided to each LLM, and responses included pharmacologic and nonpharmacologic interventions. Tramadol was the most recommended weak opioid in 55.4% of cases, while oxycodone was the most frequently recommended strong opioid in 33.2% of cases. Relative to GPT-4, Gemini was more likely to rate a patient's pain as \"severe\" (OR: 0.57 95% CI: [0.54, 0.60]; P < 0.001), recommend strong opioids (OR: 2.05 95% CI: [1.59, 2.66]; P < 0.001), and recommend opioids later (OR: 1.41 95% CI: [1.22, 1.62]; P < 0.001). Race/ethnicity and sex did not influence LLM recommendations. This study suggests that LLMs do not preferentially recommend opioid treatment for one group over another. Given that prior research shows race-based disparities in pain perception and treatment by healthcare providers, LLMs may offer physicians a helpful tool to guide their pain management and ensure equitable treatment across patient groups.",
      "journal": "Pain",
      "year": "2025",
      "doi": "10.1097/j.pain.0000000000003388",
      "authors": "Young Cameron C et al.",
      "keywords": "",
      "mesh_terms": "Adult; Aged; Female; Humans; Male; Middle Aged; Analgesics, Opioid; Ethnicity; Large Language Models; Pain; Pain Management; Racial Groups; Sexism",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39283333/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12042288",
      "ft_text_length": 1746,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12042288)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39314265",
      "title": "TARGETING UNDERREPRESENTED POPULATIONS IN PRECISION MEDICINE: A FEDERATED TRANSFER LEARNING APPROACH.",
      "abstract": "The limited representation of minorities and disadvantaged populations in large-scale clinical and genomics research poses a significant barrier to translating precision medicine research into practice. Prediction models are likely to underperform in underrepresented populations due to heterogeneity across populations, thereby exacerbating known health disparities. To address this issue, we propose FETA, a two-way data integration method that leverages a federated transfer learning approach to integrate heterogeneous data from diverse populations and multiple healthcare institutions, with a focus on a target population of interest having limited sample sizes. We show that FETA achieves performance comparable to the pooled analysis, where individual-level data is shared across institutions, with only a small number of communications across participating sites. Our theoretical analysis and simulation study demonstrate how FETA's estimation accuracy is influenced by communication budgets, privacy restrictions, and heterogeneity across populations. We apply FETA to multisite data from the electronic Medical Records and Genomics (eMERGE) Network to construct genetic risk prediction models for extreme obesity. Compared to models trained using target data only, source data only, and all data without accounting for population-level differences, FETA shows superior predictive performance. FETA has the potential to improve estimation and prediction accuracy in underrepresented populations and reduce the gap in model performance across populations.",
      "journal": "The annals of applied statistics",
      "year": "2023",
      "doi": "10.1214/23-AOAS1747",
      "authors": "Li By Sai et al.",
      "keywords": "Federated learning; health equity; precision medicine; risk prediction; transfer learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39314265/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11417462",
      "ft_text_length": 1563,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11417462)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39561770",
      "title": "The PRIMED Consortium: Reducing disparities in polygenic risk assessment.",
      "abstract": "By improving disease risk prediction, polygenic risk scores (PRSs) could have a significant impact on health promotion and disease prevention. Due to the historical oversampling of populations with European ancestry for genome-wide association studies, PRSs perform less well in other, understudied populations, leading to concerns that clinical use in their current forms could widen health care disparities. The PRIMED Consortium was established to develop methods to improve the performance of PRSs in global populations and individuals of diverse genetic ancestry. To this end, PRIMED is aggregating and harmonizing multiple phenotype and genotype datasets on AnVIL, an interoperable secure cloud-based platform, to perform individual- and summary-level analyses using population and statistical genetics approaches. Study sites, the coordinating center, and representatives from the NIH work alongside other NHGRI and global consortia to achieve these goals. PRIMED is also evaluating ethical and social implications of PRS implementation and investigating the joint modeling of social determinants of health and PRS in computing disease risk. The phenotypes of interest are primarily cardiometabolic diseases and cancer, the leading causes of death and disability worldwide. Early deliverables of the consortium include methods for data sharing on AnVIL, development of a common data model to harmonize phenotype and genotype data from cohort studies as well as electronic health records, adaptation of recent guidelines for population descriptors to global cohorts, and sharing of PRS methods/tools. As a multisite collaboration, PRIMED aims to foster equity in the development and use of polygenic risk assessment.",
      "journal": "American journal of human genetics",
      "year": "2024",
      "doi": "10.1016/j.ajhg.2024.10.010",
      "authors": "Kullo Iftikhar J et al.",
      "keywords": "diversity; equity; polygenic risk score",
      "mesh_terms": "Humans; Genetic Predisposition to Disease; Genome-Wide Association Study; Genotype; Multifactorial Inheritance; Neoplasms; Phenotype; Risk Assessment; Risk Factors",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39561770/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11639095",
      "ft_text_length": 42593,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11639095)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39819978",
      "title": "Analyzing Geospatial and Socioeconomic Disparities in Breast Cancer Screening Among Populations in the United States: Machine Learning Approach.",
      "abstract": "BACKGROUND: Breast cancer screening plays a pivotal role in early detection and subsequent effective management of the disease, impacting patient outcomes and survival rates. OBJECTIVE: This study aims to assess breast cancer screening rates nationwide in the United States and investigate the impact of social determinants of health on these screening rates. METHODS: Data on mammography screening at the census tract level for 2018 and 2020 were collected from the Behavioral Risk Factor Surveillance System. We developed a large-scale dataset of social determinants of health, comprising 13 variables for 72,337 census tracts. Spatial analysis employing Getis-Ord Gi statistics was used to identify clusters of high and low breast cancer screening rates. To evaluate the influence of these social determinants, we implemented a random forest model, with the aim of comparing its performance to linear regression and support vector machine models. The models were evaluated using R2 and root mean squared error metrics. Shapley Additive Explanations values were subsequently used to assess the significance of variables and direction of their influence. RESULTS: Geospatial analysis revealed elevated screening rates in the eastern and northern United States, while central and midwestern regions exhibited lower rates. The random forest model demonstrated superior performance, with an R2=64.53 and root mean squared error of 2.06, compared to linear regression and support vector machine models. Shapley Additive Explanations values indicated that the percentage of the Black population, the number of mammography facilities within a 10-mile radius, and the percentage of the population with at least a bachelor's degree were the most influential variables, all positively associated with mammography screening rates. CONCLUSIONS: These findings underscore the significance of social determinants and the accessibility of mammography services in explaining the variability of breast cancer screening rates in the United States, emphasizing the need for targeted policy interventions in areas with relatively lower screening rates.",
      "journal": "JMIR cancer",
      "year": "2025",
      "doi": "10.2196/59882",
      "authors": "Hashtarkhani Soheil et al.",
      "keywords": "breast neoplasms; geographic information systems; machine learning; mammography; social determinants of health",
      "mesh_terms": "Humans; Breast Neoplasms; Female; United States; Early Detection of Cancer; Mammography; Machine Learning; Socioeconomic Factors; Middle Aged; Healthcare Disparities; Social Determinants of Health; Spatial Analysis; Behavioral Risk Factor Surveillance System; Aged; Mass Screening; Socioeconomic Disparities in Health",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39819978/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11756836",
      "ft_text_length": 26662,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11756836)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39964728",
      "title": "Toward equitable major histocompatibility complex binding predictions.",
      "abstract": "Deep learning tools that predict peptide binding by major histocompatibility complex (MHC) proteins play an essential role in developing personalized cancer immunotherapies and vaccines. In order to ensure equitable health outcomes from their application, MHC binding prediction methods must work well across the vast landscape of MHC alleles observed across human populations. Here, we show that there are alarming disparities across individuals in different racial and ethnic groups in how much binding data are associated with their MHC alleles. We introduce a machine learning framework to assess the impact of this data imbalance for predicting binding for any given MHC allele, and apply it to develop a state-of-the-art MHC binding prediction model that additionally provides per-allele performance estimates. We demonstrate that our MHC binding model successfully mitigates much of the data disparities observed across racial groups. To address remaining inequities, we devise an algorithmic strategy for targeted data collection. Our work lays the foundation for further development of equitable MHC binding models for use in personalized immunotherapies.",
      "journal": "Proceedings of the National Academy of Sciences of the United States of America",
      "year": "2025",
      "doi": "10.1073/pnas.2405106122",
      "authors": "Glynn Eric et al.",
      "keywords": "cancer immunotherapies; deep learning; health equity in precision oncology; neoantigen prediction; predicting major histocompatibility complex (MHC) binding",
      "mesh_terms": "Humans; Major Histocompatibility Complex; Protein Binding; Alleles; Deep Learning; Machine Learning; Peptides; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39964728/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11874272",
      "ft_text_length": 55851,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11874272)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "39980817",
      "title": "Scientific Mapping of Global Research on Health Equity by 2023: A Bibliometric Study.",
      "abstract": "BACKGROUND AND AIMS: There has been a growing focus on health equity (HE) within health systems. Studies in HE have revealed significant domains that have been the focus of attention for academics and research institutions. This research aimed to provide an in-depth perspective on the research efforts conducted around the world concerning HE. METHODS: In this bibliometric study, we used co-word analysis to map HE studies indexed in Scopus, Web of Science, ScienceDirect, and PubMed by the end of 2023. A comprehensive search was carried out in the databases, and the data were analyzed employing VOSviewer software. Along with analyzing publication trends, thematic clusters, and emerging topics in HE were identified. RESULTS: The compound annual growth rate of the HE documents in PubMed, Scopus, ScienceDirect, and Web of Science were 0.157, 0.173, 0.453, and 0.317, respectively. Topic clusters of HE keywords in the period preceding the COVID-19 pandemic were \"Health care,\" \"Health economics,\" \"Race and ethnicity,\" \"Social determinants of health,\" and \"Age and gender.\" The analyses related to the time following the onset of the COVID-19 pandemic resulted in the identification of six topic clusters, namely \"Health workforce,\" \"Risk factors,\" \"Maternal and child health,\" \"COVID-19,\" \"Cancer,\" and \"Mental health.\" Moreover, \"Artificial intelligence,\" \"Racial disparity,\" \"Machine learning,\" and \"COVID-19,\" were four key emerging topics of HE pertinent to the post-COVID-19 period. CONCLUSION: In recent years, there has been a significant increase in research focused on HE. The focus of research in HE has shifted to an emphasis on various diseases and their risk factors. Emerging topics identified in this study represent significant areas of interest as novel research domains, particularly within low- and middle-income countries.",
      "journal": "Health science reports",
      "year": "2025",
      "doi": "10.1002/hsr2.70478",
      "authors": "Sarikhani Yaser et al.",
      "keywords": "bibliometric analysis; bibliometrics; health equity; research",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39980817/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11839487",
      "ft_text_length": 26567,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC11839487)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40417471",
      "title": "Toward Automated Detection of Biased Social Signals from the Content of Clinical Conversations.",
      "abstract": "Implicit bias can impede patient-provider interactions and lead to inequities in care. Raising awareness is key to reducing such bias, but its manifestations in the social dynamics of patient-provider communication are difficult to detect. In this study, we used automated speech recognition (ASR) and natural language processing (NLP) to identify social signals in patient-provider interactions. We built an automated pipeline to predict social signals from audio recordings of 782 primary care visits that achieved 90.1% average accuracy across codes, and exhibited fairness in its predictions for white and non-white patients. Applying this pipeline, we identified statistically significant differences in provider communication behavior toward white versus non-white patients. In particular, providers expressed more patient-centered behaviors towards white patients including more warmth, engagement, and attentiveness. Our study underscores the potential of automated tools in identifying subtle communication signals that may be linked with bias and impact healthcare quality and equity.",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "year": "2024",
      "doi": "",
      "authors": "Chen Feng et al.",
      "keywords": "",
      "mesh_terms": "Humans; Natural Language Processing; Physician-Patient Relations; Communication; Speech Recognition Software; Primary Health Care; Racism",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40417471/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12099337",
      "ft_text_length": 1103,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12099337)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40533324",
      "title": "Toward equitable biomarkers of aging: rethinking methylation clocks.",
      "abstract": "DNA methylation clocks, which measure biological age by analyzing age-related DNA methylation patterns, offer powerful biomarkers of aging. But as a recent preprint highlights, current models underperform in diverse populations. The next generation of clocks must prioritize equity to avoid reinforcing disparities in precision aging and disease risk prediction.",
      "journal": "Trends in genetics : TIG",
      "year": "2025",
      "doi": "10.1016/j.tig.2025.06.001",
      "authors": "Wu Selina et al.",
      "keywords": "aging; methylation clocks; multi-omics",
      "mesh_terms": "Humans; Aging; Biomarkers; DNA Methylation; Epigenesis, Genetic",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40533324/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12221226",
      "ft_text_length": 362,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12221226)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40561471",
      "title": "Using a Multilingual AI Care Agent to Reduce Disparities in Colorectal Cancer Screening for Higher Fecal Immunochemical Test Adoption Among Spanish-Speaking Patients: Retrospective Analysis.",
      "abstract": "BACKGROUND: Colorectal cancer (CRC) screening rates remain disproportionately low among Hispanic and Latino populations compared to non-Hispanic White populations. While artificial intelligence (AI) shows promise in health care delivery, concerns exist that AI-based interventions may disadvantage non-English-speaking populations due to biases in development and deployment. OBJECTIVE: This study aimed to evaluate the effectiveness of a multilingual AI care agent in engaging Spanish-speaking patients for CRC screening compared to that with English-speaking patients. METHODS: This retrospective analysis examined an AI-powered outreach initiative at WellSpan Health in Pennsylvania and Maryland during September 2024. The study included 1878 patients (517 Spanish-speaking, 1361 English-speaking) eligible for CRC screening who lacked active web-based health profiles. A multilingual AI conversational agent conducted personalized telephone calls in the patient's preferred language to provide education about CRC screening and facilitate fecal immunochemical test (FIT) kit requests. The primary outcome was the FIT test opt-in rate, with secondary outcomes including connect rates and call duration. Statistical analysis included descriptive statistics, bivariate comparisons, and multivariate logistic regression. RESULTS: Spanish-speaking patients demonstrated significantly higher engagement across all measures than English-speaking patients with respect to FIT test opt-in rates (18.2% vs 7.1%, P<.001), connect rates (69.6% vs 53.0%, P<.001), and call duration (6.05 vs 4.03 minutes, P<.001). Demographically, Spanish-speaking patients were younger (mean age 57 vs 61 years, P<.001) and more likely to be female (49.1% vs 38.4%, P<.001). In multivariate analysis, Spanish language preference remained an independent predictor of FIT test opt-in (adjusted odds ratio 2.012, 95% CI 1.340-3.019; P<.001) after controlling for demographic factors and call duration. CONCLUSIONS: AI-powered outreach achieved significantly higher engagement among Spanish-speaking patients, challenging the assumption that technological interventions inherently disadvantage non-English-speaking populations. The 2.6-fold higher FIT test opt-in rate among Spanish-speaking patients represents a notable departure from historical patterns of health care disparities. These findings suggest that language-concordant AI interactions may help address longstanding disparities in preventive care access. Study limitations include its single health care system setting, short duration, and lack of follow-up data on completed screenings. Future research should assess long-term adherence and whether higher engagement translates to improved clinical outcomes.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/71211",
      "authors": "Bhimani Meenesh et al.",
      "keywords": "Hispanic Americans; artificial intelligence; colorectal cancer screening; gastroenterology; health care disparities; preventive health services",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Artificial Intelligence; Colorectal Neoplasms; Early Detection of Cancer; Healthcare Disparities; Hispanic or Latino; Language; Multilingualism; Occult Blood; Pennsylvania; Retrospective Studies",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40561471/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12278996",
      "ft_text_length": 25998,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12278996)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40595602",
      "title": "Mitigating data bias and ensuring reliable evaluation of AI models with shortcut hull learning.",
      "abstract": "Shortcut learning poses a significant challenge to both the interpretability and robustness of artificial intelligence, arising from dataset biases that lead models to exploit unintended correlations, or shortcuts, which undermine performance evaluations. Addressing these inherent biases is particularly difficult due to the complex, high-dimensional nature of data. Here, we introduce shortcut hull learning, a diagnostic paradigm that unifies shortcut representations in probability space and utilizes diverse models with different inductive biases to efficiently learn and identify shortcuts. This paradigm establishes a comprehensive, shortcut-free evaluation framework, validated by developing a shortcut-free topological dataset to assess deep neural networks' global capabilities, enabling a shift from Minsky and Papert's representational analysis to an empirical investigation of learning capacity. Unexpectedly, our experimental results suggest that under this framework, convolutional models-typically considered weak in global capabilities-outperform transformer-based models, challenging prevailing beliefs. By enabling robust and bias-free evaluation, our framework uncovers the true model capabilities beyond architectural preferences, offering a foundation for advancing AI interpretability and reliability.",
      "journal": "Nature communications",
      "year": "2025",
      "doi": "10.1038/s41467-025-60801-6",
      "authors": "Zhou Wenhao et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Neural Networks, Computer; Humans; Bias; Deep Learning; Reproducibility of Results; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40595602/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12219125",
      "ft_text_length": 103922,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12219125)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40770226",
      "title": "Invulnerability bias in perceptions of artificial intelligence's future impact on employment.",
      "abstract": "The adoption of Artificial Intelligence (AI) is reshaping the labor market; however, individuals' perceptions of its impact remain inconsistent. This study investigates the presence of the Invulnerability Bias (IB), where workers perceive that AI will have a greater impact on others' jobs than on their own, and Optimism Bias by Type of Impact (OBTI), where individuals perceive AI's future impact on their own job as more positive than on others'. The study analyzes survey data collected from 201 participants, recruited through social media using convenience sampling. The data were analyzed using a combination of statistical and machine learning methods, including the Wilcoxon test, ordinary least squares regression, clustering, random forests, and decision trees. Results confirm a significant IB, but not OBTI; only 31.8% perceived AI's future impact on their own job as more positive than on others'. Analysis shows that greater knowledge of AI correlates with lower IB, suggesting that familiarity with AI reduces the tendency to externalize perceived risk. Furthermore, bias levels vary across professional sectors: healthcare, law, and public administration exhibit the highest IB, while technology-related professions show lower levels. These findings highlight the need for interventions to improve workers' awareness of AI's potential future impact on employment.",
      "journal": "Scientific reports",
      "year": "2025",
      "doi": "10.1038/s41598-025-14698-2",
      "authors": "Barrera-Jimenez Felipe et al.",
      "keywords": "AI biases; Artificial intelligence; Future of work; Invulnerability bias; Optimism bias; Unrealistic optimism",
      "mesh_terms": "Humans; Artificial Intelligence; Employment; Female; Male; Adult; Surveys and Questionnaires; Middle Aged; Perception; Bias; Young Adult; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40770226/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12328832",
      "ft_text_length": 43203,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12328832)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40794581",
      "title": "Integrating Artificial Intelligence Into Cancer Care: Enhancing Nursing Practice and Bridging Disparities.",
      "abstract": "Artificial intelligence (AI) in cancer care has the potential to transform nursing practice, reduce cancer disparities, and enhance patient outcomes across the continuum of care from prevention, screening, and treatment to su.",
      "journal": "Clinical journal of oncology nursing",
      "year": "2025",
      "doi": "10.1188/25.CJON.345-351",
      "authors": "Lee Youran et al.",
      "keywords": "artificial intelligence; cancer care; cancer disparities",
      "mesh_terms": "Artificial Intelligence; Humans; Neoplasms; Oncology Nursing; Healthcare Disparities; Female; Male",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40794581/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12339281",
      "ft_text_length": 1077,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12339281)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40841953",
      "title": "Evaluating the o1 reasoning large language model for cognitive bias: a vignette study.",
      "abstract": "BACKGROUND: Cognitive biases, systematic deviations from logical judgment, are well documented in clinical decision-making, particularly in clinical settings characterized by high decision load, limited time, and diagnostic uncertainty-such as critical care. Prior work demonstrated that large language models, particularly GPT-4, reproduce many of these biases, sometimes to a greater extent than human clinicians. METHODS: We tested whether the o1 model (o1-2024-12-17), a newly released AI system with enhanced reasoning capabilities, is susceptible to cognitive biases that commonly affect medical decision-making. Following the methodology established by Wang and Redelmeier [15], we used ten pairs of clinical scenarios, each designed to test a specific cognitive bias known to influence clinicians. Each scenario had two versions, differed by subtle modifications designed to trigger the bias (such as presenting mortality rates versus survival rates). The o1 model generated 90 independent clinical recommendations for each scenario version, totalling 1,800 responses. We measured cognitive bias as systematic differences in recommendation rates between the paired scenarios, which should not occur with unbiased reasoning. The o1 model's performance was compared against previously published results from both the GPT-4 model and historical human clinician studies. RESULTS: The o1 model showed no measurable cognitive bias in seven of the ten vignettes. In two vignettes, the o1 model showed significant bias, but its absolute magnitude was lower than values previously reported for GPT-4 and human clinicians. In a single vignette, Occam's razor, the o1 model exhibited consistent bias. Therefore, although overall bias appears less frequent overall with the reasoning model than with GPT-4, it was worse in one vignette. The model was more prone to bias in vignettes that included a gap-closing cue, seemingly resolving the clinical uncertainty. Across eight vignette versions, intra\u2011scenario agreement exceeded 94%, indicating lower decision variability than previously described with GPT\u20114 and human clinicians. CONCLUSION: Reasoning models may reduce cognitive bias and random variation in judgment (i.e., \"noise\"). However, our findings caution that reasoning models are still not entirely immune to cognitive bias. These findings suggest that reasoning models may impart some benefits as decision-support tools in medicine, but they also imply a need to explore further the circumstances in which these tools may fail.",
      "journal": "Critical care (London, England)",
      "year": "2025",
      "doi": "10.1186/s13054-025-05591-5",
      "authors": "Degany Or et al.",
      "keywords": "Artificial intelligence; Clinical decision-making; Cognitive bias; Critical care; Diagnostic error; Heuristics; Large language models; Noise (decision variability); Observer variation; Step-by-step reasoning",
      "mesh_terms": "Humans; Bias; Clinical Decision-Making; Cognition; Female; Male; Adult; Language; Middle Aged; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40841953/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12372181",
      "ft_text_length": 24682,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12372181)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40901720",
      "title": "Transport-based transfer learning on Electronic Health Records: application to detection of treatment disparities.",
      "abstract": "OBJECTIVES: Electronic Health Records (EHRs) sampled from different populations can introduce unwanted biases, limit individual-level data sharing, and make the data and fitted model hardly transferable across different population groups. In this context, our main goal is to design an effective method to transfer knowledge between population groups, with computable guarantees for suitability, and that can be applied to quantify treatment disparities. MATERIALS AND METHODS: For a model trained in an embedded feature space of one subgroup, our proposed framework, Optimal Transport-based Transfer Learning for EHRs (OTTEHR), combines feature embedding of the data and unbalanced optimal transport (OT) for domain adaptation to another population group. To test our method, we processed and divided the MIMIC-III and MIMIC-IV databases into multiple population groups using ICD codes and multiple labels. RESULTS: We derive a theoretical bound for the generalization error of our method, and interpret it in terms of the Wasserstein distance, unbalancedness between the source and target domains, and labeling divergence, which can be used as a guide for assessing the suitability of binary classification and regression tasks. In general, our method achieves better accuracy and computational efficiency compared with standard and machine learning transfer learning methods on various tasks. Upon testing our method for populations with different insurance plans, we detect various levels of disparities in hospital duration stay between groups. DISCUSSION AND CONCLUSION: By leveraging tools from OT theory, our proposed framework allows to compare statistical models on EHR data between different population groups. As a potential application for clinical decision making, we quantify treatment disparities between different population groups. Future directions include applying OTTEHR to broader regression and classification tasks and extending the method to semi-supervised learning.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2026",
      "doi": "10.1093/jamia/ocaf134",
      "authors": "Li Wanxin et al.",
      "keywords": "Electronic Health Records; optimal transport; transfer learning; treatment disparities",
      "mesh_terms": "Electronic Health Records; Humans; Machine Learning; Healthcare Disparities; International Classification of Diseases; Health Status Disparities",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40901720/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12758479",
      "ft_text_length": 39527,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12758479)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "40990064",
      "title": "A communication-efficient federated learning algorithm to assess racial disparities in post-transplantation survival time.",
      "abstract": "OBJECTIVE: Patients of different race have different outcomes following renal transplantation. Patients of different race also undergo renal transplantation at different hospitals. We used a novel decentralized multisite approach to quantitatively assess the effect of site of care on racial disparities between non-Hispanic Black (NHB) and non-Hispanic White (NHW) patients in post-transplantation survival times. MATERIALS AND METHODS: In this study, we develop a communication-efficient federated learning algorithm to assess site-of-care associated racial disparities based on decentralized time-to-event data, called Communication-Efficient Distributed Analysis for Racial Disparity in Time-to-event Data (CEDAR-t2e). The algorithm includes 2 modules. Module I is to estimate the site-specific proportional hazards model for time-to-event outcomes in a distributed manner, in which the Poissonization is used to simplify the estimation procedure. Based on the estimated results from Module I, Module II calculates how long the kidney failure time of NHB patients would be extended had they been admitted to transplant centers in the same distribution as NHW patients were admitted. RESULTS: With application to United States Renal Data System data covering 39\u2009043 patients across 73 transplant centers, we found no evidence suggesting the presence of site-of-care associated racial disparities in post-transplantation survival times. In particular, restricting to one year after transplantation, the counterfactual graft failure time would have been extended by only 0.61 days on average if NHB had the same admission distribution to transplant centers as NHW patients. DISCUSSION: The proposed approach offers a quantitative measure to evaluate site-of-care associated racial disparities. CONCLUSION: Our approach has the potential to be extended to investigate site-of-care related disparities in other time-to-event outcomes, thus promoting health equity and improving patient health in various fields.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2025",
      "doi": "10.1093/jamia/ocaf138",
      "authors": "Wang Yudong et al.",
      "keywords": "communication-efficient; federated learning; kidney transplantation; racial disparity; time-to-event outcomes",
      "mesh_terms": "Female; Humans; Male; Middle Aged; Algorithms; Black or African American; Federated Learning; Healthcare Disparities; Kidney Transplantation; Proportional Hazards Models; United States; White",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40990064/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12646379",
      "ft_text_length": 2024,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12646379)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41326577",
      "title": "Impacts of cognitive forcing and need for cognition on biased AI-assisted decision making about mental health emergencies.",
      "abstract": "UNLABELLED: Artificial intelligence (AI) trained to predict psychiatric inpatient violence may overestimate risks for marginalized groups, making it critical to find ways to mitigate reliance on biased AI in this context. One potential solution is Cognitive forcing (CF), or interventions that delay AI information or slow the decision-making process. Benefits of CF may be modulated by traits, such as Need for Cognition (NFC), or the tendency to engage with complex, cognitive tasks. To examine how CF and NFC impact AI-assisted decision-making about violence risk, we conducted two experiments. In Experiment 1, participants (n\u2009=\u2009281) made decisions about violence risk based on vignettes describing various patients experiencing mental health emergencies, and they were randomized to view biased or unbiased AI recommendations. In Experiment 2, participants (n\u2009=\u2009373) made similar decisions, and they were randomized to view biased AI recommendations with one of three CF interventions or no CF. All participants completed measures of NFC. In both experiments, participants made biased decisions (overestimating violence risk for marginalized patients) when viewing biased AI recommendations. In Experiment 2, CF interventions did not mitigate this decision-making bias; however, participants reporting high NFC were less likely to make biased decisions when viewing biased AI recommendations, compared to those with low NFC. CF may not effectively safeguard against the impact of biased AI in high-stakes settings, like acute mental health care, or for decisions about violence risk prediction, which are fraught with social or racial stereotypes. However, trait NFC may mitigate reliance on biased AI information, highlighting a role of psychological factors. Further research is needed into various factors that promote equitable AI-assisted decision-making for mental health. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1038/s41598-025-30506-3.",
      "journal": "Scientific reports",
      "year": "2025",
      "doi": "10.1038/s41598-025-30506-3",
      "authors": "Vejandla Shrika et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41326577/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12779943",
      "ft_text_length": 54835,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12779943)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41381075",
      "title": "Cardiovascular disease risk disparities between immigrants and native Koreans: a population-based study in Gwangju, Korea.",
      "abstract": "OBJECTIVES: Korea is becoming a multiethnic society, with immigrants comprising nearly 5% of the population. Evidence on cardiovascular disease (CVD) risk among immigrants remains limited. METHODS: We conducted a population-based study of 582 immigrants in Gwangju and 2,328 age-matched and gender-matched native Koreans (2022-2023). Immigrant data were obtained from direct health assessments, while native Korean data were drawn from the Korea National Health and Nutrition Examination Survey. CVD risk was estimated using the Framingham risk score (FRS) and pooled cohort equations (PCE). Logistic regression was employed to compare the odds of elevated risk (10-year CVD risk \u22657.5%), adjusting for socio-demographic and behavioral factors. RESULTS: Immigrants had a higher prevalence of hypertension (37.3 vs. 16.1%), diabetes (11.5 vs. 5.6%), poor self-rated health (69.6 vs. 61.3%), and unmet medical needs (30.9 vs. 8.9%), as well as lower rates of health checkups and cancer screening (all p<0.001), compared to native Koreans. Elevated CVD risk was more frequent in immigrants (FRS, 31.4 vs. 20.8%; PCE, 33.6 vs. 22.8%). The adjusted odds ratios (95% confidence intervals) were 1.47 (1.14 to 1.88) for FRS and 1.49 (1.07 to 2.08) for PCE. Disparities were greatest among women, adults \u226540 years, uninsured people, low-income groups, and migrants from Central Asia, Russia, and Africa. CONCLUSIONS: Immigrants in Korea face substantially higher CVD risk than native Koreans, particularly within socioeconomically vulnerable subgroups. Targeted prevention and policies addressing structural barriers are urgently needed.",
      "journal": "Epidemiology and health",
      "year": "2025",
      "doi": "10.4178/epih.e2025067",
      "authors": "Yang Jung-Ho et al.",
      "keywords": "Cardiovascular disease; Health disparities; Immigrant; Korea",
      "mesh_terms": "Humans; Republic of Korea; Female; Male; Emigrants and Immigrants; Cardiovascular Diseases; Middle Aged; Adult; Health Status Disparities; Nutrition Surveys; Aged; Prevalence; Heart Disease Risk Factors; East Asian People",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41381075/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12884040",
      "ft_text_length": 23903,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12884040)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    },
    {
      "pmid": "41630487",
      "title": "Geographic disparities and health inequality evolution in the disease burden of tracheal, bronchus, and lung cancer in Asia: a 33-year longitudinal analysis and future projections.",
      "abstract": "OBJECTIVE: This study examined geographic patterns, temporal trends, and determinants of tracheal, bronchus, and lung (TBL) cancer burden in Asia (1990-2023), evaluated disparities in health outcomes, and developed projections for targeted prevention initiatives. METHODS: Data were obtained from the Global Burden of Disease Study 2023. We estimated age-standardized rates (ASPR, ASIR, ASMR, ASDR). Joinpoint regression assessed temporal trends; age-period-cohort and Das Gupta analyses identified underlying mechanisms and drivers. Data envelopment analysis (DEA) evaluated efficiency relative to the Human Development Index (HDI). Health inequalities were measured with the slope index of inequality (SII) and concentration index (CI). Bayesian age-period-cohort (BAPC) modeling projected trends to 2038. RESULTS: Asian populations experienced 2.05 million prevalent cases, 1.40 million incident cases, 1.26 million fatalities, and 29.45 million DALYs in 2023. East Asia had the highest burden. From 1990 to 2023, ASPR increased, while ASMR and ASDR declined. Males and older adults carried disproportionately higher burdens. Decomposition analysis revealed that epidemiological changes and population growth were the major contributors. Inequality worsened, with SII rising from 275.8 to 311.6 and CI consistently negative, indicating concentration in low-HDI regions. Projections suggested further increases in ASPR (+28.7%) and ASIR (+21.6%) by 2038. CONCLUSION: The TBL cancer burden in Asia is rising, with marked regional, gender, age disparities and deteriorating health equity. East Asia is most affected. Targeted strategies, including enhanced early screening in high-burden areas and improved healthcare accessibility in low-HDI regions, are essential to reduce the disease burden and promote equity.",
      "journal": "Annals of medicine",
      "year": "2026",
      "doi": "10.1080/07853890.2026.2624212",
      "authors": "Wang Hongming et al.",
      "keywords": "Asian epidemiology; TBL cancer; disease burden; health inequality; prediction model; trend analysis",
      "mesh_terms": "Humans; Male; Lung Neoplasms; Female; Middle Aged; Adult; Asia; Tracheal Neoplasms; Health Status Disparities; Bronchial Neoplasms; Aged; Longitudinal Studies; Bayes Theorem; Global Burden of Disease; Incidence; Forecasting; Prevalence; Young Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41630487/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12872098",
      "ft_text_length": 52230,
      "ft_status": "Full text screened \u2014 EXCLUDED (PMC12872098)",
      "ft_reason": "Excluded: insufficient approach content (0 indicators)"
    }
  ],
  "no_fulltext": [
    {
      "pmid": "11008073",
      "title": "Lung allocation in the United States, 1995-1997: an analysis of equity and utility.",
      "abstract": "BACKGROUND: Waiting time for organ transplantation varies widely between programs of different sizes and by geographic regions. The purpose of this study was to determine if the current lung-allocation policy is equitable for candidates waiting at various-sized centers, and to model how national allocation based solely on waiting time might affect patients and programs. METHODS: UNOS provided data on candidate registrations; transplants and outcomes; waiting times; and deaths while waiting for all U.S. lung-transplant programs during 1995-1997. Transplant centers were categorized based on average yearly volume: small (< or = 10 pounds sterling transplants/year; n = 46), medium (11-30 transplants/year; n = 29), or large (>30 transplants/year; n = 6). This data was used to model national organ allocation based solely on accumulated waiting time for candidates listed at the end of 1997. RESULTS: Median waiting time for patients transplanted was longest at large programs (724-848 days) compared to small and medium centers (371-552 days and 337-553 days, respectively) and increased at programs of all sizes during the study period. Wait-time-adjusted risk of death correlated inversely with program size (365 vs 261 vs 148 deaths per 1,000 patient-years-at-risk at small, medium, and large centers, respectively). Mortality as a percentage of new candidate registrations was similar for all program categories, ranging from 21 to 25%. Survival rates following transplantation were equivalent at medium-sized centers vs large centers (p = 0.50), but statistically lower when small centers were compared to either large- or medium-size centers (p < or = 0.05). Using waiting time as the primary criterion lung allocation would acutely shift 10 to 20% of lung-transplant activity from medium to large programs. CONCLUSIONS: 1) Waiting list mortality rates are not higher at large lung-transplant programs with long average waiting times. 2) A lung-allocation algorithm based primarily on waiting-list seniority would probably disadvantage candidates at medium-size centers without improving overall lung-transplant outcomes. 3) If fairness is measured by equal distribution of opportunity and risk, we conclude that the current allocation system is relatively equitable for patients currently entering the lung-transplant system.",
      "journal": "The Journal of heart and lung transplantation : the official publication of the International Society for Heart Transplantation",
      "year": "2000",
      "doi": "10.1016/s1053-2498(00)00151-0",
      "authors": "Pierson R N et al.",
      "keywords": "Empirical Approach; Health Care and Public Health",
      "mesh_terms": "Actuarial Analysis; Health Care Rationing; Humans; Lung Transplantation; Retrospective Studies; Tissue and Organ Procurement; United States; Waiting Lists",
      "pub_types": "Journal Article; Multicenter Study; Research Support, U.S. Gov't, Non-P.H.S.; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/11008073/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "11128891",
      "title": "Clinician judgments of functional outcomes: how bias and perceived accuracy affect rating.",
      "abstract": "OBJECTIVE: To evaluate the accuracy of clinician judgments of patient function, the susceptibility of judges to bias, and the relation between a judge's degree of belief in his/her accuracy of classification to observed accuracy when using the FIM instrument. PARTICIPANTS: Fifty rehabilitation professionals. SETTING: 3 urban medical centers. DESIGN: Four randomized experiments among subjects to examine the effect of potentially biasing information on FIM ratings of patient vignettes. Participants answered 60 true/false questions regarding patient function and FIM score and indicated confidence in the accuracy of their answers. INTERVENTIONS: Manipulation of patient information. MAIN OUTCOME MEASURES: The standard FIM 7-point scale, observed proportion of correct responses to the 60 true/false questions, and a 6-category confidence scale for each of the 60 questions were used as dependent measures. RESULTS: FIM ratings assigned to others biased participants' FIM ratings of patient vignettes. Functional ability was overestimated when ratings in other domains were high and underestimated when they were low. Participants were overconfident in their ability to answer FIM questions accurately across all professional disciplines. CONCLUSION: Bias and poor judgment of level accuracy play a significant role in clinician ratings of patient functioning. Blind ratings and training in debiasing are potential solutions to the problem.",
      "journal": "Archives of physical medicine and rehabilitation",
      "year": "2000",
      "doi": "10.1053/apmr.2000.16345",
      "authors": "Wolfson A M et al.",
      "keywords": "",
      "mesh_terms": "Activities of Daily Living; Adult; Bias; Disability Evaluation; Female; Humans; Judgment; Male; Observer Variation; Rehabilitation; Statistics, Nonparametric; Washington",
      "pub_types": "Clinical Trial; Journal Article; Randomized Controlled Trial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/11128891/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "11700251",
      "title": "Use of missing-data methods to correct bias and improve precision in case-control studies in which cases are subtyped but subtype information is incomplete.",
      "abstract": "Histologic and genetic markers can sometimes make it possible to refine a disease into subtypes. In a case-control study, an attempt to subcategorize a disease in this way can be important to elucidating its etiology if the subtypes tend to result from distinct causal pathways. Using subtyped case outcomes, one can carry out either a case-case analysis to investigate etiologic heterogeneity or do polytomous logistic regression to estimate odds ratios specific to subtypes. Unfortunately, especially when such an analysis is undertaken after the study has been completed, it may be compromised by the unavailability of tissue specimens, resulting in missing subtype data for many enrolled cases. The authors propose that one can more fully use the available data, including that provided by cases with missing subtype, by using the expectation-maximization algorithm to estimate risk parameters. For illustration, they apply the method to a study of non-Hodgkin's lymphoma in the midwestern United States. The simulations then demonstrate that, under assumptions likely to hold in many settings, the approach eliminates bias that would arise if unclassified cases were ignored and also improves the precision of estimation. Under the same assumptions, empirical confidence interval coverage is consistent with the nominal 95%.",
      "journal": "American journal of epidemiology",
      "year": "2001",
      "doi": "10.1093/aje/154.10.954",
      "authors": "Schroeder J C et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Analysis of Variance; Bias; Case-Control Studies; Computer Simulation; Humans; Likelihood Functions; Lymphoma, Non-Hodgkin; Midwestern United States; Models, Statistical; Odds Ratio; Outcome Assessment, Health Care",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/11700251/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "11779685",
      "title": "The problem of bias in training data in regression problems in medical decision support.",
      "abstract": "This paper describes a bias problem encountered in a machine learning approach to outcome prediction in anticoagulant drug therapy. The outcome to be predicted is a measure of the clotting time for the patient; this measure is continuous and so the prediction task is a regression problem. Artificial neural networks (ANNs) are a powerful mechanism for learning to predict such outcomes from training data. However, experiments have shown that an ANN is biased towards values more commonly occurring in the training data and is thus, less likely to be correct in predicting extreme values. This issue of bias in training data in regression problems is similar to the associated problem with minority classes in classification. However, this bias issue in classification is well documented and is an on-going area of research. In this paper, we consider stratified sampling and boosting as solutions to this bias problem and evaluate them on this outcome prediction problem and on two other datasets. Both approaches produce some improvements with boosting showing the most promise.",
      "journal": "Artificial intelligence in medicine",
      "year": "2002",
      "doi": "10.1016/s0933-3657(01)00092-6",
      "authors": "Mac Namee B et al.",
      "keywords": "",
      "mesh_terms": "Bias; Computer Simulation; Decision Making, Computer-Assisted; Decision Support Techniques; Humans; Regression Analysis",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/11779685/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "12848923",
      "title": "[An effective method to reduce bias between two compared groups: propensity score].",
      "abstract": "OBJECTIVE: Through introduction of principal theory and algorithm of propensity score to design SAS macro programs for binary data. METHODS: Propensity score method was used to compare the differences of character variables between two groups, and the association of DNR (Do Not Resuscitate) with the mortality of congestive heart failure was evaluated with different methods. RESULTS: Significant differences among the character variables between two groups were effectively balanced with stratification or matching method. The odds ratios of DNR with the in-hospital mortality rate of congestive heart failure were estimated identical with different algorithms and to find that the association of DNR to in-hospital mortality was highly significant. CONCLUSION: Propensity score was a good algorithm that could be used to analyze any kind of observational data for matching the effects among the character variables.",
      "journal": "Zhonghua liu xing bing xue za zhi = Zhonghua liuxingbingxue zazhi",
      "year": "2003",
      "doi": "",
      "authors": "Zhao Shou-jun et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Bias; Heart Failure; Humans; Models, Statistical",
      "pub_types": "Comparative Study; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/12848923/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "17096661",
      "title": "Improving SVT discrimination in single-chamber ICDs: a new electrogram morphology-based algorithm.",
      "abstract": "INTRODUCTION: Wide-spread adoption of ICD therapy has focused efforts on improving the quality of life for patients by reducing \"inappropriate\" shock therapies. To this end, distinguishing supraventricular tachycardia from ventricular tachycardia remains a major challenge for ICDs. More sophisticated discrimination algorithms based on ventricular electrogram morphology have been made practicable by the increased computational ability of modern ICDs. METHODS AND RESULTS: We report results from a large prospective study (1,122 pts) of a new ventricular electrogram morphology tachycardia discrimination algorithm (Wavelet Dynamic Discrimination, Medtronic, Minneapolis, MN, USA) operating at minimal algorithm setting (RV coil-can electrogram, match threshold of 70%). This is a nonrandomized cohort study of ICD patients using the morphology discrimination of the Wavelet algorithm to distinguish SVT and VT/VF. The Wavelet criterion was required ON in all patients and all other supraventricular tachycardia discriminators were required to be OFF. Spontaneous episodes (N = 2,235) eligible for ICD therapy were adjudicated for detection algorithm performance. The generalized estimating equations method was used to remove bias introduced when an individual patient contributes multiple episodes. Inappropriate therapies for supraventricular tachycardia were reduced by 78% (90% CI: 72.8-82.9%) for episodes within the range of rates where Wavelet was programmed to discriminate. Sensitivity for sustained ventricular tachycardia was 98.6% (90% CI: 97-99.3%) without the use of high-rate time out. CONCLUSIONS: Results from this prospective study of the Wavelet electrogram morphology discrimination algorithm operating as the sole discriminator in the ON mode demonstrate that inappropriate therapy for supraventricular tachycardia in a single-chamber ICD can be dramatically reduced compared to rate detection alone.",
      "journal": "Journal of cardiovascular electrophysiology",
      "year": "2006",
      "doi": "10.1111/j.1540-8167.2006.00643.x",
      "authors": "Klein George J et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Cohort Studies; Defibrillators, Implantable; Diagnosis, Computer-Assisted; Diagnosis, Differential; Discriminant Analysis; Electric Countershock; Electrocardiography; Female; Humans; Male; Middle Aged; Reproducibility of Results; Sensitivity and Specificity; Tachycardia, Supraventricular; Tachycardia, Ventricular; Therapy, Computer-Assisted",
      "pub_types": "Controlled Clinical Trial; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/17096661/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "17328979",
      "title": "Economic evaluation of services for a National Health scheme: the case for a fairness-based framework.",
      "abstract": "In this paper we argue that the usual framework for evaluating health services may need modification in the context of a National Health Scheme (NHS). Some costs and benefits may need to be ignored or discounted, others included at face value, and some transfer payments included in the decision algorithm. In contrast with the standard framework, we argue that economic evaluation in the context of an NHS should focus on 'social transfers' between taxpayers and beneficiaries, and that the nature and scope of these transfers is determined by the level of social generosity. Some of the implications of a modified framework are illustrated with a re-examination of (i) costs and transfer payments, (ii) unrelated future costs, (iii) moral hazard, and (iv) the rule that marginal costs should equal marginal benefits. We argue that an explicitly 'fairness-based' framework is needed for the evaluation of services in an NHS. In contrast, the usual welfare economic theoretic framework facilitates the sidelining of issues of fairness.",
      "journal": "Journal of health economics",
      "year": "2007",
      "doi": "10.1016/j.jhealeco.2006.11.004",
      "authors": "Richardson Jeff et al.",
      "keywords": "",
      "mesh_terms": "Evaluation Studies as Topic; Health Services Accessibility; Humans; National Health Programs; Social Justice; Victoria",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/17328979/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "17354730",
      "title": "Many heads are better than one: jointly removing bias from multiple MRIs using nonparametric maximum likelihood.",
      "abstract": "The correction of multiplicative bias in magnetic resonance images is an important problem in medical image processing, especially as a preprocessing step for quantitative measurements and other numerical procedures. Most previous approaches have used a maximum likelihood method to increase the probability of the pixels in a single image by adaptively estimating a correction to the unknown image bias field. The pixel probabilities are defined either in terms of a pre-existing tissue model, or nonparametrically in terms of the image's own pixel values. In both cases, the specific location of a pixel in the image does not influence the probability calculation. Our approach, similar to methods of joint registration, simultaneously eliminates the bias from a set of images of the same anatomy, but from different patients. We use the statistics from the same location across different patients' images, rather than within an image, to eliminate bias fields from all of the images simultaneously. Evaluating the likelihood of a particular voxel in one patient's scan with respect to voxels in the same location in a set of other patients' scans disambiguates effects that might be due to either bias fields or anatomy. We present a variety of \"two-dimensional\" experimental results (working with one image from each patient) showing how our method overcomes serious problems experienced by other methods. We also present preliminary results on full three-dimensional volume correction across patients.",
      "journal": "Information processing in medical imaging : proceedings of the ... conference",
      "year": "2005",
      "doi": "10.1007/11505730_51",
      "authors": "Learned-Miller Erik G et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artificial Intelligence; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Likelihood Functions; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",
      "pub_types": "Evaluation Study; Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/17354730/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "17354851",
      "title": "Validation of image segmentation by estimating rater bias and variance.",
      "abstract": "The accuracy and precision of segmentations of medical images has been difficult to quantify in the absence of a \"ground truth\" or reference standard segmentation for clinical data. Although physical or digital phantoms can help by providing a reference standard, they do not allow the reproduction of the full range of imaging and anatomical characteristics observed in clinical data. An alternative assessment approach is to compare to segmentations generated by domain experts. Segmentations may be generated by raters who are trained experts or by automated image analysis algorithms. Typically these segmentations differ due to intra-rater and inter-rater variability. The most appropriate way to compare such segmentations has been unclear. We present here a new algorithm to enable the estimation of performance characteristics, and a true labeling, from observations of segmentations of imaging data where segmentation labels may be ordered or continuous measures. This approach may be used with, amongst others, surface, distance transform or level set representations of segmentations, and can be used to assess whether or not a rater consistently over-estimates or under-estimates the position of a boundary.",
      "journal": "Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "year": "2006",
      "doi": "10.1007/11866763_103",
      "authors": "Warfield Simon K et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artificial Intelligence; Brain Neoplasms; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Observer Variation; Pattern Recognition, Automated; Professional Competence; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Task Performance and Analysis",
      "pub_types": "Evaluation Study; Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/17354851/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "18065243",
      "title": "The impact of skull-stripping and radio-frequency bias correction on grey-matter segmentation for voxel-based morphometry.",
      "abstract": "This study evaluates the application of (i) skull-stripping methods (hybrid watershed algorithm (HWA), brain surface extractor (BSE) and brain-extraction tool (BET2)) and (ii) bias correction algorithms (nonparametric nonuniform intensity normalisation (N3), bias field corrector (BFC) and FMRIB's automated segmentation tool (FAST)) as pre-processing pipelines for the technique of voxel-based morphometry (VBM) using statistical parametric mapping v.5 (SPM5). The pipelines were evaluated using a BrainWeb phantom, and those that performed consistently were further assessed using artificial-lesion masks applied to 10 healthy controls compared to the original unlesioned scans, and finally, 20 Alzheimer's disease (AD) patients versus 23 controls. In each case, pipelines were compared to each other and to those from default SPM5 methodology. The BET2+N3 pipeline was found to produce the least miswarping to template induced by real abnormalities, and performed consistently better than the other methods for the above experiments. Occasionally, the clusters of significant differences located close to the boundary were dragged out of the glass-brain projections -- this could be corrected by adding background noise to low-probability voxels in the grey matter segments. This method was confirmed in a one-dimensional simulation and was preferable to threshold and explicit (simple) masking which excluded true abnormalities.",
      "journal": "NeuroImage",
      "year": "2008",
      "doi": "10.1016/j.neuroimage.2007.10.051",
      "authors": "Acosta-Cabronero Julio et al.",
      "keywords": "",
      "mesh_terms": "Adult; Aged; Algorithms; Alzheimer Disease; Brain; Brain Mapping; Echo-Planar Imaging; Female; Humans; Image Processing, Computer-Assisted; Male; Middle Aged; Models, Statistical; Skull",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/18065243/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "18177784",
      "title": "New algorithm for treatment allocation reduced selection bias and loss of power in small trials.",
      "abstract": "OBJECTIVE: In clinical trials, patients become available for treatment sequentially. Especially in trials with a small number of patients, loss of power may become an important issue, if treatments are not allocated equally or if prognostic factors differ between the treatment groups. We present a new algorithm for sequential allocation of two treatments in small clinical trials, which is concerned with the reduction of both selection bias and imbalance. STUDY DESIGN AND SETTING: With the algorithm, an element of chance is added to the treatment as allocated by minimization. The amount of chance depends on the actual amount of imbalance of treatment allocations of the patients already enrolled. The sensitivity to imbalance may be tuned. We performed trial simulations with different numbers of patients and prognostic factors, in which we quantified loss of power and selection bias. RESULTS: With our method, selection bias is smaller than with minimization, and loss of power is lower than with pure randomization or treatment allocation according to a biased coin principle. CONCLUSION: Our method combines the conflicting aims of reduction of bias by predictability and reduction of loss of power, as a result of imbalance. The method may be of use in small trials.",
      "journal": "Journal of clinical epidemiology",
      "year": "2008",
      "doi": "10.1016/j.jclinepi.2007.04.002",
      "authors": "Hofmeijer J et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Humans; Patient Selection; Prognosis; Random Allocation; Randomized Controlled Trials as Topic; Research Design; Selection Bias",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/18177784/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "19110834",
      "title": "Human factors of the confirmation bias in intelligence analysis: decision support from graphical evidence landscapes.",
      "abstract": "OBJECTIVE: This study addresses the human factors challenge of designing and validating decision support to promote less biased intelligence analysis. BACKGROUND: The confirmation bias can compromise objectivity in ambiguous medical and military decision making through neglect of conflicting evidence and judgments not reflective of the entire evidence spectrum. Previous debiasing approaches have had mixed success and have tended to place additional demands on users' decision making. METHOD: Two new debiasing interventions that help analysts picture the full spectrum of evidence, the relation of evidence to a hypothesis, and other analysts' evidence assessments were manipulated in a repeated-measures design: (a) an integrated graphical evidence layout, compared with a text baseline; and (b) evidence tagged with other analysts' assessments, compared with participants' own assessments. Twenty-seven naval trainee analysts and reservists assessed, selected, and prioritized evidence in analysis vignettes carefully constructed to have balanced supporting and conflicting evidence sets. Bias was measured for all three evidence analysis steps. RESULTS: A bias to select a skewed distribution of confirming evidence occurred across conditions. However, graphical evidence layout, but not other analysts' assessments, significantly reduced this selection bias, resulting in more balanced evidence selection. Participants systematically prioritized the most supportive evidence as most important. CONCLUSION: Domain experts exhibited confirmation bias in a realistic intelligence analysis task and apparently conflated evidence supportiveness with importance. Graphical evidence layout promoted more balanced and less biased evidence selection. APPLICATION: Results have application to real-world decision making, implications for basic decision theory, and lessons for how shrewd visualization can help reduce bias.",
      "journal": "Human factors",
      "year": "2008",
      "doi": "10.1518/001872008X354183",
      "authors": "Cook Maia B et al.",
      "keywords": "",
      "mesh_terms": "Decision Making; Decision Support Techniques; Female; Humans; Intelligence; Male; Military Science; Observer Variation",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/19110834/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "2033767",
      "title": "Biased estimates of expected acute myocardial infarction mortality using MedisGroups admission severity groups.",
      "abstract": "This study examines whether the MedisGroups admission severity groups give unbiased estimates of 30-day mortality in 3037 Medicare-aged patients who were hospitalized in 1985 through 1986 with acute myocardial infarction. The average observed death rate for all acute myocardial infarction patients in the study who were in a given admission severity group was used to estimate the expected death probability for each case in a given group. (This is the same method used by the Pennsylvania Health Care Cost Containment Council for risk adjusting hospital mortality by diagnosis related groups in that state.) When compared with observed deaths, estimates of expected mortality were significantly biased for many patient attributes (eg, age, location of acute myocardial infarction, history of congestive heart failure, serum potassium level, serum urea nitrogen level, pulse rate, and blood pressure). These results are consistent with a conclusion that the MedisGroups scoring algorithm omits some important risk variables, inappropriately includes some other variables reflecting postadmission status, and gives the wrong weights to some appropriate risk variables. To the extent that these findings are also applicable to current MedisGroups scoring algorithms and to other conditions and procedures, MedisGroups admission severity groups cannot fairly adjust for interhospital case mix differences in outcome studies.",
      "journal": "JAMA",
      "year": "1991",
      "doi": "",
      "authors": "Blumberg M S",
      "keywords": "",
      "mesh_terms": "Aged; Aged, 80 and over; Algorithms; Bias; Hospitals; Humans; Length of Stay; Myocardial Infarction; Outcome and Process Assessment, Health Care; Probability; Professional Review Organizations; Random Allocation; Severity of Illness Index; United States",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/2033767/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "21097200",
      "title": "Non-iterative relative bias correction for 3D reconstruction of in utero fetal brain MR imaging.",
      "abstract": "The slice intersection motion correction (SIMC) method is a powerful tool to compensate for motion that occurs during in utero acquisition of the multislice magnetic resonance (MR) images of the human fetal brain. The SIMC method makes use of the slice intersection intensity profiles of orthogonally planned slice pairs to simultaneously correct for the relative motion occurring between all the acquired slices. This approach is based on the assumption that the bias field is consistent between slices. However, for some clinical studies where there is a strong bias field combined with significant fetal motion relative to the coils, this assumption is broken and the resulting motion estimate and the reconstruction to a 3D volume can both contain errors. In this work, we propose a method to correct for the relative differences in bias field between all slice pairs. For this, we define the energy function as the mean square difference of the intersection profiles, that is then minimized with respect to the bias field parameters of the slices. A non iterative method which considers the relative bias between each slice simultaneously is used to efficiently remove inconsistencies. The method, when tested on synthetic simulations and actual clinical imaging studies where bias was an issue, brought a significant improvement to the final reconstructed image.",
      "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference",
      "year": "2010",
      "doi": "10.1109/IEMBS.2010.5627876",
      "authors": "Kim Kio et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artifacts; Artificial Intelligence; Brain; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Pattern Recognition, Automated; Prenatal Diagnosis; Reproducibility of Results; Sensitivity and Specificity",
      "pub_types": "Evaluation Study; Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/21097200/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "21273023",
      "title": "Bias field reduction by localized Lloyd-Max quantization.",
      "abstract": "Bias field reduction is a common problem in medical imaging. A bias field usually manifests itself as a smooth intensity variation across the image. The resulting image inhomogeneity is a severe problem for posterior image processing and analysis techniques such as registration or segmentation. In this article, we present a novel debiasing technique based on localized Lloyd-Max quantization (LMQ). The local bias is modeled as a multiplicative field and is assumed to be slowly varying. The method is based on the assumption that the global, undegraded histogram is characterized by a limited number of gray values. The goal is then to find the discrete intensity values such that spreading those values according to the local bias field reproduces the global histogram as good as possible. We show that our method is capable of efficiently reducing (even strong) bias fields in 3D volumes.",
      "journal": "Magnetic resonance imaging",
      "year": "2011",
      "doi": "10.1016/j.mri.2010.10.015",
      "authors": "Mai Zhenhua et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Computer Simulation; Humans; Image Enhancement; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Models, Statistical; Normal Distribution; Pattern Recognition, Automated; Phantoms, Imaging; Reproducibility of Results",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/21273023/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "2220312",
      "title": "Thermal discrimination thresholds: a comparison of different methods.",
      "abstract": "Thermal testing was carried out on 55 healthy subjects in order to establish normal results and reproducibility of warm and cold thresholds. Diurnal variations of thresholds were investigated in a further 30 normal subjects. Then the sensitivity of different testing procedures was investigated in 33 patients with diabetes mellitus, but without severe polyneuropathy. Forced choice testing takes 6 times longer than the method of limits, and the results are not considerably different. It is thought that the forced choice algorithm does not provide a method for clinical routine. Another new approach, the double random staircase method, may help to exclude bias without taking too much time.",
      "journal": "Acta neurologica Scandinavica",
      "year": "1990",
      "doi": "10.1111/j.1600-0404.1990.tb01015.x",
      "authors": "Claus D et al.",
      "keywords": "",
      "mesh_terms": "Adolescent; Adult; Aged; Circadian Rhythm; Diabetes Mellitus, Type 1; Diabetes Mellitus, Type 2; Diabetic Neuropathies; Female; Humans; Male; Middle Aged; Sensory Thresholds; Skin; Thermoreceptors; Thermosensing",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/2220312/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "22614791",
      "title": "Limited sampling strategies to estimate the area under the concentration-time curve. Biases and a proposed more accurate method.",
      "abstract": "BACKGROUND: Over 100 limited sampling strategies (LSSs) have been proposed to reduce the number of blood samples necessary to estimate the area under the concentration-time curve (AUC). The conditions under which these strategies succeed or fail remain to be clarified. OBJECTIVES: We investigated the accuracy of existing LSSs both theoretically and numerically by Monte Carlo simulation. We also proposed two new methods for more accurate AUC estimations. METHODS: We evaluated the following existing methods theoretically: i) nonlinear curve fitting algorithm (NLF), ii) the trapezium rule with exponential curve approximation (TZE), and iii) multiple linear regression (MLR). Taking busulfan (BU) as a test drug, we generated a set of theoretical concentration-time curves based on the identified distribution of pharmacokinetic parameters of BU and re-evaluated the existing LSSs using these virtual validation profiles. Based on the evaluation results, we improved the TZE so that unrealistic parameter values were not used. We also proposed a new estimation method in which the most likely curve was selected from a set of pre-generated theoretical concentration-time curves. RESULTS: Our evaluation, based on clinical profiles and a virtual validation set, revealed: i) NLF sometimes overestimated the absorption rate constant Ka, ii) TZE overestimated AUC over 280% when Ka is small, and iii) MLR underestimated AUC over 30% when the elimination rate constant Ke is small. These results were consistent with our mathematical evaluations for these methods. In contrast, our two new methods had little bias and good precision. CONCLUSIONS: Our investigation revealed that existing LSSs induce different but specific biases in the estimation of AUC. Our two new LSSs, a modified TZE and one using model concentration-time curves, provided accurate and precise estimations of AUC.",
      "journal": "Methods of information in medicine",
      "year": "2012",
      "doi": "10.3414/ME11-01-0071",
      "authors": "Tsuruta H et al.",
      "keywords": "",
      "mesh_terms": "Antineoplastic Agents, Alkylating; Area Under Curve; Busulfan; Models, Statistical; Monte Carlo Method; Selection Bias",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/22614791/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "23990287",
      "title": "Evaluation of point-of-care analyzers' ability to reduce bias in conductivity-based hematocrit measurement during cardiopulmonary bypass.",
      "abstract": "Most point-of-care testing analyzers use the conductivity method to measure hematocrit (hct). During open-heart surgery, blood-conductivity is influenced by shifts in electrolyte and colloid concentrations caused by infusion media used, and this may lead to considerable bias in the hct measurement. We evaluated to what extent different analyzers correcting for 0, 1, 2, or 3 factors, respectively, compensated for this electrolyte/colloid interference: (1) the conductivity method with no correction (IRMA), (2) with a [Na(+)]-correction (GEM Premier 3000), (3) with a [Na(+)]/[K(+)]-correction (i-STAT), and (4) with a [Na(+)]/[K(+)]-correction in combination with an algorithm that estimates the protein dilution [i-STAT in cardiopulmonary bypass (CPB)-mode]. Bias in hct was measured during three consecutive stages of a CPB procedure: (I) before CPB, (II) start of CPB and (III) after cardioplegia. In order of high to low electrolyte/colloid interference: the analyzer with no correction, [Na(+)]-correction, [Na(+)/]/[K(+)]-correction, and [Na(+)/]/[K(+)]/estimated protein-correction showed a change of bias from stage I to stage III of -3.9 \u00b1 0.5, -3.4 \u00b1 0.4, -2.1 \u00b1 0.5, -0.3 \u00b1 0.5%. We conclude that correcting for more parameters (Na(+), K(+), estimated protein) gives less bias, but residual bias remains even after [Na(+)/]/[K(+)]/estimated protein-correction. This suggests that a satisfactory algorithm should also correct for other colloidal factors than protein.",
      "journal": "Journal of clinical monitoring and computing",
      "year": "2014",
      "doi": "10.1007/s10877-013-9504-z",
      "authors": "Teerenstra Steven et al.",
      "keywords": "",
      "mesh_terms": "Artifacts; Cardiopulmonary Bypass; Conductometry; Equipment Design; Equipment Failure Analysis; Hematocrit; Humans; Monitoring, Intraoperative; Point-of-Care Systems; Reproducibility of Results; Sensitivity and Specificity",
      "pub_types": "Comparative Study; Evaluation Study; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/23990287/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "24982437",
      "title": "The impact of image reconstruction bias on PET/CT 90Y dosimetry after radioembolization.",
      "abstract": "UNLABELLED: PET/CT imaging after radioembolization is a viable method for determining the posttreatment (90)Y distribution in the liver. Low true-to-random coincidence ratios in (90)Y PET studies limit the quantitative accuracy of these studies when reconstruction algorithms optimized for traditional PET imaging are used. This study examined these quantitative limitations and assessed the feasibility of generating radiation dosimetry maps in liver regions with high and low (90)Y concentrations. METHODS: (90)Y PET images were collected on a PET/CT scanner and iteratively reconstructed with the vendor-supplied reconstruction algorithm. PET studies on a Jaszczak cylindric phantom were performed to determine quantitative accuracy and minimum detectable concentration (MDC). (90)Y and (18)F point-source studies were used to investigate the possible increase in detected random coincidence events due to bremsstrahlung photons. Retrospective quantitative analyses were performed on (90)Y PET/CT images obtained after 65 right or left hepatic artery radioembolizations in 59 patients. Quantitative image errors were determined by comparing the measured image activity with the assayed (90)Y activity. PET images were converted to dose maps through convolution with voxel S values generated using MCNPX, a Monte Carlo N-particle transport code system for multiparticle and high-energy applications. Tumor and parenchyma doses and potential bias based on measurements found below the MDC were recorded. RESULTS: Random coincidences were found to increase in (90)Y acquisitions, compared with (18)F acquisitions, at similar positron emission rates because of bremsstrahlung photons. Positive bias was observed in all images. Quantitative accuracy was achieved for phantom inserts above the MDC of 1 MBq/mL. The mean dose to viable tumors was 183.6 \u00b1 156.5 Gy, with an average potential bias of 3.3 \u00b1 6.4 Gy. The mean dose to the parenchyma was 97.1 \u00b1 22.1 Gy, with an average potential bias of 8.9 \u00b1 4.9 Gy. CONCLUSION: The low signal-to-noise ratio caused by low positron emission rates and high bremsstrahlung photon production resulted in a positive bias on (90)Y PET images reconstructed with conventional iterative algorithms. However, quantitative accuracy was good at high activity concentrations, such as those found in tumor volumes, allowing for adequate tumor (90)Y PET/CT dosimetry after radioembolization.",
      "journal": "Journal of nuclear medicine : official publication, Society of Nuclear Medicine",
      "year": "2014",
      "doi": "10.2967/jnumed.113.133629",
      "authors": "Tapp Katie N et al.",
      "keywords": "90Y dosimetry; quantitative PET/CT; reconstruction bias",
      "mesh_terms": "Embolization, Therapeutic; Humans; Image Processing, Computer-Assisted; Phantoms, Imaging; Positron-Emission Tomography; Radiometry; Tomography, X-Ray Computed; Yttrium Radioisotopes",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/24982437/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "25164868",
      "title": "Complementary frame reconstruction: a low-biased dynamic PET technique for low count density data in projection space.",
      "abstract": "A new data handling method is presented for improving the image noise distribution and reducing bias when reconstructing very short frames from low count dynamic PET acquisition. The new method termed 'Complementary Frame Reconstruction' (CFR) involves the indirect formation of a count-limited emission image in a short frame through subtraction of two frames with longer acquisition time, where the short time frame data is excluded from the second long frame data before the reconstruction. This approach can be regarded as an alternative to the AML algorithm recently proposed by Nuyts et al, as a method to reduce the bias for the maximum likelihood expectation maximization (MLEM) reconstruction of count limited data. CFR uses long scan emission data to stabilize the reconstruction and avoids modification of algorithms such as MLEM. The subtraction between two long frame images, naturally allows negative voxel values and significantly reduces bias introduced in the final image. Simulations based on phantom and clinical data were used to evaluate the accuracy of the reconstructed images to represent the true activity distribution. Applicability to determine the arterial input function in human and small animal studies is also explored. In situations with limited count rate, e.g. pediatric applications, gated abdominal, cardiac studies, etc., or when using limited doses of short-lived isotopes such as 15O-water, the proposed method will likely be preferred over independent frame reconstruction to address bias and noise issues.",
      "journal": "Physics in medicine and biology",
      "year": "2014",
      "doi": "10.1088/0031-9155/59/18/5441",
      "authors": "Hong Inki et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Animals; Humans; Phantoms, Imaging; Positron-Emission Tomography",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/25164868/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "25599724",
      "title": "Female pseudohermaphroditism: strategy and bias in a fast diagnosis. How tricky could be a diagnosis with a wrong anamnesis.",
      "abstract": "AIM: Congenital genitalia anomalies are a spectrum of malformation, difficult to classify because similar or identical phenotypes could have several different aetiology; therefore it's essential to assess an efficient diagnostic algorithm for a quick diagnosis and to develop an efficient therapeutic strategy. The aim of this study is to underline the importance of imaging in case of ambiguous genitalia due to its high sensitivity and specificity in detecting internal organs and urogenital anatomy. MATERIAL OF STUDY: We report a case of a young girl affected by a complex genitor-urinary malformation with an initial wrong anamnesis that led to a tricky diagnosis. RESULTS: Imaging techniques - especially Magnetic Resonance Imaging (MRI) - together with karyotype, hormones and physical investigations, offered complete and reliable informations for the best surgical treatment of our patient. CONCLUSION: Karyotype, hormones investigation, and radiological examinations are the main criteria considered in the diagnostic iter. Ultrasonography (US) is the primary modality for the detection of the presence or absence of gonads and m\u00fcllerian derivatives, whereas Cystourethrography can define urethral and vaginal tract or the presence of fistulas. In our experience MRI, due to its multiplanar capability and superior soft tissue characterization, proved to be useful to provide detailed anatomic information.",
      "journal": "Annali italiani di chirurgia",
      "year": "2014",
      "doi": "",
      "authors": "Onesti Maria Giuseppina et al.",
      "keywords": "",
      "mesh_terms": "46, XX Disorders of Sex Development; Adolescent; Bias; Female; Humans; Karyotype; Magnetic Resonance Imaging; Medical History Taking; Predictive Value of Tests; Sensitivity and Specificity; Tomography, X-Ray Computed; Ultrasonography; Vagina",
      "pub_types": "Case Reports; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/25599724/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "25911333",
      "title": "A permutation test to analyse systematic bias and random measurement errors of medical devices via boosting location and scale models.",
      "abstract": "Measurement errors of medico-technical devices can be separated into systematic bias and random error. We propose a new method to address both simultaneously via generalized additive models for location, scale and shape (GAMLSS) in combination with permutation tests. More precisely, we extend a recently proposed boosting algorithm for GAMLSS to provide a test procedure to analyse potential device effects on the measurements. We carried out a large-scale simulation study to provide empirical evidence that our method is able to identify possible sources of systematic bias as well as random error under different conditions. Finally, we apply our approach to compare measurements of skin pigmentation from two different devices in an epidemiological study.",
      "journal": "Statistical methods in medical research",
      "year": "2017",
      "doi": "10.1177/0962280215581855",
      "authors": "Mayr Andreas et al.",
      "keywords": "Measurement errors; gradient boosting; permutation test; random error; regression; statistical models; systematic bias",
      "mesh_terms": "Algorithms; Bias; Colorimetry; Equipment Design; Equipment and Supplies; Female; Humans; Male; Models, Statistical; Regression Analysis; Research Design; Skin Pigmentation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/25911333/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "26865273",
      "title": "Analysis of a Rapid Evolutionary Radiation Using Ultraconserved Elements: Evidence for a Bias in Some Multispecies Coalescent Methods.",
      "abstract": "Rapid evolutionary radiations are expected to require large amounts of sequence data to resolve. To resolve these types of relationships many systematists believe that it will be necessary to collect data by next-generation sequencing (NGS) and use multispecies coalescent (\"species tree\") methods. Ultraconserved element (UCE) sequence capture is becoming a popular method to leverage the high throughput of NGS to address problems in vertebrate phylogenetics. Here we examine the performance of UCE data for gallopheasants (true pheasants and allies), a clade that underwent a rapid radiation 10-15 Ma. Relationships among gallopheasant genera have been difficult to establish. We used this rapid radiation to assess the performance of species tree methods, using \u223c600 kilobases of DNA sequence data from \u223c1500 UCEs. We also integrated information from traditional markers (nuclear intron data from 15 loci and three mitochondrial gene regions). Species tree methods exhibited troubling behavior. Two methods [Maximum Pseudolikelihood for Estimating Species Trees (MP-EST) and Accurate Species TRee ALgorithm (ASTRAL)] appeared to perform optimally when the set of input gene trees was limited to the most variable UCEs, though ASTRAL appeared to be more robust than MP-EST to input trees generated using less variable UCEs. In contrast, the rooted triplet consensus method implemented in Triplec performed better when the largest set of input gene trees was used. We also found that all three species tree methods exhibited a surprising degree of dependence on the program used to estimate input gene trees, suggesting that the details of likelihood calculations (e.g., numerical optimization) are important for loci with limited phylogenetic information. As an alternative to summary species tree methods we explored the performance of SuperMatrix Rooted Triple - Maximum Likelihood (SMRT-ML), a concatenation method that is consistent even when gene trees exhibit topological differences due to the multispecies coalescent. We found that SMRT-ML performed well for UCE data. Our results suggest that UCE data have excellent prospects for the resolution of difficult evolutionary radiations, though specific attention may need to be given to the details of the methods used to estimate species trees.",
      "journal": "Systematic biology",
      "year": "2016",
      "doi": "10.1093/sysbio/syw014",
      "authors": "Meiklejohn Kelly A et al.",
      "keywords": "Galliformes, gene tree estimation error; Phasianidae; Triplec; polytomy; supermatrix rooted triplets; total evidence",
      "mesh_terms": "Biological Evolution; Classification; High-Throughput Nucleotide Sequencing; Models, Biological; Phylogeny; Probability",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/26865273/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "27271102",
      "title": "Can Bias Evaluation Provide Protection Against False-Negative Results in QT Studies Without a Positive Control Using Exposure-Response Analysis?",
      "abstract": "The revised ICH E14 document allows the use of exposure-response analysis to exclude a small QT effect of a drug. If plasma concentrations exceeding clinically relevant levels is achieved, a positive control is not required. In cases when this cannot be achieved, there may be a need for metrics to protect against false-negative results. The objectives of this study were to create bias in electrocardiogram laboratory QT-interval measurements and define a metric that can be used to detect bias severe enough to cause false-negative results using exposure-response analysis. Data from the IQ-CSRC study, which evaluated the QT effect of 5 QT-prolonging drugs, were used. Negative bias using 3 deterministic and 2 random methods was introduced into the reported QTc values and compared with fully automated data from the underlying electrocardiogram algorithm (COMPAS). The slope estimate of the Bland-Altman plot was used as a bias metric. With the deterministic bias methods, negative bias, measured between electrocardiogram laboratory values and COMPAS, had to be larger than approximately -20 milliseconds over a QTcF range of 100 milliseconds to cause failures to predict the QT effect of ondansetron, quinine, dolasetron, moxifloxacin, and dofetilide. With the random methods, the rate of false-negatives was \u22645% with bias severity < -10 milliseconds for all 5 drugs when plasma levels exceeded those of interest. Severe and therefore detectable bias has to be introduced into reported QTc values to cause false-negative predictions with exposure-response analysis.",
      "journal": "Journal of clinical pharmacology",
      "year": "2017",
      "doi": "10.1002/jcph.779",
      "authors": "Ferber Georg et al.",
      "keywords": "QT; bias; early phase; exposure response analysis; first-in-human; positive control",
      "mesh_terms": "Cardiotoxins; Dose-Response Relationship, Drug; Electrocardiography; Heart Conduction System; Heart Rate; Humans; Long QT Syndrome",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/27271102/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "27411847",
      "title": "Estimating causal contrasts involving intermediate variables in the presence of selection bias.",
      "abstract": "An important goal across the biomedical and social sciences is the quantification of the role of intermediate factors in explaining how an exposure exerts an effect on an outcome. Selection bias has the potential to severely undermine the validity of inferences on direct and indirect causal effects in observational as well as in randomized studies. The phenomenon of selection may arise through several mechanisms, and we here focus on instances of missing data. We study the sign and magnitude of selection bias in the estimates of direct and indirect effects when data on any of the factors involved in the analysis is either missing at random or not missing at random. Under some simplifying assumptions, the bias formulae can lead to nonparametric sensitivity analyses. These sensitivity analyses can be applied to causal effects on the risk difference and risk-ratio scales irrespectively of the estimation approach employed. To incorporate parametric assumptions, we also develop a sensitivity analysis for selection bias in mediation analysis in the spirit of the expectation-maximization algorithm. The approaches are applied to data from a health disparities study investigating the role of stage at diagnosis on racial disparities in colorectal cancer survival. Copyright \u00a9 2016 John Wiley & Sons, Ltd.",
      "journal": "Statistics in medicine",
      "year": "2016",
      "doi": "10.1002/sim.7025",
      "authors": "Valeri Linda et al.",
      "keywords": "EM algorithm; controlled direct effects; mediation analysis; missing at random; natural direct and indirect effects; not missing at random; selection bias; sensitivity analyses",
      "mesh_terms": "Bias; Humans; Randomized Controlled Trials as Topic; Risk; Selection Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/27411847/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "28114009",
      "title": "A New Variational Method for Bias Correction and Its Applications to Rodent Brain Extraction.",
      "abstract": "Brain extraction is an important preprocessing step for further analysis of brain MR images. Significant intensity inhomogeneity can be observed in rodent brain images due to the high-field MRI technique. Unlike most existing brain extraction methods that require bias corrected MRI, we present a high-order and L0 regularized variational model for bias correction and brain extraction. The model is composed of a data fitting term, a piecewise constant regularization and a smooth regularization, which is constructed on a 3-D formulation for medical images with anisotropic voxel sizes. We propose an efficient multi-resolution algorithm for fast computation. At each resolution layer, we solve an alternating direction scheme, all subproblems of which have the closed-form solutions. The method is tested on three T2 weighted acquisition configurations comprising a total of 50 rodent brain volumes, which are with the acquisition field strengths of 4.7 Tesla, 9.4 Tesla and 17.6 Tesla, respectively. On one hand, we compare the results of bias correction with N3 and N4 in terms of the coefficient of variations on 20 different tissues of rodent brain. On the other hand, the results of brain extraction are compared against manually segmented gold standards, BET, BSE and 3-D PCNN based on a number of metrics. With the high accuracy and efficiency, our proposed method can facilitate automatic processing of large-scale brain studies.",
      "journal": "IEEE transactions on medical imaging",
      "year": "2017",
      "doi": "10.1109/TMI.2016.2636026",
      "authors": "Chang Huibin et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Animals; Brain; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Mice; Mice, Inbred C57BL; Phantoms, Imaging",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/28114009/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "28615228",
      "title": "Stronger Together: Aggregated Z-values of Traditional Quality Control Measurements and Patient Medians Improve Detection of Biases.",
      "abstract": "BACKGROUND: In clinical chemistry, quality control (QC) often relies on measurements of control samples, but limitations, such as a lack of commutability, compromise the ability of such measurements to detect out-of-control situations. Medians of patient results have also been used for QC purposes, but it may be difficult to distinguish changes observed in the patient population from analytical errors. This study aims to combine traditional control measurements and patient medians for facilitating detection of biases. METHODS: The software package \"rSimLab\" was developed to simulate measurements of 5 analytes. Internal QC measurements and patient medians were assessed for detecting impermissible biases. Various control rules combined these parameters. A Westgard-like algorithm was evaluated and new rules that aggregate Z-values of QC parameters were proposed. RESULTS: Mathematical approximations estimated the required sample size for calculating meaningful patient medians. The appropriate number was highly dependent on the ratio of the spread of sample values to their center. Instead of applying a threshold to each QC parameter separately like the Westgard algorithm, the proposed aggregation of Z-values averaged these parameters. This behavior was found beneficial, as a bias could affect QC parameters unequally, resulting in differences between their Z-transformed values. In our simulations, control rules tended to outperform the simple QC parameters they combined. The inclusion of patient medians substantially improved bias detection for some analytes. CONCLUSIONS: Patient result medians can supplement traditional QC, and aggregations of Z-values are novel and beneficial tools for QC strategies to detect biases.",
      "journal": "Clinical chemistry",
      "year": "2017",
      "doi": "10.1373/clinchem.2016.269845",
      "authors": "Bietenbeck Andreas et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Bias; Chemistry, Clinical; Humans; Laboratories; Quality Control; Software",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/28615228/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "29253575",
      "title": "Brain extraction in partial volumes T2*@7T by using a quasi-anatomic segmentation with bias field correction.",
      "abstract": "BACKGROUND: Poor brain extraction in Magnetic Resonance Imaging (MRI) has negative consequences in several types of brain post-extraction such as tissue segmentation and related statistical measures or pattern recognition algorithms. Current state of the art algorithms for brain extraction work on weighted T1 and T2, being not adequate for non-whole brain images such as the case of T2*FLASH@7T partial volumes. NEW METHOD: This paper proposes two new methods that work directly in T2*FLASH@7T partial volumes. The first is an improvement of the semi-automatic threshold-with-morphology approach adapted to incomplete volumes. The second method uses an improved version of a current implementation of the fuzzy c-means algorithm with bias correction for brain segmentation. RESULTS: Under high inhomogeneity conditions the performance of the first method degrades, requiring user intervention which is unacceptable. The second method performed well for all volumes, being entirely automatic. COMPARISON WITH EXISTING METHODS: State of the art algorithms for brain extraction are mainly semi-automatic, requiring a correct initialization by the user and knowledge of the software. These methods can't deal with partial volumes and/or need information from atlas which is not available in T2*FLASH@7T. Also, combined volumes suffer from manipulations such as re-sampling which deteriorates significantly voxel intensity structures making segmentation tasks difficult. The proposed method can overcome all these difficulties, reaching good results for brain extraction using only T2*FLASH@7T volumes. CONCLUSIONS: The development of this work will lead to an improvement of automatic brain lesions segmentation in T2*FLASH@7T volumes, becoming more important when lesions such as cortical Multiple-Sclerosis need to be detected.",
      "journal": "Journal of neuroscience methods",
      "year": "2018",
      "doi": "10.1016/j.jneumeth.2017.12.006",
      "authors": "Valente Jo\u00e3o et al.",
      "keywords": "Brain extraction; High resolution MRI; Multiple sclerosis; Partial brain scanning",
      "mesh_terms": "Algorithms; Brain; Fuzzy Logic; Head; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Movement; Pattern Recognition, Automated; Software",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29253575/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "29267847",
      "title": "Adjusting for bias introduced by instrumental variable estimation in the Cox proportional hazards model.",
      "abstract": "Instrumental variable (IV) methods are widely used for estimating average treatment effects in the presence of unmeasured confounders. However, the capability of existing IV procedures, and most notably the two-stage residual inclusion (2SRI) algorithm recommended for use in non-linear contexts, to account for unmeasured confounders in the Cox proportional hazard model is unclear. We show that instrumenting an endogenous treatment induces an unmeasured covariate, referred to as an individual frailty in survival analysis parlance, which if not accounted for leads to bias. We propose a new procedure that augments 2SRI with an individual frailty and prove that it is consistent under certain conditions. The finite sample-size behavior is studied across a broad set of conditions via Monte Carlo simulations. Finally, the proposed methodology is used to estimate the average effect of carotid endarterectomy versus carotid stenting on the mortality of patients suffering from carotid artery disease. Results suggest that the 2SRI-frailty estimator generally reduces the bias of both point and interval estimators compared to traditional 2SRI.",
      "journal": "Biostatistics (Oxford, England)",
      "year": "2019",
      "doi": "10.1093/biostatistics/kxx062",
      "authors": "Mart\u00ednez-Camblor Pablo et al.",
      "keywords": "",
      "mesh_terms": "Bias; Biostatistics; Data Interpretation, Statistical; Humans; Monte Carlo Method; Proportional Hazards Models",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29267847/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "29281263",
      "title": "Correction for the Hematocrit Bias in Dried Blood Spot Analysis Using a Nondestructive, Single-Wavelength Reflectance-Based Hematocrit Prediction Method.",
      "abstract": "The hematocrit (Hct) effect is one of the most important hurdles currently preventing more widespread implementation of quantitative dried blood spot (DBS) analysis in a routine context. Indeed, the Hct may affect both the accuracy of DBS methods as well as the interpretation of DBS-based results. We previously developed a method to determine the Hct of a DBS based on its hemoglobin content using noncontact diffuse reflectance spectroscopy. Despite the ease with which the analysis can be performed (i.e., mere scanning of the DBS) and the good results that were obtained, the method did require a complicated algorithm to derive the total hemoglobin content from the DBS's reflectance spectrum. As the total hemoglobin was calculated as the sum of oxyhemoglobin, methemoglobin, and hemichrome, the three main hemoglobin derivatives formed in DBS upon aging, the reflectance spectrum needed to be unmixed to determine the quantity of each of these derivatives. We now simplified the method by only using the reflectance at a single wavelength, located at a quasi-isosbestic point in the reflectance curve. At this wavelength, assuming 1-to-1 stoichiometry of the aging reaction, the reflectance is insensitive to the hemoglobin degradation and only scales with the total amount of hemoglobin and, hence, the Hct. This simplified method was successfully validated. At each quality control level as well as at the limits of quantitation (i.e., 0.20 and 0.67) bias, intra- and interday imprecision were within 10%. Method reproducibility was excellent based on incurred sample reanalysis and surpassed the reproducibility of the original method. Furthermore, the influence of the volume spotted, the measurement location within the spot, as well as storage time and temperature were evaluated, showing no relevant impact of these parameters. Application to 233 patient samples revealed a good correlation between the Hct determined on whole blood and the predicted Hct determined on venous DBS. The bias obtained with Bland and Altman analysis was -0.015 and the limits of agreement were -0.061 and 0.031, indicating that the simplified, noncontact Hct prediction method even outperforms the original method. In addition, using caffeine as a model compound, it was demonstrated that this simplified Hct prediction method can effectively be used to implement a Hct-dependent correction factor to DBS-based results to alleviate the Hct bias.",
      "journal": "Analytical chemistry",
      "year": "2018",
      "doi": "10.1021/acs.analchem.7b03784",
      "authors": "Capiau Sara et al.",
      "keywords": "",
      "mesh_terms": "Adult; Algorithms; Dried Blood Spot Testing; Hematocrit; Humans; Linear Models; Reproducibility of Results; Spectrophotometry",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29281263/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "29677939",
      "title": "Estimating a Bias in ICD Encodings for Billing Purposes.",
      "abstract": "ICD encoded diagnoses are a popular criterion for eligibility algorithms for study cohort recruitment. However, \"official\" ICD encoded diagnoses used for billing purposes are afflicted with a bias originating from legal issues. This work presents an approach to estimate the degree of the encoding bias for the complete ICD catalogue at a German university hospital. The free text diagnoses sections of discharge letters are automatically classified using a supervised machine learning algorithm. The automatic classifications are compared with the official, manually classified codes. For selected ICD codes the approach works sufficiently well.",
      "journal": "Studies in health technology and informatics",
      "year": "2018",
      "doi": "",
      "authors": "Fette Georg et al.",
      "keywords": "ICD; classification; machine learning; natural language processing",
      "mesh_terms": "Algorithms; Bias; Humans; International Classification of Diseases; Patient Discharge; Supervised Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29677939/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "30064041",
      "title": "Potential of near infrared spectroscopy and pattern recognition for rapid discrimination and quantification of Gleditsia sinensis thorn powder with adulterants.",
      "abstract": "The Gleditsia sinensis Lam thorn (GST) is a classical traditional Chinese medical herb, which is of high medical and economic value. GST could be easily adulterated with branch of Rosa multiflora thunb (BRM) and Rosa rugosa thumb (BRR), because of their similar appearances and much lower cost for these adulterants. In this study Fourier transform near-infrared spectroscopy (FT-NIR) combined with chemical pattern recognition techniques was explored for the first time to discriminate and quantify of cheaper materials (BRM and BRR) in GST. The Savitzkye-Golay (SG) smoothing, vector normalization (VN), min max normalization (MMN), first derivative (1\u202fst D) and second derivative (2nd D) methods were used to pre-process the raw FT-NIR spectra. Successive projections algorithm was adopted to select the characteristic variables and linear discriminate analysis (LDA), support vector machine (SVM), as while as back propagation neural network (BPNN) algorithms were applied to construct the identification models. Results showed that BPNN models performance best compared with LDA and SVM models for it could reach 100% accuracy for identifying authentic GST, and GST adulterated with BRM and BRR based on the spectral region of 6500-5500 cm-1 combined with 1\u202fst D pre-processing. In addition, the BRM and BRR content in adulterated GST were determined by partial least squares (PLS) regression. The correlation coefficient of prediction (rp), root mean square error of prediction (RMSEP) and bias for the prediction by PLS regression model were 0.9972, 1.969% and 0.3198 for BRM, 0.9972, 1.879% and 0.05408 for BRR, respectively. These results suggest that the combination of NIR spectroscopy and chemometric methods offers a simple, fast and reliable method for classification and quantification in the quality control of the tradition Chinese medicine herb of GST.",
      "journal": "Journal of pharmaceutical and biomedical analysis",
      "year": "2018",
      "doi": "10.1016/j.jpba.2018.07.036",
      "authors": "Wang Lijun et al.",
      "keywords": "BPNN; Gleditsia sinensis thorn; LDA; Near-infrared spectroscopy; PLS regression; SVM",
      "mesh_terms": "Drug Contamination; Gleditsia; Models, Chemical; Neural Networks, Computer; Powders; Quality Control; Reproducibility of Results; Spectroscopy, Near-Infrared; Support Vector Machine; Technology, Pharmaceutical",
      "pub_types": "Journal Article; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30064041/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "30195425",
      "title": "Attentional bias in MDD: ERP components analysis and classification using a dot-probe task.",
      "abstract": "BACKGROUND AND OBJECTIVE: Strands of evidence have supported existence of negative attentional bias in patients with depression. This study aimed to assess the behavioral and electrophysiological signatures of attentional bias in major depressive disorder (MDD) and explore whether ERP components contain valuable information for discriminating between MDD patients and healthy controls (HCs). METHODS: Electroencephalography data were collected from 17 patients with MDD and 17 HCs in a dot-probe task, with emotional-neutral pairs as experimental materials. Fourteen features related to ERP waveform shape were generated. Then, Correlated Feature Selection (CFS), ReliefF and GainRatio (GR) were applied for feature selection. For discriminating between MDDs and HCs, k-nearest neighbor (KNN), C4.5, Sequential Minimal Optimization (SMO) and Logistic Regression (LR) were used. RESULTS: Behaviorally, MDD patients showed significantly shorter reaction time (RT) to valid than invalid sad trials, with significantly higher bias score for sad-neutral pairs. Analysis of split-half reliability in RT indices indicated a strong reliability in RT, while coefficients of RT bias scores neared zero. These behavioral effects were supported by ERP results. MDD patients had higher P300 amplitude with the probe replacing a sad face than a neutral face, indicating difficult attention disengagement from negative emotional faces. Meanwhile, data mining analysis based on ERP components suggested that CFS was the best feature selection algorithm. Especially for the P300 induced by valid sad trials, the classification accuracy of CFS combination with any classifier was above 85%, and the KNN (k\u202f=\u202f3) classifier achieved the highest accuracy (94%). CONCLUSIONS: MDD patients show difficulty in attention disengagement from negative stimuli, reflected by P300. The CFS over other methods leads to a good overall performance in most cases, especially when KNN classifier is used for P300 component classification, illustrating that ERP component may be applied as a tool for auxiliary diagnosis of depression.",
      "journal": "Computer methods and programs in biomedicine",
      "year": "2018",
      "doi": "10.1016/j.cmpb.2018.07.003",
      "authors": "Li Xiaowei et al.",
      "keywords": "Attentional bias; Classification; Event-related potentials; Feature selection; Major depressive disorder",
      "mesh_terms": "Adult; Algorithms; Attentional Bias; Case-Control Studies; Major Depressive Disorder; Diagnosis, Computer-Assisted; Electroencephalography; Event-Related Potentials, P300; Evoked Potentials; Facial Expression; Female; Humans; Male; Photic Stimulation; Reaction Time; Young Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30195425/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "30508423",
      "title": "Machine Learning, Health Disparities, and Causal Reasoning.",
      "abstract": "",
      "journal": "Annals of internal medicine",
      "year": "2018",
      "doi": "10.7326/M18-3297",
      "authors": "Goodman Steven N et al.",
      "keywords": "",
      "mesh_terms": "Health Equity; Machine Learning; Problem Solving",
      "pub_types": "Editorial; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30508423/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "30794127",
      "title": "Can AI Help Reduce Disparities in General Medical and Mental Health Care?",
      "abstract": "BACKGROUND: As machine learning becomes increasingly common in health care applications, concerns have been raised about bias in these systems' data, algorithms, and recommendations. Simply put, as health care improves for some, it might not improve for all. METHODS: Two case studies are examined using a machine learning algorithm on unstructured clinical and psychiatric notes to predict intensive care unit (ICU) mortality and 30-day psychiatric readmission with respect to race, gender, and insurance payer type as a proxy for socioeconomic status. RESULTS: Clinical note topics and psychiatric note topics were heterogenous with respect to race, gender, and insurance payer type, which reflects known clinical findings. Differences in prediction accuracy and therefore machine bias are shown with respect to gender and insurance type for ICU mortality and with respect to insurance policy for psychiatric 30-day readmission. CONCLUSIONS: This analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care.",
      "journal": "AMA journal of ethics",
      "year": "2019",
      "doi": "10.1001/amajethics.2019.167",
      "authors": "Chen Irene Y et al.",
      "keywords": "",
      "mesh_terms": "Adult; Aged; Aged, 80 and over; Artificial Intelligence; Delivery of Health Care; Female; Healthcare Disparities; Humans; Intensive Care Units; Male; Mental Health Services; Middle Aged; Mortality; Patient Readmission; Sex Factors",
      "pub_types": "Comparative Study; Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30794127/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "30841773",
      "title": "Bias-corrected estimates of reduction of post-surgery length of stay and corresponding cost savings through the widespread national implementation of fast-tracking after liver transplantation: a quasi-experimental study.",
      "abstract": "Background: Fast-tracking is an approach adopted by Mayo Clinic in Florida's (MCF) liver transplant (LT) program, which consists of early tracheal extubation and transfer of patients to surgical ward, eliminating a stay in the intensive care unit in select patients. Since adopting this approach in 2002, MCF has successfully fast-tracked 54.3% of patients undergoing LT. Objectives: This study evaluated the reduction in post-operative length of stay (LOS) that resulted from the fast-tracking protocol and assessed the potential cost saving in the case of nationwide implementation. Methods: A propensity score for fast-tracking was generated based on MCF liver transplant databases during 2011-2013. Various propensity score matching algorithms were used to form control groups from the United Network of Organ Sharing Standard Analysis and Research (STAR) file that had comparable demographic characteristics and health status to the treatment group identified in MCF. Multiple regression and matching estimators were employed for evaluation of the post-surgery LOS. The algorithm generated from the analysis was also applied to the STAR data to determine the proportion of patients in the US who could potentially be candidates for fast-tracking, and the potential savings. Results: The effect of the fast-tracking on the post-transplant LOS was estimated at approximately from 2.5 (p-value\u2009=\u20090.001) to 3.2 (p-value\u2009<\u20090.001) days based on various matching algorithms. The cost saving from a nationwide implementation of fast-tracking of liver transplant patients was estimated to be at least $78 million during the 2-year period. Conclusion: The fast-track program was found to be effective in reducing post-transplant LOS, although the reduction appeared to be less than previously reported. Nationwide implementation of fast-tracking could result in substantial cost savings without compromising the patient outcome.",
      "journal": "Journal of medical economics",
      "year": "2019",
      "doi": "10.1080/13696998.2019.1592179",
      "authors": "Loh Chung-Ping A et al.",
      "keywords": "C40; C90; Fast-tracking; I11; I19; length of stay; liver transplant; matching; propensity score; quasi-experimental study",
      "mesh_terms": "Academic Medical Centers; Age Factors; Cohort Studies; Cost Savings; Databases, Factual; Early Ambulation; Female; Florida; Humans; Intensive Care Units; Length of Stay; Liver Transplantation; Logistic Models; Male; Middle Aged; Multivariate Analysis; Postoperative Care; Retrospective Studies; Risk Factors; Selection Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30841773/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "31241574",
      "title": "Algorithmic Bias and Computer-Assisted Scoring of Patient Notes in the USMLE Step 2 Clinical Skills Exam.",
      "abstract": "",
      "journal": "Academic medicine : journal of the Association of American Medical Colleges",
      "year": "2019",
      "doi": "10.1097/ACM.0000000000002746",
      "authors": "Spadafore Maxwell et al.",
      "keywords": "",
      "mesh_terms": "Bias; Clinical Competence; Educational Measurement; Humans; Licensure, Medical; Natural Language Processing",
      "pub_types": "Letter; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31241574/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "31278181",
      "title": "Benefits, Pitfalls, and Potential Bias in Health Care AI.",
      "abstract": "As the health care industry adopts artificial intelligence, machine learning, and other modeling techniques, it is seeing benefits to both patient outcomes and cost reduction; however, it needs to be cognizant of and ensure proper management of the risks, including bias. Lessons learned from other industries may provide a framework for acknowledging and managing data, machine, and human biases that arise while implementing AI.",
      "journal": "North Carolina medical journal",
      "year": "2019",
      "doi": "10.18043/ncm.80.4.219",
      "authors": "Hague Douglas C",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Bias; Data Analysis; Delivery of Health Care; Humans; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31278181/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "31313407",
      "title": "Diffusion gradient nonlinearity bias correction reduces bias of breast cancer bone metastasis ADC values.",
      "abstract": "CONTRACT GRANT SPONSOR: Health Research Fund of Central Denmark Region. BACKGROUND: Diffusion gradient nonlinearity (DGNL) bias causes apparent diffusion coefficient (ADC) values to drop with increasing superior-inferior (SI) isocenter offset. This is a concern when performing quantitative diffusion-weighted imaging (DWI). PURPOSE/HYPOTHESIS: To investigate if DGNL ADC bias can be corrected in breast cancer bone metastases using a clinical DWI protocol and an online correction algorithm. STUDY TYPE: Prospective. SUBJECTS/PHANTOM: A diffusion phantom (Model 128, High Precision Devices, Boulder, CO) was used for in vitro validation. Twenty-three women with bone-metastasizing breast cancer were enrolled to assess DGNL correction in vivo. FIELD STRENGTH/SEQUENCE: DWI was performed on a 1.5T MRI system as single-shot, spin-echo, echo-planar imaging with short-tau inversion recovery (STIR) fat-saturation. ADC maps with and without DGNL correction were created from the b50 and b800 images. ASSESSMENT: Uncorrected and DGNL-corrected ADC values were measured in phantom and bone metastases by placing regions of interest on b800 images and copying them to the ADC map. The SI offset was recorded. STATISTICAL TESTS: In all, 79 bone metastases were assessed. ADC values with and without DGNL correction were compared at 14 cm SI offset using a two-tailed t-test. RESULTS: In the diffusion phantom, DGNL correction increased SI offset, where ADC bias was lower than 5%, from 7.3-13.8 cm. Of the 23 patients examined, six had no metastases in the covered regions. In the remaining patients, bias of uncorrected bone metastasis ADC values was 19.1% (95% confidence interval [CI]: 15.4-22.9%) at 14 cm SI offset. After DGNL correction, ADC bias was significantly reduced to 3.5% (95% CI: 0.7-6.3%, P\u2009<\u20090.001), thus reducing bias due to DGNL by 82%. DATA CONCLUSION: Online DGNL correction corrects DGNL ADC value bias and allows increased station lengths in the SI direction. LEVEL OF EVIDENCE: 2 Technical Efficacy: Stage 2 J. Magn. Reson. Imaging 2020;51:904-911.",
      "journal": "Journal of magnetic resonance imaging : JMRI",
      "year": "2020",
      "doi": "10.1002/jmri.26873",
      "authors": "Buus Thomas W et al.",
      "keywords": "bone marrow diseases; breast neoplasms; diffusion magnetic resonance imaging; software validation",
      "mesh_terms": "Breast Neoplasms; Diffusion Magnetic Resonance Imaging; Female; Humans; Image Interpretation, Computer-Assisted; Prospective Studies; Reproducibility of Results",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31313407/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "31352179",
      "title": "Learning the implicit strain reconstruction in ultrasound elastography using privileged information.",
      "abstract": "Quasi-static ultrasound elastography is an importance imaging technology to assess the conditions of various diseases through reconstructing the tissue strain from radio frequency data. State-of-the-art strain reconstruction techniques suffer from the inexperienced user unfriendliness, high model bias, and low effectiveness-to-efficiency ratio. The three challenges result from the explicitness characteristic (i.e. explicit formulation of the reconstruction model) in these techniques. For these challenges, we are the first to develop an implicit strain reconstruction framework by a deep neural network architecture. However, the classic neural network methods are unsuitable to the strain reconstruction task because they are difficult to impose any direct influence on the intermediate state of the learning process. This may lead the map learned by the neural network to be biased with the desired map. In order to correct the intermediate state of the learning process, our framework proposes the learning-using-privileged-information (LUPI) paradigm with causality in the network. It provides the causal privileged information besides the training examples to help the network learning, while makes these privileged information unavailable at the test stage. This improvement can narrow the search region of the map learned by the network, and thus prompts the network to evolve towards the actual ultrasound elastography process. Moreover, in order to ensure the causality in LUPI, our framework proposes a physically-based data generation strategy to produce the triplets of privileged information, training examples and labels. This data generation process can approximately describes the actual ultrasound elastography process by the numerical simulation based on the tissue biomechanics and ultrasound physics. It thus can build the causal relationship between the privileged information and training examples/labels. It can also address the medical data insufficiency problem. The performance of our framework has been validated on 100 simulation data, 42 phantom data and 4 real clinical data by comparing with the ground truth performed by an ultrasound simulation system and four state-of-the-art methods. The experimental results show that our framework is well agreed (average bias is 0.065 for strain reconstruction) with the ground truth, as well as superior to these state-of-the-art methods. These results can demonstrate the effectiveness of our framework in the strain reconstruction.",
      "journal": "Medical image analysis",
      "year": "2019",
      "doi": "10.1016/j.media.2019.101534",
      "authors": "Gao Zhifan et al.",
      "keywords": "Deep neural network; Implicit model; Learning using privileged information; Quasi-static ultrasound elastography; Strain reconstruction",
      "mesh_terms": "Data Compression; Elasticity Imaging Techniques; Humans; Image Interpretation, Computer-Assisted; Neural Networks, Computer",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31352179/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "31649194",
      "title": "Dissecting racial bias in an algorithm used to manage the health of populations.",
      "abstract": "Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.",
      "journal": "Science (New York, N.Y.)",
      "year": "2019",
      "doi": "10.1126/science.aax2342",
      "authors": "Obermeyer Ziad et al.",
      "keywords": "",
      "mesh_terms": "Black or African American; Algorithms; Bias; Chronic Disease; Health Care Costs; Health Status Disparities; Humans; Medical Records; Racism; Risk Assessment; United States; White People",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31649194/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "31828959",
      "title": "MUBD-DecoyMaker 2.0: A Python GUI Application to Generate Maximal Unbiased Benchmarking Data Sets for Virtual Drug Screening.",
      "abstract": "Ligand enrichment assessment based on benchmarking data sets has become a necessity for the rational selection of the best-suited approach for prospective data mining of drug-like molecules. Up to now, a variety of benchmarking data sets had been generated and frequently used. Among them, MUBD-HDACs from our prior research efforts was regarded as one of five state-of-the-art benchmarks in 2017 by Frontiers in Pharmacology. This benchmarking set was generated by one of our unique de-biasing algorithms. It also rendered quite a few other cases of successful applications in recent years, thus is expected to have more impact in modern drug discovery. To make our algorithm amenable to more users, we developed a Python GUI application called MUBD-DecoyMaker 2.0. Moreover, it has two new additional functional modules, i.\u2009e. \"Detect 2D Bias\" and \"Quality Control\". This new GUI version had been proved to be easy to use while generate benchmarking data sets of the same quality. MUBD-DecoyMaker 2.0 is freely available at https://github.com/jwxia2014/MUBD-DecoyMaker2.0, along with its manual and testcase.",
      "journal": "Molecular informatics",
      "year": "2020",
      "doi": "10.1002/minf.201900151",
      "authors": "Xia Jie et al.",
      "keywords": "Python; drug discovery; ligand enrichment; unbiased benchmark; virtual screening",
      "mesh_terms": "Algorithms; Databases, Pharmaceutical; Datasets as Topic; Drug Discovery; Drug Evaluation, Preclinical; Pharmaceutical Preparations; Programming Languages; User-Computer Interface",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31828959/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "32012005",
      "title": "Unsupervised Domain Adaptation to Classify Medical Images Using Zero-Bias Convolutional Auto-Encoders and Context-Based Feature Augmentation.",
      "abstract": "The accuracy and robustness of image classification with supervised deep learning are dependent on the availability of large-scale labelled training data. In medical imaging, these large labelled datasets are sparse, mainly related to the complexity in manual annotation. Deep convolutional neural networks (CNNs), with transferable knowledge, have been employed as a solution to limited annotated data through: 1) fine-tuning generic knowledge with a relatively smaller amount of labelled medical imaging data, and 2) learning image representation that is invariant to different domains. These approaches, however, are still reliant on labelled medical image data. Our aim is to use a new hierarchical unsupervised feature extractor to reduce reliance on annotated training data. Our unsupervised approach uses a multi-layer zero-bias convolutional auto-encoder that constrains the transformation of generic features from a pre-trained CNN (for natural images) to non-redundant and locally relevant features for the medical image data. We also propose a context-based feature augmentation scheme to improve the discriminative power of the feature representation. We evaluated our approach on 3 public medical image datasets and compared it to other state-of-the-art supervised CNNs. Our unsupervised approach achieved better accuracy when compared to other conventional unsupervised methods and baseline fine-tuned CNNs.",
      "journal": "IEEE transactions on medical imaging",
      "year": "2020",
      "doi": "10.1109/TMI.2020.2971258",
      "authors": "Ahn Euijoon et al.",
      "keywords": "",
      "mesh_terms": "Diagnostic Imaging; Neural Networks, Computer",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32012005/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "32209237",
      "title": "Integrated longitudinal analysis does not compromise precision and reduces bias in the study of imaging outcomes: A comparative 5-year analysis in the DESIR cohort.",
      "abstract": "OBJECTIVE: To assess if an integrated longitudinal analysis using all available imaging data affects the precision of estimates of change in patients with axial spondyloarthritis (axSpA), with completers analysis as reference standard. METHODS: Patients from the DESIR cohort fulfilling the ASAS axSpA criteria were included. Radiographs and MRIs of the sacroiliac joints and spine were obtained at baseline, 1, 2 and 5 years. Each image was scored by 2 or 3 readers in 3 'reading-waves' (or campaigns). Each outcome was analyzed: i. According to a 'combination algorithm' (e.g. '2 out of 3' for binary scores); and ii. Per reader. Change over time was analyzed with generalized estimating equations by 3 approaches: (a)'integrated-analysis' (all patients with \u22651 score from \u22651 reader from all waves); (b1)Completers-only analysis (patients with 5-year follow-up, using scores from individual readers); (b2)Completers analysis using a 'combination algorithm' (as (b1) but with combined scores). Approaches (b1) and (b2) were considered the 'reference'. RESULTS: In total, 413 patients were included. The 'integrated analysis' was more inclusive with similar levels of precision of the change estimates as compared to both completers analyses. In fact, for low-incident outcomes (e.g.% mNY-positive over 5-years), an increased incidence was 'captured', with more precision, by the 'integrated analysis' compared to the completers analysis with combined scores (% change/year (95%CI): 1.1 (0.7; 1.5) vs 1.2 (0.5; 1.8), respectively). CONCLUSION: An efficient and entirely assumption-free 'integrated analysis' does not jeopardize precision of the estimates of change in imaging parameters and may yield increased statistical power for detecting changes with low incidence.",
      "journal": "Seminars in arthritis and rheumatism",
      "year": "2020",
      "doi": "10.1016/j.semarthrit.2020.02.017",
      "authors": "Sepriano Alexandre et al.",
      "keywords": "Axial spondyloarthritis; Imaging; Statistical methods",
      "mesh_terms": "Cohort Studies; Humans; Magnetic Resonance Imaging; Radiography; Sacroiliac Joint; Spine; Spondylarthritis",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32209237/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "32563687",
      "title": "Colorblind Algorithms: Racism in the Era of COVID-19.",
      "abstract": "This commentary offers a critique of the recent policy document issued by White et al. (2020) to guide critical care resource (e.g. ventilators) allocation during public health emergencies such as COVID-19. We argue that, if disseminated widely, this criteria would result in a racially inequitable resource distribution in the current COVID-19 crisis. We link the White et al. (2020) resource distribution protocol to other \"colorblind\" healthcare algorithms that have relied on seemingly objective but fundamentally biased data, thereby reinforcing and exacerbating pre-existing racial health disparities. We suggest a health equity framework to ensure unbiased distribution of critical care resources during COVID-19 and in general practice.",
      "journal": "Journal of the National Medical Association",
      "year": "2020",
      "doi": "10.1016/j.jnma.2020.05.010",
      "authors": "Williams J Corey et al.",
      "keywords": "Health disparities; Health policy; Race/ethnicity",
      "mesh_terms": "Algorithms; COVID-19; Humans; Racial Groups; Racism; SARS-CoV-2",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32563687/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "32570556",
      "title": "Health 'Big Data' Value, Benefit, and Control: The Patient eHealth Equity Gap.",
      "abstract": "Improvements in artificial intelligence and machine learning combined with the availability and exponential growth in individual and population health data offer opportunities for innovative patient/citizen-centered eHealth solutions. However, this confluence of social, technical, and economic interests can result in the privatization of control of patient data and contribute to widening inequity in access to healthcare. This paper explores these issues and advocates for a more equitable approach to advances in health big data for patients and citizens.",
      "journal": "Studies in health technology and informatics",
      "year": "2020",
      "doi": "10.3233/SHTI200337",
      "authors": "Demuro Paul et al.",
      "keywords": "Health disparities; artificial intelligence; big data; digital divide; eHealth; health equity; patient-centered",
      "mesh_terms": "Big Data; Delivery of Health Care; Humans; Machine Learning; Telemedicine",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32570556/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "32838582",
      "title": "Diagnostic accuracy of faecal calprotectin in a symptom-based algorithm for early diagnosis of inflammatory bowel disease adjusting for differential verification bias using a Bayesian approach.",
      "abstract": "BACKGROUND: Diagnostic delay in IBD is a major problem and diagnosis is frequently arrived when irreversible damage has already occurred. This study evaluated accuracy of faecal calprotectin (fCAL) integrated with diagnostic criteria for early diagnosis of IBD in a primary care setting. METHODS: General practitioners (GPs) were trained to recognize alarm symptoms for IBD classified as major and minor criteria. Fulfilment of one major or at least two minor criteria was followed by free fCAL testing and a visit by an IBD specialist and follow-up over 12\u00a0months. All patients with positive fCAL testing, i.e., \u226570\u2009\u03bcg/g underwent colonoscopy. The diagnostic accuracy of fCAL was estimated after adjusting for differential-verification bias following a Bayesian approach. RESULTS: Thirty-four GPs participated in the study and 133 patients were tested for fCAL between July 2016 and August 2017. Positivity of fCAL was seen in 45/133 patients (34%) and a final IBD diagnosis was made in 10/45 (22%). According to the threshold of 70\u2009\u03bcg/g, fCAL achieved a sensitivity of 74.8% (95%CI: 39.10-96.01%), a specificity of 70.4% (95%CI: 61.76-78.16%) and an overall diagnostic accuracy of 70.6% (95%CI: 61.04-78.37%). As for prognostic accuracy, despite positive predictive value being low, 21.9% (95%CI: 11.74-35.18%), the negative predictive value was definitely higher: 96.2% (95%CI: 84.96-99.51%). CONCLUSIONS: fCAL with a threshold set at 70\u2009\u03bcg/g seems to represent a potentially reliable negative test to be used in primary care settings for patients with symptoms suggestive of IBD.",
      "journal": "Scandinavian journal of gastroenterology",
      "year": "2020",
      "doi": "10.1080/00365521.2020.1807599",
      "authors": "Viola Anna et al.",
      "keywords": "Diagnostic delay; diagnostic accuracy; early diagnosis",
      "mesh_terms": "Algorithms; Bayes Theorem; Biomarkers; Delayed Diagnosis; Early Diagnosis; Feces; Humans; Inflammatory Bowel Diseases; Leukocyte L1 Antigen Complex",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32838582/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33002153",
      "title": "Estimated Risk for Insulin Dose Error Among Hospital Patients Due to Glucose Meter Hematocrit Bias in 2020.",
      "abstract": "CONTEXT.\u2014: Glycemic control requires accurate blood glucose testing. The extent of hematocrit interference is difficult to assess to assure quality patient care. OBJECTIVE.\u2014: To predict the effect of patient hematocrit on the performance of a glucose meter and its corresponding impact on insulin-dosing error. DESIGN.\u2014: Multilevel mixed regression was conducted to assess the extent that patient hematocrit influences Roche Accu-Chek Inform II glucose meters, using the Radiometer ABL 837 as a reference method collected during validation of 35 new meters. Regression coefficients of fixed effects for reference glucose, hematocrit, an interaction term, and random error were applied to 4 months of patient reference method results extracted from the laboratory information system. A hospital inpatient insulin dose algorithm was used to determine the frequency of insulin dose error between reference glucose and meter glucose results. RESULTS.\u2014: Fixed effects regression for method and hematocrit predicted biases to glucose meter results that met the \"95% within \u00b112%\" for the US Food and Drug Administration goal, but combinations of fixed and random effects exceeded that target in emergency and hospital inpatient units. Insulin dose errors were predicted from the meter results. Twenty-eight percent of intensive care unit, 20.8% of hospital inpatient, and 17.7% of emergency department results were predicted to trigger a \u00b11 insulin dose error by fixed and random effects. CONCLUSIONS.\u2014: The current extent of hematocrit interference on glucose meter performance is anticipated to cause insulin error by 1-dose category, which is likely associated with low patient risk.",
      "journal": "Archives of pathology & laboratory medicine",
      "year": "2020",
      "doi": "10.5858/arpa.2020-0101-RA",
      "authors": "Inman Mark et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Blood Glucose; Hematocrit; Humans; Hypoglycemic Agents; Insulin; Medical Errors; Risk Assessment; United States",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33002153/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33152602",
      "title": "Detect and correct bias in multi-site neuroimaging datasets.",
      "abstract": "The desire to train complex machine learning algorithms and to increase the statistical power in association studies drives neuroimaging research to use ever-larger datasets. The most obvious way to increase sample size is by pooling scans from independent studies. However, simple pooling is often ill-advised as selection, measurement, and confounding biases may creep in and yield spurious correlations. In this work, we combine 35,320 magnetic resonance images of the brain from 17 studies to examine bias in neuroimaging. In the first experiment, Name That Dataset, we provide empirical evidence for the presence of bias by showing that scans can be correctly assigned to their respective dataset with 71.5% accuracy. Given such evidence, we take a closer look at confounding bias, which is often viewed as the main shortcoming in observational studies. In practice, we neither know all potential confounders nor do we have data on them. Hence, we model confounders as unknown, latent variables. Kolmogorov complexity is then used to decide whether the confounded or the causal model provides the simplest factorization of the graphical model. Finally, we present methods for dataset harmonization and study their ability to remove bias in imaging features. In particular, we propose an extension of the recently introduced ComBat algorithm to control for global variation across image features, inspired by adjusting for unknown population stratification in genetics. Our results demonstrate that harmonization can reduce dataset-specific information in image features. Further, confounding bias can be reduced and even turned into a causal relationship. However, harmonization also requires caution as it can easily remove relevant subject-specific information. Code is available at https://github.com/ai-med/Dataset-Bias.",
      "journal": "Medical image analysis",
      "year": "2021",
      "doi": "10.1016/j.media.2020.101879",
      "authors": "Wachinger Christian et al.",
      "keywords": "Bias; Big data; Causal inference; Harmonization; MRI",
      "mesh_terms": "Algorithms; Bias; Humans; Machine Learning; Magnetic Resonance Imaging; Neuroimaging",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33152602/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33203514",
      "title": "Associations Between Socioeconomic Context and Congenital Heart Disease Related Outcomes in Adolescents and Adults.",
      "abstract": "Little is known about the relation between socioeconomic factors and health outcomes in adults and adolescents with congenital heart defects (CHD). Population-level data from the Colorado CHD surveillance system from 2011 to 2013 was used to examine the association between area deprivation and outcomes including hospitalizations, emergency department visits, cardiac procedures, all-cause and cardiac-related mortality, and major adverse cardiac events. Socioeconomic context was measured by the Area Deprivation Index at census tract level. Missing race/ethnicity was imputed using the Bayesian Improved Surname Geocoding algorithm. Generalized linear models were utilized to examine health disparities across deprivation quintiles after adjusting for insurance type, race/ethnicity, age, gender, urbanicity, and CHD severity in 5,748 patients. Cases residing in the most deprived quintile had 51% higher odds of inpatient admission, 74% higher odds of emergency department visit, 41% higher odds of cardiac surgeries, and 45% higher odds of major adverse cardiac events compared with cases in the least deprived quintile. Further, rates of hospitalizations, emergency department admissions, and cardiac surgeries were elevated in the most deprived compared with the least deprived quintile. Mortality was not significantly different across quintiles. In conclusion, findings suggest significant health equity issues for adolescent and adults with CHD based on area-based deprivation.",
      "journal": "The American journal of cardiology",
      "year": "2021",
      "doi": "10.1016/j.amjcard.2020.10.040",
      "authors": "Tillman Alexandra R et al.",
      "keywords": "",
      "mesh_terms": "Adolescent; Adult; Child; Emergency Service, Hospital; Female; Follow-Up Studies; Heart Defects, Congenital; Hospitalization; Humans; Male; Middle Aged; Morbidity; Prognosis; Retrospective Studies; Socioeconomic Factors; United States; Young Adult",
      "pub_types": "Journal Article; Multicenter Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33203514/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33250145",
      "title": "The automation of bias in medical Artificial Intelligence (AI): Decoding the past to create a better future.",
      "abstract": "Medicine is at a disciplinary crossroads. With the rapid integration of Artificial Intelligence (AI) into the healthcare field the future care of our patients will depend on the decisions we make now. Demographic healthcare inequalities continue to persist worldwide and the impact of medical biases on different patient groups is still being uncovered by the research community. At a time when clinical AI systems are scaled up in response to the Covid19 pandemic, the role of AI in exacerbating health disparities must be critically reviewed. For AI to account for the past and build a better future, we must first unpack the present and create a new baseline on which to develop these tools. The means by which we move forwards will determine whether we project existing inequity into the future, or whether we reflect on what we hold to be true and challenge ourselves to be better. AI is an opportunity and a mirror for all disciplines to improve their impact on society and for medicine the stakes could not be higher.",
      "journal": "Artificial intelligence in medicine",
      "year": "2020",
      "doi": "10.1016/j.artmed.2020.101965",
      "authors": "Straw Isabel",
      "keywords": "Artificial intelligence; Bias; Data science; Digital health; Disparities; Health; Healthcare; Inequality; Medicine",
      "mesh_terms": "Artificial Intelligence; Automation; Bias; COVID-19; Humans; SARS-CoV-2",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33250145/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33315263",
      "title": "The Emerging Hazard of AI-Related Health Care Discrimination.",
      "abstract": "Artificial intelligence holds great promise for improved health-care outcomes. But it also poses substantial new hazards, including algorithmic discrimination. For example, an algorithm used to identify candidates for beneficial \"high risk care management\" programs routinely failed to select racial minorities. Furthermore, some algorithms deliberately adjust for race in ways that divert resources away from minority patients. To illustrate, algorithms have underestimated African Americans' risks of kidney stones and death from heart failure. Algorithmic discrimination can violate Title VI of the Civil Rights Act and Section 1557 of the Affordable Care Act when it unjustifiably disadvantages underserved populations. This article urges that both legal and technical tools be deployed to promote AI fairness. Plaintiffs should be able to assert disparate impact claims in health-care litigation, and Congress should enact an Algorithmic Accountability Act. In addition, fairness should be a key element in designing, implementing, validating, and employing AI.",
      "journal": "The Hastings Center report",
      "year": "2021",
      "doi": "10.1002/hast.1203",
      "authors": "Hoffman Sharona",
      "keywords": "algorithmic fairness; artificial intelligence; civil rights; discrimination; disparate impact",
      "mesh_terms": "Artificial Intelligence; Civil Rights; Delivery of Health Care; Humans; Minority Groups; Patient Protection and Affordable Care Act; United States",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33315263/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33328028",
      "title": "Addressing bias: artificial intelligence in cardiovascular medicine.",
      "abstract": "",
      "journal": "The Lancet. Digital health",
      "year": "2020",
      "doi": "10.1016/S2589-7500(20)30249-1",
      "authors": "Tat Emily et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Bias; Cardiology; Female; Health Equity; Humans; Male",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33328028/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33328054",
      "title": "Ethical limitations of algorithmic fairness solutions in health care machine learning.",
      "abstract": "",
      "journal": "The Lancet. Digital health",
      "year": "2020",
      "doi": "10.1016/S2589-7500(20)30065-0",
      "authors": "McCradden Melissa D et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Delivery of Health Care; Female; Health Equity; Humans; Machine Learning; Male; Models, Biological; Social Justice",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33328054/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33404333",
      "title": "Using Precision Public Health to Manage Climate Change: Opportunities, Challenges, and Health Justice.",
      "abstract": "Amid public health concerns over climate change, \"precision public health\" (PPH) is emerging in next generation approaches to practice. These novel methods promise to augment public health operations by using ever larger and more robust health datasets combined with new tools for collecting and analyzing data. Precision strategies to protecting the public health could more effectively or efficiently address the systemic threats of climate change, but may also propagate or exacerbate health disparities for the populations most vulnerable in a changing climate. How PPH interventions collect and aggregate data, decide what to measure, and analyze data pose potential issues around privacy, neglecting social determinants of health, and introducing algorithmic bias into climate responses. Adopting a health justice framework, guided by broader social and climate justice tenets, can reveal principles and policy actions which may guide more responsible implementation of PPH in climate responses.",
      "journal": "The Journal of law, medicine & ethics : a journal of the American Society of Law, Medicine & Ethics",
      "year": "2020",
      "doi": "10.1177/1073110520979374",
      "authors": "Johnson Walter G",
      "keywords": "",
      "mesh_terms": "Big Data; Climate Change; Data Analysis; Data Collection; Data Science; Health Equity; Healthcare Disparities; Humans; Precision Medicine; Public Health; Social Determinants of Health; Social Justice",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33404333/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33479160",
      "title": "The SEE Study: Safety, Efficacy, and Equity of Implementing Autonomous Artificial Intelligence for Diagnosing Diabetic Retinopathy in Youth.",
      "abstract": "OBJECTIVE: Diabetic retinopathy (DR) is a leading cause of vision loss worldwide. Screening for DR is recommended in children and adolescents, but adherence is poor. Recently, autonomous artificial intelligence (AI) systems have been developed for early detection of DR and have been included in the American Diabetes Association's guidelines for screening in adults. We sought to determine the diagnostic efficacy of autonomous AI for the diabetic eye exam in youth with diabetes. RESEARCH DESIGN AND METHODS: In this prospective study, point-of-care diabetic eye exam was implemented using a nonmydriatic fundus camera with an autonomous AI system for detection of DR in a multidisciplinary pediatric diabetes center. Sensitivity, specificity, and diagnosability of AI was compared with consensus grading by retinal specialists, who were masked to AI output. Adherence to screening guidelines was measured before and after AI implementation. RESULTS: Three hundred ten youth with diabetes aged 5-21 years were included, of whom 4.2% had DR. Diagnosability of AI was 97.5% (302 of 310). The sensitivity and specificity of AI to detect more-than-mild DR was 85.7% (95% CI 42.1-99.6%) and 79.3% (74.3-83.8%), respectively, compared with the reference standard as defined by retina specialists. Adherence improved from 49% to 95% after AI implementation. CONCLUSIONS: Use of a nonmydriatic fundus camera with autonomous AI was safe and effective for the diabetic eye exam in youth in our study. Adherence to screening guidelines improved with AI implementation. As the prevalence of diabetes increases in youth and adherence to screening guidelines remains suboptimal, effective strategies for diabetic eye exams in this population are needed.",
      "journal": "Diabetes care",
      "year": "2021",
      "doi": "10.2337/dc20-1671",
      "authors": "Wolf Risa M et al.",
      "keywords": "",
      "mesh_terms": "Adolescent; Adult; Artificial Intelligence; Child; Diabetes Mellitus; Diabetic Retinopathy; Humans; Mass Screening; Prospective Studies; Sensitivity and Specificity",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33479160/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33524292",
      "title": "Building Trust to Promote a More Equitable Health Care System.",
      "abstract": "",
      "journal": "Annals of internal medicine",
      "year": "2021",
      "doi": "10.7326/M20-6984",
      "authors": "Baron Richard J et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; COVID-19; Health Equity; Humans; Pandemics; Physician-Patient Relations; Racism; SARS-CoV-2; Trust; United States",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33524292/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33569645",
      "title": "Discrimination of malignant from benign thyroid lesions through neural networks using FTIR signals obtained from tissues.",
      "abstract": "The current gold standard in cancer diagnosis-the microscopic examination of hematoxylin and eosin (H&E)-stained biopsies-is prone to bias since it greatly relies on visual examination. Hence, there is a need to develop a more sensitive and specific method for diagnosing cancer. Here, Fourier transform infrared (FTIR) spectroscopy of thyroid tumors (n\u2009=\u00a0164; 76 malignant, 88 benign) was performed and five (5) neural network (NN) models were designed to discriminate the obtained spectral data. PCA-LDA was used as classical benchmark for comparison. Each NN model was evaluated using a stratified 10-fold cross-validation method to avoid overfitting, and the performance metrics-accuracy, area under the curve (AUC), positive predictive value (PPV), negative predictive value (NPV), specificity rate (SR), and recall rate (RR)-were averaged for comparison. All NN models were able to perform excellently as classifiers, and all were able to surpass the LDA model in terms of accuracy. Among the NN models, the RNN model performed best, having an AUC of 95.29%\u2009\u00b1\u20096.08%, an accuracy of 98.06%\u2009\u00b1\u20092.87%, a PPV of 98.57%\u2009\u00b1\u20094.52%, a NPV of 93.18%\u2009\u00b1\u20097.93%, a SR value of 98.89%\u2009\u00b1\u20093.51%, and a RR value of 91.25%\u2009\u00b1\u200910.29%. The RNN model outperformed the LDA model for all metrics except for the AUC, NPV, and RR. In conclusion, NN-based tools were able to predict thyroid cancer based on infrared spectroscopy of tissues with a high level of diagnostic performance in comparison to the gold standard.",
      "journal": "Analytical and bioanalytical chemistry",
      "year": "2021",
      "doi": "10.1007/s00216-021-03183-0",
      "authors": "Santillan Abegail et al.",
      "keywords": "Diagnosis; Infrared spectroscopy; Neural networks; Pathologists; Scaled exponential linear units (SELU); Thyroid cancer",
      "mesh_terms": "Adolescent; Adult; Aged; Female; Humans; Male; Middle Aged; Neural Networks, Computer; Sensitivity and Specificity; Spectroscopy, Fourier Transform Infrared; Thyroid Gland; Thyroid Neoplasms; Young Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33569645/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33665879",
      "title": "Accounting for selection bias due to death in estimating the effect of wealth shock on cognition for the Health and Retirement Study.",
      "abstract": "The Health and Retirement Study (HRS) is a longitudinal study of U.S. adults enrolled at age 50 and older. We were interested in investigating the effect of a sudden large decline in wealth on the cognitive ability of subjects measured using a dataset provided composite score. However, our analysis was complicated by the lack of randomization, time-dependent confounding, and a substantial fraction of the sample and population will die during follow-up leading to some of our outcomes being censored. The common method to handle this type of problem is marginal structural models (MSM). Although MSM produces valid estimates, this may not be the most appropriate method to reflect a useful real-world situation because MSM upweights subjects who are more likely to die to obtain a hypothetical population that over time, resembles that would have been obtained in the absence of death. A more refined and practical framework, principal stratification (PS), would be to restrict analysis to the strata of the population that would survive regardless of negative wealth shock experience. In this work, we propose a new algorithm for the estimation of the treatment effect under PS by imputing the counterfactual survival status and outcomes. Simulation studies suggest that our algorithm works well in various scenarios. We found no evidence that a negative wealth shock experience would affect the cognitive score of HRS subjects.",
      "journal": "Statistics in medicine",
      "year": "2021",
      "doi": "10.1002/sim.8921",
      "authors": "Tan Yaoyuan Vincent et al.",
      "keywords": "Bayesian additive regression trees; causal inference; longitudinal study; missing data; penalized spline of propensity methods in treatment comparisons; time-dependent confounding",
      "mesh_terms": "Humans; Middle Aged; Bias; Cognition; Longitudinal Studies; Retirement; Selection Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33665879/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33691020",
      "title": "CheXclusion: Fairness gaps in deep chest X-ray classifiers.",
      "abstract": "Machine learning systems have received much attention recently for their ability to achieve expert-level performance on clinical tasks, particularly in medical imaging. Here, we examine the extent to which state-of-the-art deep learning classifiers trained to yield diagnostic labels from X-ray images are biased with respect to protected attributes. We train convolution neural networks to predict 14 diagnostic labels in 3 prominent public chest X-ray datasets: MIMIC-CXR, Chest-Xray8, CheXpert, as well as a multi-site aggregation of all those datasets. We evaluate the TPR disparity - the difference in true positive rates (TPR) - among different protected attributes such as patient sex, age, race, and insurance type as a proxy for socioeconomic status. We demonstrate that TPR disparities exist in the state-of-the-art classifiers in all datasets, for all clinical tasks, and all subgroups. A multi-source dataset corresponds to the smallest disparities, suggesting one way to reduce bias. We find that TPR disparities are not significantly correlated with a subgroup's proportional disease burden. As clinical models move from papers to products, we encourage clinical decision makers to carefully audit for algorithmic disparities prior to deployment. Our supplementary materials can be found at, http://www.marzyehghassemi.com/chexclusion-supp-3/.",
      "journal": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing",
      "year": "2021",
      "doi": "",
      "authors": "Seyyed-Kalantari Laleh et al.",
      "keywords": "",
      "mesh_terms": "Computational Biology; Humans; Machine Learning; Neural Networks, Computer; X-Rays",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33691020/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33997972",
      "title": "BiasCorrector: Fast and accurate correction of all types of experimental biases in quantitative DNA methylation data derived by different technologies.",
      "abstract": "Quantification of DNA methylation in neoplastic cells is crucial both from mechanistic and diagnostic perspectives. However, such measurements are prone to different experimental biases. Polymerase chain reaction (PCR) bias results in an unequal recovery of methylated and unmethylated alleles at the sample preparation step. Post-PCR biases get introduced additionally by the readout processes. Correcting the biases is more practicable than optimising experimental conditions, as demonstrated previously. However, utilisation of our earlier developed algorithm strongly necessitates automation. Here, we present two R packages: rBiasCorrection, the core algorithms to correct biases; and BiasCorrector, its web-based graphical user interface frontend. The software detects and analyses experimental biases in calibration DNA samples at a single base resolution by using cubic polynomial and hyperbolic regression. The correction coefficients from the best regression type are employed to compensate for the bias. Three common technologies-bisulphite pyrosequencing, next-generation sequencing and oligonucleotide microarrays-were used to comprehensively test BiasCorrector. We demonstrate the accuracy of BiasCorrector's performance and reveal technology-specific PCR- and post-PCR biases. BiasCorrector effectively eliminates biases regardless of their nature, locus, the number of interrogated methylation sites and the detection method, thus representing a user-friendly tool for producing accurate epigenetic results.",
      "journal": "International journal of cancer",
      "year": "2021",
      "doi": "10.1002/ijc.33681",
      "authors": "Kapsner Lorenz A et al.",
      "keywords": "BiasCorrector; DNA methylation; PCR-bias; cancer; post-PCR bias",
      "mesh_terms": "Algorithms; Bias; CpG Islands; DNA Methylation; Humans; Neoplasms; Polymerase Chain Reaction; Sequence Analysis, DNA; Software; Technology",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33997972/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "34022618",
      "title": "Gender disparities in clozapine prescription in a cohort of treatment-resistant schizophrenia in the South London and Maudsley case register.",
      "abstract": "BACKGROUND: Gender disparities in treatment are apparent across many areas of healthcare. There has been little research into whether clozapine prescription, the first-line treatment for treatment-resistant schizophrenia (TRS), is affected by patient gender. METHODS: This retrospective cohort study identified 2244 patients with TRS within the South London and Maudsley NHS Trust, by using a bespoke method validated against a gold-standard, manually coded, dataset of TRS cases. The outcome and exposures were identified from the free-text using natural language processing applications (including machine learning and rules-based approaches) and from information entered in structured fields. Multivariable logistic regression was carried out to calculate the odds ratios for clozapine prescription according to patients' gender, and adjusting for numerous potential confounders including sociodemographic, clinical (e.g., psychiatric comorbidities and substance use), neutropenia, functional factors (e.g., problems with occupation), and clinical monitoring. RESULTS: Clozapine was prescribed to 77% of the women and 85% of the men with TRS. Women had reduced odds of being prescribed clozapine as compared to men after adjusting for all factors included in the present study (adjusted OR: 0.66; 95% CI 0.44-0.97; p = 0.037). CONCLUSION: Women with TRS are less likely to be prescribed clozapine than men with TRS, even when considering the effects of multiple clinical and functional factors. This finding suggests there could be gender bias in clozapine prescription, which carries ramifications for the relatively poorer care of women with TRS regarding many outcomes such as increased hospitalisation, mortality, and poorer quality of life.",
      "journal": "Schizophrenia research",
      "year": "2021",
      "doi": "10.1016/j.schres.2021.05.006",
      "authors": "Wellesley Wesley Emma et al.",
      "keywords": "Healthcare inequality; Psychopharmacology; Refractory psychosis; Sex",
      "mesh_terms": "Antipsychotic Agents; Clozapine; Cohort Studies; Female; Humans; London; Male; Prescriptions; Quality of Life; Retrospective Studies; Schizophrenia; Sexism",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34022618/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "34213259",
      "title": "[Establishment of chromatographic fingerprint of Squama Manis and its applications in animal source identification and quality grade discrimination].",
      "abstract": "Squama Manis, or \"Chuanshanjia\" in Chinese, is a traditional Chinese medicine (TCM) for promoting blood circulation and reducing swelling and discharge; the only animal source used in TCM is the scales of Manis pentadactyla. However, in today's pharmaceutical market, there are many scales from other species of the same genus that are difficult to distinguish from Squama Manis. High-quality and low-quality scales are also severely confused. To solve the above problems, various analytical methods have been developed, such as thin-layer chromatography, mass spectrometry and DNA detection. Owing to their low resolving ability, high equipment cost, and inconvenient operation, none of these methods are appropriate for routine identification of Squama Manis. A chromatographic fingerprint can comprehensively reflect the synergic action of multiple chemical compositions in TCM and has been widely used for the quality control of TCM. In the present study, we established a fingerprint of Squama Manis and explored its feasibility in identifying the origin and quality grade of scales. First, Squama Manis powder was hydrolyzed by hydrochloric acid (1 mol/L). Next, the extract was analyzed on a Symmetry 300 C18 column by linear gradient elution, using 0.1% trifluoroacetic acid (v/v) in water and 0.1% trifluoroacetic acid (v/v) in acetonitrile as the mobile phase and 280 nm as the detection wavelength. The established method was systematically validated, demonstrating good precision, repeatability and sample stability (relative standard deviation (RSD)<5%). Subsequently, samples of different sources and quality grades were distinguished by similarity evaluation and discrimination analysis based on the fingerprint data. In the similarity evaluation, the reference fingerprint was defined as the average fingerprint of twelve first-class samples, and seventeen chromatographic peaks were identified as common peaks. Similarities between the reference fingerprint and fingerprints with different base sources and quality grades were calculated using the absolute area of common peaks as original data. The similarities between Squama Manis and scales from other animals were all less than 0.776, while the similarities between Squama Manis of different grades overlapped significantly, varying from 0.988 to 0.996 for first-class samples and 0.950 to 0.995 for general samples. The results reflected the feasibility of similarity evaluation for discriminating base source and its limitation in the distinguishing between quality grades. Nonetheless, first-class scales showed higher average similarity and lower RSD than general scales, which indicates some level of revelation between fingerprint similarity and quality grade. Thus, a better algorithm or discriminant model is required to distinguish between quality grades. Therefore, a supervised chemometric technique, kernel-based support vector machine (SVM), was applied to construct predictive models. The SVM is a common discriminant model that classifies samples by constructing a separate hyperplane in n-dimensional space, maximizing the margin between classes. Combination with a kernel function can effectively avoid \"dimension disaster\" when dealing with nonlinear data. In the model, the quality grade was defined as a sample label, and the absolute peak areas constituted the data matrix. Verified by 10-fold cross-validation, the unbiased prediction accuracy was up to 95.83%. The predicted results were highly consistent with the actual classifications. The results indicate the high feasibility of the established model for determining quality grade, as it performed significantly better than the similarity evaluation. Samples from batches A and B were completely discriminated and only two samples from batch S were incorrectly classified. Given the batch bias, we believe that model error may have been caused by man-made tag errors rather than the model itself. In conclusion, we established a chromatographic fingerprint for Squama Manis quality analysis and demonstrated its feasibility in animal source identification and quality determination by combining different data analysis methods. The established strategy may provide a new method for improving the the validity and accuracy of Squama Manis in clinical use.",
      "journal": "Se pu = Chinese journal of chromatography",
      "year": "2020",
      "doi": "10.3724/SP.J.1123.2020.04007",
      "authors": "Qiao Yali et al.",
      "keywords": "Squama Manis; base resource; chromatographic fingerprint; discrimination analysis; quality grade; similarity evaluation; traditional Chinese medicine",
      "mesh_terms": "Animals; Biological Products; Chromatography, High Pressure Liquid; Mass Spectrometry; Medicine, Chinese Traditional; Pangolins; Powders; Quality Control",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34213259/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "34460977",
      "title": "Contextual bias, the democratization of healthcare, and medical artificial intelligence in low- and middle-income countries.",
      "abstract": "Medical artificial intelligence (MAI) creates an opportunity to radically expand access to healthcare across the globe by allowing us to overcome the persistent labor shortages that limit healthcare access. This democratization of healthcare is the greatest moral promise of MAI. Whatever comes of the enthusiastic discourse about the ability of MAI to improve the state-of-the-art in high-income countries (HICs), it will be far less impactful than improving the desperate state-of-the-actual in low- and middle-income countries (LMICs). However, the almost exclusive development of MAI in HICs risks this promise being thwarted by contextual bias, an algorithmic bias that arises when the context of the training data is significantly dissimilar from potential contexts of application, which makes the unreflective application of HIC-based MAI in LMIC contexts dangerous. The use of MAI in LMICs demands careful attention to context. In this paper, I aim to provide that attention. First, I illustrate the dire state of healthcare in LMICs and the hope that MAI may help us to improve this state. Next, I show that the radical differences between the health contexts of HICs and those of LMICs create an extraordinary risk of contextual bias. Then, I explore ethical challenges raised by this risk, and propose policies that will help to overcome those challenges. Finally, I sketch a wide range of related issues that need to be addressed to ensure that MAI has a positive impact on LMICs-and is able to improve, rather than worsen, global health equity.",
      "journal": "Bioethics",
      "year": "2022",
      "doi": "10.1111/bioe.12927",
      "authors": "Weissglass Daniel E",
      "keywords": "contextual bias; health disparity; health equity; low- and middle-income countries; medical artificial intelligence",
      "mesh_terms": "Artificial Intelligence; Developing Countries; Health Equity; Health Services Accessibility; Humans; Poverty",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34460977/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "34468879",
      "title": "Patient Experience Surveys Reveal Gender-Biased Descriptions of Their Care Providers.",
      "abstract": "Patient experience surveys (PES) are collected by healthcare systems as a surrogate marker of quality and published unedited online for the purpose of transparency, but these surveys may reflect gender biases directed toward healthcare providers. This retrospective study evaluated PES at a single university hospital between July 2016 and June 2018. Surveys were stratified by overall provider rating and self-identified provider gender. Adjectives from free-text survey comments were extracted using natural language processing techniques and applied to a statistical machine learning model to identify descriptors predictive of provider gender. 109,994 surveys were collected, 17,395 contained free-text comments describing 687 unique providers. The mean overall rating between male (8.84, n\u2009=\u20098558) and female (8.80, n\u2009=\u20098837) providers did not differ (p\u2009=\u20090.149). However, highly-rated male providers were more often described for their agentic qualities using adjectives such as \"informative,\" \"forthright,\" \"superior,\" and \"utmost\" (OR 1.48, p\u2009<\u20090.01)-whereas highly-rated female providers were more often described by their communal qualities through adjectives such as \"empathetic,\" \"sweet,\" \"warm,\" \"attentive,\" and \"approachable\" (OR 2.11, p\u2009<\u20090.0001). PES may contain gender stereotypes, raising questions about their impact on physicians and their validity as a quality metric which must be balanced with the need for unedited transparency. Future prospective studies are needed to further characterize this trend across geographically and racially diverse healthcare providers.",
      "journal": "Journal of medical systems",
      "year": "2021",
      "doi": "10.1007/s10916-021-01766-z",
      "authors": "Haynes Dylan et al.",
      "keywords": "Bias; Gender; Patient experience; Survey",
      "mesh_terms": "Delivery of Health Care; Female; Health Personnel; Humans; Male; Patient Outcome Assessment; Patient Satisfaction; Retrospective Studies; Surveys and Questionnaires",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34468879/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "34515682",
      "title": "Construction of a Wireless-Enabled Endoscopically Implantable Sensor for pH Monitoring with Zero-Bias Schottky Diode-based Receiver.",
      "abstract": "Ambulatory pH monitoring of pathological reflux is an opportunity to observe the relationship between symptoms and exposure of the esophagus to acidic or non-acidic refluxate. This paper describes a method for the development, manufacturing, and implantation of a miniature wireless-enabled pH sensor. The sensor is designed to be implanted endoscopically with a single hemostatic clip. A fully passive rectenna-based receiver based on a zero-bias Schottky diode is also constructed and tested. To construct the device, a two-layer printed circuit board and off-the-shelf components were used. A miniature microcontroller with integrated analog peripherals is used as an analog front end for the ion-sensitive field-effect transistor (ISFET) sensor and to generate a digital signal which is transmitted with an amplitude shift keying transmitter chip. The device is powered by two primary alkaline cells. The implantable device has a total volume of 0.6 cm3 and a weight of 1.2 grams, and its performance was verified in an ex vivo model (porcine esophagus and stomach). Next, a small footprint passive rectenna-based receiver which can be easily integrated either into an external receiver or the implantable neurostimulator, was constructed and proven to receive the RF signal from the implant when in proximity (20 cm) to it. The small size of the sensor provides continuous pH monitoring with minimal obstruction of the esophagus. The sensor could be used in routine clinical practice for 24/96 h esophageal pH monitoring without the need to insert a nasal catheter. The \"zero-power\" nature of the receiver also enables the use of the sensor for automatic in-vivo calibration of miniature lower esophageal sphincter neurostimulation devices. An active sensor-based control enables the development of advanced algorithms to minimize the used energy to achieve a desirable clinical outcome. One of the examples of such an algorithm would be a closed-loop system for on-demand neurostimulation therapy of gastroesophageal reflux disease (GERD).",
      "journal": "Journal of visualized experiments : JoVE",
      "year": "2021",
      "doi": "10.3791/62864",
      "authors": "Nov\u00e1k Marek et al.",
      "keywords": "",
      "mesh_terms": "Animals; Esophageal pH Monitoring; Gastroesophageal Reflux; Hydrogen-Ion Concentration; Prostheses and Implants; Swine",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Video-Audio Media",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34515682/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "34542183",
      "title": "Highlighting psychological pain avoidance and decision-making bias as key predictors of suicide attempt in major depressive disorder-A novel investigative approach using machine learning.",
      "abstract": "OBJECTIVE: Predicting suicide is notoriously difficult and complex, but a serious public health issue. An innovative approach utilizing machine learning (ML) that incorporates features of psychological mechanisms and decision-making characteristics related to suicidality could create an improved model for identifying suicide risk in patients with major depressive disorder (MDD). METHOD: Forty-four patients with MDD and past suicide attempts (MDD_SA, N\u2009=\u200944); 48 patients with MDD but without past suicide attempts (MDD_NS, N\u2009=\u200948-42 of whom with suicide ideation [MDD_SI, N\u2009=\u200942]), and healthy controls (HCs, N\u2009=\u200951) completed seven psychometric assessments including the Three-dimensional\u2002Psychological Pain Scale (TDPPS), and one behavioral assessment, the Balloon Analogue Risk Task (BART). Descriptive statistics, group comparisons, logistic regressions, and ML were used to explore and compare the groups and generate predictors of suicidal acts. RESULTS: MDD_SA and MDD_NS differed in TDPPS\u2002total score, pain arousal and avoidance subscale scores, suicidal ideation scores, and relevant decision-making indicators in BART. Logistic regression tests linked suicide attempts to psychological pain avoidance and a risk decision-making indicator. The resultant key ML model distinguished MDD_SA/MDD_NS with 88.2% accuracy. The model could also distinguish MDD_SA/MDD_SI with 81.25% accuracy. The ML model using hopelessness could classify MDD_SI/HC with 94.4% accuracy. CONCLUSION: ML analyses showed that motivation to avoid intolerable psychological pain, coupled with impaired decision-making bias toward under-valuing life's worth are highly predictive of suicide attempts. Analyses also demonstrated that suicidal ideation and attempts differed in potential mechanisms, as suicidal ideation was more related to hopelessness. ML algorithms show useful promises as a predictive instrument.",
      "journal": "Journal of clinical psychology",
      "year": "2022",
      "doi": "10.1002/jclp.23246",
      "authors": "Ji Xinlei et al.",
      "keywords": "machine learning; major depressive disorder; psychological pain; risk decision-making; suicide",
      "mesh_terms": "Major Depressive Disorder; Humans; Machine Learning; Pain; Suicidal Ideation; Suicide, Attempted",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34542183/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "34674847",
      "title": "Use of artificial intelligence for gender bias analysis in letters of recommendation for general surgery residency candidates.",
      "abstract": "BACKGROUND: Letters of recommendation (LoRs) play an important role in resident selection. Author language varies implicitly toward male and female applicants. We examined gender bias in LoRs written for surgical residency candidates across three decades at one institution. METHODS: Retrospective analysis of LoRs written for general surgery residency candidates between 1980 and 2011 using artificial intelligence (AI) to conduct natural language processing (NLP) and sentiment analysis, and computer-based algorithms to detect gender bias. Applicants were grouped by scaled clerkship grades and USMLE scores. Data were analyzed among groups with t-tests, ANOVA, and non-parametric tests, as appropriate. RESULTS: A total of 611 LoRs were analyzed for 171 applicants (16.4% female), and 95.3% of letter authors were male. Scaled USMLE scores and clerkship grades (SCG) were similar for both genders (p\u00a0>\u00a00.05 for both). Average word count for all letters was 290 words and was not significantly different between genders (p\u00a0=\u00a00.18). LoRs written before 2000 were significantly shorter than those written after, among applicants of both genders (female p\u00a0=\u00a00.004; male p\u00a0<\u00a00.001). Gender bias analysis of female LoRs revealed more gendered wording compared to male LoRs (p\u00a0=\u00a00.04) and was most prominent among females with lower SCG (9.5 vs 5.1, p\u00a0=\u00a00.01). Sentiment analysis revealed male LoRs with female authors had significantly more positive sentiment compared to female LoRs (p\u00a0=\u00a00.02), and males with higher SCG had more positive sentiment compared to those with lower SCG (9.4 vs 8.2, p\u00a0=\u00a00.03). NLP detected more \"fear\" in male LoRs with lower SCGs (0.11 vs 0.09, p\u00a0=\u00a00.02). Female LoRs with higher SCGs had more positive sentiment (0.78 vs 0.83, p\u00a0=\u00a00.03) and \"joy\" (0.60 vs 0.63, p\u00a0=\u00a00.02), although those written before 2000 had less \"joy\" (0.5 vs 0.63, p\u00a0=\u00a00.006). CONCLUSION: AI and computer-based algorithms detected linguistic differences and gender bias in LoRs written for general surgery residency applicants, even following stratification by clerkship grades and when analyzed by decade.",
      "journal": "American journal of surgery",
      "year": "2021",
      "doi": "10.1016/j.amjsurg.2021.09.034",
      "authors": "Sarraf Daniel et al.",
      "keywords": "Gender bias; General surgery residency; Graduate medical education; LoRs",
      "mesh_terms": "Artificial Intelligence; Correspondence as Topic; Female; General Surgery; Humans; Internship and Residency; Male; School Admission Criteria; Sexism",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34674847/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35006495",
      "title": "Deep learning prediction of sex on chest radiographs: a potential contributor to biased algorithms.",
      "abstract": "BACKGROUND: Deep convolutional neural networks (DCNNs) for diagnosis of disease on chest radiographs (CXR) have been shown to be biased against males or females if the datasets used to train them have unbalanced sex representation. Prior work has suggested that DCNNs can predict sex on CXR, which could aid forensic evaluations, but also be a source of bias. OBJECTIVE: To (1) evaluate the performance of DCNNs for predicting sex across different datasets and architectures and (2) evaluate visual biomarkers used by DCNNs to predict sex on CXRs. MATERIALS AND METHODS: Chest radiographs were obtained from the Stanford CheXPert and NIH Chest XRay14 datasets which comprised of 224,316 and 112,120 CXRs, respectively. To control for dataset size and class imbalance, random undersampling was used to reduce each dataset to 97,560 images that were balanced for sex. Each dataset was randomly split into training (70%), validation (10%), and test (20%) sets. Four DCNN architectures pre-trained on ImageNet were used for transfer learning. DCNNs were externally validated using a test set from the opposing dataset. Performance was evaluated using area under the receiver operating characteristic curve (AUC). Class activation mapping (CAM) was used to generate heatmaps visualizing the regions contributing to the DCNN's prediction. RESULTS: On the internal test set, DCNNs achieved AUROCs ranging from 0.98 to 0.99. On external validation, the models reached peak cross-dataset performance of 0.94 for the VGG19-Stanford model and 0.95 for the InceptionV3-NIH model. Heatmaps highlighted similar regions of attention between model architectures and datasets, localizing to the mediastinal and upper rib regions, as well as to the lower chest/diaphragmatic regions. CONCLUSION: DCNNs trained on two large CXR datasets accurately predicted sex on internal and external test data with similar heatmap localizations across DCNN architectures and datasets. These findings support the notion that DCNNs can leverage imaging biomarkers to predict sex and potentially confound the accurate prediction of disease on CXRs and contribute to biased models. On the other hand, these DCNNs can be beneficial to emergency radiologists for forensic evaluations and identifying patient sex for patients whose identities are unknown, such as in acute trauma.",
      "journal": "Emergency radiology",
      "year": "2022",
      "doi": "10.1007/s10140-022-02019-3",
      "authors": "Li David et al.",
      "keywords": "Anatomy; Bias; Chest; Deep learning; Fairness; Forensics; Radiograph; Sex prediction",
      "mesh_terms": "Algorithms; Deep Learning; Female; Humans; Male; Neural Networks, Computer; Radiography; Radiologists",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35006495/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35033310",
      "title": "Demographic Reporting in Publicly Available Chest Radiograph Data Sets: Opportunities for Mitigating Sex and Racial Disparities in Deep Learning Models.",
      "abstract": "OBJECTIVE: Data sets with demographic imbalances can introduce bias in deep learning models and potentially amplify existing health disparities. We evaluated the reporting of demographics and potential biases in publicly available chest radiograph (CXR) data sets. METHODS: We reviewed publicly available CXR data sets available on February 1, 2021, with >100 CXRs and performed a thorough search of various repositories, including Radiopaedia and Kaggle. For each data set, we recorded the total number of images and whether the data set reported demographic variables (age, race or ethnicity, sex, insurance status) in aggregate and on an image-level basis. RESULTS: Twenty-three CXR data sets were included (range, 105-371,858 images). Most data sets reported demographics in some form (19 of 23; 82.6%) and on an image level (17 of 23; 73.9%). The majority reported age (19 of 23; 82.6%) and sex (18 of 23; 78.2%), but a minority reported race or ethnicity (2 of 23; 8.7%) and insurance status (1 of 23; 4.3%). Of the 13 data sets with sex distribution readily available, the average breakdown was 55.2% male subjects, ranging from 47.8% to 69.7% male representation. Of these, 8 (61.5%) overrepresented male subjects and 5 (38.5%) overrepresented female subjects. DISCUSSION: Although most publicly available CXR data sets report age and sex on an image-basis level, few report race or ethnicity and insurance status. Furthermore, these data sets frequently underrepresent one of the sexes, more frequently the female sex. We recommend that data sets report standard demographic variables, and when possible, balance demographic representation to mitigate bias. Furthermore, for researchers using these data sets, we recommend that attention be paid to balancing demographic labels in addition to disease labels, as well as developing training methods that can account for these imbalances.",
      "journal": "Journal of the American College of Radiology : JACR",
      "year": "2022",
      "doi": "10.1016/j.jacr.2021.08.018",
      "authors": "Yi Paul H et al.",
      "keywords": "Artificial intelligence; bias; chest radiograph; deep learning; fairness",
      "mesh_terms": "Bias; Deep Learning; Ethnicity; Female; Humans; Male; Radiography",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35033310/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35049447",
      "title": "On Algorithmic Fairness in Medical Practice.",
      "abstract": "The application of machine-learning technologies to medical practice promises to enhance the capabilities of healthcare professionals in the assessment, diagnosis, and treatment, of medical conditions. However, there is growing concern that algorithmic bias may perpetuate or exacerbate existing health inequalities. Hence, it matters that we make precise the different respects in which algorithmic bias can arise in medicine, and also make clear the normative relevance of these different kinds of algorithmic bias for broader questions about justice and fairness in healthcare. In this paper, we provide the building blocks for an account of algorithmic bias and its normative relevance in medicine.",
      "journal": "Cambridge quarterly of healthcare ethics : CQ : the international journal of healthcare ethics committees",
      "year": "2022",
      "doi": "10.1017/S0963180121000839",
      "authors": "Grote Thomas et al.",
      "keywords": "algorithmic bias; discrimination; fairness; machine learning; medical practice",
      "mesh_terms": "Data Collection; Delivery of Health Care; Humans; Machine Learning; Social Justice",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35049447/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35130064",
      "title": "The Potential For Bias In Machine Learning And Opportunities For Health Insurers To Address It.",
      "abstract": "As the use of machine learning algorithms in health care continues to expand, there are growing concerns about equity, fairness, and bias in the ways in which machine learning models are developed and used in clinical and business decisions. We present a guide to the data ecosystem used by health insurers to highlight where bias can arise along machine learning pipelines. We suggest mechanisms for identifying and dealing with bias and discuss challenges and opportunities to increase fairness through analytics in the health insurance industry.",
      "journal": "Health affairs (Project Hope)",
      "year": "2022",
      "doi": "10.1377/hlthaff.2021.01287",
      "authors": "Gervasi Stephanie S et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Bias; Ecosystem; Humans; Insurance Carriers; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35130064/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35579815",
      "title": "Identifying and Mitigating Potential Biases in Predicting Drug Approvals.",
      "abstract": "INTRODUCTION: Machine learning models are increasingly applied to predict the drug development outcomes based on intermediary clinical trial results. A key challenge to this task is to address various forms of bias in the historical drug approval data. OBJECTIVE: We aimed to identify and mitigate the bias in drug approval predictions and quantify the impacts of debiasing in terms of financial value and drug safety. METHODS: We instantiated the Debiasing Variational Autoencoder, the state-of-the-art model for automated debiasing. We trained and evaluated the model on the Citeline dataset provided by Informa Pharma Intelligence\u00a0to predict the final drug development outcome from phase II trial results. RESULTS: The debiased Debiasing Variational Autoencoder model achieved better performance (measured by the [Formula: see text] score 0.48) in predicting the drug development outcomes than its un-debiased baseline ([Formula: see text] score 0.25). It had a much higher true-positive rate than baseline (60% vs 15%), while its true-negative rate was slightly lower (88% vs 99%). The Debiasing Variational Autoencoder distinguished between drugs developed by large pharmaceutical firms and those by small biotech companies. The model prediction is strongly influenced by multiple factors such as prior approval of the drug for another indication, whether the trial meets the positive/negative endpoints, and the year when the trial is completed. We estimate that the debiased model generates financial value for the drug developer in six major therapeutic areas, with a range of US$763-1,365 million. CONCLUSIONS: Our analysis shows that debiasing improves the financial efficiency of late-stage drug development. From the pharmacovigilance perspective, the debiased model is more likely to identify drugs that are both safe and effective. Meanwhile, it may predict a higher probability of success for drugs with potential adverse effects (because of its lower true-negative rate), thus it must be used with caution to predict the development outcomes of drug candidates currently in the pipeline.",
      "journal": "Drug safety",
      "year": "2022",
      "doi": "10.1007/s40264-022-01160-9",
      "authors": "Xu Qingyang et al.",
      "keywords": "",
      "mesh_terms": "Bias; Drug Approval; Drug-Related Side Effects and Adverse Reactions; Humans; Machine Learning; Pharmacovigilance",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35579815/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35685000",
      "title": "Designing Equitable Health Care Outreach Programs From Machine Learning Patient Risk Scores.",
      "abstract": "There is growing interest in ensuring equity and guarding against bias in the use of risk scores produced by machine learning and artificial intelligence models. Risk scores are used to select patients who will receive outreach and support. Inappropriate use of risk scores, however, can perpetuate disparities. Commonly advocated solutions to improve equity are nontrivial to implement and may not pass legal scrutiny. In this article, we introduce pragmatic tools that support better use of risk scores for more equitable outreach programs. Our model output charts allow modeling and care management teams to see the equity consequences of different threshold choices and to select the optimal risk thresholds to trigger outreach. For best results, as with any health equity tool, we recommend that these charts be used by a diverse team and shared with relevant stakeholders.",
      "journal": "Medical care research and review : MCRR",
      "year": "2023",
      "doi": "10.1177/10775587221098831",
      "authors": "Hane Christopher A et al.",
      "keywords": "artificial intelligence; civil rights; health care disparities; health equity; health status disparities; structural inequity",
      "mesh_terms": "Humans; Artificial Intelligence; Delivery of Health Care; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35685000/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35796067",
      "title": "The impact of discrimination on binge eating in a nationally representative sample of Latine individuals.",
      "abstract": "OBJECTIVE: Latines have higher rates of eating disorders characterized by binge eating compared to their non-Latine white counterparts, yet culturally and socially relevant factors related to binge eating in Latines have been largely understudied. The purpose of the current study was to examine how discrimination and acculturative stress were associated with binge eating in a nationally representative sample of Latines. An additional aim was to test the extent to which family cohesion and social support could buffer against the effects of discrimination and acculturative stress on binge eating. METHOD: Participants (56% female, N\u00a0=\u00a02550) were Latines enrolled in the National Latino and Asian American Study. Structural equation modeling using 1000 re-sampled data sets built from machine learning iterative sampling procedures was used to examine the effects of discrimination, acculturative stress, family cohesion, and social support on binge eating. RESULTS: Results indicated that only discrimination was significantly associated with binge eating. Neither the direct effect of acculturative stress, interaction of family cohesion and acculturative stress, interaction of social cohesion and acculturative stress, nor the interaction of social support and discrimination were significantly associated with binge eating. DISCUSSION: This study highlights the need for mental-health providers to understand and assess discrimination among Latines presenting with concerns of binge eating. Treatments that effectively provide coping strategies to manage discriminatory experiences and reduce binge eating could improve both effectiveness of treatment and retention rates for Latine individuals with binge eating. PUBLIC SIGNIFICANCE: This study examined the association of discrimination, acculturative stress, family cohesion, and social support with binge eating in Latines. Only discrimination was significantly associated with binge eating, highlighting the importance for providers to assess discrimination among Latines with binge-eating concerns and to improve equity, inclusion, and belonging at a societal level. Modifying existing treatments to address coping with discrimination could improve the effectiveness for Latines with binge-eating concerns.",
      "journal": "The International journal of eating disorders",
      "year": "2022",
      "doi": "10.1002/eat.23773",
      "authors": "Johnson Sarah N et al.",
      "keywords": "Latine; Latino; acculturative stress; binge eating; binge-eating disorder; bulimia nervosa; discrimination; eating disorder; ethnic minority; feeding and eating disorders",
      "mesh_terms": "Acculturation; Binge-Eating Disorder; Bulimia; Female; Hispanic or Latino; Humans; Male; Stress, Psychological",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35796067/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35802380",
      "title": "A new model for categorizing cognitive biases and debiasing strategies in dermatology.",
      "abstract": "Cognitive biases are a significant cause of medical error. They arise from \"system 1\" thinking, which depends on heuristics to make quick decisions in complex situations. Heuristics make us \"predictably irrational,\" distorting our ability to accurately assess probabilities in clinical scenarios. It is well reported in the literature that metacognition, the art of reflecting on one's thought processes, is the optimal way to deal with cognitive biases. However, it is unclear how this can be consistently implemented in dermatological practice. Our debiasing attempts thus far have been sporadic at best. This article categorizes important cognitive biases according to each stage of the doctor-patient interaction (history taking, clinical examination, investigations, diagnosis, and management). We hope that providing this clinically relevant framework can foster metacognition and a platform for algorithmic debiasing. This will enable us to engage \"system 2\" (analytical thinking) in a targeted way, thereby avoiding excessive cognitive load. Organization-level interventions should also be implemented to free up the cognitive capacity of an individual and to enable them to employ system 2 thinking more regularly.",
      "journal": "International journal of dermatology",
      "year": "2023",
      "doi": "10.1111/ijd.16348",
      "authors": "Yesudian Rohan I et al.",
      "keywords": "",
      "mesh_terms": "Humans; Dermatology; Diagnostic Errors; Bias; Cognition",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35802380/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35914194",
      "title": "Predicting Race And Ethnicity To Ensure Equitable Algorithms For Health Care Decision Making.",
      "abstract": "Algorithms are currently used to assist in a wide array of health care decisions. Despite the general utility of these health care algorithms, there is growing recognition that they may lead to unintended racially discriminatory practices, raising concerns about the potential for algorithmic bias. An intuitive precaution against such bias is to remove race and ethnicity information as an input to health care algorithms, mimicking the idea of \"race-blind\" decisions. However, we argue that this approach is misguided. Knowledge, not ignorance, of race and ethnicity is necessary to combat algorithmic bias. When race and ethnicity are observed, many methodological approaches can be used to enforce equitable algorithmic performance. When race and ethnicity information is unavailable, which is often the case, imputing them can expand opportunities to not only identify and assess algorithmic bias but also combat it in both clinical and nonclinical settings. A valid imputation method, such as Bayesian Improved Surname Geocoding, can be applied to standard data collected by public and private payers and provider entities. We describe two applications in which imputation of race and ethnicity can help mitigate potential algorithmic biases: equitable disease screening algorithms using machine learning and equitable pay-for-performance incentives.",
      "journal": "Health affairs (Project Hope)",
      "year": "2022",
      "doi": "10.1377/hlthaff.2022.00095",
      "authors": "Cabreros Irineo et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Bayes Theorem; Decision Making; Delivery of Health Care; Ethnicity; Humans; Reimbursement, Incentive",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35914194/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35932546",
      "title": "Interpretability-Guided Inductive Bias For Deep Learning Based Medical Image.",
      "abstract": "Deep learning methods provide state of the art performance for supervised learning based medical image analysis. However it is essential that trained models extract clinically relevant features for downstream tasks as, otherwise, shortcut learning and generalization issues can occur. Furthermore in the medical field, trustability and transparency of current deep learning systems is a much desired property. In this paper we propose an interpretability-guided inductive bias approach enforcing that learned features yield more distinctive and spatially consistent saliency maps for different class labels of trained models, leading to improved model performance. We achieve our objectives by incorporating a class-distinctiveness loss and a spatial-consistency regularization loss term. Experimental results for medical image classification and segmentation tasks show our proposed approach outperforms conventional methods, while yielding saliency maps in higher agreement with clinical experts. Additionally, we show how information from unlabeled images can be used to further boost performance. In summary, the proposed approach is modular, applicable to existing network architectures used for medical imaging applications, and yields improved learning rates, model robustness, and model interpretability.",
      "journal": "Medical image analysis",
      "year": "2022",
      "doi": "10.1016/j.media.2022.102551",
      "authors": "Mahapatra Dwarikanath et al.",
      "keywords": "Inductive bias; Interpretability; Medical image classification; Medical image segmentation",
      "mesh_terms": "Deep Learning; Diagnostic Imaging; Humans; Image Processing, Computer-Assisted",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35932546/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35976935",
      "title": "Should We Rely on AI to Help Avoid Bias in Patient Selection for Major Surgery?",
      "abstract": "Many regard iatrogenic injuries as consequences of diagnosis or intervention actions. But inaction-not offering indicated major surgery-can also result in iatrogenic injury. This article explores some surgeons' overestimations of operative risk based on patients' race and socioeconomic status as unduly influential in their decisions about whether to perform major cancer or cardiac surgery on some patients with appropriate clinical indications. This article also considers artificial intelligence and machine learning-based clinical decision support systems that might offer more accurate, individualized risk assessment that could make patient selection processes more equitable, thereby mitigating racial and ethnic inequity in cancer and cardiac disease.",
      "journal": "AMA journal of ethics",
      "year": "2022",
      "doi": "10.1001/amajethics.2022.773",
      "authors": "Binkley Charles E et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Decision Support Systems, Clinical; Humans; Iatrogenic Disease; Neoplasms; Patient Selection",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35976935/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36086432",
      "title": "Fair and Privacy-Preserving Alzheimer's Disease Diagnosis Based on Spontaneous Speech Analysis via Federated Learning.",
      "abstract": "As the most common neurodegenerative disease among older adults, Alzheimer's disease (AD) would lead to loss of memory, impaired language and judgment, gait disorders, and other cognitive deficits severe enough to interfere with daily activities and significantly diminish quality of life. Recent research has shown promising results in automatic AD diagnosis via speech, leveraging the advances of deep learning in the audio domain. However, most existing studies rely on a centralized learning framework which requires subjects' voice data to be gathered to a central server, raising severe privacy concerns. To resolve this, in this paper, we propose the first federated-learning-based approach for achieving automatic AD diagnosis via spontaneous speech analysis while ensuring the subjects' data privacy. Extensive experiments under various federated learning settings on the ADReSS challenge dataset show that the proposed model can achieve high accuracy for AD detection while achieving privacy preservation. To ensure fairness of the model performance across clients in federated settings, we further deploy fair aggregation mechanisms, particularly q-FEDAvg and q-FEDSgd, which greatly reduces the algorithmic biases due to the data heterogeneity among the clients. Clinical Relevance -The experiments were conducted on publicly available clinical datasets. No humans or animals were involved.",
      "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference",
      "year": "2022",
      "doi": "10.1109/EMBC48229.2022.9871204",
      "authors": "Ali Meerza Syed Irfan et al.",
      "keywords": "",
      "mesh_terms": "Alzheimer Disease; Humans; Neurodegenerative Diseases; Privacy; Quality of Life; Speech",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36086432/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36126552",
      "title": "Jointly estimating bias field and reconstructing uniform MRI image by deep learning.",
      "abstract": "Bias field is one of the main artifacts that degrade the quality of magnetic resonance images. It introduces intensity inhomogeneity and affects image analysis such as segmentation. In this work, we proposed a deep learning approach to jointly estimate bias field and reconstruct uniform image. By modeling the quality degradation process as the product of a spatially varying field and a uniform image, the network was trained on 800 images with true bias fields from 12 healthy subjects. A network structure of bias field estimation and uniform image reconstruction was designed to compensate for the intensity loss. To further evaluate the benefit of bias field correction, a quantitative analysis was made on image segmentation. Experimental results show that the proposed BFCNet improves the image uniformity by 8.3% and 10.1%, the segmentation accuracy by 4.1% and 6.8% on white and grey matter in T2-weighted brain images. Moreover, BFCNet outperforms the state-of-the-art traditional methods and deep learning methods on estimating bias field and preserving image structure, and BFCNet is robust to different levels of bias field and noise.",
      "journal": "Journal of magnetic resonance (San Diego, Calif. : 1997)",
      "year": "2022",
      "doi": "10.1016/j.jmr.2022.107301",
      "authors": "Song Wenke et al.",
      "keywords": "Bias field correction; Deep learning; Intensity inhomogeneity",
      "mesh_terms": "Humans; Algorithms; Deep Learning; Magnetic Resonance Imaging; Image Processing, Computer-Assisted; Artifacts; Brain",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36126552/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36155458",
      "title": "D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias.",
      "abstract": "With the rise of AI, algorithms have become better at learning underlying patterns from the training data including ingrained social biases based on gender, race, etc. Deployment of such algorithms to domains such as hiring, healthcare, law enforcement, etc. has raised serious concerns about fairness, accountability, trust and interpretability in machine learning algorithms. To alleviate this problem, we propose D-BIAS, a visual interactive tool that embodies human-in-the-loop AI approach for auditing and mitigating social biases from tabular datasets. It uses a graphical causal model to represent causal relationships among different features in the dataset and as a medium to inject domain knowledge. A user can detect the presence of bias against a group, say females, or a subgroup, say black females, by identifying unfair causal relationships in the causal network and using an array of fairness metrics. Thereafter, the user can mitigate bias by refining the causal model and acting on the unfair causal edges. For each interaction, say weakening/deleting a biased causal edge, the system uses a novel method to simulate a new (debiased) dataset based on the current causal model while ensuring a minimal change from the original dataset. Users can visually assess the impact of their interactions on different fairness metrics, utility metrics, data distortion, and the underlying data distribution. Once satisfied, they can download the debiased dataset and use it for any downstream application for fairer predictions. We evaluate D-BIAS by conducting experiments on 3 datasets and also a formal user study. We found that D-BIAS helps reduce bias significantly compared to the baseline debiasing approach across different fairness metrics while incurring little data distortion and a small loss in utility. Moreover, our human-in-the-loop based approach significantly outperforms an automated approach on trust, interpretability and accountability.",
      "journal": "IEEE transactions on visualization and computer graphics",
      "year": "2023",
      "doi": "10.1109/TVCG.2022.3209484",
      "authors": "Ghai Bhavya et al.",
      "keywords": "",
      "mesh_terms": "Female; Humans; Computer Graphics; Causality; Algorithms; Machine Learning; Bias",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36155458/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36165796",
      "title": "Generalizability and Bias in a Deep Learning Pediatric Bone Age Prediction Model Using Hand Radiographs.",
      "abstract": "Background Although deep learning (DL) models have demonstrated expert-level ability for pediatric bone age prediction, they have shown poor generalizability and bias in other use cases. Purpose To quantify generalizability and bias in a bone age DL model measured by performance on external versus internal test sets and performance differences between different demographic groups, respectively. Materials and Methods The winning DL model of the 2017 RSNA Pediatric Bone Age Challenge was retrospectively evaluated and trained on 12\u2009611 pediatric hand radiographs from two U.S. hospitals. The DL model was tested from September 2021 to December 2021 on an internal validation set and an external test set of pediatric hand radiographs with diverse demographic representation. Images reporting ground-truth bone age were included for study. Mean absolute difference (MAD) between ground-truth bone age and the model prediction bone age was calculated for each set. Generalizability was evaluated by comparing MAD between internal and external evaluation sets with use of t tests. Bias was evaluated by comparing MAD and clinically significant error rate (rate of errors changing the clinical diagnosis) between demographic groups with use of t tests or analysis of variance and \u03c72 tests, respectively (statistically significant difference defined as P < .05). Results The internal validation set had images from 1425 individuals (773 boys), and the external test set had images from 1202 individuals (mean age, 133 months \u00b1 60 [SD]; 614 boys). The bone age model generalized well to the external test set, with no difference in MAD (6.8 months in the validation set vs 6.9 months in the external set; P = .64). Model predictions would have led to clinically significant errors in 194 of 1202 images (16%) in the external test set. The MAD was greater for girls than boys in the internal validation set (P = .01) and in the subcategories of age and Tanner stage in the external test set (P < .001 for both). Conclusion A deep learning (DL) bone age model generalized well to an external test set, although clinically significant sex-, age-, and sexual maturity-based biases in DL bone age were identified. \u00a9 RSNA, 2022 Online supplemental material is available for this article See also the editorial by Larson in this issue.",
      "journal": "Radiology",
      "year": "2023",
      "doi": "10.1148/radiol.220505",
      "authors": "Beheshtian Elham et al.",
      "keywords": "",
      "mesh_terms": "Male; Female; Humans; Child; Infant; Deep Learning; Retrospective Studies; Radiography",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36165796/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36190605",
      "title": "Scheduling mobile dental clinics: A heuristic approach considering fairness among school districts.",
      "abstract": "Mobile dental clinics (MDCs) are suitable solutions for servicing people living in rural and urban areas that require dental healthcare. MDCs can provide dental care to the most vulnerable high-school students. However, scheduling MDCs to visit patients is critical to developing efficient dental programs. Here, we study a mobile dental clinic scheduling problem that arises from the real-life logistics management challenge faced by a school-based mobile dental care program in Southern Chile. This problem involves scheduling MDCs to treat high-school students at public schools while considering a fairness constraint among districts. Schools are circumscribed into districts, and by program regulations, at least 50% of the students in each district must receive dental care during the first semester. Fairness prevents some districts from waiting more time to receive dental care than others. We model the problem as a parallel machine scheduling problem with sequence-dependent setup costs and batch due dates and propose a mathematical model and a genetic algorithm-based solution to solve the problem. Our computational results demonstrate the effectiveness of our approaches in obtaining near-optimal solutions. Finally, dental program managers can use the methodologies presented in this work to schedule mobile dental clinics and improve their operations.",
      "journal": "Health care management science",
      "year": "2024",
      "doi": "10.1007/s10729-022-09612-5",
      "authors": "Sep\u00falveda Ignacio A et al.",
      "keywords": "Dental care; Health care management; Mobile dental clinic; Operations research; Scheduling",
      "mesh_terms": "Humans; Dental Clinics; Heuristics; Delivery of Health Care; Students; Costs and Cost Analysis",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36190605/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36396503",
      "title": "Algorithmic bias in health care: Opportunities for nurses to improve equality in the age of artificial intelligence.",
      "abstract": "",
      "journal": "Nursing outlook",
      "year": "2022",
      "doi": "10.1016/j.outlook.2022.09.003",
      "authors": "O'Connor Siobhan et al.",
      "keywords": "Algorithms; Artificial Intelligence; Bias; Health care; Machine learning; Natural language processing; Neural networks; Nursing",
      "mesh_terms": "Humans; Artificial Intelligence; Bias; Delivery of Health Care",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36396503/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36481995",
      "title": "When the Process Is the Problem: Racial/Ethnic and Language Disparities in Care Management.",
      "abstract": "OBJECTIVES: Achieving health equity requires addressing disparities at every level of care delivery. Yet, little literature exists examining racial/ethnic disparities in processes of high-risk care management, a foundational tool for population health. This study sought to determine whether race, ethnicity, and language are associated with patient entry into and service intensity within a large care management program. DESIGN: Retrospective cohort study. METHODS: Subjects were 23,836 adult patients eligible for the program between 2015 and 2018. Adjusting for demographics, utilization, and medical risk, we analyzed the association between race/ethnicity and language and outcomes of patient selection, enrollment, care plan completion, and care management encounters. RESULTS: Among all identified as eligible by an algorithm, Asian and Spanish-speaking patients had significantly lower odds of being selected by physicians for care management [OR 0.74 (0.58-0.93), OR 0.79 (0.64-0.97)] compared with White and English-speaking patients, respectively. Once selected, Hispanic/Latino and Asian patients had significantly lower odds compared to White counterparts of having care plans completed by care managers [OR 0.69 (0.50-0.97), 0.50 (0.32-0.79), respectively]. Patients speaking languages other than English or Spanish had a lower odds of care plan completion and had fewer staff encounters than English-speaking counterparts [OR 0.62 (0.44-0.87), RR 0.87 (0.75-1.00), respectively]. CONCLUSIONS: Race/ethnicity and language-based disparities exist at every process level within a large health system's care management program, from selection to outreach. These results underscore the importance of assessing for disparities not just in outcomes but also in program processes, to prevent population health innovations from inadvertently creating new inequities.",
      "journal": "Journal of racial and ethnic health disparities",
      "year": "2023",
      "doi": "10.1007/s40615-022-01469-2",
      "authors": "Wang Priscilla G et al.",
      "keywords": "Care management; Disparity; Equity; Language; Population health; Race",
      "mesh_terms": "Adult; Humans; Delivery of Health Care; Ethnicity; Healthcare Disparities; Language; Retrospective Studies; Racial Groups",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36481995/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36533881",
      "title": "Quantification of MR spectra by deep learning in an idealized setting: Investigation of forms of input, network architectures, optimization by ensembles of networks, and training bias.",
      "abstract": "PURPOSE: The aims of this work are (1) to explore deep learning (DL) architectures, spectroscopic input types, and learning designs toward optimal quantification in MR spectroscopy of simulated pathological spectra; and (2) to demonstrate accuracy and precision of DL predictions in view of inherent bias toward the training distribution. METHODS: Simulated 1D spectra and 2D spectrograms that mimic an extensive range of pathological in vivo conditions are used to train and test 24 different DL architectures. Active learning through altered training and testing data distributions is probed to optimize quantification performance. Ensembles of networks are explored to improve DL robustness and reduce the variance of estimates. A set of scores compares performances of DL predictions and traditional model fitting (MF). RESULTS: Ensembles of heterogeneous networks that combine 1D frequency-domain and 2D time-frequency domain spectrograms as input perform best. Dataset augmentation with active learning can improve performance, but gains are limited. MF is more accurate, although DL appears to be more precise at low SNR. However, this overall improved precision originates from a strong bias for cases with high uncertainty toward the dataset the network has been trained with, tending toward its average value. CONCLUSION: MF mostly performs better compared to the faster DL approach. Potential intrinsic biases on training sets are dangerous in a clinical context that requires the algorithm to be unbiased to outliers (i.e., pathological data). Active learning and ensemble of networks are good strategies to improve prediction performances. However, data quality (sufficient SNR) has proven as a bottleneck for adequate unbiased performance-like in the case of MF.",
      "journal": "Magnetic resonance in medicine",
      "year": "2023",
      "doi": "10.1002/mrm.29561",
      "authors": "Rizzo Rudy et al.",
      "keywords": "active learning; bias; deep learning; ensemble of networks; magnetic resonance spectroscopy; model fitting; quantification",
      "mesh_terms": "Deep Learning; Algorithms; Bias",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36533881/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36538072",
      "title": "The uncovered biases and errors in clinical determination of bone age by using deep learning models.",
      "abstract": "OBJECTIVES: To evaluate AI biases and errors in estimating bone age (BA) by comparing AI and radiologists' clinical determinations of BA. METHODS: We established three deep learning models from a Chinese private dataset (CHNm), an American public dataset (USAm), and a joint dataset combining the above two datasets (JOIm). The test data CHNt (n = 1246) were labeled by ten senior pediatric radiologists. The effects of data site differences, interpretation bias, and interobserver variability on BA assessment were evaluated. The differences between the AI models' and radiologists' clinical determinations of BA (normal, advanced, and delayed BA groups by using the Brush data) were evaluated by the chi-square test and Kappa values. The heatmaps of CHNm-CHNt were generated by using Grad-CAM. RESULTS: We obtained an MAD value of 0.42 years on CHNm-CHNt; this result indicated an appropriate accuracy for the whole group but did not indicate an accurate estimation of individual BA because with a kappa value of 0.714, the agreement between AI and human clinical determinations of BA was significantly different. The features of the heatmaps were not fully consistent with the human vision on the X-ray films. Variable performance in BA estimation by different AI models and the disagreement between AI and radiologists' clinical determinations of BA may be caused by data biases, including patients' sex and age, institutions, and radiologists. CONCLUSIONS: The deep learning models outperform external validation in predicting BA on both internal and joint datasets. However, the biases and errors in the models' clinical determinations of child development should be carefully considered. KEY POINTS: \u2022 With a kappa value of 0.714, clinical determinations of bone age by using AI did not accord well with clinical determinations by radiologists. \u2022 Several biases, including patients' sex and age, institutions, and radiologists, may cause variable performance by AI bone age models and disagreement between AI and radiologists' clinical determinations of bone age. \u2022 AI heatmaps of bone age were not fully consistent with human vision on X-ray films.",
      "journal": "European radiology",
      "year": "2023",
      "doi": "10.1007/s00330-022-09330-0",
      "authors": "Bai Mei et al.",
      "keywords": "Child development; Computers; Deep learning; Radiologists; X-ray film",
      "mesh_terms": "Child; Humans; Bias; Deep Learning; Radiologists; United States; Age Determination by Skeleton; Wrist; Fingers; Male; Female; Child, Preschool; Adolescent; Observer Variation; Diagnostic Errors; Computer Simulation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36538072/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36633513",
      "title": "SeroTracker-RoB: A decision rule-based algorithm for reproducible risk of bias assessment of seroprevalence studies.",
      "abstract": "Risk of bias (RoB) assessments are a core element of evidence synthesis but can be time consuming and subjective. We aimed to develop a decision rule-based algorithm for RoB assessment of seroprevalence studies. We developed the SeroTracker-RoB algorithm. The algorithm derives seven objective and two subjective critical appraisal items from the Joanna Briggs Institute Critical Appraisal Checklist for Prevalence studies and implements decision rules that determine study risk of bias based on the items. Decision rules were validated using the SeroTracker seroprevalence study database, which included non-algorithmic RoB judgments from two reviewers. We quantified efficiency as the mean difference in time for the algorithmic and non-algorithmic assessments of 80 randomly selected articles, coverage as the proportion of studies where the decision rules yielded an assessment, and reliability using intraclass correlations comparing algorithmic and non-algorithmic assessments for 2070 articles. A set of decision rules with 61 branches was developed using responses to the nine critical appraisal items. The algorithmic approach was faster than non-algorithmic assessment (mean reduction 2.32\u2009min [SD 1.09] per article), classified 100% (n\u00a0=\u20092070) of studies, and had good reliability compared to non-algorithmic assessment (ICC 0.77, 95% CI 0.74-0.80). We built the SeroTracker-RoB Excel Tool, which embeds this algorithm for use by other researchers. The SeroTracker-RoB decision-rule based algorithm was faster than non-algorithmic assessment with complete coverage and good reliability. This algorithm enabled rapid, transparent, and reproducible RoB evaluations of seroprevalence studies and may support evidence synthesis efforts during future disease outbreaks. This decision rule-based approach could be applied to other types of prevalence studies.",
      "journal": "Research synthesis methods",
      "year": "2023",
      "doi": "10.1002/jrsm.1620",
      "authors": "Bobrovitz Niklas et al.",
      "keywords": "algorithm; critical appraisal; decision rule; evidence synthesis; infectious disease; prevalence; risk of bias",
      "mesh_terms": "Reproducibility of Results; Seroepidemiologic Studies; Research Design; Bias; Risk Assessment",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36633513/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36633971",
      "title": "How We Got Here: The Legacy of Anti-Black Discrimination in Radiology.",
      "abstract": "Current disparities in the access to diagnostic imaging for Black patients and the underrepresentation of Black physicians in radiology, relative to their representation in the general U.S. population, reflect contemporary consequences of historical anti-Black discrimination. These disparities have existed within the field of radiology and professional medical organizations since their inception. Explicit and implicit racism against Black patients and physicians was institutional policy in the early 20th century when radiology was being developed as a clinical medical field. Early radiology organizations also embraced this structural discrimination, creating strong barriers to professional Black radiologist involvement. Nevertheless, there were numerous pioneering Black radiologists who advanced scholarship, patient care, and diversity within medicine and radiology during the early 20th century. This work remains important in the present day, as race-based health care disparities persist and continue to decrease the quality of radiology-delivered patient care. There are also structural barriers within radiology affecting workforce diversity that negatively impact marginalized groups. Multiple opportunities exist today for antiracism work to improve quality of care and to apply standards of social justice and health equity to the field of radiology. An initial step is to expand education on the disparities in access to imaging and health care among Black patients. Institutional interventions include implementing community-based outreach and applying antibias methodology in artificial intelligence algorithms, while systemic interventions include identifying national race-based quality measures and ensuring imaging guidelines properly address the unique cancer risks in the Black patient population. These approaches reflect some of the strategies that may mutually serve to address health care disparities in radiology. \u00a9 RSNA, 2023 See the invited commentary by Scott in this issue. Quiz questions for this article are available in the supplemental material.",
      "journal": "Radiographics : a review publication of the Radiological Society of North America, Inc",
      "year": "2023",
      "doi": "10.1148/rg.220112",
      "authors": "Goldberg Julia E et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Radiology; Radiography; Physicians; Radiologists",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36633971/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36651834",
      "title": "Bias and Non-Diversity of Big Data in Artificial Intelligence: Focus on Retinal Diseases.",
      "abstract": "Artificial intelligence (AI) applications in healthcare will have a potentially far-reaching impact on patient care, however issues regarding algorithmic bias and fairness have recently surfaced. There is a recognized lack of diversity in the available ophthalmic datasets, with 45% of the global population having no readily accessible representative images, leading to potential misrepresentations of their unique anatomic features and ocular pathology. AI applications in retinal disease may show less accuracy with underrepresented populations that may further widen the gap of health inequality if left unaddressed. Beyond disease symptomatology, social determinants of health must be integrated into our current paradigms of disease understanding, with the goal of more personalized care. AI has the potential to decrease global healthcare inequality, but it will need to be based on a more diverse, transparent and responsible use of healthcare data.",
      "journal": "Seminars in ophthalmology",
      "year": "2023",
      "doi": "10.1080/08820538.2023.2168486",
      "authors": "Jacoba Cris Martin P et al.",
      "keywords": "Artificial Intelligence; Big Data; Diversity; Retina; Social Determinants of Health",
      "mesh_terms": "Humans; Big Data; Artificial Intelligence; Health Status Disparities; Retinal Diseases; Eye",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36651834/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36651866",
      "title": "Sex differences and disparities in cardiovascular outcomes of COVID-19.",
      "abstract": "AIMS: Previous analyses on sex differences in case fatality rates at population-level data had limited adjustment for key patient clinical characteristics thought to be associated with coronavirus disease 2019 (COVID-19) outcomes. We aimed to estimate the risk of specific organ dysfunctions and mortality in women and men. METHODS AND RESULTS: This retrospective cross-sectional study included 17 hospitals within 5 European countries participating in the International Survey of Acute Coronavirus Syndromes COVID-19 (NCT05188612). Participants were individuals hospitalized with positive severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) from March 2020 to February 2022. Risk-adjusted ratios (RRs) of in-hospital mortality, acute respiratory failure (ARF), acute heart failure (AHF), and acute kidney injury (AKI) were calculated for women vs. men. Estimates were evaluated by inverse probability weighting and logistic regression models. The overall care cohort included 4499 patients with COVID-19-associated hospitalizations. Of these, 1524 (33.9%) were admitted to intensive care unit (ICU), and 1117 (24.8%) died during hospitalization. Compared with men, women were less likely to be admitted to ICU [RR: 0.80; 95% confidence interval (CI): 0.71-0.91]. In general wards (GWs) and ICU cohorts, the adjusted women-to-men RRs for in-hospital mortality were of 1.13 (95% CI: 0.90-1.42) and 0.86 (95% CI: 0.70-1.05; pinteraction = 0.04). Development of AHF, AKI, and ARF was associated with increased mortality risk (odds ratios: 2.27, 95% CI: 1.73-2.98; 3.85, 95% CI: 3.21-4.63; and 3.95, 95% CI: 3.04-5.14, respectively). The adjusted RRs for AKI and ARF were comparable among women and men regardless of intensity of care. In contrast, female sex was associated with higher odds for AHF in GW, but not in ICU (RRs: 1.25; 95% CI: 0.94-1.67 vs. 0.83; 95% CI: 0.59-1.16, pinteraction = 0.04). CONCLUSIONS: Women in GW were at increased risk of AHF and in-hospital mortality for COVID-19 compared with men. For patients receiving ICU care, fatal complications including AHF and mortality appeared to be independent of sex. Equitable access to COVID-19 ICU care is needed to minimize the unfavourable outcome of women presenting with COVID-19-related complications.",
      "journal": "Cardiovascular research",
      "year": "2023",
      "doi": "10.1093/cvr/cvad011",
      "authors": "Bugiardini Raffaele et al.",
      "keywords": "Acute heart failure; Acute kidney injury; Acute respiratory failure; COVID-19; Mortality; Sex; Women",
      "mesh_terms": "Humans; Female; Male; COVID-19; SARS-CoV-2; Retrospective Studies; Sex Characteristics; Cross-Sectional Studies; Risk Factors; Acute Kidney Injury",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36651866/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36716733",
      "title": "Do not treat Bill Gates for prostate cancer! Algorithmic bias and causality in medical prediction.",
      "abstract": "",
      "journal": "BJU international",
      "year": "2023",
      "doi": "10.1111/bju.15951",
      "authors": "Vickers Andrew",
      "keywords": "",
      "mesh_terms": "Male; Humans; Bias; Prostatic Neoplasms",
      "pub_types": "Editorial; Research Support, N.I.H., Extramural; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36716733/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36717257",
      "title": "IARC-NCI workshop on an epidemiological toolkit to assess biases in human cancer studies for hazard identification: beyond the algorithm.",
      "abstract": "",
      "journal": "Occupational and environmental medicine",
      "year": "2023",
      "doi": "10.1136/oemed-2022-108724",
      "authors": "Schubauer-Berigan Mary K et al.",
      "keywords": "Environment; Epidemiology; Occupational Health; Statistics",
      "mesh_terms": "Humans; Neoplasms; Carcinogens; Bias; Algorithms",
      "pub_types": "Editorial; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36717257/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36738708",
      "title": "Real-time gastric intestinal metaplasia diagnosis tailored for bias and noisy-labeled data with multiple endoscopic imaging.",
      "abstract": "This work presents real-time segmentation viz. gastric intestinal metaplasia (GIM). Recently, GIM segmentation of endoscopic images has been carried out to differentiate GIM from a healthy stomach. However, real-time detection is difficult to achieve. Conditions are challenging, and include multiple color modes (white light endoscopy and narrow-band imaging), other abnormal lesions (erosion and ulcer), noisy labels etc. Herein, our model is based on BiSeNet and can overcome the many issues regarding GIM. Application of auxiliary head and additional loss are seen to improve performance as well as enhance multiple color modes accurately. Further, multiple pre-processing techniques are utilized for leveraging detection performance: namely, location-wise negative sampling, jigsaw augmentation, and label smoothing. Finally, the decision threshold can be adjusted separately for each color mode. Work undertaken at King Chulalongkorn Memorial Hospital examined 940 histologically proven GIM images and 1239 non-GIM images, obtained over 173 frames per second (FPS). In terms of accuracy, our model is seen to outperform all baselines. Our results demonstrate sensitivity, specificity, positive predictive, negative predictive, accuracy, and mean intersection over union (IoU), achieving GIM segmentation values of 91%, 96%, 91%, 91%, 96%, and 55%, respectively.",
      "journal": "Computers in biology and medicine",
      "year": "2023",
      "doi": "10.1016/j.compbiomed.2023.106582",
      "authors": "Pornvoraphat Passin et al.",
      "keywords": "Deep learning; Gastric intestinal metaplasia; Real-time semantic segmentation",
      "mesh_terms": "Humans; Gastroscopy; Stomach Neoplasms; Narrow Band Imaging; Metaplasia; Precancerous Conditions",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36738708/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36754076",
      "title": "How Should Clinicians and Health Care Organizations Promote Equity in Child Abuse and Neglect Suspicion, Evaluation, and Reporting?",
      "abstract": "Victims of child abuse and neglect come from every racial, ethnic, and socioeconomic background, yet clinical evaluation, reporting to child protective services, and responses to reports inequitably harm Black children and malign families of color. Racial bias and inequity in suspicion, reporting, and substantiation of abuse and neglect and in services offered and delivered, foster care placement, and criminal prosecution are widely documented. In response, clinicians and health care organizations should promote equity by educating clinicians about racial bias, standardizing evaluation using clinical decision support tools, and working with policy makers to support prevention services. If we decide that it is ethically justifiable for clinicians to err on the side of overreporting, we must ensure fair distribution of associated benefits and harms among all children and families.",
      "journal": "AMA journal of ethics",
      "year": "2023",
      "doi": "10.1001/amajethics.2023.133",
      "authors": "Lane Wendy G et al.",
      "keywords": "",
      "mesh_terms": "Child; Humans; Child Abuse; Child Welfare; Racial Groups; Delivery of Health Care",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36754076/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36939724",
      "title": "Effects of Racial Bias in Pulse Oximetry on Children and How to Address Algorithmic Bias in Clinical Medicine.",
      "abstract": "",
      "journal": "JAMA pediatrics",
      "year": "2023",
      "doi": "10.1001/jamapediatrics.2023.0077",
      "authors": "Gray Keyaria D et al.",
      "keywords": "",
      "mesh_terms": "Child; Humans; Racism; Oximetry; Oxygen; Bias; Clinical Medicine",
      "pub_types": "Editorial; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36939724/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36997714",
      "title": "AI 'fairness' research held back by lack of diversity.",
      "abstract": "",
      "journal": "Nature",
      "year": "2023",
      "doi": "10.1038/d41586-023-00935-z",
      "authors": "Wong Carissa",
      "keywords": "Health care; Machine learning; Scientific community",
      "mesh_terms": "",
      "pub_types": "News",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36997714/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37018335",
      "title": "Proportionally Fair Hospital Collaborations in Federated Learning of Histopathology Images.",
      "abstract": "Medical centers and healthcare providers have concerns and hence restrictions around sharing data with external collaborators. Federated learning, as a privacy-preserving method, involves learning a site-independent model without having direct access to patient-sensitive data in a distributed collaborative fashion. The federated approach relies on decentralized data distribution from various hospitals and clinics. The collaboratively learned global model is supposed to have acceptable performance for the individual sites. However, existing methods focus on minimizing the average of the aggregated loss functions, leading to a biased model that performs perfectly for some hospitals while exhibiting undesirable performance for other sites. In this paper, we improve model \"fairness\" among participating hospitals by proposing a novel federated learning scheme called Proportionally Fair Federated Learning, short Prop-FFL. Prop-FFL is based on a novel optimization objective function to decrease the performance variations among participating hospitals. This function encourages a fair model, providing us with more uniform performance across participating hospitals. We validate the proposed Prop-FFL on two histopathology datasets as well as two general datasets to shed light on its inherent capabilities. The experimental results suggest promising performance in terms of learning speed, accuracy, and fairness.",
      "journal": "IEEE transactions on medical imaging",
      "year": "2023",
      "doi": "10.1109/TMI.2023.3234450",
      "authors": "Hosseini S Maryam et al.",
      "keywords": "",
      "mesh_terms": "Humans; Hospitals; Supervised Machine Learning; Pathology",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37018335/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37027665",
      "title": "Hierarchical Bias Mitigation for Semi-Supervised Medical Image Classification.",
      "abstract": "Semi-supervised learning (SSL) has demonstrated remarkable advances on medical image classification, by harvesting beneficial knowledge from abundant unlabeled samples. The pseudo labeling dominates current SSL approaches, however, it suffers from intrinsic biases within the process. In this paper, we retrospect the pseudo labeling and identify three hierarchical biases: perception bias, selection bias and confirmation bias, at feature extraction, pseudo label selection and momentum optimization stages, respectively. In this regard, we propose a HierArchical BIas miTigation (HABIT) framework to amend these biases, which consists of three customized modules including Mutual Reconciliation Network (MRNet), Recalibrated Feature Compensation (RFC) and Consistency-aware Momentum Heredity (CMH). Firstly, in the feature extraction, MRNet is devised to jointly utilize convolution and permutator-based paths with a mutual information transfer module to exchanges features and reconcile spatial perception bias for better representations. To address pseudo label selection bias, RFC adaptively recalibrates the strong and weak augmented distributions to be a rational discrepancy and augments features for minority categories to achieve the balanced training. Finally, in the momentum optimization stage, in order to reduce the confirmation bias, CMH models the consistency among different sample augmentations into network updating process to improve the dependability of the model. Extensive experiments on three semi-supervised medical image classification datasets demonstrate that HABIT mitigates three biases and achieves state-of-the-art performance. Our codes are available at https://github.com/CityU-AIM-Group/HABIT.",
      "journal": "IEEE transactions on medical imaging",
      "year": "2023",
      "doi": "10.1109/TMI.2023.3247440",
      "authors": "Yang Qiushi et al.",
      "keywords": "",
      "mesh_terms": "Bias; Motion; Supervised Machine Learning",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37027665/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37036329",
      "title": "Craniofacial Soft-Tissue Anthropomorphic Database with Magnetic Resonance Imaging and Unbiased Diffeomorphic Registration.",
      "abstract": "BACKGROUND: Objective assessment of craniofacial surgery outcomes in a pediatric population is challenging because of the complexity of patient presentations, diversity of procedures performed, and rapid craniofacial growth. There is a paucity of robust methods to quantify anatomical measurements by age and objectively compare craniofacial dysmorphology and postoperative outcomes. Here, the authors present data in developing a racially and ethnically sensitive anthropomorphic database, providing plastic and craniofacial surgeons with \"normal\" three-dimensional anatomical parameters with which to appraise and optimize aesthetic and reconstructive outcomes. METHODS: Patients with normal craniofacial anatomy undergoing head magnetic resonance imaging (MRI) scans from 2008 to 2021 were included in this retrospective study. Images were used to construct composite (template) images with diffeomorphic image registration method using the Advanced Normalization Tools package. Composites were thresholded to generate binary three-dimensional segmentations used for anatomical measurements in Materalise Mimics. RESULTS: High-resolution MRI scans from 130 patients generated 12 composites from an average of 10 MRI sequences each: four 3-year-olds, four 4-year-olds, and four 5-year-olds (two male, two female, two Black, and two White). The average head circumference of 3-, 4-, and 5-year-old composites was 50.3, 51.5, and 51.7 cm, respectively, comparable to normative data published by the World Health Organization. CONCLUSIONS: Application of diffeomorphic registration-based image template algorithm to MRI is effective in creating composite templates to represent \"normal\" three-dimensional craniofacial and soft-tissue anatomy. Future research will focus on development of automated computational tools to characterize anatomical normality, generation of indices to grade preoperative severity, and quantification of postoperative results to reduce subjectivity bias.",
      "journal": "Plastic and reconstructive surgery",
      "year": "2024",
      "doi": "10.1097/PRS.0000000000010526",
      "authors": "Villavisanis Dillan F et al.",
      "keywords": "",
      "mesh_terms": "Humans; Child; Male; Female; Child, Preschool; Retrospective Studies; Image Processing, Computer-Assisted; Cephalometry; Algorithms; Magnetic Resonance Imaging; Imaging, Three-Dimensional",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37036329/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37097792",
      "title": "Post-Processing Fairness Evaluation of Federated Models: An Unsupervised Approach in Healthcare.",
      "abstract": "Modern Healthcare cyberphysical systems have begun to rely more and more on distributed AI leveraging the power of Federated Learning (FL). Its ability to train Machine Learning (ML) and Deep Learning (DL) models for the wide variety of medical fields, while at the same time fortifying the privacy of the sensitive information that are present in the medical sector, makes the FL technology a necessary tool in modern health and medical systems. Unfortunately, due to the polymorphy of distributed data and the shortcomings of distributed learning, the local training of Federated models sometimes proves inadequate and thus negatively imposes the federated learning optimization process and in extend in the subsequent performance of the rest Federated models. Badly trained models can cause dire implications in the healthcare field due to their critical nature. This work strives to solve this problem by applying a post-processing pipeline to models used by FL. In particular, the proposed work ranks the model by finding how fair they are by discovering and inspecting micro-Manifolds that cluster each neural model's latent knowledge. The produced work applies a completely unsupervised both model and data agnostic methodology that can be leveraged for general model fairness discovery. The proposed methodology is tested against a variety of benchmark DL architectures and in the FL environment, showing an average 8.75% increase in Federated model accuracy in comparison with similar work.",
      "journal": "IEEE/ACM transactions on computational biology and bioinformatics",
      "year": "2023",
      "doi": "10.1109/TCBB.2023.3269767",
      "authors": "Siniosoglou Ilias et al.",
      "keywords": "",
      "mesh_terms": "Benchmarking; Machine Learning; Delivery of Health Care",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37097792/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37099916",
      "title": "Automatic sleep staging for the young and the old - Evaluating age bias in deep learning.",
      "abstract": "BACKGROUND: Various deep-learning systems have been proposed for automated sleep staging. Still, the significance of age-specific underrepresentation in training data and the resulting errors in clinically used sleep metrics are unknown. METHODS: We adopted XSleepNet2, a deep neural network for automated sleep staging, to train and test models using polysomnograms of 1232 children (7.0\u00a0\u00b1\u00a01.4 years) and 3757 adults (56.9\u00a0\u00b1\u00a019.4 years) and 2788 older adults (mean 80.7\u00a0\u00b1\u00a04.2 years). We developed four separate sleep stage classifiers using exclusively pediatric (P), adult (A), older adults (O) as well as PSG from mixed cohorts: pediatric, adult, and older adult (PAO). Results were compared against an alternative sleep stager (DeepSleepNet) for validation purposes. RESULTS: When pediatric PSG was classified by XSleepNet2 exclusively trained on pediatric PSG, the overall accuracy was 88.9%, dropping to 78.9% when subjected to a system trained exclusively on adult PSG. Errors performed by the system staging PSG of older people were comparably lower. However, all systems produced significant errors in clinical markers when considering individual PSG. Results obtained with DeepSleepNet showed similar patterns. CONCLUSION: Underrepresentation of age groups, in particular children, can significantly lower the performance of automatic deep-learning sleep stagers. In general, automated sleep stagers may behave unexpectedly, limiting clinical use. Future evaluation of automated systems must pay attention to PSG-level performance and overall accuracy.",
      "journal": "Sleep medicine",
      "year": "2023",
      "doi": "10.1016/j.sleep.2023.04.002",
      "authors": "Baumert Mathias et al.",
      "keywords": "Sleep staging; machine learning; polysomnography",
      "mesh_terms": "Humans; Child; Aged; Deep Learning; Sleep Stages; Sleep; Neural Networks, Computer; Polysomnography",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37099916/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37130756",
      "title": "Addressing bias in artificial intelligence for public health surveillance.",
      "abstract": "Components of artificial intelligence (AI) for analysing social big data, such as natural language processing (NLP) algorithms, have improved the timeliness and robustness of health data. NLP techniques have been implemented to analyse large volumes of text from social media platforms to gain insights on disease symptoms, understand barriers to care and predict disease outbreaks. However, AI-based decisions may contain biases that could misrepresent populations, skew results or lead to errors. Bias, within the scope of this paper, is described as the difference between the predictive values and true values within the modelling of an algorithm. Bias within algorithms may lead to inaccurate healthcare outcomes and exacerbate health disparities when results derived from these biased algorithms are applied to health interventions. Researchers who implement these algorithms must consider when and how bias may arise. This paper explores algorithmic biases as a result of data collection, labelling and modelling of NLP algorithms. Researchers have a role in ensuring that efforts towards combating bias are enforced, especially when drawing health conclusions derived from social media posts that are linguistically diverse. Through the implementation of open collaboration, auditing processes and the development of guidelines, researchers may be able to reduce bias and improve NLP algorithms that improve health surveillance.",
      "journal": "Journal of medical ethics",
      "year": "2024",
      "doi": "10.1136/jme-2022-108875",
      "authors": "Flores Lidia et al.",
      "keywords": "decision making; ethics; ethics- medical; ethics- research; information technology",
      "mesh_terms": "Humans; Artificial Intelligence; Public Health Surveillance; Bias; Data Collection; Disease Outbreaks",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37130756/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37203610",
      "title": "Assessing the FAIRness of Deep Learning Models in Cardiovascular Disease Using Computed Tomography Images: Data and Code Perspective.",
      "abstract": "The interest in the application of AI in medicine has intensely increased over the past decade with most of the changes in the past five years. Most recently, the application of deep learning algorithms in prediction and classification of cardiovascular diseases (CVD) using computed tomography (CT) images showed promising results. The notable and exciting advancement in this area of study is, however, associated with different challenges related to the findability (F), accessibility(A), interoperability(I), reusability(R) of both data and source code. The aim of this work is to identify reoccurring missing FAIR-related features and to assess the level of FAIRness of data and models used to predict/diagnose cardiovascular diseases from CT images. We evaluated the FAIRness of data and models in published studies using the RDA (Research Data Alliance) FAIR Data maturity model and FAIRshake toolkit. The finding showed that although AI is anticipated to bring ground breaking solutions for complex medical problems, the findability, accessibility, interoperability and reusability of data/metadata/code is still a prominent challenge.",
      "journal": "Studies in health technology and informatics",
      "year": "2023",
      "doi": "10.3233/SHTI230065",
      "authors": "Shiferaw Kirubel Biruk et al.",
      "keywords": "Deep learning; FAIR Principles; RDA FAIR Data maturity model; cardiovascular disease; computed tomography",
      "mesh_terms": "Humans; Deep Learning; Cardiovascular Diseases; Tomography, X-Ray Computed; Software; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37203610/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37203728",
      "title": "Fairness in Artificial Intelligence: Regulatory Sanbox Evaluation of Bias Prevention for ECG Classification.",
      "abstract": "As the use of artificial intelligence within healthcare is on the rise, an increased attention has been directed towards ethical considerations. Defining fairness in machine learning is a well explored topic with an extensive literature. However, such definitions often rely on the existence of metrics on the input data and well-defined outcome measurements, while regulatory definitions use general terminology. This work aims to study fairness within AI, particularly bringing regulation and theoretical knowledge closer. The study is done via a regulatory sandbox implemented on a healthcare case, specifically ECG classification.",
      "journal": "Studies in health technology and informatics",
      "year": "2023",
      "doi": "10.3233/SHTI230184",
      "authors": "Ranjbar Arian et al.",
      "keywords": "Artificial Intelligence; Bias; Ethics; Fairness; GDPR; Regulation",
      "mesh_terms": "Artificial Intelligence; Machine Learning; Benchmarking; Bias; Electrocardiography",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37203728/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37224362",
      "title": "Decoupled Unbiased Teacher for Source-Free Domain Adaptive Medical Object Detection.",
      "abstract": "Source-free domain adaptation (SFDA) aims to adapt a lightweight pretrained source model to unlabeled new domains without the original labeled source data. Due to the privacy of patients and storage consumption concerns, SFDA is a more practical setting for building a generalized model in medical object detection. Existing methods usually apply the vanilla pseudo-labeling technique, while neglecting the bias issues in SFDA, leading to limited adaptation performance. To this end, we systematically analyze the biases in SFDA medical object detection by constructing a structural causal model (SCM) and propose an unbiased SFDA framework dubbed decoupled unbiased teacher (DUT). Based on the SCM, we derive that the confounding effect causes biases in the SFDA medical object detection task at the sample level, feature level, and prediction level. To prevent the model from emphasizing easy object patterns in the biased dataset, a dual invariance assessment (DIA) strategy is devised to generate counterfactual synthetics. The synthetics are based on unbiased invariant samples in both discrimination and semantic perspectives. To alleviate overfitting to domain-specific features in SFDA, we design a cross-domain feature intervention (CFI) module to explicitly deconfound the domain-specific prior with feature intervention and obtain unbiased features. Besides, we establish a correspondence supervision prioritization (CSP) strategy for addressing the prediction bias caused by coarse pseudo-labels by sample prioritizing and robust box supervision. Through extensive experiments on multiple SFDA medical object detection scenarios, DUT yields superior performance over previous state-of-the-art unsupervised domain adaptation (UDA) and SFDA counterparts, demonstrating the significance of addressing the bias issues in this challenging task. The code is available at https://github.com/CUHK-AIM-Group/Decoupled-Unbiased-Teacher.",
      "journal": "IEEE transactions on neural networks and learning systems",
      "year": "2024",
      "doi": "10.1109/TNNLS.2023.3272389",
      "authors": "Liu Xinyu et al.",
      "keywords": "",
      "mesh_terms": "Humans; Neural Networks, Computer; Algorithms; Machine Learning; Pattern Recognition, Automated",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37224362/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37312237",
      "title": "Testing for an ignorable sampling bias under random double truncation.",
      "abstract": "In clinical and epidemiological research doubly truncated data often appear. This is the case, for instance, when the data registry is formed by interval sampling. Double truncation generally induces a sampling bias on the target variable, so proper corrections of ordinary estimation and inference procedures must be used. Unfortunately, the nonparametric maximum likelihood estimator of a doubly truncated distribution has several drawbacks, like potential nonexistence and nonuniqueness issues, or large estimation variance. Interestingly, no correction for double truncation is needed when the sampling bias is ignorable, which may occur with interval sampling and other sampling designs. In such a case the ordinary empirical distribution function is a consistent and fully efficient estimator that generally brings remarkable variance improvements compared to the nonparametric maximum likelihood estimator. Thus, identification of such situations is critical for the simple and efficient estimation of the target distribution. In this article, we introduce for the first time formal testing procedures for the null hypothesis of ignorable sampling bias with doubly truncated data. The asymptotic properties of the proposed test statistic are investigated. A bootstrap algorithm to approximate the null distribution of the test in practice is introduced. The finite sample performance of the method is studied in simulated scenarios. Finally, applications to data on onset for childhood cancer and Parkinson's disease are given. Variance improvements in estimation are discussed and illustrated.",
      "journal": "Statistics in medicine",
      "year": "2023",
      "doi": "10.1002/sim.9828",
      "authors": "de U\u00f1a-\u00c1lvarez Jacobo",
      "keywords": "bootstrap; goodness-of-fit; interval sampling; nonparametric statistics; survival analysis",
      "mesh_terms": "Humans; Child; Selection Bias; Likelihood Functions; Computer Simulation; Research Design; Algorithms; Bias",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37312237/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37423238",
      "title": "Making decisions: Bias in artificial intelligence and data\u2011driven diagnostic tools.",
      "abstract": "BACKGROUND: Although numerous studies have shown the potential of artificial intelligence (AI) systems in drastically improving clinical practice, there are concerns that these AI systems could replicate existing biases. OBJECTIVE: This paper provides a brief overview of\u00a0'algorithmic bias', which refers to the\u00a0tendency of some AI systems to\u00a0perform poorly for disadvantaged or\u00a0marginalised groups. DISCUSSION: AI relies on data generated, collected, recorded and labelled by humans. If AI systems remain unchecked, whatever biases that exist in the real world that are embedded in data will be incorporated into the AI algorithms. Algorithmic bias can be considered as an extension, if not a new manifestation, of existing social biases, understood as negative attitudes towards or the discriminatory treatment of some groups. In medicine, algorithmic bias can compromise patient safety and risks perpetuating disparities in care and outcome. Thus, clinicians should consider the risk of bias when deploying AI-enabled tools in their practice.",
      "journal": "Australian journal of general practice",
      "year": "2023",
      "doi": "10.31128/AJGP-12-22-6630",
      "authors": "Saint James Aquino Yves",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Decision Making; Bias; Medicine; Patient Safety",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37423238/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37566406",
      "title": "AI in Medicine-JAMA's Focus on Clinical Outcomes, Patient-Centered Care, Quality, and Equity.",
      "abstract": "",
      "journal": "JAMA",
      "year": "2023",
      "doi": "10.1001/jama.2023.15481",
      "authors": "Khera Rohan et al.",
      "keywords": "",
      "mesh_terms": "Humans; Editorial Policies; Medicine; Patient-Centered Care; Artificial Intelligence; Outcome Assessment, Health Care; Quality of Health Care; Health Equity",
      "pub_types": "Editorial; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37566406/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37579574",
      "title": "Algorithmic bias in artificial intelligence is a problem-And the root issue is power.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) in health care continues to expand at a rapid rate, impacting both nurses and communities we accompany in care. PURPOSE: We argue algorithmic bias is but a symptom of a more systemic and longstanding problem: power imbalances related to the creation, development, and use of health care technologies. METHODS: This commentary responds to Drs. O'Connor and Booth's 2022 article, \"Algorithmic bias in health care: Opportunities for nurses to improve equality in the age of artificial intelligence.\" DISCUSSION: Nurses need not 'reinvent the wheel' when it comes to AI policy, curricula, or ethics. We can and should follow the lead of communities already working 'from the margins' who provide ample guidance. CONCLUSION: Its neither feasible nor just to expect individual nurses to counter systemic injustice in health care through individual actions, more technocentric curricula, or industry partnerships. We need disciplinary supports for collective action to renegotiate power for AI tech.",
      "journal": "Nursing outlook",
      "year": "2023",
      "doi": "10.1016/j.outlook.2023.102023",
      "authors": "Walker Rae et al.",
      "keywords": "Algorithms; Artificial intelligence; Bias; Machine learning; Nursing ethics; Racism",
      "mesh_terms": "Humans; Artificial Intelligence; Delivery of Health Care",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37579574/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37660301",
      "title": "An intersectional framework for counterfactual fairness in risk prediction.",
      "abstract": "Along with the increasing availability of health data has come the rise of data-driven models to inform decision making and policy. These models have the potential to benefit both patients and health care providers but can also exacerbate health inequities. Existing \"algorithmic fairness\" methods for measuring and correcting model bias fall short of what is needed for health policy in two key ways. First, methods typically focus on a single grouping along which discrimination may occur rather than considering multiple, intersecting groups. Second, in clinical applications, risk prediction is typically used to guide treatment, creating distinct statistical issues that invalidate most existing techniques. We present novel unfairness metrics that address both challenges. We also develop a complete framework of estimation and inference tools for our metrics, including the unfairness value (\"u-value\"), used to determine the relative extremity of unfairness, and standard errors and confidence intervals employing an alternative to the standard bootstrap. We demonstrate application of our framework to a COVID-19 risk prediction model deployed in a major Midwestern health system.",
      "journal": "Biostatistics (Oxford, England)",
      "year": "2024",
      "doi": "10.1093/biostatistics/kxad021",
      "authors": "Wastvedt Solvejg et al.",
      "keywords": "Algorithmic fairness; COVID-19; Causal inference; Intersectionality; Risk prediction",
      "mesh_terms": "Humans; COVID-19; Risk Assessment; Models, Statistical; SARS-CoV-2",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37660301/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37703989",
      "title": "Evaluating the fairness and accuracy of machine learning-based predictions of clinical outcomes after anatomic and reverse total shoulder arthroplasty.",
      "abstract": "BACKGROUND: Machine learning (ML)-based clinical decision support tools (CDSTs) make personalized predictions for different treatments; by comparing predictions of multiple treatments, these tools can be used to optimize decision making for a particular patient. However, CDST prediction accuracy varies for different patients and also for different treatment options. If these differences are sufficiently large and consistent for a particular subcohort of patients, then that bias may result in those patients not receiving a particular treatment. Such level of bias would deem the CDST \"unfair.\" The purpose of this study is to evaluate the \"fairness\" of ML CDST-based clinical outcomes predictions after anatomic (aTSA) and reverse total shoulder arthroplasty (rTSA) for patients of different demographic attributes. METHODS: Clinical data from 8280 shoulder arthroplasty patients with 19,249 postoperative visits was used to evaluate the prediction fairness and accuracy associated with the following patient demographic attributes: ethnicity, sex, and age at the time of surgery. Performance of clinical outcome and range of motion regression predictions were quantified by the mean absolute error (MAE) and performance of minimal clinically important difference (MCID) and substantial clinical benefit classification predictions were quantified by accuracy, sensitivity, and the F1 score. Fairness of classification predictions leveraged the \"four-fifths\" legal guideline from the US Equal Employment Opportunity Commission and fairness of regression predictions leveraged established MCID thresholds associated with each outcome measure. RESULTS: For both aTSA and rTSA clinical outcome predictions, only minor differences in MAE were observed between patients of different ethnicity, sex, and age. Evaluation of prediction fairness demonstrated that 0 of 486 MCID (0%) and only 3 of 486 substantial clinical benefit (0.6%) classification predictions were outside the 20% fairness boundary and only 14 of 972 (1.4%) regression predictions were outside of the MCID fairness boundary. Hispanic and Black patients were more likely to have ML predictions out of fairness tolerance for aTSA and rTSA. Additionally, patients <60 years old were more likely to have ML predictions out of fairness tolerance for rTSA. No disparate predictions were identified for sex and no disparate regression predictions were observed for forward elevation, internal rotation score, American Shoulder and Elbow Surgeons Standardized Shoulder Assessment Form score, or global shoulder function. CONCLUSION: The ML algorithms analyzed in this study accurately predict clinical outcomes after aTSA and rTSA for patients of different ethnicity, sex, and age, where only 1.4% of regression predictions and only 0.3% of classification predictions were out of fairness tolerance using the proposed fairness evaluation method and acceptance criteria. Future work is required to externally validate these ML algorithms to ensure they are equally accurate for all legally protected patient groups.",
      "journal": "Journal of shoulder and elbow surgery",
      "year": "2024",
      "doi": "10.1016/j.jse.2023.08.005",
      "authors": "Allen Christine et al.",
      "keywords": "Machine learning; accuracy evaluation; anatomic total shoulder arthroplasty; clinical outcomes; fairness and equity; reverse total shoulder arthroplasty",
      "mesh_terms": "Humans; Middle Aged; Arthroplasty, Replacement, Shoulder; Shoulder Joint; Treatment Outcome; Retrospective Studies; Range of Motion, Articular",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37703989/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37819812",
      "title": "Bipartite Ranking Fairness Through a Model Agnostic Ordering Adjustment.",
      "abstract": "Recently, with the applications of algorithms in various risky scenarios, algorithmic fairness has been a serious concern and received lots of interest in machine learning community. In this article, we focus on the bipartite ranking scenario, where the instances come from either the positive or negative class and the goal is to learn a ranking function that ranks positive instances higher than negative ones. We are interested in whether the learned ranking function can cause systematic disparity across different protected groups defined by sensitive attributes. While there could be a trade-off between fairness and performance, we propose a model agnostic post-processing framework xOrder for achieving fairness in bipartite ranking and maintaining the algorithm classification performance. In particular, we optimize a weighted sum of the utility as identifying an optimal warping path across different protected groups and solve it through a dynamic programming process. xOrder is compatible with various classification models and ranking fairness metrics, including supervised and unsupervised fairness metrics. In addition to binary groups, xOrder can be applied to multiple protected groups. We evaluate our proposed algorithm on four benchmark data sets and two real-world patient electronic health record repositories. xOrder consistently achieves a better balance between the algorithm utility and ranking fairness on a variety of datasets with different metrics. From the visualization of the calibrated ranking scores, xOrder mitigates the score distribution shifts of different groups compared with baselines. Moreover, additional analytical results verify that xOrder achieves a robust performance when faced with fewer samples and a bigger difference between training and testing ranking score distributions.",
      "journal": "IEEE transactions on pattern analysis and machine intelligence",
      "year": "2023",
      "doi": "10.1109/TPAMI.2023.3290949",
      "authors": "Cui Sen et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37819812/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37861713",
      "title": "Maintaining High-Touch in High-Tech Digital Health Monitoring and Multi-Omics Prognostication: Ethical, Equity, and Societal Considerations in Precision Health for Palliative Care.",
      "abstract": "Advances in digital health, systems biology, environmental monitoring, and artificial intelligence (AI) continue to revolutionize health care, ushering a precision health future. More than disease treatment and prevention, precision health aims at maintaining good health throughout the lifespan. However, how can precision health impact care for people with a terminal or life-limiting condition? We examine here the ethical, equity, and societal/relational implications of two precision health modalities, (1) integrated systems biology/multi-omics analysis for disease prognostication and (2) digital health technologies for health status monitoring and communication. We focus on three main ethical and societal considerations: benefits and risks associated with integration of these modalities into the palliative care system; inclusion of underrepresented and marginalized groups in technology development and deployment; and the impact of high-tech modalities on palliative care's highly personalized and \"high-touch\" practice. We conclude with 10 recommendations for ensuring that precision health technologies, such as multi-omics prognostication and digital health monitoring, for palliative care are developed, tested, and implemented ethically, inclusively, and equitably.",
      "journal": "Omics : a journal of integrative biology",
      "year": "2023",
      "doi": "10.1089/omi.2023.0120",
      "authors": "Viana John Noel et al.",
      "keywords": "digital health; equity; ethics; palliative care; precision health; systems biology",
      "mesh_terms": "Humans; Artificial Intelligence; Multiomics; Palliative Care; Precision Medicine",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37861713/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37936347",
      "title": "Algorithmic fairness in precision psychiatry: analysis of prediction models in individuals at clinical high risk for psychosis.",
      "abstract": "BACKGROUND: Computational models offer promising potential for personalised treatment of psychiatric diseases. For their clinical deployment, fairness must be evaluated alongside accuracy. Fairness requires predictive models to not unfairly disadvantage specific demographic groups. Failure to assess model fairness prior to use risks perpetuating healthcare inequalities. Despite its importance, empirical investigation of fairness in predictive models for psychiatry remains scarce. AIMS: To evaluate fairness in prediction models for development of psychosis and functional outcome. METHOD: Using data from the PRONIA study, we examined fairness in 13 published models for prediction of transition to psychosis (n = 11) and functional outcome (n = 2) in people at clinical high risk for psychosis or with recent-onset depression. Using accuracy equality, predictive parity, false-positive error rate balance and false-negative error rate balance, we evaluated relevant fairness aspects for the demographic attributes 'gender' and 'educational attainment' and compared them with the fairness of clinicians' judgements. RESULTS: Our findings indicate systematic bias towards assigning less favourable outcomes to individuals with lower educational attainment in both prediction models and clinicians' judgements, resulting in higher false-positive rates in 7 of 11 models for transition to psychosis. Interestingly, the bias patterns observed in algorithmic predictions were not significantly more pronounced than those in clinicians' predictions. CONCLUSIONS: Educational bias was present in algorithmic and clinicians' predictions, assuming more favourable outcomes for individuals with higher educational level (years of education). This bias might lead to increased stigma and psychosocial burden in patients with lower educational attainment and suboptimal psychosis prevention in those with higher educational attainment.",
      "journal": "The British journal of psychiatry : the journal of mental science",
      "year": "2024",
      "doi": "10.1192/bjp.2023.141",
      "authors": "\u015eahin Derya et al.",
      "keywords": "Ethics; psychotic disorders/schizophrenia; risk assessment; schizophrenia; stigma and discrimination",
      "mesh_terms": "Humans; Psychotic Disorders; Psychiatry",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37936347/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37963314",
      "title": "Delivering Data-Driven Precision and Equitable Artificial Intelligence Decision Tools in Oncology: Exploring the Informarker Concept.",
      "abstract": "",
      "journal": "JCO clinical cancer informatics",
      "year": "2023",
      "doi": "10.1200/CCI.23.00142",
      "authors": "Lin Frank P",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Medical Oncology; Precision Medicine",
      "pub_types": "Editorial; Research Support, Non-U.S. Gov't; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37963314/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38095879",
      "title": "Burned Out on Burnout-The Urgency of Equity-Minded Structural Approaches to Support Nurses.",
      "abstract": "This JAMA Forum discusses a health equity framework to address burnout and professional fulfillment among nurses.",
      "journal": "JAMA health forum",
      "year": "2023",
      "doi": "10.1001/jamahealthforum.2023.5249",
      "authors": "Cunningham Tim et al.",
      "keywords": "",
      "mesh_terms": "Humans; Burnout, Professional; Burnout, Psychological",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38095879/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38123252",
      "title": "Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a model evaluation study.",
      "abstract": "BACKGROUND: Large language models (LLMs) such as GPT-4 hold great promise as transformative tools in health care, ranging from automating administrative tasks to augmenting clinical decision making. However, these models also pose a danger of perpetuating biases and delivering incorrect medical diagnoses, which can have a direct, harmful impact on medical care. We aimed to assess whether GPT-4 encodes racial and gender biases that impact its use in health care. METHODS: Using the Azure OpenAI application interface, this model evaluation study tested whether GPT-4 encodes racial and gender biases and examined the impact of such biases on four potential applications of LLMs in the clinical domain-namely, medical education, diagnostic reasoning, clinical plan generation, and subjective patient assessment. We conducted experiments with prompts designed to resemble typical use of GPT-4 within clinical and medical education applications. We used clinical vignettes from NEJM Healer and from published research on implicit bias in health care. GPT-4 estimates of the demographic distribution of medical conditions were compared with true US prevalence estimates. Differential diagnosis and treatment planning were evaluated across demographic groups using standard statistical tests for significance between groups. FINDINGS: We found that GPT-4 did not appropriately model the demographic diversity of medical conditions, consistently producing clinical vignettes that stereotype demographic presentations. The differential diagnoses created by GPT-4 for standardised clinical vignettes were more likely to include diagnoses that stereotype certain races, ethnicities, and genders. Assessment and plans created by the model showed significant association between demographic attributes and recommendations for more expensive procedures as well as differences in patient perception. INTERPRETATION: Our findings highlight the urgent need for comprehensive and transparent bias assessments of LLM tools such as GPT-4 for intended use cases before they are integrated into clinical care. We discuss the potential sources of these biases and potential mitigation strategies before clinical implementation. FUNDING: Priscilla Chan and Mark Zuckerberg.",
      "journal": "The Lancet. Digital health",
      "year": "2024",
      "doi": "10.1016/S2589-7500(23)00225-X",
      "authors": "Zack Travis et al.",
      "keywords": "",
      "mesh_terms": "Female; Humans; Male; Health Facilities; Clinical Decision-Making; Diagnosis, Differential; Education, Medical; Delivery of Health Care",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38123252/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38127076",
      "title": "Joint radiomics and spatial distribution model for MRI-based discrimination of multiple sclerosis, neuromyelitis optica spectrum disorder, and myelin-oligodendrocyte-glycoprotein-IgG-associated disorder.",
      "abstract": "OBJECTIVE: To develop a discrimination pipeline concerning both radiomics and spatial distribution features of brain lesions for discrimination of multiple sclerosis (MS), aquaporin-4-IgG-seropositive neuromyelitis optica spectrum disorder (NMOSD), and myelin-oligodendrocyte-glycoprotein-IgG-associated disorder (MOGAD). METHODS: Hyperintensity T2 lesions were delineated in 212 brain MRI scans of MS (n\u2009=\u200963), NMOSD (n\u2009=\u200987), and MOGAD (n\u2009=\u200945) patients. To avoid the effect of fixed training/test dataset sampling when developing machine learning models, patients were allocated into 4 sub-groups for cross-validation. For each scan, 351 radiomics and 27 spatial distribution features were extracted. Three models, i.e., multi-lesion radiomics, spatial distribution, and joint models, were constructed using random forest and logistic regression algorithms for differentiating: MS from the others (MS models) and MOGAD from NMOSD (MOG-NMO models), respectively. Then, the joint models were combined with demographic characteristics (i.e., age and sex) to create MS and MOG-NMO discriminators, respectively, based on which a three-disease discrimination pipeline was generated and compared with radiologists. RESULTS: For classification of both MS-others and MOG-NMO, the joint models performed better than radiomics or spatial distribution model solely. The MS discriminator achieved AUC\u2009=\u20090.909\u2009\u00b1\u20090.027 and bias-corrected C-index\u2009=\u20090.909\u2009\u00b1\u20090.027, and the MOG-NMO discriminator achieved AUC\u2009=\u20090.880\u2009\u00b1\u20090.064 and bias-corrected C-index\u2009=\u20090.883\u2009\u00b1\u20090.068. The three-disease discrimination pipeline differentiated MS, NMOSD, and MOGAD patients with 75.0% accuracy, prominently outperforming the three radiologists (47.6%, 56.6%, and 66.0%). CONCLUSIONS: The proposed pipeline integrating multi-lesion radiomics and spatial distribution features could effectively differentiate MS, NMOSD, and MOGAD. CLINICAL RELEVANCE STATEMENT: The discrimination pipeline merging both radiomics and spatial distribution features of brain lesions may facilitate the differential diagnoses of multiple sclerosis, neuromyelitis optica spectrum disorder, and myelin-oligodendrocyte-glycoprotein-IgG-associated disorder. KEY POINTS: \u2022 Our study introduces an approach by combining radiomics and spatial distribution models. \u2022 The joint model exhibited superior performance in distinguishing multiple sclerosis from aquaporin-4-IgG-seropositive neuromyelitis optica spectrum disorder and myelin-oligodendrocyte-glycoprotein-IgG-associated disorder as well as discriminating the latter two diseases. \u2022 The three-disease discrimination pipeline showcased remarkable accuracy, surpassing the performance of experienced radiologists, highlighting its potential as a valuable diagnostic tool.",
      "journal": "European radiology",
      "year": "2024",
      "doi": "10.1007/s00330-023-10529-y",
      "authors": "Luo Xiao et al.",
      "keywords": "Demyelinating autoimmune diseases, CNS; Image processing, computer-assisted; Machine learning; Multiple sclerosis; Neuromyelitis optica",
      "mesh_terms": "Humans; Neuromyelitis Optica; Multiple Sclerosis; Magnetic Resonance Imaging; Female; Male; Adult; Myelin-Oligodendrocyte Glycoprotein; Middle Aged; Immunoglobulin G; Diagnosis, Differential; Brain; Aquaporin 4; Radiomics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38127076/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38236216",
      "title": "Attitudes among the Australian public toward AI and CCTV in suicide prevention research: A mixed methods study.",
      "abstract": "Research is underway exploring the use of closed-circuit television (CCTV) cameras and artificial intelligence (AI) for suicide prevention research in public locations where suicides occur. Given the sensitive nature and potential implications of this research, this study explored ethical concerns the public may have about research of this nature. Developed based on the principle of respect, a survey was administered to a representative sample of 1,096 Australians to understand perspectives on the research. The sample was aged 18 and older, 53% female, and 9% ethnic minority. Following an explanatory mixed methods approach, interviews and a focus group were conducted with people with a lived experience of suicide and first responders to contextualize the findings. There were broad levels of acceptance among the Australian public. Younger respondents, females, and those declining to state their ethnicity had lower levels of acceptance of CCTV research using AI for suicide prevention. Those with lived experience of suicide had higher acceptance. Qualitative data indicated concern regarding racial bias in AI and police response to suicidal crises and the need for lived experience involvement in the development and implementation of any resulting interventions. Broad public acceptance of the research aligns with the principle of respect for persons. Beneficence emerged in the context of findings emphasizing the importance of meaningfully including people with lived experience in the development and implementation of interventions resulting from this research, while justice emerged in themes expressing concerns about racial bias in AI and police response to mental health crises. (PsycInfo Database Record (c) 2024 APA, all rights reserved).",
      "journal": "The American psychologist",
      "year": "2024",
      "doi": "10.1037/amp0001215",
      "authors": "Hardy Rebecca C et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Male; Suicide Prevention; Suicide; Artificial Intelligence; Ethnicity; Australia; Minority Groups; Australasian People",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38236216/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38244317",
      "title": "Evolving paradigms in breast cancer screening: Balancing efficacy, personalization, and equity.",
      "abstract": "Breast cancer remains a significant global health challenge, with projections indicating a troubling increase in incidence. Breast cancer screening programs have long been hailed as life-saving initiatives, yet their true impact on mortality rates is a subject of ongoing debate. Screening poses the risk of false positives and the detection of indolent tumors, potentially leading to overtreatment. Bias factors, including lead time, length time, and selection biases, further complicate the assessment of screening efficacy. Recent studies suggest that AI-driven image analysis may revolutionize breast cancer screening, maintaining diagnostic accuracy while reducing radiologists' workload. However, the generalizability of these findings to diverse populations is a critical consideration. Personalized screening approaches and equitable access to advanced technologies are essential to mitigate disparities. In conclusion, the breast cancer screening landscape is evolving, emphasizing the need for risk stratification, appropriate imaging modalities, and a personalized approach to reduce overdiagnosis and focus on cancers with the potential to impact lives while prioritizing patient-centered care.",
      "journal": "European journal of radiology",
      "year": "2024",
      "doi": "10.1016/j.ejrad.2024.111321",
      "authors": "Pesapane Filippo et al.",
      "keywords": "Artificial intelligence; Breast neoplasms; Diagnostic imaging; Early detection of cancer; Health disparities",
      "mesh_terms": "Humans; Female; Breast Neoplasms; Early Detection of Cancer; Radiologists; Incidence; Mammography; Mass Screening",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38244317/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38244325",
      "title": "Association between increased Subcutaneous Adipose Tissue Radiodensity and cancer mortality: Automated computation, comparison of cancer types, gender, and scanner bias.",
      "abstract": "PURPOSE: Body composition analysis using computed tomography (CT) is proposed as a predictor of cancer mortality. An association between subcutaneous adipose tissue radiodensity (SATr) and cancer-specific mortality was established, while gender effects and equipment bias were estimated. METHODS: 7,475 CT studies were selected from 17 cohorts containing CT images of untreated cancer patients who underwent follow-up for a period of 2.1-118.8 months. SATr measures were collected from published data (n\u00a0=\u00a06,718) or calculated according to CT images using a deep-learning network (n\u00a0=\u00a0757). The association between SATr and mortality was ascertained for each cohort and gender using the p-value from either logistic regression or ROC analysis. The Kruskal-Wallis test was used to analyze differences between gender distributions, and automatic segmentation was evaluated using the Dice score and five-point Likert quality scale. Gender effect, scanner bias and changes in the Hounsfield unit (HU) to detect hazards were also estimated. RESULTS: Higher SATr was associated with mortality in eight cancer types (p\u00a0<\u00a00.05). Automatic segmentation produced a score of 0.949 while the quality scale measurement was good to excellent. The extent of gender effect was 5.2 HU while the scanner bias was 10.3 HU. The minimum proposed HU change to detect a patient at risk of death was between 5.6 and 8.3 HU. CONCLUSIONS: CT imaging provides valuable assessments of body composition as part of the staging process for several cancer types, saving both time and cost. Gender specific scales and scanner bias adjustments should be carried out to successfully implement SATr measures in clinical practice.",
      "journal": "Applied radiation and isotopes : including data, instrumentation and methods for use in agriculture, industry and medicine",
      "year": "2024",
      "doi": "10.1016/j.apradiso.2024.111181",
      "authors": "Machado Marcos A D et al.",
      "keywords": "Body composition; Computed tomography; Deep-learning; Oncology; Standardization",
      "mesh_terms": "Humans; Neoplasms; Tomography, X-Ray Computed; Subcutaneous Fat; Adipose Tissue",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38244325/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38246015",
      "title": "Unveiling biases of artificial intelligence in healthcare: Navigating the promise and pitfalls.",
      "abstract": "",
      "journal": "Injury",
      "year": "2024",
      "doi": "10.1016/j.injury.2024.111358",
      "authors": "Rashid Dawood et al.",
      "keywords": "Artificial intelligence; Bias mitigation; Diverse demographic data; Healthcare disparities; Machine learning",
      "mesh_terms": "Humans; Artificial Intelligence; Bias; Delivery of Health Care",
      "pub_types": "Letter; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38246015/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38264771",
      "title": "Label-free cell classification in holographic flow cytometry through an unbiased learning strategy.",
      "abstract": "Nowadays, label-free imaging flow cytometry at the single-cell level is considered the stepforward lab-on-a-chip technology to address challenges in clinical diagnostics, biology, life sciences and healthcare. In this framework, digital holography in microscopy promises to be a powerful imaging modality thanks to its multi-refocusing and label-free quantitative phase imaging capabilities, along with the encoding of the highest information content within the imaged samples. Moreover, the recent achievements of new data analysis tools for cell classification based on deep/machine learning, combined with holographic imaging, are urging these systems toward the effective implementation of point of care devices. However, the generalization capabilities of learning-based models may be limited from biases caused by data obtained from other holographic imaging settings and/or different processing approaches. In this paper, we propose a combination of a Mask R-CNN to detect the cells, a convolutional auto-encoder, used to the image feature extraction and operating on unlabelled data, thus overcoming the bias due to data coming from different experimental settings, and a feedforward neural network for single cell classification, that operates on the above extracted features. We demonstrate the proposed approach in the challenging classification task related to the identification of drug-resistant endometrial cancer cells.",
      "journal": "Lab on a chip",
      "year": "2024",
      "doi": "10.1039/d3lc00385j",
      "authors": "Ciaparrone Gioele et al.",
      "keywords": "",
      "mesh_terms": "Flow Cytometry; Algorithms; Image Processing, Computer-Assisted; Microscopy; Holography",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38264771/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38269933",
      "title": "Equitable Machine Learning for Hypoglycaemia Risk Management.",
      "abstract": "We developed a machine learning (ML) model for the detection of patients with high risk of hypoglycaemic events during their hospital stay to improve the detection and management of hypoglycaemia. Our model was trained on data from a regional local health care district in Australia. The model was found to have good predictive performance in the general case (AUC 0.837). We conducted subgroup analysis to ensure that the model performed in a way that did not disadvantage population subgroups, in this case based on gender or indigenous status. We found that our specific problem domain assisted us in reducing unwanted bias within the model, because it did not rely on practice patterns or subjective judgements for the outcome measure. With careful analysis for equity there is great potential for ML models to automate the detection of high-risk cohorts and automate mitigation strategies to reduce preventable errors.",
      "journal": "Studies in health technology and informatics",
      "year": "2024",
      "doi": "10.3233/SHTI231089",
      "authors": "Rodriguez Jhordany et al.",
      "keywords": "AI; Machine learning; diabetes; emr; equity; fairness; hypoglycaemia",
      "mesh_terms": "Humans; Hypoglycemia; Hypoglycemic Agents; Australia; Machine Learning; Risk Management",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38269933/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38290289",
      "title": "Understanding skin color bias in deep learning-based skin lesion segmentation.",
      "abstract": "BACKGROUND: The field of dermatological image analysis using deep neural networks includes the semantic segmentation of skin lesions, pivotal for lesion analysis, pathology inference, and diagnoses. While biases in neural network-based dermatoscopic image classification against darker skin tones due to dataset imbalance and contrast disparities are acknowledged, a comprehensive exploration of skin color bias in lesion segmentation models is lacking. It is imperative to address and understand the biases in these models. METHODS: Our study comprehensively evaluates skin tone bias within prevalent neural networks for skin lesion segmentation. Since no information about skin color exists in widely used datasets, to quantify the bias we use three distinct skin color estimation methods: Fitzpatrick skin type estimation, Individual Typology Angle estimation as well as manual grouping of images by skin color. We assess bias across common models by training a variety of U-Net-based models on three widely-used datasets with 1758 different dermoscopic and clinical images. We also evaluate commonly suggested methods to mitigate bias. RESULTS: Our findings expose a significant and large correlation between segmentation performance and skin color, revealing consistent challenges in segmenting lesions for darker skin tones across diverse datasets. Using various methods of skin color quantification, we have found significant bias in skin lesion segmentation against darker-skinned individuals when evaluated both in and out-of-sample. We also find that commonly used methods for bias mitigation do not result in any significant reduction in bias. CONCLUSIONS: Our findings suggest a pervasive bias in most published lesion segmentation methods, given our use of commonly employed neural network architectures and publicly available datasets. In light of our findings, we propose recommendations for unbiased dataset collection, labeling, and model development. This presents the first comprehensive evaluation of fairness in skin lesion segmentation.",
      "journal": "Computer methods and programs in biomedicine",
      "year": "2024",
      "doi": "10.1016/j.cmpb.2024.108044",
      "authors": "Ben\u010devi\u0107 Marin et al.",
      "keywords": "AI fairness; Deep neural networks; Dermatological image analysis; Skin lesion segmentation",
      "mesh_terms": "Humans; Skin Pigmentation; Deep Learning; Dermoscopy; Skin Diseases; Skin; Image Processing, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38290289/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38387381",
      "title": "Bias reduction using combined stain normalization and augmentation for AI-based classification of histological images.",
      "abstract": "Artificial intelligence (AI)-assisted diagnosis is an ongoing revolution in pathology. However, a frequent drawback of AI models is their propension to make decisions based rather on bias in training dataset than on concrete biological features, thus weakening pathologists' trust in these tools. Technically, it is well known that microscopic images are altered by tissue processing and staining procedures, being one of the main sources of bias in machine learning for digital pathology. So as to deal with it, many teams have written about color normalization and augmentation methods. However, only a few of them have monitored their effects on bias reduction and model generalizability. In our study, two methods for stain augmentation (AugmentHE) and fast normalization (HEnorm) have been created and their effect on bias reduction has been monitored. Actually, they have also been compared to previously described strategies. To that end, a multicenter dataset created for breast cancer histological grading has been used. Thanks to it, classification models have been trained in a single center before assessing its performance in other centers images. This setting led to extensively monitor bias reduction while providing accurate insight of both augmentation and normalization methods. AugmentHE provided an 81% increase in color dispersion compared to geometric augmentations only. In addition, every classification model that involved AugmentHE presented a significant increase in the area under receiving operator characteristic curve (AUC) over the widely used RGB shift. More precisely, AugmentHE-based models showed at least 0.14 AUC increase over RGB shift-based models. Regarding normalization, HEnorm appeared to be up to 78x faster than conventional methods. It also provided satisfying results in terms of bias reduction. Altogether, our pipeline composed of AugmentHE and HEnorm improved AUC on biased data by up to 21.7% compared to usual augmentations. Conventional normalization methods coupled with AugmentHE yielded similar results while being much slower. In conclusion, we have validated an open-source tool that can be used in any deep learning-based digital pathology project on H&E whole slide images (WSI) that efficiently reduces stain-induced bias and later on might help increase pathologists' confidence when using AI-based products.",
      "journal": "Computers in biology and medicine",
      "year": "2024",
      "doi": "10.1016/j.compbiomed.2024.108130",
      "authors": "Franchet Camille et al.",
      "keywords": "Bias mitigation; Color-induced bias; Data augmentation; Deep learning; Histopathology; Normalization",
      "mesh_terms": "Female; Humans; Artificial Intelligence; Breast Neoplasms; Coloring Agents; Machine Learning; Staining and Labeling; Multicenter Studies as Topic",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38387381/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38413380",
      "title": "Vital sign measurements demonstrate terminal digit bias and boundary effects.",
      "abstract": "OBJECTIVE: The measurement and recording of vital signs may be impacted by biases, including preferences for even and round numbers. However, other biases, such as variation due to defined numerical boundaries (also known as boundary effects), may be present in vital signs data and have not yet been investigated in a medical setting. We aimed to assess vital signs data for such biases. These parameters are clinically significant as they influence care escalation. METHODS: Vital signs data (heart rate, respiratory rate, oxygen saturation and systolic blood pressure) were collected from a tertiary hospital electronic medical record over a 2-year period. These data were analysed using polynomial regression with additional terms to assess for underreporting of out-of-range observations and overreporting numbers with terminal digits of 0 (round numbers), 2 (even numbers) and 5. RESULTS: It was found that heart rate, oxygen saturation and systolic blood pressure demonstrated 'boundary effects', with values inside the 'normal' range disproportionately more likely to be recorded. Even number bias was observed in systolic heart rate, respiratory rate and blood pressure. Preference for multiples of 5 was observed for heart rate and blood pressure. Independent overrepresentation of multiples of 10 was demonstrated in heart rate data. CONCLUSION: Although often considered objective, vital signs data are affected by bias. These biases may impact the care patients receive. Additionally, it may have implications for creating and training machine learning models that utilise vital signs data.",
      "journal": "Emergency medicine Australasia : EMA",
      "year": "2024",
      "doi": "10.1111/1742-6723.14395",
      "authors": "Kleinig Oliver et al.",
      "keywords": "bias; boundary effect; bunching; even number; round number; vital sign",
      "mesh_terms": "Humans; Vital Signs; Bias; Female; Male; Electronic Health Records; Middle Aged; Respiratory Rate; Aged; Heart Rate",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38413380/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38423556",
      "title": "Bridging the equity gap towards inclusive artificial intelligence in healthcare diagnostics.",
      "abstract": "",
      "journal": "BMJ (Clinical research ed.)",
      "year": "2024",
      "doi": "10.1136/bmj.q490",
      "authors": "Chan See Chai Carol et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Healthcare Disparities; Diagnosis",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38423556/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38432311",
      "title": "Assessment of bias in scoring of AI-based radiotherapy segmentation and planning studies using modified TRIPOD and PROBAST guidelines as an example.",
      "abstract": "BACKGROUND AND PURPOSE: Studies investigating the application of Artificial Intelligence (AI) in the field of radiotherapy exhibit substantial variations in terms of quality. The goal of this study was to assess the amount of transparency and bias in scoring articles with a specific focus on AI based segmentation and treatment planning, using modified PROBAST and TRIPOD checklists, in order to provide recommendations for future guideline developers and reviewers. MATERIALS AND METHODS: The TRIPOD and PROBAST checklist items were discussed and modified using a Delphi process. After consensus was reached, 2 groups of 3 co-authors scored 2 articles to evaluate usability and further optimize the adapted checklists. Finally, 10 articles were scored by all co-authors. Fleiss' kappa was calculated to assess the reliability of agreement between observers. RESULTS: Three of the 37 TRIPOD items and 5 of the 32 PROBAST items were deemed irrelevant. General terminology in the items (e.g., multivariable prediction model, predictors) was modified to align with AI-specific terms. After the first scoring round, further improvements of the items were formulated, e.g., by preventing the use of sub-questions or subjective words and adding clarifications on how to score an item. Using the final consensus list to score the 10 articles, only 2 out of the 61 items resulted in a statistically significant kappa of 0.4 or more demonstrating substantial agreement. For 41 items no statistically significant kappa was obtained indicating that the level of agreement among multiple observers is due to chance alone. CONCLUSION: Our study showed low reliability scores with the adapted TRIPOD and PROBAST checklists. Although such checklists have shown great value during development and reporting, this raises concerns about the applicability of such checklists to objectively score scientific articles for AI applications. When developing or revising guidelines, it is essential to consider their applicability to score articles without introducing bias.",
      "journal": "Radiotherapy and oncology : journal of the European Society for Therapeutic Radiology and Oncology",
      "year": "2024",
      "doi": "10.1016/j.radonc.2024.110196",
      "authors": "Hurkmans Coen et al.",
      "keywords": "Artificial intelligence; Bias; Checklists; Distinctiveness; Guidelines; Inter-observer variation; Oncology; Radiation therapy; Transparency",
      "mesh_terms": "Humans; Artificial Intelligence; Radiotherapy Planning, Computer-Assisted; Checklist; Delphi Technique; Practice Guidelines as Topic; Bias; Reproducibility of Results; Neoplasms",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38432311/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "3853482",
      "title": "A computer algorithm for the assessment of age reporting bias in censal population estimates using Myers' 'blended' method.",
      "abstract": "A population's age structure is widely used in the computation of many vital statistics. The importance of highly accurate vital statistics cannot be overemphasized--such statistics are used extensively by governments to determine the proper allocation of health resources and services, and by demographers, sociologists and epidemiologists to study secular trends. A computer program has been developed for use on an Apple II+ microcomputer for the analysis of population age profiles and determination of age reporting bias.",
      "journal": "Computer methods and programs in biomedicine",
      "year": "1985",
      "doi": "10.1016/0169-2607(85)90069-0",
      "authors": "Ayiomamitis A",
      "keywords": "",
      "mesh_terms": "Adolescent; Adult; Age Factors; Aged; Child; Computers; Demography; Female; Humans; Life Expectancy; Male; Microcomputers; Middle Aged; Software; Vital Statistics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/3853482/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38547488",
      "title": "Equitable Artificial Intelligence in Obstetrics, Maternal-Fetal Medicine, and Neonatology.",
      "abstract": "Artificial intelligence (AI) offers potential benefits in the interconnected fields of obstetrics, maternal-fetal medicine, and neonatology to bridge disciplinary silos for a unified approach. Artificial intelligence has the capacity to improve diagnostic accuracy and clinical decision making for the birthing parent-neonate dyad. There is an inherent risk of ingrained biases in AI that perpetuate existing inequalities; thus, care must be taken to include diverse data sets with interdisciplinary collaboration that centers equitable AI implementation. As AI plays an increasingly important role in perinatal care, we advocate for its cautious, equity-focused application to benefit the perinatal dyad while avoiding the intensification of health care disparities and disciplinary silos.",
      "journal": "Obstetrics and gynecology",
      "year": "2024",
      "doi": "10.1097/AOG.0000000000005563",
      "authors": "McAdams Ryan M et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38547488/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38548006",
      "title": "Participant flow diagrams for health equity in AI.",
      "abstract": "Selection bias can arise through many aspects of a study, including recruitment, inclusion/exclusion criteria, input-level exclusion and outcome-level exclusion, and often reflects the underrepresentation of populations historically disadvantaged in medical research. The effects of selection bias can be further amplified when non-representative samples are used in artificial intelligence (AI) and machine learning (ML) applications to construct clinical algorithms. Building on the \"Data Cards\" initiative for transparency in AI research, we advocate for the addition of a participant flow diagram for AI studies detailing relevant sociodemographic and/or clinical characteristics of excluded participants across study phases, with the goal of identifying potential algorithmic biases before their clinical implementation. We include both a model for this flow diagram as well as a brief case study explaining how it could be implemented in practice. Through standardized reporting of participant flow diagrams, we aim to better identify potential inequities embedded in AI applications, facilitating more reliable and equitable clinical algorithms.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104631",
      "authors": "Ellen Jacob G et al.",
      "keywords": "Data cards; Flow diagram; Health equity; Machine learning; Selection bias",
      "mesh_terms": "Humans; Artificial Intelligence; Health Equity; Algorithms; Machine Learning; Biomedical Research",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38548006/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38568767",
      "title": "De-Biased Disentanglement Learning for Pulmonary Embolism Survival Prediction on Multimodal Data.",
      "abstract": "Health disparities among marginalized populations with lower socioeconomic status significantly impact the fairness and effectiveness of healthcare delivery. The increasing integration of artificial intelligence (AI) into healthcare presents an opportunity to address these inequalities, provided that AI models are free from bias. This paper aims to address the bias challenges by population disparities within healthcare systems, existing in the presentation of and development of algorithms, leading to inequitable medical implementation for conditions such as pulmonary embolism (PE) prognosis. In this study, we explore the diverse bias in healthcare systems, which highlights the demand for a holistic framework to reducing bias by complementary aggregation. By leveraging de-biasing deep survival prediction models, we propose a framework that disentangles identifiable information from images, text reports, and clinical variables to mitigate potential biases within multimodal datasets. Our study offers several advantages over traditional clinical-based survival prediction methods, including richer survival-related characteristics and bias-complementary predicted results. By improving the robustness of survival analysis through this framework, we aim to benefit patients, clinicians, and researchers by enhancing fairness and accuracy in healthcare AI systems.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2024",
      "doi": "10.1109/JBHI.2024.3384848",
      "authors": "Zhong Zhusi et al.",
      "keywords": "",
      "mesh_terms": "Humans; Pulmonary Embolism; Algorithms; Survival Analysis; Female; Male; Middle Aged; Aged; Prognosis; Databases, Factual",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38568767/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38631532",
      "title": "Reduction of ADC bias in diffusion MRI with deep learning-based acceleration: A phantom validation study at 3.0\u00a0T.",
      "abstract": "PURPOSE: Further acceleration of DWI in diagnostic radiology is desired but challenging mainly due to low SNR in high b-value images and associated bias in quantitative ADC values. Deep learning-based reconstruction and denoising may provide a solution to address this challenge. METHODS: The effects of SNR reduction on ADC bias and variability were investigated using a commercial diffusion phantom and numerical simulations. In the phantom, performance of different reconstruction methods, including conventional parallel (SENSE) imaging, compressed sensing (C-SENSE), and compressed SENSE acceleration with an artificial intelligence deep learning-based technique (C-SENSE AI), was compared at different acceleration factors and flip angles using ROI-based analysis. ADC bias was assessed by Lin's Concordance correlation coefficient (CCC) followed by bootstrapping to calculate confidence intervals (CI). ADC random measurement error (RME) was assessed by the mean coefficient of variation (CV\u00af) and non-parametric statistical tests. RESULTS: The simulations predicted increasingly negative bias and loss of precision towards lower SNR. These effects were confirmed in phantom measurements of increasing acceleration, for which CCC decreased from 0.947 to 0.279 and CV\u00af increased from 0.043 to 0.439, and of decreasing flip angle, for which CCC decreased from 0.990 to 0.063 and CV\u00af increased from 0.037 to 0.508. At high acceleration and low flip angle, C-SENSE AI reconstruction yielded best denoised ADC maps. For the lowest investigated flip angle, CCC\u00a0=\u00a0{0.630, 0.771 and 0.987} and CV\u00af={0.508, 0.426 and 0.254} were obtained for {SENSE, C-SENSE, C-SENSE AI}, the improvement by C-SENSE AI being significant as compared to the other methods (CV: p\u00a0=\u00a00.033 for C-SENSE AI vs. C-SENSE and p\u00a0<\u00a00.001 for C-SENSE AI vs. SENSE; CCC: non-overlapping CI between reconstruction methods). For the highest investigated acceleration factor, CCC\u00a0=\u00a0{0.479,0.926,0.960} and CV\u00af={0.519,0.119,0.118} were found, confirming the reduction of bias and RME by C-SENSE AI as compared to C-SENSE (by trend) and to SENSE (CV: p\u00a0<\u00a00.001; CCC: non-overlapping CI). CONCLUSION: ADC bias and random measurement error in DWI at low SNR, typically associated with scan acceleration, can be effectively reduced by deep-learning based C-SENSE AI reconstruction.",
      "journal": "Magnetic resonance imaging",
      "year": "2024",
      "doi": "10.1016/j.mri.2024.04.018",
      "authors": "Lemainque Teresa et al.",
      "keywords": "Apparent diffusion coefficient; Compressed sensing; Deep learning; Diffusion MRI; Magnetic resonance imaging",
      "mesh_terms": "Phantoms, Imaging; Deep Learning; Diffusion Magnetic Resonance Imaging; Image Processing, Computer-Assisted; Signal-To-Noise Ratio; Humans; Reproducibility of Results; Algorithms; Computer Simulation",
      "pub_types": "Journal Article; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38631532/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38635456",
      "title": "Understanding and Mitigating Bias in Imaging Artificial Intelligence.",
      "abstract": "Artificial intelligence (AI) algorithms are prone to bias at multiple stages of model development, with potential for exacerbating health disparities. However, bias in imaging AI is a complex topic that encompasses multiple coexisting definitions. Bias may refer to unequal preference to a person or group owing to preexisting attitudes or beliefs, either intentional or unintentional. However, cognitive bias refers to systematic deviation from objective judgment due to reliance on heuristics, and statistical bias refers to differences between true and expected values, commonly manifesting as systematic error in model prediction (ie, a model with output unrepresentative of real-world conditions). Clinical decisions informed by biased models may lead to patient harm due to action on inaccurate AI results or exacerbate health inequities due to differing performance among patient populations. However, while inequitable bias can harm patients in this context, a mindful approach leveraging equitable bias can address underrepresentation of minority groups or rare diseases. Radiologists should also be aware of bias after AI deployment such as automation bias, or a tendency to agree with automated decisions despite contrary evidence. Understanding common sources of imaging AI bias and the consequences of using biased models can guide preventive measures to mitigate its impact. Accordingly, the authors focus on sources of bias at stages along the imaging machine learning life cycle, attempting to simplify potentially intimidating technical terminology for general radiologists using AI tools in practice or collaborating with data scientists and engineers for AI tool development. The authors review definitions of bias in AI, describe common sources of bias, and present recommendations to guide quality control measures to mitigate the impact of bias in imaging AI. Understanding the terms featured in this article will enable a proactive approach to identifying and mitigating bias in imaging AI. Published under a CC BY 4.0 license. Test Your Knowledge questions for this article are available in the supplemental material. See the invited commentary by Rouzrokh and Erickson in this issue.",
      "journal": "Radiographics : a review publication of the Radiological Society of North America, Inc",
      "year": "2024",
      "doi": "10.1148/rg.230067",
      "authors": "Tejani Ali S et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Algorithms; Automation; Machine Learning; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38635456/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38641744",
      "title": "Demographic bias in misdiagnosis by computational pathology models.",
      "abstract": "Despite increasing numbers of regulatory approvals, deep learning-based computational pathology systems often overlook the impact of demographic factors on performance, potentially leading to biases. This concern is all the more important as computational pathology has leveraged large public datasets that underrepresent certain demographic groups. Using publicly available data from The Cancer Genome Atlas and the EBRAINS brain tumor atlas, as well as internal patient data, we show that whole-slide image classification models display marked performance disparities across different demographic groups when used to subtype breast and lung carcinomas and to predict IDH1 mutations in gliomas. For example, when using common modeling approaches, we observed performance gaps (in area under the receiver operating characteristic curve) between white and Black patients of 3.0% for breast cancer subtyping, 10.9% for lung cancer subtyping and 16.0% for IDH1 mutation prediction in gliomas. We found that richer feature representations obtained from self-supervised vision foundation models reduce performance variations between groups. These representations provide improvements upon weaker models even when those weaker models are combined with state-of-the-art bias mitigation strategies and modeling choices. Nevertheless, self-supervised vision foundation models do not fully eliminate these discrepancies, highlighting the continuing need for bias mitigation efforts in computational pathology. Finally, we demonstrate that our results extend to other demographic factors beyond patient race. Given these findings, we encourage regulatory and policy agencies to integrate demographic-stratified evaluation into their assessment guidelines.",
      "journal": "Nature medicine",
      "year": "2024",
      "doi": "10.1038/s41591-024-02885-z",
      "authors": "Vaidya Anurag et al.",
      "keywords": "",
      "mesh_terms": "Humans; Bias; Black or African American; Black People; Demography; Diagnostic Errors; Glioma; Lung Neoplasms; White; Isocitrate Dehydrogenase",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38641744/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38689433",
      "title": "Advancing Health Equity Through Artificial Intelligence: An Educational Framework for Preparing Nurses in Clinical Practice and Research.",
      "abstract": "The integration of artificial intelligence (AI) into health care offers the potential to enhance patient care, improve diagnostic precision, and broaden access to health-care services. Nurses, positioned at the forefront of patient care, play a pivotal role in utilizing AI to foster a more efficient and equitable health-care system. However, to fulfil this role, nurses will require education that prepares them with the necessary skills and knowledge for the effective and ethical application of AI. This article proposes a framework for nurses which includes AI principles, skills, competencies, and curriculum development focused on the practical use of AI, with an emphasis on care that aims to achieve health equity. By adopting this educational framework, nurses will be prepared to make substantial contributions to reducing health disparities and fostering a health-care system that is more efficient and equitable.",
      "journal": "Creative nursing",
      "year": "2024",
      "doi": "10.1177/10784535241249193",
      "authors": "Cary Michael P et al.",
      "keywords": "artificial intelligence; curriculum development; ethics; health equity; nursing education",
      "mesh_terms": "Humans; Artificial Intelligence; Health Equity; Curriculum; Education, Nursing; Adult; Clinical Competence; Middle Aged; Female; Male",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38689433/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38691911",
      "title": "Use of natural language processing to uncover racial bias in obstetrical documentation.",
      "abstract": "Natural Language Processing (NLP), a form of Artificial Intelligence, allows free-text based clinical documentation to be integrated in ways that facilitate data analysis, data interpretation and formation of individualized medical and obstetrical care. In this cross-sectional study, we identified all births during the study period carrying the radiology-confirmed diagnosis of fibroid uterus in pregnancy (defined as size of largest diameter of >5\u00a0cm) by using an NLP platform and compared it to non-NLP derived data using ICD10 codes of the same diagnosis. We then compared the two sets of data and stratified documentation gaps by race. Using fibroid uterus in pregnancy as a marker, we found that Black patients were more likely to have the diagnosis entered late into the patient's chart or had missing documentation of the diagnosis. With appropriate algorithm definitions, cross referencing and thorough validation steps, NLP can contribute to identifying areas of documentation gaps and improve quality of care.",
      "journal": "Clinical imaging",
      "year": "2024",
      "doi": "10.1016/j.clinimag.2024.110164",
      "authors": "Futterman Itamar D et al.",
      "keywords": "Artificial intelligence; Fibroids; Natural language processing; Racial bias",
      "mesh_terms": "Humans; Natural Language Processing; Female; Pregnancy; Cross-Sectional Studies; Documentation; Uterine Neoplasms; Racism; Leiomyoma; Adult; Obstetrics; Pregnancy Complications, Neoplastic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38691911/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38702378",
      "title": "Missingness and algorithmic bias: an example from the United States National Outbreak Reporting System, 2009-2019.",
      "abstract": "Growing debates about algorithmic bias in public health surveillance lack specific examples. We tested a common assumption that exposure and illness periods coincide and demonstrated how algorithmic bias can arise due to missingness of critical information related to illness and exposure durations. We examined 9407 outbreaks recorded by the United States National Outbreak Reporting System (NORS) from January 1, 2009 through December 31, 2019 and detected algorithmic bias, a systematic over- or under-estimation of foodborne disease outbreak (FBDO) durations due to missing start and end dates. For 7037 (75%) FBDOs with complete date-time information,\u2009~\u200960% reported that the exposure period ended before the illness period started. For 2079 (87.7%) FBDOs with missing exposure dates, average illness durations were\u2009~\u20095.3 times longer (p\u2009<\u20090.001) than those with complete information, prompting the potential for algorithmic bias. Modern surveillance systems must be equipped with investigative capacities to examine and assess structural data missingness that can lead to bias.",
      "journal": "Journal of public health policy",
      "year": "2024",
      "doi": "10.1057/s41271-024-00477-2",
      "authors": "Diemer Emily et al.",
      "keywords": "Foodborne disease outbreak; Missing data; National Outbreak Reporting System (NORS); Outbreak progression",
      "mesh_terms": "Humans; United States; Disease Outbreaks; Algorithms; Bias; Foodborne Diseases; Public Health Surveillance; Population Surveillance",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38702378/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38718715",
      "title": "Achieve fairness without demographics for dermatological disease diagnosis.",
      "abstract": "In medical image diagnosis, fairness has become increasingly crucial. Without bias mitigation, deploying unfair AI would harm the interests of the underprivileged population and potentially tear society apart. Recent research addresses prediction biases in deep learning models concerning demographic groups (e.g., gender, age, and race) by utilizing demographic (sensitive attribute) information during training. However, many sensitive attributes naturally exist in dermatological disease images. If the trained model only targets fairness for a specific attribute, it remains unfair for other attributes. Moreover, training a model that can accommodate multiple sensitive attributes is impractical due to privacy concerns. To overcome this, we propose a method enabling fair predictions for sensitive attributes during the testing phase without using such information during training. Inspired by prior work highlighting the impact of feature entanglement on fairness, we enhance the model features by capturing the features related to the sensitive and target attributes and regularizing the feature entanglement between corresponding classes. This ensures that the model can only classify based on the features related to the target attribute without relying on features associated with sensitive attributes, thereby improving fairness and accuracy. Additionally, we use disease masks from the Segment Anything Model (SAM) to enhance the quality of the learned feature. Experimental results demonstrate that the proposed method can improve fairness in classification compared to state-of-the-art methods in two dermatological disease datasets.",
      "journal": "Medical image analysis",
      "year": "2024",
      "doi": "10.1016/j.media.2024.103188",
      "authors": "Chiu Ching-Hao et al.",
      "keywords": "AI fairness; Dermatological disease diagnosis; Fairness through unawareness",
      "mesh_terms": "Humans; Skin Diseases; Deep Learning; Image Interpretation, Computer-Assisted; Demography",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38718715/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38739385",
      "title": "Safe and Equitable Pediatric Clinical Use of AI.",
      "abstract": "This Viewpoint provides recommendations and stakeholder actions to support safe and equitable use of artificial intelligence (AI) in pediatric clinical settings.",
      "journal": "JAMA pediatrics",
      "year": "2024",
      "doi": "10.1001/jamapediatrics.2024.0897",
      "authors": "Handley Jessica L et al.",
      "keywords": "",
      "mesh_terms": "Humans; Pediatrics; Artificial Intelligence; Child",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38739385/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38773783",
      "title": "Machine learning evaluation of inequities and disparities associated with nurse sensitive indicator safety events.",
      "abstract": "PURPOSE: To use machine learning to examine health equity and clinical outcomes in patients who experienced a nurse sensitive indicator (NSI) event, defined as a fall, a hospital-acquired pressure injury (HAPI) or a hospital-acquired infection (HAI). DESIGN: This was a retrospective observational study from a single academic hospital over six calendar years (2016-2021). Machine learning was used to examine patients with an NSI compared to those without. METHODS: Inclusion criteria: all adult inpatient admissions (2016-2021). Three approaches were used to analyze the NSI group compared to the No-NSI group. In the univariate analysis, descriptive statistics, and absolute standardized differences (ASDs) were employed to compare the demographics and clinical variables of patients who experienced a NSI and those who did not experience any NSIs. For the multivariate analysis, a light grading boosting machine (LightGBM) model was utilized to comprehensively examine the relationships associated with the development of an NSI. Lastly, a simulation study was conducted to quantify the strength of associations obtained from the machine learning model. RESULTS: From 163,507 admissions, 4643 (2.8%) were associated with at least one NSI. The mean, standard deviation (SD) age was 59.5 (18.2) years, males comprised 82,397 (50.4%). Non-Hispanic White 84,760 (51.8%), non-Hispanic Black 8703 (5.3%), non-Hispanic Asian 23,368 (14.3%), non-Hispanic Other 14,284 (8.7%), and Hispanic 30,271 (18.5%). Race and ethnicity alone were not associated with occurrence of an NSI. The NSI group had a statistically significant longer length of stay (LOS), longer intensive care unit (ICU) LOS, and was more likely to have an emergency admission compared to the group without an NSI. The simulation study results demonstrated that likelihood of NSI was higher in patients admitted under the major diagnostic categories (MDC) associated with circulatory, digestive, kidney/urinary tract, nervous, and infectious and parasitic disease diagnoses. CONCLUSION: In this study, race/ethnicity was not associated with the risk of an NSI event. The risk of an NSI event was associated with emergency admission, longer LOS, longer ICU-LOS and certain MDCs (circulatory, digestive, kidney/urinary, nervous, infectious, and parasitic diagnoses). CLINICAL RELEVANCE: Machine learning methodologies provide a new mechanism to investigate NSI events through the lens of health equity/disparity. Understanding which patients are at higher risk for adverse outcomes can help hospitals improve nursing care and prevent NSI injury and harm.",
      "journal": "Journal of nursing scholarship : an official publication of Sigma Theta Tau International Honor Society of Nursing",
      "year": "2025",
      "doi": "10.1111/jnu.12983",
      "authors": "Georgantes Erika R et al.",
      "keywords": "diversity; health equity; machine learning; nurse sensitive indicators; patient safety",
      "mesh_terms": "Humans; Machine Learning; Male; Retrospective Studies; Female; Middle Aged; Aged; Adult; Accidental Falls; Patient Safety; Cross Infection; Healthcare Disparities; Pressure Ulcer",
      "pub_types": "Journal Article; Observational Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38773783/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38820189",
      "title": "Equity and AI governance at academic medical centers.",
      "abstract": "OBJECTIVES: To understand whether and how equity is considered in artificial intelligence/machine learning governance processes at academic medical centers. STUDY DESIGN: Qualitative analysis of interview data. METHODS: We created a database of academic medical centers from the full list of Association of American Medical Colleges hospital and health system members in 2022. Stratifying by census region and restricting to nonfederal and nonspecialty centers, we recruited chief medical informatics officers and similarly positioned individuals from academic medical centers across the country. We created and piloted a semistructured interview guide focused on (1) how academic medical centers govern artificial intelligence and prediction and (2) to what extent equity is considered in these processes. A total of 17 individuals representing 13 institutions across 4 census regions of the US were interviewed. RESULTS: A minority of participants reported considering inequity, racism, or bias in governance. Most participants conceptualized these issues as characteristics of a tool, using frameworks such as algorithmic bias or fairness. Fewer participants conceptualized equity beyond the technology itself and asked broader questions about its implications for patients. Disparities in health information technology resources across health systems were repeatedly identified as a threat to health equity. CONCLUSIONS: We found a lack of consistent equity consideration among academic medical centers as they develop their governance processes for predictive technologies despite considerable national attention to the ways these technologies can cause or reproduce inequities. Health systems and policy makers will need to specifically prioritize equity literacy among health system leadership, design oversight policies, and promote critical engagement with these tools and their implications to prevent the further entrenchment of inequities in digital health care.",
      "journal": "The American journal of managed care",
      "year": "2024",
      "doi": "10.37765/ajmc.2024.89555",
      "authors": "Nong Paige et al.",
      "keywords": "",
      "mesh_terms": "Academic Medical Centers; Humans; United States; Artificial Intelligence; Qualitative Research; Health Equity; Interviews as Topic; Racism",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38820189/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38851413",
      "title": "Evaluating accuracy and fairness of clinical decision support algorithms when health care resources are limited.",
      "abstract": "OBJECTIVE: Guidance on how to evaluate accuracy and algorithmic fairness across subgroups is missing for clinical models that flag patients for an intervention but when health care resources to administer that intervention are limited. We aimed to propose a framework of metrics that would fit this specific use case. METHODS: We evaluated the following metrics and applied them to a Veterans Health Administration clinical model that flags patients for intervention who are at risk of overdose or a suicidal event among outpatients who were prescribed opioids (N\u00a0=\u00a0405,817): Receiver - Operating Characteristic and area under the curve, precision - recall curve, calibration - reliability curve, false positive rate, false negative rate, and false omission rate. In addition, we developed a new approach to visualize false positives and false negatives that we named 'per true positive bars.' We demonstrate the utility of these metrics to our use case for three cohorts of patients at the highest risk (top 0.5\u00a0%, 1.0\u00a0%, and 5.0\u00a0%) by evaluating algorithmic fairness across the following age groups: <=30, 31-50, 51-65, and\u00a0>65\u00a0years old. RESULTS: Metrics that allowed us to assess group differences more clearly were the false positive rate, false negative rate, false omission rate, and the new 'per true positive bars'. Metrics with limited utility to our use case were the Receiver - Operating Characteristic and area under the curve, the calibration - reliability curve, and the precision - recall curve. CONCLUSION: There is no \"one size fits all\" approach to model performance monitoring and bias analysis. Our work informs future researchers and clinicians who seek to evaluate accuracy and fairness of predictive models that identify patients to intervene on in the context of limited health care resources. In terms of ease of interpretation and utility for our use case, the new 'per true positive bars' may be the most intuitive to a range of stakeholders and facilitates choosing a threshold that allows weighing false positives against false negatives, which is especially important when predicting severe adverse events.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104664",
      "authors": "Meerwijk Esther L et al.",
      "keywords": "Algorithmic fairness; Bias; Prediction models; Suicide; Veterans",
      "mesh_terms": "Humans; Algorithms; Decision Support Systems, Clinical; Middle Aged; Adult; Aged; Reproducibility of Results; ROC Curve; Female; Male; United States; United States Department of Veterans Affairs",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38851413/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38876452",
      "title": "Identify and mitigate bias in electronic phenotyping: A comprehensive study from computational perspective.",
      "abstract": "Electronic phenotyping is a fundamental task that identifies the special group of patients, which plays an important role in precision medicine in the era of digital health. Phenotyping provides real-world evidence for other related biomedical research and clinical tasks, e.g., disease diagnosis, drug development, and clinical trials, etc. With the development of electronic health records, the performance of electronic phenotyping has been significantly boosted by advanced machine learning techniques. In the healthcare domain, precision and fairness are both essential aspects that should be taken into consideration. However, most related efforts are put into designing phenotyping models with higher accuracy. Few attention is put on the fairness perspective of phenotyping. The neglection of bias in phenotyping leads to subgroups of patients being underrepresented which will further affect the following healthcare activities such as patient recruitment in clinical trials. In this work, we are motivated to bridge this gap through a comprehensive experimental study to identify the bias existing in electronic phenotyping models and evaluate the widely-used debiasing methods' performance on these models. We choose pneumonia and sepsis as our phenotyping target diseases. We benchmark 9 kinds of electronic phenotyping methods spanning from rule-based to data-driven methods. Meanwhile, we evaluate the performance of the 5 bias mitigation strategies covering pre-processing, in-processing, and post-processing. Through the extensive experiments, we summarize several insightful findings from the bias identified in the phenotyping and key points of the bias mitigation strategies in phenotyping.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104671",
      "authors": "Ding Sirui et al.",
      "keywords": "Algorithm fairness; Bias mitigation; Electronic phenotyping; Fairness in healthcare",
      "mesh_terms": "Bias; Benchmarking; Data Mining; Electronic Health Records; Algorithms; Pneumonia; Sepsis; Cohort Studies; Machine Learning; Phenotype; Humans; Male; Female; Racial Groups; Sex Factors; Datasets as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38876452/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38925281",
      "title": "Assessing racial bias in healthcare predictive models: Practical lessons from an empirical evaluation of 30-day hospital readmission models.",
      "abstract": "OBJECTIVE: Despite increased availability of methodologies to identify algorithmic bias, the operationalization of bias evaluation for healthcare predictive models is still limited. Therefore, this study proposes a process for bias evaluation through an empirical assessment of common hospital readmission models. The process includes selecting bias measures, interpretation, determining disparity impact and potential mitigations. METHODS: This retrospective analysis evaluated racial bias of four common models predicting 30-day unplanned readmission (i.e., LACE Index, HOSPITAL Score, and the CMS readmission measure applied as is and retrained). The models were assessed using 2.4 million adult inpatient discharges in Maryland from 2016 to 2019. Fairness metrics that are model-agnostic, easy to compute, and interpretable were implemented and apprised to select the most appropriate bias measures. The impact of changing model's risk thresholds on these measures was further assessed to guide the selection of optimal thresholds to control and mitigate bias. RESULTS: Four bias measures were selected for the predictive task: zero-one-loss difference, false negative rate (FNR) parity, false positive rate (FPR) parity, and generalized entropy index. Based on these measures, the HOSPITAL score and the retrained CMS measure demonstrated the lowest racial bias. White patients showed a higher FNR while Black patients resulted in a higher FPR and zero-one-loss. As the models' risk threshold changed, trade-offs between models' fairness and overall performance were observed, and the assessment showed all models' default thresholds were reasonable for balancing accuracy and bias. CONCLUSIONS: This study proposes an Applied Framework to Assess Fairness of Predictive Models (AFAFPM) and demonstrates the process using 30-day hospital readmission model as the example. It suggests the feasibility of applying algorithmic bias assessment to determine optimized risk thresholds so that predictive models can be used more equitably and accurately. It is evident that a combination of qualitative and quantitative methods and a multidisciplinary team are necessary to identify, understand and respond to algorithm bias in real-world healthcare settings. Users should also apply multiple bias measures to ensure a more comprehensive, tailored, and balanced view. The results of bias measures, however, must be interpreted with caution and consider the larger operational, clinical, and policy context.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104683",
      "authors": "Wang H Echo et al.",
      "keywords": "Algorithmic Bias; Algorithmic Fairness; Health Disparity; Hospital Readmission; Population Health Management; Predictive Models",
      "mesh_terms": "Humans; Patient Readmission; Racism; Retrospective Studies; Male; Female; Middle Aged; Adult; Aged; Maryland; Algorithms; Healthcare Disparities",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38925281/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38964129",
      "title": "Use of artificial intelligence to address health disparities in low- and middle-income countries: a thematic analysis of ethical issues.",
      "abstract": "OBJECTIVES: Artificial intelligence (AI) is reshaping health and medicine, especially through its potential to address health disparities in low- and middle-income countries (LMICs). However, there are several issues associated with the use of AI that may reduce its impact and potentially exacerbate global health disparities. This study presents the key issues in AI deployment faced by LMICs. STUDY DESIGN: Thematic analysis. METHODS: PubMed, Scopus, Embase and the Web of Science databases were searched, from the date of their inception until September 2023, using the terms \"artificial intelligence\", \"LMICs\", \"ethic\u2217\" and \"global health\". Additional searches were conducted by snowballing references before and after the primary search. The final studies were chosen based on their relevance to the topic of this article. RESULTS: After reviewing 378 articles, 14 studies were included in the final analysis. A concept named the 'AI Deployment Paradox' was introduced to focus on the challenges of using AI to address health disparities in LMICs, and the following three categories were identified: (1) data poverty and contextual shifts; (2) cost-effectiveness and health equity; and (3) new technological colonisation and potential exploitation. CONCLUSIONS: The relationship between global health, AI and ethical considerations is an area that requires systematic investigation. Relying on health data inherent with structural biases and deploying AI without systematic ethical considerations may exacerbate global health inequalities. Addressing these challenges requires nuanced socio-political comprehension, localised stakeholder engagement, and well-considered ethical and regulatory frameworks.",
      "journal": "Public health",
      "year": "2024",
      "doi": "10.1016/j.puhe.2024.05.029",
      "authors": "Yu Lanyi et al.",
      "keywords": "Artificial intelligence; Ethics; Global health; Low- and middle-income countries",
      "mesh_terms": "Humans; Artificial Intelligence; Cost-Benefit Analysis; Developing Countries; Global Health; Health Equity; Health Status Disparities; Healthcare Disparities",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38964129/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38981215",
      "title": "Promoting fairness in activity recognition algorithms for patient's monitoring and evaluation systems in healthcare.",
      "abstract": "Researchers face the challenge of defining subject selection criteria when training algorithms for human activity recognition tasks. The ongoing uncertainty revolves around which characteristics should be considered to ensure algorithmic robustness across diverse populations. This study aims to address this challenge by conducting an analysis of heterogeneity in the training data to assess the impact of physical characteristics and soft-biometric attributes on activity recognition performance. The performance of various state-of-the-art deep neural network architectures (tCNN, hybrid-LSTM, Transformer model) processing time-series data using the IntelliRehab (IRDS) dataset was evaluated. By intentionally introducing bias into the training data based on human characteristics, the objective is to identify the characteristics that influence algorithms in motion analysis. Experimental findings reveal that the CNN-LSTM model achieved the highest accuracy, reaching 88%. Moreover, models trained on heterogeneous distributions of disability attributes exhibited notably higher accuracy, reaching 51%, compared to those not considering such factors, which scored an average of 33%. These evaluations underscore the significant influence of subjects' characteristics on activity recognition performance, providing valuable insights into the algorithm's robustness across diverse populations. This study represents a significant step forward in promoting fairness and trustworthiness in artificial intelligence by quantifying representation bias in multi-channel time-series activity recognition data within the healthcare domain.",
      "journal": "Computers in biology and medicine",
      "year": "2024",
      "doi": "10.1016/j.compbiomed.2024.108826",
      "authors": "Mennella Ciro et al.",
      "keywords": "Artificial intelligence; Bias; Deep learning; Motion analysis; Rehabilitation; Time-series",
      "mesh_terms": "Humans; Algorithms; Male; Female; Neural Networks, Computer; Adult; Middle Aged; Human Activities; Aged",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38981215/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39009174",
      "title": "Evaluating gender bias in ML-based clinical risk prediction models: A study on multiple use cases at different hospitals.",
      "abstract": "BACKGROUND: An inherent difference exists between male and female bodies, the historical under-representation of females in clinical trials widened this gap in existing healthcare data. The fairness of clinical decision-support tools is at risk when developed based on biased data. This paper aims to quantitatively assess the gender bias in risk prediction models. We aim to generalize our findings by performing this investigation on multiple use cases at different hospitals. METHODS: First, we conduct a thorough analysis of the source data to find gender-based disparities. Secondly, we assess the model performance on different gender groups at different hospitals and on different use cases. Performance evaluation is quantified using the area under the receiver-operating characteristic curve (AUROC). Lastly, we investigate the clinical implications of these biases by analyzing the underdiagnosis and overdiagnosis rate, and the decision curve analysis (DCA). We also investigate the influence of model calibration on mitigating gender-related disparities in decision-making processes. RESULTS: Our data analysis reveals notable variations in incidence rates, AUROC, and over-diagnosis rates across different genders, hospitals and clinical use cases. However, it is also observed the underdiagnosis rate is consistently higher in the female population. In general, the female population exhibits lower incidence rates and the models perform worse when applied to this group. Furthermore, the decision curve analysis demonstrates there is no statistically significant difference between the model's clinical utility across gender groups within the interested range of thresholds. CONCLUSION: The presence of gender bias within risk prediction models varies across different clinical use cases and healthcare institutions. Although inherent difference is observed between male and female populations at the data source level, this variance does not affect the parity of clinical utility. In conclusion, the evaluations conducted in this study highlight the significance of continuous monitoring of gender-based disparities in various perspectives for clinical risk prediction models.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104692",
      "authors": "Cabanillas Silva Patricia et al.",
      "keywords": "Clinical risk prediction; Gender bias; Machine learning; Model evaluation; Model fairness; Prediction model",
      "mesh_terms": "Humans; Female; Male; Sexism; ROC Curve; Risk Assessment; Hospitals; Area Under Curve; Decision Support Systems, Clinical",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39009174/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39049285",
      "title": "Fairness in Classifying and Grouping Health Equity Information.",
      "abstract": "This paper explores the balance between fairness and performance in machine learning classification, predicting the likelihood of a patient receiving anti-microbial treatment using structured data in community nursing wound care electronic health records. The data includes two important predictors (gender and language) of the social determinants of health, which we used to evaluate the fairness of the classifiers. At the same time, the impact of various groupings of language codes on classifiers' performance and fairness is analyzed. Most common statistical learning-based classifiers are evaluated. The findings indicate that while K-Nearest Neighbors offers the best fairness metrics among different grouping settings, the performance of all classifiers is generally consistent across different language code groupings. Also, grouping more variables tends to improve the fairness metrics over all classifiers while maintaining their performance.",
      "journal": "Studies in health technology and informatics",
      "year": "2024",
      "doi": "10.3233/SHTI240171",
      "authors": "Jin Ruinan et al.",
      "keywords": "Electronic Health Record; Fairness and Bias; Feature Engineering",
      "mesh_terms": "Health Equity; Electronic Health Records; Machine Learning; Humans; Social Determinants of Health",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39049285/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39106773",
      "title": "Fairness gaps in Machine learning models for hospitalization and emergency department visit risk prediction in home healthcare patients with heart failure.",
      "abstract": "OBJECTIVES: This study aims to evaluate the fairness performance metrics of Machine Learning (ML) models to predict hospitalization and emergency department (ED) visits in heart failure patients receiving home healthcare. We analyze biases, assess performance disparities, and propose solutions to improve model performance in diverse subpopulations. METHODS: The study used a dataset of 12,189 episodes of home healthcare collected between 2015 and 2017, including structured (e.g., standard assessment tool) and unstructured data (i.e., clinical notes). ML risk prediction models, including Light Gradient-boosting model (LightGBM) and AutoGluon, were developed using demographic information, vital signs, comorbidities, service utilization data, and the area deprivation index (ADI) associated with the patient's home address. Fairness metrics, such as Equal Opportunity, Predictive Equality, Predictive Parity, and Statistical Parity, were calculated to evaluate model performance across subpopulations. RESULTS: Our study revealed significant disparities in model performance across diverse demographic subgroups. For example, the Hispanic, Male, High-ADI subgroup excelled in terms of Equal Opportunity with a metric value of 0.825, which was 28% higher than the lowest-performing Other, Female, Low-ADI subgroup, which scored 0.644. In Predictive Parity, the gap between the highest and lowest-performing groups was 29%, and in Statistical Parity, the gap reached 69%. In Predictive Equality, the difference was 45%. DISCUSSION AND CONCLUSION: The findings highlight substantial differences in fairness metrics across diverse patient subpopulations in ML risk prediction models for heart failure patients receiving home healthcare services. Ongoing monitoring and improvement of fairness metrics are essential to mitigate biases.",
      "journal": "International journal of medical informatics",
      "year": "2024",
      "doi": "10.1016/j.ijmedinf.2024.105534",
      "authors": "Davoudi Anahita et al.",
      "keywords": "Bias; Healthcare Disparities; Heart Failure; Home Care Services; Machine Learning; Socioeconomic Factors",
      "mesh_terms": "Humans; Heart Failure; Machine Learning; Emergency Service, Hospital; Male; Female; Hospitalization; Home Care Services; Aged; Risk Assessment; Middle Aged; Aged, 80 and over; Emergency Room Visits",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39106773/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39126673",
      "title": "Predictive roles of cognitive biases in health anxiety: A machine learning approach.",
      "abstract": "Prior work suggests that cognitive biases may contribute to health anxiety. Yet there is little research investigating how biased attention, interpretation, and memory for health threats are collectively associated with health anxiety, as well as the relative importance of these cognitive processes in predicting health anxiety. This study aimed to build a prediction model for health anxiety with multiple cognitive biases as potential predictors and to identify the biased cognitive processes that best predict individual differences in health anxiety. A machine learning algorithm (elastic net) was performed to recognise the predictors of health anxiety, using various tasks of attention, interpretation, and memory measured across behavioural, self-reported, and computational modelling approaches. Participants were 196 university students with a range of health anxiety severity from mild to severe. The results showed that only the interpretation bias for illness and the attention bias towards symptoms significantly contributed to the prediction model of health anxiety, with both biases having positive weights and the former being the most important predictor. These findings underscore the central role of illness-related interpretation bias and suggest that combined cognitive bias modification may be a promising method for alleviating health anxiety.",
      "journal": "Stress and health : journal of the International Society for the Investigation of Stress",
      "year": "2024",
      "doi": "10.1002/smi.3463",
      "authors": "Shi Congrong et al.",
      "keywords": "attention bias; health anxiety; interpretation bias; machine learning; memory bias",
      "mesh_terms": "Humans; Machine Learning; Male; Female; Young Adult; Adult; Anxiety; Cognition; Adolescent; Attention",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39126673/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39176898",
      "title": "How Data Infrastructure Deals with Bias Problems in Medical Imaging.",
      "abstract": "The paper discusses biases in medical imaging analysis, particularly focusing on the challenges posed by the development of machine learning algorithms and generative models. It introduces a taxonomy of bias problems and addresses them through a data infrastructure initiative: the PADME (Platform for Analytics and Distributed Machine-Learning for Enterprises), which is a part of the National Research Data Infrastructure for Personal Health Data (NFDI4Health) project. The PADME facilitates the structuring and sharing of health data while ensuring privacy and adherence to FAIR principles. The paper presents experimental results that show that generative methods can be effective in data augmentation. Complying with PADME infrastructure, this work proposes a solution framework to deal with bias in the different data stations and preserve privacy when transferring images. It highlights the importance of standardized data infrastructure in mitigating biases and promoting FAIR, reusable, and privacy-preserving research environments in healthcare.",
      "journal": "Studies in health technology and informatics",
      "year": "2024",
      "doi": "10.3233/SHTI240517",
      "authors": "Li Feifei et al.",
      "keywords": "Bias; Data Infrastructure; Differential Privacy; Federated Learning; Machine Learning; Medical Imaging",
      "mesh_terms": "Diagnostic Imaging; Humans; Machine Learning; Bias; Algorithms; Confidentiality; Computer Security",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39176898/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39230911",
      "title": "Addressing AI Algorithmic Bias in Health Care.",
      "abstract": "This Viewpoint discusses the bias that exists in artificial intelligence (AI) algorithms used in health care despite recent federal rules to prohibit discriminatory outcomes from AI and recommends ways in which health care facilities, AI developers, and regulators could share responsibilities and actions to address bias.",
      "journal": "JAMA",
      "year": "2024",
      "doi": "10.1001/jama.2024.13486",
      "authors": "Ratwani Raj M et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Bias; Digital Health; Decision Support Systems, Clinical; United States Dept. of Health and Human Services; Software Design; Certification",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39230911/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39232303",
      "title": "Bias Perpetuates Bias: ChatGPT Learns Gender Inequities in Academic Surgery Promotions.",
      "abstract": "OBJECTIVE: Gender inequities persist in academic surgery with implicit bias impacting hiring and promotion at all levels. We hypothesized that creating letters of recommendation for both female and male candidates for academic promotion in surgery using an AI platform, ChatGPT, would elucidate the entrained gender biases already present in the promotion process. DESIGN: Using ChatGPT, we generated 6 letters of recommendation for \"a phenomenal surgeon applying for job promotion to associate professor position\", specifying \"female\" or \"male\" before surgeon in the prompt. We compared 3 \"female\" letters to 3 \"male\" letters for differences in length, language, and tone. RESULTS: The letters written for females averaged 298 words compared to 314 for males. Female letters more frequently referred to \"compassion\", \"empathy\", and \"inclusivity\"; whereas male letters referred to \"respect\", \"reputation\", and \"skill\". CONCLUSIONS: These findings highlight the gender bias present in promotion letters generated by ChatGPT, reiterating existing literature regarding real letters of recommendation in academic surgery. Our study suggests that surgeons should use AI tools, such as ChatGPT, with caution when writing LORs for academic surgery faculty promotion.",
      "journal": "Journal of surgical education",
      "year": "2024",
      "doi": "10.1016/j.jsurg.2024.07.023",
      "authors": "Desai Pooja et al.",
      "keywords": "ChatGPT; Gender disparities in medicine; academic promotions; artificial intelligence; implicit bias; letters of recommendation",
      "mesh_terms": "Female; Humans; Male; Sexism; Faculty, Medical; General Surgery; Career Mobility; Personnel Selection; Correspondence as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39232303/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39232870",
      "title": "Predicting serious postoperative complications and evaluating racial fairness in machine learning algorithms for metabolic and bariatric surgery.",
      "abstract": "BACKGROUND: Predicting the risk of complications is critical in metabolic and bariatric surgery (MBS). OBJECTIVES: To develop machine learning (ML) models to predict serious postoperative complications of MBS and evaluate racial fairness of the models. SETTING: Metabolic and Bariatric Surgery Accreditation and Quality Improvement Program (MBSAQIP) national database, United States. METHODS: We developed logistic regression, random forest (RF), gradient-boosted tree (GBT), and XGBoost model using the MBSAQIP Participant Use Data File from 2016 to 2020. To address the class imbalance, we randomly undersampled the complication-negative class to match the complication-positive class. Model performance was evaluated using the area under the receiver operating characteristic curve (AUROC), precision, recall, and F1 score. Fairness across White and non-White patient groups was assessed using equal opportunity difference and disparate impact metrics. RESULTS: A total of 40,858 patients were included after undersampling the complication-negative class. The XGBoost model was the best-performing model in terms of AUROC; however, the difference was not statistically significant. While the F1 score and precision did not vary significantly across models, the RF exhibited better recall compared to the logistic regression. Surgery type was the most important feature to predict complications, followed by operative time. The logistic regression model had the best fairness metrics for race. CONCLUSIONS: The XGBoost model achieved the highest AUROC, albeit without a statistically significant difference. The RF may be useful when recall is the primary concern. Undersampling of the privileged group may improve the fairness of boosted tree models.",
      "journal": "Surgery for obesity and related diseases : official journal of the American Society for Bariatric Surgery",
      "year": "2024",
      "doi": "10.1016/j.soard.2024.08.008",
      "authors": "Kang Dong-Won et al.",
      "keywords": "Complication; Machine learning; Metabolic and bariatric surgery; Roux-en-Y gastric bypass; Sleeve gastrectomy",
      "mesh_terms": "Humans; Bariatric Surgery; Machine Learning; Postoperative Complications; Female; Male; Middle Aged; Adult; United States; Algorithms; Obesity, Morbid; Risk Assessment",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39232870/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39264601",
      "title": "Artificial Intelligence for Language Translation: The Equity Is in the Details.",
      "abstract": "This Viewpoint discusses the challenges to implementing artificial intelligence\u2013based translation in clinical settings and what health care organizations can do to mitigate these challenges.",
      "journal": "JAMA",
      "year": "2024",
      "doi": "10.1001/jama.2024.15296",
      "authors": "Lion K Casey et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39264601/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39393825",
      "title": "Artificial intelligence and global health equity.",
      "abstract": "",
      "journal": "BMJ (Clinical research ed.)",
      "year": "2024",
      "doi": "10.1136/bmj.q2194",
      "authors": "Dychiao Robyn Gayle et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Health Equity; Global Health",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39393825/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39433263",
      "title": "Racial and Ethnic Disparities in Predictive Accuracy of Machine Learning Algorithms Developed Using a National Database for 30-Day Complications Following Total Joint Arthroplasty.",
      "abstract": "BACKGROUND: While predictive capabilities of machine learning (ML) algorithms for hip and knee total joint arthroplasty (TJA) have been demonstrated in previous studies, their performance in racial and ethnic minority patients has not been investigated. This study aimed to assess the performance of ML algorithms in predicting 30-days complications following TJA in racial and ethnic minority patients. METHODS: A total of 267,194 patients undergoing primary TJA between 2013 and 2020 were identified from a national outcomes database. The patient cohort was stratified according to race, with further substratification into Hispanic or non-Hispanic ethnicity. There were two ML algorithms, histogram-based gradient boosting (HGB), and random forest (RF), that were modeled to predict 30-days complications following primary TJA in the overall population. They were subsequently assessed in each racial and ethnic subcohort using discrimination, calibration, accuracy, and potential clinical usefulness. RESULTS: Both models achieved excellent (Area under the curve (AUC) > 0.8) discrimination (AUCHGB\u00a0= AUCRF\u00a0= 0.86), calibration, and accuracy (HGB: slope\u00a0= 1.00, intercept\u00a0=\u00a0-0.03, Brier score\u00a0= 0.12; RF: slope\u00a0= 0.97, intercept\u00a0= 0.02, Brier score\u00a0= 0.12) in the non-Hispanic White population (N\u00a0= 224,073). Discrimination decreased in the White Hispanic (N\u00a0= 10,429; AUC\u00a0= 0.75 to 0.76), Black (N\u00a0= 25,116; AUC\u00a0= 0.77), Black Hispanic (N\u00a0= 240; AUC\u00a0= 0.78), Asian non-Hispanic (N\u00a0= 4,809; AUC\u00a0= 0.78 to 0.79), and overall (N\u00a0= 267,194; AUC\u00a0= 0.75 to 0.76) cohorts, but remained well-calibrated. We noted the poorest model discrimination (N\u00a0= 1,870; AUC\u00a0= 0.67 to 0.68) and calibration in the American-Indian cohort. CONCLUSIONS: The ML algorithms demonstrate an inferior predictive ability for 30-days complications following primary TJA in racial and ethnic minorities when trained on existing healthcare big data. This may be attributed to the disproportionate underrepresentation of minority groups within these databases, as demonstrated by the smaller sample sizes available to train the ML models. The ML models developed using smaller datasets (e.g., in racial and ethnic minorities) may not be as accurate as larger datasets, highlighting the need for equity-conscious model development. LEVEL OF EVIDENCE: III; retrospective cohort study.",
      "journal": "The Journal of arthroplasty",
      "year": "2025",
      "doi": "10.1016/j.arth.2024.10.060",
      "authors": "Pean Christian A et al.",
      "keywords": "artificial intelligence; big data; health disparities; health inequity; machine learning; total joint arthroplasty",
      "mesh_terms": "Humans; Female; Machine Learning; Male; Middle Aged; Aged; Arthroplasty, Replacement, Knee; Arthroplasty, Replacement, Hip; Databases, Factual; Algorithms; Postoperative Complications; Ethnicity; Hispanic or Latino; Retrospective Studies; Racial Groups; White",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39433263/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39436296",
      "title": "Uncovering Demographic Bias in Natural Language Processing Tools for Radiology.",
      "abstract": "",
      "journal": "Radiology",
      "year": "2024",
      "doi": "10.1148/radiol.242723",
      "authors": "Cai Wenli",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39436296/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39488166",
      "title": "Mitigating biases in feature selection and importance assessments in predictive models using LASSO regression.",
      "abstract": "Yuan et al. developed a predictive model for early response using sub-regional radiomic features from multi-sequence MRI alongside clinical factors. However, biases in feature selection and assessment may lead to misleading conclusions regarding feature importance. This paper elucidates the biases induced by machine learning models and advocates for a robust methodology utilizing statistical techniques, such as Chi-squared tests and p-values, to uncover true associations. By emphasizing the vital distinction between true and model-specific associations, we promote a comprehensive approach that integrates multiple modeling techniques. This strategy enhances the reliability of predictive models in medical imaging, ensuring that outcomes are based on objective relationships and ultimately improving patient care.",
      "journal": "Oral oncology",
      "year": "2024",
      "doi": "10.1016/j.oraloncology.2024.107090",
      "authors": "Takefuji Yoshiyasu",
      "keywords": "Feature selection; LASSO regression; Machine learning; Predictive modeling; Statistical methods",
      "mesh_terms": "Humans; Machine Learning; Bias; Magnetic Resonance Imaging; Models, Statistical",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39488166/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39492552",
      "title": "Disparities in Diagnosis, Access to Specialist Care and Treatment for Inborn Errors of Immunity.",
      "abstract": "Inborn errors of immunity represent a rapidly expanding group of genetic disorders of the immune system. Significant advances have been made in recent years in diagnosis, including using genetic testing and newborn screening; treatment, including precision therapies, gene therapy and hematopoietic stem cell transplant; and development of patient registries to inform prevalence, understand morbidity of these disorders and guide the development of clinical trials. However, significant disparities due to age, race, ethnicity, socioeconomic status, or geographic location exist in all aspects of care of patients with inborn errors of immunity, beginning with delays in diagnosis and further compounded by impaired access to specialist care and treatment, leading to a notable impact on outcomes including morbidity and mortality. Addressing and correcting these disparities will require coordinated, deliberate and prolonged effort. Proposed strategies to improve equity at different levels include public health measures such as implementing universal newborn screening, supporting expanded health insurance coverage for diagnostic testing and treatment, improving access to novel therapeutics in low and middle income countries and developing artificial intelligence / machine learning tools to reduce delays in diagnosis, particularly in rural or less developed areas where access to specialist care is limited.",
      "journal": "The journal of allergy and clinical immunology. In practice",
      "year": "2023",
      "doi": "10.1016/j.jaip.2023.10.041",
      "authors": "Lawrence Monica G et al.",
      "keywords": "Health disparities; inborn errors of immunity; primary immunodeficiency",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39492552/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39495385",
      "title": "Evaluating machine learning model bias and racial disparities in non-small cell lung cancer using SEER registry data.",
      "abstract": "BACKGROUND: Despite decades of pursuing health equity, racial and ethnic disparities persist in healthcare in America. For cancer specifically, one of the leading observed disparities is worse mortality among non-Hispanic Black patients compared to non-Hispanic White patients across the cancer care continuum. These real-world disparities are reflected in the data used to inform the decisions made to alleviate such inequities. Failing to account for inherently biased data underlying these observations could intensify racial cancer disparities and lead to misguided efforts that fail to appropriately address the real causes of health inequity. OBJECTIVE: Estimate the racial/ethnic bias of machine learning models in predicting two-year survival and surgery treatment recommendation for non-small cell lung cancer (NSCLC) patients. METHODS: A Cox survival model, and a LOGIT model as well as three other machine learning models for predicting surgery recommendation were trained using SEER data from NSCLC patients diagnosed from 2000-2018. Models were trained with a 70/30 train/test split (both including and excluding race/ethnicity) and evaluated using performance and fairness metrics. The effects of oversampling the training data were also evaluated. RESULTS: The survival models show disparate impact towards non-Hispanic Black patients regardless of whether race/ethnicity is used as a predictor. The models including race/ethnicity amplified the disparities observed in the data. The exclusion of race/ethnicity as a predictor in the survival and surgery recommendation models improved fairness metrics without degrading model performance. Stratified oversampling strategies reduced disparate impact while reducing the accuracy of the model. CONCLUSION: NSCLC disparities are complex and multifaceted. Yet, even when accounting for age and stage at diagnosis, non-Hispanic Black patients with NSCLC are less often recommended to have surgery than non-Hispanic White patients. Machine learning models amplified the racial/ethnic disparities across the cancer care continuum (which are reflected in the data used to make model decisions). Excluding race/ethnicity lowered the bias of the models but did not affect disparate impact. Developing analytical strategies to improve fairness would in turn improve the utility of machine learning approaches analyzing population-based cancer data.",
      "journal": "Health care management science",
      "year": "2024",
      "doi": "10.1007/s10729-024-09691-6",
      "authors": "Trentz Cameron et al.",
      "keywords": "Fairness in AI; Health disparities; Non-small cell lung cancer survival; Racial disparities",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Bias; Black or African American; Carcinoma, Non-Small-Cell Lung; Ethnicity; Healthcare Disparities; Lung Neoplasms; Machine Learning; Proportional Hazards Models; Racial Groups; SEER Program; United States; White",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39495385/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39495690",
      "title": "UnBias: Unveiling Bias Implications in Deep Learning Models for Healthcare Applications.",
      "abstract": "The rapid integration of deep learning-powered artificial intelligence systems in diverse applications such as healthcare, credit assessment, employment, and criminal justice has raised concerns about their fairness, particularly in how they handle various demographic groups. This study delves into the existing biases and their ethical implications in deep learning models. It introduces an UnBias approach for assessing bias in different deep neural network architectures and detects instances where bias seeps into the learning process, shifting the model's focus away from the main features. This contributes to the advancement of equitable and trustworthy AI applications in diverse social settings, especially in healthcare. A case study on COVID-19 detection is carried out, involving chest X-ray scan datasets from various publicly accessible repositories and five well-represented and underrepresented gender-based models across four deep-learning architectures: ResNet50V2, DenseNet121, InceptionV3, and Xception.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2025",
      "doi": "10.1109/JBHI.2024.3484951",
      "authors": "AbdulQawy Asmaa et al.",
      "keywords": "",
      "mesh_terms": "Deep Learning; Humans; COVID-19; SARS-CoV-2; Male; Female; Bias; Neural Networks, Computer",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39495690/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39499598",
      "title": "Bias Amplification to Facilitate the Systematic Evaluation of Bias Mitigation Methods.",
      "abstract": "The future of artificial intelligence (AI) safety is expected to include bias mitigation methods from development to application. The complexity and integration of these methods could grow in conjunction with advances in AI and human-AI interactions. Numerous methods are being proposed to mitigate bias, but without a structured way to compare their strengths and weaknesses. In this work, we present two approaches to systematically amplify subgroup performance bias. These approaches allow for the evaluation and comparison of the effectiveness of bias mitigation methods on AI models by varying the degrees of bias, and can be applied to any classification model. We used these approaches to compare four off-the-shelf bias mitigation methods. Both amplification approaches promote the development of learning shortcuts in which the model forms associations between patient attributes and AI output. We demonstrate these approaches in a case study, evaluating bias in the determination of COVID status from chest x-rays. The maximum achieved increase in performance bias, measured as a difference in predicted prevalence, was 72% and 32% for bias between subgroups related to patient sex and race, respectively. These changes in predicted prevalence were not accompanied by substantial changes in the differences in subgroup area under the receiver operating characteristic curves, indicating that the increased bias is due to the formation of learning shortcuts, not a difference in ability to distinguish positive and negative patients between subgroups.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2025",
      "doi": "10.1109/JBHI.2024.3491946",
      "authors": "Burgon Alexis et al.",
      "keywords": "",
      "mesh_terms": "Humans; COVID-19; Bias; Artificial Intelligence; SARS-CoV-2; Female; Male; Radiography, Thoracic",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39499598/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39500799",
      "title": "Unbiased and reproducible liver MRI-PDFF estimation using a scan protocol-informed deep learning method.",
      "abstract": "OBJECTIVE: To estimate proton density fat fraction (PDFF) from chemical shift encoded (CSE) MR images using a deep learning (DL)-based method that is precise and robust to different MR scanners and acquisition echo times (TEs). METHODS: Variable echo times neural network (VET-Net) is a two-stage framework that first estimates nonlinear variables of the CSE-MR signal model, to posteriorly estimate water/fat signal components using the least-squares method. VET-Net incorporates a vector with TEs as an auxiliary input, therefore enabling PDFF calculation with any TE setting. A single-site liver CSE-MRI dataset (188 subjects, 4146 axial slices) was considered, which was split into training (150 subjects), validation (18), and testing (20) subsets. Testing subjects were scanned using several protocols with different TEs, which we then used to measure the PDFF reproducibility coefficient (RDC) at two regions of interest (ROIs): the right posterior and left hepatic lobes. An open-source multi-site and multi-vendor fat-water phantom dataset was also used for PDFF bias assessment. RESULTS: VET-Net showed RDCs of 1.71% and 1.04% on the right posterior and left hepatic lobes, respectively, across different TEs, which was comparable to a reference graph cuts-based method (RDCs\u2009=\u20091.71% and 0.86%). VET-Net also showed a smaller PDFF bias (-0.55%) than graph cuts (0.93%) when tested on a multi-site phantom dataset. Reproducibility (1.94% and 1.59%) and bias (-2.04%) were negatively affected when the auxiliary TE input was not considered. CONCLUSION: VET-Net provided unbiased and precise PDFF estimations using CSE-MR images from different hardware vendors and different TEs, outperforming conventional DL approaches. KEY POINTS: Question Reproducibility of liver PDFF DL-based approaches on different scan protocols or manufacturers is not validated. Findings VET-Net showed a PDFF bias of -0.55% on a multi-site phantom dataset, and RDCs of 1.71% and 1.04% at two liver ROIs. Clinical relevance VET-Net provides efficient, in terms of scan and processing times, and unbiased PDFF estimations across different MR scanners and scan protocols, and therefore it can be leveraged to expand the use of MRI-based liver fat quantification to assess hepatic steatosis.",
      "journal": "European radiology",
      "year": "2025",
      "doi": "10.1007/s00330-024-11164-x",
      "authors": "Meneses Juan P et al.",
      "keywords": "Biomarkers; Deep learning; Liver; Magnetic resonance imaging",
      "mesh_terms": "Humans; Deep Learning; Magnetic Resonance Imaging; Reproducibility of Results; Liver; Male; Female; Phantoms, Imaging; Middle Aged; Adult; Adipose Tissue; Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39500799/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39612748",
      "title": "The role of artificial intelligence in enhancing healthcare for people with disabilities.",
      "abstract": "The integration of artificial intelligence (AI) in healthcare delivery represents a transformative opportunity to enhance the lives of people living with disabilities. AI-driven technologies, such as assistive devices, conversational agents, and rehabilitation tools, can mitigate health disparities, improve diagnostic accuracy, and facilitate effective communication with healthcare providers, fostering more equitable healthcare environments. This commentary explores these applications while addressing the ethical challenges and limitations associated with AI deployment. Specific challenges, such as algorithmic bias, privacy risks with patient data, and the complexity of designing inclusive technologies, are discussed to provide a balanced perspective. For example, biased diagnostic tools may lead to inequitable care, and privacy breaches can compromise sensitive data. Key areas of focus include personalised care through AI-powered systems, the design of inclusive AI technologies incorporating continuous feedback loops and partnerships with advocacy groups, and the development of AI-enabled robotics for physical assistance. This commentary paper emphasises the importance of addressing these limitations alongside advancing ethical AI practices and ensuring continuous user involvement to meet the diverse needs of people living with disabilities, ultimately promoting greater independence and participation in society. Consequently, while AI holds transformative potential in advancing equitable and inclusive healthcare for people with disabilities, addressing ethical challenges, overcoming limitations, and fostering user-centred design are essential to fully realise its benefits and ensure these innovations promote autonomy, accessibility, and well-being.",
      "journal": "Social science & medicine (1982)",
      "year": "2025",
      "doi": "10.1016/j.socscimed.2024.117560",
      "authors": "Olawade David Bamidele et al.",
      "keywords": "Artificial intelligence; Disability care; Ethical challenges; Healthcare accessibility; Inclusive design",
      "mesh_terms": "Humans; Artificial Intelligence; Persons with Disabilities; Delivery of Health Care; Self-Help Devices",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39612748/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39627045",
      "title": "Gender bias in text-to-image generative artificial intelligence depiction of Australian paramedics and first responders.",
      "abstract": "INTRODUCTION: In Australia, almost 50\u00a0% of paramedics are female yet they remain under-represented in stereotypical depictions of the profession. The potentially transformative value of generative artificial intelligence (AI) may be limited by stereotypical errors, misrepresentations and bias. Increasing use of text-to-image generative AI, like DALL-E 3, could reinforce gender and ethnicity biases and, therefore, is important to objectively evaluate. METHOD: In March 2024, DALL-E 3 was utilised via GPT-4 to generate a series of individual and group images of Australian paramedics, ambulance officers, police officers and firefighters. In total, 82 images were produced including 60 individual-character images, and 22 multiple-character group images. All 326 depicted characters were independently analysed by three reviewers for apparent gender, age, skin tone and ethnicity. RESULTS: Among first responders, 90.8\u00a0% (N\u00a0=\u00a0296) were depicted as male, 90.5\u00a0% (N\u00a0=\u00a0295) as Caucasian, 95.7\u00a0% (N\u00a0=\u00a0312) as a light skin tone, and 94.8\u00a0% (N\u00a0=\u00a0309) as under 55 years of age. For paramedics and police the gender distribution was a statistically significant variation from that of actual Australian workforce data (all p\u00a0<\u00a00.001). Among the images of individual paramedics and ambulance officers (N\u00a0=\u00a032), DALL-E 3 depicted 100\u00a0% as male, 100\u00a0% as Caucasian and 100\u00a0% with light skin tone. CONCLUSION: Gender and ethnicity bias is a significant limitation for text-to-image generative AI using DALL-E 3 among Australian first responders. Generated images have a disproportionately high misrepresentation of males, Caucasians and light skin tones that are not representative of the diversity of paramedics in Australia today.",
      "journal": "Australasian emergency care",
      "year": "2025",
      "doi": "10.1016/j.auec.2024.11.003",
      "authors": "Currie Geoffrey et al.",
      "keywords": "Diversity; First responder; Generative artificial intelligence; Inclusivity",
      "mesh_terms": "Humans; Australia; Male; Female; Allied Health Personnel; Adult; Artificial Intelligence; Sexism; Emergency Responders; Middle Aged; Generative Artificial Intelligence; Paramedics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39627045/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39653977",
      "title": "Machine Learning Reveals Demographic Disparities in Palliative Care Timing Among Patients With Traumatic Brain Injury Receiving Neurosurgical Consultation.",
      "abstract": "BACKGROUND: Timely palliative care (PC) consultations offer demonstrable benefits for patients with traumatic brain injury (TBI), yet their implementation remains inconsistent. This study employs machine learning methods to identify distinct patient phenotypes and elucidate the primary drivers of PC consultation timing variability in TBI management, aiming to uncover disparities and inform more equitable care strategies. METHODS: Data on admission, hospital course, and outcomes were collected for a cohort of 232 patients with TBI who received both PC consultations and neurosurgical consultations during the same hospitalization. Patient phenotypes were uncovered using principal component analysis and K-means clustering; time-to-PC consultation for each phenotype was subsequently compared by Kaplan-Meier analysis. An extreme gradient boosting model with Shapley Additive Explanations identified key factors influencing PC consultation timing. RESULTS: Three distinct patient clusters emerged: cluster A (n\u2009=\u200986), comprising older adult White women (median 87\u00a0years) with mild TBI, received the earliest PC consultations (median 2.5\u00a0days); cluster B (n\u2009=\u2009108), older adult White men (median 81\u00a0years) with mild TBI, experienced delayed PC consultations (median 5.0\u00a0days); and cluster C (n\u2009=\u200938), middle-aged (median: 46.5\u00a0years), severely injured, non-White patients, had the latest PC consultations (median 9.0\u00a0days). The clusters did not differ by discharge disposition (p\u2009=\u20090.4) or inpatient mortality (p\u2009>\u20090.9); however, Kaplan-Meier analysis revealed a significant difference in time-to-PC consultation (p\u2009<\u20090.001), despite no differences in time-to-mortality (p\u2009=\u20090.18). Shapley Additive Explanations analysis of the extreme gradient boosting model identified age, sex, and race as the most influential drivers of PC consultation timing. CONCLUSIONS: This study unveils crucial disparities in PC consultation timing for patients with TBI, primarily driven by demographic factors rather than clinical presentation or injury characteristics. The identification of distinct patient phenotypes and quantification of factors influencing PC consultation timing provide a foundation for developing for standardized protocols and decision support tools to ensure timely and equitable palliative care access for patients with TBI.",
      "journal": "Neurocritical care",
      "year": "2025",
      "doi": "10.1007/s12028-024-02172-2",
      "authors": "Aude Carlos A et al.",
      "keywords": "Age factors; Cluster analysis; Critical care; Decision support techniques; Health care disparities; Machine learning; Neurosurgery; Palliative care; Prognosis; Quality of health care; Race factors; Sex factors; Traumatic brain injury",
      "mesh_terms": "Humans; Brain Injuries, Traumatic; Male; Female; Palliative Care; Aged; Middle Aged; Healthcare Disparities; Aged, 80 and over; Machine Learning; Referral and Consultation; Adult; Time-to-Treatment; Neurosurgical Procedures",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39653977/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39666377",
      "title": "Language disparities in pandemic information: Autocomplete analysis of COVID-19 searches in New York.",
      "abstract": "Objective: To audit and compare search autocomplete results in Spanish and English during the early COVID-19 pandemic in the New York metropolitan area. The pandemic led to significant online search activity about the disease, its spread, and remedies. As gatekeepers, search engines like Google can influence public opinion. Autocomplete predictions help users complete searches faster but may also shape their views. Understanding these differences is crucial to identify biases and ensure equitable information dissemination. Methods: The study tracked autocomplete results daily for five COVID-19 related search terms in English and Spanish over 100+ days in 2020, yielding a total of 9164 autocomplete predictions. Results: Queries in Spanish yielded fewer autocomplete options and often included more negative content than English autocompletes. The topical coverage differed, with Spanish autocompletes including themes related to religion and spirituality that were absent in the English search autocompletes. Conclusion: The contrast in search autocomplete results could lead to divergent impressions about the pandemic and remedial actions among different sections of society. Continuous auditing of autocompletes by public health stakeholders and search engine organizations is recommended to reduce potential bias and misinformation.",
      "journal": "Health informatics journal",
      "year": "2024",
      "doi": "10.1177/14604582241307836",
      "authors": "Singh Vivek K et al.",
      "keywords": "COVID health information; algorithmic bias; health disparity; search autocompletes; search bias",
      "mesh_terms": "COVID-19; Humans; Language; Search Engine; Pandemics; New York; SARS-CoV-2; New York City; Information Seeking Behavior; Information Dissemination",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39666377/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39672853",
      "title": "Addressing oral cancer inequalities: a multifaceted approach to equitable healthcare.",
      "abstract": "Oral cancer remains a persistent health challenge globally, with rising incidence and flat survival rates, particularly among disadvantaged populations. This paper explores the socioeconomic, ethnic and cultural factors contributing to inequalities in oral cancer care, such as limited access to healthcare, lower education levels, financial constraints and systemic disadvantages based on ethnicity and cultural practices. Addressing these inequalities requires a multi-faceted approach, including community outreach, patient education and policy advocacy.Effective strategies include mobile clinics, free screening events, culturally sensitive educational materials, digital tools and social media outreach. Integrating telehealth services, artificial intelligence for early diagnosis, community-based participatory research and microfinance initiatives are also crucial. Furthermore, improving health literacy and promoting preventive behaviours are essential steps towards mitigating these inequalities.Public health education and social services must collaborate to enhance access to dental care, ensuring better outcomes across all populations, regardless of socioeconomic or ethnic background. This paper highlights the role of dental professionals in reducing inequalities and promoting oral health equity through innovative and comprehensive strategies.",
      "journal": "British dental journal",
      "year": "2024",
      "doi": "10.1038/s41415-024-8118-9",
      "authors": "Jerjes Waseem",
      "keywords": "",
      "mesh_terms": "Humans; Mouth Neoplasms; Healthcare Disparities; Health Services Accessibility; Socioeconomic Factors; Health Equity; Health Literacy",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39672853/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39689864",
      "title": "Building a Time-Series Model to Predict Hospitalization Risks in Home Health Care: Insights Into Development, Accuracy, and Fairness.",
      "abstract": "OBJECTIVES: Home health care (HHC) serves more than 5 million older adults annually in the United States, aiming to prevent unnecessary hospitalizations and emergency department (ED) visits. Despite efforts, up to 25% of patients in HHC experience these adverse events. The underutilization of clinical notes, aggregated data approaches, and potential demographic biases have limited previous HHC risk prediction models. This study aimed to develop a time-series risk model to predict hospitalizations and ED visits in patients in HHC, examine model performance over various prediction windows, identify top predictive variables and map them to data standards, and assess model fairness across demographic subgroups. SETTING AND PARTICIPANTS: A total of 27,222 HHC episodes between 2015 and\u00a02017. METHODS: The study used health care process modeling of electronic health records, including clinical notes processed with natural language processing techniques and Medicare claims data. A Light Gradient Boosting Machine algorithm was used to develop the risk prediction model, with performance evaluated using 5-fold cross-validation. Model fairness was assessed across gender, race/ethnicity, and socioeconomic subgroups. RESULTS: The model achieved high predictive performance, with an F1 score of 0.84 for a 5-day prediction window. Twenty top predictive variables were identified, including novel indicators such as the length of nurse-patient visits and visit frequency. Eighty-five percent of these variables mapped completely to the US Core Data for Interoperability standard. Fairness assessment revealed performance disparities across demographic and socioeconomic groups, with lower model effectiveness for more historically underserved populations. CONCLUSIONS AND IMPLICATIONS: This study developed a robust time-series risk model for predicting adverse events in patients in HHC, incorporating diverse data types and demonstrating high predictive accuracy. The findings highlight the importance of considering established and novel risk factors in HHC. Importantly, the observed performance disparities across subgroups emphasize the need for fairness adjustments to ensure equitable risk prediction across all patient populations.",
      "journal": "Journal of the American Medical Directors Association",
      "year": "2025",
      "doi": "10.1016/j.jamda.2024.105417",
      "authors": "Topaz Maxim et al.",
      "keywords": "Home health care service; model fairness; natural language processing; risk prediction",
      "mesh_terms": "Humans; Male; Female; Aged; Hospitalization; Home Care Services; United States; Risk Assessment; Aged, 80 and over; Emergency Service, Hospital; Electronic Health Records",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39689864/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39708960",
      "title": "Health System Purchasing Professionals' Approaches to Considering Equity in Procurement.",
      "abstract": "BACKGROUND: Continuing data on racial bias in pulse oximeters and artificial intelligence have sparked calls for health systems to drive innovation against racial bias in health care device and artificial intelligence markets by incorporating equity concerns explicitly into purchasing decisions. RESEARCH QUESTION: How do health care purchasing professionals integrate equity concerns into purchasing decision-making? STUDY DESIGN AND METHODS: Between August 2023 and March 2024, we conducted semistructured interviews via videoconferencing with health care purchasing professionals about purchasing processes for pulse oximeters and other devices-and whether and where equity concerns arise in decision-making. An abductive approach was used to analyze perspectives on how equity and disparity concerns currently are integrated into health care purchasing decision-making. Health care purchasing professionals (N\u00a0= 30) worked in varied supply chain roles for various health systems and supply chain support and consulting companies across the United States. RESULTS: Health care purchasing professionals described limited considerations of equity in current purchasing processes. They described some receptivity to diversity, equity, and inclusion initiatives, largely focused on diversifying suppliers rather than ensuring that devices and products functioned equitably. Respondents reported that they depended on clinician partners to raise and delineate requirements for equitable performance. Respondents also depicted current sources of evidence used in making purchasing decisions as providing limited information about equitable performance and that large contracts, including with group purchasing organizations, may limit purchasing options. INTERPRETATION: Health system purchasing professionals suggested interest and some nascent successes in diversity, equity, and inclusion considerations in health system purchasing processes, including diverse supplier initiatives, but also expressed a need for strong clinical partnership to ensure equitable performance. Explicit approaches for incorporating equitable performance into health care purchasing likely are needed.",
      "journal": "Chest",
      "year": "2025",
      "doi": "10.1016/j.chest.2024.12.016",
      "authors": "Hauschildt Katrina E et al.",
      "keywords": "biomedical technology assessment; diversity, equity, and inclusion; health equity; health policy; hospital purchasing",
      "mesh_terms": "Humans; United States; Health Personnel; Decision Making",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39708960/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39731446",
      "title": "De-biasing the bias: methods for improving disparity assessments with noisy group measurements.",
      "abstract": "Health care decisions are increasingly informed by clinical decision support algorithms, but these algorithms may perpetuate or increase racial and ethnic disparities in access to and quality of health care. Further complicating the problem, clinical data often have missing or poor quality racial and ethnic information, which can lead to misleading assessments of algorithmic bias. We present novel statistical methods that allow for the use of probabilities of racial/ethnic group membership in assessments of algorithm performance and quantify the statistical bias that results from error in these imputed group probabilities. We propose a sensitivity analysis approach to estimating the statistical bias that allows practitioners to assess disparities in algorithm performance under a range of assumed levels of group probability error. We also prove theoretical bounds on the statistical bias for a set of commonly used fairness metrics and describe real-world scenarios where our theoretical results are likely to apply. We present a case study using imputed race and ethnicity from the modified Bayesian Improved First and Surname Geocoding algorithm for estimation of disparities in a clinical decision support algorithm used to inform osteoporosis treatment. Our novel methods allow policymakers to understand the range of potential disparities under a given algorithm even when race and ethnicity information is missing and to make informed decisions regarding the implementation of machine learning for clinical decision support.",
      "journal": "Biometrics",
      "year": "2024",
      "doi": "10.1093/biomtc/ujae155",
      "authors": "Wastvedt Solvejg et al.",
      "keywords": "Bayesian improved surname geocoding; algorithmic fairness; race imputation; sensitivity analysis",
      "mesh_terms": "Humans; Algorithms; Bias; Bayes Theorem; Healthcare Disparities; Ethnicity; Osteoporosis; Racial Groups; Decision Support Systems, Clinical; Biometry; Models, Statistical",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39731446/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39738559",
      "title": "Generalizability, robustness, and correction bias of segmentations of thoracic organs at risk in CT images.",
      "abstract": "OBJECTIVE: This study aims to assess and compare two state-of-the-art deep learning approaches for segmenting four thoracic organs at risk\u00a0(OAR)-the esophagus, trachea, heart, and aorta-in CT images in the context of radiotherapy planning. MATERIALS AND METHODS: We compare a multi-organ segmentation approach and the fusion of multiple single-organ models, each dedicated to one OAR. All were trained using nnU-Net with the default parameters and the full-resolution configuration. We evaluate their robustness with adversarial perturbations, and their generalizability on external datasets, and explore potential biases introduced by expert corrections compared to fully manual delineations. RESULTS: The two approaches show excellent performance with an average Dice score of 0.928 for the multi-class setting and 0.930 when fusing the four single-organ models. The evaluation of external datasets and common procedural adversarial noise demonstrates the good generalizability of these models. In addition, expert corrections of both models show significant bias to the original automated segmentation. The average Dice score between the two corrections is 0.93, ranging from 0.88 for the trachea to 0.98 for the heart. CONCLUSION: Both approaches demonstrate excellent performance and generalizability in segmenting four thoracic OARs, potentially improving efficiency in radiotherapy planning. However, the multi-organ setting proves advantageous for its efficiency, requiring less training time and fewer resources, making it a preferable choice for this task. Moreover, corrections of AI segmentation by clinicians may lead to biases in the results of AI approaches. A test set, manually annotated, should be used to assess the performance of such methods. KEY POINTS: Question While manual delineation of thoracic organs at risk is labor-intensive, prone to errors, and time-consuming, evaluation of AI models performing this task lacks robustness. Findings The deep-learning model using the nnU-Net framework showed excellent performance, generalizability, and robustness in segmenting thoracic organs in CT, enhancing radiotherapy planning efficiency. Clinical relevance Automatic segmentation of thoracic organs at risk can save clinicians time without compromising the quality of the delineations, and extensive evaluation across diverse settings demonstrates the potential of integrating such models into clinical practice.",
      "journal": "European radiology",
      "year": "2025",
      "doi": "10.1007/s00330-024-11321-2",
      "authors": "Gu\u00e9rendel Corentin et al.",
      "keywords": "Computed tomography; Computer-assisted image analysis; Deep learning; Organs at risk; Thorax",
      "mesh_terms": "Humans; Organs at Risk; Tomography, X-Ray Computed; Deep Learning; Radiotherapy Planning, Computer-Assisted; Esophagus; Heart; Trachea; Radiography, Thoracic; Aorta; Thorax",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39738559/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39784101",
      "title": "Cognitive biases in osteopathic diagnosis: a mixed study among French osteopaths.",
      "abstract": "OBJECTIVES: Although cognitive biases are one of the most frequent causes of diagnostic errors, their influence remains underestimated in allied health professions, especially in osteopathy. Yet, a part of osteopathic clinical reasoning and diagnosis rely on the practitioner's intuition and subjective haptic perceptions. The aim of this study is to highlight links between the cognitive biases perceived by the practitioner to understand cognitive patterns during osteopathic diagnosis, and to suggest debiasing strategies. METHODS: A mixed method based on an explanatory sequential type is used. (QUAN\u2192QUAL). A quantitative cross-sectional survey of 272 French osteopaths and three focus groups including 24 osteopaths were carried out. The quantitative analysis includes multinominal logistic regression models and multiple correspondence analysis. The qualitative analysis is based on the framework method (within thematic analysis) and followed a step-by-step guide (Gale et\u00a0al.). RESULTS: Among 19 selected biases, osteopaths feel to be affected by 9.4\u00a0\u00b1\u00a00.28 biases (range [1-19], median=9). Some presumed biases would be associated, and socio-demographic (gender, age) and professional (experience and types of practice) factors would modify how practitioners perceive the presence of biases. Main debiasing solutions are supervision and transcultural clinical competences. CONCLUSIONS: Osteopaths believe their diagnosis is impaired by the presence of cognitive biases as observed in clinical reality. Some biases are shared with medical doctors, but others are more specific to osteopaths, such as confirmation bias. To reduce their effect, the practitioner needs to be aware of these cognitive patterns of clinical reasoning, understand the patient and himself better, and use objective tests.",
      "journal": "Diagnosis (Berlin, Germany)",
      "year": "2025",
      "doi": "10.1515/dx-2024-0144",
      "authors": "Siffert Cassandra et al.",
      "keywords": "clinical reasoning; cognitive bias; cognitive debiasing; decision-making; diagnostic errors; osteopathic diagnosis",
      "mesh_terms": "Humans; Female; Male; France; Cross-Sectional Studies; Osteopathic Physicians; Adult; Osteopathic Medicine; Middle Aged; Cognition; Attitude of Health Personnel; Focus Groups; Diagnostic Errors; Clinical Reasoning; Surveys and Questionnaires; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39784101/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39848457",
      "title": "Gender-Equity Model for Liver Allocation Using Artificial Intelligence (GEMA-AI) for Waiting List Liver Transplant Prioritization.",
      "abstract": "BACKGROUND & AIMS: We aimed to develop and validate an artificial intelligence score (gender-equity model for liver allocation using artificial intelligence [GEMA-AI]) to predict liver transplantation (LT) waiting list outcomes using the same input variables contained in existing models. METHODS: This was a cohort study including adult LT candidates enlisted in the United Kingdom (2010-2020) for model training and internal validation and in Australia (1998-2020) for external validation. GEMA-AI combined international normalized ratio, bilirubin, sodium, and the Royal Free Hospital glomerular filtration rate in an explainable artificial neural network. GEMA-AI was compared with gender-equity model for liver allocation corrected by serum sodium (GEMA-Na), Model for End-Stage Liver Disease 3.0, and Model for End-Stage Liver Disease corrected by serum sodium for waiting list prioritization. RESULTS: The study included 9320 patients: 5762 in the training cohort, 1920 in the internal validation cohort, and 1638 in the external validation cohort. The prevalence of 90-day mortality or delisting for sickness ranged from 5.3% to 6% across different cohorts. GEMA-AI showed better discrimination than GEMA-Na, Model for End-Stage Liver Disease corrected by serum sodium, and Model for End-Stage Liver Disease 3.0 in the internal and external validation cohorts, with a more pronounced benefit in women and in patients showing at least 1 extreme analytical value. Accounting for identical input variables, the transition from a linear to a nonlinear score (from GEMA-Na to GEMA-AI) resulted in a differential prioritization of 6.4% of patients within the first 90 days and would potentially save 1 in 59 deaths overall, and 1 in 13 deaths among women. Results did not substantially change when ascites was not included in the models. CONCLUSIONS: The use of explainable machine learning models may be preferred over conventional regression-based models for waiting list prioritization in LT. GEMA-AI made more accurate predictions of waiting list outcomes, particularly for the sickest patients.",
      "journal": "Clinical gastroenterology and hepatology : the official clinical practice journal of the American Gastroenterological Association",
      "year": "2025",
      "doi": "10.1016/j.cgh.2024.12.010",
      "authors": "G\u00f3mez-Orellana Antonio Manuel et al.",
      "keywords": "Artificial Neural Networks; Disparities; Explainable Artificial Intelligence; Gender; Liver Allocation; Machine Learning",
      "mesh_terms": "Humans; Waiting Lists; Liver Transplantation; Female; Male; Middle Aged; Artificial Intelligence; Adult; Cohort Studies; End Stage Liver Disease; United Kingdom; Australia; Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39848457/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39871015",
      "title": "Assessing online chat-based artificial intelligence models for weight loss recommendation appropriateness and bias in the presence of guideline incongruence.",
      "abstract": "BACKGROUND AND AIM: Managing obesity requires a comprehensive approach that involves therapeutic lifestyle changes, medications, or metabolic surgery. Many patients seek health information from online sources and artificial intelligence models like ChatGPT, Google Gemini, and Microsoft Copilot before consulting health professionals. This study aims to evaluate the appropriateness of the responses of Google Gemini and Microsoft Copilot to questions on pharmacologic and surgical management of obesity and assess for bias in their responses to either the ADA or AACE guidelines. METHODS: Ten questions were compiled into a set and posed separately to the free editions of Google Gemini and Microsoft Copilot. Recommendations for the questions were extracted from the ADA and the AACE websites, and the responses were graded by reviewers for appropriateness, completeness, and bias to any of the guidelines. RESULTS: All responses from Microsoft Copilot and 8/10 (80%) responses from Google Gemini were appropriate. There were no inappropriate responses. Google Gemini refused to respond to two questions and insisted on consulting a physician. Microsoft Copilot (10/10; 100%) provided a higher proportion of complete responses than Google Gemini (5/10; 50%). Of the eight responses from Google Gemini, none were biased towards any of the guidelines, while two of the responses from Microsoft Copilot were biased. CONCLUSION: The study highlights the role of Microsoft Copilot and Google Gemini in weight loss management. The differences in their responses may be attributed to the variation in the quality and scope of their training data and design.",
      "journal": "International journal of obesity (2005)",
      "year": "2025",
      "doi": "10.1038/s41366-025-01717-5",
      "authors": "Annor Eugene et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Obesity; Weight Loss; Internet; Practice Guidelines as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39871015/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39888636",
      "title": "Pursuing Equity With Artificial Intelligence in Health Care.",
      "abstract": "This Viewpoint discusses the pursuit of fairness and equity in artificial intelligence in health care to drive transformative changes and reduce health disparities.",
      "journal": "JAMA health forum",
      "year": "2025",
      "doi": "10.1001/jamahealthforum.2024.5031",
      "authors": "Johnson Kevin B et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39888636/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39925011",
      "title": "Is more data always better? On alternative policies to mitigate bias in Artificial Intelligence health systems.",
      "abstract": "The development and implementation of Artificial Intelligence (AI) health systems represent a great power that comes with great responsibility. Their capacity to improve and transform healthcare involves inevitable risks. A major risk in this regard is the propagation of bias throughout the life cycle of the AI system, leading to harmful or discriminatory outcomes. This paper argues that the European medical device regulations may prove inadequate to address this-not only technical but also social challenge. With the advent of new regulatory remedies, it seems that the European policymakers also want to reinforce the current medical device legal framework. In this paper, we analyse different policies to mitigate bias in AI health systems included in the Artificial Intelligence Act and in the proposed European Health Data Space. As we shall see, the different remedies based on processing sensitive data for such purpose devised by the European policymakers may have very different effects both on privacy and on protection against discrimination. We find the focus on mitigation during the pre-commercialisation stages rather weak, and believe that bias control once the system has been implemented in the real world would have merited greater ambition.",
      "journal": "Bioethics",
      "year": "2025",
      "doi": "10.1111/bioe.13398",
      "authors": "Lazcoz Guillermo et al.",
      "keywords": "Artificial Intelligence; Artificial Intelligence Act; bias; healthcare; medical device regulation",
      "mesh_terms": "Artificial Intelligence; Humans; Europe; Delivery of Health Care; Privacy; Bias; Health Policy; Confidentiality",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39925011/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39970491",
      "title": "Predicting cancer survival at different stages: Insights from fair and explainable machine learning approaches.",
      "abstract": "OBJECTIVES: While prior machine learning (ML) models for cancer survivability prediction often treated all cancer stages uniformly, cancer survivability prediction should involve understanding how different stages impact the outcomes. Additionally, the success of ML-powered cancer survival prediction models depends a lot on being fair and easy to understand, especially for different stages of cancer. This study addresses cancer survivability prediction using fair and explainable ML methods. METHODS: Focusing on bladder, breast, and prostate cancers using SEER Program data, we developed and validated fair and explainable ML strategies to train separate models for each stage. These computational strategies also advance the fairness and explainability of the ML models. RESULTS: The current work highlights the important role of ML fairness and explainability in stage-specific cancer survivability prediction, capturing and interpreting the associated factors influencing cancer survivability. CONCLUSIONS: This contribution advocates for integrating fairness and explainability in these ML models to ensure equitable, fair, interpretable, and transparent predictions, ultimately enhancing patient care and shared decision-making in cancer treatment.",
      "journal": "International journal of medical informatics",
      "year": "2025",
      "doi": "10.1016/j.ijmedinf.2025.105822",
      "authors": "Kamble Tejasvi Sanjay et al.",
      "keywords": "Cancer survivability; Machine learning explainability; Machine learning fairness",
      "mesh_terms": "Humans; Machine Learning; Male; Neoplasms; SEER Program; Female; Neoplasm Staging; Prostatic Neoplasms; Breast Neoplasms; Prognosis",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39970491/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39973012",
      "title": "Equity in Radiology Research Is Essential for Improving Diversity and Inclusivity in Clinical Data, Reducing Bias and Increasing Artificial Intelligence Accuracy.",
      "abstract": "",
      "journal": "Canadian Association of Radiologists journal = Journal l'Association canadienne des radiologistes",
      "year": "2025",
      "doi": "10.1177/08465371251322365",
      "authors": "Sharma Sonali et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39973012/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40039481",
      "title": "Understanding Bias in Multispectral Autofluorescence Lifetime Imaging: Are Models Sensitive to Oral Location?",
      "abstract": "While bias in artificial intelligence is gaining attention across applications, model fairness is especially concerning in medical applications because a person's health may depend on the model outcome. Sources of bias in medical applications include age, gender, race, and social history. However, in oral cancer diagnosis, the oral location may be a source of bias. Variability in performance based on the oral location has been reported but is not well understood. To help ensure that models perform equitably regardless of location, we design three experiments to study the effect of oral location on model performance. We show that multispectral autofluorescence images retain tissue-type characteristics, but that the tissue-specific information is degraded in lesion images. Furthermore, we show that the tissue-specific features are not disentangled from the disease-associated features. Our results show that automated diagnosis models need to be thoughtfully designed to remove bias from the oral location to ensure equitable performance. Based on these insights, we propose a tissue-specific fine-tuning approach that increases overall performance and lowers the fairness gap by over 5%.Clinical relevance- This paper explores sources of offtarget variance in multispectral autofluorescence images. By understanding sources of bias in multispectral autofluorescence images, fairer and more robust models for oral cancer diagnosis and margin delineation can be developed, leading to greater clinical acceptance and more equitable patient outcomes.",
      "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference",
      "year": "2024",
      "doi": "10.1109/EMBC53108.2024.10781827",
      "authors": "Caughlin Kayla et al.",
      "keywords": "",
      "mesh_terms": "Humans; Optical Imaging; Mouth Neoplasms; Mouth; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40039481/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40056233",
      "title": "Integration of AI into psychodermatology: concern for racial bias.",
      "abstract": "",
      "journal": "Archives of dermatological research",
      "year": "2025",
      "doi": "10.1007/s00403-025-04012-5",
      "authors": "Modanlo Nina et al.",
      "keywords": "Artificial intelligence; Bias; Dermatology; Psychodermatology; Race",
      "mesh_terms": "",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40056233/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40090759",
      "title": "Focus on Health Equity through Medical Education, Clinical Studies and Artificial Intelligence.",
      "abstract": "",
      "journal": "Journal of the National Medical Association",
      "year": "2025",
      "doi": "10.1016/j.jnma.2025.03.001",
      "authors": "Borum Marie L",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40090759/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40111818",
      "title": "Covariate-adjusted inference for doubly adaptive biased coin design.",
      "abstract": "Randomized controlled trials (RCTs) are pivotal for evaluating the efficacy of medical treatments and interventions, serving as a cornerstone in clinical research. In addition to randomization, achieving balances among multiple targets, such as statistical validity, efficiency, and ethical considerations, is also a central issue in RCTs. The doubly-adaptive biased coin design (DBCD) is notable for its high flexibility and efficiency in achieving any predetermined optimal allocation ratio and reducing variance for a given target allocation. However, DBCD does not account for abundant covariates that may be correlated with responses, which could further enhance trial efficiency. To address this limitation, this article explores the use of covariates in the analysis stage and evaluates the benefits of nonlinear covariate adjustment for estimating treatment effects. We propose a general framework to capture the intricate relationship between subjects' covariates and responses, supported by rigorous theoretical derivation and empirical validation via simulation study. Additionally, we introduce the use of sample splitting techniques for machine learning methods under DBCD, demonstrating the effectiveness of the corresponding estimators in high-dimensional cases. This paper aims to advance both the theoretical research and practical application of DBCD, thereby achieving more accurate and ethical clinical trials.",
      "journal": "Statistical methods in medical research",
      "year": "2025",
      "doi": "10.1177/09622802251324750",
      "authors": "Tu Fuyi et al.",
      "keywords": "Response-adaptive randomization; covariate adjustment; doubly-adaptive biased coin design; machine learning; sample splitting",
      "mesh_terms": "Humans; Randomized Controlled Trials as Topic; Research Design; Models, Statistical; Computer Simulation; Machine Learning; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40111818/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40112222",
      "title": "Cracking the Facade: Analyzing Ohio's \"Don't Say Gay\" Legislation as Disguised Discrimination Under the First and Fourteenth Amendments.",
      "abstract": "The Ohio State Legislature is among the growing nationwide trend in attacking LGBTQ+ rights. Chief among these is Ohio House Bill 8, which claims to limit the types of content children encounter in schools. While the drafters cite this noble intent, the bill's actual impact further harms queer students and teachers, who already bear heavier mental health burdens due to such legislation and its societal implications. This type of legislation recently originated in Florida, where it was signed into law by Governor Ron DeSantis in 2022 and garnered national media attention. As Ohio Governor Mike DeWine signed a near-identical bill in January 2025, the outcomes observed in Florida inform the constitutional analyses for the Ohio constituency. As in Florida, Ohio's bill is left intentionally vague, banning \"gender ideology\" and \"sexual concepts\" in classrooms or constraining them to what is deemed age-appropriate without providing sufficient guidelines for what may be acceptable. The disparate impact of this legislation is rooted entirely in gender classifications, triggering intermediate scrutiny. The bill's ambiguity creates a chilling effect on students' First Amendment rights by restricting the ability to express gender non-conformity without the school disclosing such changes to their families, disregarding the child's safety, and limiting the type of instruction children may receive in the classroom. Consequently, this compels schools to treat LGBTQ+ students and age-appropriate content differently from their heteronormative counterparts, inherently relegating those with queer identities as second-class citizens under the Fourteenth Amendment's Equal Protection and Substantive Due Process clauses.",
      "journal": "Journal of law and health",
      "year": "2025",
      "doi": "",
      "authors": "Porter Sydni L",
      "keywords": "",
      "mesh_terms": "Humans; Ohio; Sexual and Gender Minorities; Male; Civil Rights; Female; Schools; Child",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40112222/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40139775",
      "title": "False hope of a single generalisable AI sepsis prediction model: bias and proposed mitigation strategies for improving performance based on a retrospective multisite cohort study.",
      "abstract": "OBJECTIVE: To identify bias in using a single machine learning (ML) sepsis prediction model across multiple hospitals and care locations; evaluate the impact of six different bias mitigation strategies and propose a generic modelling approach for developing best-performing models. METHODS: We developed a baseline ML model to predict sepsis using retrospective data on patients in emergency departments (EDs) and wards across nine hospitals. We set model sensitivity at 70% and determined the number of alerts required to be evaluated (number needed to evaluate (NNE), 95%\u2009CI) for each case of true sepsis and the number of hours between the first alert and timestamped outcomes meeting sepsis-3 reference criteria (HTS3). Six bias mitigation models were compared with the baseline model for impact on NNE and HTS3. RESULTS: Across 969\u2009292 admissions, mean NNE for the baseline model was significantly lower for EDs (6.1 patients, 95% CI 6 to 6.2) than for wards (7.5 patients, 95% CI 7.4 to 7.5). Across all sites, median HTS3 was 20 hours (20-21) for wards vs 5 (5-5) for EDs. Bias mitigation models significantly impacted NNE but not HTS3. Compared with the baseline model, the best-performing models for NNE with reduced interhospital variance were those trained separately on data from ED patients or from ward patients across all sites. These models generated the lowest NNE results for all care locations in seven of nine hospitals. CONCLUSIONS: Implementing a single sepsis prediction model across all sites and care locations within multihospital systems may be unacceptable given large variances in NNE across multiple sites. Bias mitigation methods can identify models demonstrating improved performance across most sites in reducing alert burden but with no impact on the length of the prediction window.",
      "journal": "BMJ quality & safety",
      "year": "2025",
      "doi": "10.1136/bmjqs-2024-018328",
      "authors": "Schnetler Rudolf et al.",
      "keywords": "Decision support, computerized; Hospital medicine; Information technology; Patient Safety",
      "mesh_terms": "Humans; Sepsis; Retrospective Studies; Machine Learning; Emergency Service, Hospital; Bias; Female; Male; Middle Aged; Aged",
      "pub_types": "Journal Article; Multicenter Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40139775/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40155096",
      "title": "Announcing the Lancet Global Health Commission on artificial intelligence (AI) and HIV: leveraging AI for equitable and sustainable impact.",
      "abstract": "",
      "journal": "The Lancet. Global health",
      "year": "2025",
      "doi": "10.1016/S2214-109X(25)00049-X",
      "authors": "Reid Michael J A et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40155096/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40164561",
      "title": "Impact of AI on Access to Care a Three-Pronged Approach for Enhancing Access to Surgical Care: Lessons From Electrical Safety and the Impact of AI on Health Equity.",
      "abstract": "As we enter the 21st century, a new wave of transformation is occurring in health care, mainly through artificial intelligence (AI). Like electricity, AI is a powerful tool that can either harm or heal, depending on how it is managed. In 2015, the United Nations created the 2030 Agenda for Sustainable Development to achieve peace and prosperity for people and the planet. Goal Three of the agenda aims to \"Ensure healthy lives and promote well-being for all at all ages\" by improving access to health care, reducing disparities, and reducing mortality and morbidity for the world's population. Nowhere is this more critical than surgical care access, where AI can reduce disparities and improve outcomes for underserved populations-rural communities, special populations, and oncology patients-while posing risks if not adequately grounded in ethical and equitable practices. We introduce the three-pronged approach as a metaphorical framework to mitigate the risks and enhance AI's benefits in addressing access to surgical care. By focusing on data quality, continuous system evaluation, and ethical governance, we can ensure AI delivers equitable, effective, and safe health care outcomes for all, especially for the most vulnerable.",
      "journal": "The American surgeon",
      "year": "2025",
      "doi": "10.1177/00031348251329495",
      "authors": "Tetteh Hassan A et al.",
      "keywords": "resident education; robotic surgery; social and electronic media; socioeconomic; special topics",
      "mesh_terms": "Humans; Health Services Accessibility; Health Equity; Artificial Intelligence; Surgical Procedures, Operative; Healthcare Disparities",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40164561/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40173063",
      "title": "Neighbor-Guided Unbiased Framework for Generalized Category Discovery in Medical Image Classification.",
      "abstract": "Generalized category discovery (GCD) utilizes seen category knowledge to automatically discover new semantic categories that are not defined in the training phase. Nevertheless, there has been no research conducted on identifying new classes using medical images and disease categories, which is essential for understanding and diagnosing specific diseases. Moreover, existing methods still produce predictions that are biased towards seen categories since the model is mainly supervised by labeled seen categories, which in turn leads to sub-optimal clustering performance. In this paper, we propose a new neighbor-guided unbiased framework (NGUF) that leverages neighbor information to mitigate prediction bias to address the GCD problem in medical tasks. Specifically, we devise a neighbor-guided cross-pseudo-clustering strategy, which exploits the knowledge of the nearest-neighbor samples to adjust the model predictions thereby generating unbiased pseudo-clustering supervision. Then, based on the unbiased pseudo-clustering supervision, we use a view-invariant learning strategy to assign labels to all samples. In addition, we propose an adaptive weight learning strategy that dynamically determines the degree of adjustment of the predictions of different samples based on the distance density values. Finally, we further propose a cross-batch knowledge distillation module to utilize information from successive iterations to encourage training consistency. Extensive experiments on four medical image datasets show that NGUF is effective in mitigating the model's prediction bias and has superior performance to other state-of-the-art GCD algorithms. Our code will be released soon.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2025",
      "doi": "10.1109/JBHI.2025.3556984",
      "authors": "Feng Wei et al.",
      "keywords": "",
      "mesh_terms": "Humans; Algorithms; Cluster Analysis; Image Interpretation, Computer-Assisted; Machine Learning; Semantics; Image Processing, Computer-Assisted; Diagnostic Imaging",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40173063/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40184460",
      "title": "FairDiffusion: Enhancing equity in latent diffusion models via fair Bayesian perturbation.",
      "abstract": "Recent advancements in generative AI, particularly diffusion models, have proven valuable for text-to-image synthesis. In health care, these models offer immense potential in generating synthetic datasets and aiding medical training. Despite these strong performances, it remains uncertain whether the image generation quality is consistent across different demographic subgroups. To address this, we conduct a comprehensive analysis of fairness in medical text-to-image diffusion models. Evaluations of the Stable Diffusion model reveal substantial disparities across gender, race, and ethnicity. To reduce these biases, we propose FairDiffusion, an equity-aware latent diffusion model that improves both image quality and the semantic alignment of clinical features. In addition, we design and curate FairGenMed, a dataset tailored for fairness studies in medical generative models. FairDiffusion is further assessed on HAM10000 (dermatoscopic images) and CheXpert (chest x-rays), demonstrating its effectiveness in diverse medical imaging modalities. Together, FairDiffusion and FairGenMed advance research in fair generative learning, promoting equitable benefits of generative AI in health care.",
      "journal": "Science advances",
      "year": "2025",
      "doi": "10.1126/sciadv.ads4593",
      "authors": "Luo Yan et al.",
      "keywords": "",
      "mesh_terms": "Humans; Bayes Theorem; Datasets as Topic; Image Interpretation, Computer-Assisted; Bias; Generative Artificial Intelligence",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40184460/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40195448",
      "title": "Sociodemographic biases in medical decision making by large language models.",
      "abstract": "Large language models (LLMs) show promise in healthcare, but concerns remain that they may produce medically unjustified clinical care recommendations reflecting the influence of patients' sociodemographic characteristics. We evaluated nine LLMs, analyzing over 1.7 million model-generated outputs from 1,000 emergency department cases (500 real and 500 synthetic). Each case was presented in 32 variations (31 sociodemographic groups plus a control) while holding clinical details constant. Compared to both a physician-derived baseline and each model's own control case without sociodemographic identifiers, cases labeled as Black or unhoused or identifying as LGBTQIA+ were more frequently directed toward urgent care, invasive interventions or mental health evaluations. For example, certain cases labeled as being from LGBTQIA+ subgroups were recommended mental health assessments approximately six to seven times more often than clinically indicated. Similarly, cases labeled as having high-income status received significantly more recommendations (P\u2009<\u20090.001) for advanced imaging tests such as computed tomography and magnetic resonance imaging, while low- and middle-income-labeled cases were often limited to basic or no further testing. After applying multiple-hypothesis corrections, these key differences persisted. Their magnitude was not supported by clinical reasoning or guidelines, suggesting that they may reflect model-driven bias, which could eventually lead to health disparities rather than acceptable clinical variation. Our findings, observed in both proprietary and open-source models, underscore the need for robust bias evaluation and mitigation strategies to ensure that LLM-driven medical advice remains equitable and patient centered.",
      "journal": "Nature medicine",
      "year": "2025",
      "doi": "10.1038/s41591-025-03626-6",
      "authors": "Omar Mahmud et al.",
      "keywords": "",
      "mesh_terms": "Humans; Clinical Decision-Making; Male; Female; Language; Emergency Service, Hospital; Bias; Sociodemographic Factors; Adult; Middle Aged; Socioeconomic Factors; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40195448/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40267866",
      "title": "Assessing bias in AI-driven psychiatric recommendations: A comparative cross-sectional study of chatbot-classified and CANMAT 2023 guideline for adjunctive therapy in difficult-to-treat depression.",
      "abstract": "The integration of chatbots into psychiatry introduces a novel approach to support clinical decision-making, but biases in their recommendations pose significant concerns. This study investigates potential biases in chatbot-generated recommendations for adjunctive therapy in difficult-to-treat depression, comparing these outputs with the Canadian Network for Mood and Anxiety Treatments (CANMAT) 2023 guidelines. The analysis involved calculating Cohen's kappa coefficients to measure the overall level of agreement between chatbot-generated classifications and CANMAT guidelines. Differences between chatbot-generated and CANMAT classifications for each medication were assessed using the Wilcoxon signed-rank test. Results reveal substantial agreement for high-performing models, such as Google AI's Gemini 2.0 Flash, which achieved the highest Cohen's kappa value of 0.82 (SE = 0.052). In contrast, OpenAI's o1 model showed a lower agreement of 0.746 (SE = 0.057). Notable discrepancies were observed in the overestimation of medications such as quetiapine and lithium and the underestimation of modafinil and ketamine. Additionally, a distinct bias pattern was observed in OpenAI's chatbots, which demonstrated a tendency to over-recommend lithium and bupropion. Our study highlights both the promise and the challenges of employing AI tools in psychiatric practice, and advocates for multi-model approaches to mitigate bias and improve clinical reliability.",
      "journal": "Psychiatry research",
      "year": "2025",
      "doi": "10.1016/j.psychres.2025.116501",
      "authors": "Chang Yu et al.",
      "keywords": "Artificial intelligence; Depression; Evidence-based medicine; Generative artificial intelligence; Guideline",
      "mesh_terms": "Humans; Cross-Sectional Studies; Practice Guidelines as Topic; Artificial Intelligence; Depressive Disorder, Treatment-Resistant; Antidepressive Agents; Clinical Decision-Making; Canada; Psychiatry; Bias; Generative Artificial Intelligence",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40267866/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40269315",
      "title": "Evaluating and mitigating bias in AI-based medical text generation.",
      "abstract": "Artificial intelligence (AI) systems, particularly those based on deep learning models, have increasingly achieved expert-level performance in medical applications. However, there is growing concern that such AI systems may reflect and amplify human bias, reducing the quality of their performance in historically underserved populations. The fairness issue has attracted considerable research interest in the medical imaging classification field, yet it remains understudied in the text-generation domain. In this study, we investigate the fairness problem in text generation within the medical field and observe substantial performance discrepancies across different races, sexes and age groups, including intersectional groups, various model scales and different evaluation metrics. To mitigate this fairness issue, we propose an algorithm that selectively optimizes those underserved groups to reduce bias. Our evaluations across multiple backbones, datasets and modalities demonstrate that our proposed algorithm enhances fairness in text generation without compromising overall performance.",
      "journal": "Nature computational science",
      "year": "2025",
      "doi": "10.1038/s43588-025-00789-7",
      "authors": "Chen Xiuying et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Algorithms; Female; Male; Deep Learning; Bias; Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40269315/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40295097",
      "title": "AI interventions in cancer screening: balancing equity and cost-effectiveness.",
      "abstract": "This paper examines the integration of artificial intelligence (AI) into cancer screening programmes, focusing on the associated equity challenges and resource allocation implications. While AI technologies promise significant benefits-such as improved diagnostic accuracy, shorter waiting times, reduced reliance on radiographers, and overall productivity gains and cost-effectiveness-current interventions disproportionately favour those already engaged in screening. This neglect of non-attenders, who face the worst cancer outcomes, exacerbates existing health disparities and undermines the core objectives of screening programmes.Using breast cancer screening as a case study, we argue that AI interventions must not only improve health outcomes and demonstrate cost-effectiveness but also address inequities by prioritising non-attenders. To this end, we advocate for the design and implementation of cost-saving AI interventions. Such interventions could enable reinvestment into strategies specifically aimed at increasing engagement among non-attenders, thereby reducing disparities in cancer outcomes. Decision modelling is presented as a practical method to identify and evaluate these cost-saving interventions. Furthermore, the paper calls for greater transparency in decision-making, urging policymakers to explicitly account for the equity implications and opportunity costs associated with AI investments. Only then will they be able to balance the promise of technological innovation with the ethical imperative to improve health outcomes for all, particularly underserved populations. Methods such as distributional cost-effectiveness analysis are recommended to quantify and address disparities, ensuring more equitable healthcare delivery.",
      "journal": "Journal of medical ethics",
      "year": "2025",
      "doi": "10.1136/jme-2025-110707",
      "authors": "Roadevin Cristina et al.",
      "keywords": "Decision Making; Ethics; Policy; Resource Allocation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40295097/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40320624",
      "title": "SDoH Impact on Periodontal Disease Using Machine Learning and Dental Records.",
      "abstract": "The impact of social determinants of health (SDoH) on periodontal disease (PD) is critical to study, as a deeper understanding of SDoH offers significant potential to inform policy and help clinicians provide holistic patient care. The use of machine learning (ML) to analyze the association of SDoH with PD provides significant advantages over traditional statistical methods. While statistical approaches are effective for identifying trends, they often struggle with the complexity and unstructured nature of data from dental electronic health records (DEHRs). The objective of this study was to determine the association between PD and SDoH using big data through linked DEHR and census data using ML. We used the records of 89,937 unique patients (754,414 longitudinal records) from the Temple University School of Dentistry who received at least 1 treatment between 2007 and 2023. Patient PD outcomes were categorized based on progression, improvement, or no change, using longitudinal data spanning up to 16 y. We applied ML models, including logistic regression, Gaussian naive Bayes, random forest, and XGBoost, to identify SDoH predictors and their associations with PD. XGBoost demonstrated the best performance with 94% accuracy and high precision, recall, and F1 scores. SHapley Additive exPlanations (SHAP) values were used to provide explainable ML analysis. The leading predictors for PD progression were higher social vulnerability index, poverty, population density, fewer dental offices, more fast-food restaurants, longer travel times, higher stress levels, tobacco use, and multiple comorbidities. Our findings underscore the critical role of SDoH in PD progression and oral health inequity, advocating for the integration of these factors in PD risk assessment and management. This study also demonstrates the potential of big data analytics and ML in providing valuable insights for clinicians and researchers to study oral health disparities and promote equitable health outcomes.",
      "journal": "Journal of dental research",
      "year": "2025",
      "doi": "10.1177/00220345251328968",
      "authors": "Patel J et al.",
      "keywords": "artificial intelligence; deep learning/machine learning; dental informatics/bioinformatics; electronic dental records; inequalities; social determinants",
      "mesh_terms": "Humans; Machine Learning; Periodontal Diseases; Social Determinants of Health; Male; Female; Middle Aged; Electronic Health Records; Dental Records; Adult; Longitudinal Studies; Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40320624/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40379962",
      "title": "Computational challenges arising in algorithmic fairness and health equity with generative AI.",
      "abstract": "",
      "journal": "Nature computational science",
      "year": "2025",
      "doi": "10.1038/s43588-025-00806-9",
      "authors": "Suriyakumar Vinith M et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40379962/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40380414",
      "title": "Bias Detection in Histology Images Using Explainable AI and Image Darkness Assessment.",
      "abstract": "The study underscores the importance of addressing biases in medical AI models to improve fairness, generalizability, and clinical utility. In this paper, we present a novel framework that combines Explainable AI (XAI) with image darkness assessment to detect and mitigate bias in cervical histology image classification. Four deep learning architectures were employed-AlexNet, ResNet-50, EfficientNet-B0, and DenseNet-121-with EfficientNet-B0 demonstrating the highest accuracy post-mitigation. Grad-CAM and saliency maps were used to identify biases in the models' predictions. After applying brightness normalisation and synthetic data augmentation, the models shifted focus toward clinically relevant features, improving both accuracy and fairness. Statistical analysis using ANOVA confirmed a reduction in the influence of image darkness on model predictions after mitigation, as evidenced by a decrease in the F-statistic from 120.79 to 14.05, indicating improved alignment of the models with clinically relevant features.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI250302",
      "authors": "Skarga-Bandurova Inna et al.",
      "keywords": "Explainable AI (XAI); bias detection; histopathology; image darkness",
      "mesh_terms": "Humans; Deep Learning; Female; Bias; Uterine Cervical Neoplasms; Artificial Intelligence; Image Interpretation, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40380414/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40380757",
      "title": "Achieving SDoH Resource Equity in PICU Using an AI-Enabled Patient Navigator.",
      "abstract": "Trauma care coordination in the pediatric intensive care unit (PICU), including personalization of resources based on social determinants of health (SDoH), is challenging for already strained healthcare providers. Patient SDoH data collection is inconsistent, fragmented, and lacks standardization during hospitalization, complicating resource allocation and usability for both patients and providers. We propose implementing AI tools to standardize SDoH data collection and enhance resource navigation for patients and providers during and after trauma hospitalization. We will use the RE-AIM Framework to initially evaluate the reach and effectiveness of the individual tools and overall process, including user-friendliness and the percentage of resource allocation and utilization.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI250654",
      "authors": "West Alina N et al.",
      "keywords": "Large-Language Models; Pediatrics; Social Determinants of Health",
      "mesh_terms": "Intensive Care Units, Pediatric; Humans; Social Determinants of Health; Artificial Intelligence; Child; Health Equity; Resource Allocation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40380757/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40424948",
      "title": "Automated motor-leg scoring in stroke via a stable graph causality debiasing model.",
      "abstract": "Difficulty in resisting gravity is a common leg motor impairment in stroke patients, significantly impacting daily life. Automated clinical-level quantification of motor-leg videos based on the National Institutes of Health Stroke Scale is crucial for consistent and timely stroke diagnosis and assessment. However, real-world applications are challenged by interference impacting motion representation and decision-making, leading to performance instability. To address this, we propose a causality debiasing graph convolutional network. This model systematically reduces interference in both motor and non-motor body parts, extracting causal representations from human skeletons to ensure reliable decision-making. Specifically, an intra-class causality enhancement module is first proposed to resolve instability in motor-leg representations. This involves separating skeletal graphs with the same score, generating unbiased samples with similar discriminative features, and improving causal consistency. Subsequently, an inter-class non-causality suppression module is designed to handle biases in non-motor body parts. By decoupling skeletal graphs with different scores, this module constructs biased samples and enhances decision stability despite non-causal factors. Extensive validation on the clinical video dataset highlights the strong performance of our method for motor-leg scoring, achieving an impressive correlation above 0.82 with clinical scores, while independent testing at two additional hospitals further reinforces its stability. Furthermore, performance on another motor-arm scoring task and an additional Parkinsonian gait assessment task also successfully confirmed the method's reliability. Even when faced with potential real-world interferences, our approach consistently shows substantial value, offering both clinical significance and credibility. In summary, this work provides new insights for daily stroke assessment and telemedicine, with significant potential for widespread clinical adoption.",
      "journal": "Medical image analysis",
      "year": "2025",
      "doi": "10.1016/j.media.2025.103643",
      "authors": "Guo Rui et al.",
      "keywords": "Causality debiasing; Graph convolutional network; Stroke; Video-based assessment",
      "mesh_terms": "Humans; Stroke; Video Recording; Leg; Image Interpretation, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40424948/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40436299",
      "title": "Balancing Novelty and Pragmatism: Integrating Gender-Equity Model for Liver Allocation Using Artificial Intelligence (GEMA-AI) Into Clinical Workflows.",
      "abstract": "",
      "journal": "Clinical gastroenterology and hepatology : the official clinical practice journal of the American Gastroenterological Association",
      "year": "2026",
      "doi": "10.1016/j.cgh.2025.02.034",
      "authors": "Tu Xinzhuo et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40436299/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40440484",
      "title": "Impact of analytical bias on machine learning models for sepsis prediction using laboratory data.",
      "abstract": "OBJECTIVES: Machine learning (ML) models, using laboratory data, support early sepsis prediction. However, analytical bias in laboratory measurements can compromise their performance and validity in real-world settings. We aimed to evaluate how analytically acceptable bias may affect the validity and generalizability of ML models trained on laboratory\u00a0data. METHODS: A support vector machine model (SVM) for sepsis prediction was developed using complete blood count and erythrocyte sedimentation rate data from outpatients (CS, n=104) and patients from acute inflammatory status wards (SS, n=107). Twenty-six combinations were derived by white blood cells (WBC), platelets (PLT), and erythrocyte sedimentation rate (ESR) biases from analytical performance specifications (APS). The diagnostic performances of the 26 conditions tested were compared to the original dataset. RESULTS: SVM performance of the original dataset was AUC 90.6\u202f% [95\u202f%CI: 80.6-98.7\u202f%]. Minimum, desirable and optimum acceptable biases for WBC were 7.7\u202f, 5.1 and 2.6\u202f%, respectively, for PLT were 6.7\u202f, 4.5 and 2.2\u202f%, respectively and for ESR were 31.6\u202f, 21.1 and 10.5\u202f%, respectively. Across all conditions, AUC varied from 89.8\u202f% [95\u202f%CI: 79.0-97.7\u202f%] (for PLT bias\u00a0-6.7\u202f%), to 89.5\u202f% [95\u202f%CI: 79.1-98.0\u202f%] (for ESR Bias\u00a0+31.6\u202f%) to 90.4\u202f% [95\u202f%CI: 79.3-98.4\u202f%] (for WBC Bias\u00a0-5.1\u202f%). Using a combination of biases, the lowest AUC was 87.8\u202f% [95\u202f%CI: 75.9-96.6\u202f%]. No statistically significant differences were observed for AUC (p>0.05). CONCLUSIONS: Bias can influence model performance depending on the parameters and their combinations. Developing new validation strategies to assess the impact of analytical bias on laboratory data in ML models could improve their reliability.",
      "journal": "Clinical chemistry and laboratory medicine",
      "year": "2025",
      "doi": "10.1515/cclm-2025-0491",
      "authors": "Yesil Meryem Rumeysa et al.",
      "keywords": "analytical bias; artificial intelligence; machine learning; model performance; sepsis",
      "mesh_terms": "Humans; Sepsis; Machine Learning; Blood Sedimentation; Female; Male; Support Vector Machine; Middle Aged; Aged; Leukocyte Count; Bias; Blood Cell Count",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40440484/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40472465",
      "title": "Mitigating medical dataset bias by learning adaptive agreement from a biased council.",
      "abstract": "Dataset bias in images is an important yet less explored topic in medical images. Deep learning could be prone to learning spurious correlation raised by dataset bias, resulting in inaccurate, unreliable, and unfair models, which impedes its adoption in real-world clinical applications. Despite its significance, there is a dearth of research in the medical image classification domain to address dataset bias. Furthermore, the bias labels are often agnostic, as identifying biases can be laborious and depend on post-hoc interpretation. This paper proposes learning Adaptive Agreement from a Biased Council (Ada-ABC), a debiasing framework that does not rely on explicit bias labels to tackle dataset bias in medical images. Ada-ABC develops a biased council consisting of multiple classifiers optimized with generalized cross entropy loss to learn the dataset bias. A debiasing model is then simultaneously trained under the guidance of the biased council. Specifically, the debiasing model is required to learn adaptive agreement with the biased council by agreeing on the correctly predicted samples and disagreeing on the wrongly predicted samples by the biased council. In this way, the debiasing model could learn the target attribute on the samples without spurious correlations while also avoiding ignoring the rich information in samples with spurious correlations. We theoretically demonstrated that the debiasing model could learn the target features when the biased model successfully captures dataset bias. Moreover, we constructed the first medical debiasing benchmark focusing on addressing spurious correlation from four datasets containing seven different bias scenarios. Our extensive experiments practically showed that our proposed Ada-ABC outperformed competitive approaches, verifying its effectiveness in mitigating dataset bias for medical image classification. The codes and organized benchmark datasets can be accessed via https://github.com/LLYXC/Ada-ABC.",
      "journal": "Medical image analysis",
      "year": "2025",
      "doi": "10.1016/j.media.2025.103629",
      "authors": "Luo Luyang et al.",
      "keywords": "Dataset bias; Shortcut learning; Trustworthy artificial intelligence",
      "mesh_terms": "Humans; Deep Learning; Bias; Datasets as Topic; Algorithms; Image Interpretation, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40472465/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40479961",
      "title": "Multi-instance curriculum learning for histopathology image classification with bias reduction.",
      "abstract": "Multi-instance learning (MIL) exhibits advanced and surpassed capabilities in understanding and recognizing complex patterns within gigapixel histopathological images. However, current MIL methods for the analysis of the histopathological images still give rise to two main concerns. On one hand, vanilla MIL methods intuitively focus on identifying salient instances (easy-to-classify instances) without considering hard-to-classify instances, which is biased and prone to produce false positive instances. On the other hand, since the positive tissue occupies only a small fraction of histopathological images, it is commonly suffer from class imbalance between positive and negative instances, causing the MIL model to overly focus on the majority class in training instances classifier. In light of these issues of bias learning, we propose a multi-instance curriculum learning method that collaboratively incorporates hard negative instance mining and positive instance augmentation to improve classification performance of the model. Specifically, we first initialize the MIL model using easy-to-classify instances, then we mine the hard negative instances (hard-to-classify instances) and augment the positive instances via the diffusion model. Finally, the MIL model is retrained with memory rehearsal method by combining the mined negative instances and the augmented positive instances. Technically, the diffusion model is first designed to generate lesion instances, which optimally augment diverse features to reflect realistic positive samples with post screening scenario. Extensive experimental results show that the proposed method alleviates model bias in MIL and improves model interpretability.",
      "journal": "Medical image analysis",
      "year": "2025",
      "doi": "10.1016/j.media.2025.103647",
      "authors": "Mi Zihao et al.",
      "keywords": "Curriculum learning; Diffusion model; Hard negative instance mining; Histopathology image; Multi-instance learning; Positive instance augmentation",
      "mesh_terms": "Humans; Image Interpretation, Computer-Assisted; Machine Learning; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40479961/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40550353",
      "title": "Dental services use prediction among adults in Southern Brazil: A gender and racial fairness-oriented machine learning approach.",
      "abstract": "OBJECTIVE: To develop machine learning models to predict the use of dental services among adults aged 18 and older. METHODS: This is a prospective cohort study that uses data from the survey \"EAI Pelotas?\". The sample consisted of individuals who participated in both the baseline and follow-up, totaling 3461 people. Predictors were collected as baseline and comprised 47 sociodemographic, behavioral, oral and general health characteristics. The outcome was dental service use in the last year assessed during the one-year follow-up. Data was divided into training (80 %) and test (20 %) sets. Five machine learning models were tested. Hyperparameter tuning was optimized through 10-fold cross-validation, utilizing 30 iterations. Model performance was assessed based on the area under the Receiver Operating Characteristic (ROC) curve (AUC), accuracy, recall, precision, and F1-score. RESULTS: The prevalence of dental service use in the follow-up was 47.2 % (95 % CI, 45.5 - 48.9). All models in the test set demonstrated an AUC-ROC between 0.76 and 0.77. The CatBoost Classifier model exhibited the highest performance in the test dataset among the models concerning the AUC metric (AUC = 0.77, CI95 %,[0.73-0.80]), displaying an accuracy = 0.69, recall = 0.69, precision = 0.68, and F1-score = 0.69. Fairness estimations for the best model indicated consistent performance across gender categories. However, disparities were observed among racial groups, AUC = 0.57 for individuals who self-reported mixed (\"pardos\") skin color. The explainability analysis shows that the most important features were the last dental visit at baseline and education level. CONCLUSION: Despite our findings suggesting a sufficient prediction of overall dental services' use, performance varied across racial groups. CLINICAL SIGNIFICANCE: Our findings highlight the potential of machine learning models to predict dental service use with good overall accuracy. However, the significantly lower performance for mixed-race individuals raises concerns about fairness and equity. Therefore, despite promising results, the model requires further refinement before it can be applied in real-world public health settings.",
      "journal": "Journal of dentistry",
      "year": "2025",
      "doi": "10.1016/j.jdent.2025.105929",
      "authors": "Chisini Luiz Alexandre et al.",
      "keywords": "Artificial intelligence; Dental health services; Longitudinal study; Machine learning; Oral health",
      "mesh_terms": "Humans; Adult; Female; Male; Machine Learning; Prospective Studies; Middle Aged; Brazil; Sex Factors; Young Adult; Dental Health Services; Adolescent; Aged; Dental Care; ROC Curve",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40550353/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40578092",
      "title": "Evaluation of nursing students' ethical decision-making biases and attitudes toward artificial intelligence in nursing education.",
      "abstract": "AIM: This study examines nursing students' attitudes toward artificial intelligence (AI) and their association with biases in ethical decision-making processes. BACKGROUND: AI technologies are central to healthcare, particularly clinical decision support systems and simulations. While AI accelerates decision-making processes, it also brings ethical responsibilities. DESIGN: This is a descriptive cross-sectional study. METHOD: 265 nursing students were selected through stratified sampling from two universities in Turkey. Data were collected via an online survey using a demographic information form, the AI Attitude Scale (GAAIS) and the Ethical Decision-Making Bias Scale (EDBS). RESULTS: Most participants were female (n\u202f=\u202f223), with an average age of 20.45 (SD 1.67) years. The results of the GAAIS revealed that students generally had a positive attitude (3.38\u202fSD 0.47). However, 36.2\u202f% expressed distrust toward AI students who trusted AI more successfully to solve ethical issues and used AI tools more effectively. The average score on the EDBS was 2.48\u202fSD 0.41. Additionally, students who encountered ethical decisions more frequently (2.30\u202fSD 0.32) showed lower bias levels than those who experienced them less often (2.50SD0.44). Positive attitudes toward AI were positively associated with students' confidence in ethical decision-making (p\u202f<\u202f0.05). Distrust in AI and difficulty accessing accurate information were identified as significant barriers. CONCLUSIONS: Attitudes toward AI significantly influence students' biases in ethical decision-making processes. The nursing curriculum should include AI ethics, critical thinking and decision-making skills. Integrating ethical decision-making in AI usage within nursing education can ensure that future nurses can provide patient-centered care while maintaining ethical values.",
      "journal": "Nurse education in practice",
      "year": "2025",
      "doi": "10.1016/j.nepr.2025.104432",
      "authors": "Sengul Tuba et al.",
      "keywords": "Artificial Intelligence; Bias; Chat-GPT; Ethical Decision-Making; Nursing Education; Nursing Students",
      "mesh_terms": "Humans; Students, Nursing; Cross-Sectional Studies; Female; Turkey; Artificial Intelligence; Male; Surveys and Questionnaires; Decision Making; Attitude of Health Personnel; Young Adult; Education, Nursing, Baccalaureate; Adult; Ethics, Nursing",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40578092/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40596800",
      "title": "Toward equitable documentation: Evaluating ChatGPT's role in identifying and rephrasing stigmatizing language in electronic health records.",
      "abstract": "Stigmatizing language in electronic health records (EHRs) harms clinician and patient relationships, reinforcing health disparities. To assess ChatGPT's ability to reduce stigmatizing language in clinical notes. We analyzed 140 clinical notes and 150 stigmatizing examples from 2 urban hospitals. ChatGPT-4 identified and rephrased stigmatizing language. Identification performance was evaluated using precision, recall, and F1 score, with human expert annotations as the gold standard. Rephrasing quality was rated by experts on a three-point Likert scale for de-stigmatization, faithfulness, conciseness, and clarity. ChatGPT showed poor overall identification (micro-F1\u00a0=\u00a00.51) but moderate-to-high performance across individual stigmatizing language categories (micro-F1\u00a0=\u00a00.69-0.91). Rephrasing scored 2.7 for de-stigmatization, 2.8 for faithfulness, and 3.0 for conciseness and clarity. Prompt design significantly affected ChatGPT's performance. While ChatGPT has limitations in automatic identification, it can be used to support real-time identification and rephrasing stigmatizing language in EHRs with appropriate prompt design and human oversight.",
      "journal": "Nursing outlook",
      "year": "2025",
      "doi": "10.1016/j.outlook.2025.102472",
      "authors": "Zhang Zhihong et al.",
      "keywords": "ChatGPT; De-stigmatization; Large language model; Stigmatizing language",
      "mesh_terms": "Humans; Electronic Health Records; Documentation; Language; Stereotyping; Female; Male; Adult; Social Stigma; Middle Aged; Generative Artificial Intelligence",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40596800/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40628581",
      "title": "Ethnoracial Disparities in Advance Care Planning Among People With Alzheimer's Disease and Related Dementias.",
      "abstract": "OBJECTIVES: This study aims to investigate racial/ethnic variations in advance care planning (ACP) among people with Alzheimer's disease and related dementias (ADRD) and identify race/ethnicity-specific correlates of ACP. METHODS: The study used data from four waves of the Health and Retirement Study (HRS, 2012-2018). We included 5,420 observations with dementia, which was estimated using the machine-learning based Gianattasio-Power algorithm. Five types of engagement in ACP were measured: durable power of attorney, living will, future treatment discussion, limiting medical treatment, and number of ACP engaged. Besides ethnoracial identity, potential correlates of ACP were selected based on the literature. Regression analyses with subgroup analyses by race/ethnicity were performed. RESULTS: Ethnoracial identity was significantly associated with the likelihood of ACP engagement. The association between ethnoracial identity and the number of ACP engagement was significant after adjusting for covariates. Non-Hispanic Black (Risk Ratio [RR] = 0.670, 95% Confidence Interval [CI] = [0.607, 0.740]) and Hispanic (RR= 0.597, 95% CI = [0.518, 0.688]) individuals with ADRD engaged in fewer ACP than non-Hispanic White counterparts. Factors such as gender, marital status, household wealth and income, number of ADL difficulty, number of health conditions, self-rated health, and nursing home residency were differentially associated with the number of ACP engagement among three ethnoracial groups. CONCLUSIONS: The prevalence of engagement in ACP varies across ethnoracial groups. Non-Hispanic Black and Hispanic individuals are less likely to engage in various aspects of ACP than their non-Hispanic white counterparts. Race/ethnicity-specific correlates of ACP should be considered to develop equitable strategies that promote ACP among diverse populations.",
      "journal": "The American journal of geriatric psychiatry : official journal of the American Association for Geriatric Psychiatry",
      "year": "2025",
      "doi": "10.1016/j.jagp.2025.06.006",
      "authors": "Amano Takashi et al.",
      "keywords": "Advance care planning (ACP); Alzheimer\u2019s disease and related dementias (ADRD); ethnoracial disparities; health and retirement study (HRS)",
      "mesh_terms": "Aged; Aged, 80 and over; Female; Humans; Male; Middle Aged; Advance Care Planning; Alzheimer Disease; Black or African American; Dementia; Healthcare Disparities; Hispanic or Latino; United States; White",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40628581/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40663686",
      "title": "Debiasing Medical Knowledge for Prompting Universal Model in CT Image Segmentation.",
      "abstract": "With the assistance of large language models, which offer universal medical prior knowledge via text prompts, state-of-the-art Universal Models (UM) have demonstrated considerable potential in the field of medical image segmentation. Semantically detailed text prompts, on the one hand, indicate comprehensive knowledge; on the other hand, they bring biases that may not be applicable to specific cases involving heterogeneous organs or rare cancers. To this end, we propose a Debiased Universal Model (DUM) to consider instance-level context information and remove knowledge biases in text prompts from the causal perspective. We are the first to discover and mitigate the bias introduced by universal knowledge. Specifically, we propose to extract organ-level text prompts via language models and instance-level context prompts from the visual features of each image. We aim to highlight more on factual instance-level information and mitigate organ-level's knowledge bias. This process can be derived and theoretically supported by a causal graph, and instantiated by designing a standard UM (SUM) and a biased UM. The debiased output is finally obtained by subtracting the likelihood distribution output by biased UM from that of the SUM. Experiments on three large-scale multi-center external datasets and MSD internal tumor datasets show that our method enhances the model's generalization ability in handling diverse medical scenarios and reducing the potential biases, even with an improvement of 4.16% compared with popular universal model on the AbdomenAtlas dataset, showing the strong generalizability. The code is publicly available at https://github.com/DeepMed-Lab-ECNU/DUM.",
      "journal": "IEEE transactions on medical imaging",
      "year": "2025",
      "doi": "10.1109/TMI.2025.3589399",
      "authors": "Yun Boxiang et al.",
      "keywords": "",
      "mesh_terms": "Humans; Tomography, X-Ray Computed; Image Processing, Computer-Assisted; Algorithms; Natural Language Processing",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40663686/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40668069",
      "title": "AI-generated dermatologic images show deficient skin tone diversity and poor diagnostic accuracy: An experimental study.",
      "abstract": "BACKGROUND: Generative AI models are increasingly used in dermatology, yet biases in training datasets may reduce diagnostic accuracy and perpetuate ethnic health disparities. OBJECTIVES: To evaluate two key AI outputs: (1) skin tone representation and (2) diagnostic accuracy of generated dermatologic conditions. METHODS: Using the standard prompt 'Generate a photo of a person with [skin condition],' this cross-sectional study investigated skin tone diversity and accuracy of four leading AI models-Adobe Firefly, ChatGPT-4o, Midjourney and Stable Diffusion-across the 20 most common skin conditions. All images (n\u2009=\u20094000) were evaluated for skin tone representation from June to July 2024. Two independent raters used the Fitzpatrick scale to assess skin tone diversity compared to U.S. Census demographics using \u03c72. Two blinded dermatology residents evaluated a randomized 200-image subset for diagnostic accuracy. An inter-rater kappa statistic was calculated to assess rater agreement. RESULTS: Across all generated images, 89.8% depicted light skin, and 10.2% depicted dark skin. Adobe Firefly demonstrated the highest alignment with U.S. demographic data, with a non-significant chi-square result (38.1% dark skin, \u03c72(1)\u2009=\u20090.320, p\u2009=\u20090.572), indicating no meaningful difference between its generated skin tone diversity and census demographics. ChatGPT-4o, Midjourney and Stable Diffusion significantly underrepresented dark skin with Fitzpatrick scores of >IV (6.0%, 3.9% and 8.7% dark skin, respectively; all p\u2009<\u20090.001). Across all platforms, only 15% of images were identifiable by raters as the intended condition. Adobe Firefly had the lowest accuracy (0.94%), while ChatGPT-4o, Midjourney and Stable Diffusion demonstrated higher but still suboptimal accuracy (22%, 12.2% and 22.5%, respectively). CONCLUSIONS: The study highlights substantial deficiencies in the diversity and accuracy of AI-generated dermatological images. AI programs may exacerbate cognitive bias and health inequity, suggesting the need for ethical AI guidelines and diverse datasets to improve disease diagnosis and dermatologic care.",
      "journal": "Journal of the European Academy of Dermatology and Venereology : JEADV",
      "year": "2025",
      "doi": "10.1111/jdv.20849",
      "authors": "Joerg Lucie et al.",
      "keywords": "artificial intelligence; dermatology; diagnostic accuracy; racial disparities; representation; skin of color",
      "mesh_terms": "Humans; Cross-Sectional Studies; Skin Diseases; Artificial Intelligence; Skin Pigmentation; Female; Male; Photography; Dermatology",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40668069/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40675093",
      "title": "Comparison of synthetic LGE with optimal inversion time vs. conventional LGE via representation learning: Quantification of Bias in Population Analysis.",
      "abstract": "PURPOSE: Late Gadolinium Enhancement (LGE) images are crucial elements of CMR protocols for evaluating myocardial infarct (MI) severity and size. However, these images rely on signal intensity changes and manual inversion time (TI) settings, leading to suboptimal lesion/remote contrast in many cases. Here, we propose an original approach to evaluate the impact of suboptimal TI on the retrospective analysis of ST-elevation MI (STEMI) patients, using a representation learning methodology tailored to consider infarct- and image-based characteristics across the studied population. METHODS: We analyzed 133 pairs of conventional and synthetic LGE short-axis images from the HIBISCUS-STEMI cohort (ClinicalTrials ID: NCT03070496). Optimal TI was identified among co-registered synthetic LGE images, using a mixture of the Mann-Whitney U-test, standard deviation, and saturation of pixel values, while the TI used for conventional LGE image generation was extracted from the DICOM header. Images were realigned to a reference for pixel-wise inter-subject comparisons. Population analysis relied on Attribute-based Regularized Variational Autoencoders which provide a latent representation of the population that is both easier to analyze (lower dimensionality) and ordered by infarct-relevant attributes. RESULTS: Despite visual quality control in the clinic, our study demonstrates that nearly 50% of conventional LGE slices may include a suboptimal TI setting, mostly related to TI settings shorter than the optimal TI determined from synthetic LGE. Additionally, our findings showed that when isolating contrast effects and suboptimal TI settings, contrast had a minimal impact on infarct lesion metrics such as infarct size or transmurality in the latent space. This suggests that other factors than contrast setting are leading (for both cases) to systematic and proportional bias (p<0.05) and loss of precision (respectively \u03c1=0.42 and \u03c1=0.43) in the latent space. CONCLUSION: Suboptimal TI undermines the analysis of infarct patterns in populations. Representation learning is a powerful method to retro-analyze cohorts, enabling the identification of imperfect settings, a crucial step for accurately characterizing representative patterns of a population. Our strategy can be considered a promising candidate for monitoring longitudinal changes and evaluating therapy outcomes on broader populations.",
      "journal": "Computers in biology and medicine",
      "year": "2025",
      "doi": "10.1016/j.compbiomed.2025.110643",
      "authors": "Deleat-Besson Romain et al.",
      "keywords": "Cardiac magnetic resonance; Dimensionality reduction; Late gadolinium enhancement; Myocardial infarction; Representation learning",
      "mesh_terms": "Humans; Male; Female; Middle Aged; Gadolinium; Retrospective Studies; Aged; Myocardial Infarction; Contrast Media; Magnetic Resonance Imaging; Image Processing, Computer-Assisted; ST Elevation Myocardial Infarction; Machine Learning",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40675093/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40680485",
      "title": "Toward fair medical advice: Addressing and mitigating bias in large language model-based healthcare applications.",
      "abstract": "Large Language Models (LLMs) are increasingly deployed in web-based medical advice applications, offering scalable and accessible healthcare solutions. However, their outputs often reflect demographic biases, raising concerns about fairness and equity for vulnerable populations. In this work, we propose FairMed, a framework designed to mitigate biases in LLM-generated medical advice through fine-tuning and prompt engineering strategies. We evaluate FairMed using language-based and content-level metrics across demographic groups on publicly available (MedQA), synthetic (Synthea), and private (CBHS) datasets. Experimental results demonstrate consistent improvements over Llama3 - Med42, as well as over the zero-shot prompting baseline. For instance, in sentiment analysis for gender groups using MedQA, FairMed with Descriptive Prompting reduces the Statistical Parity Difference (SPD) from 0.0902 to 0.0658, improves the Disparate Impact Ratio from 1.1916 to 1.1566, and decreases the Kullback-Leibler Divergence from 0.0045 to 0.0024. Similarly, in directive language evaluation for gender groups using Synthea, SPD improves from 0.1056 to nearly zero, achieving near-perfect parity. On the CBHS dataset, FairMed with Descriptive Prompting increases Diagnostic Recommendation Divergence (DRD) for race groups from 0.9530 to 0.9848, indicating improved group-specific tailoring, while reducing the Action Disparity Index (ADI) from 0.0857 to 0.0469 and Referral Frequency Parity (RFP) from 0.0791 to 0.0511, reflecting enhanced fairness. These findings highlight FairMed's effectiveness in addressing demographic disparities and promoting equitable healthcare guidance through web technologies. This framework contributes to building trustworthy and inclusive systems for delivering medical advice by ensuring fairness in sensitive applications.",
      "journal": "Artificial intelligence in medicine",
      "year": "2025",
      "doi": "10.1016/j.artmed.2025.103216",
      "authors": "Lu Haohui et al.",
      "keywords": "AI applications; Bias mitigation; Fairness in large language models; Large language models",
      "mesh_terms": "Humans; Language; Delivery of Health Care; Internet; Bias; Male; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40680485/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40682331",
      "title": "The 2024 Declaration of Helsinki Revision: Relevance to Nursing Research.",
      "abstract": "BACKGROUND: The 2024 revision of the Declaration of Helsinki (DoH) marks a pivotal shift in biomedical research ethics, with significant implications for nursing research. This paper critically evaluates the Declaration's relevance to nursing practice, with particular attention to challenges in low-resource settings. Key updates emphasising global health equity, environmental sustainability, participant-centred consent and artificial intelligence (AI) governance are examined through nursing's ethical lenses of justice, beneficence and patient advocacy. METHODS: Using a multidimensional ethical framework grounded in Virtue Ethics, utilitarianism and phenomenology, the manuscript explores how nurses can ethically engage vulnerable populations, safeguard data privacy and advance inclusive, community-based research. RESULTS: It highlights gaps in the Declaration, particularly regarding algorithmic bias and digital consent and proposes practical strategies for nurse researchers, such as AI governance tools, dynamic consent models and context-sensitive sustainability practices. CONCLUSIONS: Rather than treating ethics as an abstract principle, the paper grounds theory in real-world practice, offering case examples that reflect the lived constraints of nursing researchers in underfunded and culturally diverse environments. By aligning ethical ideals with operational realities, this work reinforces nursing's critical role in shaping equitable and ethically resilient research practices under the revised Declaration.",
      "journal": "Journal of advanced nursing",
      "year": "2025",
      "doi": "10.1111/jan.70082",
      "authors": "Nashwan Abdulqadir J et al.",
      "keywords": "Global Health equity; community engagement; environmental sustainability; ethics; justice and beneficence; research; nursing research; informed consent; artificial intelligence; declaration of Helsinki; virtue ethics",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40682331/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40696767",
      "title": "Minimizing Racial Algorithmic Bias when Predicting Electronic Health Record Data Completeness.",
      "abstract": "The previously developed algorithm for identifying subjects with high electronic health record (EHR)-continuity performed suboptimally in racially diverse populations. We aimed to improve the performance by optimizing the race modeling strategy. We randomly divided TriNetX claims-linked EHR dataset from 11 US-based healthcare organizations into training (70%) and testing data (30%) to develop and test models with and without race interactions and race-specific models. We held out a Medicaid-linked EHR dataset as validation data. Study subjects were \u226518\u2009years with \u2265365\u2009days of continuous insurance enrollment overlapping an EHR encounter. We used cross-validated least absolute shrinkage and selection operator (LASSO) to select predictors of high EHR-continuity. We compared the model performance using area under receiver operating curve (AUC). There were 550,859, 236,089, and 65,956 subjects in the training, testing, and validation datasets, respectively. In the validation set, the introduction of race-interaction terms resulted in improved model performance in Black (AUC 0.821 vs. 0.812, P\u2009<\u20090.001) and other non-White race (AUC 0.828 vs. 0.812, P\u2009<\u20090.001) subgroups. The performance of the race-specific models did not differ substantially from that of the models with race-interaction terms in the race subgroups. Using the race interactions model, subjects in the top 50% of predicted EHR-continuity had 2-3-fold lesser misclassification of 40 comparative effectiveness research (CER) relevant variables. The inclusion of race-interaction terms improved model performance in the race subgroups. Using the EHR-continuity prediction algorithm with race-interaction terms can potentially reduce algorithmic bias for racial minorities.",
      "journal": "Clinical pharmacology and therapeutics",
      "year": "2025",
      "doi": "10.1002/cpt.3758",
      "authors": "Anand Priyanka et al.",
      "keywords": "",
      "mesh_terms": "Humans; Electronic Health Records; Algorithms; Male; Female; Adult; United States; Middle Aged; Racial Groups; Medicaid; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40696767/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40699955",
      "title": "Mitigating Data Bias in Healthcare AI with Self-Supervised Standardization.",
      "abstract": "The rapid advancement of artificial intelligence (AI) in healthcare has accelerated innovations in medical algorithms, yet its broader adoption faces critical ethical and technical barriers. A key challenge lies in algorithmic bias stemming from heterogeneous medical data across institutions, equipment, and workflows, which may perpetuate disparities in AI-driven diagnoses and exacerbate inequities in patient care. While AI's ability to extract deep features from large-scale data offers transformative potential, its effectiveness heavily depends on standardized, high-quality datasets. Current standardization gaps not only limit model generalizability but also raise concerns about reliability and fairness in real-world clinical settings, particularly for marginalized populations. Addressing these urgent issues, this paper proposes an ethical AI framework centered on a novel self-supervised medical image standardization method. By integrating self-supervised image style conversion, channel attention mechanisms, and contrastive learning-based loss functions, our approach enhances structural and style consistency in diverse datasets while preserving patient privacy through decentralized learning paradigms. Experiments across multi-institutional medical image datasets demonstrate that our method significantly improves AI generalizability without requiring centralized data sharing. By bridging the data standardization gap, this work advances technical foundations for trustworthy AI in healthcare.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2025",
      "doi": "10.1109/JBHI.2025.3588196",
      "authors": "Lan Guipeng et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40699955/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40713312",
      "title": "What black doctors know that AI can't: Confronting algorithmic bias and structural racism in modern medicine.",
      "abstract": "",
      "journal": "Journal of the National Medical Association",
      "year": "2025",
      "doi": "10.1016/j.jnma.2025.07.009",
      "authors": "Gomez Luis Emilio",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40713312/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40763511",
      "title": "Leveraging artificial intelligence to detect stigmatizing language in electronic health records to advance health equity.",
      "abstract": "BACKGROUND: The use of stigmatizing language within electronic health records (EHRs) is a significant concern, as it can impact patient-provider relationships, exacerbate healthcare disparities, influence clinical decision-making, and effective communication, which in turn affects patient outcomes. PURPOSE: To identify stigmatizing language in EHRs and its associations with patient outcomes. METHODS: A retrospective analysis was conducted on 75,654 clinical notes from 500 patients with hospital-acquired conditions at an academic medical center. Machine learning techniques were utilized to detect stigmatizing language within the EHRs. DISCUSSION: The model demonstrated high accuracy in identifying stigmatizing language (F1 score: 0.95), and stigmatizing language had a significant association with the length of stay. The study also revealed that older patients and those with government insurance are more likely to have stigmatizing language in their notes. CONCLUSION: Using AI to model language is useful for identifying care patterns and patients at risk due to stigmatizing language.",
      "journal": "Nursing outlook",
      "year": "2025",
      "doi": "10.1016/j.outlook.2025.102493",
      "authors": "Xavier Teenu et al.",
      "keywords": "Electronic health records; Health inequities; Hospital-acquired conditions (HACs); Informatics; Medical documentation; Natural language processing; Stigmatizing language",
      "mesh_terms": "Humans; Electronic Health Records; Health Equity; Male; Female; Retrospective Studies; Artificial Intelligence; Middle Aged; Aged; Adult; Social Stigma; Language",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40763511/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40775930",
      "title": "Biased AI: A Case for Positive Bias in Healthcare AI.",
      "abstract": "Bias in artificial intelligence (AI) is a pervasive challenge, often reinforcing systemic inequities in healthcare systems. This paper proposes an innovative framework to repurpose bias in AI, leveraging it as a tool for addressing structural injustices and improving outcomes for underrepresented and marginalized groups. Traditional healthcare algorithms often exhibit racial biases, such as underestimating risks for black patients or failing to detect dark-skinned individuals in diagnostic or safety-critical applications. This paper redefines AI bias as a tool for equity, proposing a framework to correct systemic healthcare disparities. By introducing purpose-driven bias, AI can enhance fairness in diagnostics, safety, and medical interventions. The approach involves bias analysis, diverse data curation, and AI fine-tuning to align with fairness objectives. This framework highlights the potential of \"biased AI\" to drive more inclusive and equitable healthcare.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI250912",
      "authors": "Shah Hurmat Ali et al.",
      "keywords": "AI; AI in healthcare; bias in AI; large language models",
      "mesh_terms": "Artificial Intelligence; Humans; Healthcare Disparities; Racism",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40775930/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40775971",
      "title": "AI Bias and Confounding Risk in Health Feature Engineering for Machine Learning Classification Task.",
      "abstract": "Recent advancements in machine learning bring unique opportunities in health fields but also pose considerable challenges. Due to stringent ethical considerations and resource constraints, health data can vary in scope, population coverage, and collection granularity, prone to different AI bias and confounding risks in the performance of a classification task. This experimental study explored the impact on hidden confounding risk of model performance in a cardiovascular readmission prediction task using real-life health data from 'Data-derived Risk assessment using the Electronic medical record through Application of Machine Learning' (DREAM). Five commonly used machine learning models-k-nearest neighbors (KNN), random forest (RF), decision tree (DT), Catboost and Xgboost-were selected for this task. Model performance was assessed via the area under the receiver operating characteristics curve (AUC) and F1 score, both before and after propensity score adjustment. Based on density plot comparison of the adjustment, the difference mainly contributed from patients aged 20 and 40. High fluctuation on the model performance has been noted by including and excluding patients under this age group. After reasoning, high-risk pregnant females may serve as a confounding factor in the original model generation. The pregnancy rate in the non-readmitted group is significantly higher than that in the readmitted group (x2 = 10.2, p < 0.001). However, pregnant status required additional information query from a different hospital system. Without carefully consideration of confounding risks, traditional pipeline may generate a less robotic classifier in the clinical setting. Incorporating propensity score matching could be a solution to randomise invisible confounding factors between the classes.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI250953",
      "authors": "Guo Ruihua et al.",
      "keywords": "AI bias; Classification; Confounding bias; Feature Engineering; Machine Learning; Quality Control; Readmission risk prediction",
      "mesh_terms": "Machine Learning; Humans; Electronic Health Records; Female; Risk Assessment; Bias; Adult; Patient Readmission; Cardiovascular Diseases; Pregnancy; Confounding Factors, Epidemiologic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40775971/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40776043",
      "title": "Algorithmic Fairness in Machine Learning Prediction of Autism Using Electronic Health Records.",
      "abstract": "Efforts to improve early diagnosis of autism spectrum disorder (ASD) in children are beginning to use machine learning (ML) approaches applied to real-world clinical datasets, such as electronic health records (EHRs). However, sex-based disparities in ASD diagnosis highlight the need for fair prediction models that ensure equitable performance across demographic groups for ASD identification. This retrospective case-control study aimed to develop ML-based prediction models for ASD diagnosis using risk factors found in EHRs and assess their algorithmic fairness. The study cohorts included 70,803 children diagnosed with ASD and 212,409 matched controls without ASD. We built logistic regression and Xgboost models and evaluated their performance using standard metrics, including accuracy, recall, precision, F1-score, and area under the curve (AUC). To assess fairness, we examined model performance by sex and calculated fairness-specific metrics, such as equal opportunity (recall parity) and equalized odds, to identify potential biases in model predictions between boys and girls. Our results revealed significant fairness issues in ML models for ASD prediction using EHRs.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI251025",
      "authors": "Angell Amber M et al.",
      "keywords": "Autism; electronic health records; predictive modeling",
      "mesh_terms": "Humans; Electronic Health Records; Machine Learning; Male; Female; Retrospective Studies; Case-Control Studies; Child; Autism Spectrum Disorder; Algorithms; Child, Preschool",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40776043/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40776262",
      "title": "FairFML: A Unified Approach to Algorithmic Fair Federated Learning with Applications to Reducing Gender Disparities in Cardiac Arrest Outcomes.",
      "abstract": "Addressing algorithmic bias in healthcare is crucial for ensuring equity in patient outcomes, particularly in cross-institutional collaborations where privacy constraints often limit data sharing. Federated learning (FL) offers a solution by enabling institutions to collaboratively train models without sharing sensitive data, but challenges related to fairness remain. To tackle this, we propose Fair Federated Machine Learning (FairFML), a model-agnostic framework designed to reduce algorithmic disparities while preserving patient privacy. Validated in a real-world study on gender disparities in cardiac arrest outcomes, FairFML improved fairness by up to 65% compared to centralized models, without compromising predictive performance.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI251245",
      "authors": "Li Siqi et al.",
      "keywords": "Clinical decision-making; Demographic disparity; Electronic health records; Federated Learning; Model fairness",
      "mesh_terms": "Humans; Machine Learning; Female; Male; Heart Arrest; Algorithms; Healthcare Disparities; Sex Factors; Federated Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40776262/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40788801",
      "title": "Adversarial Debiasing for Equitable and Fair Detection of Acute Coronary Syndrome Using 12-Lead ECG.",
      "abstract": "OBJECTIVE: Acute coronary syndrome (ACS) is a life-threatening condition requiring accurate diagnosis for better outcomes. However, variability in signs and symptoms among racial subgroups could cause disparities in diagnostic accuracy. In this study, we use machine learning models to diagnose ACS, focusing on mitigating disparities and ensuring fairness between Black and non-Black populations. METHODS: We built on a state-of-the-art random forest classifier to compare three mitigation strategies. The first two approaches involved resampling or partitioning the data prior to training, while the third approach proposed an innovative framework called adversarial debiasing. To evaluate our model performance, we used the receiver operating characteristic (ROC) curve and an operating point at 80% specificity for clinical importance. RESULTS: After mitigation with adversarial debiasing, the difference in sensitivities between the two subgroups decreased from 9.8% to 1.3%. Specifically, this approach achieved areas under the ROC of 0.810 and 0.817, and sensitivities of 70.1% and 71.4%, respectively for Black and non-Black subgroups. CONCLUSION: The proposed adversarial debiasing model outperformed the other two methods in both diagnostic accuracy and effectiveness in minimizing disparities. SIGNIFICANCE: We expect this framework to achieve fair diagnostic models across diverse demographic populations globally and be generalizable to other outcomes.",
      "journal": "IEEE transactions on bio-medical engineering",
      "year": "2026",
      "doi": "10.1109/TBME.2025.3597527",
      "authors": "Ji Rui Qi et al.",
      "keywords": "",
      "mesh_terms": "Humans; Acute Coronary Syndrome; Electrocardiography; Machine Learning; ROC Curve; Female; Male; Middle Aged; Aged; Signal Processing, Computer-Assisted; Diagnosis, Computer-Assisted; Sensitivity and Specificity; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40788801/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40802160",
      "title": "Navigating Healthcare AI Governance: the Comprehensive Algorithmic Oversight and Stewardship Framework for Risk and Equity.",
      "abstract": "Integrating artificial intelligence (AI) in healthcare has sparked innovation but exposed vulnerabilities in regulatory oversight. Unregulated \"shadow\" AI systems, operating outside formal frameworks, pose risks such as algorithmic drift, bias, and disparities. The Comprehensive Algorithmic Oversight and Stewardship (CAOS) Framework addresses these challenges, combining risk assessments, data protection, and equity-focused methodologies to ensure responsible AI implementation. This framework offers a solution to bridge oversight gaps while supporting responsible healthcare innovation. CAOS functions as both a normative governance model and a practical system design, offering a scalable framework for ethical oversight, policy development, and operational implementation of AI systems in healthcare.",
      "journal": "Health care analysis : HCA : journal of health philosophy and policy",
      "year": "2025",
      "doi": "10.1007/s10728-025-00537-y",
      "authors": "Kumar Rahul et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40802160/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40833926",
      "title": "Over-reliance on AI for diagnosis: the potential for algorithmic bias and the erosion of clinical skills.",
      "abstract": "",
      "journal": "Journal of medical engineering & technology",
      "year": "2025",
      "doi": "10.1080/03091902.2025.2548478",
      "authors": "Shahzaib Fnu et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40833926/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40834630",
      "title": "BiasPruner: Mitigating bias transfer in continual learning for fair medical image analysis.",
      "abstract": "Continual Learning (CL) enables neural networks to learn new tasks while retaining previous knowledge. However, most CL methods fail to address bias transfer, where spurious correlations propagate to future tasks or influence past knowledge. This bidirectional bias transfer negatively impacts model performance and fairness, especially in medical imaging, where it can lead to misdiagnoses and unequal treatment. In this work, we show that conventional CL methods amplify these biases, posing risks for diverse patient cohorts. To address this, we propose BiasPruner, a framework that mitigates bias propagation through debiased subnetworks, while preserving sequential learning and avoiding catastrophic forgetting. BiasPruner computes a bias attribution score to identify and prune network units responsible for spurious correlations, creating task-specific subnetworks that learn unbiased representations. As new tasks are learned, the framework integrates non-biased units from previous subnetworks to preserve transferable knowledge and prevent bias transfer. During inference, a task-agnostic gating mechanism selects the optimal subnetwork for robust predictions. We evaluate BiasPruner on medical imaging benchmarks, including skin lesion and chest X-ray classification tasks, where biased data (e.g., spurious skin tone correlations) can exacerbate disparities. Our experiments show that BiasPruner outperforms state-of-the-art CL methods in both accuracy and fairness. Code is available at: BiasPruner.",
      "journal": "Medical image analysis",
      "year": "2025",
      "doi": "10.1016/j.media.2025.103764",
      "authors": "Bayasi Nourhan et al.",
      "keywords": "Bias transfer; Catastrophic forgetting; Continual learning; Debiased representations; Debiased subnetwork; Dynamic subnetwork pruning; Spurious correlations",
      "mesh_terms": "Humans; Neural Networks, Computer; Image Processing, Computer-Assisted; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40834630/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40840292",
      "title": "Enhancing feature discrimination with pseudo-labels for foundation model in segmentation of 3D medical images.",
      "abstract": "Development of medical image segmentation foundation models relies on large-scale samples. However, it is more time-consuming to annotate 3D medical images than 2D natural images, making it challenging to collect sufficient annotated samples. While pseudo-labeling offers a potential solution to expand the annotated dataset, it may introduce noisy labels that can create systematic biases, particularly affecting the segmentation performance of smaller anatomical structures. To this end, we propose a pseudo-label enriched segmentation framework (PESF), which integrates confidence filtering and perturbation-based curriculum learning. To begin with, our pseudo-labeling approach applies a well-pretrained foundation model to generate pseudo-labels for previously unannotated organ categories, effectively expanding the number of classes in the original dataset. Subsequently, we develop a confidence-based filtering mechanism, leveraging a feature extraction module combined with a confidence prediction module to quantitatively assess and filter out low-quality pseudo-labels, thereby minimizing the detrimental effects of noisy pseudo-labels on the model's optimization. Furthermore, a progressive sampling strategy that integrates curriculum learning with Gaussian random perturbations is proposed, systematically introducing training samples from simpler to more complex cases, thereby enhancing the model's generalization capability across organs of varying shapes and sizes. Additionally, our theoretical analysis reveals that incorporating these extra pseudo-labeled classes strengthens feature discrimination by increasing the angular margins between class decision boundaries in the embedding space. Experimental results demonstrate that PESF achieves a 6.8% improvement in the overall average Dice Similarity Coefficient (DSC) compared to the baseline SAM-Med3D on (Amos, FLARE22, WORD, BTCV), with particularly gains in challenging anatomical structures such as the pancreas and esophagus. The code is available at https://github.com/lonezhizi/PESF.",
      "journal": "Neural networks : the official journal of the International Neural Network Society",
      "year": "2026",
      "doi": "10.1016/j.neunet.2025.107979",
      "authors": "Jin Ge et al.",
      "keywords": "Curriculum learning; Foundation model; Medical image segmentation; Pseudo-label; Segment anything model",
      "mesh_terms": "Humans; Imaging, Three-Dimensional; Neural Networks, Computer; Image Processing, Computer-Assisted; Machine Learning; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40840292/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40844259",
      "title": "Ethical Reflections on Integrating Artificial Intelligence in Care Practices.",
      "abstract": "BackgroundThe integration of artificial intelligence (AI) and robotics into disability care presents transformative opportunities while simultaneously raising pressing ethical concerns. Issues related to autonomy, human dignity, and equitable access require careful consideration, particularly as these technologies reshape the dynamics of care delivery and clinical relationships. PurposeDrawing on an interdisciplinary approach that synthesizes insights from bioethical literature, illustrative case studies, and expert perspectives from healthcare, law, and technology, this reflection examines the ethical landscape of AI-supported rehabilitation and assistance. Particular attention is given to risks such as algorithmic bias, over-reliance on automation, and the potential erosion of the human dimension in care. A biopsychosocial model serves as a guiding framework to analyze how technological systems intersect with the lived experiences of individuals with disabilities. Ethical tensions emerge around personalized care, transparency in decision-making, and the inclusivity of data and design processes.ConclusionsThe analysis emphasizes the need for governance models that embed ethical safeguards and promote fairness, while also encouraging participatory design involving patients, caregivers, and healthcare professionals. By situating technological developments within broader socio-political and clinical contexts, this reflection identifies pathways toward a more equitable and human-centered integration of AI. Recommendations include investment in inclusive datasets, the development of fairness-aware algorithms, and the establishment of regulatory mechanisms that align innovation with fundamental rights and principles of social justice in healthcare.",
      "journal": "Community health equity research & policy",
      "year": "2025",
      "doi": "10.1177/2752535X251370928",
      "authors": "Ricchezze Giulia et al.",
      "keywords": "artificial intelligence; disability; equity; health care; rehabilitation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40844259/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40853158",
      "title": "Big Data in Neurosurgery: A Guideline on Data Structures, Machine Learning Models, and Ethical Considerations.",
      "abstract": "Artificial intelligence (AI) is reshaping neurosurgery, offering unprecedented opportunities to enhance diagnostics, personalize treatment, and predict outcomes. At the heart of this transformation is the ability to effectively harness big data (BD) within the electronic medical record. Understanding these data structures is essential for making sense of the vast volumes of information generated in modern neurosurgical practice. Equally important are the machine learning (ML) models driving these advancements. From supervised learning and convolutional neural networks to generative AI, these tools are already making a mark in areas such as brain tumor segmentation and spine surgery outcome predictions. Their versatility highlights the potential of ML to complement clinical expertise and streamline decision-making in neurosurgery. However, adopting BD and ML also brings ethical challenges that cannot be ignored. Bias in algorithms threatens to reinforce health disparities, whereas concerns about data privacy demand vigilance in handling sensitive patient information. In addition, the question of liability looms large as ML increasingly influences clinical decisions. The aim of the study was to provide a roadmap for neurosurgeons navigating the evolving intersection of BD, ML, and ethical responsibility in the AI era.",
      "journal": "Operative neurosurgery (Hagerstown, Md.)",
      "year": "2025",
      "doi": "10.1227/ons.0000000000001751",
      "authors": "Singh Rohin et al.",
      "keywords": "Artificial intelligence; Big data; Epic; Ethics; Machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40853158/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40887086",
      "title": "Using Artificial Intelligence and Machine Learning to Promote Child Health Equity.",
      "abstract": "Artificial intelligence (AI) and machine learning (ML), used injudiciously, have the potential to exacerbate health inequalities. Conversely, there is a potential to use ML to give insight into the impact of socioeconomic factors, which allows us to make predictions on an individual as well as a population level. This paper outlines the potential applications of ML in child health, with a specific focus on its impact on health equity. We describe our experience in applying 2 novel ML use cases to promote population health equity in a diverse population in inner-city London, United Kingdom: (1) an ML algorithm developed and trained using routinely collected demographic data to predict nonattendance at outpatient appointments across a wide range of settings and (2) a risk-prediction tool for children with asthma, which uses a number of routine metrics across the spectrum of health determinants to target preventive interventions toward high-risk patients with asthma. Using this experience, we outline the possible ways in which inequity can be inadvertently embedded in the training data, the ML model and how it is deployed and outline ways to mitigate that now and in the future.",
      "journal": "Pediatrics",
      "year": "2025",
      "doi": "10.1542/peds.2025-070739L",
      "authors": "Cheung C Ronny et al.",
      "keywords": "",
      "mesh_terms": "Humans; Machine Learning; Health Equity; Artificial Intelligence; Child; Child Health; Asthma; London; Male; Female; Child, Preschool",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40887086/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40899524",
      "title": "Towards Fairness in Synthetic Healthcare Data: A Framework for the Evaluation of Synthetization Algorithms.",
      "abstract": "INTRODUCTION: Synthetic data generation is a rapidly evolving field, with significant potential for improving data privacy. However, evaluating the performance of synthetic data generation methods, especially the tradeoff between fairness and utility of the generated data, remains a challenge. METHODOLOGY: In this work, we present our comprehensive framework, which evaluates fair synthetic data generation methods, benchmarking them against state-of-the-art synthesizers. RESULTS: The proposed framework consists of selection, evaluation, and application components that assess fairness, utility, and resemblance in real-world scenarios. The framework was applied to state-of-the-art data synthesizers, including TabFairGAN, DECAF, TVAE, and CTGAN, using a publicly available medical dataset. DISCUSSION: The results reveal the strengths and limitations of each synthesizer, including their bias mitigation strategies and trade-offs between fairness and utility, thereby showing the framework's effectiveness. The proposed framework offers valuable insights into the fairness-utility tradeoff and evaluation of synthetic data generation methods, with far-reaching implications for various applications in the medical domain and beyond. CONCLUSION: The findings demonstrate the importance of considering fairness in synthetic data generation and the need for fairness focused evaluation frameworks, highlighting the significance of continued research in this area.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI251376",
      "authors": "Warnecke Yannik et al.",
      "keywords": "Algorithms; Artificial Intelligence; Data Quality; Delivery of Health Care; Health Equity; Humans; Medical Informatics",
      "mesh_terms": "Humans; Algorithms; Confidentiality; Data Collection",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40899524/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40930712",
      "title": "Relational accountability in AI-driven pharmaceutical practices: an ethics approach to bias, inequity and structural harm.",
      "abstract": "The integration of artificial intelligence (AI) into pharmaceutical practices raises critical ethical concerns, including algorithmic bias, data commodification and global health inequities. While existing AI ethics frameworks emphasise transparency and fairness, they often overlook structural vulnerabilities tied to race, gender and socioeconomic status. This paper introduces relational accountability-a feminist ethics framework-to critique AI-driven pharmaceutical practices, arguing that corporate reliance on biased algorithms exacerbates inequalities by design. Through case studies of Pfizer-IBM Watson's immuno-oncology collaboration and Google DeepMind's National Health Service partnership, we demonstrate how AI entrenches disparities in drug pricing, access and development. We propose a causal pathway linking biased training data to inequitable health outcomes, supported by empirical evidence of AI-driven price discrimination and exclusionary clinical trial recruitment algorithms. Policy solutions, including algorithmic audits and equity-centred data governance, are advanced to realign AI with the ethical imperative. This work bridges feminist bioethics and AI governance, offering a novel lens to address structural harm in healthcare innovation.",
      "journal": "Journal of medical ethics",
      "year": "2025",
      "doi": "10.1136/jme-2025-110913",
      "authors": "Biswas Irfan",
      "keywords": "Quality of Health Care",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40930712/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40938311",
      "title": "Enhanced guidance on artificial intelligence for medical publication and communication professionals.",
      "abstract": "The International Society for Medical Publication Professionals (ISMPP) position statement and call to action on the use of artificial intelligence (AI), published in 2024, recognized the value of AI while advocating for best practices to guide its use. In this commentary, we offer enhanced guidance on the call to action for ISMPP members and other medical communication professionals on the topics of education and training, implementation and use, and advocacy and community engagement. With AI rapidly revolutionizing scientific communication, members should stay up to date with advancements in the field by completing AI training courses, engaging with ISMPP AI education and training and other external training platforms, developing a practice of lifelong learning, and improving AI literacy. Members can successfully integrate and use AI by complying with organizational policies, ensuring fair access to AI models, complying with authorship guidance, properly disclosing the use of AI models or tools, respecting academic integrity and copyright restrictions, and understanding privacy protections. Members also need to be familiar with the systemic problem of bias with large language models, which can reinforce health inequities, as well as the limits of transparency and explainability with AI models, which can undermine source verification, bias detection, and even scientific integrity. AI models can produce hallucinations, results that are factually incorrect, irrelevant, or nonsensical, which is why all outputs from AI models should be reviewed and verified for accuracy by humans. With respect to advocacy and community engagement, members should advocate for the responsible use of AI, participate in developing AI policy and governance, work with underserved communities to get access to AI tools, and share findings for AI use cases or research results in peer-reviewed journals, conferences, and other professional platforms.",
      "journal": "Current medical research and opinion",
      "year": "2025",
      "doi": "10.1080/03007995.2025.2556012",
      "authors": "Goldman Keith et al.",
      "keywords": "Artificial intelligence; call\u00a0to\u00a0action; enhanced guidance; medical communications; medical publication professional",
      "mesh_terms": "Artificial Intelligence; Humans; Communication",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40938311/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40955197",
      "title": "Global Research Trends and Disparities in Cesarean Section-Related Placenta Accreta Spectrum Disorders: A 93-Year Bibliometric Analysis.",
      "abstract": "AIM: Placenta accreta spectrum disorders (PASD) complicate 1 in 2500 deliveries globally, with incidence rising parallel to a 31% increase in cesarean section (CS) rates over three decades. This bibliometric analysis maps 93 years (1933-2025) of CS-PASD research to identify collaboration gaps and prioritize solutions focused on equity. METHODS: Articles concerning CS and PASD from 1933 to early 2025 were retrieved from the Web of Science Core Collection. A bibliometric analysis was conducted using VOSviewer, CiteSpace, and R- Bibliometrix to evaluate co-authorship networks, institutional collaborations, and keyword co-occurrence patterns. RESULTS: A total of 758 articles were identified, with an average annual growth rate of 2.64%. Contributions came from 57 countries/regions and involved 3814 authors, with the USA and UK leading in citations. Notable institutions included Sichuan University and University of California System. Key journals included the American Journal of Obstetrics and Gynecology and Obstetrics & Gynecology. Key contributors included Takahashi Hironori (9 articles) and Liu Xinghui (highest collaboration frequency, Total Link Strength (TLS) = 24), with emerging networks centered on Chinese and European institutions. Key keywords included \"placenta accreta spectrum\", \"prior cesarean section\", \"ultrasound diagnosis\", and \"F\u00e9d\u00e9ration Internationale de Gyn\u00e9cologie et d'Obst\u00e9trique (FIGO) guidelines\", reflecting major research themes in PASD management. Trends highlighted advancements in diagnostic standardization, such as artificial intelligence (AI)-enhanced placental magnetic resonance imaging (MRI), multidisciplinary care models, and AI-driven risk stratification. Additionally, disparities in global resource allocation underscored the need for equitable healthcare interventions. CONCLUSIONS: Research on CS-PASD has evolved into a multidisciplinary issue, facilitated by advancements in imaging technologies and collaborative efforts. Future investigations should prioritize the integration of AI diagnostics, the development of cost-effective preventive strategies, and the establishment of standardized protocols to enhance maternal safety and mitigate healthcare inequities.",
      "journal": "Annali italiani di chirurgia",
      "year": "2025",
      "doi": "10.62713/aic.4093",
      "authors": "Huang Qing et al.",
      "keywords": "FIGO guidelines; bibliometric analysis; cesarean section; multidisciplinary management; peripartum hysterectomy; placenta accreta spectrum disorders; risk stratification; ultrasound diagnosis",
      "mesh_terms": "Placenta Accreta; Bibliometrics; Humans; Pregnancy; Cesarean Section; Female; Biomedical Research; Healthcare Disparities; Global Health",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40955197/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40965022",
      "title": "Developments in the Management Strategies for Allergy: Advances in Artificial Intelligence and Future Perspectives.",
      "abstract": "INTRODUCTION: Artificial intelligence (AI) is rapidly transforming biomedical research by offering advanced tools to analyse complex datasets. In the field of allergy studies, however, the translation of AI-generated insights into clinical practice remains limited and underutilised. METHOD: This review critically discussed the current applications of AI in allergy studies. It focuses on the methodological foundations of AI, including machine learning and clustering algorithms, and assesses their practical benefits and limitations. Representative case studies are explored to demonstrate real-world applications, and challenges in data quality, integration, and algorithmic fairness are examined. RESULTS: AI techniques have shown promise in tasks such as disease phenotyping and patient stratification within allergy research. Case studies reveal that AI can uncover immunological insights and support precision medicine approaches. However, the field faces challenges, including fragmented data sources, algorithmic bias, and the limited presence of therapeutic AI tools in clinical practice. DISCUSSION: Despite the demonstrated potential, several barriers hinder the broader adoption of AI in allergy care. These include the need for high-quality, standardised datasets, ethical oversight, and transparent methodologies. The review highlights the importance of these factors in ensuring the reliability, reproducibility, and equity of AI-driven interventions in allergy research. CONCLUSION: AI holds significant promise for improving diagnostic accuracy and enabling personalised treatment strategies in allergy care. Realising its full potential will require robust frameworks, ethical governance, and interdisciplinary collaboration to overcome current limitations and drive clinical translation.",
      "journal": "Anti-inflammatory & anti-allergy agents in medicinal chemistry",
      "year": "2025",
      "doi": "10.2174/0118715230389406250906142050",
      "authors": "Kumar Suraj et al.",
      "keywords": "Allergy; allergy management; artificial intelligence; deep learning; diagnosis; machine learning; precision medicine.",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40965022/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40965098",
      "title": "Advancing equity in generative AI dermatology requires representative data and transparent evaluation.",
      "abstract": "",
      "journal": "Journal of the European Academy of Dermatology and Venereology : JEADV",
      "year": "2025",
      "doi": "10.1111/jdv.70052",
      "authors": "Kabakova Margaret et al.",
      "keywords": "algorithmic bias; artificial intelligence; dermatology; health equity; machine learning; skin pigmentation",
      "mesh_terms": "",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40965098/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40981366",
      "title": "The FAIR framework: ethical hybrid peer review.",
      "abstract": "OBJECTIVES: Traditional peer review faces critical challenges including systematic bias, prolonged delays, reviewer fatigue, and lack of transparency. These failures violate ethical obligations of beneficence, justice, and autonomy while hindering scientific progress and costing billions annually in academic\u00a0labor. To propose an ethically-guided hybrid peer review system that integrates generative artificial intelligence with human expertise while addressing fundamental shortcomings of current review processes. METHODS: We developed the FAIR Framework (Fairness, Accountability, Integrity, and Responsibility) through systematic analysis of peer review failures and integration of AI capabilities. The framework employs standardized prompt engineering to guide AI evaluation of manuscripts while maintaining human oversight throughout all stages. RESULTS: FAIR addresses bias through algorithmic detection and standardized evaluation protocols, ensures accountability via transparent audit trails and documented decisions, maintains integrity through secure local AI processing and confidentiality safeguards, and upholds responsibility through ethical oversight and constructive feedback mechanisms. The hybrid model automates repetitive tasks including initial screening, methodological verification, and plagiarism detection while preserving human judgment for novelty assessment, ethical evaluation, and final decisions. CONCLUSIONS: The FAIR Framework offers a principled solution to peer review inefficiencies by combining AI-enabled consistency and speed with essential human expertise. This hybrid approach reduces review delays, eliminates systematic bias, and enhances transparency while maintaining confidentiality and editorial control. Implementation could significantly reduce the estimated 100 million hours of global reviewer time annually while improving review quality and equity across diverse research communities.",
      "journal": "Journal of perinatal medicine",
      "year": "2025",
      "doi": "10.1515/jpm-2025-0285",
      "authors": "Gr\u00fcnebaum Amos et al.",
      "keywords": "artificial intelligence; hybrid systems; medical publishing; peer review; research ethics; scientific publishing",
      "mesh_terms": "Humans; Artificial Intelligence; Peer Review, Research",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40981366/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41005257",
      "title": "Fairness in machine learning-based hand load estimation: A case study on load carriage tasks.",
      "abstract": "Predicting external hand load from sensor data is essential for ergonomic exposure assessments, as obtaining this information typically requires direct observation or supplementary data. While machine learning can estimate hand load from posture or force data, we found systematic bias tied to biological sex, with predictive disparities worsening in imbalanced training datasets. To address this, we developed a fair predictive model using a Variational Autoencoder with feature disentanglement, which separates sex-agnostic from sex-specific motion features. This enables predictions based only on sex-agnostic patterns. Our proposed algorithm outperformed conventional machine learning models, including k-Nearest Neighbors, Support Vector Machine, and Random Forest, achieving a mean absolute error of 3.42 and improving fairness metrics like statistical parity and positive and negative residual differences, even when trained on imbalanced sex datasets. These results underscore the importance of fairness-aware algorithms in avoiding health and safety disadvantages for specific worker groups in the workplace.",
      "journal": "Applied ergonomics",
      "year": "2025",
      "doi": "10.1016/j.apergo.2025.104642",
      "authors": "Rahman Arafat et al.",
      "keywords": "Algorithmic bias; Fairness; Gait kinematics; Load carriage; Machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41005257/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41006145",
      "title": "Not All EPAs Are Created Equal: Fixing Sampling Bias With Utility Modeling.",
      "abstract": "BACKGROUND: Entrustable professional activities (EPAs) are foundational for understanding resident progress towards practice readiness. Unfortunately, when EPAs were initiated manually, EPA assessment completion has been uneven, creating biases from assessment variability across individuals, specialties, and institutions. Therefore, we introduce EPA assessment utility modeling, which can retrospectively correct for and prospectively avoid these biases by informing each attending of the usefulness of each EPA assessment opportunity and highlighting when EPA assessments are most needed. METHODS: We performed a longitudinal analysis of general surgery EPA assessments using an EHR-integrable medical-education platform across 37 institutions. EPA assessment counts were fitted with power law curves to measure skewing. Raw EPA assessment ratings, combined with historical case logs and OR schedules, were analyzed with the platform's large-scale Bayesian network model to quantify each EPA assessment's impact on entrustment learning curves. Lastly, we used Monte Carlo simulations to develop an assessment utility score, as an intuitive label for the predicted benefit of each EPA assessment opportunity, in order to prompt faculty members to complete the most highly useful assessments. RESULTS: From 6/2023 to 5/2025, 444 faculty assessed 532 residents with 17,245 EPA assessments. EPA assessment counts showed substantial skewing across several factors. By EPA type, 52.8% of EPA assessments were of the top 4 (22.2%) types (power law \u03b1\u202f=\u202f0.27, 2 p\u202f\u2248\u202f0). By faculty, 33.5% of EPA assessments were from the most active 15 (4.3%) faculty members (\u03b1\u202f=\u202f0.15, 2 p\u202f\u2248\u202f0). By faculty specialty, 31.0% were from the most active 2 (9.5%) specialties (\u03b1\u202f=\u202f0.24, 2 p\u202f\u2248\u202f0). By resident, 20.1% were received by the 20 (4.5%) most assessed residents (\u03b1\u202f=\u202f0.21, 2 p\u202f\u2248\u202f0). CONCLUSION: EPA assessments were heavily skewed with sampling biases, misrepresenting entrustment levels. To fix these biases and provide a data-driven approach to CBE measurement, we propose an assessment utility framework to optimize EPA assessment timing, assessor, and prioritization.",
      "journal": "Journal of surgical education",
      "year": "2025",
      "doi": "10.1016/j.jsurg.2025.103708",
      "authors": "Jenkins Phillip et al.",
      "keywords": "artificial intelligence; competency-based medical education; entrustable professional activities; human-centered design; surgical education; utility model",
      "mesh_terms": "Internship and Residency; Humans; Clinical Competence; Education, Medical, Graduate; General Surgery; Longitudinal Studies; Educational Measurement; Competency-Based Education; Selection Bias; Retrospective Studies; Bayes Theorem",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41006145/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41032562",
      "title": "Enhancing Fairness and Accuracy in Diagnosing Type 2 Diabetes in Young Adult Population.",
      "abstract": "While type 2 diabetes is predominantly found in the elderly population, recent publications indicate an increasing prevalence in the young adult population. Failing to diagnose it in the minority younger age group could have significant adverse effects on their health. Several previous works acknowledge the bias of machine learning models towards different gender and race groups and propose various approaches to mitigate it. However, those works failed to propose any effective methodologies to diagnose diabetes in the young population, which is the minority group in the diabetic population. This is the first paper where we mention digital ageism towards the young adult population diagnosing diabetes. In this paper, we identify this deficiency in traditional machine learning models and propose an algorithm to mitigate the bias towards the young population when predicting diabetes. Deviating from the traditional concept of one-model-fits-all, we train customized machine-learning models for each age group. Our pipeline trains a separate machine learning model for every 5-year age band (i.e., age groups 30-34, 35-39, and 40-44). The proposed solution consistently improves recall of diabetes class by 26% to 40% in the young age group (30-44). Moreover, our technique outperforms 7 commonly used whole-group resampling techniques (i.e., random oversampling, random undersampling, SMOTE, ADASYN, Tomek-links, ENN, and Near Miss) by at least 36% in terms of diabetes recall in the young age group. Feature important analysis shows that the age attribute has a significant contribution to the decision of the original model, which was marginalized in the age-personalized model. Our method shows improved performance (e.g., balanced accuracy improved 7-12%) over multiple machine learning models and multiple sampling algorithms.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2025",
      "doi": "10.1109/JBHI.2025.3616312",
      "authors": "Pias Tanmoy Sarkar et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41032562/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41032683",
      "title": "Regulating AI in Nursing and Healthcare: Ensuring Safety, Equity, and Accessibility in the Era of Federal Innovation Policy.",
      "abstract": "The rapid integration of artificial intelligence in healthcare, accelerated by the Trump administration's 2025 AI Action Plan and private sector innovations from companies like Nvidia and Hippocratic AI, poses urgent challenges for nursing and health policy. This policy analysis examines the intersection of federal AI initiatives, emerging healthcare technologies, and nursing workforce implications through document analysis of regulatory frameworks, the federal AI Action Plan's 90+ initiatives, and insights from the American Academy of Nursing's November 2024 policy dialogue on AI transformation. The analysis reveals that while AI demonstrates measurable improvements in discrete clinical tasks-including 16% better medication assessment accuracy and 43% greater precision in identifying drug interactions at $9 per hour compared to nurses' median $41.38 hourly wage-current federal policy lacks critical healthcare-specific safeguards. The AI Action Plan's emphasis on rapid deployment and deregulation fails to address safety-net infrastructure needs, implementation pathways for vulnerable populations, or mechanisms ensuring health equity. Evidence from the Academy dialogue indicates that AI's \"technosocial reality\" fundamentally alters care delivery while potentially exacerbating disparities in underserved communities, as demonstrated by algorithmic bias in systems like Optum's care allocation algorithm. The findings suggest that achieving equitable AI integration requires comprehensive regulatory frameworks coordinating FDA, CMS, OCR, and HRSA oversight; community-centered governance approaches redistributing decision-making power to affected populations; and nursing leadership in AI development to preserve patient-centered care values. Without proactive nursing engagement in AI governance, healthcare risks adopting technologies that prioritize efficiency over the holistic, compassionate care fundamental to nursing practice.",
      "journal": "Policy, politics & nursing practice",
      "year": "2026",
      "doi": "10.1177/15271544251381228",
      "authors": "Yang Y Tony et al.",
      "keywords": "algorithms; artificial intelligence; health care delivery; health equity; health policy; nursing",
      "mesh_terms": "Artificial Intelligence; Humans; United States; Health Policy; Health Equity; Health Services Accessibility; Delivery of Health Care; Patient Safety",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41032683/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41041783",
      "title": "Towards an Analytical System for Supervising Fairness, Robustness, and Dataset Shifts in Health AI.",
      "abstract": "Ensuring trustworthy use of Artificial Intelligence (AI)-based Clinical Decision Support Systems (CDSSs) requires continuous evaluation of their performance and fairness, given the potential impact on patient safety and individual rights as high-risk AI systems. However, the practical implementation of health AI performance and fairness monitoring dashboards presents several challenges. Confusion-matrix-derived performance and fairness metrics are non-additive and cannot be reliably aggregated or disaggregated across time or population subgroups. Furthermore, acquiring ground-truth labels or sensitive variable information, and controlling dataset shifts-changes in data statistical distributions-may require additional interoperability with the electronic health records. We present the design of ShinAI-Agent, a modular system that enables continuous, interpretable, and privacy-aware monitoring of health AI and CDSS performance and fairness. An exploratory dashboard combines time series navigation for multiple performance and fairness metrics, model calibration and decision cutoff exploration, and dataset shift monitoring. The system adopts a two-layer database. First, a proxy database, mapping AI outcomes and essential case-level data such as the ground-truth and sensitive variables. And second, an OLAP architecture with aggregable primitives, including case-based confusion matrices and binned probability distributions for flexible computation of performance and fairness metrics across time or sensitive subgroups. The ShinAI-Agent approach supports compliance with the ethical and robustness requirements of the EU AI Act, enables advisory for model retraining and promotes the operationalisation of Trustworthy AI.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI251537",
      "authors": "S\u00e1nchez-Garc\u00eda \u00c1ngel et al.",
      "keywords": "Ethical AI; Fairness; Robust AI; Software Engineering; Trustworthy AI",
      "mesh_terms": "Artificial Intelligence; Electronic Health Records; Decision Support Systems, Clinical; Humans",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41041783/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41055822",
      "title": "Invisible Bias in GPT-4o-mini: Detecting Disparities in AI-Generated Patient Messaging.",
      "abstract": "Artificial intelligence (AI), specifically large language models (LLM), have gained significant popularity over the last decade with increased performance and expanding applications. AI could improve the quality of patient care in medicine but hidden biases introduced during training could be harmful. This work utilizes GPT-4o-mini to generate patient communications based on systematically generated, synthetic patient data that would be commonly available in a patient's medical record. To evaluate the AI generated communications for disparities, GPT-4o-mini was used to score the generated communications on empathy, encouragement, accuracy, clarity, professionalism, and respect. Disparities in scores associated with specific components of a patient's history were used to detect potential biases. A patient's sex and religious preference were found to have a statistically significant impact on scores. However, further work is needed to evaluate a wider collection of LLMs utilizing more specific and human validated scoring criteria. Overall, this work proposes a novel method of evaluating bias in LLMs by creating synthetic patient histories to formulate AI generated communications and score them with opportunities for further investigation.",
      "journal": "Journal of medical systems",
      "year": "2025",
      "doi": "10.1007/s10916-025-02276-y",
      "authors": "Reagen Christopher et al.",
      "keywords": "Artificial intelligence; Bias; Empathy; Large language models; Medicine; Patient communication",
      "mesh_terms": "Humans; Artificial Intelligence; Communication; Male; Female; Language; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41055822/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41056069",
      "title": "Bias or Best Fit? A Comparative Analysis of the SEER and NCDB Data Sets in Single-model Machine Learning for Predicting Osteosarcoma Survival Outcomes.",
      "abstract": "BACKGROUND: Machine-learning models are increasingly used in orthopaedic oncology to predict survival outcomes for patients with osteosarcoma. Typically, these models are trained on a single data set, such as the Surveillance, Epidemiology, and End Results (SEER) or the National Cancer Database (NCDB). However, because any single database, even if it is large, may emphasize different data points and may include errors, models trained on single data sets may learn database-specific patterns rather than generalizable clinical relationships, limiting their clinical utility when applied to different patient populations. QUESTIONS/PURPOSES: We developed separate machine-learning models using SEER and NCDB databases and (1) compared the accuracy of SEER- and NCDB-trained models in estimating 2- and 5-year overall survival when validated on their respective databases, (2) assessed which database produced a more generalizable machine-learning model (defined as one that maintains high performance when applied to unseen external data) by using the model trained on one database to externally validate the other, and (3) identified key factors contributing to prediction accuracy. METHODS: From 2000 to 2018 (SEER) and 2004 to 2018 (NCDB), we identified 15,241 SEER patients and 11,643 NCDB patients with osteosarcoma. After excluding patients with tumors outside the extremities/pelvis, including unconfirmed osteosarcoma histology results (52% [7989] SEER, 22% [2537] NCDB) and those with missing metastasis, treatment, or prognosis data (20% [2974] SEER, 43% [5057] NCDB), we included 4049 patients from NCDB and 4278 patients from SEER, all with confirmed osteosarcoma. SEER provides population-based coverage with detailed staging but limited treatment information, while NCDB offers hospital-based data with comprehensive treatment details. We developed separate models for each data set, randomly splitting each into training (80%) and validation (20%) sets. This separation was crucial because it allowed us to test how well our models performed on completely new, unseen data-to test whether a model will work in real-world clinical practice. Primary outcomes included accuracy (proportion of correct predictions), area under the receiver operating characteristic curve (AUC) (discriminative ability between survival outcomes, with values > 0.8 indicating good performance), Brier score (probabilistic prediction accuracy, with values < 0.25 indicating useful models), precision (proportion of positive predictions that were correct), recall (sensitivity for identifying actual outcomes), and F1 score (harmonic mean of precision and recall). The median patient age was 22 years in the NCDB versus 17 years in SEER (p = 0.005), with similar sex distributions (56% male in NCDB, 56% male in SEER) but different racial compositions and overall survival rates (72% and 52% at 2 and 5 years, respectively, for NCDB versus 65% and 43% for SEER). RESULTS: Internal validation showed excellent performance: NCDB-trained models achieved an AUC of 0.93 (95% confidence interval [CI] 0.92 to 0.94) at 2 years and 0.91 (95% CI 0.90 to 0.92) at 5 years, while SEER-trained models achieved 0.90 (95% CI 0.89 to 0.91) and 0.92 (95% CI 0.91 to 0.92), respectively. These AUC values > 0.90 indicate excellent discriminative ability; the models can reliably distinguish between patients who will survive and those who will not. The small differences between NCDB and SEER models (95% CI 0.90 to 0.93) are not clinically meaningful given overlapping confidence intervals. However, external validation revealed poor transferability: NCDB models tested on SEER data achieved an AUC of 0.67 (95% CI 0.65 to 0.68) and 0.60 (95% CI 0.58 to 0.62), while SEER models tested on NCDB data achieved 0.61 (95% CI 0.59 to 0.62) and 0.56 (95% CI 0.55 to 0.58). These external validation AUC values < 0.70 indicate poor predictive performance (barely better than chance and unsuitable for clinical decision-making). This dramatic performance drop demonstrates that models cannot be reliably transferred between different healthcare databases. NCDB models prioritized treatment variables, while SEER models emphasized demographic factors, reflecting the different clinical information captured by each database and explaining why cross-database application fails. CONCLUSION: Models should be validated within the same database environment where they will be applied. These results highlight differences between the NCDB and SEER data sets, showing that models learn database-specific patterns rather than generalizable disease patterns. Cross-database application of models leads to poor predictive performance and should be avoided without revalidation. LEVEL OF EVIDENCE: Level III, prognostic study.",
      "journal": "Clinical orthopaedics and related research",
      "year": "2026",
      "doi": "10.1097/CORR.0000000000003701",
      "authors": "Girgis Andrew G et al.",
      "keywords": "",
      "mesh_terms": "Humans; Osteosarcoma; SEER Program; Machine Learning; Bone Neoplasms; Male; Female; Databases, Factual; United States; Middle Aged; Adult; Young Adult; Adolescent; Child; Predictive Value of Tests; Aged; Prognosis; Bias",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41056069/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41072211",
      "title": "Use of artificial intelligence image generation to promote self-reflection and recognition of unconscious bias: A cross-sectional study of nursing students.",
      "abstract": "AIM: To determine the value of an artificial intelligence (AI)-image generation learning sequence on higher-education nursing student self-reflection and recognition of unconscious bias in the context of disability. BACKGROUND: Self-reflection and recognition of bias amongst undergraduate nursing students enhances reasoning skills and self-awareness in clinical situations. Teaching self-reflection to a diverse cohort can be challenging, making it essential to develop and assess innovative technological tools that support engagement in reflective practice. DESIGN: A multi-methods approach was adopted, obtaining both quantitative and qualitative data for analysis through a survey. METHODS: Twenty-nine nursing students from the Australian Catholic University were surveyed. Qualitative data underwent both content and inductive thematic analysis. Quantitative data were summarised using descriptive statistics. The study is reported according to the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) cross-sectional study guideline. RESULTS: AI-image generation aided self-reflection on personal views about disability and recognition of potential personal and society biases towards disability amongst 90\u202f% (n\u202f=\u202f26) and 70\u202f% of participants respectively. Visualisation of thoughts supported self-reflection and identification of generalisations held about disability. Eighty percent of respondents felt AI-image generation prompted them to consider how views and biases about disability may influence nursing practice. AI-image generation was identified to be an interesting and novel tool for self-reflection. CONCLUSION: Findings suggest AI-image generation may be a useful tool in supporting students to practice self-reflection and identify unconscious biases. AI-image generation may assist students to consider how personal views can impact on clinical practice.",
      "journal": "Nurse education in practice",
      "year": "2025",
      "doi": "10.1016/j.nepr.2025.104579",
      "authors": "Mullan Leanne et al.",
      "keywords": "AI; Artificial Intelligence; Higher education; Nursing; Self-reflection; Technology-enabled learning; University",
      "mesh_terms": "Humans; Students, Nursing; Cross-Sectional Studies; Artificial Intelligence; Male; Female; Surveys and Questionnaires; Education, Nursing, Baccalaureate; Adult; Qualitative Research; Australia; Young Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41072211/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41090693",
      "title": "Deep Learning-Enabled Unbiased Precision Toxicity Assessment of Zebrafish Organ Development.",
      "abstract": "Precise assessment of toxicological effects remains a key bottleneck in biomedical and environmental health assessments. Traditional toxicology relies on macroscopic end points and manual image analysis, which limit sensitivity to structural damage and introduce subjective bias. We developed an automated deep learning approach based on U-Net for the precise assessment of toxic effects and established a general framework for objective toxicological analysis. Our U-Net model can perform pixel-level segmentation and morphological quantification on thousands of biological images in 1 min without bias. This developed model was then applied to distinguish size-dependent developmental toxicity induced by Ag+, 15 nm, and 100 nm silver nanoparticles (AgNPs) in zebrafish, including the photoreceptor cell layer, inner plexiform layer, skeletal muscle, and spinal cord, which revealed previously undetectable size-dependent and organ-specific toxicity disparities that conventional analytical approaches failed to resolve. The method has the potential to be widely applied to the toxicity assessment of other emerging materials and contaminants. Our model displays great potential to improve toxicity assessment accuracy, efficiency, and reproducibility, providing a scalable application for precise toxicological assessments, including imaging analysis and standardization of assessment processes.",
      "journal": "Environmental science & technology",
      "year": "2025",
      "doi": "10.1021/acs.est.5c10763",
      "authors": "Wang Mengyu et al.",
      "keywords": "AgNPs; deep learning; environmental risk assessment; imaging; toxicity assessment",
      "mesh_terms": "Animals; Zebrafish; Deep Learning; Silver; Metal Nanoparticles; Toxicity Tests",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41090693/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41100248",
      "title": "FairFedMed: Benchmarking Group Fairness in Federated Medical Imaging with FairLoRA.",
      "abstract": "Fairness remains a critical concern in healthcare, where unequal access to services and treatment outcomes can adversely affect patient health. While Federated Learning (FL) presents a collaborative and privacy-preserving approach to model training, ensuring fairness is challenging due to heterogeneous data across institutions, and current research primarily addresses non-medical applications. To fill this gap, we establish the first experimental benchmark for fairness in medical FL, evaluating six representative FL methods across diverse demographic attributes and imaging modalities. We introduce FairFedMed, the first medical FL dataset specifically designed to study group fairness (i.e., consistent performance across demographic groups). It comprises two parts: FairFedMed-Oph, featuring 2D fundus and 3D OCT ophthalmology samples with six demographic attributes; and FairFedMed-Chest, which simulates real cross-institutional FL using subsets of CheXpert and MIMIC-CXR. Together, they support both simulated and real-world FL across diverse medical modalities and demographic groups. Existing FL models often underperform on medical images and overlook fairness across demographic groups. To address this, we propose FairLoRA, a fairness-aware FL framework based on SVD-based low-rank approximation. It customizes singular value matrices per demographic group while sharing singular vectors, ensuring both fairness and efficiency. Experimental results on the FairFedMed dataset demonstrate that FairLoRA not only achieves state-of-the-art performance in medical image classification but also significantly improves fairness across diverse populations. Our code and dataset can be accessible via GitHub link: https://github.com/Harvard-AI-and-Robotics-Lab/FairFedMed.",
      "journal": "IEEE transactions on medical imaging",
      "year": "2025",
      "doi": "10.1109/TMI.2025.3622522",
      "authors": "Li Minghan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41100248/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41104854",
      "title": "Role of artificial intelligence in cognitive debiasing within clinical decision-making.",
      "abstract": "",
      "journal": "Minerva medica",
      "year": "2025",
      "doi": "10.23736/S0026-4806.25.09831-3",
      "authors": "Volpe Giovanni et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41104854/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41106227",
      "title": "Development and evaluation of neighborhood social risk indices for surgery using outcome-specific machine-learning models.",
      "abstract": "BACKGROUND: Social risk factors, which influence health outcomes, are used in measures applied to federal payment adjustment models and in health disparities research for risk adjusting outcomes. However, existing composite measures of social risk have notable limitations in predicting specific health outcomes. METHODS: Using data from the American College of Surgeons National Surgical Quality Improvement Program hospitals, this study constructed machine learning-based social risk indices, comprised of the underlying US Census data components used in the Area Deprivation Index, a tool developed to measure neighborhood-level disadvantage. Social risk components were tuned to surgical outcomes to assess how social risk influences specific outcomes. RESULTS: In this study of 3,206,836 patients from 688 US hospitals, models using machine learning-based indices outperformed models using the Area Deprivation Index. Machine learning-based models had greater predictive power for all 14 outcome models than models using the Area Deprivation Index directly (median 8.15-fold increase in standardized parameter estimates). Mean calculated contributions of each underlying Area Deprivation Index component used in the machine learning-based indices varied across the 14 outcomes, providing insight into social risk factors that contributed the most to specific postoperative outcomes. CONCLUSION: The machine learning-based indices developed in this study provide social risk assessments with higher informational value and may function as a more reliable measure in health equity risk adjustments than existing tools currently used for health care payment adjustments.",
      "journal": "Surgery",
      "year": "2025",
      "doi": "10.1016/j.surg.2025.109716",
      "authors": "Liu Jessica K et al.",
      "keywords": "",
      "mesh_terms": "Humans; Machine Learning; Male; Female; United States; Middle Aged; Surgical Procedures, Operative; Risk Assessment; Risk Factors; Neighborhood Characteristics; Aged; Social Determinants of Health",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41106227/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41106548",
      "title": "Evaluating Long-Term Health Disparity Impacts of Clinical Algorithms Using a Patient-Level Simulation Framework.",
      "abstract": "OBJECTIVES: This study applies a simulation framework to evaluate the long-term effects of omitting race from a colon cancer decision algorithm for adjuvant chemotherapy, assessing impacts on health outcomes, costs, and disparities while accounting for measurement errors across racial groups. METHODS: We developed a patient-level state-transition model using electronic health records from a large Southern California health system to project outcomes for 4839 adults with stage II and III colon cancer after surgery. We compared 30-year quality-adjusted life-years (QALYs), healthcare costs, and QALY distribution among racial groups under 3 chemotherapy treatment scenarios: (1) current practice, (2) treatment guided by an algorithm that includes race, and (3) the same algorithm with race omitted. An additional health state addressed racial bias in cancer recurrence ascertainment, and probabilistic sensitivity analysis (PSA) assessed uncertainty. RESULTS: The clinical algorithm, compared with current practice, could improve average health by 0.048 QALYs and reduce racial health disparity by 0.20 QALYs at an incremental cost of $3221, with the disparity gap decreasing in 96% of PSA iterations. Omitting race showed minimal effects on overall health or costs but resulted in 13% fewer Black patients receiving treatment, decreasing their QALYs by 0.07 and widening the disparity gap by 0.13 QALY. Health disparity increased in 94% of PSA iterations. CONCLUSIONS: A cancer decision algorithm can improve population health and reduce health disparities, but omitting race may harm disadvantaged groups and limit reductions in disparities. Patient-level simulations can be routinely used to evaluate the potential health disparity impacts of algorithms before implementation.",
      "journal": "Value in health : the journal of the International Society for Pharmacoeconomics and Outcomes Research",
      "year": "2025",
      "doi": "10.1016/j.jval.2025.09.3066",
      "authors": "Khor Sara et al.",
      "keywords": "clinical algorithms; health disparity; microsimulation; patient-level simulation; racial disparity",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41106548/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41108716",
      "title": "Comparative analysis of human-generated versus Artificial Intelligence-drafted summary paragraphs for medical student performance evaluations.",
      "abstract": "PURPOSE: This study evaluated the efficiency and effectiveness of using Generative Artificial Intelligence (GenAI) to draft Medical Student Performance Evaluation (MSPE) summary paragraphs for medical students. MATERIALS AND METHODS: Evaluations on the pediatrics clerkship were used to develop MSPE summary paragraphs. Time to completion was noted for paragraphs drafted by GenAI, created using Microsoft 365 Copilot, and compared to human-generated. Undergraduate Medical Education (UME) leaders were recruited to evaluate 10 randomized pairs of paragraphs through a blinded survey. RESULTS: Copilot-drafted paragraphs required significantly less time to completion compared to human-generated paragraphs (median 6 vs. 12.5\u2009min, p\u2009=\u20090.002). UME leaders showed no significant preference and were unable to consistently identify Copilot vs human authorship. When stratified by perception of authorship, human-generated paragraphs were significantly less likely to be preferred if they were perceived as being Copilot-drafted than if they were perceived as being human-generated (p\u2009=\u20090.017), suggesting an element of anti-AI bias. Competencies were highlighted to a similar degree, and Copilot-drafted paragraphs were perceived as having significantly less biased language by both UME leaders (p\u2009=\u20090.004) and an independent analysis using a validated gender bias calculator (p\u2009=\u20090.029). CONCLUSIONS: Copilot-drafted MSPE summaries are efficient, comparable in quality, and may reduce the introduction of bias.",
      "journal": "Medical teacher",
      "year": "2025",
      "doi": "10.1080/0142159X.2025.2574382",
      "authors": "Maheshwari Atul et al.",
      "keywords": "Artificial Intelligence; Medical student performance evaluations; bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41108716/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41110892",
      "title": "Considerations for fairness and practicality in deep learning model predicting treatment response in lupus nephritis.",
      "abstract": "",
      "journal": "Kidney international",
      "year": "2025",
      "doi": "10.1016/j.kint.2025.07.016",
      "authors": "Liu Shuyi et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41110892/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41126537",
      "title": "Balancing Bias and Variance in Deep Learning-Based Tumor Microstructural Parameter Mapping.",
      "abstract": "PURPOSE: Time-dependent diffusion MRI enables quantification of tumor microstructural parameters useful for diagnosis and prognosis. Nevertheless, current model fitting approaches exhibit suboptimal bias-variance trade-offs; specifically, nonlinear least squares fitting (NLLS) demonstrated low bias but high variance, whereas supervised deep learning methods trained with mean squared error loss (MSE-Net) yielded low variance but elevated bias. This study investigates these bias-variance characteristics and proposes a method to control fitting bias and variance. METHODS: Random walk with barrier model was used as a representative biophysical model. NLLS and MSE-Net were reformulated within the Bayesian framework to elucidate their bias-variance behaviors. We introduced B2V-Net, a supervised learning approach using a loss function with adjustable bias-variance weighting, to control bias-variance trade-off. B2V-Net was evaluated and compared against NLLS and MSE-Net numerically across a wide range of parameters and noise levels, as well as in vivo in patients with head and neck cancer. RESULTS: Flat posterior distributions that were not centered at ground truth parameters explained the bias-variance behaviors of NLLS and MSE-Net. B2V-Net controlled the bias-variance trade-off, achieving a 56% reduction in standard deviation relative to NLLS and an 18% reduction in bias compared to MSE-Net. In vivo parameter maps from B2V-Net demonstrated a balance between smoothness and accuracy. CONCLUSION: We demonstrated and explained the low bias-high variance of NLLS and the low variance-high bias of MSE-Net. The proposed B2V-Net can balance bias and variance. Our work provided insights and methods to guide the design of customized loss functions tailored to specific clinical imaging needs.",
      "journal": "Magnetic resonance in medicine",
      "year": "2026",
      "doi": "10.1002/mrm.70154",
      "authors": "Zou Jiaren et al.",
      "keywords": "deep learning; diffusion MRI; head and neck cancers; model fitting; tissue microstructure",
      "mesh_terms": "Humans; Deep Learning; Head and Neck Neoplasms; Algorithms; Bayes Theorem; Diffusion Magnetic Resonance Imaging; Image Processing, Computer-Assisted; Image Interpretation, Computer-Assisted; Least-Squares Analysis; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41126537/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41172593",
      "title": "CIMB-MVQA: Causal intervention on modality-specific biases for medical visual question answering.",
      "abstract": "Medical Visual Question Answering (Med-VQA) systems frequently rely on spurious visual and language cues produced by dataset biases and structural con-founders, which undermines robustness and real-world generalization. To alleviate spurious cue reliance attributable to particular confounders, we propose CIMB-MVQA, a framework for Causal Intervention on Modality-specific Biases, which suppresses cross-modal bias by explicitly modeling and adjusting for confounding factors. For unobservable visual confounders, we introduce a front-door adjustment pipeline combining contrastive representation learning, feature disentanglement, and dual semantic masking to eliminate co-occurring but non-causal visual patterns. For observable linguistic confounders, we apply a back-door adjustment strategy using a global language bias dictionary to detect spurious signals. A vision-guided pseudo-token injection mechanism is further designed to embed critical visual cues into the language stream, reducing language dominance and aligning causal semantics across modalities. This is followed by a causal graph reasoning module that explicitly intervenes in bias-inducing paths. Experiments on multiple Med-VQA benchmarks demonstrate that CIMB-MVQA significantly improves answer accuracy and causal interpretability. Additionally, on the curated imbalanced VQA-RAD* and a suite of controlled-shift datasets, confounder-level experiments consistently show robust causal generalization under realistic bias conditions. The source code is publicly available at https://github.com/cloneiq/CIMB-MVQA.",
      "journal": "Medical image analysis",
      "year": "2026",
      "doi": "10.1016/j.media.2025.103850",
      "authors": "Liu Bing et al.",
      "keywords": "Causal inference; Causal intervention; Medical visual question answering; Multimodal bias mitigation",
      "mesh_terms": "Algorithms; Cues; Language; Semantics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41172593/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41202615",
      "title": "FairREAD: Re-fusing demographic attributes after disentanglement for fair medical image classification.",
      "abstract": "Recent advancements in deep learning have shown transformative potential in medical imaging, yet concerns about fairness persist due to performance disparities across demographic subgroups. Existing methods aim to address these biases by mitigating sensitive attributes in image data; however, these attributes often carry clinically relevant information, and their removal can compromise model performance-a highly undesirable outcome. To address this challenge, we propose Fair Re-fusion After Disentanglement (FairREAD), a novel, simple, and efficient framework that mitigates unfairness by re-integrating sensitive demographic attributes into fair image representations. FairREAD employs orthogonality constraints and adversarial training to disentangle demographic information while using a controlled re-fusion mechanism to preserve clinically relevant details. Additionally, subgroup-specific threshold adjustments ensure equitable performance across demographic groups. Comprehensive evaluations and out-of-distribution testing on large-scale clinical X-ray datasets demonstrate that, given demographic attributes of each patient, FairREAD is able to significantly reduce unfairness metrics while maintaining diagnostic accuracy. Our code is available at: https://github.com/Advanced-AI-in-Medicine-and-Physics-Lab/FairREAD/.",
      "journal": "Medical image analysis",
      "year": "2026",
      "doi": "10.1016/j.media.2025.103858",
      "authors": "Gao Yicheng et al.",
      "keywords": "Bias mitigation; Chest X-ray; Disentanglement; Fairness",
      "mesh_terms": "Humans; Deep Learning; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41202615/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41239120",
      "title": "Comments on: Developing and Applying the BE\u2011FAIR Equity Framework to a Population Health Predictive Model: A Retrospective Observational Cohort Study.",
      "abstract": "",
      "journal": "Journal of general internal medicine",
      "year": "2025",
      "doi": "10.1007/s11606-025-09974-w",
      "authors": "Triwiyanto Triwiyanto et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41239120/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41276758",
      "title": "Low Risk, High Stakes: Factors Associated with Low COVID-19 Risk Perception Among Black Chicago Residents.",
      "abstract": "Black Americans consistently face dramatic health disparities throughout the United Status. The COVID-19 pandemic disproportionately impacted Black communities, leading to disparities in hospitalizations and death. Using a health equity lens, this study aimed to explore factors associated with COVID-19 risk perception, including the role of medical mistrust and experience of racism and discrimination among Black adults in Chicago. Survey data was collected among 538 Black Chicago residents between September 2021 and March 2022. We tested predictors of low-risk perception outcomes (i.e., the strong disbelief that one will get COVID-19 in the next three months and not being worried about getting COVID-19) utilizing a backward stepwise algorithm. Sociodemographic variables (gender-age, income) were significant predictors of both risk perception outcomes. Participants who stated that they very closely followed the recommendation to wear a face mask were significantly more likely to have the strong disbelief they would get COVID-19 within the next three months and those who experienced greater racial discrimination were significantly less likely to have the strong disbelief. Results also showed that individuals who were classified as non-essential employees or had an unknown work status, had fewer medical conditions, and closely adhered to socially distance guidelines were significantly more likely to not be worried about getting COVID-19. Findings add to ongoing literature on COVID-19 perceptions and predictors of COVID-19 risk perception and highlight the need for further research on the influence of individual, social and structural factors on risk perceptions.",
      "journal": "Journal of racial and ethnic health disparities",
      "year": "2025",
      "doi": "10.1007/s40615-025-02733-x",
      "authors": "Neal Melissa L et al.",
      "keywords": "COVID-19; Health disparities; Medical mistrust; Racism; Risk perception; Vaccination",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41276758/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41283808",
      "title": "AI chatbots in the PICU: parental enthusiasm contrasts with socioeconomic usage disparities.",
      "abstract": "Parents of children admitted to the PICU face an overwhelming informational landscape, necessitating accessible, patient-specific information. Large Language Models (LLMs) powering AI chatbots offer a promising solution for simplifying complex medical information. We aimed to characterize parental online health information-seeking (OHIS) behaviors and attitudes toward AI chatbots by conducting a cross-sectional survey of 139 English-speaking parents of children admitted to a large academic PICU between April-August 2024. We assessed OHIS behaviors, knowledge of and experience with AI chatbots, and attitudes regarding their potential healthcare utility. Most parents (87%) engaged in OHIS using search engines (86%). Parents with higher income and education sought information more frequently (OR 3.3, 95% CI 1.8-6.2; OR 2.9, 95% CI 1.5-5.7, respectively); those with higher education were less satisfied with online resources (OR 0.5, 95% CI 0.25-0.97). Parents expressed openness toward AI chatbots in healthcare applications (median 4/6). Significant socioeconomic disparities in current AI chatbot use favored male (OR 2.5, 95% CI 1.1-6.0) and higher income (OR 3.8, 95% CI 1.1-12.7) parents. Parents of critically ill children show high OHIS behaviors and positive attitudes toward AI chatbots. Addressing significant socioeconomic disparities in AI chatbot use is crucial for developing equitable implementation strategies in the PICU.",
      "journal": "Informatics for health & social care",
      "year": "2026",
      "doi": "10.1080/17538157.2025.2589195",
      "authors": "Hunter R Brandon et al.",
      "keywords": "Artificial intelligence; large language models; online health information seeking; patient education; pediatric intensive care unit",
      "mesh_terms": "Humans; Parents; Male; Intensive Care Units, Pediatric; Female; Cross-Sectional Studies; Socioeconomic Factors; Information Seeking Behavior; Adult; Child; Child, Preschool; Surveys and Questionnaires; Middle Aged; Generative Artificial Intelligence",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41283808/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41297683",
      "title": "Chloroplast structure, codon usage bias, and machine learning-based molecular identification using DNA barcoding of Sophorae Tonkinensis Radix et Rhizoma (Shan Dou Gen) and its analogues.",
      "abstract": "To investigate the complete chloroplast genome sequences and codon usage bias of \"Shan Dou Gen\" (Sophorae Tonkinensis Radix et Rhizoma) and its six easily confused species, analyze their evolutionary patterns, and evaluate the identification efficiency of four DNA barcodes combined with two machine learning methods for these seven plant species. Chloroplast gene structures of the seven species were aligned to construct phylogenetic trees. Codon usage bias was analyzed using CodonW and CUSP. Genetic distances were calculated based on the Kimura-2-Parameter model to assess the barcoding gap and reconstruct phylogenetic trees.Species discrimination capabilities of four DNA barcodes (ITS2, matK, psbA-trnH, and rbcL) were compared. Species identification was performed using BLOG and WEKA machine learning algorithms. Single-nucleotide SSRs predominated in chloroplast genomes, primarily composed of A/T bases. Complete species differentiation was achieved using cpDNA. Natural selection was the primary factor influencing codon usage bias, followed by mutation pressure. Among synonymous codons, A\u00a0>\u00a0T and G\u00a0>\u00a0C in base composition, with optimal codons ending in A/U at the third position across all seven species. All four DNA barcodes successfully discriminated Shandougen from its confusable species. The BLOG algorithm achieved >85\u00a0% identification accuracy, outperforming WEKA. This research provides a theoretical foundation for ensuring clinical medication safety, elucidating plant phylogeny, facilitating species identification, and guiding resource conservation and utilization of Shandougen and its analogues.",
      "journal": "Fitoterapia",
      "year": "2026",
      "doi": "10.1016/j.fitote.2025.107005",
      "authors": "Zheng Mengdi et al.",
      "keywords": "Chloroplast genome; Codon bias; Machine learning; Molecular identification; Shan Dou Gen",
      "mesh_terms": "DNA Barcoding, Taxonomic; Codon Usage; Machine Learning; Phylogeny; Genome, Chloroplast; Chloroplasts",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41297683/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41336319",
      "title": "Comparative assessment of fairness definitions and bias mitigation strategies in machine learning-based diagnosis of Alzheimer's disease from MR images.",
      "abstract": "The present study performs a comprehensive fairness analysis of machine learning (ML) models for the diagnosis of Mild Cognitive Impairment (MCI) and Alzheimer's disease (AD) from MRI-derived neuroimaging features. Biases associated with age, race, and gender in a multi-cohort dataset, as well as the influence of proxy features encoding these sensitive attributes, are investigated. The reliability of various fairness definitions and metrics in the identification of such biases is also assessed. Based on the most appropriate fairness measures, a comparative analysis of widely used pre-processing, in-processing, and post-processing bias mitigation strategies is performed. Moreover, a novel composite measure is introduced to quantify the trade-off between fairness and performance by considering the F1-score and the equalized odds ratio, making it appropriate for medical diagnostic applications. The obtained results reveal the existence of biases related to age and race, while no significant gender bias is observed. The deployed mitigation strategies yield varying improvements in terms of fairness across the different sensitive attributes and studied subproblems. For race and gender, Reject Option Classification improves equalized odds by 46% and 57%, respectively, and achieves harmonic mean scores of 0.75 and 0.80 in the MCI versus AD subproblem, whereas for age, in the same subproblem, adversarial debiasing yields the highest equalized odds improvement of 40% with a harmonic mean score of 0.69. Insights are provided into how variations in AD neuropathology and risk factors, associated with demographic characteristics, influence model fairness.",
      "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference",
      "year": "2025",
      "doi": "10.1109/EMBC58623.2025.11254128",
      "authors": "Vlontzou Maria Eleftheria et al.",
      "keywords": "",
      "mesh_terms": "Alzheimer Disease; Humans; Machine Learning; Magnetic Resonance Imaging; Male; Female; Aged; Cognitive Dysfunction; Bias; Aged, 80 and over",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41336319/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41336568",
      "title": "ScorER: Exploring Annotation Bias in Vision-Based Neonatal Pain Assessment.",
      "abstract": "Neonatal pain can impair brain development, highlighting the critical clinical importance of accurate pain assessment. In recent years, automated neonatal pain assessment systems have received substantial research attention. However, their advancement is hindered by challenges in data collection and annotation. Vision-based neonatal pain annotation is convenient but susceptible to real-world disturbances that may result in information loss and incorrect annotations. In this study, we propose a deep neural network-based tool, Scoring Error Recognition (ScorER), which aims to automatic identification and intelligent diagnosis of vision-based annotation bias. By integrating the Grad-CAM algorithm, we systematically identify key regions associated with annotation bias and employ a data-driven approach to investigate their underlying causes. This research proposes a novel approach to improve annotation quality and provides new insights for advancing automated neonatal pain assessment systems in future studies.Clinical relevance- ScorER can assist caregivers in achieving high-quality vision-based neonatal pain annotations, supporting the development of large-scale, high-quality clinical neonatal pain databases for real-world applications. This facilitates AI-driven automated neonatal pain assessment research and system development for precise neonatal pain management.",
      "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference",
      "year": "2025",
      "doi": "10.1109/EMBC58623.2025.11253405",
      "authors": "Ni Yuxin et al.",
      "keywords": "",
      "mesh_terms": "Humans; Infant, Newborn; Pain Measurement; Algorithms; Pain; Neural Networks, Computer",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41336568/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41336889",
      "title": "Enhancing Fairness in Ultrasound Imaging: Evaluating Adversarial Debiasing Across Diverse Patient Demographics.",
      "abstract": "This paper explores the use of adversarial debiasing algorithms to mitigate bias in ultrasound imaging datasets, focusing on breast and lung images. The study evaluates fairness metrics like area under the curve (AUC), False Positive Rate (FPR), False Negative Rate (FNR), and demographic parity to assess the impact of debiasing using the recently published MEDFAIR framework. While debiasing improves fairness overall, disparities remain in certain subgroups, such as age in the breast dataset and sex in the lung dataset. The paper also compares artificial intelligence (AI) models (ResNet18, AlexNet, VGG16, MobileNetV2, DenseNet121), revealing differences in susceptibility to bias and effectiveness post-debiasing. These findings underscore the challenges of achieving full fairness in AI-driven medical imaging and highlight the need for continued refinement of debiasing methods to ensure equitable outcomes across diverse patient populations.",
      "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference",
      "year": "2025",
      "doi": "10.1109/EMBC58623.2025.11252877",
      "authors": "Kiani Parmiss et al.",
      "keywords": "",
      "mesh_terms": "Humans; Algorithms; Ultrasonography; Female; Artificial Intelligence; Male; Lung; Demography; Image Processing, Computer-Assisted; Breast",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41336889/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41337389",
      "title": "FairCauseSyn: Towards Causally Fair LLM-augmented Synthetic Data Generation.",
      "abstract": "Synthetic data generation creates data based on real-world data using generative models. In health applications, generating high-quality data while maintaining fairness for sensitive attributes is essential for equitable outcomes. Existing GAN-based and LLM-based methods focus on counterfactual fairness and are primarily applied in finance and legal domains. Causal fairness provides a more comprehensive evaluation framework by preserving causal structure, but current synthetic data generation methods do not address it in health settings. To fill this gap, we develop the first LLM-augmented synthetic data generation method to enhance causal fairness using real-world tabular health data. Our generated data deviates by less than 10% from real data on causal fairness metrics. When trained on causally fair predictors, synthetic data reduces bias on the sensitive attribute by 70% compared to real data. This work improves access to fair synthetic data, supporting equitable health research and healthcare delivery.",
      "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference",
      "year": "2025",
      "doi": "10.1109/EMBC58623.2025.11252705",
      "authors": "Nagesh Nitish et al.",
      "keywords": "",
      "mesh_terms": "Humans; Algorithms; Data Analysis; Large Language Models; Prediction Methods, Machine",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41337389/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41339188",
      "title": "Rethinking fairness in AI to improve current practice in oncology.",
      "abstract": "Fairness in artificial intelligence (AI) is often assessed with flawed metrics, particularly in oncology where patient diversity and structural inequities shape outcomes. Ground truth labels, predictions, and demographic attributes all carry biases that distort fairness evaluations. We argue for rethinking fairness frameworks to better capture equity in cancer care.",
      "journal": "Trends in cancer",
      "year": "2026",
      "doi": "10.1016/j.trecan.2025.11.005",
      "authors": "Konate Salamata et al.",
      "keywords": "",
      "mesh_terms": "Humans; Medical Oncology; Artificial Intelligence; Bias; Health Equity; Healthcare Disparities",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41339188/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41354138",
      "title": "From Clinical Trials to Real-World Impact: Introducing a Computational Framework to Detect Endpoint Bias in Opioid Use Disorder Research.",
      "abstract": "INTRODUCTION: Clinical trial endpoints are a 'finite sequence of instructions to perform a task' (measure treatment effectiveness), making them algorithms. Consequently, they may exhibit algorithmic bias: internal and external performance can vary across demographic groups, impacting fairness, validity and clinical decision-making. METHODS: We developed the open-source Detecting Algorithmic Bias (DAB) Pipeline in Python to identify endpoint 'performance variance'-a specific algorithmic bias-as the proportion of minority participants changes. This pipeline assesses internal performance (on demographically matched test data) and external performance (on demographically diverse validation data) using metrics including F1 scores and area under the receiver operating characteristic curve (AUROC). We applied it to representative opioid use disorder (OUD) trial endpoints. RESULTS: F1 scores remained stable across minority representation levels, suggesting consistency in precision-recall balance (F1) despite demographic shifts. Conversely, AUROC measures were more sensitive, revealing significant performance variance. Training on demographically homogeneous populations boosted internal performance (accuracy within similar cohorts) but critically compromised external generalisability (accuracy within diverse cohorts). This pattern reveals an 'endpoint bias trade-off': optimising performance for homogeneous populations vs. having generalisable performance for the real world. DISCUSSION AND CONCLUSIONS: Consistently performing endpoints for one demographic profile may lose generalisability during population shifts, potentially introducing endpoint bias. Increasing minority representation in the training data consistently improved generalisability. The endpoint bias trade-off reinforces the importance of diverse recruitment in OUD trials. The DAB Pipeline helps researchers systematically pinpoint when an endpoint may suffer 'performance variance' (i.e., bias). As an open-source tool, it promotes transparent endpoint evaluation and supports selecting demographically invariant OUD endpoints.",
      "journal": "Drug and alcohol review",
      "year": "2026",
      "doi": "10.1111/dar.70085",
      "authors": "Odom Gabriel J et al.",
      "keywords": "algorithmic bias; demographic parity; open\u2010source software; opioid use disorder; performance variance",
      "mesh_terms": "Humans; Opioid-Related Disorders; Clinical Trials as Topic; Algorithms; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41354138/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41354230",
      "title": "Development and validation of a novel machine learning-based algorithm to predict incident atrial fibrillation: A multicohort analysis.",
      "abstract": "BACKGROUND: Existing atrial fibrillation (AF) risk prediction models incorporate race as a covariate, systematically underestimating AF risk in black individuals and potentially perpetuating health care disparities. OBJECTIVE: This study aimed to develop and validate machine learning (ML)-based race-agnostic risk scores to predict AF risk and assess differences in risk stratification and bias compared with the CHARGE-AF score. METHODS: The derivation cohort included 16,719 participants free of AF at baseline (Atherosclerosis Risk in Communities visit 5, 2011-2013; Cardiovascular Health Study baseline, 1989-1990), and the validation cohort included 13,928 (Multi-Ethnic Study of Atherosclerosis and Framingham Offspring and Generation 3 studies). The primary outcome was the incidence of AF within 5 years. Model performance was assessed using concordance index, Brier score, and index of prediction accuracy. Bias was evaluated using disparate impact, equal opportunity difference, and Theil index. Population-attributable risk percentage was calculated across racial groups. RESULTS: During the 5-year follow-up, incident AF occurred in 507 participants (3.0%) in the derivation cohort and 262 (1.9%) in the validation cohort. The ML model demonstrated superior performance compared with CHARGE-AF, with better discrimination (concordance index 0.83 [95% confidence interval 0.80-0.85] vs 0.77 [95% confidence interval 0.74-0.79]; P < .001) and improved calibration (Brier score 1.82 vs 1.92; P < .001). Key predictors included age, clinical factors (electrocardiographic parameters, cardiac biomarkers, and blood pressure), and education level. Population-attributable risk analysis demonstrated marked racial differences in AF risk contribution from age (non-Hispanic black 14.3% vs white participants 34.6%). The ML model reduced algorithmic bias vs CHARGE-AF across all metrics. CONCLUSION: Race-agnostic ML models demonstrated superior predictive performance and reduced bias compared with CHARGE-AF, potentially improving clinical risk stratification while promoting health equity.",
      "journal": "Heart rhythm",
      "year": "2025",
      "doi": "10.1016/j.hrthm.2025.12.008",
      "authors": "Segar Matthew W et al.",
      "keywords": "Algorithmic bias; Atrial fibrillation; Machine learning; Race; Risk prediction",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41354230/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41359739",
      "title": "Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields in Efficient CNNs for Fair Medical Image Classification.",
      "abstract": "Efficient convolutional neural network (CNN) architecture design has attracted growing research interests. However, they typically apply single receptive field (RF), small asymmetric RFs, or pyramid RFs to learn different feature representations, still encountering two significant challenges in medical image classification tasks: i) They have limitations in capturing diverse lesion characteristics efficiently, e.g., tiny, coordination, small and salient, which have unique roles on the classification results, especially imbalanced medical image classification. ii) The predictions generated by those CNNs are often unfair/biased, bringing a high risk when employing them to real-world medical diagnosis conditions. To tackle these issues, we develop a new concept, Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields (ERoHPRF), to simultaneously boost medical image classification performance and fairness. This concept aims to mimic the multi-expert consultation mode by applying the well-designed heterogeneous pyramid RF bag to capture lesion characteristics with varying significances effectively via convolution operations with multiple heterogeneous kernel sizes. Additionally, ERoHPRF introduces an expertlike structural reparameterization technique to merge its parameters with the two-stage strategy, ensuring competitive computation cost and inference speed through comparisons to a single RF. To manifest the effectiveness and generalization ability of ERoHPRF, we incorporate it into mainstream efficient CNN architectures. The extensive experiments show that our proposed ERoHPRF maintains a better trade-off than state-of-the-art methods in terms of medical image classification, fairness, and computation overhead. The code of this paper is available at https://github.com/XiaoLing12138/Expert-Like-Reparameterization-of-Heterogeneous-Pyramid-Receptive-Fields.",
      "journal": "IEEE transactions on medical imaging",
      "year": "2025",
      "doi": "10.1109/TMI.2025.3641192",
      "authors": "Wu Xiao et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41359739/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41380307",
      "title": "TransFair: Transferring fairness from ocular disease classification to progression prediction.",
      "abstract": "The use of artificial intelligence (AI) in automated disease classification significantly reduces healthcare costs and improves the accessibility of services. However, this transformation has given rise to concerns about the fairness of AI, which disproportionately affects certain groups, particularly patients from underprivileged populations. Recently, a number of methods and large-scale datasets have been proposed to address group performance disparities. Although these methods have shown effectiveness in disease classification tasks, they may fall short in ensuring fair prediction of disease progression, mainly because of limited longitudinal data with diverse demographics available for training a robust and equitable prediction model. In this paper, we introduce TransFair to enhance demographic fairness in progression prediction for ocular diseases. TransFair aims to transfer a fairness-enhanced disease classification model to the task of progression prediction with fairness preserved. First, we train a fairness-aware EfficientNet called FairEN using extensive data for ocular disease classification. Subsequently, this fair classification model is adapted to a fair progression prediction model through knowledge distillation, which minimizes the latent feature distances between classification and progression prediction models. We evaluate FairEN and TransFair for fairness-enhanced ocular disease classification and progression prediction using both two-dimensional (2D) and 3D retinal images. Extensive experiments and comparisons show that TransFair enhances group fairness in predicting ocular disease progression.",
      "journal": "Artificial intelligence in medicine",
      "year": "2026",
      "doi": "10.1016/j.artmed.2025.103331",
      "authors": "Shi Min et al.",
      "keywords": "AI fairness; Disease progression; OCT B-scans; Ocular disease; RNFLT maps",
      "mesh_terms": "Humans; Disease Progression; Artificial Intelligence; Eye Diseases",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41380307/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41386070",
      "title": "Exploring hospital staff experiences and requirements for safer and more equitable EHR-integrated medication management system: A qualitative study.",
      "abstract": "BACKGROUND: Medication-related harm remains a global challenge, causing preventable illness, mortality, and cost. Electronic health records (EHRs) and electronic medication management systems (eMMS) aim to improve safety through standardisation and decision support, yet persistent usability issues, fragmented workflows, and limited consideration of equity continue to compromise outcomes, particularly for Indigenous, older, and disabled populations. AIM: To explore how doctors, nurses, pharmacists, digital health professionals, and organisational leaders experience EHR-integrated medication systems with particular attention to medication safety, workflow, and inequities. METHODS: An exploratory descriptive qualitative study was conducted at a large tertiary hospital in New Zealand (January - February 2025). Data were analysed using Braun and Clarke's reflexive thematic analysis, guided by User-Centred Design, the Sociotechnical Model, the Digital Health Equity Framework, and the M\u0101ori wellbeing model Te Whare Tapa Wh\u0101. Rigour was supported through reflexive journaling, supervisory review, and COREQ adherence. RESULTS: Thirteen semi-structured interviews and two focus groups were held with 22 participants. Three overlapping themes were identified. System design and organisational factors influencing medication safety showed how complex interfaces, rigid automation, and unreliable infrastructure increased cognitive load and required vigilance. Limited recognition of cultural and social needs in eMMS, revealed that missing fields for language, disability, and family context constrained culturally safe communication and equitable care. Design priorities for safer, more inclusive systems captured participants' vision of adaptive decision support, integrated data views, and embedded equity features such as interpreter prompts, disability and family engagement fields, and co-design to ensure systems evolve with clinical and cultural practice. CONCLUSIONS: Improving medication safety requires more than digital transformation of medication processes. Integrating user-centred design, sociotechnical awareness, and equity principles can create intelligent, context-aware systems that support, rather than replace, clinical judgement, empathy, and human connection in patient care.",
      "journal": "International journal of medical informatics",
      "year": "2026",
      "doi": "10.1016/j.ijmedinf.2025.106222",
      "authors": "Murthi Sreyon et al.",
      "keywords": "Clinical decision support; Electronic health records; Health equity; Medication safety; User experience",
      "mesh_terms": "Humans; Qualitative Research; Electronic Health Records; New Zealand; Medication Errors; Female; Male; Medication Systems, Hospital; Personnel, Hospital; Attitude of Health Personnel; Patient Safety; Adult; Focus Groups",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41386070/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41397397",
      "title": "Counterfactual fairness for small subgroups.",
      "abstract": "While methods for measuring and correcting differential performance in risk prediction models have proliferated in recent years, most existing techniques can only be used to assess fairness across relatively large subgroups. The purpose of algorithmic fairness efforts is often to redress discrimination against groups that are both marginalized and small, so this sample size limitation can prevent existing techniques from accomplishing their main aim. In clinical applications, this challenge combines with statistical issues that arise when models are used to guide treatment. We take a 3-step approach to addressing both of these challenges, building on the \"counterfactual fairness\" framework that accounts for confounding by treatment. First, we propose new estimands that leverage information across groups. Second, we estimate these quantities using a larger volume of data than existing techniques. Finally, we propose a novel data borrowing approach to incorporate \"external data\" that lacks outcomes and predictions but contains covariate and group membership information. We demonstrate application of our estimators to a risk prediction model used by a major Midwestern health system during the coronavirus disease 2019 (COVID-19) pandemic.",
      "journal": "Biostatistics (Oxford, England)",
      "year": "2024",
      "doi": "10.1093/biostatistics/kxaf046",
      "authors": "Wastvedt Solvejg et al.",
      "keywords": "algorithmic fairness; causal inference; risk prediction; small subgroups",
      "mesh_terms": "Humans; COVID-19; Models, Statistical; Risk Assessment; Sample Size; SARS-CoV-2",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41397397/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41443971",
      "title": "A call for ethical, equitable, and effective artificial intelligence to improve care for all people with epilepsy: A\u00a0roadmap. A report by the ILAE Global Advocacy Council and Big Data Commission.",
      "abstract": "The artificial intelligence (AI) revolution is upon us. It will inevitably form a central component of epilepsy workflows and patient advocacy. Therefore, it behooves us as health care providers to ride the crest of this wave and guide its direction for the benefit of all people with epilepsy. Emerging AI-based solutions include decision support tools, automated interpretation of electroencephalography (EEG) and brain imaging, and wearable devices that detect seizures and improve patient safety. Pipelines, including decentralized approaches and federated learning, are now being built that will democratize access and facilitate the next generation of AI tools for the global epilepsy community. Despite this, enduring issues remain incompletely addressed. For example, AI requires high volumes of data, leading to concerns about ethical ownership, stewardship, and privacy. Few AI-based tools have progressed from derivation to validation stages, and only rare exceptions undergo real-world evaluation. Inadvertent harmful algorithmic and decision allocation biases also continue to represent major risks to the global epilepsy population. Additional barriers include geographical disparities in computing resources, proprietary ownership of electronic health records, EEG, and brain-imaging platforms, and greenhouse gas emissions related to the demanding power requirements of AI. Therefore, to fully avail ourselves of the benefits of AI, we assert that ethical, equitable, and effective AI for epilepsy requires collaboration from the entirety of the global epilepsy community. Fundamental to this is early and deliberate engagement of people from low- and middle-income countries to ensure that AI-based solutions do not exacerbate existing global disparities. Ultimately, we advocate for \"decision intelligence\" approaches to the development of AI-based epilepsy solutions, which involves early engagement of all interest-holders to ensure that the correct questions are addressed and the right technical approaches are deployed to maximize value for the global epilepsy community.",
      "journal": "Epilepsia",
      "year": "2025",
      "doi": "10.1002/epi.70058",
      "authors": "Josephson Colin B et al.",
      "keywords": "AI ethics; Intersectoral Global Action Plan; computational intelligence; data science; deep learning; machine intelligence; machine learning; synthetic intelligence",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41443971/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41460164",
      "title": "Addressing algorithmic bias in lung cancer screening eligibility.",
      "abstract": "BACKGROUND: The US Preventive Services Task Force (USPSTF) lung cancer screening eligibility guidelines and proposed risk models have been developed using data predominantly from White populations. Studies show that these eligibility strategies perform inconsistently across racially diverse populations, suggesting evidence of algorithmic bias. We assessed several lung cancer screening eligibility strategies and explored how algorithmic bias can be resolved to improve equity in eligibility. METHODS: Using the Southern Community Cohort Study, a large US study of predominantly Black/African American individuals, we evaluated the performance of 8 existing lung cancer screening eligibility strategies (USPSTF 2021; American Cancer Society 2023 recommendations; USPSTFSmokeDuration; Prostate, Lung, Colorectal and Ovarian 2012 risk prediction model [PLCOm2012]; PLCOm2012NoRace; PLCOm2012Update; Lung Cancer Risk Assessment Tool; and Lung Cancer Death Risk Assessment tool) and 2 new race-aware strategies proposed by our team (USPSTFRaceSpecific and PLCOm2012RaceSpecific). RESULTS: Among 52\u200a667 adults (65% Black/African American, 31% White, 4% Multiracial/Other) with a smoking history, 1689 developed lung cancer over 15\u2009years. Most screening strategies identified fewer Black/African American participants who developed lung cancer as eligible for screening vs their White counterparts (sensitivity for Black/African American individuals\u2009=\u20090.46-0.73 vs 0.72-0.80 for their White counterparts). Racial eligibility disparities were not resolved by removing race, removing the \"years since quit\" criterion, or using uniform risk thresholds. Replacing pack-years with smoking duration improved equity but overinflated the false-positive rate (0.71 for Black/African American persons vs 0.61 for White persons). Instead, race-aware approaches that tailored eligibility thresholds by race yielded the best sensitivity-specificity trade-off and minimized inequities (sensitivity\u2009=\u20090.71-0.73 for Black/African American persons vs 0.72-0.74 for White persons; false-positive rate\u2009=\u20090.49-0.50 for Black/African American persons vs 0.50-0.53 for White persons). CONCLUSION: Our findings suggest that race-aware approaches are necessary to address algorithmic bias and ensure equitable opportunities for lung cancer screening.",
      "journal": "Journal of the National Cancer Institute",
      "year": "2026",
      "doi": "10.1093/jnci/djaf298",
      "authors": "Manful Adoma et al.",
      "keywords": "",
      "mesh_terms": "Humans; Lung Neoplasms; Female; Early Detection of Cancer; Male; Middle Aged; Algorithms; Aged; Black or African American; United States; Eligibility Determination; Risk Assessment; White People; Bias; Cohort Studies; Adult; Mass Screening; White",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41460164/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41489253",
      "title": "Bridging digital divides: A quantitative assessment of equity, access, and determinants of AI adoption for women's reproductive cancer care in China.",
      "abstract": "A study in China using data from the National Health Services Survey and the Cancer Registry examined factors influencing women's access to AI-assisted breast and cervical cancer screening. Analysis of data from 10,250 women aged 18-65 revealed that urban residence, higher education and income levels, and digital literacy significantly increased the likelihood of access to such screenings. Hospitals equipped with AI systems demonstrated substantially higher early detection rates. However, major barriers included distance to AI-enabled facilities and residence in western provinces. The study concludes that while AI improves diagnostic accuracy, access remains stratified along socioeconomic and geographic lines, necessitating digital infrastructure investments and equity-centered AI governance to ensure that all women benefit. Une \u00e9tude en Chine, utilisant les donn\u00e9es de l'Enqu\u00eate nationale sur les services de sant\u00e9 et du Registre du cancer, a examin\u00e9 les facteurs influen\u00e7ant l'acc\u00e8s des femmes au d\u00e9pistage du cancer du sein et du col de l'ut\u00e9rus assist\u00e9 par l'IA. L'analyse des donn\u00e9es de 10 250 femmes \u00e2g\u00e9es de 18 \u00e0 65 ans a r\u00e9v\u00e9l\u00e9 que la r\u00e9sidence urbaine, des niveaux d'\u00e9ducation et de revenus plus \u00e9lev\u00e9s, ainsi que la litt\u00e9ratie num\u00e9rique augmentaient significativement la probabilit\u00e9 d'acc\u00e8s \u00e0 ces d\u00e9pistages. Les h\u00f4pitaux \u00e9quip\u00e9s de syst\u00e8mes d'IA ont d\u00e9montr\u00e9 des taux de d\u00e9tection pr\u00e9coce substantiellement plus \u00e9lev\u00e9s. Cependant, les principaux obstacles comprenaient la distance aux installations \u00e9quip\u00e9es d'IA et la r\u00e9sidence dans les provinces occidentales. L'\u00e9tude conclut que bien que l'IA am\u00e9liore la pr\u00e9cision du diagnostic, l'acc\u00e8s reste stratifi\u00e9 selon des lignes socio-\u00e9conomiques et g\u00e9ographiques, n\u00e9cessitant des investissements dans les infrastructures num\u00e9riques et une gouvernance de l'IA ax\u00e9e sur l'\u00e9quit\u00e9 pour garantir que toutes les femmes en b\u00e9n\u00e9ficient.",
      "journal": "African journal of reproductive health",
      "year": "2025",
      "doi": "10.29063/ajrh2025/v29i12s.9",
      "authors": "Peng Dongli et al.",
      "keywords": "Artificial Intelligence; Breast Cancer; Cervical Cancer; Digital Divide; Health Equity; Reproductive Health",
      "mesh_terms": "Humans; Female; Adult; China; Middle Aged; Health Services Accessibility; Early Detection of Cancer; Uterine Cervical Neoplasms; Breast Neoplasms; Young Adult; Adolescent; Aged; Socioeconomic Factors",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41489253/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41511794",
      "title": "Ten Core Concepts for Ensuring Data Equity in Public Health.",
      "abstract": "IMPORTANCE: Public health decisions increasingly rely on large-scale data and emerging technologies such as artificial intelligence and mobile health. However, many populations-including those in rural areas, with disabilities, experiencing homelessness, or living in low- and middle-income regions of the world-remain underrepresented in health datasets, leading to biased findings and suboptimal health outcomes for certain subgroups. Addressing data inequities is critical to ensuring that technological and digital advances improve health outcomes for all. OBSERVATIONS: This article proposes 10 core concepts to improve data equity throughout the operational arc of data science research and practice in public health. The framework integrates computer science principles such as fairness, transparency, and privacy protection, with best practices in public health data science that focus on mitigating information and selection biases, learning causality, and ensuring generalizability. These concepts are applied together throughout the data life cycle, from study design to data collection, analysis, and interpretation to policy translation, offering a structured approach for evaluating whether data practices adequately represent and serve all populations. CONCLUSIONS AND RELEVANCE: Data equity is a foundational requirement for producing trustworthy inference and actionable evidence. When data equity is built into public health research from the start, technological and digital advances are more likely to improve health outcomes for everyone rather than widening existing health gaps. These 10 core concepts can be used to operationalize data equity in public health. Although data equity is an essential first step, it does not automatically guarantee information, learning, or decision equity. Advancing data equity must be accompanied by parallel efforts in information theory and structural changes that promote informed decision-making.",
      "journal": "JAMA health forum",
      "year": "2026",
      "doi": "10.1001/jamahealthforum.2025.6031",
      "authors": "Wang Yiran et al.",
      "keywords": "",
      "mesh_terms": "Humans; Public Health; Data Science",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41511794/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41512215",
      "title": "Understanding algorithmic fairness for clinical prediction in terms of subgroup net benefit and health equity.",
      "abstract": "There are concerns about the fairness of clinical prediction models. 'Fair' models are defined as those for which their performance or predictions are not inappropriately influenced by protected attributes such as ethnicity, gender, or socio-economic status. Researchers have raised concerns that current algorithmic fairness paradigms enforce strict egalitarianism in healthcare, leveling down the performance of models in higher-performing subgroups instead of improving it in lower-performing ones. We propose assessing the fairness of a prediction model by expanding the concept of net benefit, using it to quantify and compare the clinical impact of a model in different subgroups. We use this to explore how a model distributes benefit across a population, its impact on health inequalities, and its role in the achievement of health equity. We show how resource constraints might introduce necessary trade-offs between health equity and other objectives of healthcare systems. We showcase our proposed approach with the development of two clinical prediction models: 1) a prognostic type 2 diabetes model used by clinicians to enrol patients into a preventive care lifestyle intervention programme, and 2) a lung cancer screening algorithm used to allocate diagnostic scans across the population. This approach helps modelers better understand if a model upholds health equity by considering its performance in a clinical and social context.",
      "journal": "Epidemiology (Cambridge, Mass.)",
      "year": "2025",
      "doi": "10.1097/EDE.0000000000001949",
      "authors": "Benitez-Aurioles Jose et al.",
      "keywords": "Clinical prediction models; UK Biobank; fairness; health equity; net benefit",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41512215/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41525640",
      "title": "Enhancing Fairness in Skin Lesion Classification for Medical Diagnosis Using Prune Learning.",
      "abstract": "Recent advances in deep learning have significantly improved the accuracy of skin lesion classification models, supporting medical diagnoses and promoting equitable healthcare. However, concerns remain about potential biases related to skin color, which can impact diagnostic outcomes. Ensuring fairness is challenging due to difficulties in classifying skin tones, high computational demands, and the complexity of objectively verifying fairness, given the continuous and context-dependent nature of skin tone and the dependence of fairness conclusions on metric choice and subgroup representation. To address these challenges, we propose a fairness algorithm for skin lesion classification that overcomes the challenges associated with achieving diagnostic fairness across varying skin tones. By calculating the skewness of the feature map in the convolution layer of the Visual Geometry Group network (VGG) and the patches and the heads of the Vision Transformer (ViT), our method reduces unnecessary channels related to skin tone, focusing instead on the lesion area. Application on VGG11 and ViT-B16, showed improved fairness metrics by 15-20% on average while maintaining accuracy and F1-score within 0.01 of the baseline. Additionally, the method reduced model size by 16% for VGG11 and decreased memory footprint for ViT-B16, without requiring skin tone labels at inference. Thus, the approach lowers computational costs and mitigates bias without relying on conventional statistical methods. It potentially reduces model size while maintaining fairness, making it more practical for real-world applications.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2026",
      "doi": "10.1109/JBHI.2026.3652910",
      "authors": "Paxton Kuniko et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41525640/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41528321",
      "title": "Auditor models to suppress poor artificial intelligence predictions can improve human-artificial intelligence collaborative performance.",
      "abstract": "OBJECTIVE: Healthcare decisions are increasingly made with the assistance of machine learning (ML). ML has been known to have unfairness-inconsistent outcomes across subpopulations. Clinicians interacting with these systems can perpetuate such unfairness by overreliance. Recent work exploring ML suppression-silencing predictions based on auditing the ML-shows promise in mitigating performance issues originating from overreliance. This study aims to evaluate the impact of suppression on collaboration fairness and evaluate ML uncertainty as desiderata to audit the ML. MATERIALS AND METHODS: We used data from the Vanderbilt University Medical Center electronic health record (n\u2009=\u200958\u2009817) and the MIMIC-IV-ED dataset (n\u2009=\u2009363\u2009145) to predict likelihood of death or intensive care unit transfer and likelihood of 30-day readmission using gradient-boosted trees and an artificially high-performing oracle model. We derived clinician decisions directly from the dataset and simulated clinician acceptance of ML predictions based on previous empirical work on acceptance of clinical decision support alerts. We measured performance as area under the receiver operating characteristic curve and algorithmic fairness using absolute averaged odds difference. RESULTS: When the ML outperforms humans, suppression outperforms the human alone (P\u2009<\u20098.2\u2009\u00d7\u200910-6) and at least does not degrade fairness. When the human outperforms the ML, the human is either fairer than suppression (P\u2009<\u20098.2\u2009\u00d7\u200910-4) or there is no statistically significant difference in fairness. Incorporating uncertainty quantification into suppression approaches can improve performance. CONCLUSION: Suppression of poor-quality ML predictions through an auditor model shows promise in improving collaborative human-AI performance and fairness.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2026",
      "doi": "10.1093/jamia/ocaf235",
      "authors": "Brown Katherine E et al.",
      "keywords": "artificial intelligence; human-AI collaboration; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41528321/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41528598",
      "title": "From glycolytic signatures to patients: A translational roadmap for reproducible, equitable deployment of multi-omics and AI in colorectal cancer.",
      "abstract": "Recent advances in Medical Oncology highlight the integration of bulk and single-cell transcriptomics to reveal glycolytic heterogeneity in colorectal cancer. Translating these discoveries into reliable clinical tools requires rigorous methods, transparent validation, and equity-minded implementation. This communication proposes a standards-first roadmap for reproducible and globally relevant biomarker development. It identifies major technical pitfalls such as batch-effect over-correction and normalization bias, and recommends the application of internationally recognized frameworks-TRIPOD\u2009+\u2009AI, PROBAST\u2009+\u2009AI, and DECIDE-AI-to ensure transparency, calibration, and staged clinical evaluation. Orthogonal validation using metabolic imaging and spectroscopy is emphasized to confirm biological realism beyond transcriptomic data. The roadmap concludes with strategies for global equity, including LMIC-inclusive trial design, FAIR data standards, and cost-aware clinical surrogates. This structured approach bridges discovery science with practical implementation, aligning precision oncology with reproducibility, accountability, and global accessibility.",
      "journal": "Medical oncology (Northwood, London, England)",
      "year": "2026",
      "doi": "10.1007/s12032-026-03236-3",
      "authors": "Vijayasimha M",
      "keywords": "Artificial intelligence; Colorectal cancer; DECIDE-AI; Equity; Glycolysis; Multi-omics; PROBAST\u2009+\u2009AI; Reproducibility; TRIPOD\u2009+\u2009AI; Translational oncology",
      "mesh_terms": "Humans; Artificial Intelligence; Biomarkers, Tumor; Colorectal Neoplasms; Glycolysis; Multiomics; Precision Medicine; Reproducibility of Results; Translational Research, Biomedical",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41528598/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41553870",
      "title": "Enhancing Fairness in Large Language Models for Clinical Artificial Intelligence Applications Through Fine-Tuning and Prompting.",
      "abstract": "Large Language Models (LLMs) are increasingly being applied to sensitive domains such as medical care, which pose multiple technical and ethical challenges. The most immediate issues include biases built into these models, which are trained on large datasets that may reflect societal prejudices. Such biases can yield outputs that are unfair, ungeneralizable, or even harmful, rendering them clinically inapplicable in the real world and noncompliant with ethical and regulatory constraints. We examine how well bias suppression works when fine-tuned models are prompted across four critical categories (gender, race, profession, and religion). We manually curate a diverse inference dataset and create twelve prompt variants -- six of which are debiased -- to test the outputs of three open-source LLMs: Llama2-7B, Mistral-7B, and Dolly-7B. The fairness and interpretability of the outputs are evaluated using a bias-scoring metric, where lower scores indicate better fairness and interpretability. We also note that debiased prompts reduce bias, and fine-tuning the model performs even better. The results emphasize the critical importance of timely action, model robustness, and ongoing ethical scrutiny for trustworthy and fair LLM deployment in real-world settings, such as medical imaging, and the maintenance of Electronic Health Records (EHR).",
      "journal": "Journal of visualized experiments : JoVE",
      "year": "2026",
      "doi": "10.3791/69132",
      "authors": "Kamboj Pradeep et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Humans; Language; Large Language Models",
      "pub_types": "Journal Article; Video-Audio Media",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41553870/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41564634",
      "title": "Rethinking fairness in medical imaging: Maximizing group-specific performance with application to skin disease diagnosis.",
      "abstract": "Recent efforts in medical image computing have focused on improving fairness by balancing it with accuracy within a single, unified model. However, this often creates a trade-off: gains for underrepresented groups can come at the expense of reduced accuracy for groups that were previously well-served. In high-stakes clinical contexts, even minor drops in accuracy can lead to serious consequences, making such trade-offs highly contentious. Rather than accepting this compromise, we reframe the fairness objective in this paper as maximizing diagnostic accuracy for each patient group by leveraging additional computational resources to train group-specific models. To achieve this goal, we introduce SPARE, a novel data reweighting algorithm designed to optimize performance for a given group. SPARE evaluates the value of each training sample using two key factors: utility, which reflects the sample's contribution to refining the model's decision boundary, and group similarity, which captures its relevance to the target group. By assigning greater weight to samples that score highly on both metrics, SPARE rebalances the training process-particularly leveraging the value of out-of-group data-to improve group-specific accuracy while avoiding the traditional fairness-accuracy trade-off. Experiments on two skin disease datasets demonstrate that SPARE significantly improves group-specific performance while maintaining comparable fairness metrics, highlighting its promise as a more practical fairness paradigm for improving clinical reliability.",
      "journal": "Medical image analysis",
      "year": "2026",
      "doi": "10.1016/j.media.2026.103950",
      "authors": "Xu Gelei et al.",
      "keywords": "Fairness; Group-specific model; Skin disease",
      "mesh_terms": "Humans; Algorithms; Skin Diseases",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41564634/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41579486",
      "title": "VHU-Net: Variational hadamard U-Net for body MRI bias field correction.",
      "abstract": "Bias field artifacts in magnetic resonance imaging (MRI) scans introduce spatially smooth intensity inhomogeneities that degrade image quality and hinder downstream analysis. To address this challenge, we propose a novel variational Hadamard U-Net (VHU-Net) for effective body MRI bias field correction. The encoder comprises multiple convolutional Hadamard transform blocks (ConvHTBlocks), each integrating convolutional layers with a Hadamard transform (HT) layer. Specifically, the HT layer performs channel-wise frequency decomposition to isolate low-frequency components, while a subsequent scaling layer and semi-soft thresholding mechanism suppress redundant high-frequency noise. To compensate for the HT layer's inability to model inter-channel dependencies, the decoder incorporates an inverse HT-reconstructed transformer block, enabling global, frequency-aware attention for the recovery of spatially consistent bias fields. The stacked decoder ConvHTBlocks further enhance the capacity to reconstruct the underlying ground-truth bias field. Building on the principles of variational inference, we formulate a new evidence lower bound (ELBO) as the training objective, promoting sparsity in the latent space while ensuring accurate bias field estimation. Comprehensive experiments on body MRI datasets demonstrate the superiority of VHU-Net over existing state-of-the-art methods in terms of intensity uniformity. Moreover, the corrected images yield substantial downstream improvements in segmentation accuracy. Our framework offers computational efficiency, interpretability, and robust performance across multi-center datasets, making it suitable for clinical deployment. The codes are available at https://github.com/Holmes696/Probabilistic-Hadamard-U-Net.",
      "journal": "Medical image analysis",
      "year": "2026",
      "doi": "10.1016/j.media.2026.103955",
      "authors": "Zhu Xin et al.",
      "keywords": "Bias field correction; Hadamard transform; MRI segmentation; U-Net; Variational inference",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41579486/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41604946",
      "title": "Gender-based data bias and model fairness evaluation in benchmarked open-access disease prediction datasets.",
      "abstract": "The widespread use of open-access datasets for validating machine learning (ML) models has raised critical concerns about data bias and model fairness, particularly in relation to gender. This study systematically investigates gender-based data bias in disease prediction datasets and evaluates the fairness of ML algorithms trained on them. A total of 74 datasets were selected from Kaggle and the UCI Machine Learning Repository, based on the inclusion of gender as a feature and classification labels. Data bias was quantified using Earth Mover's Distance to measure disparities in class-wise gender distributions, with statistical significance assessed via bootstrapping. Fairness was evaluated across seven ML algorithms (Decision Tree, Random Forest, Logistic Regression, Artificial Neural Networks, Support Vector Machine, K-Nearest Neighbours, and Na\u00efve Bayes) using k-fold cross-validation and statistical tests. Two fairness definitions, Equalised Odds and Treatment Equality, were applied. Results showed that 35 datasets exhibited gender-based data bias, disproportionately affecting females. Heart disease datasets had the highest prevalence of data bias, while the lung cancer and mental health datasets were found to be bias-free. Fairness outcomes varied significantly across algorithms, with Decision Tree showing the fewest issues and Logistic Regression the most. Bias-free datasets consistently produced fewer fairness concerns, with statistically significant differences (p\u00a0<\u00a00.01) across all algorithm groups. These findings highlight the importance of addressing gender-based data bias and selecting appropriate algorithms to improve fairness in ML applications. The study highlights the importance of addressing gender-based data bias in enhancing model fairness. It contributes to the development of equitable AI systems, thereby supporting data-driven decision-making in healthcare.",
      "journal": "Computers in biology and medicine",
      "year": "2026",
      "doi": "10.1016/j.compbiomed.2026.111503",
      "authors": "Uddin Shahadat et al.",
      "keywords": "Data bias; Disease prediction; Gender; Model fairness; Open-access dataset",
      "mesh_terms": "Humans; Female; Male; Machine Learning; Databases, Factual; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41604946/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41636665",
      "title": "PREVENT Equations in Young Adults: Fairness, Calibration, and Performance Across Racial and Ethnic Groups.",
      "abstract": "BACKGROUND: Cardiovascular disease (CVD) is increasing among young adults. The American Heart Association's PREVENT (Predicting Risk of Cardiovascular Disease Events) equations estimate risk of CVD, atherosclerotic cardiovascular disease (ASCVD), and heart failure (HF) for primary prevention. Augmented equations additionally include zip code-based social deprivation index (SDI) to address adverse social exposures. OBJECTIVES: We assessed performance and algorithmic fairness of base and SDI-augmented PREVENT equations in young adults aged 30 to 39 years, defining fairness as similar performance across racial and ethnic groups. An exploratory analysis was conducted among young adults aged 20 to 29 years. METHODS: We included Kaiser Permanente Southern California members aged 20 to 39 years without prior CVD between 2008 and 2009, followed through 2019. We compared 10-year predicted and observed CVD, ASCVD, and HF events for base and SDI-augmented PREVENT models. Performance (Harell's C, calibration slopes, mean calibration) and fairness (concordance imparity, fair calibration) were estimated by race and ethnicity and age group (30-39 years [primary analysis], 20-29 years [exploratory analysis]). RESULTS: Among 161,202 young adults aged 30 to 39 years (60.0% women; 51.7% Hispanic, 26.9% non-Hispanic White, 12.5% Asian/Pacific Islander, 8.9% non-Hispanic Black), 10-year CVD incidence was 0.7%. Race-specific Harrell's C-statistics for the base PREVENT CVD model ranged from 0.68 to 0.72, yielding low concordance imparity (0.04; 95% CI: 0.02-0.22) which implies fair discrimination. Mean calibration showed underprediction in non-Hispanic Black participants (0.54; 95% CI: 0.48-0.65) vs other groups (range: 0.96-1.07). In fair calibration testing, prediction errors differed across racial and ethnic groups. Results were similar for ASCVD and HF. Adding SDI did not improve performance or fairness despite disparities across groups. In exploratory analyses among 80,978 individuals aged 20 to 29 years, performance and fairness results were similar. CONCLUSIONS: This large, diverse cohort of young adults demonstrates how the PREVENT equations may perform when applied in real-world clinical settings, reflecting the true operational environment faced by large health systems. Applications of PREVENT in clinical patient care, eg, early initiation of preventive strategies, should consider variations in model performance across age, race, and ethnicity.",
      "journal": "Journal of the American College of Cardiology",
      "year": "2026",
      "doi": "10.1016/j.jacc.2025.12.019",
      "authors": "Gauen Abigail M et al.",
      "keywords": "algorithmic fairness; cardiovascular disease; epidemiology; primary prevention; risk prediction",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41636665/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41663205",
      "title": "Evaluating Sociodemographic Biases in Artificial Intelligence-Based Glioblastoma Response Assessment Algorithms.",
      "abstract": "BACKGROUND AND PURPOSE: Recent studies have demonstrated bias in various medical imaging artificial intelligence (AI) models, yet the factors underpinning these biases remain relatively unclear. This study evaluated potential sociodemographic biases in AI-based glioblastoma MRI segmentation models trained on datasets varying in size and demographic composition. We evaluated four nnUNet models with different training datasets: (1) the Federated Tumor Segmentation postoperative (FeTS2) model trained on a large (>10k exams) multi-national, multi-institution dataset, (2) the Brain Tumor Segmentation (BraTS) 2024 postoperative glioma model trained on a moderate size (>2k exams) multi-institution, North American dataset, (3) a model trained on a small (>200 exams), private, demographically homogenous, single-institution dataset, and (4) a model trained on an equally small (>200 exams), but demographically heterogenous dataset. MATERIALS AND METHODS: Models were evaluated for bias using an independent, manually corrected dataset of 480 patients (mean age 52 \u00b1 14) that was prospectively collected from a single high-volume academic brain tumor center. Automated FLAIR and enhancing tumor segmentations from the AI models were evaluated using Dice scores. Sociodemographic factors were collected and analyzed using beta regression to assess their influence on model performance. RESULTS: The model trained exclusively on White, non-Hispanic males had the lowest overall Dice scores (0.943 for FLAIR, 0.909 for Enhancement) and exhibited biases in age and smoking status. The BraTS model demonstrated the highest Dice scores (0.996 for FLAIR, 0.999 for Enhancement) and had the least bias overall. CONCLUSIONS: Demographic bias was relatively low in glioblastoma MRI segmentation models. The model trained on the smallest and most homogenous dataset exhibited the most bias. Greater demographic heterogeneity even without increasing training dataset size was associated with reduced bias. The BraTS model, trained on a moderate-sized cohort that included more diverse tumor types, performed better and demonstrated less bias than the FeTS2 model, despite the FeTS2 being trained on the largest dataset.",
      "journal": "AJNR. American journal of neuroradiology",
      "year": "2026",
      "doi": "10.3174/ajnr.A9217",
      "authors": "Lee Rachel S et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41663205/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41666048",
      "title": "Causal modeling of chronic kidney disease in a participatory framework for informing the inclusion of social drivers in health algorithms.",
      "abstract": "OBJECTIVES: Incomplete or incorrect causal theories are a key source of bias in machine learning (ML) algorithms. Community-engaged methodologies provide an avenue for mitigating this bias through incorporating causal insights from community stakeholders into ML development. In health applications, community-engaged approaches can enable the study of social drivers of health (SDOH), which are known to shape health inequities. However, it remains challenging for SDOH to inform ML algorithms, partially because SDOH variables are known to be interrelated, yet it is difficult to elucidate the causal relationships between them. Community-based system dynamics is a community-engaged methodology that can be used to cocreate formal causal graphs, called causal loop diagrams, with patients. MATERIALS AND METHODS: We used community-based system dynamics to create a causal graph representing the impacts of SDOH on the progression of chronic kidney disease, a chronic condition with SDOH-driven health disparities. We conducted focus groups with 42 participants and a day-long model building workshop with 11 participants. RESULTS: Our model building workshop resulted in a final graph comprising 16 variables, 42 causal links, and 5 subsystems of semantically related SDOH variables. CONCLUSION: This final graph, representing the causal relationships between social variables relevant to chronic kidney disease, can inform the development of clinical ML algorithms and other technological interventions.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2026",
      "doi": "10.1093/jamia/ocag019",
      "authors": "Foryciarz Agata et al.",
      "keywords": "chronic; community-based participatory research; health inequities; renal insufficiency; social determinants of health; systems theory",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41666048/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41666966",
      "title": "From Representation to Reform: A Qualitative Study of Gender Equity in Interventional Cardiology.",
      "abstract": "Women represent fewer than 5% of practicing interventional cardiologists in the United States (U.S.). The WECARE (WomEn in interventional Cardiology: A qualitative REsearch) study explored the career experiences, challenges, and support systems of women practicing interventional cardiology (IC) in the U.S. Semi-structured interviews were conducted with 18 women IC attendings and fellows representing diverse backgrounds. Interviews were analyzed using thematic content analysis. Five major themes emerged: (A) Career entry and negotiation: Participants described variable recruitment and promotion experiences, from early discouragement regarding lifestyle suitability for women to strong mentorship enabling advanced fellowships and leadership roles. Salary negotiation was often opaque and appeared inequitable, requiring persistent self-advocacy. (B) Motherhood and institutional gaps: Participants reported complex decisions about pregnancy timing, inconsistent maternity leave, and breastfeeding challenges requiring improvisation. Concerns about radiation safety persisted amid inconsistent institutional policies. (C) Bias and representation: Gender bias was acknowledged as being present but often manageable; women employed strategies such as humor, education, and assertiveness to mitigate impact. Some reported exclusion from informal networks and limited leadership opportunities. (D) Mentorship and Support systems: Mentorship was described as pivotal for training, negotiation, and resilience. Family support, childcare, and peer networks were crucial for work-life balance. (E) Resilience and advocacy: Despite persistent barriers, most participants expressed strong professional satisfaction and commitment to IC. Women in IC report high career fulfillment yet continue to face structural and cultural challenges. Mentorship, institutional transparency, fairness and standardized family-supportive policies are essential to sustain an equitable and diverse workforce.",
      "journal": "The American journal of cardiology",
      "year": "2026",
      "doi": "10.1016/j.amjcard.2026.02.009",
      "authors": "Alexandrou Michaella et al.",
      "keywords": "WIC; diversity; gender equity; qualitative research; women in cardiology",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41666966/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41673000",
      "title": "Biomedical Data Manifest: A lightweight data documentation mapping to increase transparency for AI/ML.",
      "abstract": "Biomedical machine learning (ML) models raise critical concerns about embedded assumptions influencing clinical decision-making, necessitating robust documentation frameworks for datasets that are shared via external repositories. Fairness-aware algorithm effectiveness hinges on users' prior awareness of specific issues in the data - information such as data collection methodology, provenance and quality. Current ML-focused documentation approaches impose impractical burdens on data generators and conflate data/model accountability. This is problematic for resource datasets not explicitly created for ML applications. This study addresses these gaps through a two-step process: First, we derived consensus documentation fields by mapping elements across four key templates. Second, we surveyed biomedical stakeholders across four roles (clinicians, bench scientists, data manager and computationalists) to assess field importance and relevance. This revealed important role-dependent prioritization differences, motivating the development of the Biomedical Data Manifest - a modular template employing persona-specific field presentation reducing generator burden while ensuring end-users receive role-relevant information. The Biomedical Data Manifest improves transparency for datasets deposited in public or controlled-access repositories and bias mitigation across ML applications.",
      "journal": "Scientific data",
      "year": "2026",
      "doi": "10.1038/s41597-026-06670-0",
      "authors": "Bottomly Daniel et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41673000/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41679906",
      "title": "Neuroimaging in Low- to Middle-Income Countries: A Health Equity Perspective.",
      "abstract": "BACKGROUND: Inequities in neuroimaging access represent a major barrier to timely diagnosis and treatment of neurologic disease, particularly in low- and middle-income countries. This article examines factors contributing to neuroimaging access challenges in low- and middle-income countries and discusses practical, sustainable strategies to improve equity in global neuroimaging capacity. METHODS: This State of Practice was developed by the members of the American Society of Neuroradiology's Diversity and Inclusion Committee, who reviewed published literature, global radiology workforce data, and Accreditation Council for Graduate Medical Education training statistics. The committee members also integrated insights from outreach experiences and collaborations with professional organizations working to improve global health. KEY MESSAGE: Bridging neuroimaging disparities requires coordinated action that integrates technological innovation, workforce development, and long-term collaboration. Expanding the use of portable and low-cost imaging systems, global teleradiology, and artificial intelligence alongside education, outreach, and policy engagement can help build capacity and improve access. Sustained partnerships between high- and low-resource regions are essential to achieving a more equitable and globally connected neuroradiology community with the potential to improve health outcomes worldwide.",
      "journal": "AJNR. American journal of neuroradiology",
      "year": "2026",
      "doi": "10.3174/ajnr.A9098",
      "authors": "Budigi Bhavana et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41679906/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41680855",
      "title": "AI/ML-based strategies for enhancing equity, diversity, and inclusion in randomized clinical trials.",
      "abstract": "This paper introduces a conceptual framework designed to embed equity, diversity, and inclusion (EDI) across all stages of the clinical trial lifecycle. Randomized clinical trials (RCTs) remain the most reliable method for evaluating medical treatments, yet persistent gaps in representation undermine their validity and fairness. Women, older adults, racial and ethnic minorities, and socioeconomically disadvantaged groups are often underrepresented, raising concerns about whether trial results can be generalized to all patients. This lack of inclusivity not only limits scientific rigor but also risks reinforcing existing health disparities. Recent advances in artificial intelligence (AI) and machine learning (ML) provide new opportunities to address these challenges. These technologies can support more inclusive study designs, enable targeted recruitment of underrepresented populations, and monitor diversity in real time throughout the trial process. They can also be applied to analyze outcomes with fairness-aware methods, helping ensure that results are meaningful across diverse subgroups. In this work, we propose an AI/ML-based framework aimed at operationalizing equity, diversity, and inclusion in clinical research. The framework integrates predictive modeling, adaptive trial designs, and continuous bias detection with ethical and legal safeguards to ensure responsible deployment. By embedding fairness into every stage of the trial lifecycle, this approach offers a pathway toward more representative and trustworthy evidence in medical science. Our analysis reveals the persistent gaps across demographic groups in current RCTs, demonstrating the urgent requirement for systematic intervention. This study also contributes a comprehensive AI/ML framework that operationalizes equity through predictive modeling, adaptive designs, and continuous bias monitoring, providing a structured pathway for researchers to enhance both the scientific validity and ethical integrity of clinical trials.",
      "journal": "Trials",
      "year": "2026",
      "doi": "10.1186/s13063-026-09537-2",
      "authors": "Abbidi Shashidar Reddy et al.",
      "keywords": "Artificial intelligence; Bias detection; Clinical trials; Diversity; Fairness; Health equity; Inclusion; Machine learning; Recruitment optimization",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41680855/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41681030",
      "title": "Demographic-aware deep learning for multi-organ segmentation: Mitigating gender and age biases in CT images.",
      "abstract": "BACKGROUND: Deep learning algorithms have shown promising results for automated organ-at-risk (OAR) segmentation in medical imaging. However, their performance is frequently compromised by demographic bias. This limitation becomes pronounced when conventional models fail to account for Complex 3D anatomical variations across diverse groups, as they often overlook critical factors such as age and gender. Consequently, this oversight can lead to inaccurate segmentations, thereby posing significant risks to clinical safety in\u00a0radiotherapy. PURPOSE: To address this challenge, in this work, we develop a demographic-aware deep learning framework for multi-organ segmentation in computed tomography (CT) images. Our approach is designed to explicitly mitigate age- and gender-specific biases by incorporating demographic prompts and adaptive attention mechanisms, enabling the capture of multi-view anatomical features across diverse\u00a0groups. METHODS: We propose the Demographic-Aware Network (DA-Net), a novel framework trained on a unified dataset of 489 adult (AMOS2022) and 370 pediatric (Pediatric CT-SEG) CT scans, covering 30 organs and including 355 female scans. To robustly learn group-specific anatomical characteristics, DA-Net integrates the Demographic-Aware Hyper-Convolution (DA-HyperConv) module that dynamically adapts convolutional kernels based on demographic prompts. Additionally, an Adaptive Triplet Attention Block (ATAB) is embedded to further leverage multi-view features and enhance segmentation accuracy. We validate the generalizability and effectiveness of our framework on an external dataset (WORD, 150 adults, 62 females). The framework is evaluated quantitatively using the Dice Similarity Coefficient (DICE) and Normalized Surface Dice (NSD). RESULTS: DA-Net surpasses state-of-the-art (SOTA) methods across both the general group and specific demographic subgroups. In the AMOS2022 dataset (mean age 52.8 \u00b1 $\\pm$  16.1 years), DA-Net achieves the highest average DICE of 88.6% and NSD of 76.3% for adults. On the Pediatric CT-SEG (mean age 6.9 \u00b1 $\\pm$  4.5 years), it achieves top performance with an average DICE of 75.3% \u00b1 $\\pm$  20.4% and NSD of 54.8% \u00b1 $\\pm$  20.9%. Notably, our proposed framework achieves substantial DICE improvements of 11% to 30% for gender-specific organs, significantly reducing performance disparities. Robustness and generalizability are further supported by consistent results on external validation using the WORD dataset. Compared with the SOTA methods, the performance improvement of our approach is of substantial importance in both the WORD dataset and the Pediatric\u00a0CT-SEG. CONCLUSIONS: In this work, we propose DA-Net, a segmentation network that explicitly incorporates age and gender attributes to mitigate performance disparities between pediatric and adult groups while combining multiple views of anatomic features to improve performance. By leveraging demographic information, DA-Net enhances segmentation accuracy, especially for gender-specific organs. The proposed framework highlights the necessity of developing fair and personalized models tailored to clinical applications, providing a foundation for building more equitable artificial intelligence systems in medical\u00a0imaging.",
      "journal": "Medical physics",
      "year": "2026",
      "doi": "10.1002/mp.70322",
      "authors": "Ma Junqiang et al.",
      "keywords": "age bias; computed tomography; deep learning; gender bias; multi\u2010organs segmentation",
      "mesh_terms": "Deep Learning; Humans; Tomography, X-Ray Computed; Female; Male; Adult; Image Processing, Computer-Assisted; Child; Adolescent; Middle Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41681030/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41692962",
      "title": "Addressing Algorithmic Bias in Artificial Intelligence-Driven Medical Education Assessment.",
      "abstract": "",
      "journal": "Academic medicine : journal of the Association of American Medical Colleges",
      "year": "2026",
      "doi": "10.1093/acamed/wvag039",
      "authors": "Du Zhicheng",
      "keywords": "artificial intelligence; assessment; large language models; moral injury",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41692962/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41697511",
      "title": "Epidemiological Trajectories and Quality of Care Disparities of MASLD in Asia, 1990-2023.",
      "abstract": "BACKGROUND: To characterize temporal and geographic patterns of metabolic dysfunction-associated steatotic liver disease (MASLD) across Asia from 1990 to 2023, assess disparities in disease burden and care quality, and identify key demographic and metabolic determinants shaping its trajectory. METHODS: We analyzed Global Burden of Disease (GBD) 2023 data for 47 Asian countries (1990-2023). Age-standardized incidence (ASIR), mortality (ASMR), and disability-adjusted life-years (ASDR) were examined using Joinpoint regression. A Quality-of-Care Index (QCI) was derived through principal component analysis. Explainable machine learning (SHAP models) evaluated demographic and metabolic determinants and projected future trends to 2050. RESULTS: MASLD burden rose substantially across Asia. East Asia exhibited the fastest incidence growth (AAPC 0.61%), whereas Central Asia experienced the sharpest increases in ASMR (1.04%) and ASDR (1.09%). In 2023, ASIR peaked in Malaysia (763.5 per 100,000), ASMR in Mongolia (7.08), and ASDR in Turkmenistan (184.9). A shift toward metabolic risk dominance was evident, with hyperglycemia and obesity surpassing smoking as primary contributors to mortality and disability burden. QCI was highest in East and Western Asia and lowest in South and Central Asia. SHAP analysis identified aging and hyperglycemia as dominant predictors of future burden. CONCLUSIONS: MASLD is accelerating across Asia, driven by metabolic risk accumulation, demographic aging, and unequal care capacity. Strengthening weight and glycemic control, expanding metabolic liver disease surveillance, and improving equitable hepatometabolic care are urgent priorities to curb future disease burden.",
      "journal": "Digestive diseases and sciences",
      "year": "2026",
      "doi": "10.1007/s10620-026-09758-0",
      "authors": "Zhang Kexin et al.",
      "keywords": "Asia; Global burden of disease; MASLD; Machine learning; Quality of care",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41697511/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41701928",
      "title": "Assessing the Impact of Sociodemographic Factors on Artificial Intelligence Models in Predicting Dementia: Retrospective Cohort Study.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) is increasingly applied to health care, yet concerns about fairness persist, particularly in relation to sociodemographic disparities. Previous studies suggest that socioeconomic status (SES) and sex may influence AI model performance, potentially affecting groups that are historically underserved or understudied. OBJECTIVE: This study aimed to (1) assess algorithmic bias in AI-driven dementia prediction models based on SES and sex (biological sex), (2) compare the utility of an individual-level SES measure (the Housing-Based Socioeconomic Status [HOUSES] Index) versus an area-level measure (the Area Deprivation Index) for bias detection, and (3) evaluate the effectiveness of an oversampling technique (the Synthetic Minority Oversampling Technique for Nominal and Continuous features) for bias mitigation. METHODS: This study used data from two population-based cohorts: the Mayo Clinic Study on Aging (n=3041) and the Rochester Epidemiology Project (n=19,572). Four AI models (random forest, logistic regression, support vector machine, and Na\u00efve Bayes) were trained using a 5-year observation window of structured electronic health record data to predict dementia onset within the subsequent 1-year window. Fairness and model performance were assessed using the balanced error rate (BER) across intersecting SES-sex subgroups. The Synthetic Minority Oversampling Technique for Nominal and Continuous features algorithm was applied to the training data to balance the representation of SES groups. RESULTS: Across both cohorts, individuals with lower SES generally exhibited higher BERs (worse performance) than high SES groups, confirming the presence of bias. In the MCSA cohort, males with high SES, as indicated by the HOUSES Index, consistently exhibited the lowest BERs across all evaluated models. Balancing the training data based on a specific SES measure showed a trend toward reducing the BER disparity when evaluated using that same measure. However, this targeted improvement demonstrated nonuniversal benefits; in some cases, it exacerbated disparities when evaluated using other, unbalanced SES measures. This pattern suggests that fairness interventions are not universally beneficial across different definitions of the protected attribute. While the balancing approach improved fairness in model performance for lower SES groups, it often came at the cost of a slight reduction in overall model performance. However, an exception was observed in the MCSA cohort when balancing based on the HOUSES Index using logistic regression, support vector machine, and Na\u00efve Bayes, where the performances of both the high and low SES groups improved. CONCLUSIONS: This research highlights the importance of incorporating sociodemographic context into AI modeling in health care. The choice of SES measure may lead to different assessments of algorithmic bias. The HOUSES Index, as a validated individual-level SES measure, may be more effective for bias mitigation than area-level measures. Future AI development should integrate bias mitigation strategies to ensure models do not reinforce existing disparities in health outcomes.",
      "journal": "JMIR medical informatics",
      "year": "2026",
      "doi": "10.2196/80405",
      "authors": "Liu Xingyi et al.",
      "keywords": "AI bias; Area Deprivation Index; artificial intelligence; dementia; machine learning; socioeconomic status",
      "mesh_terms": "Humans; Dementia; Artificial Intelligence; Male; Female; Retrospective Studies; Aged; Sociodemographic Factors; Aged, 80 and over; Middle Aged; Bayes Theorem; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41701928/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "7572581",
      "title": "Comparison of the sensitivity and specificity of exercise electrocardiography in biased and unbiased populations of men and women.",
      "abstract": "To assess for sex-related differences in posttest referral bias, we compared the accuracy of exercise electrocardiography in biased (coronary angiography only) and unbiased (all unselected) populations with possible coronary disease. A retrospective analysis of clinical and exercise test data from 4467 patients (788 who underwent angiography) was performed (2824 men and 1643 women). The accuracy of a positive exercise test result was assessed in the entire unbiased group with a method that used disease probability (derived with a logistic algorithm) rather than angiography results. We found that the sensitivity and specificity were significantly greater in men than in women with use of the biased or unbiased groups. When the results for the unbiased and biased groups were compared, the sensitivities for the unbiased group were significantly lower and the specificities were significantly higher than those of the biased group. These differences reflect the effects of posttest referral bias. The amounts that sensitivity decreased and specificity increased, however, was not different for men and women. Therefore, we conclude that the accuracy of exercise electrocardiography is lower in women than men irrespective of whether a biased or an unbiased group is used. However, these differences cannot be explained on the basis of sex-related differences in posttest referral bias.",
      "journal": "American heart journal",
      "year": "1995",
      "doi": "10.1016/0002-8703(95)90072-1",
      "authors": "Morise A P et al.",
      "keywords": "",
      "mesh_terms": "Adult; Electrocardiography; Exercise Test; Female; Humans; Male; Middle Aged; Predictive Value of Tests; Selection Bias; Sensitivity and Specificity; Sex Characteristics",
      "pub_types": "Comparative Study; Journal Article; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/7572581/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "9291452",
      "title": "Optimal procedures for detecting analytic bias using patient samples.",
      "abstract": "We recently described the performance characteristics of the exponentially adjusted moving mean (EAMM), a patient-data, moving block mean procedure, which is a generalized algorithm that unifies Bull's algorithm and the classic average of normals (AON) procedure. Herein we describe the trend EAMM (TEAMM), a continuous signal analog of the EAMM procedure related to classic trend analysis. Using computer simulation, we have compared EAMM and TEAMM over a range of biases for various sample sizes (N or equivalent smoothing factor alpha) and exponential parameters (P) under conditions of equivalent false rejection (fixed on a per patient sample basis). We found optimal pairs of N and P for each level of bias by determination of minimum mean patient samples to rejection. Overall optimal algorithms were determined through calculation of undetected lost medical utility (ULMU), a novel function that quantifies the medical damage due to analytic bias. The ULMU function was calculated based on lost test specificity in a normal population. We found that optimized TEAMM was superior to optimized EAMM for all levels of analytic bias. If these observations hold true for non-Gaussian populations, TEAMM procedures are the method of choice for detecting bias using patient samples or as an event gauge to trigger use of known-value control materials.",
      "journal": "American journal of clinical pathology",
      "year": "1997",
      "doi": "10.1093/ajcp/108.3.254",
      "authors": "Smith F A et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Clinical Laboratory Information Systems; Computer Simulation; Humans; Laboratories, Hospital; Models, Statistical; Pathology, Clinical; Quality Control; Reproducibility of Results; Selection Bias",
      "pub_types": "Comparative Study; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/9291452/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "20493550",
      "title": "Enhancement of outpatient treatment adherence: Patients' perceptions of coercion, fairness and effectiveness.",
      "abstract": "Interventions aimed at enhancing psychiatric patients' outpatient treatment adherence frequently include informal coercion such as inducements, and threats of negative sanctions. Little is known about patients' subjective perspective concerning these practices. This study examined the perceived coercion, and also the appraisal of the fairness and effectiveness of the treatment. A total of 187 psychiatric patients with different diagnoses were interviewed using a structured questionnaire that included socio-demographic and clinical data, insight into illness (ITAQ), psychopathology (BPRS), functioning (GAF) and a modified version of the MacArthur Admission Experience Survey (AES) as a measure of perceived coercion, fairness and effectiveness. Bivariate correlations and logistic regression analyses were used to identify the influencing factors on the outcome variables. Perceived coercion was associated with experience with informal coercive treatment pressures (OR 2.5-2.9; P<0.05), and a main diagnosis of a schizophrenic disorder (OR 3.8; P<0.001). Experience with informal coercion was inversely associated with fairness (OR 0.3-0.4; P<0.05), but not with effectiveness. Patients with more insight into their illness indicated higher fairness and effectiveness concerning the procedures for enhancing their treatment adherence (OR 3.1; P<0.01). Insight into illness was not associated with perceived coercion. This might indicate that enhancing insight using psycho-educational approaches and high transparency when applying informal coercive practices could improve patients' appreciation for these procedures. They could probably cope better with perceived coercion when understanding its purpose, and when they perceive they are being treated fairly.",
      "journal": "Psychiatry research",
      "year": "2010",
      "doi": "10.1016/j.psychres.2009.09.011",
      "authors": "Jaeger Matthias et al.",
      "keywords": "",
      "mesh_terms": "Adolescent; Adult; Aged; Coercion; Female; Humans; Male; Mental Disorders; Middle Aged; Outpatients; Patient Admission; Patient Compliance; Patient Participation; Perception; Psychiatric Status Rating Scales; Regression Analysis; Statistics, Nonparametric; Surveys and Questionnaires; Young Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/20493550/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "23253307",
      "title": "Disparities in access to hospitals with robotic surgery for patients with prostate cancer undergoing radical prostatectomy.",
      "abstract": "PURPOSE: We described population level trends in radical prostatectomy for patients with prostate cancer by hospitals with robotic surgery, and assessed whether socioeconomic disparities exist in access to such hospitals. MATERIALS AND METHODS: After merging the NIS (Nationwide Inpatient Sample) and the AHA (American Hospital Association) survey from 2006 to 2008, we identified 29,837 patients with prostate cancer who underwent radical prostatectomy. The primary outcome was treatment with radical prostatectomy at hospitals that have adopted robotic surgery. Multivariate logistic regression was used to identify patient and hospital characteristics associated with radical prostatectomy performed at hospitals with robotic surgery. RESULTS: Overall 20,424 (68.5%) patients were surgically treated with radical prostatectomy at hospitals with robotic surgery, while 9,413 (31.5%) underwent radical prostatectomy at hospitals without robotic surgery. There was a marked increase in radical prostatectomy at hospital adopters from 55.8% in 2006 and 70.7% in 2007 to 76.1% in 2008 (p <0.001 for trend). After adjusting for patient and hospital features, lower odds of undergoing radical prostatectomy at hospitals with robotic surgery were seen in black patients (OR 0.81, p <0.001) and Hispanic patients (OR 0.77, p <0.001) vs white patients. Compared to having private health insurance, being primarily insured with Medicaid (OR 0.70, p <0.001) was also associated with lower odds of being treated at hospitals with robotic surgery. CONCLUSIONS: Although there was a rapid shift of patients who underwent radical prostatectomy to hospitals with robotic surgery from 2006 to 2008, black and Hispanic patients or those primarily insured by Medicaid were less likely to undergo radical prostatectomy at such hospitals.",
      "journal": "The Journal of urology",
      "year": "2013",
      "doi": "10.1016/j.juro.2012.09.033",
      "authors": "Kim Simon P et al.",
      "keywords": "",
      "mesh_terms": "Aged; Health Services Accessibility; Healthcare Disparities; Hospitals; Humans; Male; Middle Aged; Prostatectomy; Prostatic Neoplasms; Robotics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/23253307/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "26674595",
      "title": "The feature selection bias problem in relation to high-dimensional gene data.",
      "abstract": "OBJECTIVE: Feature selection is a technique widely used in data mining. The aim is to select the best subset of features relevant to the problem being considered. In this paper, we consider feature selection for the classification of gene datasets. Gene data is usually composed of just a few dozen objects described by thousands of features. For this kind of data, it is easy to find a model that fits the learning data. However, it is not easy to find one that will simultaneously evaluate new data equally well as learning data. This overfitting issue is well known as regards classification and regression, but it also applies to feature selection. METHODS AND MATERIALS: We address this problem and investigate its importance in an empirical study of four feature selection methods applied to seven high-dimensional gene datasets. We chose datasets that are well studied in the literature-colon cancer, leukemia and breast cancer. All the datasets are characterized by a significant number of features and the presence of exactly two decision classes. The feature selection methods used are ReliefF, minimum redundancy maximum relevance, support vector machine-recursive feature elimination and relaxed linear separability. RESULTS: Our main result reveals the existence of positive feature selection bias in all 28 experiments (7 datasets and 4 feature selection methods). Bias was calculated as the difference between validation and test accuracies and ranges from 2.6% to as much as 41.67%. The validation accuracy (biased accuracy) was calculated on the same dataset on which the feature selection was performed. The test accuracy was calculated for data that was not used for feature selection (by so called external cross-validation). CONCLUSIONS: This work provides evidence that using the same dataset for feature selection and learning is not appropriate. We recommend using cross-validation for feature selection in order to reduce selection bias.",
      "journal": "Artificial intelligence in medicine",
      "year": "2016",
      "doi": "10.1016/j.artmed.2015.11.001",
      "authors": "Krawczuk Jerzy et al.",
      "keywords": "Convex and piecewise linear classifier; Feature selection bias; Gene selection; Microarray data; Support vector machine",
      "mesh_terms": "Algorithms; Bias; Biomarkers, Tumor; Computational Biology; Data Mining; Databases, Genetic; Decision Support Techniques; Gene Expression Profiling; Gene Expression Regulation, Neoplastic; Humans; Linear Models; Oligonucleotide Array Sequence Analysis; Pattern Recognition, Automated; Reproducibility of Results; Support Vector Machine",
      "pub_types": "Comparative Study; Journal Article; Research Support, Non-U.S. Gov't; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/26674595/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "28936795",
      "title": "The Ugly Truth About Ourselves and Our Robot Creations: The Problem of Bias and Social Inequity.",
      "abstract": "Recently, there has been an upsurge of attention focused on bias and its impact on specialized artificial intelligence (AI) applications. Allegations of racism and sexism have permeated the conversation as stories surface about search engines delivering job postings for well-paying technical jobs to men and not women, or providing arrest mugshots when keywords such as \"black teenagers\" are entered. Learning algorithms are evolving; they are often created from parsing through large datasets of online information while having truth labels bestowed on them by crowd-sourced masses. These specialized AI algorithms have been liberated from the minds of researchers and startups, and released onto the public. Yet intelligent though they may be, these algorithms maintain some of the same biases that permeate society. They find patterns within datasets that reflect implicit biases and, in so doing, emphasize and reinforce these biases as global truth. This paper describes specific examples of how bias has infused itself into current AI and robotic systems, and how it may affect the future design of such systems. More specifically, we draw attention to how bias may affect the functioning of (1) a robot peacekeeper, (2) a self-driving car, and (3) a medical robot. We conclude with an overview of measures that could be taken to mitigate or halt bias from permeating robotic technology.",
      "journal": "Science and engineering ethics",
      "year": "2018",
      "doi": "10.1007/s11948-017-9975-2",
      "authors": "Howard Ayanna et al.",
      "keywords": "Artificial intelligence; Design ethics; Implicit bias; Professional ethics; Robot ethics",
      "mesh_terms": "Algorithms; Artificial Intelligence; Automobiles; Bias; Biomedical Technology; Datasets as Topic; Female; Humans; Learning; Male; Prejudice; Racism; Robotics; Sexism; Social Justice",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/28936795/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "29715684",
      "title": "Biased Dropout and Crossmap Dropout: Learning towards effective Dropout regularization in convolutional neural network.",
      "abstract": "Training a deep neural network with a large number of parameters often leads to overfitting problem. Recently, Dropout has been introduced as a simple, yet effective regularization approach to combat overfitting in such models. Although Dropout has shown remarkable results on many deep neural network cases, its actual effect on CNN has not been thoroughly explored. Moreover, training a Dropout model will significantly increase the training time as it takes longer time to converge than a non-Dropout model with the same architecture. To deal with these issues, we address Biased Dropout and Crossmap Dropout, two novel approaches of Dropout extension based on the behavior of hidden units in CNN model. Biased Dropout divides the hidden units in a certain layer into two groups based on their magnitude and applies different Dropout rate to each group appropriately. Hidden units with higher activation value, which give more contributions to the network final performance, will be retained by a lower Dropout rate, while units with lower activation value will be exposed to a higher Dropout rate to compensate the previous part. The second approach is Crossmap Dropout, which is an extension of the regular Dropout in convolution layer. Each feature map in a convolution layer has a strong correlation between each other, particularly in every identical pixel location in each feature map. Crossmap Dropout tries to maintain this important correlation yet at the same time break the correlation between each adjacent pixel with respect to all feature maps by applying the same Dropout mask to all feature maps, so that all pixels or units in equivalent positions in each feature map will be either dropped or active during training. Our experiment with various benchmark datasets shows that our approaches provide better generalization than the regular Dropout. Moreover, our Biased Dropout takes faster time to converge during training phase, suggesting that assigning noise appropriately in hidden units can lead to an effective regularization.",
      "journal": "Neural networks : the official journal of the International Neural Network Society",
      "year": "2018",
      "doi": "10.1016/j.neunet.2018.03.016",
      "authors": "Poernomo Alvin et al.",
      "keywords": "Convolutional neural network; Dropout; Regularization",
      "mesh_terms": "Bias; Neural Networks, Computer; Supervised Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29715684/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "29717941",
      "title": "Understanding and diagnosing the potential for bias when using machine learning methods with doubly robust causal estimators.",
      "abstract": "Data-adaptive methods have been proposed to estimate nuisance parameters when using doubly robust semiparametric methods for estimating marginal causal effects. However, in the presence of near practical positivity violations, these methods can produce a separation of the two exposure groups in terms of propensity score densities which can lead to biased estimates of the treatment effect. To motivate the problem, we evaluated the Targeted Minimum Loss-based Estimation procedure using a simulation scenario to estimate the average treatment effect. We highlight the divergence in estimates obtained when using parametric and data-adaptive methods to estimate the propensity score. We then adapted an existing diagnostic tool based on a bootstrap resampling of the subjects and simulation of the outcome data in order to show that the estimation using data-adaptive methods for the propensity score in this study may lead to large bias and poor coverage. The adapted bootstrap procedure is able to identify this instability and can be used as a diagnostic tool.",
      "journal": "Statistical methods in medical research",
      "year": "2019",
      "doi": "10.1177/0962280218772065",
      "authors": "Bahamyirou Asma et al.",
      "keywords": "Causal inference; IPTW; TMLE; doubly robust; positivity; super learner",
      "mesh_terms": "Bias; Causality; Humans; Machine Learning; Models, Statistical; Probability; Propensity Score",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29717941/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "31932779",
      "title": "Treating health disparities with artificial intelligence.",
      "abstract": "",
      "journal": "Nature medicine",
      "year": "2020",
      "doi": "10.1038/s41591-019-0649-2",
      "authors": "Chen Irene Y et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artificial Intelligence; Clinical Decision-Making; Healthcare Disparities; Humans",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31932779/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "31932798",
      "title": "Diagnosing bias in data-driven algorithms for healthcare.",
      "abstract": "",
      "journal": "Nature medicine",
      "year": "2020",
      "doi": "10.1038/s41591-019-0726-6",
      "authors": "Wiens Jenna et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Bias; Costs and Cost Analysis; Data Analysis; Delivery of Health Care; Health Expenditures; Humans; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31932798/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "32976062",
      "title": "One Algorithm May Not Fit All: How Selection Bias Affects Machine Learning Performance.",
      "abstract": "Machine learning (ML) algorithms have demonstrated high diagnostic accuracy in identifying and categorizing disease on radiologic images. Despite the results of initial research studies that report ML algorithm diagnostic accuracy similar to or exceeding that of radiologists, the results are less impressive when the algorithms are installed at new hospitals and are presented with new images. This phenomenon is potentially the result of selection bias in the data that were used to develop the ML algorithm. Selection bias has long been described by clinical epidemiologists as a key consideration when designing a clinical research study, but this concept has largely been unaddressed in the medical imaging ML literature. The authors discuss the importance of selection bias and its relevance to ML algorithm development to prepare the radiologist to critically evaluate ML literature for potential selection bias and understand how it might affect the applicability of ML algorithms in real clinical environments. \u00a9RSNA, 2020.",
      "journal": "Radiographics : a review publication of the Radiological Society of North America, Inc",
      "year": "2020",
      "doi": "10.1148/rg.2020200040",
      "authors": "Yu Alice C et al.",
      "keywords": "",
      "mesh_terms": "Diagnostic Imaging; Humans; Machine Learning; Selection Bias; Terminology as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32976062/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33453777",
      "title": "Deep learning and cancer biomarkers: recognising lead-time bias - Authors' reply.",
      "abstract": "",
      "journal": "Lancet (London, England)",
      "year": "2021",
      "doi": "10.1016/S0140-6736(21)00035-0",
      "authors": "Liest\u00f8l Knut et al.",
      "keywords": "",
      "mesh_terms": "Bias; Biomarkers, Tumor; Deep Learning; Humans; Neoplasms",
      "pub_types": "Letter; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33453777/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33473211",
      "title": "Monitoring hiring discrimination through online recruitment platforms.",
      "abstract": "Women (compared to men) and individuals from minority ethnic groups (compared to the majority group) face unfavourable labour market outcomes in many economies1,2, but the extent to which discrimination is responsible for these effects, and the channels through which they occur, remain unclear3,4. Although correspondence tests5-in which researchers send fictitious CVs that are identical except for the randomized minority trait to be tested (for example, names that are deemed to sound 'Black' versus those deemed to sound 'white')-are an increasingly popular method to quantify discrimination in hiring practices6,7, they can usually consider only a few applicant characteristics in select occupations at a particular point in time. To overcome these limitations, here we develop an approach to investigate hiring discrimination that combines tracking of the search behaviour of recruiters on employment websites and supervised machine learning to control for all relevant jobseeker characteristics that are visible to recruiters. We apply this methodology to the online recruitment platform of the Swiss public employment service and find that rates of contact by recruiters are 4-19% lower for individuals from immigrant and minority ethnic groups, depending on their country of origin, than for citizens from the majority group. Women experience a penalty of 7% in professions that are dominated by men, and the opposite pattern emerges for men in professions that are dominated by women. We find no evidence that recruiters spend less time evaluating the profiles of individuals from minority ethnic groups. Our methodology provides a widely applicable, non-intrusive and cost-efficient tool that researchers and policy-makers can use to continuously monitor hiring discrimination, to identify some of the drivers of discrimination and to inform approaches to counter it.",
      "journal": "Nature",
      "year": "2021",
      "doi": "10.1038/s41586-020-03136-0",
      "authors": "Hangartner Dominik et al.",
      "keywords": "",
      "mesh_terms": "Emigrants and Immigrants; Employment; Ethnicity; Female; Gender Role; Humans; Internationality; Internet; Male; Minority Groups; Occupations; Personnel Selection; Prejudice; Salaries and Fringe Benefits; Sexism; Stereotyping; Supervised Machine Learning; Switzerland; Time Factors",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33473211/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33487321",
      "title": "Selection Bias in the Predictive Analytics With Machine-Learning Algorithm.",
      "abstract": "",
      "journal": "Annals of emergency medicine",
      "year": "2021",
      "doi": "10.1016/j.annemergmed.2020.09.004",
      "authors": "Jiang Jiyuan",
      "keywords": "",
      "mesh_terms": "Acute Kidney Injury; Algorithms; Electronic Health Records; Emergency Service, Hospital; Humans; Machine Learning; Selection Bias",
      "pub_types": "Letter; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33487321/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33866303",
      "title": "Reducing bias to source samples for unsupervised domain adaptation.",
      "abstract": "Unsupervised Domain Adaptation (UDA) makes predictions for the target domain data while labels are only available in the source domain. Lots of works in UDA focus on finding a common representation of the two domains via domain alignment, assuming that a classifier trained in the source domain can be generalized well to the target domain. Thus, most existing UDA methods only consider minimizing the domain discrepancy without enforcing any constraint on the classifier. However, due to the uniqueness of each domain, it is difficult to achieve a perfect common representation, especially when there is low similarity between the source domain and the target domain. As a consequence, the classifier is biased to the source domain features and makes incorrect predictions on the target domain. To address this issue, we propose a novel approach named reducing bias to source samples for unsupervised domain adaptation (RBDA) by jointly matching the distribution of the two domains and reducing the classifier's bias to source samples. Specifically, RBDA first conditions the adversarial networks with the cross-covariance of learned features and classifier predictions to match the distribution of two domains. Then to reduce the classifier's bias to source samples, RBDA is designed with three effective mechanisms: a mean teacher model to guide the training of the original model, a regularization term to regularize the model and an improved cross-entropy loss for better supervised information learning. Comprehensive experiments on several open benchmarks demonstrate that RBDA achieves state-of-the-art results, which show its effectiveness for unsupervised domain adaptation scenarios.",
      "journal": "Neural networks : the official journal of the International Neural Network Society",
      "year": "2021",
      "doi": "10.1016/j.neunet.2021.03.021",
      "authors": "Ye Yalan et al.",
      "keywords": "Domain adaptation; Generative adversarial network; Transfer learning",
      "mesh_terms": "Bias; Deep Learning; Supervised Machine Learning; Unsupervised Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33866303/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "33934362",
      "title": "Australia in 2030: what is our path to health for all?",
      "abstract": "CHAPTER 1: HOW AUSTRALIA IMPROVED HEALTH EQUITY THROUGH ACTION ON THE SOCIAL DETERMINANTS OF HEALTH: Do not think that the social determinants of health equity are old hat. In reality, Australia is very far away from addressing the societal level drivers of health inequity. There is little progressive policy that touches on the conditions of daily life that matter for health, and action to redress inequities in power, money and resources is almost non-existent. In this chapter we ask you to pause this reality and come on a fantastic journey where we envisage how COVID-19 was a great disruptor and accelerator of positive progressive action. We offer glimmers of what life could be like if there was committed and real policy action on the social determinants of health equity. It is vital that the health sector assists in convening the multisectoral stakeholders necessary to turn this fantasy into reality. CHAPTER 2: ABORIGINAL AND TORRES STRAIT ISLANDER CONNECTION TO CULTURE: BUILDING STRONGER INDIVIDUAL AND COLLECTIVE WELLBEING: Aboriginal and Torres Strait Islander peoples have long maintained that culture (ie, practising, maintaining and reclaiming it) is vital to good health and wellbeing. However, this knowledge and understanding has been dismissed or described as anecdotal or intangible by Western research methods and science. As a result, Aboriginal and Torres Strait Islander culture is a poorly acknowledged determinant of health and wellbeing, despite its significant role in shaping individuals, communities and societies. By extension, the cultural determinants of health have been poorly defined until recently. However, an increasing amount of scientific evidence supports what Aboriginal and Torres Strait Islander people have always said - that strong culture plays a significant and positive role in improved health and wellbeing. Owing to known gaps in knowledge, we aim to define the cultural determinants of health and describe their relationship with the social determinants of health, to provide a full understanding of Aboriginal and Torres Strait Islander wellbeing. We provide examples of evidence on cultural determinants of health and links to improved Aboriginal and Torres Strait Islander health and wellbeing. We also discuss future research directions that will enable a deeper understanding of the cultural determinants of health for Aboriginal and Torres Strait Islander people. CHAPTER 3: PHYSICAL DETERMINANTS OF HEALTH: HEALTHY, LIVEABLE AND SUSTAINABLE COMMUNITIES: Good city planning is essential for protecting and improving human and planetary health. Until recently, however, collaboration between city planners and the public health sector has languished. We review the evidence on the health benefits of good city planning and propose an agenda for public health advocacy relating to health-promoting city planning for all by 2030. Over the next 10 years, there is an urgent need for public health leaders to collaborate with city planners - to advocate for evidence-informed policy, and to evaluate the health effects of city planning efforts. Importantly, we need integrated planning across and between all levels of government and sectors, to create healthy, liveable and sustainable cities for all. CHAPTER 4: HEALTH PROMOTION IN THE ANTHROPOCENE: THE ECOLOGICAL DETERMINANTS OF HEALTH: Human health is inextricably linked to the health of the natural environment. In this chapter, we focus on ecological determinants of health, including the urgent and critical threats to the natural environment, and opportunities for health promotion arising from the human health co-benefits of actions to protect the health of the planet. We characterise ecological determinants in the Anthropocene and provide a sobering snapshot of planetary health science, particularly the momentous climate change health impacts in Australia. We highlight Australia's position as a major fossil fuel producer and exporter, and a country lacking cohesive and timely emissions reduction policy. We offer a roadmap for action, with four priority directions, and point to a scaffold of guiding approaches - planetary health, Indigenous people's knowledge systems, ecological economics, health co-benefits and climate-resilient development. Our situation requires a paradigm shift, and this demands a recalibration of health promotion education, research and practice in Australia over the coming decade. CHAPTER 5: DISRUPTING THE COMMERCIAL DETERMINANTS OF HEALTH: Our vision for 2030 is an Australian economy that promotes optimal human and planetary health for current and future generations. To achieve this, current patterns of corporate practice and consumption of harmful commodities and services need to change. In this chapter, we suggest ways forward for Australia, focusing on pragmatic actions that can be taken now to redress the power imbalances between corporations and Australian governments and citizens. We begin by exploring how the terms of health policy making must change to protect it from conflicted commercial interests. We also examine how marketing unhealthy products and services can be more effectively regulated, and how healthier business practices can be incentivised. Finally, we make recommendations on how various public health stakeholders can hold corporations to account, to ensure that people come before profits in a healthy and prosperous future Australia. CHAPTER 6: DIGITAL DETERMINANTS OF HEALTH: THE DIGITAL TRANSFORMATION: We live in an age of rapid and exponential technological change. Extraordinary digital advancements and the fusion of technologies, such as artificial intelligence, robotics, the Internet of Things and quantum computing constitute what is often referred to as the digital revolution or the Fourth Industrial Revolution (Industry 4.0). Reflections on the future of public health and health promotion require thorough consideration of the role of digital technologies and the systems they influence. Just how the digital revolution will unfold is unknown, but it is clear that advancements and integrations of technologies will fundamentally influence our health and wellbeing in the future. The public health response must be proactive, involving many stakeholders, and thoughtfully considered to ensure equitable and ethical applications and use. CHAPTER 7: GOVERNANCE FOR HEALTH AND EQUITY: A VISION FOR OUR FUTURE: Coronavirus disease 2019 has caused many people and communities to take stock on Australia's direction in relation to health, community, jobs, environmental sustainability, income and wealth. A desire for change is in the air. This chapter imagines how changes in the way we govern our lives and what we value as a society could solve many of the issues Australia is facing - most pressingly, the climate crisis and growing economic and health inequities. We present an imagined future for 2030 where governance structures are designed to ensure transparent and fair behaviour from those in power and to increase the involvement of citizens in these decisions, including a constitutional voice for Indigenous peoples. We imagine that these changes were made by measuring social progress in new ways, ensuring taxation for public good, enshrining human rights (including to health) in legislation, and protecting and encouraging an independent media. Measures to overcome the climate crisis were adopted and democratic processes introduced in the provision of housing, education and community development.",
      "journal": "The Medical journal of Australia",
      "year": "2021",
      "doi": "10.5694/mja2.51020",
      "authors": "Backholer Kathryn et al.",
      "keywords": "",
      "mesh_terms": "Australia; Commerce; Community Health Planning; Digital Technology; Environmental Health; Forecasting; Health Equity; Health Promotion; Health Services, Indigenous; Humans; Social Determinants of Health; Australian Aboriginal and Torres Strait Islander Peoples",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33934362/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "34062138",
      "title": "Artificial intelligence, bias, and patients' perspectives.",
      "abstract": "",
      "journal": "Lancet (London, England)",
      "year": "2021",
      "doi": "10.1016/S0140-6736(21)01152-1",
      "authors": "Obermeyer Ziad et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artificial Intelligence; Diagnosis; Humans; Patients; Prejudice; Racism",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34062138/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "34116365",
      "title": "Route map for machine learning in psychiatry: Absence of bias, reproducibility, and utility.",
      "abstract": "",
      "journal": "European neuropsychopharmacology : the journal of the European College of Neuropsychopharmacology",
      "year": "2021",
      "doi": "10.1016/j.euroneuro.2021.05.006",
      "authors": "Radua Joaquim et al.",
      "keywords": "",
      "mesh_terms": "Bias; Machine Learning; Psychiatry; Reproducibility of Results",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34116365/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "34224606",
      "title": "Gender bias in resident evaluations: Natural language processing and competency evaluation.",
      "abstract": "BACKGROUND: Research shows that female trainees experience evaluation penalties for gender non-conforming behaviour during medical training. Studies of medical education evaluations and performance scores do reflect a gender bias, though studies are of varying methodology and results have not been consistent. OBJECTIVE: We sought to examine the differences in word use, competency themes and length within written evaluations of internal medicine residents at scale, considering the impact of both faculty and resident gender. We hypothesised that female internal medicine residents receive more negative feedback, and different thematic feedback than male residents. METHODS: This study utilised a corpus of 3864 individual responses to positive and negative questions over the course of six years (2012-2018) within Yale University School of Medicine's internal medicine residency. Researchers developed a sentiment model to assess the valence of evaluation responses. We then used natural language processing (NLP) to evaluate whether female versus male residents received more positive or negative feedback and if that feedback focussed on different Accreditation Council for Graduate Medical Education (ACGME) core competencies based on their gender. Evaluator-evaluatee gender dyad was analysed to see how it impacted quantity and quality of feedback. RESULTS: We found that female and male residents did not have substantively different numbers of positive or negative comments. While certain competencies were discussed more than others, gender did not seem to influence which competencies were discussed. Neither gender trainee received more written feedback, though female evaluators tended to write longer evaluations. CONCLUSIONS: We conclude that when examined at scale, quantitative gender differences are not as prevalent as has been seen in qualitative work. We suggest that further investigation of linguistic phenomena (such as context) is warranted to reconcile this finding with prior work.",
      "journal": "Medical education",
      "year": "2021",
      "doi": "10.1111/medu.14593",
      "authors": "Andrews Jane et al.",
      "keywords": "",
      "mesh_terms": "Clinical Competence; Education, Medical, Graduate; Female; Humans; Internship and Residency; Male; Natural Language Processing; Sexism",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34224606/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "34546129",
      "title": "Artificial Intelligence and Health Care Disparities in Radiology.",
      "abstract": "",
      "journal": "Radiology",
      "year": "2021",
      "doi": "10.1148/radiol.2021210566",
      "authors": "Sorin Vera et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Healthcare Disparities; Humans; Radiography; Radiology",
      "pub_types": "Letter; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34546129/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35323862",
      "title": "Reducing healthcare disparities using multiple multiethnic data distributions with fine-tuning of transfer learning.",
      "abstract": "Healthcare disparities in multiethnic medical data is a major challenge; the main reason lies in the unequal data distribution of ethnic groups among data cohorts. Biomedical data collected from different cancer genome research projects may consist of mainly one ethnic group, such as people with European ancestry. In contrast, the data distribution of other ethnic races such as African, Asian, Hispanic, and Native Americans can be less visible than the counterpart. Data inequality in the biomedical field is an important research problem, resulting in the diverse performance of machine learning models while creating healthcare disparities. Previous researches have reduced the healthcare disparities only using limited data distributions. In our study, we work on fine-tuning of deep learning and transfer learning models with different multiethnic data distributions for the prognosis of 33 cancer types. In previous studies, to reduce the healthcare disparities, only a single ethnic cohort was used as the target domain with one major source domain. In contrast, we focused on multiple ethnic cohorts as the target domain in transfer learning using the TCGA and MMRF CoMMpass study datasets. After performance comparison for experiments with new data distributions, our proposed model shows promising performance for transfer learning schemes compared to the baseline approach for old and new data distributation experiments.",
      "journal": "Briefings in bioinformatics",
      "year": "2022",
      "doi": "10.1093/bib/bbac078",
      "authors": "Toseef Muhammad et al.",
      "keywords": "deep learning; domain adaptation; ethnic disparities; transfer learning",
      "mesh_terms": "Ethnicity; Healthcare Disparities; Hispanic or Latino; Humans; Machine Learning; Neoplasms",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35323862/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35430003",
      "title": "Follow-up after post-partum psychiatric emergency department visits: an equity-focused population-based study in Canada.",
      "abstract": "BACKGROUND: Emergency department visits for a psychiatric reason in the post-partum period represent an acute need for mental health care at a crucial time, but little is known about the extent of timely outpatient follow-up after these visits or how individual and intersecting social determinants of health influence this outcome. This study aimed to examine outpatient mental health care follow-up by a physician in the 30 days after an individual attended the emergency department for a psychiatric reason in the post-partum period and understand how social determinants of health affect who receives follow-up care. METHODS: In this population-based cohort study, routinely collected health data from Ontario, Canada were accessed through ICES to identify all post-partum individuals whose sex was listed as female on their health card and who had attended an emergency department in Ontario before the COVID-19 pandemic for a psychiatric reason. Individuals admitted to hospital at the time of the emergency department visit, who died during the visit, or who left without being seen were excluded from the study. Ethnicity data for individuals were not collected. The primary outcome was the proportion of individuals with any outpatient physician (psychiatrist or family physician) visit for a mental health reason within 30 days of the index emergency department visit. Family physician mental health visits were identified using a validated algorithm for Ontario Health Insurance Plan-billed visits and mental health diagnostic codes for community health centre visits. We examined the associations between social determinants of health (age, neighbourhood income, community size, immigration, neighbourhood ethnic diversity) and who received an outpatient mental health visit. We used modified Poisson regression adjusting for the other social determinants of health, clinical, and health services characteristics to examine independent associations with follow-up, and conditional inference trees to explore how social determinants of health intersect with each other and with clinical and health services characteristics in relation to follow-up. FINDINGS: We analysed data collected between April 1, 2008, and March 10, 2020, after exclusions we identified 12\u2008158 people who had attended the emergency department for a psychiatric reason in the post-partum period (mean age 26\u00b79 years [SD 6\u00b72]; range 13-47); 9848 individuals lived in an urban area, among these 1518 (15\u00b75%) were immigrants and 2587 (26\u00b73%) lived in areas with high ethnic diversity. 5442 (44\u00b78%) of 12\u2008158 individuals received 30-day follow-up. In modified Poisson regression models, younger age, lower neighbourhood income, smaller community size, and being an immigrant were associated with a lower likelihood of follow-up. In the CTREE, similar variables were important, with several intersections between social determinants of health and between social determinants of health and other variables. INTERPRETATION: Fewer than half of emergency department visits for a psychiatric reason in the post-partum period were followed by timely outpatient care, with social-determinants-of-health-based disparities in access to care. Improvements in equitable access to post-emergency department mental health care are urgently needed in this high-risk post-partum population. FUNDING: Department of Psychiatry, University of Toronto; Canadian Institutes of Health Research.",
      "journal": "The lancet. Psychiatry",
      "year": "2022",
      "doi": "10.1016/S2215-0366(22)00099-2",
      "authors": "Barker Lucy C et al.",
      "keywords": "",
      "mesh_terms": "Adult; COVID-19; Cohort Studies; Emergency Service, Hospital; Female; Follow-Up Studies; Humans; Ontario; Pandemics; Postpartum Period",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35430003/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35612086",
      "title": "Preprocessing to Address Bias in Healthcare Data.",
      "abstract": "Multimorbidity, having a diagnosis of two or more chronic conditions, increases as people age. It is a predictor used in clinical decision-making, but underdiagnosis in underserved populations produces bias in the data that support algorithms used in the healthcare processes. Artificial intelligence (AI) systems could produce inaccurate predictions if patients have multiple unknown conditions. Rural patients are more likely to be underserved and also more likely to have multiple chronic conditions. In this study, data collected during the course of care in a centrally located academic hospital, multimorbidity decreased with rurality. This decrease suggests a bias against rural patients for algorithms that rely on diagnosis information to calculate risk. To test preprocessing to address bias in healthcare data, we measured the amount of discrimination in favor of metropolitan patients in the classification of multimorbidity. We built a model using the biased data to test optimum classification performance. A new unbiased training data set and model were created and tested against unaltered validation data. The new model's classification performance on unaltered data did not diverge significantly from the performance of the initial optimal model trained on the biased data suggesting that bias can be removed with preprocessing.",
      "journal": "Studies in health technology and informatics",
      "year": "2022",
      "doi": "10.3233/SHTI220468",
      "authors": "Seker Emel et al.",
      "keywords": "Artificial Intelligence; Data Bias; Underserved populations",
      "mesh_terms": "Algorithms; Artificial Intelligence; Bias; Delivery of Health Care; Health Facilities; Humans",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35612086/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35710992",
      "title": "Reply to: 'Potential sources of dataset bias complicate investigation of underdiagnosis by machine learning algorithms' and 'Confounding factors need to be accounted for in assessing bias by machine learning algorithms'.",
      "abstract": "",
      "journal": "Nature medicine",
      "year": "2022",
      "doi": "10.1038/s41591-022-01854-8",
      "authors": "Seyyed-Kalantari Laleh et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Bias; Machine Learning",
      "pub_types": "Letter; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35710992/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35710993",
      "title": "Potential sources of dataset bias complicate investigation of underdiagnosis by machine learning algorithms.",
      "abstract": "",
      "journal": "Nature medicine",
      "year": "2022",
      "doi": "10.1038/s41591-022-01846-8",
      "authors": "Bernhardt M\u00e9lanie et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Bias; Machine Learning",
      "pub_types": "Letter; Research Support, Non-U.S. Gov't; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35710993/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "35973209",
      "title": "Racial Fairness in Precision Medicine: Pediatric Asthma Prediction Algorithms.",
      "abstract": "PURPOSE: Quantify and examine the racial fairness of two widely used childhood asthma predictive precision medicine algorithms: the asthma predictive index (API) and the pediatric asthma risk score (PARS). DESIGN: Apply the API and PARS and evaluate model performance overall and when stratified by race. SETTING: Cincinnati, OH, USA. SUBJECTS: A prospective birth cohort of 590 children with clinically measured asthma diagnosis by age seven. MEASURES: Model diagnostic criteria included sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). ANALYSIS: Significant differences in model performance between Black and white children were considered to be present if the P-value associated with a t-test based on 100 bootstrap replications was less than .05. RESULTS: Compared to predictions for white children, predictions for Black children using the PARS had a higher sensitivity (.88 vs .57), lower specificity (.55 vs .83), higher PPV (.42 vs .33), but a similar NPV (.93 vs .93). Within the API and compared to predictions for white children, predictions for Black children had a higher sensitivity (.63 vs .53), similar specificity (.81 vs .80), higher PPV (.54 vs .28), and lower NPV (.86 vs .92). CONCLUSIONS: Overall, racial disparities in model diagnostic criteria were greatest for sensitivity and specificity in the PARS, but racial disparities existed in three of the four criteria for both the PARS and the API.",
      "journal": "American journal of health promotion : AJHP",
      "year": "2023",
      "doi": "10.1177/08901171221121639",
      "authors": "Pennington Jordan et al.",
      "keywords": "asthma; fairness; pediatric; precision medicine; racial minority",
      "mesh_terms": "Child; Humans; Prospective Studies; Precision Medicine; Risk Factors; Asthma; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35973209/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36074837",
      "title": "Exclusion cycles: Reinforcing disparities in medicine.",
      "abstract": "Clinical practice, data collection, and medical AI constitute self-reinforcing and interacting cycles of exclusion.",
      "journal": "Science (New York, N.Y.)",
      "year": "2022",
      "doi": "10.1126/science.abo2788",
      "authors": "Bracic Ana et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Big Data; Healthcare Disparities; Humans; Minority Groups; Social Isolation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36074837/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36409207",
      "title": "In defence of the machines: How artificial intelligence may help to improve dermatologic outcomes and diminish barriers and disparities in care.",
      "abstract": "",
      "journal": "The Australasian journal of dermatology",
      "year": "2023",
      "doi": "10.1111/ajd.13956",
      "authors": "Cheraghlou Shayan",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Dermatology; Healthcare Disparities",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36409207/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36437714",
      "title": "Diagnosing the Model Bias in Simulating Daily Surface Ozone Variability Using a Machine Learning Method: The Effects of Dry Deposition and Cloud Optical Depth.",
      "abstract": "Machine learning methods are increasingly used in air quality studies to predict air pollution levels, while few applied them to diagnose and improve the underlying mechanisms controlling air pollution represented in chemical transport models (CTMs). Here, we use the random forest (RF) method to diagnose high biases of surface daily maximum 8 h average (MDA8) ozone concentrations in the GEOS-Chem CTM evaluated against measurements from the nationwide monitoring network in summer 2018 over China. The feature importance results show that cloud optical depth (COD), relative humidity, and precipitation are the top three factors affecting CTM high biases. Such results indicate that the high ozone biases in summer over China mainly occur on wet/cloudy days (\u223c40% biased high), while biases on dry/clear days are small (within 5%). We link the important features with model parameterizations and variables, identifying model underestimates in the dry deposition velocity and COD on wet/cloudy days. By accounting for the enhanced dry deposition on wet plant cuticles and using satellite observation constrained COD, we find that CTM high ozone biases can be halved with an improved agreement in the temporal variability, highlighting the effects of dry deposition and COD on ozone, as suggested by the RF outcomes.",
      "journal": "Environmental science & technology",
      "year": "2022",
      "doi": "10.1021/acs.est.2c05712",
      "authors": "Ye Xingpei et al.",
      "keywords": "GEOS-Chem; MDA8 ozone; bias diagnosis; cloud optical depth; dry deposition; random forest",
      "mesh_terms": "Ozone; Air Pollutants; Environmental Monitoring; Air Pollution; Machine Learning; Bias",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36437714/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36529662",
      "title": "Examining Implicit Bias Differences in Pediatric Surgical Fellowship Letters of Recommendation Using Natural Language Processing.",
      "abstract": "OBJECTIVE: We analyzed the prevalence and type of bias in letters of recommendation (LOR) for pediatric surgical fellowship applications from 2016-2021\u202fusing natural language processing (NLP) at a quaternary care\u202facademic hospital. DESIGN: Demographics were extracted from submitted applications. The\u202fValence Aware Dictionary for\u202fsEntiment\u202fReasoning (VADER) model\u202fwas used to calculate polarity scores. The National Research Council\u202fdataset was used for emotion and intensity analysis. \u202fThe Kruskal-Wallis H-test\u202fwas used to determine statistical significance.\u202f SETTING: This study took place at a single, academic, free standing quaternary care children's hospital with an ACGME accredited pediatric surgery fellowship. PARTICIPANTS: Applicants to a single pediatric surgery fellowship were selected for this study from 2016 to 2021. A total of 182 individual applicants were included and 701 letters of recommendation were analyzed. RESULTS: Black applicants had the highest mean\u202fpolarity\u202f(most positive), while Hispanic\u202fapplicants had the lowest. \u202fOverall differences between polarity distributions were not statistically significant.\u202f\u202f\u00a0The intensity of emotions showed that differences in \"anger\" were statistically significant (p=0.03).\u202f\u00a0Mean polarity was higher for applicants that successfully matched in pediatric surgery. DISCUSSION: This study identified differences in LORs based on racial and gender demographics submitted as part of pediatric surgical fellowship applications to a single training program. The presence of bias in letters of recommendation can lead to inequities in demographics to a given program. While difficult to detect for humans, natural language processing is able to detect bias as well as differences in polarity and emotional intensity. While the\u202ftypes of emotions identified in this study are highly similar among race and gender groups, the intensity of these emotions\u202frevealed differences,\u202fwith \"anger\" being most significant. CONCLUSION: From this work, it can be concluded that\u202fbias in LORs, as reflected as\u202fdifferences in polarity,\u202fwhich is likely a result of the intensity of the emotions being used and not the types of emotions being expressed.\u202f\u202f\u00a0Natural language processing shows promise in identification of subtle areas of bias that may influence an individual's likelihood of successful matching.",
      "journal": "Journal of surgical education",
      "year": "2023",
      "doi": "10.1016/j.jsurg.2022.12.002",
      "authors": "Gray Geoffrey M et al.",
      "keywords": "bias; letters of recommendation; natural language processing; pediatric surgery fellowship; valence aware dictionary for\u202fsentiment\u202freasoning (VADER)",
      "mesh_terms": "Child; Humans; Fellowships and Scholarships; Natural Language Processing; Bias, Implicit; Personnel Selection; Specialties, Surgical; Internship and Residency",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36529662/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36578121",
      "title": "Implicit Bias and Machine Learning in Health Care.",
      "abstract": "",
      "journal": "Southern medical journal",
      "year": "2023",
      "doi": "10.14423/SMJ.0000000000001489",
      "authors": "Zaidi Danish et al.",
      "keywords": "",
      "mesh_terms": "Humans; Bias, Implicit; Attitude of Health Personnel; Curriculum; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36578121/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "36655991",
      "title": "The Bias-Variance Tradeoff in Cognitive Science.",
      "abstract": "The bias-variance tradeoff is a theoretical concept that suggests machine learning algorithms are susceptible to two kinds of error, with some algorithms tending to suffer from one more than the other. In this letter, we claim that the bias-variance tradeoff is a general concept that can be applied to human cognition as well, and we discuss implications for research in cognitive science. In particular, we show how various strands of research in cognitive science can be interpreted in light of the bias-variance tradeoff, giving insight into individual differences in learning, the nature of cognitive processes, and debates in cognitive science research.",
      "journal": "Cognitive science",
      "year": "2023",
      "doi": "10.1111/cogs.13241",
      "authors": "Doroudi Shayan et al.",
      "keywords": "Bias-variance tradeoff; Epistemology; Human learning; Individual differences; Machine learning",
      "mesh_terms": "Humans; Algorithms; Machine Learning; Cognition; Bias",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36655991/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37203624",
      "title": "Classification of Parkinson's Disease from Voice - Analysis of Data Selection Bias.",
      "abstract": "A growing number of studies have been researching biomarkers of Parkinson's disease (PD) using mobile technology. Many have shown high accuracy in PD classification using machine learning (ML) and voice records from the mPower study, a large database of PD patients and healthy controls. Since the dataset has unbalanced class, gender and age distribution, it is important to consider appropriate sampling when assessing classification scores. We analyse biases, such as identity confounding and implicit learning of non-disease-specific characteristics and present a sampling strategy to highlight and prevent these problems.",
      "journal": "Studies in health technology and informatics",
      "year": "2023",
      "doi": "10.3233/SHTI230079",
      "authors": "Brenner Alexander et al.",
      "keywords": "Machine Learning; Parkinson\u2019s Disease; Selection Bias",
      "mesh_terms": "Humans; Parkinson Disease; Selection Bias; Voice; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37203624/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37330053",
      "title": "An investigation into the risk of population bias in deep learning autocontouring.",
      "abstract": "BACKGROUND AND PURPOSE: To date, data used in the development of Deep Learning-based automatic contouring (DLC) algorithms have been largely sourced from single geographic populations. This study aimed to evaluate the risk of population-based bias by determining whether the performance of an autocontouring system is impacted by geographic population. MATERIALS AND METHODS: 80 Head Neck CT deidentified scans were collected from four clinics in Europe (n\u00a0=\u00a02) and Asia (n\u00a0=\u00a02). A single observer manually delineated 16 organs-at-risk in each. Subsequently, the data was contoured using a DLC solution, and trained using single institution (European) data. Autocontours were compared to manual delineations using quantitative measures. A Kruskal-Wallis test was used to test for any difference between populations. Clinical acceptability of automatic and manual contours to observers from each participating institution was assessed using a blinded subjective evaluation. RESULTS: Seven organs showed a significant difference in volume between groups. Four organs showed statistical differences in quantitative similarity measures. The qualitative test showed greater variation in acceptance of contouring between observers than between data from different origins, with greater acceptance by the South Korean observers. CONCLUSION: Much of the statistical difference in quantitative performance could be explained by the difference in organ volume impacting the contour similarity measures and the small sample size. However, the qualitative assessment suggests that observer perception bias has a greater impact on the apparent clinical acceptability than quantitatively observed differences. This investigation of potential geographic bias should extend to more patients, populations, and anatomical regions in the future.",
      "journal": "Radiotherapy and oncology : journal of the European Society for Therapeutic Radiology and Oncology",
      "year": "2023",
      "doi": "10.1016/j.radonc.2023.109747",
      "authors": "McQuinlan Yasmin et al.",
      "keywords": "Autocontouring; Bias; Deep learning; Organ-at-Risk; Radiotherapy; Segmentation",
      "mesh_terms": "Humans; Deep Learning; Tomography, X-Ray Computed; Algorithms; Observer Variation; Europe; Organs at Risk; Radiotherapy Planning, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37330053/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "37878441",
      "title": "My Model is Unfair, Do People Even Care? Visual Design Affects Trust and Perceived Bias in Machine Learning.",
      "abstract": "Machine learning technology has become ubiquitous, but, unfortunately, often exhibits bias. As a consequence, disparate stakeholders need to interact with and make informed decisions about using machine learning models in everyday systems. Visualization technology can support stakeholders in understanding and evaluating trade-offs between, for example, accuracy and fairness of models. This paper aims to empirically answer \"Can visualization design choices affect a stakeholder's perception of model bias, trust in a model, and willingness to adopt a model?\" Through a series of controlled, crowd-sourced experiments with more than 1,500 participants, we identify a set of strategies people follow in deciding which models to trust. Our results show that men and women prioritize fairness and performance differently and that visual design choices significantly affect that prioritization. For example, women trust fairer models more often than men do, participants value fairness more when it is explained using text than as a bar chart, and being explicitly told a model is biased has a bigger impact than showing past biased performance. We test the generalizability of our results by comparing the effect of multiple textual and visual design choices and offer potential explanations of the cognitive mechanisms behind the difference in fairness perception and trust. Our research guides design considerations to support future work developing visualization systems for machine learning.",
      "journal": "IEEE transactions on visualization and computer graphics",
      "year": "2024",
      "doi": "10.1109/TVCG.2023.3327192",
      "authors": "Gaba Aimen et al.",
      "keywords": "",
      "mesh_terms": "Male; Humans; Female; Trust; Computer Graphics; Machine Learning; Bias; Surveys and Questionnaires",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37878441/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38554626",
      "title": "Comparing survival of older ovarian cancer patients treated with neoadjuvant chemotherapy versus primary cytoreductive surgery: Reducing bias through machine learning.",
      "abstract": "OBJECTIVE: To develop and evaluate a multidimensional comorbidity index (MCI) that identifies ovarian cancer patients at risk of early mortality more accurately than the Charlson Comorbidity Index (CCI) for use in health services research. METHODS: We utilized SEER-Medicare data to identify patients with stage IIIC and IV ovarian cancer, diagnosed in 2010-2015. We employed partial least squares regression, a supervised machine learning algorithm, to develop the MCI by extracting latent factors that optimally captured the variation in health insurance claims made in the year preceding cancer diagnosis, and 1-year mortality. We assessed the discrimination and calibration of the MCI for 1-year mortality and compared its performance to the commonly-used CCI. Finally, we evaluated the MCI's ability to reduce confounding in the association of neoadjuvant chemotherapy (NACT) and all-cause mortality. RESULTS: We included 4723 patients in the development cohort and 933 in the validation cohort. The MCI demonstrated good discrimination for 1-year mortality (c-index: 0.75, 95% CI: 0.72-0.79), while the CCI had poor discrimination (c-index: 0.59, 95% CI: 0.56-0.63). Calibration plots showed better agreement between predicted and observed 1-year mortality risk for the MCI compared with CCI. When comparing all-cause mortality between NACT with primary cytoreductive surgery, NACT was associated with a higher hazard of death (HR: 1.13, 95% CI: 1.04-1.23) after controlling for tumor characteristics, demographic factors, and the CCI. However, when controlling for the MCI instead of the CCI, there was no longer a significant difference (HR: 1.05, 95% CI: 0.96-1.14). CONCLUSIONS: The MCI outperformed the conventional CCI in predicting 1-year mortality, and reducing confounding due to differences in baseline health status in comparative effectiveness analysis of NACT versus primary surgery.",
      "journal": "Gynecologic oncology",
      "year": "2024",
      "doi": "10.1016/j.ygyno.2024.03.016",
      "authors": "Huang Yongmei et al.",
      "keywords": "All-cause mortality; Machine learning; Multidimensional comorbidity index; Neoadjuvant chemotherapy; Primary cytoreductive surgery",
      "mesh_terms": "Humans; Female; Cytoreduction Surgical Procedures; Neoadjuvant Therapy; Aged; Ovarian Neoplasms; Machine Learning; SEER Program; Aged, 80 and over; United States; Chemotherapy, Adjuvant; Bias; Carcinoma, Ovarian Epithelial; Neoplasm Staging; Medicare",
      "pub_types": "Journal Article; Comparative Study; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38554626/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "38663811",
      "title": "Prevalence, mortality, cost, and disparities in transcatheter mitral valve repair and replacement in cancer patients: Artificial intelligence and propensity score national 5-year analysis of 7495 procedures.",
      "abstract": "INTRODUCTION: We conducted the first comprehensive evaluation of the therapeutic value and safety profile of transcatheter mitral edge-to-edge repair (TEER) and transcatheter mitral valve replacement (TMVR) in individuals concurrently afflicted with cancer. METHODS: Utilizing the National Inpatient Sample (NIS) dataset, we analyzed all adult hospitalizations between 2016 and 2020 (n\u00a0=\u00a0148,755,036). The inclusion criteria for this retrospectively analyzed prospective cohort study were all adult hospitalizations (age 18\u00a0years and older). Regression and machine learning analyses in addition to model optimization were conducted using ML-PSr (Machine Learning-augmented Propensity Score adjusted multivariable regression) and BAyesian Machine learning-augmented Propensity Score (BAM-PS) multivariable regression. RESULTS: Of all adult hospitalizations, there were 5790 (0.004%) TMVRs and 1705 (0.001%) TEERs. Of the total TMVRs, 160 (2.76%) were done in active cancer. Of the total TEERs, 30 (1.76%) were done in active cancer. After the comparable rates of TEER/TMVR in active cancer in 2016, the prevalence of TEER/TMVR was significantly less in active cancer from 2017 to 2020 (2.61% versus 7.28% p\u00a0<\u00a00.001). From 2017 to 2020, active cancer significantly decreased the odds of receiving TEER or TMVR (OR 0.28, 95%CI 0.13-0.68, p\u00a0=\u00a00.008). In patients with active cancer who underwent TMVR/TEER, there were no significant differences in socio-economic disparities, mortality or total hospitalization costs. CONCLUSION: The presence of malignancy does not contribute to increased mortality, length of stay or procedural costs in TMVR or TEER. Whereas the prevalence of TMVR has increased in patients with active cancer, the utilization of TEER in the context of active cancer is declining despite a growing patient population.",
      "journal": "International journal of cardiology",
      "year": "2024",
      "doi": "10.1016/j.ijcard.2024.132091",
      "authors": "Marmagkiolis Konstantinos et al.",
      "keywords": "Cardio-oncology; Interventional cardiology; Valvular heart disease",
      "mesh_terms": "Humans; Male; Female; Neoplasms; Aged; Propensity Score; Heart Valve Prosthesis Implantation; Middle Aged; Artificial Intelligence; Prevalence; Mitral Valve Insufficiency; United States; Retrospective Studies; Cardiac Catheterization; Prospective Studies; Adult; Aged, 80 and over; Healthcare Disparities; Cohort Studies",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38663811/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39009778",
      "title": "A comprehensive cancer center in the cloud powered by AI can reduce health disparities.",
      "abstract": "",
      "journal": "Nature medicine",
      "year": "2024",
      "doi": "10.1038/s41591-024-03119-y",
      "authors": "Ngwa Wilfred et al.",
      "keywords": "",
      "mesh_terms": "Humans; Neoplasms; Cloud Computing; Artificial Intelligence; Cancer Care Facilities; Healthcare Disparities",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39009778/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39033881",
      "title": "Social determinants of health and disparities in spine surgery: a 10-year analysis of 8,565 cases using ensemble machine learning and multilayer perceptron.",
      "abstract": "BACKGROUND CONTEXT: The influence of SDOH on spine surgery is poorly understood. Historically, researchers commonly focused on the isolated influences of race, insurance status, or income on healthcare outcomes. However, analysis of SDOH is becoming increasingly more nuanced as viewing social factors in aggregate rather than individually may offer more precise estimates of the impact of SDOH on healthcare delivery. PURPOSE: The aim of this study was to evaluate the effects of patient social history on length of stay (LOS) and readmission within 90 days following spine surgery using ensemble machine learning and multilayer perceptron. STUDY DESIGN: Retrospective chart review. PATIENT SAMPLE: A total of 8,565 elective and emergency spine surgery cases performed from 2013 to 2023 using our institution's database of longitudinally collected electronic medical record information. OUTCOMES MEASURES: Patient LOS, discharge disposition, and rate of 90-day readmission. METHODS: Ensemble machine learning and multilayer perceptron were employed to predict LOS and readmission within 90 days following spine surgery. All other subsequent statistical analysis was performed using SPSS version 28. To further assess correlations among variables, Pearson's correlation tests and multivariate linear regression models were constructed. Independent sample t-tests, paired sample t-tests, one-way analysis of variance (ANOVA) with posthoc Bonferroni and Tukey corrections, and Pearson's chi-squared test were applied where appropriate for analysis of continuous and categorical variables. RESULTS: Black patients demonstrated a greater LOS compared to white patients, but race and ethnicity were not significantly associated with 90-day readmission rates. Insured patients had a shorter LOS and lower readmission rates compared to noninsured patients, as did privately insured patients compared to publicly insured patients. Patients discharged home had lower LOS and lower readmission rates, compared to patients discharged to other facilities. Marriage decreased both LOS and readmission rates, underweight patients showcased increased LOS and readmission rates, and religion was shown to impact LOS and readmission rates. When utilizing patient social history, lab values, and medical history, machine learning determined the top 5 most-important variables for prediction of LOS -along with their respective feature importances-to be insurance status (0.166), religion (0.100), ICU status (0.093), antibiotic use (0.061), and case status: elective or urgent (0.055). The top 5 most-important variables for prediction of 90-day readmission-along with their respective feature importances-were insurance status (0.177), religion (0.123), discharge location (0.096), emergency case status (0.064), and history of diabetes (0.041). CONCLUSIONS: This study highlights that SDOH is influential in determining patient length of stay, discharge disposition, and likelihood of readmission following spine surgery. Machine learning was utilized to accurately predict LOS and 90-day readmission with patient medical history, lab values, and social history, as well as social history alone.",
      "journal": "The spine journal : official journal of the North American Spine Society",
      "year": "2025",
      "doi": "10.1016/j.spinee.2024.07.003",
      "authors": "Shin David et al.",
      "keywords": "Artificial intelligence; Length of stay; Machine learning; Race; Readmission; Social determinants of health; Spine surgery",
      "mesh_terms": "Humans; Machine Learning; Female; Male; Retrospective Studies; Patient Readmission; Middle Aged; Length of Stay; Social Determinants of Health; Healthcare Disparities; Adult; Aged; Spine; Orthopedic Procedures; Multilayer Perceptrons",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39033881/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39107521",
      "title": "Free access via computational cloud to deep learning-based EEG assessment in neonatal hypoxic-ischemic encephalopathy: revolutionary opportunities to overcome health disparities.",
      "abstract": "In this issue of Pediatric Research, Kota et al. evaluate a novel monitoring visual trend using deep-learning - Brain State of the Newborn (BSN)- based EEG as a bedside marker for severity of the encephalopathy in 46 neonates with hypoxic-ischemic encephalopathy (HIE) compared with healthy infants. Early BSN distinguished between normal and abnormal outcome, and correlated with the Total Sarnat Score.",
      "journal": "Pediatric research",
      "year": "2024",
      "doi": "10.1038/s41390-024-03427-6",
      "authors": "Dilena Robertino et al.",
      "keywords": "",
      "mesh_terms": "Humans; Hypoxia-Ischemia, Brain; Deep Learning; Electroencephalography; Infant, Newborn; Healthcare Disparities; Brain",
      "pub_types": "Journal Article; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39107521/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39438057",
      "title": "Gender and Ethnicity Bias of Text-to-Image Generative Artificial Intelligence in Medical Imaging, Part 1: Preliminary Evaluation.",
      "abstract": "Generative artificial intelligence (AI) text-to-image production could reinforce or amplify gender and ethnicity biases. Several text-to-image generative AI tools are used for producing images that represent the medical imaging professions. White male stereotyping and masculine cultures can dissuade women and ethnically divergent people from being drawn into a profession. Methods: In March 2024, DALL-E 3, Firefly 2, Stable Diffusion 2.1, and Midjourney 5.2 were utilized to generate a series of individual and group images of medical imaging professionals: radiologist, nuclear medicine physician, radiographer, and nuclear medicine technologist. Multiple iterations of images were generated using a variety of prompts. Collectively, 184 images were produced for evaluation of 391 characters. All images were independently analyzed by 3 reviewers for apparent gender and skin tone. Results: Collectively (individual and group characters) (n = 391), 60.6% were male and 87.7% were of a light skin tone. DALL-E 3 (65.6%), Midjourney 5.2 (76.7%), and Stable Diffusion 2.1 (56.2%) had a statistically higher representation of men than Firefly 2 (42.9%) (P < 0.0001). With Firefly 2, 70.3% of characters had light skin tones, which was statistically lower (P < 0.0001) than for Stable Diffusion 2.1 (84.8%), Midjourney 5.2 (100%), and DALL-E 3 (94.8%). Overall, image quality metrics were average or better in 87.2% for DALL-E 3 and 86.2% for Midjourney 5.2, whereas 50.9% were inadequate or poor for Firefly 2 and 86.0% for Stable Diffusion 2.1. Conclusion: Generative AI text-to-image generation using DALL-E 3 via GPT-4 has the best overall quality compared with Firefly 2, Midjourney 5.2, and Stable Diffusion 2.1. Nonetheless, DALL-E 3 includes inherent biases associated with gender and ethnicity that demand more critical evaluation.",
      "journal": "Journal of nuclear medicine technology",
      "year": "2024",
      "doi": "10.2967/jnmt.124.268332",
      "authors": "Currie Geoffrey et al.",
      "keywords": "diversity; generative artificial intelligence; inclusivity; nuclear medicine; radiology",
      "mesh_terms": "Humans; Male; Female; Artificial Intelligence; Diagnostic Imaging; Sexism; Ethnicity; Image Processing, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39438057/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39438058",
      "title": "Gender and Ethnicity Bias of Text-to-Image Generative Artificial Intelligence in Medical Imaging, Part 2: Analysis of DALL-E 3.",
      "abstract": "Disparity among gender and ethnicity remains an issue across medicine and health science. Only 26%-35% of trainee radiologists are female, despite more than 50% of medical students' being female. Similar gender disparities are evident across the medical imaging professions. Generative artificial intelligence text-to-image production could reinforce or amplify gender biases. Methods: In March 2024, DALL-E 3 was utilized via GPT-4 to generate a series of individual and group images of medical imaging professionals: radiologist, nuclear medicine physician, radiographer, nuclear medicine technologist, medical physicist, radiopharmacist, and medical imaging nurse. Multiple iterations of images were generated using a variety of prompts. Collectively, 120 images were produced for evaluation of 524 characters. All images were independently analyzed by 3 expert reviewers from medical imaging professions for apparent gender and skin tone. Results: Collectively (individual and group images), 57.4% (n = 301) of medical imaging professionals were depicted as male, 42.4% (n = 222) as female, and 91.2% (n = 478) as having a light skin tone. The male gender representation was 65% for radiologists, 62% for nuclear medicine physicians, 52% for radiographers, 56% for nuclear medicine technologists, 62% for medical physicists, 53% for radiopharmacists, and 26% for medical imaging nurses. For all professions, this overrepresents men compared with women. There was no representation of persons with a disability. Conclusion: This evaluation reveals a significant overrepresentation of the male gender associated with generative artificial intelligence text-to-image production using DALL-E 3 across the medical imaging professions. Generated images have a disproportionately high representation of white men, which is not representative of the diversity of the medical imaging professions.",
      "journal": "Journal of nuclear medicine technology",
      "year": "2025",
      "doi": "10.2967/jnmt.124.268359",
      "authors": "Currie Geoffrey et al.",
      "keywords": "bias; diversity; generative artificial intelligence; nuclear medicine; radiology",
      "mesh_terms": "Artificial Intelligence; Humans; Female; Male; Diagnostic Imaging; Sexism; Ethnicity; Generative Artificial Intelligence",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39438058/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39541598",
      "title": "Reducing bias in healthcare artificial intelligence: A white paper.",
      "abstract": "Objective: Mitigation of racism in artificial intelligence (AI) is needed to improve health outcomes, yet no consensus exists on how this might be achieved. Methods: At an international conference in 2022, experts gathered to discuss strategies for reducing bias in healthcare AI. Results: This paper delineates these strategies along with their corresponding strengths and weaknesses and reviews the existing literature on these strategies. Conclusions: Five major themes resulted: reducing dataset bias, accurate modeling of existing data, transparency of artificial intelligence, regulation of artificial intelligence and the people who develop it, and bringing stakeholders to the table.",
      "journal": "Health informatics journal",
      "year": "2024",
      "doi": "10.1177/14604582241291410",
      "authors": "Sun Carolyn et al.",
      "keywords": "AI; bias; healthcare; machine learning; policy",
      "mesh_terms": "Artificial Intelligence; Humans; Bias; Racism; Delivery of Health Care",
      "pub_types": "Journal Article; Consensus Statement",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39541598/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "39701331",
      "title": "Immune-based Machine learning Prediction of Diagnosis and Illness State in schizophrenia and bipolar Disorder: How data bias and overfitting were avoided.",
      "abstract": "In a letter critiquing our manuscript, Takefuji highlights general pitfalls in machine learning, without directly engaging with our study. The comments provide generic advice rather than a specific critique of our methods or findings. Despite raising important topics, the concerns reflect standard risks in machine learning, which we were aware of and explicitly addressed in our analyses. We applied rigorous methods, including nested cross-validation, stratified sampling, and comprehensive performance metrics, to mitigate overfitting, class imbalance, and potential biases. Traditional statistical methods, such as ANCOVA and Spearman correlations, were employed and supplemented our machine learning analysis to validate findings. Concerns about collinearity, causality, and data preprocessing were acknowledged and addressed as detailed in the manuscript and supplementary materials. Although the critique underscores critical issues in machine learning, it does not identify specific missteps in our study. We conclude that our analyses align with best practices and sufficiently address the potential pitfalls discussed in the commentary.",
      "journal": "Brain, behavior, and immunity",
      "year": "2025",
      "doi": "10.1016/j.bbi.2024.11.037",
      "authors": "Skorobogatov Katrien et al.",
      "keywords": "Bipolar disorder; Machine learning; Nested cross-validation; Overfitting; Schizophrenia",
      "mesh_terms": "Humans; Machine Learning; Bipolar Disorder; Schizophrenia; Bias",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39701331/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40198356",
      "title": "Cross-institutional validation of a polar map-free 3D deep learning model for obstructive coronary artery disease prediction using myocardial perfusion imaging: insights into generalizability and bias.",
      "abstract": "PURPOSE: Deep learning (DL) models for predicting obstructive coronary artery disease (CAD) using myocardial perfusion imaging (MPI) have shown potential for enhancing diagnostic accuracy. However, their ability to maintain consistent performance across institutions and demographics remains uncertain. This study aimed to investigate the generalizability and potential biases of an in-house MPI DL model between two hospital-based cohorts. METHODS: We retrospectively included patients from two medical centers in Taiwan who underwent stress/redistribution thallium-201 MPI followed by invasive coronary angiography within 90 days as the reference standard. A polar map-free 3D DL model trained on 928 MPI images from one center to predict obstructive CAD was tested on internal (933 images) and external (3234 images from the other center) validation sets. Diagnostic performance, assessed using area under receiver operating characteristic curves (AUCs), was compared between the internal and external cohorts, demographic groups, and with the performance of stress total perfusion deficit (TPD). RESULTS: The model showed significantly lower performance in the external cohort compared to the internal cohort in both patient-based (AUC: 0.713 vs. 0.813) and vessel-based (AUC: 0.733 vs. 0.782) analyses, but still outperformed stress TPD (all p\u2009<\u20090.001). The performance was lower in patients who underwent treadmill stress MPI in the internal cohort and in patients over 70 years old in the external cohort. CONCLUSIONS: This study demonstrated adequate performance but also limitations in the generalizability of the DL-based MPI model, along with biases related to stress type and patient age. Thorough validation is essential before the clinical implementation of DL MPI models.",
      "journal": "European journal of nuclear medicine and molecular imaging",
      "year": "2025",
      "doi": "10.1007/s00259-025-07243-w",
      "authors": "Shih Yu-Cheng et al.",
      "keywords": "Bias; Coronary artery disease (CAD); Deep learning (DL); Generalizability; Myocardial perfusion imaging (MPI); Single-photon emission tomography (SPECT)",
      "mesh_terms": "Humans; Myocardial Perfusion Imaging; Male; Female; Deep Learning; Coronary Artery Disease; Middle Aged; Retrospective Studies; Aged; Imaging, Three-Dimensional; Bias",
      "pub_types": "Journal Article; Validation Study; Multicenter Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40198356/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40391479",
      "title": "Achieving Health Equity for All Canadians: Is AI Currently Up to the Task?",
      "abstract": "Artificial intelligence (AI) deployed into healthcare settings is touted as an exciting approach for improving health equity. However, several issues need to be addressed before this could be achieved, including improving the collection and use of the social determinants of health data, enhancing data interoperability, closing the digital divide and conducting rigorous assessment and evaluation of AI applications to ensure that they achieve fair and equitable outcomes in real-world settings. Importantly, we should not neglect evidence-based strategies that will truly advance health equity, such as adequate housing, poverty reduction, accessible mental healthcare, food security and many other structural and social determinants of health.",
      "journal": "HealthcarePapers",
      "year": "2025",
      "doi": "10.12927/hcpap.2025.27570",
      "authors": "Garies Stephanie et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Canada; Health Equity; Social Determinants of Health",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40391479/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40404499",
      "title": "The effect of selection bias on the performance of a deep learning-based intraoperative hypotension prediction model using real-world samples from a publicly available database.",
      "abstract": "BACKGROUND: There are models to predict intraoperative hypotension from arterial pressure waveforms. Selection bias in datasets used for model development and validation could impact model performance. We aimed to evaluate how selection bias affects the predictive performance of a deep learning (DL)-based model and a model using only mean arterial pressure (MAP) as input (MAP-only model). METHODS: We used the VitalDB open dataset. A hypotensive event was defined as a MAP <65 mm Hg for 1 min. For the 'biased dataset', 'non-hypotensive events' needed to be (a) at the centre of a 'non-hypotensive period' with a MAP of >75 mm Hg for more than 30 continuous minutes and (b) at least 20 min apart from any hypotensive event. For the 'unbiased dataset', all samples were included unless the hypotensive event was already in the input segment. The alarms per hour and positive predictive values were compared between the DL and MAP-only models. RESULTS: The DL model generally performed better than the MAP-only model. For the prediction of intraoperative hypotension 5 min before the event with the DL model, using the unbiased vs the biased testing dataset resulted in 18.1 vs 10.8 alarms per hour (P<0.001) and a positive predictive value of 0.068 vs 0.937 (P<0.001). CONCLUSIONS: Both the DL model and the MAP-only model demonstrated worse predictive performance when tested on the unbiased dataset compared with the biased dataset. Although the DL model statistically performed better than the MAP-only model, the difference between the two models was not clinically meaningful. Clinicians should consider the potential impact of selection bias on the validation and the clinical performance of hypotension prediction models. CLINICAL TRIAL REGISTRATION: NCT02914444.",
      "journal": "British journal of anaesthesia",
      "year": "2025",
      "doi": "10.1016/j.bja.2025.03.024",
      "authors": "Yang Hyun-Lim et al.",
      "keywords": "arterial blood pressure; deep learning; hypotension; perioperative medicine; selection bias",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Arterial Pressure; Databases, Factual; Deep Learning; Hypotension; Intraoperative Complications; Monitoring, Intraoperative; Predictive Value of Tests; Selection Bias; Prospective Studies; Datasets as Topic",
      "pub_types": "Journal Article; Observational Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40404499/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40419424",
      "title": "[Health Equity in Mental Health Care: Challenges for Nurses and Related Preparation].",
      "abstract": "Individuals with mental illness face significant challenges in achieving health equity due to social and structural determinants, fragmented healthcare systems, social stigmas, and disparities in digital health access. As advocates for individuals with mental illness, nurses play a crucial role in promoting health equity. Therefore, nurses must develop structural competency to understand how economic, political, and social structures impact mental healthcare. Moreover, nurses should cultivate cultural sensitivity and inclusivity to appropriately respect the cultural backgrounds of their patients during care, build trust, and reduce the negative impacts of discrimination and stigmas on their health. In addition, nurses should participate actively in health policy formulation and advocacy to eliminate structural barriers within the mental healthcare system to improve accessibility and fairness in medical services. Furthermore, nurses need to collaborate with multiple stakeholders to establish interdisciplinary community partnerships, drive systemic change through community programs and policy translation, and ensure sustainable improvements. As artificial intelligence and digital technologies continue to evolve, nurses must enhance their professional competencies continually to guarantee equitable, high-quality, and inclusive healthcare for those with mental illness and, ultimately, advance overall societal well-being. TITLE: \u7cbe\u795e\u885b\u751f\u7167\u8b77\u7684\u5065\u5eb7\u5e73\u6b0a\u2014\u8b77\u7406\u5e2b\u7684\u6311\u6230\u8207\u6e96\u5099. UNLABELLED: \u7cbe\u795e\u75c5\u4eba\u9762\u81e8\u793e\u6703\u7d50\u69cb\u6c7a\u5b9a\u56e0\u7d20\u3001\u7f3a\u4e4f\u6574\u5408\u6027\u5065\u5eb7\u7167\u8b77\u3001\u793e\u6703\u6c61\u540d\u53ca\u6578\u4f4d\u79d1\u6280\u61c9\u7528\u843d\u5dee\u7b49\u5065\u5eb7\u4e0d\u516c\u5e73\u7684\u6311\u6230\u3002\u8eab\u70ba\u7cbe\u795e\u75c5\u4eba\u7684\u4ee3\u8a00\u8005\uff0c\u8b77\u7406\u5e2b\u5728\u63a8\u52d5\u5065\u5eb7\u5e73\u6b0a\u4e0a\u626e\u6f14\u95dc\u9375\u89d2\u8272\u3002\u56e0\u6b64\uff0c\u8b77\u7406\u5e2b\u61c9\u57f9\u990a\u7d50\u69cb\u6027\u80fd\u529b\uff0c\u7406\u89e3\u7d93\u6fdf\u3001\u653f\u6cbb\u8207\u793e\u6703\u7d50\u69cb\u5c0d\u7cbe\u795e\u885b\u751f\u7167\u8b77\u7684\u5f71\u97ff\u3002\u5176\u6b21\uff0c\u8b77\u7406\u5e2b\u61c9\u5177\u5099\u6587\u5316\u654f\u611f\u6027\u8207\u5305\u5bb9\u6027\uff0c\u5728\u7167\u8b77\u904e\u7a0b\u4e2d\u5c0a\u91cd\u75c5\u4eba\u7684\u6587\u5316\u80cc\u666f\uff0c\u5efa\u7acb\u4fe1\u4efb\u95dc\u4fc2\uff0c\u6e1b\u5c11\u6b67\u8996\u8207\u6c61\u540d\u5c0d\u75c5\u4eba\u5065\u5eb7\u7684\u5f71\u97ff\u3002\u6b64\u5916\uff0c\u8b77\u7406\u5e2b\u61c9\u7a4d\u6975\u53c3\u8207\u5065\u5eb7\u653f\u7b56\u5236\u5b9a\u8207\u5021\u8b70\uff0c\u6d88\u9664\u7cbe\u795e\u885b\u751f\u7167\u8b77\u7cfb\u7d71\u4e2d\u7684\u7d50\u69cb\u6027\u969c\u7919\uff0c\u63d0\u5347\u91ab\u7642\u53ef\u53ca\u6027\u8207\u516c\u5e73\u6027\u3002\u8b77\u7406\u5e2b\u4ea6\u61c9\u8207\u591a\u65b9\u5229\u76ca\u76f8\u95dc\u8005\u5408\u4f5c\uff0c\u5efa\u7acb\u8de8\u9818\u57df\u793e\u5340\u5925\u4f34\u95dc\u4fc2\uff0c\u900f\u904e\u793e\u5340\u8a08\u756b\u8207\u653f\u7b56\u8f49\u8b6f\uff0c\u63a8\u52d5\u7cfb\u7d71\u6027\u8b8a\u9769\u3002\u800c\u96a8\u8457\u4eba\u5de5\u667a\u6167\u8207\u6578\u4f4d\u6280\u8853\u7684\u767c\u5c55\uff0c\u8b77\u7406\u5e2b\u61c9\u6301\u7e8c\u63d0\u5347\u5c08\u696d\u80fd\u529b\uff0c\u78ba\u4fdd\u7cbe\u795e\u75c5\u4eba\u7372\u5f97\u516c\u5e73\u3001\u9ad8\u54c1\u8cea\u4e14\u5305\u5bb9\u7684\u5065\u5eb7\u7167\u8b77\uff0c\u9032\u800c\u4fc3\u9032\u6574\u9ad4\u793e\u6703\u5065\u5eb7\u798f\u7949\u7684\u63d0\u5347\u3002.",
      "journal": "Hu li za zhi The journal of nursing",
      "year": "2025",
      "doi": "10.6224/JN.202506_72(3).03",
      "authors": "Feng Hsin-Pei et al.",
      "keywords": "health equity; mental health care; policy advocacy; social determinants of health; structural competency",
      "mesh_terms": "Humans; Health Equity; Mental Health Services; Mental Disorders",
      "pub_types": "English Abstract; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40419424/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40618939",
      "title": "A large language model analysis of global inequities in precision medicine research on diabetes.",
      "abstract": "PURPOSE: Although nearly 80\u202f% of patients with diabetes live in low- and middle-income countries, it is currently unknown what proportion of precision medicine research is based on these populations. Manual screening of literature is time consuming and resource intensive. Our objective is to characterize the proportionality of diabetes burden and precision medicine research across ten geographic regions using a scalable large language model (LLM) enabled workflow. METHODS: An electronic search of PubMed identified titles and abstracts of studies related to precision medicine in diabetes from 2010 to 2023 (n\u202f=\u202f129,154). Two reviewers independently labelled a random sub-sample and classified their source populations, and whether these were primary studies of precision medicine in diabetes. Using this labeled data (n\u202f=\u202f2196), we developed prompts and selected hyperparameters for GPT-4o. We then used GPT-4o to classify the remaining studies and estimated the ratio of research output to disability adjusted life years [DALY] from the Global Burden of Disease [GBD] study 2021. RESULTS: Of the 15,507 studies identified as precision medicine in diabetes, 33.8\u202f%, 20.9\u202f% and 14.3\u202f% were from North America, Western Europe, and East Asia respectively. The number of studies was the most proportionate to disease burden for North America (0.95 per 1000 DALYs) and Western Europe (0.78 per 1000 DALYs), and the least proportionate for Southeast Asia, South Asia, and Sub-Saharan Africa (0.02 each per 1000 DALYs). CONCLUSIONS: Future research investments into omics-based research should prioritize regions outside Western Europe and North America for achieving global equity in diabetes care.",
      "journal": "Annals of epidemiology",
      "year": "2025",
      "doi": "10.1016/j.annepidem.2025.06.021",
      "authors": "Soniwala Aamna et al.",
      "keywords": "Diabetes mellitus; Natural language processing; Personalized medicine; Precision medicine",
      "mesh_terms": "Humans; Precision Medicine; Diabetes Mellitus; Global Health; Biomedical Research; Global Burden of Disease; Healthcare Disparities; Quality-Adjusted Life Years; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40618939/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40776290",
      "title": "Voice for All: Evaluating the Accuracy and Equity of Automatic Speech Recognition Systems in Transcribing Patient Communications in Home Healthcare.",
      "abstract": "Integrating automatic speech recognition (ASR) systems into home healthcare workflows can enhance risk prediction models. However, ASR systems exhibit disparities in transcription accuracy across racial and linguistic groups, highlighting an equity gap that could bias healthcare delivery. We evaluated four ASR systems-AWS General, AWS Medical, Whisper, and Wave2Vec-in transcribing 860 patient-nurse utterances (475 Black, 385 White). Word error rate (WER) was the primary measure. AWS General achieved the highest accuracy (median WER 39%), but all systems were less accurate for Black patients, particularly in linguistic domains \"Affect,\" \"Social,\" and \"Drives.\" AWS Medical outperformed others on medical terms, although filler words, repetition, and nonmedical terms challenged every system. These findings underscore the need for diverse training datasets and improved dialect sensitivity to ensure equitable ASR performance and robust risk identification in home healthcare.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI251273",
      "authors": "Xu Zidu et al.",
      "keywords": "Automatic Speech Recognition systems; Home Healthcare; Transcription Disparities",
      "mesh_terms": "Speech Recognition Software; Home Care Services; Humans; Natural Language Processing; Electronic Health Records",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40776290/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40801276",
      "title": "Evaluating ChatGPT's Utility in Addressing Socioeconomic Disparities in Burn Patients: A Comparative Study With Google.",
      "abstract": "Patients from low-socioeconomic status (SES) backgrounds face barriers to quality burn care, such as limited healthcare access and follow-up. Many turn to online resources like Google, which may provide overwhelming or irrelevant information. This study compares the accuracy, readability, and SES-relevance of burn care information from ChatGPT and Google to address these disparities. A standardized set of questions on immediate burn care, medical treatments, and long-term care was developed based on clinical guidelines. Responses from ChatGPT (v4.0) and the first Google search result were analyzed. Two medical students and 2 burn surgeons assessed accuracy using the Global Quality Score (GQS) on a scale of 1 (poor) to 5 (excellent). Readability was measured using the Flesch-Kincaid grade level, and SES relevance was determined by counting responses that included themes related to affordability and access to care. Accuracy, readability, and SES relevance were then compared using a Wilcoxon signed-rank test. ChatGPT provided higher-quality responses (GQS 4.35\u2009\u00b1\u20090.60) than Google (GQS 2.25\u2009\u00b1\u20091.10, P < .01). ChatGPT was unanimously preferred for half of the questions. Both platforms had reading grade levels of 8 and 9, but ChatGPT addressed SES issues in 74% of responses, compared to Google's 33%. ChatGPT outperformed Google in providing accurate, SES-relevant burn care information. Artificial intelligence tools like ChatGPT may help reduce health information disparities for low-SES patients by offering tailored and user-friendly guidance. Future studies should validate these findings across other clinical topics and patient populations.",
      "journal": "Journal of burn care & research : official publication of the American Burn Association",
      "year": "2026",
      "doi": "10.1093/jbcr/iraf158",
      "authors": "Beohon Blancheneige et al.",
      "keywords": "AI in health care; ChatGPT; Google; burn care disparities; healthcare accessibility; socioeconomic status",
      "mesh_terms": "Humans; Burns; Internet; Healthcare Disparities; Comprehension; Socioeconomic Factors; Search Engine; Social Class; Socioeconomic Disparities in Health; Generative Artificial Intelligence",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40801276/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40934684",
      "title": "Mitigating low-frequency bias: Feature recalibration and frequency attention regularization for adversarial robustness.",
      "abstract": "Ensuring the robustness of deep neural networks against adversarial attacks remains a fundamental challenge in computer vision. While adversarial training (AT) has emerged as a promising defense strategy, our analysis reveals a critical limitation: AT-trained models exhibit a bias toward low-frequency features while neglecting high-frequency components. This bias is particularly concerning as each frequency component carries distinct and crucial information: low-frequency features encode fundamental structural patterns, while high-frequency features capture intricate details and textures. To address this limitation, we propose High-Frequency Feature Disentanglement and Recalibration (HFDR), a novel module that strategically separates and recalibrates frequency-specific features to capture latent semantic cues. We further introduce frequency attention regularization to harmonize feature extraction across the frequency spectrum and mitigate the inherent low-frequency bias of AT. Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K demonstrate that HFDR consistently enhances adversarial robustness. It achieves a 2.89 % gain on CIFAR-100 with WRN34-10, and improves robustness by 3.09 % on ImageNet-1K, with a 4.89 % gain on ViT-B against AutoAttack. These results highlight the method's adaptability to both convolutional and transformer-based architectures. Code is available at https://github.com/KejiaZhang-Robust/HFDR.",
      "journal": "Neural networks : the official journal of the International Neural Network Society",
      "year": "2026",
      "doi": "10.1016/j.neunet.2025.108070",
      "authors": "Zhang Kejia et al.",
      "keywords": "Adversarial training; Frequency; Neural network robustness",
      "mesh_terms": "Neural Networks, Computer; Humans; Attention; Deep Learning; Algorithms; Bias; Calibration",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40934684/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "40971664",
      "title": "AI and Healthcare Disparities: Lessons from a Cautionary Tale in Knee Radiology.",
      "abstract": "Enthusiasm about the use of artificial intelligence (AI) in medicine has been tempered by concern that algorithmic systems can be unfairly biased against racially minoritized populations. This article uses work on racial disparities in knee osteoarthritis diagnoses to underline that achieving justice in the use of AI in medical imaging requires attention to the entire sociotechnical system within which it operates, rather than isolated properties of algorithms. Using AI to make current diagnostic procedures more efficient risks entrenching existing disparities; a recent algorithm points to some of the problems in current procedures while highlighting systemic normative issues that need to be addressed while designing further AI systems. The article thus contributes to a literature arguing that bias and fairness issues in AI be considered as aspects of structural inequality and injustice and to highlighting ways that AI can be helpful in making progress on these.",
      "journal": "The Journal of medicine and philosophy",
      "year": "2025",
      "doi": "10.1093/jmp/jhaf020",
      "authors": "Hull Gordon",
      "keywords": "agency; artificial intelligence; fairness; imaging; structural racism",
      "mesh_terms": "Humans; Artificial Intelligence; Healthcare Disparities; Osteoarthritis, Knee; Algorithms; Social Justice; Radiology",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40971664/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41003966",
      "title": "FairDITA: Disentangled Image-Text Alignment for Fair Skin Cancer Diagnosis.",
      "abstract": "Recent advances in deep learning have significantly improved skin cancer classification, yet concerns regarding algorithmic fairness persist because of performance disparities across skin tone groups. Existing methods often attempt to mitigate bias by suppressing sensitive attributes within images. However, they are fundamentally limited by the entanglement of lesion characteristics and skin tone in visual inputs. To address this challenge, we propose a novel contrastive learning framework that leverages explicitly constructed image-text pairs to disentangle lesion condition features from skin tone attributes. Our architecture consists of a shared text encoder and two specialized image encoders that independently align image features with the corresponding textual descriptions of lesion characteristics and skin tone. Furthermore, we measure the semantic distance between lesion conditions and skin color embeddings in both image- and text-embedding spaces and perform optimal representation alignment by matching the distances in the image space to those in the text space. We validated our method using two benchmark datasets, PAD-UFES-20 and Fitzpatrick17k, which span a wide range of skin tones. The experimental results demonstrate that our approach consistently improves both classification accuracy and fairness across multiple evaluation metrics.",
      "journal": "Journal of imaging informatics in medicine",
      "year": "2025",
      "doi": "10.1007/s10278-025-01693-2",
      "authors": "Park Jiwon et al.",
      "keywords": "Machine learning fairness; Multimodal representation learning; Skin cancer diagnosis",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41003966/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41165774",
      "title": "Uncovering Racial and Genetic Disparities in FRAX Performance for Fracture Risk Assessment in Postmenopausal Women.",
      "abstract": "The Fracture Risk Assessment Tool (FRAX) is widely used in osteoporosis management, yet its performance across diverse racial, ethnic, and genetic populations remains uncertain. We evaluated the accuracy of FRAX in predicting fracture risk, with and without bone mineral density (BMD) measurements, among 27,512 postmenopausal women aged 50 to 79 years participating in the Women's Health Initiative (WHI) with 10 years of follow-up. FRAX-predicted fracture risks were compared to observed outcomes, stratified by race/ethnicity and genetic risk categories derived from a Genome-Wide Polygenic Score (GPS), calculated using 103,155 genetic variants via the LDpred algorithm and UK Biobank GWAS data. FRAX with BMD information substantially overestimated fracture risk for African American women by 82% (hazard ratio [HR], 0.18; 95% CI, 0.10-0.36) and Hispanic women by 48% (HR, 0.52; 95% CI, 0.31-0.87), compared with non-Hispanic White women (both P < .001). Conversely, FRAX with BMD information significantly underestimated fracture risk in women with the highest genetic risk (top 5%; HR, 2.15; 95% CI, 1.22-3.68; P < .001), compared with the low GPS reference group. Calibration analyses revealed systematic inaccuracies, highlighting racial disparities in overestimating fracture risk and genetic disparities in underestimating fracture risk predictions. Sensitivity analyses confirmed these findings persisted regardless of estrogen use. Our results underscore critical limitations of FRAX, suggesting that integrating genetic risk profiling and implementing race-specific recalibrations can substantially improve fracture risk prediction accuracy, promote equity in osteoporosis care, and support personalized clinical decision-making. FRAX is a widely used tool for estimating fracture risk due to osteoporosis, but its accuracy varies by race and genetics. In our study of over 27,500 postmenopausal women from diverse backgrounds, FRAX overestimated fracture risk in African American and Hispanic women while underestimating risk in those with high genetic susceptibility. These findings highlight the need to improve FRAX by incorporating genetic data and refining predictions by race, to ensure more accurate and equitable osteoporosis risk assessment for all women.",
      "journal": "Journal of bone and mineral research : the official journal of the American Society for Bone and Mineral Research",
      "year": "2025",
      "doi": "10.1093/jbmr/zjaf160",
      "authors": "Wu Qing et al.",
      "keywords": "FRAX; bone mineral density (BMD); fractures; genome-wide polygenic score (GPS); osteoporosis; race/ethnicity",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41165774/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41202130",
      "title": "Source framing triggers systematic bias in large language models.",
      "abstract": "Large language models (LLMs) are increasingly used to evaluate text, raising urgent questions about whether their judgments are consistent, unbiased, and robust to framing effects. Here, we examine inter- and intramodel agreement across four state-of-the-art LLMs tasked with evaluating 4800 narrative statements on 24 different topics of social, political, and public health relevance, for a total of 192,000 assessments. We manipulate the disclosed source of each statement to assess how attribution to either another LLM or a human author of specified nationality affects evaluation outcomes. Different LLMs display a remarkably high degree of inter- and intramodel agreement across topics, but this alignment breaks down when source framing is introduced. Attributing statements to Chinese individuals systematically lowers agreement scores across all models and, in particular, for DeepSeek Reasoner. Our findings show that LLMs' own judgment of agreement with narrative statements exhibit systematic bias from framing effects, with substantial implications for the neutrality and fairness of LLM-mediated information systems.",
      "journal": "Science advances",
      "year": "2025",
      "doi": "10.1126/sciadv.adz2924",
      "authors": "Germani Federico et al.",
      "keywords": "",
      "mesh_terms": "Humans; Language; Models, Theoretical; Judgment; Bias; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41202130/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41335728",
      "title": "Self-supervised and supervised learning in medical imaging classification: addressing the hidden bias of workflow design.",
      "abstract": "The rise of self-supervised learning (SSL) in medical imaging holds immense potential, particularly for leveraging unlabeled data and achieving surprising performance in scenarios with limited annotations. Our study shows that comparisons with traditional supervised learning (SL) are often confounded by differences in workflows, leading to potentially biased conclusions. The SL paradigm is typically employed with a one-stage training workflow, while the typical SSL linear evaluation workflow involves a two-stage process: pre-training a backbone with a projector, followed by fine-tuning a randomly initialized task-specific classification head replacing the projector. We show that the two-stage workflow, when applied to SL, can change the trained model performance. This is especially important when selecting the appropriate paradigm for medical imaging classification where the outcomes can have a clinical impact. We experimented with four medical imaging datasets, targeting age prediction and Alzheimer's disease diagnosis from brain MRI, pneumonia diagnosis from chest RX, and diagnosis of retina with choroidal neurovascularization from optical coherence tomography. For each dataset, we imposed different configurations of assumed label availability and class frequency distribution. For each configuration, we performed 30 experiments (5 for the larger dataset) and a robust statistical analysis of the results, which show that the different workflows can alter the trained model performance. This finding suggests that the typical comparisons between SL and SSL in literature may not solely reflect the learning paradigm itself but also the workflow, which is agnostic to the paradigm. In the field of medical imaging, where model performance directly impacts clinical decision-making and patient outcomes, ensuring fair and robust comparisons is critical. By addressing this overlooked bias, our work provides actionable insights to advance reliable methodologies, paving the way for more effective and trustworthy AI-driven solutions in healthcare.",
      "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference",
      "year": "2025",
      "doi": "10.1109/EMBC58623.2025.11254853",
      "authors": "Espis Andrea et al.",
      "keywords": "",
      "mesh_terms": "Humans; Supervised Machine Learning; Workflow; Diagnostic Imaging; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Alzheimer Disease; Algorithms; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41335728/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41554188",
      "title": "Mitigating data center bias in cancer classification: Transfer bias unlearning and feature size reduction via conflict-of-interest free multi-objective optimization.",
      "abstract": "Bias in the decision-making processes of trained deep models poses a significant threat to their reliability. Such bias can lead to overoptimistic results on observed data while compromising generalization to unseen datasets. Training data may contain hidden patterns related to task-irrelevant attributes, such as data centers, causing models to exploit these unintended correlations rather than learning the main task. This results in biased predictions that favor certain attributes. To address this issue, we propose an unlearning approach based on Conflict-of-Interest-Free Multi-Objective Optimization, designed to train an unlearning layer that explicitly reduces reliance on irrelevant patterns. Our method aims to minimize the gap between internal accuracy (evaluated on data centers seen during training) and external accuracy (evaluated on entirely unseen data centers) caused by biased model behavior. As a case study, we investigate how data center-specific signatures embedded in cancerous features can lead to misleadingly high internal performance and a significant drop in performance on test samples from external data centers. By evaluating various methods and objective functions, our proposed approach achieves strong generalizability on external validation data by jointly reducing feature dimensionality and excluding conflict-of-interest samples during the k-Nearest Neighbor (KNN) searching process. We compare our method against multi-task and adversarial learning approaches for bias mitigation. Results show that our method outperforms others in narrowing the internal-external performance gap while also improving external validation accuracy. To ensure robustness, we conducted experiments using k-fold cross-validation across k different data centers, further validating the generalizability of our approach. Although this study focuses on cancer-related features and data center biases, the proposed method is model-agnostic and can be applied to any biased feature set extracted by a deep learning model.",
      "journal": "Artificial intelligence in medicine",
      "year": "2026",
      "doi": "10.1016/j.artmed.2026.103351",
      "authors": "Kheiri Farnaz et al.",
      "keywords": "Bias; Conflict-of-interest free KNN; Evolutionary computation; Histopathology; Multi-objective optimization; NSGA-II; Unlearning",
      "mesh_terms": "Humans; Neoplasms; Bias; Reproducibility of Results; Deep Learning; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41554188/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "41639760",
      "title": "Machine learning performance for a small dataset: random oversampling improves data imbalances and fairness.",
      "abstract": "",
      "journal": "BMC medical research methodology",
      "year": "2026",
      "doi": "10.1186/s12874-026-02779-3",
      "authors": "Wang Lin et al.",
      "keywords": "Fairness; Machine learning; Major adverse cardiovascular events; Oversampling; Percutaneous coronary intervention; Social determinants of health",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41639760/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": null,
      "ft_status": "No PMC full text available"
    },
    {
      "pmid": "15986433",
      "title": "Quantitative evaluation of automated skull-stripping methods applied to contemporary and legacy images: effects of diagnosis, bias correction, and slice location.",
      "abstract": "Performance of automated methods to isolate brain from nonbrain tissues in magnetic resonance (MR) structural images may be influenced by MR signal inhomogeneities, type of MR image set, regional anatomy, and age and diagnosis of subjects studied. The present study compared the performance of four methods: Brain Extraction Tool (BET; Smith [2002]: Hum Brain Mapp 17:143-155); 3dIntracranial (Ward [1999] Milwaukee: Biophysics Research Institute, Medical College of Wisconsin; in AFNI); a Hybrid Watershed algorithm (HWA, Segonne et al. [2004] Neuroimage 22:1060-1075; in FreeSurfer); and Brain Surface Extractor (BSE, Sandor and Leahy [1997] IEEE Trans Med Imag 16:41-54; Shattuck et al. [2001] Neuroimage 13:856-876) to manually stripped images. The methods were applied to uncorrected and bias-corrected datasets; Legacy and Contemporary T1-weighted image sets; and four diagnostic groups (depressed, Alzheimer's, young and elderly control). To provide a criterion for outcome assessment, two experts manually stripped six sagittal sections for each dataset in locations where brain and nonbrain tissue are difficult to distinguish. Methods were compared on Jaccard similarity coefficients, Hausdorff distances, and an Expectation-Maximization algorithm. Methods tended to perform better on contemporary datasets; bias correction did not significantly improve method performance. Mesial sections were most difficult for all methods. Although AD image sets were most difficult to strip, HWA and BSE were more robust across diagnostic groups compared with 3dIntracranial and BET. With respect to specificity, BSE tended to perform best across all groups, whereas HWA was more sensitive than other methods. The results of this study may direct users towards a method appropriate to their T1-weighted datasets and improve the efficiency of processing for large, multisite neuroimaging studies. Performance of automated methods to isolate brain from nonbrain tissues in magnetic resonance (MR) structural images may be influenced by MR signal inhomogeneities, type of MR image set, regional anatomy, and age and diagnosis of subjects studied. The present study compared the performance of four methods: Brain Extraction Tool (BET; Smith [2002]: Hum Brain Mapp 17:143\u2013155); 3dIntracranial (Ward [1999] Milwaukee: Biophysics Research Institute, Medical College of Wisconsin; in AFNI); a Hybrid Watershed algorithm (HWA, Segonne et al. [2004] Neuroimage 22:1060\u20131075; in FreeSurfer); and Brain Surface Extractor (BSE, Sandor and Leahy [1997] IEEE Trans Med Imag 16:41\u201354; Shattuck et al. [2001] Neuroimage 13:856\u2013876) to manually stripped images. The methods were applied to uncorrected and bias\u2010corrected datasets; Legacy and Contemporary T1\u2010weighted image sets; and four diagnostic groups (depressed, Alzheimer's, young and elderly control). To provide a criterion for outcome assessment, two experts manually stripped six sagittal sections for each dataset in locations where brain and nonbrain tissue are difficult to distinguish. Methods were compared on Jaccard similarity coefficients, Hausdorff distances, and an Expectation\u2010Maximization algorithm. Methods tended to perform better on contemporary datasets; bias correction did not significantly improve method performance. Mesial sections were most difficult for all methods. Although AD image sets were most difficult to strip, HWA and BSE were more robust across diagnostic groups compared with 3dIntracranial and BET. With respect to specificity, BSE tended to perform best across all groups, whereas HWA was more sensitive than other methods. The results of this study may direct users towards a method appropriate to their T1\u2010weighted datasets and improve the efficiency of processing for large, multisite neuroimaging studies. Hum. Brain Mapping, 2005. \u00a9 2005 Wiley\u2010Liss, Inc.",
      "journal": "Human brain mapping",
      "year": "2006",
      "doi": "10.1002/hbm.20161",
      "authors": "Fennema-Notestine Christine et al.",
      "keywords": "",
      "mesh_terms": "Adult; Age Factors; Aged; Algorithms; Brain; Brain Diseases; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Middle Aged; Radiography; Sensitivity and Specificity; Software",
      "pub_types": "Comparative Study; Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/15986433/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC2408865",
      "ft_status": "PMC fetch failed (PMC2408865)",
      "ft_reason": "Full text not retrievable"
    },
    {
      "pmid": "33625504",
      "title": "Equity and Artificial Intelligence in Surgical Care.",
      "abstract": "",
      "journal": "JAMA surgery",
      "year": "2021",
      "doi": "10.1001/jamasurg.2020.7208",
      "authors": "Johnson-Mann Crystal N et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Bias, Implicit; Clinical Decision-Making; Healthcare Disparities; Humans; Patient Selection; Surgical Procedures, Operative",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33625504/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC8273554",
      "ft_status": "PMC fetch failed (PMC8273554)",
      "ft_reason": "Full text not retrievable"
    },
    {
      "pmid": "35337638",
      "title": "An interactive dashboard to track themes, development maturity, and global equity in clinical artificial intelligence research.",
      "abstract": "",
      "journal": "The Lancet. Digital health",
      "year": "2022",
      "doi": "10.1016/S2589-7500(22)00032-2",
      "authors": "Zhang Joe et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35337638/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC9150439",
      "ft_status": "PMC fetch failed (PMC9150439)",
      "ft_reason": "Full text not retrievable"
    },
    {
      "pmid": "36924621",
      "title": "Fairness metrics for health AI: we have a long way to go.",
      "abstract": "",
      "journal": "EBioMedicine",
      "year": "2023",
      "doi": "10.1016/j.ebiom.2023.104525",
      "authors": "Mbakwe Amarachi B et al.",
      "keywords": "",
      "mesh_terms": "Humans; Benchmarking; Social Justice; Artificial Intelligence",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36924621/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10114188",
      "ft_status": "PMC fetch failed (PMC10114188)",
      "ft_reason": "Full text not retrievable"
    },
    {
      "pmid": "37140902",
      "title": "Association of Biomarker-Based Artificial Intelligence With Risk of Racial Bias in Retinal Images.",
      "abstract": "IMPORTANCE: Although race is a social construct, it is associated with variations in skin and retinal pigmentation. Image-based medical artificial intelligence (AI) algorithms that use images of these organs have the potential to learn features associated with self-reported race (SRR), which increases the risk of racially biased performance in diagnostic tasks; understanding whether this information can be removed, without affecting the performance of AI algorithms, is critical in reducing the risk of racial bias in medical AI. OBJECTIVE: To evaluate whether converting color fundus photographs to retinal vessel maps (RVMs) of infants screened for retinopathy of prematurity (ROP) removes the risk for racial bias. DESIGN, SETTING, AND PARTICIPANTS: The retinal fundus images (RFIs) of neonates with parent-reported Black or White race were collected for this study. A u-net, a convolutional neural network (CNN) that provides precise segmentation for biomedical images, was used to segment the major arteries and veins in RFIs into grayscale RVMs, which were subsequently thresholded, binarized, and/or skeletonized. CNNs were trained with patients' SRR labels on color RFIs, raw RVMs, and thresholded, binarized, or skeletonized RVMs. Study data were analyzed from July 1 to September 28, 2021. MAIN OUTCOMES AND MEASURES: Area under the precision-recall curve (AUC-PR) and area under the receiver operating characteristic curve (AUROC) at both the image and eye level for classification of SRR. RESULTS: A total of 4095 RFIs were collected from 245 neonates with parent-reported Black (94 [38.4%]; mean [SD] age, 27.2 [2.3] weeks; 55 majority sex [58.5%]) or White (151 [61.6%]; mean [SD] age, 27.6 [2.3] weeks, 80 majority sex [53.0%]) race. CNNs inferred SRR from RFIs nearly perfectly (image-level AUC-PR, 0.999; 95% CI, 0.999-1.000; infant-level AUC-PR, 1.000; 95% CI, 0.999-1.000). Raw RVMs were nearly as informative as color RFIs (image-level AUC-PR, 0.938; 95% CI, 0.926-0.950; infant-level AUC-PR, 0.995; 95% CI, 0.992-0.998). Ultimately, CNNs were able to learn whether RFIs or RVMs were from Black or White infants regardless of whether images contained color, vessel segmentation brightness differences were nullified, or vessel segmentation widths were uniform. CONCLUSIONS AND RELEVANCE: Results of this diagnostic study suggest that it can be very challenging to remove information relevant to SRR from fundus photographs. As a result, AI algorithms trained on fundus photographs have the potential for biased performance in practice, even if based on biomarkers rather than raw images. Regardless of the methodology used for training AI, evaluating performance in relevant subpopulations is critical.",
      "journal": "JAMA ophthalmology",
      "year": "2023",
      "doi": "10.1001/jamaophthalmol.2023.1310",
      "authors": "Coyner Aaron S et al.",
      "keywords": "",
      "mesh_terms": "Infant, Newborn; Infant; Humans; Adult; Artificial Intelligence; Racism; Retina; Neural Networks, Computer; Algorithms",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37140902/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC10160994",
      "ft_status": "PMC fetch failed (PMC10160994)",
      "ft_reason": "Full text not retrievable"
    },
    {
      "pmid": "38172408",
      "title": "The Role of Pragmatic Implementation Science Methods in Achieving Equitable and Effective Use of Artificial Intelligence in Healthcare.",
      "abstract": "",
      "journal": "Journal of general internal medicine",
      "year": "2024",
      "doi": "10.1007/s11606-023-08580-y",
      "authors": "Maw Anna M et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Implementation Science; Delivery of Health Care",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38172408/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11116336",
      "ft_status": "PMC fetch failed (PMC11116336)",
      "ft_reason": "Full text not retrievable"
    },
    {
      "pmid": "38552451",
      "title": "Minimizing bias when using artificial intelligence in critical care medicine.",
      "abstract": "",
      "journal": "Journal of critical care",
      "year": "2024",
      "doi": "10.1016/j.jcrc.2024.154796",
      "authors": "Ranard Benjamin L et al.",
      "keywords": "Artificial intelligence; Bias; Critical Care; Disparities; Fairness; Health equity; Machine learning",
      "mesh_terms": "Humans; Artificial Intelligence; Critical Care; Bias",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38552451/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11139594",
      "ft_status": "PMC fetch failed (PMC11139594)",
      "ft_reason": "Full text not retrievable"
    },
    {
      "pmid": "38943446",
      "title": "Patient-Centered Equitable and Safe Artificial Intelligence in Otolaryngology-Head and Neck Surgery.",
      "abstract": "",
      "journal": "Otolaryngology--head and neck surgery : official journal of American Academy of Otolaryngology-Head and Neck Surgery",
      "year": "2024",
      "doi": "10.1002/ohn.881",
      "authors": "Tai Katherine et al.",
      "keywords": "AI; machine learning; otolaryngology; patient perspective",
      "mesh_terms": "Humans; Artificial Intelligence; Otolaryngology; Patient-Centered Care; Otorhinolaryngologic Surgical Procedures; Patient Safety",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38943446/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11461108",
      "ft_status": "PMC fetch failed (PMC11461108)",
      "ft_reason": "Full text not retrievable"
    },
    {
      "pmid": "39008553",
      "title": "Advancing Cardiovascular Health Equity With Artificial Intelligence: A Collective Ethical Responsibility.",
      "abstract": "",
      "journal": "Circulation",
      "year": "2024",
      "doi": "10.1161/CIRCULATIONAHA.124.068113",
      "authors": "Adedinsewo Demilade",
      "keywords": "artificial intelligence; digital health; ethics, clinical; ethnic and racial minorities; health equity",
      "mesh_terms": "Humans; Artificial Intelligence; Health Equity; Cardiovascular Diseases",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39008553/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11251709",
      "ft_status": "PMC fetch failed (PMC11251709)",
      "ft_reason": "Full text not retrievable"
    },
    {
      "pmid": "39207356",
      "title": "Understanding AI bias in clinical practice.",
      "abstract": "",
      "journal": "Heart rhythm",
      "year": "2024",
      "doi": "10.1016/j.hrthm.2024.08.004",
      "authors": "Adedinsewo Demilade et al.",
      "keywords": "Artificial intelligence; Bias; Cardiac electrophysiology; Cardiovascular disease; Clinical algorithms",
      "mesh_terms": "Humans; Artificial Intelligence; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39207356/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC12268356",
      "ft_status": "PMC fetch failed (PMC12268356)",
      "ft_reason": "Full text not retrievable"
    },
    {
      "pmid": "31907237",
      "title": "AI in health care: Improving outcomes or threatening equity?",
      "abstract": "",
      "journal": "CMAJ : Canadian Medical Association journal = journal de l'Association medicale canadienne",
      "year": "2020",
      "doi": "10.1503/cmaj.1095838",
      "authors": "Glauser Wendy",
      "keywords": "",
      "mesh_terms": "Algorithms; Artificial Intelligence; Canada; Delivery of Health Care; Female; Health Equity; Humans; Male; Racism",
      "pub_types": "News",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31907237/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC6944301",
      "ft_status": "PMC fetch failed (PMC6944301)",
      "ft_reason": "Full text not retrievable"
    },
    {
      "pmid": "37983817",
      "title": "Artificial Intelligence in Plastic Surgery: ChatGPT as a Tool to Address Disparities in Health Literacy.",
      "abstract": "",
      "journal": "Plastic and reconstructive surgery",
      "year": "2024",
      "doi": "10.1097/PRS.0000000000011202",
      "authors": "Wang Anya et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Health Literacy; Surgery, Plastic; Healthcare Disparities; Plastic Surgery Procedures",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37983817/",
      "source_db": "pubmed",
      "source_db_label": "PubMed/MEDLINE",
      "ta_included": true,
      "ta_reason": "Included",
      "pmcid": "PMC11090984",
      "ft_status": "PMC fetch failed (PMC11090984)",
      "ft_reason": "Full text not retrievable"
    }
  ],
  "screening_criteria": {
    "ai_terms": [
      "machine learning",
      "deep learning",
      "artificial intelligence",
      "neural network",
      "algorithm",
      "predictive model",
      "prediction model",
      "classifier",
      "classification",
      "natural language processing",
      "nlp",
      "computer vision",
      "random forest",
      "logistic regression",
      "xgboost",
      "convolutional",
      "transformer",
      "large language model",
      "llm",
      "decision support",
      "risk prediction",
      "federated learning",
      "reinforcement learning",
      "foundation model",
      "chatgpt",
      "gpt-4",
      "supervised learning"
    ],
    "health_terms": [
      "health",
      "clinical",
      "medical",
      "patient",
      "hospital",
      "disease",
      "diagnosis",
      "treatment",
      "care",
      "pathology",
      "radiology",
      "dermatology",
      "cardiology",
      "oncology",
      "ophthalmology",
      "psychiatry",
      "mental health",
      "ehr",
      "electronic health",
      "biomedical",
      "mortality",
      "readmission",
      "sepsis",
      "icu",
      "emergency",
      "chest x-ray",
      "mammograph",
      "cancer",
      "diabetes",
      "cardiovascular",
      "public health",
      "healthcare",
      "medicine"
    ],
    "strong_title_terms": [
      "bias",
      "fairness",
      "fair ",
      "unfair",
      "equitable",
      "equity",
      "disparity",
      "disparities",
      "discrimination",
      "debiasing",
      "debias",
      "underdiagnos",
      "underrepresent",
      "inequit"
    ],
    "ai_bias_terms": [
      "algorithmic bias",
      "algorithmic fairness",
      "ai bias",
      "ai fairness",
      "machine learning bias",
      "machine learning fairness",
      "model bias",
      "bias mitigation",
      "bias detection",
      "bias assessment",
      "fairness-aware",
      "fair machine learning",
      "debiasing",
      "disparate impact",
      "demographic parity",
      "equalized odds",
      "equal opportunity",
      "fairness metric",
      "fairness evaluation",
      "bias in ai",
      "bias in machine learning",
      "bias in algorithm",
      "biased model",
      "biased prediction",
      "mitigating bias",
      "addressing bias",
      "reducing bias",
      "assessing bias",
      "evaluating bias",
      "detecting bias",
      "sources of bias",
      "racial bias",
      "gender bias",
      "age bias",
      "socioeconomic bias",
      "ethnic bias",
      "underdiagnosis bias",
      "representation bias",
      "data bias",
      "label bias",
      "health disparit",
      "health equit",
      "health inequit",
      "unfair"
    ],
    "human_only_terms": [
      "implicit bias training",
      "implicit bias among",
      "clinician bias",
      "physician bias",
      "provider bias",
      "unconscious bias training",
      "implicit association test",
      "weight bias",
      "anti-fat bias"
    ],
    "ai_specific_terms": [
      "algorithmic bias",
      "ai bias",
      "model bias",
      "bias mitigation",
      "bias in ai",
      "bias in machine learning",
      "algorithmic fairness",
      "fairness-aware",
      "debiasing algorithm"
    ],
    "logic": "Include if: (1) has AI/ML terms, AND (2) has health terms, AND (3) bias/fairness is central (strong term in title OR >= 2 AI bias terms in abstract), AND (4) not about human cognitive biases only."
  },
  "query_stats": [
    {
      "id": "Q1",
      "label": "Core: algorithmic bias/fairness + health",
      "query": "(\"algorithmic bias\" OR \"algorithmic fairness\" OR \"AI bias\" OR \"AI fairness\" OR \"machine learning bias\" OR \"machine learning fairness\" OR \"bias mitigation\" OR \"debiasing\" OR \"fairness-aware\" OR \"bias detection\" OR \"bias audit\" OR \"disparate impact\" OR \"demographic parity\" OR \"equalized odds\") AND (\"health\" OR \"healthcare\" OR \"clinical\" OR \"medical\" OR \"biomedical\")",
      "total_in_pubmed": 1632,
      "retrieved": 1632,
      "new_unique": 1632,
      "cumulative": 1632
    },
    {
      "id": "Q2",
      "label": "Bias assessment/mitigation approaches in health AI",
      "query": "(\"bias\" OR \"fairness\") AND (\"assess\" OR \"mitigat\" OR \"detect\" OR \"evaluat\" OR \"framework\" OR \"approach\" OR \"method\") AND (\"artificial intelligence\" OR \"machine learning\" OR \"deep learning\" OR \"algorithm\") AND (\"health\" OR \"healthcare\" OR \"clinical\" OR \"medical\")",
      "total_in_pubmed": 6142,
      "retrieved": 6142,
      "new_unique": 5700,
      "cumulative": 7332
    },
    {
      "id": "Q3",
      "label": "Specific bias axes in clinical AI",
      "query": "(\"racial bias\" OR \"gender bias\" OR \"age bias\" OR \"socioeconomic bias\" OR \"ethnic bias\") AND (\"artificial intelligence\" OR \"machine learning\" OR \"deep learning\" OR \"clinical prediction\" OR \"clinical algorithm\" OR \"clinical decision support\")",
      "total_in_pubmed": 190,
      "retrieved": 190,
      "new_unique": 104,
      "cumulative": 7436
    },
    {
      "id": "Q4",
      "label": "Fairness metrics and frameworks in health",
      "query": "(\"fairness metric\" OR \"fairness framework\" OR \"bias framework\" OR \"equity framework\" OR \"AI Fairness 360\" OR \"fairlearn\" OR \"aequitas\") AND (\"health\" OR \"healthcare\" OR \"clinical\" OR \"medical\")",
      "total_in_pubmed": 533,
      "retrieved": 533,
      "new_unique": 365,
      "cumulative": 7801
    },
    {
      "id": "Q5",
      "label": "Bias in medical imaging / clinical NLP",
      "query": "(\"bias\" OR \"fairness\") AND (\"deep learning\" OR \"neural network\" OR \"convolutional\" OR \"natural language processing\") AND (\"medical imaging\" OR \"radiology\" OR \"pathology\" OR \"dermatology\" OR \"clinical notes\" OR \"electronic health record\")",
      "total_in_pubmed": 1354,
      "retrieved": 1354,
      "new_unique": 649,
      "cumulative": 8450
    },
    {
      "id": "Q6",
      "label": "Health disparities + AI/ML",
      "query": "\"health disparities\" AND (\"machine learning\" OR \"artificial intelligence\" OR \"deep learning\" OR \"algorithm\") AND (\"bias\" OR \"fairness\" OR \"equity\")",
      "total_in_pubmed": 262,
      "retrieved": 262,
      "new_unique": 159,
      "cumulative": 8609
    },
    {
      "id": "Q7",
      "label": "Equitable AI in clinical settings",
      "query": "(\"equitable\" OR \"equity\") AND (\"artificial intelligence\" OR \"machine learning\") AND (\"clinical\" OR \"patient\" OR \"hospital\" OR \"diagnosis\" OR \"treatment\")",
      "total_in_pubmed": 4360,
      "retrieved": 4360,
      "new_unique": 3691,
      "cumulative": 12300
    },
    {
      "id": "E1",
      "label": "MeSH: AI + Healthcare Disparities",
      "query": "\"Artificial Intelligence\"[MeSH] AND \"Healthcare Disparities\"[MeSH]",
      "total_in_pubmed": 122,
      "retrieved": 122,
      "new_unique": 64,
      "cumulative_new": 64
    },
    {
      "id": "E2",
      "label": "MeSH: ML + Bias",
      "query": "\"Machine Learning\"[MeSH] AND \"Bias\"[MeSH]",
      "total_in_pubmed": 591,
      "retrieved": 591,
      "new_unique": 443,
      "cumulative_new": 507
    },
    {
      "id": "E3",
      "label": "MeSH: AI + Prejudice/Discrimination",
      "query": "\"Artificial Intelligence\"[MeSH] AND (\"Prejudice\"[MeSH] OR \"Social Discrimination\"[MeSH])",
      "total_in_pubmed": 147,
      "retrieved": 147,
      "new_unique": 84,
      "cumulative_new": 591
    },
    {
      "id": "E4",
      "label": "Underdiagnosis + algorithm/AI",
      "query": "(\"underdiagnosis\" OR \"underdiagnosed\") AND (\"algorithm\" OR \"machine learning\" OR \"artificial intelligence\") AND (\"bias\" OR \"disparity\")",
      "total_in_pubmed": 46,
      "retrieved": 46,
      "new_unique": 14,
      "cumulative_new": 605
    },
    {
      "id": "E5",
      "label": "Risk score + race/disparity",
      "query": "(\"risk score\" OR \"risk prediction\" OR \"prediction model\") AND (\"race\" OR \"racial\" OR \"disparity\" OR \"disparities\") AND (\"bias\" OR \"fairness\" OR \"equity\")",
      "total_in_pubmed": 250,
      "retrieved": 250,
      "new_unique": 169,
      "cumulative_new": 774
    },
    {
      "id": "E6",
      "label": "Specific tools: AIF360, Fairlearn, etc.",
      "query": "(\"AI Fairness 360\" OR \"AIF360\" OR \"Fairlearn\" OR \"aequitas\" OR \"What-If Tool\") AND (\"health\" OR \"clinical\" OR \"medical\")",
      "total_in_pubmed": 15,
      "retrieved": 15,
      "new_unique": 3,
      "cumulative_new": 777
    },
    {
      "id": "E7",
      "label": "Social determinants + ML + bias",
      "query": "\"social determinants\" AND (\"machine learning\" OR \"artificial intelligence\" OR \"algorithm\") AND (\"bias\" OR \"fairness\" OR \"equity\")",
      "total_in_pubmed": 206,
      "retrieved": 206,
      "new_unique": 48,
      "cumulative_new": 825
    },
    {
      "id": "E8",
      "label": "EHR/clinical data + algorithmic bias",
      "query": "(\"electronic health record\" OR \"EHR\" OR \"clinical data\") AND (\"algorithmic bias\" OR \"model bias\" OR \"prediction bias\" OR \"fairness\")",
      "total_in_pubmed": 185,
      "retrieved": 185,
      "new_unique": 55,
      "cumulative_new": 880
    },
    {
      "id": "E9",
      "label": "Skin tone / dermatology AI bias",
      "query": "(\"skin tone\" OR \"skin color\" OR \"Fitzpatrick\") AND (\"algorithm\" OR \"deep learning\" OR \"AI\" OR \"machine learning\") AND (\"bias\" OR \"fairness\" OR \"performance\")",
      "total_in_pubmed": 150,
      "retrieved": 150,
      "new_unique": 121,
      "cumulative_new": 1001
    },
    {
      "id": "E10",
      "label": "Pulse oximetry / wearable bias",
      "query": "(\"pulse oximetry\" OR \"oximeter\" OR \"SpO2\" OR \"wearable\") AND (\"bias\" OR \"accuracy\" OR \"disparity\") AND (\"race\" OR \"skin\" OR \"pigment\")",
      "total_in_pubmed": 866,
      "retrieved": 866,
      "new_unique": 843,
      "cumulative_new": 1844
    }
  ],
  "reviews_removed": 3999,
  "review_breakdown": {
    "Review": 1501,
    "Meta-Analysis": 117,
    "Narrative Review": 436,
    "Systematic Review": 1769,
    "Scoping Review": 176
  },
  "total_unique_pmids": 14120,
  "total_fetched": 14117,
  "ta_included_count": 1146,
  "ta_excluded_count": 8972,
  "ta_exclusion_reasons": {
    "Bias/fairness not central topic": 6786,
    "No AI/ML component": 1399,
    "Not health-related": 784,
    "Human cognitive biases only": 3
  }
}