{
  "source_file": "[0211] IEEE John_Screening 1 & 2.xlsx \u7684\u526f\u672c.xlsx",
  "source_sheet": "PubMed2",
  "total_pmids_in_sheet": 1899,
  "total_fetched": 1887,
  "reviews_removed": 665,
  "ta_screened": 1222,
  "ta_included_count": 351,
  "ta_excluded_count": 871,
  "ta_exclusion_reasons": {
    "Bias/fairness not central": 835,
    "No AI/ML terms": 24,
    "No health terms": 10,
    "Human cognitive bias only": 2
  },
  "screening_criteria": {
    "ai_terms": [
      "artificial intelligence",
      "machine learning",
      "deep learning",
      "neural network",
      "natural language processing",
      "nlp",
      "computer vision",
      "random forest",
      "decision tree",
      "support vector",
      "svm",
      "logistic regression",
      "gradient boosting",
      "xgboost",
      "ensemble",
      "supervised learning",
      "unsupervised learning",
      "reinforcement learning",
      "transfer learning",
      "federated learning",
      "convolutional neural",
      "recurrent neural",
      "transformer",
      "bert",
      "gpt",
      "large language model",
      "llm",
      "generative ai",
      "foundation model",
      "predictive model",
      "prediction model",
      "clinical prediction",
      "risk prediction",
      "classification model",
      "regression model",
      "clustering",
      "automated",
      "algorithm",
      "computational",
      "data-driven",
      "clinical decision support",
      "computer-aided",
      "image recognition",
      "feature selection",
      "dimensionality reduction"
    ],
    "health_terms": [
      "health",
      "clinical",
      "medical",
      "patient",
      "hospital",
      "disease",
      "diagnosis",
      "treatment",
      "therapy",
      "prognosis",
      "mortality",
      "morbidity",
      "surgical",
      "radiology",
      "pathology",
      "oncology",
      "cardiology",
      "dermatology",
      "ophthalmology",
      "psychiatry",
      "mental health",
      "emergency",
      "icu",
      "intensive care",
      "ehr",
      "electronic health record",
      "biomedical",
      "pharmaceutical",
      "drug",
      "genomic",
      "imaging",
      "screening",
      "vaccination",
      "epidemiology",
      "public health",
      "healthcare",
      "care",
      "nursing",
      "pharmacy",
      "dental",
      "rehabilitation",
      "chronic",
      "acute",
      "outpatient",
      "inpatient",
      "primary care",
      "sepsis",
      "pneumonia",
      "diabetes",
      "cancer",
      "tumor",
      "stroke",
      "heart",
      "lung",
      "kidney",
      "liver",
      "brain"
    ],
    "strong_title_terms": [
      "bias",
      "fairness",
      "fair ",
      "unfair",
      "equity",
      "inequity",
      "disparity",
      "disparities",
      "discrimination",
      "algorithmic bias",
      "racial bias",
      "gender bias",
      "health equity",
      "health disparities",
      "equitable",
      "inequitable"
    ],
    "ai_bias_terms": [
      "algorithmic bias",
      "model bias",
      "prediction bias",
      "data bias",
      "selection bias",
      "label bias",
      "measurement bias",
      "sampling bias",
      "representation bias",
      "training bias",
      "dataset bias",
      "fairness",
      "unfairness",
      "equalized odds",
      "demographic parity",
      "equal opportunity",
      "disparate impact",
      "calibration bias",
      "subgroup",
      "performance gap",
      "performance disparity",
      "underrepresent",
      "overrepresent",
      "health equity",
      "health disparity",
      "health disparities",
      "racial disparity",
      "racial disparities",
      "gender disparity",
      "sex-based",
      "race-based",
      "ethnic",
      "socioeconomic",
      "underserved",
      "marginalized",
      "vulnerable population",
      "minority",
      "bias mitigation",
      "bias detection",
      "bias assessment",
      "bias evaluation",
      "bias audit",
      "debiasing",
      "debias",
      "fair machine learning",
      "fair ai",
      "responsible ai",
      "trustworthy ai",
      "ethical ai"
    ],
    "human_only_terms": [
      "cognitive bias",
      "confirmation bias",
      "anchoring bias",
      "availability bias",
      "recall bias",
      "observer bias",
      "interviewer bias",
      "response bias",
      "reporting bias",
      "publication bias",
      "attrition bias",
      "detection bias",
      "information bias",
      "lead-time bias",
      "length bias",
      "surveillance bias",
      "referral bias",
      "volunteer bias",
      "healthy worker",
      "berkson",
      "neyman"
    ],
    "ai_specific_terms": [
      "algorithmic",
      "algorithm",
      "machine learning",
      "deep learning",
      "artificial intelligence",
      "model bias",
      "prediction bias",
      "training bias",
      "data bias",
      "fairness",
      "fair ",
      "equalized odds",
      "demographic parity",
      "disparate impact"
    ]
  },
  "phase1_pmcid_lookup": {
    "pmcids_found": 274,
    "no_pmcid": 77
  },
  "phase2_fulltext_screening": {
    "ft_screened": 274,
    "ft_included": 213,
    "ft_excluded": 61,
    "ft_fetch_failed": 0,
    "exclusion_reasons": {
      "No approach indicators in full text": 34,
      "Insufficient evidence (approach_count=2, bias_title=False)": 10,
      "Insufficient evidence (approach_count=1, bias_title=False)": 14,
      "No AI/ML + health in full text": 3
    }
  },
  "ft_screening_criteria": {
    "approach_indicators": [
      "demographic parity",
      "equalized odds",
      "equal opportunity",
      "disparate impact",
      "predictive parity",
      "calibration across",
      "fairness metric",
      "fairness measure",
      "bias assessment",
      "bias detection",
      "bias evaluation",
      "bias audit",
      "bias analysis",
      "bias measurement",
      "subgroup analysis",
      "stratified analysis",
      "disaggregated",
      "performance gap",
      "performance disparity",
      "model audit",
      "algorithmic audit",
      "bias mitigation",
      "debiasing",
      "debias",
      "reweighting",
      "reweighing",
      "resampling",
      "oversampling",
      "undersampling",
      "adversarial debiasing",
      "fairness constraint",
      "fair representation",
      "threshold adjustment",
      "calibration",
      "data augmentation",
      "aif360",
      "fairlearn",
      "aequitas",
      "responsible ai",
      "trustworthy ai",
      "ethical ai",
      "fair machine learning",
      "evaluate fairness",
      "assess bias",
      "measure bias",
      "quantify bias",
      "detect bias",
      "identify bias",
      "examine bias",
      "analyze bias",
      "mitigate bias",
      "reduce bias",
      "address bias",
      "correct bias",
      "racial bias",
      "gender bias",
      "age bias",
      "socioeconomic",
      "health equity",
      "health disparity",
      "health disparities",
      "underrepresented",
      "minority",
      "vulnerable population"
    ],
    "bias_title_terms": [
      "bias",
      "fairness",
      "fair ",
      "unfair",
      "equity",
      "inequity",
      "disparity",
      "disparities",
      "discrimination",
      "equitable"
    ]
  },
  "ft_included": [
    {
      "pmid": "30508424",
      "title": "Ensuring Fairness in Machine Learning to Advance Health Equity.",
      "abstract": "Machine learning is used increasingly in clinical care to improve diagnosis, treatment selection, and health system efficiency. Because machine-learning models learn from historically collected data, populations that have experienced human and structural biases in the past-called protected groups-are vulnerable to harm by incorrect predictions or withholding of resources. This article describes how model design, biases in data, and the interactions of model predictions with clinicians and patients may exacerbate health care disparities. Rather than simply guarding against these harms passively, machine-learning systems should be used proactively to advance health equity. For that goal to be achieved, principles of distributive justice must be incorporated into model design, deployment, and evaluation. The article describes several technical implementations of distributive justice-specifically those that ensure equality in patient outcomes, performance, and resource allocation-and guides clinicians as to when they should prioritize each principle. Machine learning is providing increasingly sophisticated decision support and population-level monitoring, and it should encode principles of justice to ensure that models benefit all patients.",
      "authors": "Rajkomar Alvin; Hardt Michaela; Howell Michael D; Corrado Greg; Chin Marshall H",
      "year": "2018",
      "journal": "Annals of internal medicine",
      "doi": "10.7326/M18-1990",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30508424/",
      "mesh_terms": "Critical Care; Health Care Rationing; Health Equity; Healthcare Disparities; Humans; Length of Stay; Machine Learning; Patient Outcome Assessment; Social Justice",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC6594166",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "37599147",
      "title": "Illness severity assessment of older adults in critical illness using machine learning (ELDER-ICU): an international multicentre study with subgroup bias evaluation.",
      "abstract": "BACKGROUND: Comorbidity, frailty, and decreased cognitive function lead to a higher risk of death in elderly patients (more than 65 years of age) during acute medical events. Early and accurate illness severity assessment can support appropriate decision making for clinicians caring for these patients. We aimed to develop ELDER-ICU, a machine learning model to assess the illness severity of older adults admitted to the intensive care unit (ICU) with cohort-specific calibration and evaluation for potential model bias. METHODS: In this retrospective, international multicentre study, the ELDER-ICU model was developed using data from 14 US hospitals, and validated in 171 hospitals from the USA and Netherlands. Data were extracted from the Medical Information Mart for Intensive Care database, electronic ICU Collaborative Research Database, and Amsterdam University Medical Centers Database. We used six categories of data as predictors, including demographics and comorbidities, physical frailty, laboratory tests, vital signs, treatments, and urine output. Patient data from the first day of ICU stay were used to predict in-hospital mortality. We used the eXtreme Gradient Boosting algorithm (XGBoost) to develop models and the SHapley Additive exPlanations method to explain model prediction. The trained model was calibrated before internal, external, and temporal validation. The final XGBoost model was compared against three other machine learning algorithms and five clinical scores. We performed subgroup analysis based on age, sex, and race. We assessed the discrimination and calibration of models using the area under receiver operating characteristic (AUROC) and standardised mortality ratio (SMR) with 95% CIs. FINDINGS: Using the development dataset (n=50\u2008366) and predictive model building process, the XGBoost algorithm performed the best in all types of validations compared with other machine learning algorithms and clinical scores (internal validation with 5037 patients from 14 US hospitals, AUROC=0\u00b7866 [95% CI 0\u00b7851-0\u00b7880]; external validation in the US population with 20\u2008541 patients from 169 hospitals, AUROC=0\u00b7838 [0\u00b7829-0\u00b7847]; external validation in European population with 2411 patients from one hospital, AUROC=0\u00b7833 [0\u00b7812-0\u00b7853]; temporal validation with 4311 patients from one hospital, AUROC=0\u00b7884 [0\u00b7869-0\u00b7897]). In the external validation set (US population), the median AUROCs of bias evaluations covering eight subgroups were above 0\u00b781, and the overall SMR was 0\u00b799 (0\u00b796-1\u00b703). The top ten risk predictors were the minimum Glasgow Coma Scale score, total urine output, average respiratory rate, mechanical ventilation use, best state of activity, Charlson Comorbidity Index score, geriatric nutritional risk index, code status, age, and maximum blood urea nitrogen. A simplified model containing only the top 20 features (ELDER-ICU-20) had similar predictive performance to the full model. INTERPRETATION: The ELDER-ICU model reliably predicts the risk of in-hospital mortality using routinely collected clinical features. The predictions could inform clinicians about patients who are at elevated risk of deterioration. Prospective validation of this model in clinical practice and a process for continuous performance monitoring and model recalibration are needed. FUNDING: National Institutes of Health, National Natural Science Foundation of China, National Special Health Science Program, Health Science and Technology Plan of Zhejiang Province, Fundamental Research Funds for the Central Universities, Drug Clinical Evaluate Research of Chinese Pharmaceutical Association, and National Key R&D Program of China.",
      "authors": "Liu Xiaoli; Hu Pan; Yeung Wesley; Zhang Zhongheng; Ho Vanda; Liu Chao; Dumontier Clark; Thoral Patrick J; Mao Zhi; Cao Desen; Mark Roger G; Zhang Zhengbo; Feng Mengling; Li Deyu; Celi Leo Anthony",
      "year": "2023",
      "journal": "The Lancet. Digital health",
      "doi": "10.1016/S2589-7500(23)00128-0",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37599147/",
      "mesh_terms": "United States; Aged; Humans; Critical Illness; Frailty; Retrospective Studies; Intensive Care Units; Machine Learning",
      "keywords": "",
      "pub_types": "Multicenter Study; Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC12557411",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39453630",
      "title": "Advancing AI Data Ethics in Nursing: Future Directions for Nursing Practice, Research, and Education.",
      "abstract": "The ethics of artificial intelligence (AI) are increasingly recognized due to concerns such as algorithmic bias, opacity, trust issues, data security, and fairness. Specifically, machine learning algorithms, central to AI technologies, are essential in striving for ethically sound systems that mimic human intelligence. These technologies rely heavily on data, which often remain obscured within complex systems and must be prioritized for ethical collection, processing, and usage. The significance of data ethics in achieving responsible AI was first highlighted in the broader context of health care and subsequently in nursing. This viewpoint explores the principles of data ethics, drawing on relevant frameworks and strategies identified through a formal literature review. These principles apply to real-world and synthetic data in AI and machine-learning contexts. Additionally, the data-centric AI paradigm is briefly examined, emphasizing its focus on data quality and the ethical development of AI solutions that integrate human-centered domain expertise. The ethical considerations specific to nursing are addressed, including 4 recommendations for future directions in nursing practice, research, and education and 2 hypothetical nurse-focused ethical case studies. The primary objectives are to position nurses to actively participate in AI and data ethics, thereby contributing to creating high-quality and relevant data for machine learning applications.",
      "authors": "Ball Dunlap Patricia A; Michalowski Martin",
      "year": "2024",
      "journal": "JMIR nursing",
      "doi": "10.2196/62678",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39453630/",
      "mesh_terms": "Artificial Intelligence; Humans; Education, Nursing; Forecasting; Ethics, Nursing; Nursing Research; Machine Learning",
      "keywords": "AI data ethics; artificial intelligence; data literacy; data-centric AI; health care AI; machine learning; nurses; nursing informatics; responsible AI",
      "pub_types": "Journal Article",
      "pmcid": "PMC11529373",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "33981989",
      "title": "Addressing Fairness, Bias, and Appropriate Use of Artificial Intelligence and Machine Learning in Global Health.",
      "abstract": "In Low- and Middle- Income Countries (LMICs), machine learning (ML) and artificial intelligence (AI) offer attractive solutions to address the shortage of health care resources and improve the capacity of the local health care infrastructure. However, AI and ML should also be used cautiously, due to potential issues of fairness and algorithmic bias that may arise if not applied properly. Furthermore, populations in LMICs can be particularly vulnerable to bias and fairness in AI algorithms, due to a lack of technical capacity, existing social bias against minority groups, and a lack of legal protections. In order to address the need for better guidance within the context of global health, we describe three basic criteria (Appropriateness, Fairness, and Bias) that can be used to help evaluate the use of machine learning and AI systems: 1) APPROPRIATENESS is the process of deciding how the algorithm should be used in the local context, and properly matching the machine learning model to the target population; 2) BIAS is a systematic tendency in a model to favor one demographic group vs another, which can be mitigated but can lead to unfairness; and 3) FAIRNESS involves examining the impact on various demographic groups and choosing one of several mathematical definitions of group fairness that will adequately satisfy the desired set of legal, cultural, and ethical requirements. Finally, we illustrate how these principles can be applied using a case study of machine learning applied to the diagnosis and screening of pulmonary disease in Pune, India. We hope that these methods and principles can help guide researchers and organizations working in global health who are considering the use of machine learning and artificial intelligence.",
      "authors": "Fletcher Richard Rib\u00f3n; Nakeshimana Audace; Olubeko Olusubomi",
      "year": "2020",
      "journal": "Frontiers in artificial intelligence",
      "doi": "10.3389/frai.2020.561802",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33981989/",
      "mesh_terms": "",
      "keywords": "appropriate use; artificial intelligence; bias; ethics; fairness; global health; machine learning; medicine",
      "pub_types": "Editorial",
      "pmcid": "PMC8107824",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35615443",
      "title": "Beyond bias and discrimination: redefining the AI ethics principle of fairness in healthcare machine-learning algorithms.",
      "abstract": "The increasing implementation of and reliance on machine-learning (ML) algorithms to perform tasks, deliver services and make decisions in health and healthcare have made the need for fairness in ML, and more specifically in healthcare ML algorithms (HMLA), a very important and urgent task. However, while the debate on fairness in the ethics of artificial intelligence (AI) and in HMLA has grown significantly over the last decade, the very concept of fairness as an ethical value has not yet been sufficiently explored. Our paper aims to fill this gap and address the AI ethics principle of fairness from a conceptual standpoint, drawing insights from accounts of fairness elaborated in moral philosophy and using them to conceptualise fairness as an ethical value and to redefine fairness in HMLA accordingly. To achieve our goal, following a first section aimed at clarifying the background, methodology and structure of the paper, in the second section, we provide an overview of the discussion of the AI ethics principle of fairness in HMLA and show that the concept of fairness underlying this debate is framed in purely distributive terms and overlaps with non-discrimination, which is defined in turn as the absence of biases. After showing that this framing is inadequate, in the third section, we pursue an ethical inquiry into the concept of fairness and argue that fairness ought to be conceived of as an ethical value. Following a clarification of the relationship between fairness and non-discrimination, we show that the two do not overlap and that fairness requires much more than just non-discrimination. Moreover, we highlight that fairness not only has a distributive but also a socio-relational dimension. Finally, we pinpoint the constitutive components of fairness. In doing so, we base our arguments on a renewed reflection on the concept of respect, which goes beyond the idea of equal respect to include respect for individual persons. In the fourth section, we analyse the implications of our conceptual redefinition of fairness as an ethical value in the discussion of fairness in HMLA. Here, we claim that fairness requires more than non-discrimination and the absence of biases as well as more than just distribution; it needs to ensure that HMLA respects persons both as persons and as particular individuals. Finally, in the fifth section, we sketch some broader implications and show how our inquiry can contribute to making HMLA and, more generally, AI promote the social good and a fairer society.",
      "authors": "Giovanola Benedetta; Tiribelli Simona",
      "year": "2023",
      "journal": "AI & society",
      "doi": "10.1007/s00146-022-01455-6",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35615443/",
      "mesh_terms": "",
      "keywords": "Bias; Discrimination; Ethics of algorithms; Fairness; Healthcare machine-learning algorithms; Respect",
      "pub_types": "Journal Article",
      "pmcid": "PMC9123626",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35958671",
      "title": "Bias or biology? Importance of model interpretation in machine learning studies from electronic health records.",
      "abstract": "OBJECTIVE: The rate of diabetic complication progression varies across individuals and understanding factors that alter the rate of complication progression may uncover new clinical interventions for personalized diabetes management. MATERIALS AND METHODS: We explore how various machine learning (ML) models and types of electronic health records (EHRs) can predict fast versus slow onset of neuropathy, nephropathy, ocular disease, or cardiovascular disease using only patient data collected prior to diabetes diagnosis. RESULTS: We find that optimized random forest models performed best to accurately predict the diagnosis of a diabetic complication, with the most effective model distinguishing between fast versus slow nephropathy (AUROC\u2009=\u20090.75). Using all data sets combined allowed for the highest model predictive performance, and social history or laboratory alone were most predictive. SHapley Additive exPlanations (SHAP) model interpretation allowed for exploration of predictors of fast and slow complication diagnosis, including underlying biases present in the EHR. Patients in the fast group had more medical visits, incurring a potential informed decision bias. DISCUSSION: Our study is unique in the realm of ML studies as it leverages SHAP as a starting point to explore patient markers not routinely used in diabetes monitoring. A mix of both bias and biological processes is likely present in influencing a model's ability to distinguish between groups. CONCLUSION: Overall, model interpretation is a critical step in evaluating validity of a user-intended endpoint for a model when using EHR data, and predictors affected by bias and those driven by biologic processes should be equally recognized.",
      "authors": "Momenzadeh Amanda; Shamsa Ali; Meyer Jesse G",
      "year": "2022",
      "journal": "JAMIA open",
      "doi": "10.1093/jamiaopen/ooac063",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35958671/",
      "mesh_terms": "",
      "keywords": "SHAP; bias in EHR; clinical model interpretation; diabetes mellitus complication prediction; machine learning",
      "pub_types": "Journal Article",
      "pmcid": "PMC9360778",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35243993",
      "title": "Mitigating Racial Bias in Machine Learning.",
      "abstract": "When applied in the health sector, AI-based applications raise not only ethical but legal and safety concerns, where algorithms trained on data from majority populations can generate less accurate or reliable results for minorities and other disadvantaged groups.",
      "authors": "Kostick-Quenet Kristin M; Cohen I Glenn; Gerke Sara; Lo Bernard; Antaki James; Movahedi Faezah; Njah Hasna; Schoen Lauren; Estep Jerry E; Blumenthal-Barby J S",
      "year": "2022",
      "journal": "The Journal of law, medicine & ethics : a journal of the American Society of Law, Medicine & Ethics",
      "doi": "10.1017/jme.2022.13",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35243993/",
      "mesh_terms": "Artificial Intelligence; Humans; Machine Learning; Racism",
      "keywords": "Algorithmic Bias; Artificial Intelligence; Ethics; Machine Learning; Racial Bias",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, P.H.S.",
      "pmcid": "PMC12140104",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36706849",
      "title": "Evaluating and mitigating bias in machine learning models for cardiovascular disease prediction.",
      "abstract": "OBJECTIVE: The study aims to investigate whether machine learning-based predictive models for cardiovascular disease (CVD) risk assessment show equivalent performance across demographic groups (such as race and gender) and if bias mitigation methods can reduce any bias present in the models. This is important as systematic bias may be introduced when collecting and preprocessing health data, which could affect the performance of the models on certain demographic sub-cohorts. The study is to investigate this using electronic health records data and various machine learning models. METHODS: The study used large de-identified Electronic Health Records data from Vanderbilt University Medical Center. Machine learning (ML) algorithms including logistic regression, random forest, gradient-boosting trees, and long short-term memory were applied to build multiple predictive models. Model bias and fairness were evaluated using equal opportunity difference (EOD, 0 indicates fairness) and disparate impact (DI, 1 indicates fairness). In our study, we also evaluated the fairness of a non-ML baseline model, the American Heart Association (AHA) Pooled Cohort Risk Equations (PCEs). Moreover, we compared the performance of three different de-biasing methods: removing protected attributes (e.g., race and gender), resampling the imbalanced training dataset by sample size, and resampling by the proportion of people with CVD outcomes. RESULTS: The study cohort included 109,490 individuals (mean [SD] age 47.4 [14.7] years; 64.5% female; 86.3% White; 13.7% Black). The experimental results suggested that most ML models had smaller EOD and DI than PCEs. For ML models, the mean EOD ranged from -0.001 to 0.018 and the mean DI ranged from 1.037 to 1.094 across race groups. There was a larger EOD and DI across gender groups, with EOD ranging from 0.131 to 0.136 and DI ranging from 1.535 to 1.587. For debiasing methods, removing protected attributes didn't significantly reduced the bias for most ML models. Resampling by sample size also didn't consistently decrease bias. Resampling by case proportion reduced the EOD and DI for gender groups but slightly reduced accuracy in many cases. CONCLUSIONS: Among the VUMC cohort, both PCEs and ML models were biased against women, suggesting the need to investigate and correct gender disparities in CVD risk prediction. Resampling by proportion reduced the bias for gender groups but not for race groups.",
      "authors": "Li Fuchen; Wu Patrick; Ong Henry H; Peterson Josh F; Wei Wei-Qi; Zhao Juan",
      "year": "2023",
      "journal": "Journal of biomedical informatics",
      "doi": "10.1016/j.jbi.2023.104294",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36706849/",
      "mesh_terms": "Humans; Female; Middle Aged; Male; Cardiovascular Diseases; Machine Learning; Algorithms; Random Forest; Logistic Models",
      "keywords": "Bias mitigation; Cardiovascular diseases; Clinical predictive models; Electronic health records; Fairness; Machine learning",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC11104322",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36060496",
      "title": "Enabling Fairness in Healthcare Through Machine Learning.",
      "abstract": "The use of machine learning systems for decision-support in healthcare may exacerbate health inequalities. However, recent work suggests that algorithms trained on sufficiently diverse datasets could in principle combat health inequalities. One concern about these algorithms is that their performance for patients in traditionally disadvantaged groups exceeds their performance for patients in traditionally advantaged groups. This renders the algorithmic decisions unfair relative to the standard fairness metrics in machine learning. In this paper, we defend the permissible use of affirmative algorithms; that is, algorithms trained on diverse datasets that perform better for traditionally disadvantaged groups. Whilst such algorithmic decisions may be unfair, the fairness of algorithmic decisions is not the appropriate locus of moral evaluation. What matters is the fairness of final decisions, such as diagnoses, resulting from collaboration between clinicians and algorithms. We argue that affirmative algorithms can permissibly be deployed provided the resultant final decisions are fair.",
      "authors": "Grote Thomas; Keeling Geoff",
      "year": "2022",
      "journal": "Ethics and information technology",
      "doi": "10.1007/s10676-022-09658-7",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36060496/",
      "mesh_terms": "",
      "keywords": "Bias; Decision-making; Fairness; Healthcare; Machine learning",
      "pub_types": "Journal Article",
      "pmcid": "PMC9428374",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40620096",
      "title": "A practical guide for nephrologist peer reviewers: evaluating artificial intelligence and machine learning research in nephrology.",
      "abstract": "Artificial intelligence (AI) and machine learning (ML) are transforming nephrology by enhancing diagnosis, risk prediction, and treatment optimization for conditions such as acute kidney injury (AKI) and chronic kidney disease (CKD). AI-driven models utilize diverse datasets-including electronic health records, imaging, and biomarkers-to improve clinical decision-making. Applications such as convolutional neural networks for kidney biopsy interpretation, and predictive modeling for renal replacement therapies underscore AI's potential. Nonetheless, challenges including data quality, limited external validation, algorithmic bias, and poor interpretability constrain the clinical reliability of AI/ML models. To address these issues, this article offers a structured framework for nephrologist peer reviewers, integrating the TRIPOD-AI (Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis-AI Extension) checklist. Key evaluation criteria include dataset integrity, feature selection, model validation, reporting transparency, ethics, and real-world applicability. This framework promotes rigorous peer review and enhances the reproducibility, clinical relevance, and fairness of AI research in nephrology. Moreover, AI/ML studies must confront biases-data, selection, and algorithmic-that adversely affect model performance. Mitigation strategies such as data diversification, multi-center validation, and fairness-aware algorithms are essential. Overfitting in AI is driven by small patient cohorts faced with thousands of candidate features; our framework spotlights this imbalance and offers concrete remedies. Future directions in AI-driven nephrology include multimodal data fusion for improved predictive modeling, deep learning for automated imaging analysis, wearable-based monitoring, and clinical decision support systems (CDSS) that integrate comprehensive patient data. A visual summary of key manuscript sections is included.",
      "authors": "Wang Yanni; Cheungpasitporn Wisit; Ali Hatem; Qing Jianbo; Thongprayoon Charat; Kaewput Wisit; Soliman Karim M; Huang Zhengxing; Yang Min; Zhang Zhongheng",
      "year": "2025",
      "journal": "Renal failure",
      "doi": "10.1080/0886022X.2025.2513002",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40620096/",
      "mesh_terms": "Humans; Acute Kidney Injury; Artificial Intelligence; Machine Learning; Nephrologists; Nephrology; Peer Review, Research; Renal Insufficiency, Chronic; Reproducibility of Results",
      "keywords": "Artificial intelligence; kidney diseases; machine learning; nephrology; peer review; personalized treatment",
      "pub_types": "Editorial",
      "pmcid": "PMC12239107",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39116187",
      "title": "Targeting Machine Learning and Artificial Intelligence Algorithms in Health Care to Reduce Bias and Improve Population Health.",
      "abstract": "Policy Points Artificial intelligence (AI) is disruptively innovating health care and surpassing our ability to define its boundaries and roles in health care and regulate its application in legal and ethical ways. Significant progress has been made in governance in the United States and the European Union. It is incumbent on developers, end users, the public, providers, health care systems, and policymakers to collaboratively ensure that we adopt a national AI health strategy that realizes the Quintuple Aim; minimizes race-based medicine; prioritizes transparency, equity, and algorithmic vigilance; and integrates the patient and community voices throughout all aspects of AI development and deployment.",
      "authors": "Hurd Thelma C; Cobb Payton Fay; Hood Darryl B",
      "year": "2024",
      "journal": "The Milbank quarterly",
      "doi": "10.1111/1468-0009.12712",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39116187/",
      "mesh_terms": "Humans; Artificial Intelligence; Machine Learning; Population Health; United States; Delivery of Health Care; Algorithms; Bias",
      "keywords": "algorithmic bias; artificial intelligence; ethics; machine learning; minority health",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC11576591",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "37921762",
      "title": "Fairness of Machine Learning Algorithms for Predicting Foregone Preventive Dental Care for Adults.",
      "abstract": "IMPORTANCE: Access to routine dental care prevents advanced dental disease and improves oral and overall health. Identifying individuals at risk of foregoing preventive dental care can direct prevention efforts toward high-risk populations. OBJECTIVE: To predict foregone preventive dental care among adults overall and in sociodemographic subgroups and to assess the algorithmic fairness. DESIGN, SETTING, AND PARTICIPANTS: This prognostic study was a secondary analyses of longitudinal data from the US Medical Expenditure Panel Survey (MEPS) from 2016 to 2019, each with 2 years of follow-up. Participants included adults aged 18 years and older. Data analysis was performed from December 2022 to June 2023. EXPOSURE: A total of 50 predictors, including demographic and socioeconomic characteristics, health conditions, behaviors, and health services use, were assessed. MAIN OUTCOMES AND MEASURES: The outcome of interest was foregoing preventive dental care, defined as either cleaning, general examination, or an appointment with the dental hygienist, in the past year. RESULTS: Among 32\u202f234 participants, the mean (SD) age was 48.5 (18.2) years and 17\u202f386 participants (53.9%) were female; 1935 participants (6.0%) were Asian, 5138 participants (15.9%) were Black, 7681 participants (23.8%) were Hispanic, 16\u202f503 participants (51.2%) were White, and 977 participants (3.0%) identified as other (eg, American Indian and Alaska Native) or multiple racial or ethnic groups. There were 21\u202f083 (65.4%) individuals who missed preventive dental care in the past year. The algorithms demonstrated high performance, achieving an area under the receiver operating characteristic curve (AUC) of 0.84 (95% CI, 0.84-0.85) in the overall population. While the full sample model performed similarly when applied to White individuals and older adults (AUC, 0.88; 95% CI, 0.87-0.90), there was a loss of performance for other subgroups. Removing the subgroup-sensitive predictors (ie, race and ethnicity, age, and income) did not impact model performance. Models stratified by race and ethnicity performed similarly or worse than the full model for all groups, with the lowest performance for individuals who identified as other or multiple racial groups (AUC, 0.76; 95% CI, 0.70-0.81). Previous pattern of dental visits, health care utilization, dental benefits, and sociodemographic characteristics were the highest contributing predictors to the models' performance. CONCLUSIONS AND RELEVANCE: Findings of this prognostic study using cohort data suggest that tree-based ensemble machine learning models could accurately predict adults at risk of foregoing preventive dental care and demonstrated bias against underrepresented sociodemographic groups. These results highlight the importance of evaluating model fairness during development and testing to avoid exacerbating existing biases.",
      "authors": "Schuch Helena Silveira; Furtado Mariane; Silva Gabriel Ferreira Dos Santos; Kawachi Ichiro; Chiavegatto Filho Alexandre D P; Elani Hawazin W",
      "year": "2023",
      "journal": "JAMA network open",
      "doi": "10.1001/jamanetworkopen.2023.41625",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37921762/",
      "mesh_terms": "Humans; Aged; Ethnicity; Racial Groups; Algorithms; Machine Learning; Dental Care",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC10625037",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36714611",
      "title": "Fairness in the prediction of acute postoperative pain using machine learning models.",
      "abstract": "INTRODUCTION: Overall performance of machine learning-based prediction models is promising; however, their generalizability and fairness must be vigorously investigated to ensure they perform sufficiently well for all patients. OBJECTIVE: This study aimed to evaluate prediction bias in machine learning models used for predicting acute postoperative pain. METHOD: We conducted a retrospective review of electronic health records for patients undergoing orthopedic surgery from June 1, 2011, to June 30, 2019, at the University of Florida Health system/Shands Hospital. CatBoost machine learning models were trained for predicting the binary outcome of low (\u22644) and high pain (>4). Model biases were assessed against seven protected attributes of age, sex, race, area deprivation index (ADI), speaking language, health literacy, and insurance type. Reweighing of protected attributes was investigated for reducing model bias compared with base models. Fairness metrics of equal opportunity, predictive parity, predictive equality, statistical parity, and overall accuracy equality were examined. RESULTS: The final dataset included 14,263 patients [age: 60.72 (16.03) years, 53.87% female, 39.13% low acute postoperative pain]. The machine learning model (area under the curve, 0.71) was biased in terms of age, race, ADI, and insurance type, but not in terms of sex, language, and health literacy. Despite promising overall performance in predicting acute postoperative pain, machine learning-based prediction models may be biased with respect to protected attributes. CONCLUSION: These findings show the need to evaluate fairness in machine learning models involved in perioperative pain before they are implemented as clinical decision support tools.",
      "authors": "Davoudi Anis; Sajdeya Ruba; Ison Ron; Hagen Jennifer; Rashidi Parisa; Price Catherine C; Tighe Patrick J",
      "year": "2022",
      "journal": "Frontiers in digital health",
      "doi": "10.3389/fdgth.2022.970281",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36714611/",
      "mesh_terms": "",
      "keywords": "algorithmic bias; clinical decision support systems; machine learing; orthopedic procedures; postoperative pain",
      "pub_types": "Journal Article",
      "pmcid": "PMC9874861",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39163597",
      "title": "Mitigating Sociodemographic Bias in Opioid Use Disorder Prediction: Fairness-Aware Machine Learning Framework.",
      "abstract": "BACKGROUND: Opioid use disorder (OUD) is a critical public health crisis in the United States, affecting >5.5 million Americans in 2021. Machine learning has been used to predict patient risk of incident OUD. However, little is known about the fairness and bias of these predictive models. OBJECTIVE: The aims of this study are two-fold: (1) to develop a machine learning bias mitigation algorithm for sociodemographic features and (2) to develop a fairness-aware weighted majority voting (WMV) classifier for OUD prediction. METHODS: We used the 2020 National Survey on Drug and Health data to develop a neural network (NN) model using stochastic gradient descent (SGD; NN-SGD) and an NN model using Adam (NN-Adam) optimizers and evaluated sociodemographic bias by comparing the area under the curve values. A bias mitigation algorithm, based on equality of odds, was implemented to minimize disparities in specificity and recall. Finally, a WMV classifier was developed for fairness-aware prediction of OUD. To further analyze bias detection and mitigation, we did a 1-N matching of OUD to non-OUD cases, controlling for socioeconomic variables, and evaluated the performance of the proposed bias mitigation algorithm and WMV classifier. RESULTS: Our bias mitigation algorithm substantially reduced bias with NN-SGD, by 21.66% for sex, 1.48% for race, and 21.04% for income, and with NN-Adam by 16.96% for sex, 8.87% for marital status, 8.45% for working condition, and 41.62% for race. The fairness-aware WMV classifier achieved a recall of 85.37% and 92.68% and an accuracy of 58.85% and 90.21% using NN-SGD and NN-Adam, respectively. The results after matching also indicated remarkable bias reduction with NN-SGD and NN-Adam, respectively, as follows: sex (0.14% vs 0.97%), marital status (12.95% vs 10.33%), working condition (14.79% vs 15.33%), race (60.13% vs 41.71%), and income (0.35% vs 2.21%). Moreover, the fairness-aware WMV classifier achieved high performance with a recall of 100% and 85.37% and an accuracy of 73.20% and 89.38% using NN-SGD and NN-Adam, respectively. CONCLUSIONS: The application of the proposed bias mitigation algorithm shows promise in reducing sociodemographic bias, with the WMV classifier confirming bias reduction and high performance in OUD prediction.",
      "authors": "Yaseliani Mohammad; Noor-E-Alam Md; Hasan Md Mahmudul",
      "year": "2024",
      "journal": "JMIR AI",
      "doi": "10.2196/55820",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39163597/",
      "mesh_terms": "",
      "keywords": "bias mitigation; fairness and bias; machine learning; majority voting; opioid use disorder",
      "pub_types": "Journal Article",
      "pmcid": "PMC11372321",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39119589",
      "title": "Deconstructing demographic bias in speech-based machine learning models for digital health.",
      "abstract": "INTRODUCTION: Machine learning (ML) algorithms have been heralded as promising solutions to the realization of assistive systems in digital healthcare, due to their ability to detect fine-grain patterns that are not easily perceived by humans. Yet, ML algorithms have also been critiqued for treating individuals differently based on their demography, thus propagating existing disparities. This paper explores gender and race bias in speech-based ML algorithms that detect behavioral and mental health outcomes. METHODS: This paper examines potential sources of bias in the data used to train the ML, encompassing acoustic features extracted from speech signals and associated labels, as well as in the ML decisions. The paper further examines approaches to reduce existing bias via using the features that are the least informative of one's demographic information as the ML input, and transforming the feature space in an adversarial manner to diminish the evidence of the demographic information while retaining information about the focal behavioral and mental health state. RESULTS: Results are presented in two domains, the first pertaining to gender and race bias when estimating levels of anxiety, and the second pertaining to gender bias in detecting depression. Findings indicate the presence of statistically significant differences in both acoustic features and labels among demographic groups, as well as differential ML performance among groups. The statistically significant differences present in the label space are partially preserved in the ML decisions. Although variations in ML performance across demographic groups were noted, results are mixed regarding the models' ability to accurately estimate healthcare outcomes for the sensitive groups. DISCUSSION: These findings underscore the necessity for careful and thoughtful design in developing ML models that are capable of maintaining crucial aspects of the data and perform effectively across all populations in digital healthcare applications.",
      "authors": "Yang Michael; El-Attar Abd-Allah; Chaspari Theodora",
      "year": "2024",
      "journal": "Frontiers in digital health",
      "doi": "10.3389/fdgth.2024.1351637",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39119589/",
      "mesh_terms": "",
      "keywords": "anxiety; demographic bias; depression; fairness; machine learning; speech",
      "pub_types": "Journal Article",
      "pmcid": "PMC11306200",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "37347528",
      "title": "Artificial Intelligence Bias in Health Care: Web-Based Survey.",
      "abstract": "BACKGROUND: Resources are increasingly spent on artificial intelligence (AI) solutions for medical applications aiming to improve diagnosis, treatment, and prevention of diseases. While the need for transparency and reduction of bias in data and algorithm development has been addressed in past studies, little is known about the knowledge and perception of bias among AI developers. OBJECTIVE: This study's objective was to survey AI specialists in health care to investigate developers' perceptions of bias in AI algorithms for health care applications and their awareness and use of preventative measures. METHODS: A web-based survey was provided in both German and English language, comprising a maximum of 41 questions using branching logic within the REDCap web application. Only the results of participants with experience in the field of medical AI applications and complete questionnaires were included for analysis. Demographic data, technical expertise, and perceptions of fairness, as well as knowledge of biases in AI, were analyzed, and variations among gender, age, and work environment were assessed. RESULTS: A total of 151 AI specialists completed the web-based survey. The median age was 30 (IQR 26-39) years, and 67% (101/151) of respondents were male. One-third rated their AI development projects as fair (47/151, 31%) or moderately fair (51/151, 34%), 12% (18/151) reported their AI to be barely fair, and 1% (2/151) not fair at all. One participant identifying as diverse rated AI developments as barely fair, and among the 2 undefined gender participants, AI developments were rated as barely fair or moderately fair, respectively. Reasons for biases selected by respondents were lack of fair data (90/132, 68%), guidelines or recommendations (65/132, 49%), or knowledge (60/132, 45%). Half of the respondents worked with image data (83/151, 55%) from 1 center only (76/151, 50%), and 35% (53/151) worked with national data exclusively. CONCLUSIONS: This study shows that the perception of biases in AI overall is moderately fair. Gender minorities did not once rate their AI development as fair or very fair. Therefore, further studies need to focus on minorities and women and their perceptions of AI. The results highlight the need to strengthen knowledge about bias in AI and provide guidelines on preventing biases in AI health care applications.",
      "authors": "Vorisek Carina Nina; Stellmach Caroline; Mayer Paula Josephine; Klopfenstein Sophie Anne Ines; Bures Dominik Martin; Diehl Anke; Henningsen Maike; Ritter Kerstin; Thun Sylvia",
      "year": "2023",
      "journal": "Journal of medical Internet research",
      "doi": "10.2196/41089",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37347528/",
      "mesh_terms": "Humans; Female; Male; Adult; Artificial Intelligence; Algorithms; Bias; Delivery of Health Care; Internet",
      "keywords": "AI; FAIR data; age; application; artificial intelligence; bias; clinical; deep learning; development; diagnosis; digital health; disease; gender; health care; machine learning; online; prevention; survey; treatment",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC10337406",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36942183",
      "title": "Racial Equity in Healthcare Machine Learning: Illustrating Bias in Models With Minimal Bias Mitigation.",
      "abstract": "Background and objective While the potential of\u00a0machine learning (ML) in healthcare to positively impact human health\u00a0continues to grow, the potential for inequity in these methods must be assessed. In this study, we aimed to evaluate the presence of racial bias when five of the most common ML algorithms are used to create models with minimal processing to reduce racial bias. Methods By utilizing a CDC public database, we constructed models for the prediction of healthcare access (binary variable). Using area under the curve (AUC) as our performance metric, we calculated race-specific performance comparisons for each ML algorithm. We bootstrapped our entire analysis 20 times to produce confidence intervals\u00a0for our AUC performance metrics. Results With the exception of only a few cases, we found that the performance for the White group was, in general, significantly higher than that of the other racial groups across all ML algorithms. Additionally, we found that the most accurate algorithm in our modeling was Extreme Gradient Boosting (XGBoost) followed by random forest, naive Bayes, support vector machine (SVM), and k-nearest neighbors (KNN). Conclusion Our study illustrates the predictive perils of incorporating minimal racial bias mitigation in ML models, resulting in predictive disparities by race. This is particularly concerning in the setting of evidence for limited bias mitigation in healthcare-related ML. There needs to be more conversation, research, and guidelines surrounding methods for racial bias assessment and mitigation in healthcare-related ML models, both those currently used and those in development.",
      "authors": "Barton Michael; Hamza Mahmoud; Guevel Borna",
      "year": "2023",
      "journal": "Cureus",
      "doi": "10.7759/cureus.35037",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36942183/",
      "mesh_terms": "",
      "keywords": "data science; health equity; healthcare technology; machine learning; racial bias",
      "pub_types": "Journal Article",
      "pmcid": "PMC10023594",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35652218",
      "title": "Artificial Intelligence and Machine Learning Technologies in Cancer Care: Addressing Disparities, Bias, and Data Diversity.",
      "abstract": "Artificial intelligence (AI) and machine learning (ML) technologies have not only tremendous potential to augment clinical decision-making and enhance quality care and precision medicine efforts, but also the potential to worsen existing health disparities without a thoughtful, transparent, and inclusive approach that includes addressing bias in their design and implementation along the cancer discovery and care continuum. We discuss applications of AI/ML tools in cancer and provide recommendations for addressing and mitigating potential bias with AI and ML technologies while promoting cancer health equity.",
      "authors": "Dankwa-Mullan Irene; Weeraratne Dilhan",
      "year": "2022",
      "journal": "Cancer discovery",
      "doi": "10.1158/2159-8290.CD-22-0373",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35652218/",
      "mesh_terms": "Artificial Intelligence; Humans; Machine Learning; Neoplasms; Precision Medicine",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC9662931",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39850650",
      "title": "Bioethics Principles in Machine Learning-Healthcare Application Design: Achieving Health Justice and Health Equity.",
      "abstract": "Health technologies featuring artificial intelligence (AI) are becoming more common. Some healthcare AIs are exhibiting bias towards underrepresented persons and populations. Although many computer scientists and healthcare professionals agree that eliminating or mitigating bias in healthcare AIs is needed, little information exists regarding how to operationalize bioethics principles like autonomy in product design and implementation. This short course is framed with a Social Determinants of Health lens and a health justice and health equity stance to support computer scientists and healthcare professionals in building and deploying ethical healthcare AI. In this short course we introduce the bioethics principle of autonomy in the context of human-centered design (Module 1) and share options for design thinking models, suggesting four activities to embed ethics principles during design (Module 2). We then discuss the importance of gaining the perspectives of diverse groups to minimize harm and support the fundamental human values of underrepresented persons in support of health equity and health justice ideals (Module 3).",
      "authors": "Fritz Roschelle L; Nguyen-Truong Connie Kim Yen; May Thomas; Wuestney Katherine; Cook Diane J",
      "year": "2024",
      "journal": "Harvard public health review (Cambridge, Mass.)",
      "doi": "10.54111/0001/aaaa1",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39850650/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC11756589",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38260285",
      "title": "Evaluating and Improving the Performance and Racial Fairness of Algorithms for GFR Estimation.",
      "abstract": "Data-driven clinical prediction algorithms are used widely by clinicians. Understanding what factors can impact the performance and fairness of data-driven algorithms is an important step towards achieving equitable healthcare. To investigate the impact of modeling choices on the algorithmic performance and fairness, we make use of a case study to build a prediction algorithm for estimating glomerular filtration rate (GFR) based on the patient's electronic health record (EHR). We compare three distinct approaches for estimating GFR: CKD-EPI equations, epidemiological models, and EHR-based models. For epidemiological models and EHR-based models, four machine learning models of varying computational complexity (i.e., linear regression, support vector machine, random forest regression, and neural network) were compared. Performance metrics included root mean squared error (RMSE), median difference, and the proportion of GFR estimates within 30% of the measured GFR value (P30). Differential performance between non-African American and African American group was used to assess algorithmic fairness with respect to race. Our study showed that the variable race had a negligible effect on error, accuracy, and differential performance. Furthermore, including more relevant clinical features (e.g., common comorbidities of chronic kidney disease) and using more complex machine learning models, namely random forest regression, significantly lowered the estimation error of GFR. However, the difference in performance between African American and non-African American patients did not decrease, where the estimation error for African American patients remained consistently higher than non-African American patients, indicating that more objective patient characteristics should be discovered and included to improve algorithm performance.",
      "authors": "Zhang Linying; Richter Lauren R; Kim Tevin; Hripcsak George",
      "year": "2024",
      "journal": "medRxiv : the preprint server for health sciences",
      "doi": "10.1101/2024.01.07.24300943",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38260285/",
      "mesh_terms": "",
      "keywords": "algorithmic fairness; electronic health record; glomerular filtration rate; machine learning; predictive modeling",
      "pub_types": "Preprint; Journal Article",
      "pmcid": "PMC10802656",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41351059",
      "title": "A fairness-aware machine learning framework for maternal health in Ghana: integrating explainability, bias mitigation, and causal inference for ethical AI deployment.",
      "abstract": "BACKGROUND: Antenatal care (ANC) uptake in Ghana remains inequitable, with socioeconomic and geographic disparities limiting progress toward universal maternal health coverage (SDG 3). We present a novel, fairness-aware machine learning framework for predicting antenatal care uptake among women in Ghana, integrating explainability, bias mitigation, and causal inference to support ethical artificial intelligence (AI) deployment in low- and middle-income countries. METHODS: Using the 2022 Ghana Demographic and Health Survey (n\u2009=\u20093,314 eligible women with a recent live birth), we applied multiple imputation by chained equations (m\u2009=\u200910), appropriate categorical encoding, and synthetic minority oversampling (SMOTE) within training folds. Four supervised models (logistic regression, random forest, XGBoost, support vector machine) underwent stratified 5\u2011fold nested cross\u2011validation with cost\u2011sensitive threshold optimization (selected probability threshold\u2009=\u20090.45). Explainability (SHAP), fairness auditing (AIF360; metrics: statistical parity difference, disparate impact, equal opportunity difference, average odds difference, theil index), preprocessing mitigation (reweighing), counterfactual explanations (DiCE), and cautious treatment effect estimation (causal forests within a double machine learning framework) were integrated. Performance metrics included accuracy, precision, recall, F1, ROC\u2011AUC, minority class PR\u2011AUC, balanced accuracy, calibration (Brier score), and decision curve net benefit. RESULTS: The optimized random forest model achieved the highest accuracy (0.68) and recall (0.84) in identifying women with inadequate ANC contacts. Calibration was strong, with a brier score of 0.158, a calibration slope of 0.97, and an intercept of \u2212\u20090.02. Fairness auditing revealed baseline disparities in model predictions across wealth, region, ethnicity, and religion, with a statistical parity difference for wealth status of 0.182 and a Disparate Impact of 1.62. Following reweighting, disparate impact improved into the fairness range (0.92; within the recommended 0.8\u20131.25 interval), and statistical parity difference reduced to \u2212\u20090.028. Counterfactual analysis indicated that education, wealth, media exposure, and health worker contacts were the most modifiable factors for improving ANC uptake. Exploratory causal inference using double machine learning suggested that improving wealth status and education could be associated with a 16% (Average Treatment Effect [ATE]\u2009=\u20090.163) and 14% (ATE\u2009=\u20090.142) increase, respectively, in the probability of adequate ANC, with greater effects observed among urban and educated subgroups. Adjusted odds ratio (AOR) analysis showed that women in the richest quintile were nearly twice as likely to receive adequate ANC (AOR\u2009=\u20091.91, 95% CI: 1.44\u20132.53; p\u2009<\u20090.001), while those in the poorest quintile had significantly lower odds (AOR\u2009=\u20090.58, 95% CI: 0.45\u20130.75; p\u2009<\u20090.001). Additional significant predictors included health insurance coverage (AOR\u2009=\u20091.74, 95% CI: 1.19\u20132.55), health worker contacts (AOR\u2009=\u20091.33, 95% CI: 1.11\u20131.58), and pregnancy intention (AOR\u2009=\u20091.54, 95% CI: 1.30\u20131.82). CONCLUSION: This integrated, fairness-aware machine learning framework suggest robust, equitable, and actionable prediction of ANC uptake among Ghanaian women. Key modifiable determinants include wealth, education, and healthcare access barriers. The framework offers a replicable, ethical blueprint for transparent and fair AI deployment in maternal health, supporting targeted interventions to advance universal access to quality care in Ghana. Policymakers and health managers can leverage these AI tools to identify high-risk women, monitor intervention impacts, and allocate resources more equitably, advancing progress toward universal access to quality maternal care in Ghana.",
      "authors": "Osborne Augustus; Usani Kobloobase",
      "year": "2025",
      "journal": "BioData mining",
      "doi": "10.1186/s13040-025-00505-1",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41351059/",
      "mesh_terms": "",
      "keywords": "Antenatal care; Causal forests; Counterfactuals; Explainable AI; Fairness; Ghana; Health equity; Machine learning; Maternal health",
      "pub_types": "Journal Article",
      "pmcid": "PMC12781815",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39479339",
      "title": "Big data and AI for gender equality in health: bias is a big challenge.",
      "abstract": "Artificial intelligence and machine learning are rapidly evolving fields that have the potential to transform women's health by improving diagnostic accuracy, personalizing treatment plans, and building predictive models of disease progression leading to preventive care. Three categories of women's health issues are discussed where machine learning can facilitate accessible, affordable, personalized, and evidence-based healthcare. In this perspective, firstly the promise of big data and machine learning applications in the context of women's health is elaborated. Despite these promises, machine learning applications are not widely adapted in clinical care due to many issues including ethical concerns, patient privacy, informed consent, algorithmic biases, data quality and availability, and education and training of health care professionals. In the medical field, discrimination against women has a long history. Machine learning implicitly carries biases in the data. Thus, despite the fact that machine learning has the potential to improve some aspects of women's health, it can also reinforce sex and gender biases. Advanced machine learning tools blindly integrated without properly understanding and correcting for socio-cultural sex and gender biased practices and policies is therefore unlikely to result in sex and gender equality in health.",
      "authors": "Joshi Anagha",
      "year": "2024",
      "journal": "Frontiers in big data",
      "doi": "10.3389/fdata.2024.1436019",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39479339/",
      "mesh_terms": "",
      "keywords": "artificial intelligence; bias; biomarkers; machine learning; sex and gender; women's health",
      "pub_types": "Journal Article",
      "pmcid": "PMC11521869",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "33090117",
      "title": "A Racially Unbiased, Machine Learning Approach to Prediction of Mortality: Algorithm Development Study.",
      "abstract": "BACKGROUND: Racial disparities in health care are well documented in the United States. As machine learning methods become more common in health care settings, it is important to ensure that these methods do not contribute to racial disparities through biased predictions or differential accuracy across racial groups. OBJECTIVE: The goal of the research was to assess a machine learning algorithm intentionally developed to minimize bias in in-hospital mortality predictions between white and nonwhite patient groups. METHODS: Bias was minimized through preprocessing of algorithm training data. We performed a retrospective analysis of electronic health record data from patients admitted to the intensive care unit (ICU) at a large academic health center between 2001 and 2012, drawing data from the Medical Information Mart for Intensive Care-III database. Patients were included if they had at least 10 hours of available measurements after ICU admission, had at least one of every measurement used for model prediction, and had recorded race/ethnicity data. Bias was assessed through the equal opportunity difference. Model performance in terms of bias and accuracy was compared with the Modified Early Warning Score (MEWS), the Simplified Acute Physiology Score II (SAPS II), and the Acute Physiologic Assessment and Chronic Health Evaluation (APACHE). RESULTS: The machine learning algorithm was found to be more accurate than all comparators, with a higher sensitivity, specificity, and area under the receiver operating characteristic. The machine learning algorithm was found to be unbiased (equal opportunity difference 0.016, P=.20). APACHE was also found to be unbiased (equal opportunity difference 0.019, P=.11), while SAPS II and MEWS were found to have significant bias (equal opportunity difference 0.038, P=.006 and equal opportunity difference 0.074, P<.001, respectively). CONCLUSIONS: This study indicates there may be significant racial bias in commonly used severity scoring systems and that machine learning algorithms may reduce bias while improving on the accuracy of these methods.",
      "authors": "Allen Angier; Mataraso Samson; Siefkas Anna; Burdick Hoyt; Braden Gregory; Dellinger R Phillip; McCoy Andrea; Pellegrini Emily; Hoffman Jana; Green-Saxena Abigail; Barnes Gina; Calvert Jacob; Das Ritankar",
      "year": "2020",
      "journal": "JMIR public health and surveillance",
      "doi": "10.2196/22400",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33090117/",
      "mesh_terms": "APACHE; Adult; Aged; Algorithms; Cohort Studies; Early Warning Score; Electronic Health Records; Female; Forecasting; Hospital Mortality; Humans; Machine Learning; Male; Middle Aged; Retrospective Studies; Simplified Acute Physiology Score",
      "keywords": "health disparities; machine learning; mortality; prediction; racial disparities",
      "pub_types": "Journal Article",
      "pmcid": "PMC7644374",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38985468",
      "title": "Fairness in Predicting Cancer Mortality Across Racial Subgroups.",
      "abstract": "IMPORTANCE: Machine learning has potential to transform cancer care by helping clinicians prioritize patients for serious illness conversations. However, models need to be evaluated for unequal performance across racial groups (ie, racial bias) so that existing racial disparities are not exacerbated. OBJECTIVE: To evaluate whether racial bias exists in a predictive machine learning model that identifies 180-day cancer mortality risk among patients with solid malignant tumors. DESIGN, SETTING, AND PARTICIPANTS: In this cohort study, a machine learning model to predict cancer mortality for patients aged 21 years or older diagnosed with cancer between January 2016 and December 2021 was developed with a random forest algorithm using retrospective data from the Mount Sinai Health System cancer registry, Social Security Death Index, and electronic health records up to the date when databases were accessed for cohort extraction (February 2022). EXPOSURE: Race category. MAIN OUTCOMES AND MEASURES: The primary outcomes were model discriminatory performance (area under the receiver operating characteristic curve [AUROC], F1 score) among each race category (Asian, Black, Native American, White, and other or unknown) and fairness metrics (equal opportunity, equalized odds, and disparate impact) among each pairwise comparison of race categories. True-positive rate ratios represented equal opportunity; both true-positive and false-positive rate ratios, equalized odds; and the percentage of predictive positive rate ratios, disparate impact. All metrics were estimated as a proportion or ratio, with variability captured through 95% CIs. The prespecified criterion for the model's clinical use was a threshold of at least 80% for fairness metrics across different racial groups to ensure the model's prediction would not be biased against any specific race. RESULTS: The test validation dataset included 43\u202f274 patients with balanced demographics. Mean (SD) age was 64.09 (14.26) years, with 49.6% older than 65 years. A total of 53.3% were female; 9.5%, Asian; 18.9%, Black; 0.1%, Native American; 52.2%, White; and 19.2%, other or unknown race; 0.1% had missing race data. A total of 88.9% of patients were alive, and 11.1% were dead. The AUROCs, F1 scores, and fairness metrics maintained reasonable concordance among the racial subgroups: the AUROCs ranged from 0.75 (95% CI, 0.72-0.78) for Asian patients and 0.75 (95% CI, 0.73-0.77) for Black patients to 0.77 (95% CI, 0.75-0.79) for patients with other or unknown race; F1 scores, from 0.32 (95% CI, 0.32-0.33) for White patients to 0.40 (95% CI, 0.39-0.42) for Black patients; equal opportunity ratios, from 0.96 (95% CI, 0.95-0.98) for Black patients compared with White patients to 1.02 (95% CI, 1.00-1.04) for Black patients compared with patients with other or unknown race; equalized odds ratios, from 0.87 (95% CI, 0.85-0.92) for Black patients compared with White patients to 1.16 (1.10-1.21) for Black patients compared with patients with other or unknown race; and disparate impact ratios, from 0.86 (95% CI, 0.82-0.89) for Black patients compared with White patients to 1.17 (95% CI, 1.12-1.22) for Black patients compared with patients with other or unknown race. CONCLUSIONS AND RELEVANCE: In this cohort study, the lack of significant variation in performance or fairness metrics indicated an absence of racial bias, suggesting that the model fairly identified cancer mortality risk across racial groups. It remains essential to consistently review the model's application in clinical settings to ensure equitable patient care.",
      "authors": "Ganta Teja; Kia Arash; Parchure Prathamesh; Wang Min-Heng; Besculides Melanie; Mazumdar Madhu; Smith Cardinale B",
      "year": "2024",
      "journal": "JAMA network open",
      "doi": "10.1001/jamanetworkopen.2024.21290",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38985468/",
      "mesh_terms": "Humans; Neoplasms; Female; Male; Middle Aged; Aged; Machine Learning; Retrospective Studies; Adult; Racial Groups; Cohort Studies; Racism",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC11238025",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38478455",
      "title": "Harvard Glaucoma Fairness: A Retinal Nerve Disease Dataset for Fairness Learning and Fair Identity Normalization.",
      "abstract": "Fairness (also known as equity interchangeably) in machine learning is important for societal well-being, but limited public datasets hinder its progress. Currently, no dedicated public medical datasets with imaging data for fairness learning are available, though underrepresented groups suffer from more health issues. To address this gap, we introduce Harvard Glaucoma Fairness (Harvard-GF), a retinal nerve disease dataset including 3,300 subjects with both 2D and 3D imaging data and balanced racial groups for glaucoma detection. Glaucoma is the leading cause of irreversible blindness globally with Blacks having doubled glaucoma prevalence than other races. We also propose a fair identity normalization (FIN) approach to equalize the feature importance between different identity groups. Our FIN approach is compared with various state-of-the-art fairness learning methods with superior performance in the racial, gender, and ethnicity fairness tasks with 2D and 3D imaging data, demonstrating the utilities of our dataset Harvard-GF for fairness learning. To facilitate fairness comparisons between different models, we propose an equity-scaled performance measure, which can be flexibly used to compare all kinds of performance metrics in the context of fairness. The dataset and code are publicly accessible via https://ophai.hms.harvard.edu/datasets/harvard-gf3300/.",
      "authors": "Luo Yan; Tian Yu; Shi Min; Pasquale Louis R; Shen Lucy Q; Zebardast Nazlee; Elze Tobias; Wang Mengyu",
      "year": "2024",
      "journal": "IEEE transactions on medical imaging",
      "doi": "10.1109/TMI.2024.3377552",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38478455/",
      "mesh_terms": "Humans; Glaucoma; Machine Learning; Male; Databases, Factual; Female; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Middle Aged; Aged",
      "keywords": "",
      "pub_types": "Journal Article; Dataset; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC11251413",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38876453",
      "title": "Assessing fairness in machine learning models: A study of racial bias using matched counterparts in mortality prediction for patients with chronic diseases.",
      "abstract": "OBJECTIVE: Existing approaches to fairness evaluation often overlook systematic differences in the social determinants of health, like demographics and socioeconomics, among comparison groups, potentially leading to inaccurate or even contradictory conclusions. This study aims to evaluate racial disparities in predicting mortality among patients with chronic diseases using a fairness detection method that considers systematic differences. METHODS: We created five datasets from Mass General Brigham's electronic health records (EHR), each focusing on a different chronic condition: congestive heart failure (CHF), chronic kidney disease (CKD), chronic obstructive pulmonary disease (COPD), chronic liver disease (CLD), and dementia. For each dataset, we developed separate machine learning models to predict 1-year mortality and examined racial disparities by comparing prediction performances between Black and White individuals. We compared racial fairness evaluation between the overall Black and White individuals versus their counterparts who were Black and matched White individuals identified by propensity score matching, where the systematic differences were mitigated. RESULTS: We identified significant differences between Black and White individuals in age, gender, marital status, education level, smoking status, health insurance type, body mass index, and Charlson comorbidity index (p-value\u00a0<\u00a00.001). When examining matched Black and White subpopulations identified through propensity score matching, significant differences between particular covariates existed. We observed weaker significance levels in the CHF cohort for insurance type (p\u00a0=\u00a00.043), in the CKD cohort for insurance type (p\u00a0=\u00a00.005) and education level (p\u00a0=\u00a00.016), and in the dementia cohort for body mass index (p\u00a0=\u00a00.041); with no significant differences for other covariates. When examining mortality prediction models across the five study cohorts, we conducted a comparison of fairness evaluations before and after mitigating systematic differences. We revealed significant differences in the CHF cohort with p-values of 0.021 and 0.001 in terms of F1 measure and Sensitivity for the AdaBoost model, and p-values of 0.014 and 0.003 in terms of F1 measure and Sensitivity for the MLP model, respectively. DISCUSSION AND CONCLUSION: This study contributes to research on fairness assessment by focusing on the examination of systematic disparities and underscores the potential for revealing racial bias in machine learning models used in clinical settings.",
      "authors": "Wang Yifei; Wang Liqin; Zhou Zhengyang; Laurentiev John; Lakin Joshua R; Zhou Li; Hong Pengyu",
      "year": "2024",
      "journal": "Journal of biomedical informatics",
      "doi": "10.1016/j.jbi.2024.104677",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38876453/",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Black or African American; Chronic Disease; Electronic Health Records; Heart Failure; Machine Learning; Pulmonary Disease, Chronic Obstructive; Racism; White; Health Status Disparities",
      "keywords": "Chronic Disease; Electronic Health Records; Fairness Analysis; Machine Learning; Mortality Prediction; Racism",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "pmcid": "PMC11272432",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "34568771",
      "title": "Quantifying representativeness in randomized clinical trials using machine learning fairness metrics.",
      "abstract": "OBJECTIVE: We help identify subpopulations underrepresented in randomized clinical trials (RCTs) cohorts with respect to national, community-based or health system target populations by formulating population representativeness of RCTs as a machine learning (ML) fairness problem, deriving new representation metrics, and deploying them in easy-to-understand interactive visualization tools. MATERIALS AND METHODS: We represent RCT cohort enrollment as random binary classification fairness problems, and then show how ML fairness metrics based on enrollment fraction can be efficiently calculated using easily computed rates of subpopulations in RCT cohorts and target populations. We propose standardized versions of these metrics and deploy them in an interactive tool to analyze 3 RCTs with respect to type 2 diabetes and hypertension target populations in the National Health and Nutrition Examination Survey. RESULTS: We demonstrate how the proposed metrics and associated statistics enable users to rapidly examine representativeness of all subpopulations in the RCT defined by a set of categorical traits (eg, gender, race, ethnicity, smoking status, and blood pressure) with respect to target populations. DISCUSSION: The normalized metrics provide an intuitive standardized scale for evaluating representation across subgroups, which may have vastly different enrollment fractions and rates in RCT study cohorts. The metrics are beneficial complements to other approaches (eg, enrollment fractions) used to identify generalizability and health equity of RCTs. CONCLUSION: By quantifying the gaps between RCT and target populations, the proposed methods can support generalizability evaluation of existing RCT cohorts. The interactive visualization tool can be readily applied to identified underrepresented subgroups with respect to any desired source or target populations.",
      "authors": "Qi Miao; Cahan Owen; Foreman Morgan A; Gruen Daniel M; Das Amar K; Bennett Kristin P",
      "year": "2021",
      "journal": "JAMIA open",
      "doi": "10.1093/jamiaopen/ooab077",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34568771/",
      "mesh_terms": "",
      "keywords": "health equity; machine learning; population representativeness; randomized clinical trials; subgroup",
      "pub_types": "Journal Article",
      "pmcid": "PMC8460438",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38782170",
      "title": "Causal fairness assessment of treatment allocation with electronic health records.",
      "abstract": "OBJECTIVE: Healthcare continues to grapple with the persistent issue of treatment disparities, sparking concerns regarding the equitable allocation of treatments in clinical practice. While various fairness metrics have emerged to assess fairness in decision-making processes, a growing focus has been on causality-based fairness concepts due to their capacity to mitigate confounding effects and reason about bias. However, the application of causal fairness notions in evaluating the fairness of clinical decision-making with electronic health record (EHR) data remains an understudied domain. This study aims to address the methodological gap in assessing causal fairness of treatment allocation with electronic health records data. In addition, we investigate the impact of social determinants of health on the assessment of causal fairness of treatment allocation. METHODS: We propose a causal fairness algorithm to assess fairness in clinical decision-making. Our algorithm accounts for the heterogeneity of patient populations and identifies potential unfairness in treatment allocation by conditioning on patients who have the same likelihood to benefit from the treatment. We apply this framework to a patient cohort with coronary artery disease derived from an EHR database to evaluate the fairness of treatment decisions. RESULTS: Our analysis reveals notable disparities in coronary artery bypass grafting (CABG) allocation among different patient groups. Women were found to be 4.4%-7.7% less likely to receive CABG than men in two out of four treatment response strata. Similarly, Black or African American patients were 5.4%-8.7% less likely to receive CABG than others in three out of four response strata. These results were similar when social determinants of health (insurance and area deprivation index) were dropped from the algorithm. These findings highlight the presence of disparities in treatment allocation among similar patients, suggesting potential unfairness in the clinical decision-making process. CONCLUSION: This study introduces a novel approach for assessing the fairness of treatment allocation in healthcare. By incorporating responses to treatment into fairness framework, our method explores the potential of quantifying fairness from a causal perspective using EHR data. Our research advances the methodological development of fairness assessment in healthcare and highlight the importance of causality in determining treatment fairness.",
      "authors": "Zhang Linying; Richter Lauren R; Wang Yixin; Ostropolets Anna; Elhadad No\u00e9mie; Blei David M; Hripcsak George",
      "year": "2024",
      "journal": "Journal of biomedical informatics",
      "doi": "10.1016/j.jbi.2024.104656",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38782170/",
      "mesh_terms": "Humans; Electronic Health Records; Algorithms; Male; Female; Clinical Decision-Making; Coronary Artery Disease; Healthcare Disparities; Middle Aged; Social Determinants of Health; Causality",
      "keywords": "Causal fairness; Electronic health record; Health equity; Machine learning; Principal fairness",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC11180553",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35396996",
      "title": "Assessing socioeconomic bias in machine learning algorithms in health care: a case study of the HOUSES index.",
      "abstract": "OBJECTIVE: Artificial intelligence (AI) models may propagate harmful biases in performance and hence negatively affect the underserved. We aimed to assess the degree to which data quality of electronic health records (EHRs) affected by inequities related to low socioeconomic status (SES), results in differential performance of AI models across SES. MATERIALS AND METHODS: This study utilized existing machine learning models for predicting asthma exacerbation in children with asthma. We compared balanced error rate (BER) against different SES levels measured by HOUsing-based SocioEconomic Status measure (HOUSES) index. As a possible mechanism for differential performance, we also compared incompleteness of EHR information relevant to asthma care by SES. RESULTS: Asthmatic children with lower SES had larger BER than those with higher SES (eg, ratio = 1.35 for HOUSES Q1 vs Q2-Q4) and had a higher proportion of missing information relevant to asthma care (eg, 41% vs 24% for missing asthma severity and 12% vs 9.8% for undiagnosed asthma despite meeting asthma criteria). DISCUSSION: Our study suggests that lower SES is associated with worse predictive model performance. It also highlights the potential role of incomplete EHR data in this differential performance and suggests a way to mitigate this bias. CONCLUSION: The HOUSES index allows AI researchers to assess bias in predictive model performance by SES. Although our case study was based on a small sample size and a single-site study, the study results highlight a potential strategy for identifying bias by using an innovative SES measure.",
      "authors": "Juhn Young J; Ryu Euijung; Wi Chung-Il; King Katherine S; Malik Momin; Romero-Brufau Santiago; Weng Chunhua; Sohn Sunghwan; Sharp Richard R; Halamka John D",
      "year": "2022",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "doi": "10.1093/jamia/ocac052",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35396996/",
      "mesh_terms": "Artificial Intelligence; Asthma; Bias; Child; Delivery of Health Care; Humans; Machine Learning; Social Class",
      "keywords": "HOUSES; algorithmic bias; artificial intelligence; electronic health records; social determinants of health",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC9196683",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41292296",
      "title": "Real-world prediction of early-onset dementia by health record data: A multi-center machine learning study.",
      "abstract": "INTRODUCTION: We aimed to develop risk and prognostic prediction tools for early-onset dementia (EOD) using health record data shared across five major international cohorts. METHODS: More than 400,000 dementia-free individuals younger than age 65 at baseline were included. Ensemble learning was used to construct the models. Cumulative incidence and Kaplan-Meier curves were used to visualize risk stratification, and subgroup analyses were conducted to evaluate potential disparities. RESULTS: The CatBoost-based risk model achieved an area under the receiver-operating characteristic curve (AUROC) of 0.814 (<70 years) and 0.892 (<65 years). The Random Survival Forest (RF) prognostic model reached 5-year AUROC of 0.656. Key predictors included age, employment status, and education. DISCUSSION: Based on health record data, this study provides practical and scalable tools for EOD risk screening and prognosis prediction, with potential for implementation in community and primary care settings. HIGHLIGHTS: We developed risk and prognostic prediction models for early-onset dementia (EOD) using indicators shared across five international cohorts. Models showed good discrimination and calibration across internal and external sets, with key predictors including age and work status confirmed by shapley additive explanation (SHAP) analysis. Subgroup analyses supported fairness across sex, age, and comorbidity groups. Our study provides accessible and cost-effective yet effective tools for the screening, prevention, and prognostic prediction of EOD in large community populations and primary care settings.",
      "authors": "Rong Xuewen; Zhang Weijin; Luan Shanjie; Feng Du; Wang Ningning; Xiao Jingyu; He Junyi; Liu Jun; Shu Liming",
      "year": "2025",
      "journal": "Alzheimer's & dementia : the journal of the Alzheimer's Association",
      "doi": "10.1002/alz.70921",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41292296/",
      "mesh_terms": "Humans; Dementia; Male; Female; Machine Learning; Prognosis; Middle Aged; Age of Onset; Risk Assessment; Risk Factors; Aged; Cohort Studies; Incidence",
      "keywords": "CHARLS; HRS; KLoSA; SHARE; UK Biobank; early\u2010onset dementia; ensemble learning; multi\u2010center; prognosis prediction; real\u2010world; risk prediction; volunteer bias",
      "pub_types": "Journal Article; Multicenter Study",
      "pmcid": "PMC12647925",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38100101",
      "title": "Guiding Principles to Address the Impact of Algorithm Bias on Racial and Ethnic Disparities in Health and Health Care.",
      "abstract": "IMPORTANCE: Health care algorithms are used for diagnosis, treatment, prognosis, risk stratification, and allocation of resources. Bias in the development and use of algorithms can lead to worse outcomes for racial and ethnic minoritized groups and other historically marginalized populations such as individuals with lower income. OBJECTIVE: To provide a conceptual framework and guiding principles for mitigating and preventing bias in health care algorithms to promote health and health care equity. EVIDENCE REVIEW: The Agency for Healthcare Research and Quality and the National Institute for Minority Health and Health Disparities convened a diverse panel of experts to review evidence, hear from stakeholders, and receive community feedback. FINDINGS: The panel developed a conceptual framework to apply guiding principles across an algorithm's life cycle, centering health and health care equity for patients and communities as the goal, within the wider context of structural racism and discrimination. Multiple stakeholders can mitigate and prevent bias at each phase of the algorithm life cycle, including problem formulation (phase 1); data selection, assessment, and management (phase 2); algorithm development, training, and validation (phase 3); deployment and integration of algorithms in intended settings (phase 4); and algorithm monitoring, maintenance, updating, or deimplementation (phase 5). Five principles should guide these efforts: (1) promote health and health care equity during all phases of the health care algorithm life cycle; (2) ensure health care algorithms and their use are transparent and explainable; (3) authentically engage patients and communities during all phases of the health care algorithm life cycle and earn trustworthiness; (4) explicitly identify health care algorithmic fairness issues and trade-offs; and (5) establish accountability for equity and fairness in outcomes from health care algorithms. CONCLUSIONS AND RELEVANCE: Multiple stakeholders must partner to create systems, processes, regulations, incentives, standards, and policies to mitigate and prevent algorithmic bias. Reforms should implement guiding principles that support promotion of health and health care equity in all phases of the algorithm life cycle as well as transparency and explainability, authentic community engagement and ethical partnerships, explicit identification of fairness issues and trade-offs, and accountability for equity and fairness.",
      "authors": "Chin Marshall H; Afsar-Manesh Nasim; Bierman Arlene S; Chang Christine; Col\u00f3n-Rodr\u00edguez Caleb J; Dullabh Prashila; Duran Deborah Guadalupe; Fair Malika; Hernandez-Boussard Tina; Hightower Maia; Jain Anjali; Jordan William B; Konya Stephen; Moore Roslyn Holliday; Moore Tamra Tyree; Rodriguez Richard; Shaheen Gauher; Snyder Lynne Page; Srinivasan Mithuna; Umscheid Craig A; Ohno-Machado Lucila",
      "year": "2023",
      "journal": "JAMA network open",
      "doi": "10.1001/jamanetworkopen.2023.45050",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38100101/",
      "mesh_terms": "United States; Humans; Health Promotion; Racial Groups; Academies and Institutes; Algorithms; Health Equity",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, P.H.S.",
      "pmcid": "PMC11181958",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "37902833",
      "title": "Architectural Design of a Blockchain-Enabled, Federated Learning Platform for Algorithmic Fairness in Predictive Health Care: Design Science Study.",
      "abstract": "BACKGROUND: Developing effective and generalizable predictive models is critical for disease prediction and clinical decision-making, often requiring diverse samples to mitigate population bias and address algorithmic fairness. However, a major challenge is to retrieve learning models across multiple institutions without bringing in local biases and inequity, while preserving individual patients' privacy at each site. OBJECTIVE: This study aims to understand the issues of bias and fairness in the machine learning process used in the predictive health care domain. We proposed a software architecture that integrates federated learning and blockchain to improve fairness, while maintaining acceptable prediction accuracy and minimizing overhead costs. METHODS: We improved existing federated learning platforms by integrating blockchain through an iterative design approach. We used the design science research method, which involves 2 design cycles (federated learning for bias mitigation and decentralized architecture). The design involves a bias-mitigation process within the blockchain-empowered federated learning framework based on a novel architecture. Under this architecture, multiple medical institutions can jointly train predictive models using their privacy-protected data effectively and efficiently and ultimately achieve fairness in decision-making in the health care domain. RESULTS: We designed and implemented our solution using the Aplos smart contract, microservices, Rahasak blockchain, and Apache Cassandra-based distributed storage. By conducting 20,000 local model training iterations and 1000 federated model training iterations across 5 simulated medical centers as peers in the Rahasak blockchain network, we demonstrated how our solution with an improved fairness mechanism can enhance the accuracy of predictive diagnosis. CONCLUSIONS: Our study identified the technical challenges of prediction biases faced by existing predictive models in the health care domain. To overcome these challenges, we presented an innovative design solution using federated learning and blockchain, along with the adoption of a unique distributed architecture for a fairness-aware system. We have illustrated how this design can address privacy, security, prediction accuracy, and scalability challenges, ultimately improving fairness and equity in the predictive health care domain.",
      "authors": "Liang Xueping; Zhao Juan; Chen Yan; Bandara Eranga; Shetty Sachin",
      "year": "2023",
      "journal": "Journal of medical Internet research",
      "doi": "10.2196/46547",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37902833/",
      "mesh_terms": "Humans; Blockchain; Hospitals; Awareness; Clinical Decision-Making; Machine Learning",
      "keywords": "bias; blockchain; fairness; federated learning; health care; implementation; privacy; proof of concept; software",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "pmcid": "PMC10644196",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "37600144",
      "title": "Translating Intersectionality to Fair Machine Learning in Health Sciences.",
      "abstract": "Fairness approaches in machine learning should involve more than assessment of performance metrics across groups. Shifting the focus away from model metrics, we reframe fairness through the lens of intersectionality, a Black feminist theoretical framework that contextualizes individuals in interacting systems of power and oppression.",
      "authors": "Lett Elle; La Cava William G",
      "year": "2023",
      "journal": "Nature machine intelligence",
      "doi": "10.1038/s42256-023-00651-3",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37600144/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC10437125",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40435158",
      "title": "Is there a competitive advantage to using multivariate statistical or machine learning methods over the Bross formula in the hdPS framework for bias and variance estimation?",
      "abstract": "PURPOSE: We aim to evaluate various proxy selection methods within the context of high-dimensional propensity score (hdPS) analysis. This study aimed to systematically evaluate and compare the performance of traditional statistical methods and machine learning approaches within the hdPS framework, focusing on key metrics such as bias, standard error (SE), and coverage, under various exposure and outcome prevalence scenarios. METHODS: We conducted a plasmode simulation study using data from the National Health and Nutrition Examination Survey (NHANES) cycles from 2013 to 2018. We compared methods including the kitchen sink model, Bross-based hdPS, Hybrid hdPS, LASSO, Elastic Net, Random Forest, XGBoost, and Genetic Algorithm (GA). The performance of each inverse probability weighted method was assessed based on bias, MSE, coverage probability, and SE estimation across three epidemiological scenarios: frequent exposure and outcome, rare exposure and frequent outcome, and frequent exposure and rare outcome. RESULTS: XGBoost consistently demonstrated strong performance in terms of MSE and coverage, making it effective for scenarios prioritizing precision. However, it exhibited higher bias, particularly in rare exposure scenarios, suggesting it is less suited when minimizing bias is critical. In contrast, GA showed significant limitations, with consistently high bias and MSE, making it the least reliable method. Bross-based hdPS, and Hybrid hdPS methods provided a balanced approach, with low bias and moderate MSE, though coverage varied depending on the scenario. Rare outcome scenarios generally resulted in lower MSE and better precision, while rare exposure scenarios were associated with higher bias and MSE. Notably, traditional statistical approaches such as forward selection and backward elimination performed comparably to more sophisticated machine learning methods in terms of bias and coverage, suggesting that these simpler approaches may be viable alternatives due to their computational efficiency. CONCLUSION: The results highlight the importance of selecting hdPS methods based on the specific characteristics of the data, such as exposure and outcome prevalence. While advanced machine learning methods such as XGBoost can enhance precision, simpler methods such as forward selection or backward elimination may offer similar performance in terms of bias and coverage with fewer computational demands. Tailoring the choice of method to the epidemiological scenario is essential for optimizing the balance between bias reduction and precision.",
      "authors": "Ehsanul Karim Mohammad; Lei Yang",
      "year": "2025",
      "journal": "PloS one",
      "doi": "10.1371/journal.pone.0324639",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40435158/",
      "mesh_terms": "Machine Learning; Humans; Bias; Propensity Score; Nutrition Surveys; Algorithms; Multivariate Analysis; Models, Statistical; Computer Simulation",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC12118903",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36576182",
      "title": "Assessing machine learning for fair prediction of ADHD in school pupils using a retrospective cohort study of linked education and healthcare data.",
      "abstract": "OBJECTIVES: Attention deficit hyperactivity disorder (ADHD) is a prevalent childhood disorder, but often goes unrecognised and untreated. To improve access to services, accurate predictions of populations at high risk of ADHD are needed for effective resource allocation. Using a unique linked health and education data resource, we examined how machine learning (ML) approaches can predict risk of ADHD. DESIGN: Retrospective population cohort study. SETTING: South London (2007-2013). PARTICIPANTS: n=56\u2009258 pupils with linked education and health data. PRIMARY OUTCOME MEASURES: Using area under the curve (AUC), we compared the predictive accuracy of four ML models and one neural network for ADHD diagnosis. Ethnic group and language biases were weighted using a fair pre-processing algorithm. RESULTS: Random forest and logistic regression prediction models provided the highest predictive accuracy for ADHD in population samples (AUC 0.86 and 0.86, respectively) and clinical samples (AUC 0.72 and 0.70). Precision-recall curve analyses were less favourable. Sociodemographic biases were effectively reduced by a fair pre-processing algorithm without loss of accuracy. CONCLUSIONS: ML approaches using linked routinely collected education and health data offer accurate, low-cost and scalable prediction models of ADHD. These approaches could help identify areas of need and inform resource allocation. Introducing 'fairness weighting' attenuates some sociodemographic biases which would otherwise underestimate ADHD risk within minority groups.",
      "authors": "Ter-Minassian Lucile; Viani Natalia; Wickersham Alice; Cross Lauren; Stewart Robert; Velupillai Sumithra; Downs Johnny",
      "year": "2022",
      "journal": "BMJ open",
      "doi": "10.1136/bmjopen-2021-058058",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36576182/",
      "mesh_terms": "Humans; Child; Attention Deficit Disorder with Hyperactivity; Retrospective Studies; Cohort Studies; Schools; Delivery of Health Care; Machine Learning",
      "keywords": "Child & adolescent psychiatry; EPIDEMIOLOGY; MENTAL HEALTH",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC9723859",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36976634",
      "title": "Predicting Social Determinants of Health in Patient Navigation: Case Study.",
      "abstract": "BACKGROUND: Patient navigation (PN) programs have demonstrated efficacy in improving health outcomes for marginalized populations across a range of clinical contexts by addressing barriers to health care, including social determinants of health (SDoHs). However, it can be challenging for navigators to identify SDoHs by asking patients directly because of many factors, including patients' reluctance to disclose information, communication barriers, and the variable resources and experience levels of patient navigators. Navigators could benefit from strategies that augment their ability to gather SDoH data. Machine learning can be leveraged as one of these strategies to identify SDoH-related barriers. This could further improve health outcomes, particularly in underserved populations. OBJECTIVE: In this formative study, we explored novel machine learning-based approaches to predict SDoHs in 2 Chicago area PN studies. In the first approach, we applied machine learning to data that include comments and interaction details between patients and navigators, whereas the second approach augmented patients' demographic information. This paper presents the results of these experiments and provides recommendations for data collection and the application of machine learning techniques more generally to the problem of predicting SDoHs. METHODS: We conducted 2 experiments to explore the feasibility of using machine learning to predict patients' SDoHs using data collected from PN research. The machine learning algorithms were trained on data collected from 2 Chicago area PN studies. In the first experiment, we compared several machine learning algorithms (logistic regression, random forest, support vector machine, artificial neural network, and Gaussian naive Bayes) to predict SDoHs from both patient demographics and navigator's encounter data over time. In the second experiment, we used multiclass classification with augmented information, such as transportation time to a hospital, to predict multiple SDoHs for each patient. RESULTS: In the first experiment, the random forest classifier achieved the highest accuracy among the classifiers tested. The overall accuracy to predict SDoHs was 71.3%. In the second experiment, multiclass classification effectively predicted a few patients' SDoHs based purely on demographic and augmented data. The best accuracy of these predictions overall was 73%. However, both experiments yielded high variability in individual SDoH predictions and correlations that become salient among SDoHs. CONCLUSIONS: To our knowledge, this study is the first approach to applying PN encounter data and multiclass learning algorithms to predict SDoHs. The experiments discussed yielded valuable lessons, including the awareness of model limitations and bias, planning for standardization of data sources and measurement, and the need to identify and anticipate the intersectionality and clustering of SDoHs. Although our focus was on predicting patients' SDoHs, machine learning can have a broad range of applications in the field of PN, from tailoring intervention delivery (eg, supporting PN decision-making) to informing resource allocation for measurement, and PN supervision.",
      "authors": "Iacobelli Francisco; Yang Anna; Tom Laura; Leung Ivy S; Crissman John; Salgado Rufino; Simon Melissa",
      "year": "2023",
      "journal": "JMIR formative research",
      "doi": "10.2196/42683",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36976634/",
      "mesh_terms": "",
      "keywords": "case study; health care disparities; health equity; machine learning; patient navigation; social determinants of health",
      "pub_types": "Journal Article",
      "pmcid": "PMC10131925",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "34383925",
      "title": "Bias and fairness assessment of a natural language processing opioid misuse classifier: detection and mitigation of electronic health record data disadvantages across racial subgroups.",
      "abstract": "OBJECTIVES: To assess fairness and bias of a previously validated machine learning opioid misuse classifier. MATERIALS & METHODS: Two experiments were conducted with the classifier's original (n\u2009=\u20091000) and external validation (n\u2009=\u200953 974) datasets from 2 health systems. Bias was assessed via testing for differences in type II error rates across racial/ethnic subgroups (Black, Hispanic/Latinx, White, Other) using bootstrapped 95% confidence intervals. A local surrogate model was estimated to interpret the classifier's predictions by race and averaged globally from the datasets. Subgroup analyses and post-hoc recalibrations were conducted to attempt to mitigate biased metrics. RESULTS: We identified bias in the false negative rate (FNR = 0.32) of the Black subgroup compared to the FNR (0.17) of the White subgroup. Top features included \"heroin\" and \"substance abuse\" across subgroups. Post-hoc recalibrations eliminated bias in FNR with minimal changes in other subgroup error metrics. The Black FNR subgroup had higher risk scores for readmission and mortality than the White FNR subgroup, and a higher mortality risk score than the Black true positive subgroup (P\u2009<\u2009.05). DISCUSSION: The Black FNR subgroup had the greatest severity of disease and risk for poor outcomes. Similar features were present between subgroups for predicting opioid misuse, but inequities were present. Post-hoc mitigation techniques mitigated bias in type II error rate without creating substantial type I error rates. From model design through deployment, bias and data disadvantages should be systematically addressed. CONCLUSION: Standardized, transparent bias assessments are needed to improve trustworthiness in clinical machine learning models.",
      "authors": "Thompson Hale M; Sharma Brihat; Bhalla Sameer; Boley Randy; McCluskey Connor; Dligach Dmitriy; Churpek Matthew M; Karnik Niranjan S; Afshar Majid",
      "year": "2021",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "doi": "10.1093/jamia/ocab148",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34383925/",
      "mesh_terms": "Electronic Health Records; Hispanic or Latino; Humans; Machine Learning; Natural Language Processing; Opioid-Related Disorders",
      "keywords": "bias and fairness; interpretability; machine learning; natural language processing; opioid use disorder; structural racism",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, P.H.S.",
      "pmcid": "PMC8510285",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39628837",
      "title": "Active learning with human heuristics: an algorithm robust to labeling bias.",
      "abstract": "Active learning enables prediction models to achieve better performance faster by adaptively querying an oracle for the labels of data points. Sometimes the oracle is a human, for example when a medical diagnosis is provided by a doctor. According to the behavioral sciences, people, because they employ heuristics, might sometimes exhibit biases in labeling. How does modeling the oracle as a human heuristic affect the performance of active learning algorithms? If there is a drop in performance, can one design active learning algorithms robust to labeling bias? The present article provides answers. We investigate two established human heuristics (fast-and-frugal tree, tallying model) combined with four active learning algorithms (entropy sampling, multi-view learning, conventional information density, and, our proposal, inverse information density) and three standard classifiers (logistic regression, random forests, support vector machines), and apply their combinations to 15 datasets where people routinely provide labels, such as health and other domains like marketing and transportation. There are two main results. First, we show that if a heuristic provides labels, the performance of active learning algorithms significantly drops, sometimes below random. Hence, it is key to design active learning algorithms that are robust to labeling bias. Our second contribution is to provide such a robust algorithm. The proposed inverse information density algorithm, which is inspired by human psychology, achieves an overall improvement of 87% over the best of the other algorithms. In conclusion, designing and benchmarking active learning algorithms can benefit from incorporating the modeling of human heuristics.",
      "authors": "Ravichandran Sriram; Sudarsanam Nandan; Ravindran Balaraman; Katsikopoulos Konstantinos V",
      "year": "2024",
      "journal": "Frontiers in artificial intelligence",
      "doi": "10.3389/frai.2024.1491932",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39628837/",
      "mesh_terms": "",
      "keywords": "active learning; biases; fast-and-frugal heuristics; human behavior; human in the loop; robustness",
      "pub_types": "Journal Article",
      "pmcid": "PMC11611880",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36378761",
      "title": "Improving Fairness in the Prediction of Heart Failure Length of Stay and Mortality by Integrating Social Determinants of Health.",
      "abstract": "BACKGROUND: Machine learning (ML) approaches have been broadly applied to the prediction of length of stay and mortality in hospitalized patients. ML may also reduce societal health burdens, assist in health resources planning and improve health outcomes. However, the fairness of these ML models across ethnoracial or socioeconomic subgroups is rarely assessed or discussed. In this study, we aim (1) to quantify the algorithmic bias of ML models when predicting the probability of long-term hospitalization or in-hospital mortality for different heart failure (HF) subpopulations, and (2) to propose a novel method that can improve the fairness of our models without compromising predictive power. METHODS: We built 5 ML classifiers to predict the composite outcome of hospitalization length-of-stay and in-hospital mortality for 210\u2009368 HF patients extracted from the Get With The Guidelines-Heart Failure registry data set. We integrated 15 social determinants of health variables, including the Social Deprivation Index and the Area Deprivation Index, into the feature space of ML models based on patients' geographies to mitigate the algorithmic bias. RESULTS: The best-performing random forest model demonstrated modest predictive power but selectively underdiagnosed underserved subpopulations, for example, female, Black, and socioeconomically disadvantaged patients. The integration of social determinants of health variables can significantly improve fairness without compromising model performance. CONCLUSIONS: We quantified algorithmic bias against underserved subpopulations in the prediction of the composite outcome for HF patients. We provide a potential direction to reduce disparities of ML-based predictive models by integrating social determinants of health variables. We urge fellow researchers to strongly consider ML fairness when developing predictive models for HF patients.",
      "authors": "Li Yikuan; Wang Hanyin; Luo Yuan",
      "year": "2022",
      "journal": "Circulation. Heart failure",
      "doi": "10.1161/CIRCHEARTFAILURE.122.009473",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36378761/",
      "mesh_terms": "Humans; Female; Heart Failure; Length of Stay; Social Determinants of Health; Hospitalization; Hospital Mortality",
      "keywords": "bias; healthcare disparities; heart failure; machine learning; social determinants of health",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "pmcid": "PMC9673161",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38241250",
      "title": "A scientometric analysis of fairness in health AI literature.",
      "abstract": "Artificial intelligence (AI) and machine learning are central components of today's medical environment. The fairness of AI, i.e. the ability of AI to be free from bias, has repeatedly come into question. This study investigates the diversity of members of academia whose scholarship poses questions about the fairness of AI. The articles that combine the topics of fairness, artificial intelligence, and medicine were selected from Pubmed, Google Scholar, and Embase using keywords. Eligibility and data extraction from the articles were done manually and cross-checked by another author for accuracy. Articles were selected for further analysis, cleaned, and organized in Microsoft Excel; spatial diagrams were generated using Public Tableau. Additional graphs were generated using Matplotlib and Seaborn. Linear and logistic regressions were conducted using Python to measure the relationship between funding status, number of citations, and the gender demographics of the authorship team. We identified 375 eligible publications, including research and review articles concerning AI and fairness in healthcare. Analysis of the bibliographic data revealed that there is an overrepresentation of authors that are white, male, and are from high-income countries, especially in the roles of first and last author. Additionally, analysis showed that papers whose authors are based in higher-income countries were more likely to be cited more often and published in higher impact journals. These findings highlight the lack of diversity among the authors in the AI fairness community whose work gains the largest readership, potentially compromising the very impartiality that the AI fairness community is working towards.",
      "authors": "Alberto Isabelle Rose I; Alberto Nicole Rose I; Altinel Yuksel; Blacker Sarah; Binotti William Warr; Celi Leo Anthony; Chua Tiffany; Fiske Amelia; Griffin Molly; Karaca Gulce; Mokolo Nkiruka; Naawu David Kojo N; Patscheider Jonathan; Petushkov Anton; Quion Justin Michael; Senteio Charles; Taisbak Simon; T\u0131rnova \u0130smail; Tokashiki Harumi; Velasquez Adrian; Yaghy Antonio; Yap Keagan",
      "year": "2024",
      "journal": "PLOS global public health",
      "doi": "10.1371/journal.pgph.0002513",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38241250/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC10798451",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "37252970",
      "title": "Cohort bias in predictive risk assessments of future criminal justice system involvement.",
      "abstract": "Risk assessment instruments (RAIs) are widely used to aid high-stakes decision-making in criminal justice settings and other areas such as health care and child welfare. These tools, whether using machine learning or simpler algorithms, typically assume a time-invariant relationship between predictors and outcome. Because societies are themselves changing and not just individuals, this assumption may be violated in many behavioral settings, generating what we call cohort bias. Analyzing criminal histories in a cohort-sequential longitudinal study of children, we demonstrate that regardless of model type or predictor sets, a tool trained to predict the likelihood of arrest between the ages of 17 and 24 y on older birth cohorts systematically overpredicts the likelihood of arrest for younger birth cohorts over the period 1995 to 2020. Cohort bias is found for both relative and absolute risks, and it persists for all racial groups and within groups at highest risk for arrest. The results imply that cohort bias is an underappreciated mechanism generating inequality in contacts with the criminal legal system that is distinct from racial bias. Cohort bias is a challenge not only for predictive instruments with respect to crime and justice, but also for RAIs more broadly.",
      "authors": "Montana Erika; Nagin Daniel S; Neil Roland; Sampson Robert J",
      "year": "2023",
      "journal": "Proceedings of the National Academy of Sciences of the United States of America",
      "doi": "10.1073/pnas.2301990120",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37252970/",
      "mesh_terms": "Child; Humans; Adolescent; Young Adult; Adult; Longitudinal Studies; Criminal Law; Crime; Cohort Studies; Risk Assessment",
      "keywords": "bias; cohort; criminal justice; risk assessment; social change",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "pmcid": "PMC10265989",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "37266959",
      "title": "Awareness of Racial and Ethnic Bias and Potential Solutions to Address Bias With Use of Health Care Algorithms.",
      "abstract": "IMPORTANCE: Algorithms are commonly incorporated into health care decision tools used by health systems and payers and thus affect quality of care, access, and health outcomes. Some algorithms include a patient's race or ethnicity among their inputs and can lead clinicians and decision-makers to make choices that vary by race and potentially affect inequities. OBJECTIVE: To inform an evidence review on the use of race- and ethnicity-based algorithms in health care by gathering public and stakeholder perspectives about the repercussions of and efforts to address algorithm-related bias. DESIGN, SETTING, AND PARTICIPANTS: Qualitative methods were used to analyze responses. Responses were initially open coded and then consolidated to create a codebook, with themes and subthemes identified and finalized by consensus. This qualitative study was conducted from May 4, 2021, through December 7, 2022. Forty-two organization representatives (eg, clinical professional societies, universities, government agencies, payers, and health technology organizations) and individuals responded to the request for information. MAIN OUTCOMES AND MEASURES: Identification of algorithms with the potential for race- and ethnicity-based biases and qualitative themes. RESULTS: Forty-two respondents identified 18 algorithms currently in use with the potential for bias, including, for example, the Simple Calculated Osteoporosis Risk Estimation risk prediction tool and the risk calculator for vaginal birth after cesarean section. The 7 qualitative themes, with 31 subthemes, included the following: (1) algorithms are in widespread use and have significant repercussions, (2) bias can result from algorithms whether or not they explicitly include race, (3) clinicians and patients are often unaware of the use of algorithms and potential for bias, (4) race is a social construct used as a proxy for clinical variables, (5) there is a lack of standardization in how race and social determinants of health are collected and defined, (6) bias can be introduced at all stages of algorithm development, and (7) algorithms should be discussed as part of shared decision-making between the patient and clinician. CONCLUSIONS AND RELEVANCE: This qualitative study found that participants perceived widespread and increasing use of algorithms in health care and lack of oversight, potentially exacerbating racial and ethnic inequities. Increasing awareness for clinicians and patients and standardized, transparent approaches for algorithm development and implementation may be needed to address racial and ethnic biases related to algorithms.",
      "authors": "Jain Anjali; Brooks Jasmin R; Alford Cleothia C; Chang Christine S; Mueller Nora M; Umscheid Craig A; Bierman Arlene S",
      "year": "2023",
      "journal": "JAMA health forum",
      "doi": "10.1001/jamahealthforum.2023.1197",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37266959/",
      "mesh_terms": "Pregnancy; Humans; Female; Cesarean Section; Delivery of Health Care; Ethnicity; Health Facilities; Bias",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC10238944",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38875540",
      "title": "Developing Ethics and Equity Principles, Terms, and Engagement Tools to Advance Health Equity and Researcher Diversity in AI and Machine Learning: Modified Delphi Approach.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) and machine learning (ML) technology design and development continues to be rapid, despite major limitations in its current form as a practice and discipline to address all sociohumanitarian issues and complexities. From these limitations emerges an imperative to strengthen AI and ML literacy in underserved communities and build a more diverse AI and ML design and development workforce engaged in health research. OBJECTIVE: AI and ML has the potential to account for and assess a variety of factors that contribute to health and disease and to improve prevention, diagnosis, and therapy. Here, we describe recent activities within the Artificial Intelligence/Machine Learning Consortium to Advance Health Equity and Researcher Diversity (AIM-AHEAD) Ethics and Equity Workgroup (EEWG) that led to the development of deliverables that will help put ethics and fairness at the forefront of AI and ML applications to build equity in biomedical research, education, and health care. METHODS: The AIM-AHEAD EEWG was created in 2021 with 3 cochairs and 51 members in year 1 and 2 cochairs and ~40 members in year 2. Members in both years included AIM-AHEAD principal investigators, coinvestigators, leadership fellows, and research fellows. The EEWG used a modified Delphi approach using polling, ranking, and other exercises to facilitate discussions around tangible steps, key terms, and definitions needed to ensure that ethics and fairness are at the forefront of AI and ML applications to build equity in biomedical research, education, and health care. RESULTS: The EEWG developed a set of ethics and equity principles, a glossary, and an interview guide. The ethics and equity principles comprise 5 core principles, each with subparts, which articulate best practices for working with stakeholders from historically and presently underrepresented communities. The glossary contains 12 terms and definitions, with particular emphasis on optimal development, refinement, and implementation of AI and ML in health equity research. To accompany the glossary, the EEWG developed a concept relationship diagram that describes the logical flow of and relationship between the definitional concepts. Lastly, the interview guide provides questions that can be used or adapted to garner stakeholder and community perspectives on the principles and glossary. CONCLUSIONS: Ongoing engagement is needed around our principles and glossary to identify and predict potential limitations in their uses in AI and ML research settings, especially for institutions with limited resources. This requires time, careful consideration, and honest discussions around what classifies an engagement incentive as meaningful to support and sustain their full engagement. By slowing down to meet historically and presently underresourced institutions and communities where they are and where they are capable of engaging and competing, there is higher potential to achieve needed diversity, ethics, and equity in AI and ML implementation in health research.",
      "authors": "Hendricks-Sturrup Rachele; Simmons Malaika; Anders Shilo; Aneni Kammarauche; Wright Clayton Ellen; Coco Joseph; Collins Benjamin; Heitman Elizabeth; Hussain Sajid; Joshi Karuna; Lemieux Josh; Lovett Novak Laurie; Rubin Daniel J; Shanker Anil; Washington Talitha; Waters Gabriella; Webb Harris Joyce; Yin Rui; Wagner Teresa; Yin Zhijun; Malin Bradley",
      "year": "2023",
      "journal": "JMIR AI",
      "doi": "10.2196/52888",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38875540/",
      "mesh_terms": "",
      "keywords": "AI; Delphi; ML; artificial intelligence; disparities; disparity; engagement; equitable; equities; equity; ethic; ethical; ethics; fair; fairness; health disparities; health equity; humanitarian; machine learning",
      "pub_types": "Journal Article",
      "pmcid": "PMC11041493",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38929638",
      "title": "The Sociodemographic Biases in Machine Learning Algorithms: A Biomedical Informatics Perspective.",
      "abstract": "Artificial intelligence models represented in machine learning algorithms are promising tools for risk assessment used to guide clinical and other health care decisions. Machine learning algorithms, however, may house biases that propagate stereotypes, inequities, and discrimination that contribute to socioeconomic health care disparities. The biases include those related to some sociodemographic characteristics such as race, ethnicity, gender, age, insurance, and socioeconomic status from the use of erroneous electronic health record data. Additionally, there is concern that training data and algorithmic biases in large language models pose potential drawbacks. These biases affect the lives and livelihoods of a significant percentage of the population in the United States and globally. The social and economic consequences of the associated backlash cannot be underestimated. Here, we outline some of the sociodemographic, training data, and algorithmic biases that undermine sound health care risk assessment and medical decision-making that should be addressed in the health care system. We present a perspective and overview of these biases by gender, race, ethnicity, age, historically marginalized communities, algorithmic bias, biased evaluations, implicit bias, selection/sampling bias, socioeconomic status biases, biased data distributions, cultural biases and insurance status bias, conformation bias, information bias and anchoring biases and make recommendations to improve large language model training data, including de-biasing techniques such as counterfactual role-reversed sentences during knowledge distillation, fine-tuning, prefix attachment at training time, the use of toxicity classifiers, retrieval augmented generation and algorithmic modification to mitigate the biases moving forward.",
      "authors": "Franklin Gillian; Stephens Rachel; Piracha Muhammad; Tiosano Shmuel; Lehouillier Frank; Koppel Ross; Elkin Peter L",
      "year": "2024",
      "journal": "Life (Basel, Switzerland)",
      "doi": "10.3390/life14060652",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38929638/",
      "mesh_terms": "",
      "keywords": "algorithms; artificial intelligence; bias; biomedical informatics; electronic health records; health care; machine learning; models; sociodemographic",
      "pub_types": "Journal Article",
      "pmcid": "PMC11204917",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36719717",
      "title": "Black and Latinx Primary Caregiver Considerations for Developing and Implementing a Machine Learning-Based Model for Detecting Child Abuse and Neglect With Implications for Racial Bias Reduction: Qualitative Interview Study With Primary Caregivers.",
      "abstract": "BACKGROUND: Child abuse and neglect, once viewed as a social problem, is now an epidemic. Moreover, health providers agree that existing stereotypes may link racial and social class issues to child abuse. The broad adoption of electronic health records (EHRs) in clinical settings offers a new avenue for addressing this epidemic. To reduce racial bias and improve the development, implementation, and outcomes of machine learning (ML)-based models that use EHR data, it is crucial to involve marginalized members of the community in the process. OBJECTIVE: This study elicited Black and Latinx primary caregivers' viewpoints regarding child abuse and neglect while living in underserved communities to highlight considerations for designing an ML-based model for detecting child abuse and neglect in emergency departments (EDs) with implications for racial bias reduction and future interventions. METHODS: We conducted a qualitative study using in-depth interviews with 20 Black and Latinx primary caregivers whose children were cared for at a single pediatric tertiary-care ED to gain insights about child abuse and neglect and their experiences with health providers. RESULTS: Three central themes were developed in the coding process: (1) primary caregivers' perspectives on the definition of child abuse and neglect, (2) primary caregivers' experiences with health providers and medical documentation, and (3) primary caregivers' perceptions of child protective services. CONCLUSIONS: Our findings highlight essential considerations from primary caregivers for developing an ML-based model for detecting child abuse and neglect in ED settings. This includes how to define child abuse and neglect from a primary caregiver lens. Miscommunication between patients and health providers can potentially lead to a misdiagnosis, and therefore, have a negative impact on medical documentation. Additionally, the outcome and application of the ML-based models for detecting abuse and neglect may cause additional harm than expected to the community. Further research is needed to validate these findings and integrate them into creating an ML-based model.",
      "authors": "Landau Aviv Y; Blanchard Ashley; Atkins Nia; Salazar Stephanie; Cato Kenrick; Patton Desmond U; Topaz Maxim",
      "year": "2023",
      "journal": "JMIR formative research",
      "doi": "10.2196/40194",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36719717/",
      "mesh_terms": "",
      "keywords": "abuse; child; child abuse and neglect; community; development; electronic health records; epidemic; implementation; machine learning; machine learning\u2013based risk models; model; neglect; pediatric emergency departments; primary caregivers",
      "pub_types": "Journal Article",
      "pmcid": "PMC9929722",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39186324",
      "title": "Sex-Based Performance Disparities in Machine Learning Algorithms for Cardiac Disease Prediction: Exploratory Study.",
      "abstract": "BACKGROUND: The presence of bias in artificial intelligence has garnered increased attention, with inequities in algorithmic performance being exposed across the fields of criminal justice, education, and welfare services. In health care, the inequitable performance of algorithms across demographic groups may widen health inequalities. OBJECTIVE: Here, we identify and characterize bias in cardiology algorithms, looking specifically at algorithms used in the management of heart failure. METHODS: Stage 1 involved a literature search of PubMed and Web of Science for key terms relating to cardiac machine learning (ML) algorithms. Papers that built ML models to predict cardiac disease were evaluated for their focus on demographic bias in model performance, and open-source data sets were retained for our investigation. Two open-source data sets were identified: (1) the University of California Irvine Heart Failure data set and (2) the University of California Irvine Coronary Artery Disease data set. We reproduced existing algorithms that have been reported for these data sets, tested them for sex biases in algorithm performance, and assessed a range of remediation techniques for their efficacy in reducing inequities. Particular attention was paid to the false negative rate (FNR), due to the clinical significance of underdiagnosis and missed opportunities for treatment. RESULTS: In stage 1, our literature search returned 127 papers, with 60 meeting the criteria for a full review and only 3 papers highlighting sex differences in algorithm performance. In the papers that reported sex, there was a consistent underrepresentation of female patients in the data sets. No papers investigated racial or ethnic differences. In stage 2, we reproduced algorithms reported in the literature, achieving mean accuracies of 84.24% (SD 3.51%) for data set 1 and 85.72% (SD 1.75%) for data set 2 (random forest models). For data set 1, the FNR was significantly higher for female patients in 13 out of 16 experiments, meeting the threshold of statistical significance (-17.81% to -3.37%; P<.05). A smaller disparity in the false positive rate was significant for male patients in 13 out of 16 experiments (-0.48% to +9.77%; P<.05). We observed an overprediction of disease for male patients (higher false positive rate) and an underprediction of disease for female patients (higher FNR). Sex differences in feature importance suggest that feature selection needs to be demographically tailored. CONCLUSIONS: Our research exposes a significant gap in cardiac ML research, highlighting that the underperformance of algorithms for female patients has been overlooked in the published literature. Our study quantifies sex disparities in algorithmic performance and explores several sources of bias. We found an underrepresentation of female patients in the data sets used to train algorithms, identified sex biases in model error rates, and demonstrated that a series of remediation techniques were unable to address the inequities present.",
      "authors": "Straw Isabel; Rees Geraint; Nachev Parashkev",
      "year": "2024",
      "journal": "Journal of medical Internet research",
      "doi": "10.2196/46936",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39186324/",
      "mesh_terms": "Humans; Machine Learning; Female; Male; Algorithms; Heart Diseases; Sex Factors",
      "keywords": "artificial intelligence; cardiac; cardiac disease; cardiology; health care; health equity; heart failure; inequality; machine learning; management; medicine; performance; quantitative evaluation; sex",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC11384168",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "33713239",
      "title": "Towards a pragmatist dealing with algorithmic bias in medical machine learning.",
      "abstract": "Machine Learning (ML) is on the rise in medicine, promising improved diagnostic, therapeutic and prognostic clinical tools. While these technological innovations are bound to transform health care, they also bring new ethical concerns to the forefront. One particularly elusive challenge regards discriminatory algorithmic judgements based on biases inherent in the training data. A common line of reasoning distinguishes between justified differential treatments that mirror true disparities between socially salient groups, and unjustified biases which do not, leading to misdiagnosis and erroneous treatment. In the curation of training data this strategy runs into severe problems though, since distinguishing between the two can be next to impossible. We thus plead for a pragmatist dealing with algorithmic bias in healthcare environments. By recurring to a recent reformulation of William James's pragmatist understanding of truth, we recommend that, instead of aiming at a supposedly objective truth, outcome-based therapeutic usefulness should serve as the guiding principle for assessing ML applications in medicine.",
      "authors": "Starke Georg; De Clercq Eva; Elger Bernice S",
      "year": "2021",
      "journal": "Medicine, health care, and philosophy",
      "doi": "10.1007/s11019-021-10008-5",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33713239/",
      "mesh_terms": "Bias; Delivery of Health Care; Education, Medical; Humans; Machine Learning; Morals",
      "keywords": "Algorithmic bias; Artificial intelligence; Fairness; Machine learning; Philosophy of Science; Pragmatism",
      "pub_types": "Journal Article",
      "pmcid": "PMC7955212",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38681759",
      "title": "Analyzing the Impact of Personalization on Fairness in Federated Learning for Healthcare.",
      "abstract": "As machine learning (ML) usage becomes more popular in the healthcare sector, there are also increasing concerns about potential biases and risks such as privacy. One countermeasure is to use federated learning (FL) to support collaborative learning without the need for patient data sharing across different organizations. However, the inherent heterogeneity of data distributions among participating FL parties poses challenges for exploring group fairness in FL. While personalization within FL can handle performance degradation caused by data heterogeneity, its influence on group fairness is not fully investigated. Therefore, the primary focus of this study is to rigorously assess the impact of personalized FL on group fairness in the healthcare domain, offering a comprehensive understanding of how personalized FL affects group fairness in clinical outcomes. We conduct an empirical analysis using two prominent real-world Electronic Health Records (EHR) datasets, namely eICU and MIMIC-IV. Our methodology involves a thorough comparison between personalized FL and two baselines: standalone training, where models are developed independently without FL collaboration, and standard FL, which aims to learn a global model via the FedAvg algorithm. We adopt Ditto as our personalized FL approach, which enables each client in FL to develop its own personalized model through multi-task learning. Our assessment is achieved through a series of evaluations, comparing the predictive performance (i.e., AUROC and AUPRC) and fairness gaps (i.e., EOPP, EOD, and DP) of these methods. Personalized FL demonstrates superior predictive accuracy and fairness over standalone training across both datasets. Nevertheless, in comparison with standard FL, personalized FL shows improved predictive accuracy but does not consistently offer better fairness outcomes. For instance, in the 24-h in-hospital mortality prediction task, personalized FL achieves an average EOD of 27.4% across racial groups in the eICU dataset and 47.8% in MIMIC-IV. In comparison, standard FL records a better EOD of 26.2% for eICU and 42.0% for MIMIC-IV, while standalone training yields significantly worse EOD of 69.4% and 54.7% on these datasets, respectively. Our analysis reveals that personalized FL has the potential to enhance fairness in comparison to standalone training, yet it does not consistently ensure fairness improvements compared to standard FL. Our findings also show that while personalization can improve fairness for more biased hospitals (i.e., hospitals having larger fairness gaps in standalone training), it can exacerbate fairness issues for less biased ones. These insights suggest that the integration of personalized FL with additional strategic designs could be key to simultaneously boosting prediction accuracy and reducing fairness disparities. The findings and opportunities outlined in this paper can inform the research agenda for future studies, to overcome the limitations and further advance health equity research.",
      "authors": "Wang Tongnian; Zhang Kai; Cai Jiannan; Gong Yanmin; Choo Kim-Kwang Raymond; Guo Yuanxiong",
      "year": "2024",
      "journal": "Journal of healthcare informatics research",
      "doi": "10.1007/s41666-024-00164-7",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38681759/",
      "mesh_terms": "",
      "keywords": "Federated learning; Group fairness; Health disparities; Personalization; Privacy",
      "pub_types": "Journal Article",
      "pmcid": "PMC11052754",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38554069",
      "title": "The selective deployment of AI in healthcare: An ethical algorithm for algorithms.",
      "abstract": "Machine-learning algorithms have the potential to revolutionise diagnostic and prognostic tasks in health care, yet algorithmic performance levels can be materially worse for subgroups that have been underrepresented in algorithmic training data. Given this epistemic deficit, the inclusion of underrepresented groups in algorithmic processes can result in harm. Yet delaying the deployment of algorithmic systems until more equitable results can be achieved would avoidably and foreseeably lead to a significant number of unnecessary deaths in well-represented populations. Faced with this dilemma between equity and utility, we draw on two case studies involving breast cancer and melanoma to argue for the selective deployment of diagnostic and prognostic tools for some well-represented groups, even if this results in the temporary exclusion of underrepresented patients from algorithmic approaches. We argue that this approach is justifiable when the inclusion of underrepresented patients would cause them to be harmed. While the context of historic injustice poses a considerable challenge for the ethical acceptability of selective algorithmic deployment strategies, we argue that, at least for the case studies addressed in this article, the issue of historic injustice is better addressed through nonalgorithmic measures, including being transparent with patients about the nature of the current epistemic deficits, providing additional services to algorithmically excluded populations, and through urgent commitments to gather additional algorithmic training data from excluded populations, paving the way for universal algorithmic deployment that is accurate for all patient groups. These commitments should be supported by regulation and, where necessary, government funding to ensure that any delays for excluded groups are kept to the minimum. We offer an ethical algorithm for algorithms-showing when to ethically delay, expedite, or selectively deploy algorithmic systems in healthcare settings.",
      "authors": "Vandersluis Robert; Savulescu Julian",
      "year": "2024",
      "journal": "Bioethics",
      "doi": "10.1111/bioe.13281",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38554069/",
      "mesh_terms": "Humans; Algorithms; Female; Artificial Intelligence; Breast Neoplasms; Melanoma; Delivery of Health Care; Machine Learning; Social Justice; Prognosis",
      "keywords": "algorithm; artificial intelligence; bias; exclusion; machine learning; melanoma",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC7616300",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "37978250",
      "title": "Ethnic disparity in diagnosing asymptomatic bacterial vaginosis using machine learning.",
      "abstract": "While machine learning (ML) has shown great promise in medical diagnostics, a major challenge is that ML models do not always perform equally well among ethnic groups. This is alarming for women's health, as there are already existing health disparities that vary by ethnicity. Bacterial Vaginosis (BV) is a common vaginal syndrome among women of reproductive age and has clear diagnostic differences among ethnic groups. Here, we investigate the ability of four ML algorithms to diagnose BV. We determine the fairness in the prediction of asymptomatic BV using 16S rRNA sequencing data from Asian, Black, Hispanic, and white women. General purpose ML model performances vary based on ethnicity. When evaluating the metric of false positive or false negative rate, we find that models perform least effectively for Hispanic and Asian women. Models generally have the highest performance for white women and the lowest for Asian women. These findings demonstrate a need for improved methodologies to increase model fairness for predicting BV.",
      "authors": "Celeste Cameron; Ming Dion; Broce Justin; Ojo Diandra P; Drobina Emma; Louis-Jacques Adetola F; Gilbert Juan E; Fang Ruogu; Parker Ivana K",
      "year": "2023",
      "journal": "NPJ digital medicine",
      "doi": "10.1038/s41746-023-00953-1",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37978250/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC10656445",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "33220494",
      "title": "An empirical characterization of fair machine learning for clinical risk prediction.",
      "abstract": "The use of machine learning to guide clinical decision making has the potential to worsen existing health disparities. Several recent works frame the problem as that of algorithmic fairness, a framework that has attracted considerable attention and criticism. However, the appropriateness of this framework is unclear due to both ethical as well as technical considerations, the latter of which include trade-offs between measures of fairness and model performance that are not well-understood for predictive models of clinical outcomes. To inform the ongoing debate, we conduct an empirical study to characterize the impact of penalizing group fairness violations on an array of measures of model performance and group fairness. We repeat the analysis across multiple observational healthcare databases, clinical outcomes, and sensitive attributes. We find that procedures that penalize differences between the distributions of predictions across groups induce nearly-universal degradation of multiple performance metrics within groups. On examining the secondary impact of these procedures, we observe heterogeneity of the effect of these procedures on measures of fairness in calibration and ranking across experimental conditions. Beyond the reported trade-offs, we emphasize that analyses of algorithmic fairness in healthcare lack the contextual grounding and causal awareness necessary to reason about the mechanisms that lead to health disparities, as well as about the potential of algorithmic fairness methods to counteract those mechanisms. In light of these limitations, we encourage researchers building predictive models for clinical use to step outside the algorithmic fairness frame and engage critically with the broader sociotechnical context surrounding the use of machine learning in healthcare.",
      "authors": "Pfohl Stephen R; Foryciarz Agata; Shah Nigam H",
      "year": "2021",
      "journal": "Journal of biomedical informatics",
      "doi": "10.1016/j.jbi.2020.103621",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33220494/",
      "mesh_terms": "Delivery of Health Care; Empirical Research; Machine Learning",
      "keywords": "Algorithmic fairness; Bias; Clinical risk prediction",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "pmcid": "PMC7871979",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35727616",
      "title": "The Benefits of Crowdsourcing to Seed and Align an Algorithm in an mHealth Intervention for African American and Hispanic Adults: Survey Study.",
      "abstract": "BACKGROUND: The lack of publicly available and culturally relevant data sets on African American and bilingual/Spanish-speaking Hispanic adults' disease prevention and health promotion priorities presents a major challenge for researchers and developers who want to create and test personalized tools built on and aligned with those priorities. Personalization depends on prediction and performance data. A recommender system (RecSys) could predict the most culturally and personally relevant preventative health information and serve it to African American and Hispanic users via a novel smartphone app. However, early in a user's experience, a RecSys can face the \"cold start problem\" of serving untailored and irrelevant content before it learns user preferences. For underserved African American and Hispanic populations, who are consistently being served health content targeted toward the White majority, the cold start problem can become an example of algorithmic bias. To avoid this, a RecSys needs population-appropriate seed data aligned with the app's purposes. Crowdsourcing provides a means to generate population-appropriate seed data. OBJECTIVE: Our objective was to identify and test a method to address the lack of culturally specific preventative personal health data and sidestep the type of algorithmic bias inherent in a RecSys not trained in the population of focus. We did this by collecting a large amount of data quickly and at low cost from members of the population of focus, thereby generating a novel data set based on prevention-focused, population-relevant health goals. We seeded our RecSys with data collected anonymously from self-identified Hispanic and self-identified non-Hispanic African American/Black adult respondents, using Amazon Mechanical Turk (MTurk). METHODS: MTurk provided the crowdsourcing platform for a web-based survey in which respondents completed a personal profile and a health information-seeking assessment, and provided data on family health history and personal health history. Respondents then selected their top 3 health goals related to preventable health conditions, and for each goal, reviewed and rated the top 3 information returns by importance, personal utility, whether the item should be added to their personal health library, and their satisfaction with the quality of the information returned. This paper reports the article ratings because our intent was to assess the benefits of crowdsourcing to seed a RecSys. The analysis of the data from health goals will be reported in future papers. RESULTS: The MTurk crowdsourcing approach generated 985 valid responses from 485 (49%) self-identified Hispanic and 500 (51%) self-identified non-Hispanic African American adults over the course of only 64 days at a cost of US $6.74 per respondent. Respondents rated 92 unique articles to inform the RecSys. CONCLUSIONS: Researchers have options such as MTurk as a quick, low-cost means to avoid the cold start problem for algorithms and to sidestep bias and low relevance for an intended population of app users. Seeding a RecSys with responses from people like the intended users allows for the development of a digital health tool that can recommend information to users based on similar demography, health goals, and health history. This approach minimizes the potential, initial gaps in algorithm performance; allows for quicker algorithm refinement in use; and may deliver a better user experience to individuals seeking preventative health information to improve health and achieve health goals.",
      "authors": "Sehgal Neil Jay; Huang Shuo; Johnson Neil Mason; Dickerson John; Jackson Devlon; Baur Cynthia",
      "year": "2022",
      "journal": "Journal of medical Internet research",
      "doi": "10.2196/30216",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35727616/",
      "mesh_terms": "Adult; Black or African American; Algorithms; Crowdsourcing; Humans; Surveys and Questionnaires; Telemedicine",
      "keywords": "African American, Black, Latino, and Hispanic populations; MTurk; Mechanical Turk; RecSys; crowdsourcing; health information; health promotion; machine learning; mobile phone; prevention; public health informatics; recommender system",
      "pub_types": "Journal Article",
      "pmcid": "PMC9257620",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38686337",
      "title": "Explainable machine learning for predicting conversion to neurological disease: Results from 52,939 medical records.",
      "abstract": "OBJECTIVE: This study assesses the application of interpretable machine learning modeling using electronic medical record data for the prediction of conversion to neurological disease. METHODS: A retrospective dataset of Cleveland Clinic patients diagnosed with Alzheimer's disease, amyotrophic lateral sclerosis, multiple sclerosis, or Parkinson's disease, and matched controls based on age, sex, race, and ethnicity was compiled. Individualized risk prediction models were created using eXtreme Gradient Boosting for each neurological disease at four timepoints in patient history. The prediction models were assessed for transparency and fairness. RESULTS: At timepoints 0-months, 12-months, 24-months, and 60-months prior to diagnosis, Alzheimer's disease models achieved the area under the receiver operating characteristic curve on a holdout test dataset of 0.794, 0.742, 0.709, and 0.645; amyotrophic lateral sclerosis of 0.883, 0.710, 0.658, and 0.620; multiple sclerosis of 0.922, 0.877, 0.849, and 0.781; and Parkinson's disease of 0.809, 0.738, 0.700, and 0.651, respectively. CONCLUSIONS: The results demonstrate that electronic medical records contain latent information that can be used for risk stratification for neurological disorders. In particular, patient-reported outcomes, sleep assessments, falls data, additional disease diagnoses, and longitudinal changes in patient health, such as weight change, are important predictors.",
      "authors": "Felix Christina; Johnston Joshua D; Owen Kelsey; Shirima Emil; Hinds Sidney R; Mandl Kenneth D; Milinovich Alex; Alberts Jay L",
      "year": "2024",
      "journal": "Digital health",
      "doi": "10.1177/20552076241249286",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38686337/",
      "mesh_terms": "",
      "keywords": "Machine learning; disease; elderly; medicine; neurology; personalized medicine; public health",
      "pub_types": "Journal Article",
      "pmcid": "PMC11057348",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38315667",
      "title": "Assessment of differentially private synthetic data for utility and fairness in end-to-end machine learning pipelines for tabular data.",
      "abstract": "Differentially private (DP) synthetic datasets are a solution for sharing data while preserving the privacy of individual data providers. Understanding the effects of utilizing DP synthetic data in end-to-end machine learning pipelines impacts areas such as health care and humanitarian action, where data is scarce and regulated by restrictive privacy laws. In this work, we investigate the extent to which synthetic data can replace real, tabular data in machine learning pipelines and identify the most effective synthetic data generation techniques for training and evaluating machine learning models. We systematically investigate the impacts of differentially private synthetic data on downstream classification tasks from the point of view of utility as well as fairness. Our analysis is comprehensive and includes representatives of the two main types of synthetic data generation algorithms: marginal-based and GAN-based. To the best of our knowledge, our work is the first that: (i) proposes a training and evaluation framework that does not assume that real data is available for testing the utility and fairness of machine learning models trained on synthetic data; (ii) presents the most extensive analysis of synthetic dataset generation algorithms in terms of utility and fairness when used for training machine learning models; and (iii) encompasses several different definitions of fairness. Our findings demonstrate that marginal-based synthetic data generators surpass GAN-based ones regarding model training utility for tabular data. Indeed, we show that models trained using data generated by marginal-based algorithms can exhibit similar utility to models trained using real data. Our analysis also reveals that the marginal-based synthetic data generated using AIM and MWEM PGM algorithms can train models that simultaneously achieve utility and fairness characteristics close to those obtained by models trained with real data.",
      "authors": "Pereira Mayana; Kshirsagar Meghana; Mukherjee Sumit; Dodhia Rahul; Lavista Ferres Juan; de Sousa Rafael",
      "year": "2024",
      "journal": "PloS one",
      "doi": "10.1371/journal.pone.0297271",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38315667/",
      "mesh_terms": "Algorithms; Health Facilities; Interior Design and Furnishings; Knowledge; Machine Learning",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC10843030",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39682584",
      "title": "Mitigating Algorithmic Bias in AI-Driven Cardiovascular Imaging for Fairer Diagnostics.",
      "abstract": "Background/Objectives: The research addresses algorithmic bias in deep learning models for cardiovascular risk prediction, focusing on fairness across demographic and socioeconomic groups to mitigate health disparities. It integrates fairness-aware algorithms, susceptible carrier-infected-recovered (SCIR) models, and interpretability frameworks to combine fairness with actionable AI insights supported by robust segmentation and classification metrics. Methods: The research utilised quantitative 3D/4D heart magnetic resonance imaging and tabular datasets from the Cardiac Atlas Project's (CAP) open challenges to explore AI-driven methodologies for mitigating algorithmic bias in cardiac imaging. The SCIR model, known for its robustness, was adapted with the Capuchin algorithm, adversarial debiasing, Fairlearn, and post-processing with equalised odds. The robustness of the SCIR model was further demonstrated in the fairness evaluation metrics, which included demographic parity, equal opportunity difference (0.037), equalised odds difference (0.026), disparate impact (1.081), and Theil Index (0.249). For interpretability, YOLOv5, Mask R-CNN, and ResNet18 were implemented with LIME and SHAP. Bias mitigation improved disparate impact (0.80 to 0.95), reduced equal opportunity difference (0.20 to 0.05), and decreased false favourable rates for males (0.0059 to 0.0033) and females (0.0096 to 0.0064) through balanced probability adjustment. Results: The SCIR model outperformed the SIR model (recovery rate: 1.38 vs 0.83) with a -10% transmission bias impact. Parameters (\u03b2=0.5, \u03b4=0.2, \u03b3=0.15) reduced susceptible counts to 2.53\u00d710-12 and increased recovered counts to 9.98 by t=50. YOLOv5 achieved high Intersection over Union (IoU) scores (94.8%, 93.7%, 80.6% for normal, severe, and abnormal cases). Mask R-CNN showed 82.5% peak confidence, while ResNet demonstrated a 10.4% accuracy drop under noise. Performance metrics (IoU: 0.91-0.96, Dice: 0.941-0.980, Kappa: 0.95) highlighted strong predictive accuracy and reliability. Conclusions: The findings validate the effectiveness of fairness-aware algorithms in addressing cardiovascular predictive model biases. The integration of fairness and explainable AI not only promotes equitable diagnostic precision but also significantly reduces diagnostic disparities across vulnerable populations. This reduction in disparities is a key outcome of the research, enhancing clinical trust in AI-driven systems. The promising results of this study pave the way for future work that will explore scalability in real-world clinical settings and address limitations such as computational complexity in large-scale data processing.",
      "authors": "Sufian Md Abu; Alsadder Lujain; Hamzi Wahiba; Zaman Sadia; Sagar A S M Sharifuzzaman; Hamzi Boumediene",
      "year": "2024",
      "journal": "Diagnostics (Basel, Switzerland)",
      "doi": "10.3390/diagnostics14232675",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39682584/",
      "mesh_terms": "",
      "keywords": "LIME; Mask R-CNN; ResNet-18; SCIR model; SHAP; YOLOv5; adversarial debiasing; algorithmic bias; cardiovascular risk prediction; demographic fairness; fairness-aware AI; predictive analytics",
      "pub_types": "Journal Article",
      "pmcid": "PMC11640708",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35699997",
      "title": "Fairness in Mobile Phone-Based Mental Health Assessment Algorithms: Exploratory Study.",
      "abstract": "BACKGROUND: Approximately 1 in 5 American adults experience mental illness every year. Thus, mobile phone-based mental health prediction apps that use phone data and artificial intelligence techniques for mental health assessment have become increasingly important and are being rapidly developed. At the same time, multiple artificial intelligence-related technologies (eg, face recognition and search results) have recently been reported to be biased regarding age, gender, and race. This study moves this discussion to a new domain: phone-based mental health assessment algorithms. It is important to ensure that such algorithms do not contribute to gender disparities through biased predictions across gender groups. OBJECTIVE: This research aimed to analyze the susceptibility of multiple commonly used machine learning approaches for gender bias in mobile mental health assessment and explore the use of an algorithmic disparate impact remover (DIR) approach to reduce bias levels while maintaining high accuracy. METHODS: First, we performed preprocessing and model training using the data set (N=55) obtained from a previous study. Accuracy levels and differences in accuracy across genders were computed using 5 different machine learning models. We selected the random forest model, which yielded the highest accuracy, for a more detailed audit and computed multiple metrics that are commonly used for fairness in the machine learning literature. Finally, we applied the DIR approach to reduce bias in the mental health assessment algorithm. RESULTS: The highest observed accuracy for the mental health assessment was 78.57%. Although this accuracy level raises optimism, the audit based on gender revealed that the performance of the algorithm was statistically significantly different between the male and female groups (eg, difference in accuracy across genders was 15.85%; P<.001). Similar trends were obtained for other fairness metrics. This disparity in performance was found to reduce significantly after the application of the DIR approach by adapting the data used for modeling (eg, the difference in accuracy across genders was 1.66%, and the reduction is statistically significant with P<.001). CONCLUSIONS: This study grounds the need for algorithmic auditing in phone-based mental health assessment algorithms and the use of gender as a protected attribute to study fairness in such settings. Such audits and remedial steps are the building blocks for the widespread adoption of fair and accurate mental health assessment algorithms in the future.",
      "authors": "Park Jinkyung; Arunachalam Ramanathan; Silenzio Vincent; Singh Vivek K",
      "year": "2022",
      "journal": "JMIR formative research",
      "doi": "10.2196/34366",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35699997/",
      "mesh_terms": "",
      "keywords": "algorithmic bias; gender bias; health equity; health information systems; medical informatics; mental health; mobile phone",
      "pub_types": "Journal Article",
      "pmcid": "PMC9240929",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "34830566",
      "title": "Artificial Intelligence, Heuristic Biases, and the Optimization of Health Outcomes: Cautionary Optimism.",
      "abstract": "The use of artificial intelligence (AI) and machine learning (ML) in clinical care offers great promise to improve patient health outcomes and reduce health inequity across patient populations. However, inherent biases in these applications, and the subsequent potential risk of harm can limit current use. Multi-modal workflows designed to minimize these limitations in the development, implementation, and evaluation of ML systems in real-world settings are needed to improve efficacy while reducing bias and the risk of potential harms. Comprehensive consideration of rapidly evolving AI technologies and the inherent risks of bias, the expanding volume and nature of data sources, and the evolving regulatory landscapes, can contribute meaningfully to the development of AI-enhanced clinical decision making and the reduction in health inequity.",
      "authors": "Feehan Michael; Owen Leah A; McKinnon Ian M; DeAngelis Margaret M",
      "year": "2021",
      "journal": "Journal of clinical medicine",
      "doi": "10.3390/jcm10225284",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34830566/",
      "mesh_terms": "",
      "keywords": "artificial intelligence; bias; electronic health record; health outcomes; heuristics; machine learning; ophthalmology; population health",
      "pub_types": "Journal Article",
      "pmcid": "PMC8620813",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38106012",
      "title": "A Fair Individualized Polysocial Risk Score for Identifying Increased Social Risk in Type 2 Diabetes.",
      "abstract": "BACKGROUND: Racial and ethnic minority groups and individuals facing social disadvantages, which often stem from their social determinants of health (SDoH), bear a disproportionate burden of type 2 diabetes (T2D) and its complications. It is crucial to implement effective social risk management strategies at the point of care. OBJECTIVE: To develop an electronic health records (EHR)-based machine learning (ML) analytical pipeline to address unmet social needs associated with hospitalization risk in patients with T2D. METHODS: We identified real-world patients with T2D from the EHR data from University of Florida (UF) Health Integrated Data Repository (IDR), incorporating both contextual SDoH (e.g., neighborhood deprivation) and individual-level SDoH (e.g., housing instability). The 2015-2020 data were used for training and validation and 2021-2022 data for independent testing. We developed a machine learning analytic pipeline, namely individualized polysocial risk score (iPsRS), to identify high social risk associated with hospitalizations in T2D patients, along with explainable AI (XAI) and fairness optimization. RESULTS: The study cohort included 10,192 real-world patients with T2D, with a mean age of 59 years and 58% female. Of the cohort, 50% were non-Hispanic White, 39% were non-Hispanic Black, 6% were Hispanic, and 5% were other races/ethnicities. Our iPsRS, including both contextual and individual-level SDoH as input factors, achieved a C statistic of 0.72 in predicting 1-year hospitalization after fairness optimization across racial and ethnic groups. The iPsRS showed excellent utility for capturing individuals at high hospitalization risk because of SDoH, that is, the actual 1-year hospitalization rate in the top 5% of iPsRS was 28.1%, ~13 times as high as the bottom decile (2.2% for 1-year hospitalization rate). CONCLUSION: Our ML pipeline iPsRS can fairly and accurately screen for patients who have increased social risk leading to hospitalization in real word patients with T2D.",
      "authors": "Huang Yu; Guo Jingchuan; Donahoo William T; Fan Zhengkang; Lu Ying; Chen Wei-Han; Tang Huilin; Bilello Lori; Saguil Aaron A; Rosenberg Eric; Shenkman Elizabeth A; Bian Jiang",
      "year": "2023",
      "journal": "Research square",
      "doi": "10.21203/rs.3.rs-3684698/v1",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38106012/",
      "mesh_terms": "",
      "keywords": "Fairness; Machine Learning; Machine learning; Prediction; Type 2 diabetes",
      "pub_types": "Preprint; Journal Article",
      "pmcid": "PMC10723535",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39369018",
      "title": "A fair individualized polysocial risk score for identifying increased social risk in type 2 diabetes.",
      "abstract": "Racial and ethnic minorities bear a disproportionate burden of type 2 diabetes (T2D) and its complications, with social determinants of health (SDoH) recognized as key drivers of these disparities. Implementing efficient and effective social needs management strategies is crucial. We propose a machine learning analytic pipeline to calculate the individualized polysocial risk score (iPsRS), which can identify T2D patients at high social risk for hospitalization, incorporating explainable AI techniques and algorithmic fairness optimization. We use electronic health records (EHR) data from T2D patients in the University of Florida Health Integrated Data Repository, incorporating both contextual SDoH (e.g., neighborhood deprivation) and person-level SDoH (e.g., housing instability). After fairness optimization across racial and ethnic groups, the iPsRS achieved a C statistic of 0.71 in predicting 1-year hospitalization. Our iPsRS can fairly and accurately screen patients with T2D who are at increased social risk for hospitalization.",
      "authors": "Huang Yu; Guo Jingchuan; Donahoo William T; Lee Yao An; Fan Zhengkang; Lu Ying; Chen Wei-Han; Tang Huilin; Bilello Lori; Saguil Aaron A; Rosenberg Eric; Shenkman Elizabeth A; Bian Jiang",
      "year": "2024",
      "journal": "Nature communications",
      "doi": "10.1038/s41467-024-52960-9",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39369018/",
      "mesh_terms": "Adult; Aged; Female; Humans; Male; Middle Aged; Diabetes Mellitus, Type 2; Electronic Health Records; Ethnicity; Florida; Hospitalization; Machine Learning; Risk Assessment; Risk Factors; Social Determinants of Health; Racial Groups",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC11455957",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38322109",
      "title": "Accelerating health disparities research with artificial intelligence.",
      "abstract": "",
      "authors": "Green B Lee; Murphy Anastasia; Robinson Edmondo",
      "year": "2024",
      "journal": "Frontiers in digital health",
      "doi": "10.3389/fdgth.2024.1330160",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38322109/",
      "mesh_terms": "",
      "keywords": "artificial intelligence; bias; equity; health disparities; machine learning",
      "pub_types": "Journal Article",
      "pmcid": "PMC10844447",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41286153",
      "title": "Personalized fitness recommendations using machine learning for optimized national health strategy.",
      "abstract": "Rising concerns over public health and chronic disease prevalence have intensified the demand for data-driven, personalized fitness interventions. While national health programs offer general guidelines, they often lack the granularity required to address individual variability in health status, lifestyle, and demographic context. This paper presents a machine learning framework to generate personalized fitness recommendations aligned with national health goals. Leveraging population-scale data, the aim is to optimize physical activity planning while maintaining fairness and clinical relevance across demographic subgroups. The study utilizes the National Health and Nutrition Examination Survey (NHANES) dataset, integrating biometric, behavioral, and demographic features. To enhance the behavioral relevance of our predictions, we integrated supplemental variables from the Behavioral Risk Factor Surveillance System (BRFSS), capturing psychological, motivational, and environmental factors that influence physical activity adherence. After preprocessing, models were developed using XGBoost, Decision Trees, and Artificial Neural Networks. Both regression (to estimate weekly activity minutes) and classification (to assign risk groups) tasks were addressed. Performance was evaluated through MeanIoU, Dice Score, sensitivity, and specificity. Demographic fairness was assessed via subgroup residuals and fairness gap analysis. XGBoost achieved superior performance, with a MeanIoU of 0.789 and F1 scores exceeding 0.79 across all risk categories. Model consistency was observed across age, gender, and ethnicity, with fairness gaps below 0.05. Residual error analysis and risk classification confirmed high reliability and low variance. The proposed system demonstrates the feasibility of using AI to personalize fitness plans at scale. It offers a pathway to integrate precision fitness with national policy, supporting equitable and effective public health strategies.",
      "authors": "Chen Juan; Wang Yan",
      "year": "2025",
      "journal": "Scientific reports",
      "doi": "10.1038/s41598-025-25566-4",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41286153/",
      "mesh_terms": "Humans; Machine Learning; Male; Female; Middle Aged; Adult; Nutrition Surveys; Exercise; Physical Fitness; Precision Medicine; Aged; Young Adult",
      "keywords": "Machine learning; NHANES; Personalized fitness recommendations; Physical activity prediction; Public health strategy; Risk classification; XGBoost",
      "pub_types": "Journal Article",
      "pmcid": "PMC12645047",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40095575",
      "title": "Shaping the Future of Healthcare: Ethical Clinical Challenges and Pathways to Trustworthy AI.",
      "abstract": "Background/Objectives: Artificial intelligence (AI) is transforming healthcare, enabling advances in diagnostics, treatment optimization, and patient care. Yet, its integration raises ethical, regulatory, and societal challenges. Key concerns include data privacy risks, algorithmic bias, and regulatory gaps that struggle to keep pace with AI advancements. This study aims to synthesize a multidisciplinary framework for trustworthy AI in healthcare, focusing on transparency, accountability, fairness, sustainability, and global collaboration. It moves beyond high-level ethical discussions to provide actionable strategies for implementing trustworthy AI in clinical contexts. Methods: A structured literature review was conducted using PubMed, Scopus, and Web of Science. Studies were selected based on relevance to AI ethics, governance, and policy in healthcare, prioritizing peer-reviewed articles, policy analyses, case studies, and ethical guidelines from authoritative sources published within the last decade. The conceptual approach integrates perspectives from clinicians, ethicists, policymakers, and technologists, offering a holistic \"ecosystem\" view of AI. No clinical trials or patient-level interventions were conducted. Results: The analysis identifies key gaps in current AI governance and introduces the Regulatory Genome-an adaptive AI oversight framework aligned with global policy trends and Sustainable Development Goals. It introduces quantifiable trustworthiness metrics, a comparative analysis of AI categories for clinical applications, and bias mitigation strategies. Additionally, it presents interdisciplinary policy recommendations for aligning AI deployment with ethical, regulatory, and environmental sustainability goals. This study emphasizes measurable standards, multi-stakeholder engagement strategies, and global partnerships to ensure that future AI innovations meet ethical and practical healthcare needs. Conclusions: Trustworthy AI in healthcare requires more than technical advancements-it demands robust ethical safeguards, proactive regulation, and continuous collaboration. By adopting the recommended roadmap, stakeholders can foster responsible innovation, improve patient outcomes, and maintain public trust in AI-driven healthcare.",
      "authors": "Goktas Polat; Grzybowski Andrzej",
      "year": "2025",
      "journal": "Journal of clinical medicine",
      "doi": "10.3390/jcm14051605",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40095575/",
      "mesh_terms": "",
      "keywords": "artificial intelligence; bias; ethics; health policy; large language model; machine learning; natural language processing; privacy; regulation",
      "pub_types": "Journal Article",
      "pmcid": "PMC11900311",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "37463884",
      "title": "Detecting shortcut learning for fair medical AI using shortcut testing.",
      "abstract": "Machine learning (ML) holds great promise for improving healthcare, but it is critical to ensure that its use will not propagate or amplify health disparities. An important step is to characterize the (un)fairness of ML models-their tendency to perform differently across subgroups of the population-and to understand its underlying mechanisms. One potential driver of algorithmic unfairness, shortcut learning, arises when ML models base predictions on improper correlations in the training data. Diagnosing this phenomenon is difficult as sensitive attributes may be causally linked with disease. Using multitask learning, we propose a method to directly test for the presence of shortcut learning in clinical ML systems and demonstrate its application to clinical tasks in radiology and dermatology. Finally, our approach reveals instances when shortcutting is not responsible for unfairness, highlighting the need for a holistic approach to fairness mitigation in medical AI.",
      "authors": "Brown Alexander; Tomasev Nenad; Freyberg Jan; Liu Yuan; Karthikesalingam Alan; Schrouff Jessica",
      "year": "2023",
      "journal": "Nature communications",
      "doi": "10.1038/s41467-023-39902-7",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37463884/",
      "mesh_terms": "Health Facilities; Machine Learning",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC10354021",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41459572",
      "title": "Machine learning-based mortality prediction in critically ill patients with hypertension: comparative analysis, fairness, and interpretability.",
      "abstract": "BACKGROUND: Hypertension is a leading global health concern, significantly contributing to cardiovascular, cerebrovascular, and renal diseases. In critically ill patients, hypertension poses increased risks of complications and mortality. Early and accurate mortality prediction in this population is essential for timely intervention and improved outcomes. Machine learning (ML) and deep learning (DL) approaches offer promising solutions by leveraging high-dimensional electronic health record (EHR) data. OBJECTIVE: To develop and evaluate ML and DL models for predicting in-hospital mortality in hypertensive patients using the MIMIC-IV critical care dataset, and to assess the fairness and interpretability of the models. METHODS: We developed four ML models-gradient boosting machine (GBM), logistic regression, support vector machine (SVM), and random forest-and two DL models-multilayer perceptron (MLP) and long short-term memory (LSTM). A comprehensive set of features, including demographics, lab values, vital signs, comorbidities, and ICU-specific variables, were extracted or engineered. Models were trained using 5-fold cross-validation and evaluated on a separate test set. Feature importance was analyzed using SHapley Additive exPlanations (SHAP) values, and fairness was assessed using demographic parity difference (DPD) and equalized odds difference (EOD), with and without the application of debiasing techniques. RESULTS: The GBM model outperformed all other models, with an AUC-ROC score of 96.3%, accuracy of 89.4%, sensitivity of 87.8%, specificity of 90.7%, and F1 score of 89.2%. Key features contributing to mortality prediction included Glasgow Coma Scale (GCS) scores, Braden Scale scores, blood urea nitrogen, age, red cell distribution width (RDW), bicarbonate, and lactate levels. Fairness analysis revealed that models trained on the top 30 most important features demonstrated lower DPD and EOD, suggesting reduced bias. Debiasing methods improved fairness in models trained with all features but had limited effects on models using the top 30 features. CONCLUSION: ML models show strong potential for mortality prediction in critically ill hypertensive patients. Feature selection not only enhances interpretability and reduces computational complexity but may also contribute to improved model fairness. These findings support the integration of interpretable and equitable AI tools in critical care settings to assist with clinical decision-making.",
      "authors": "Zhang Shenghan; Ding Sirui; Xu Zidu; Ye Jiancheng",
      "year": "2025",
      "journal": "Frontiers in artificial intelligence",
      "doi": "10.3389/frai.2025.1686378",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41459572/",
      "mesh_terms": "",
      "keywords": "SHAP; deep learning; fairness in AI; hypertension; intensive care unit; machine learning; mortality prediction",
      "pub_types": "Journal Article",
      "pmcid": "PMC12738824",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39705068",
      "title": "Early Attrition Prediction for Web-Based Interpretation Bias Modification to Reduce Anxious Thinking: A Machine Learning Study.",
      "abstract": "BACKGROUND: Digital mental health is a promising paradigm for individualized, patient-driven health care. For example, cognitive bias modification programs that target interpretation biases (cognitive bias modification for interpretation [CBM-I]) can provide practice thinking about ambiguous situations in less threatening ways on the web without requiring a therapist. However, digital mental health interventions, including CBM-I, are often plagued with lack of sustained engagement and high attrition rates. New attrition detection and mitigation strategies are needed to improve these interventions. OBJECTIVE: This paper aims to identify participants at a high risk of dropout during the early stages of 3 web-based trials of multisession CBM-I and to investigate which self-reported and passively detected feature sets computed from the participants interacting with the intervention and assessments were most informative in making this prediction. METHODS: The participants analyzed in this paper were community adults with traits such as anxiety or negative thinking about the future (Study 1: n=252, Study 2: n=326, Study 3: n=699) who had been assigned to CBM-I conditions in 3 efficacy-effectiveness trials on our team's public research website. To identify participants at a high risk of dropout, we created 4 unique feature sets: self-reported baseline user characteristics (eg, demographics), self-reported user context and reactions to the program (eg, state affect), self-reported user clinical functioning (eg, mental health symptoms), and passively detected user behavior on the website (eg, time spent on a web page of CBM-I training exercises, time of day during which the exercises were completed, latency of completing the assessments, and type of device used). Then, we investigated the feature sets as potential predictors of which participants were at high risk of not starting the second training session of a given program using well-known machine learning algorithms. RESULTS: The extreme gradient boosting algorithm performed the best and identified participants at high risk with macro-F1-scores of .832 (Study 1 with 146 features), .770 (Study 2 with 87 features), and .917 (Study 3 with 127 features). Features involving passive detection of user behavior contributed the most to the prediction relative to other features. The mean Gini importance scores for the passive features were as follows: .033 (95% CI .019-.047) in Study 1; .029 (95% CI .023-.035) in Study 2; and .045 (95% CI .039-.051) in Study 3. However, using all features extracted from a given study led to the best predictive performance. CONCLUSIONS: These results suggest that using passive indicators of user behavior, alongside self-reported measures, can improve the accuracy of prediction of participants at a high risk of dropout early during multisession CBM-I programs. Furthermore, our analyses highlight the challenge of generalizability in digital health intervention studies and the need for more personalized attrition prevention strategies.",
      "authors": "Baee Sonia; Eberle Jeremy W; Baglione Anna N; Spears Tyler; Lewis Elijah; Wang Hongning; Funk Daniel H; Teachman Bethany; E Barnes Laura",
      "year": "2024",
      "journal": "JMIR mental health",
      "doi": "10.2196/51567",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39705068/",
      "mesh_terms": "Humans; Machine Learning; Female; Male; Adult; Anxiety; Middle Aged; Patient Dropouts; Cognitive Behavioral Therapy; Internet-Based Intervention; Internet; Young Adult; Thinking",
      "keywords": "CBM-I; attrition prediction; cognitive bias modification; digital mental health intervention; dropout rate; personalization; user engagement",
      "pub_types": "Journal Article",
      "pmcid": "PMC11699492",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40616933",
      "title": "Evaluating the Performance and Potential Bias of Predictive Models for Detection of Transthyretin Cardiac Amyloidosis.",
      "abstract": "BACKGROUND: Delays in the diagnosis of transthyretin amyloid cardiomyopathy (ATTR-CM) contribute to the significant morbidity of the condition, especially in the era of disease-modifying therapies. Screening for ATTR-CM with artificial intelligence and other algorithms may improve timely diagnosis, but these algorithms have not been directly compared. OBJECTIVES: The aim of this study was to compare the performance of 4 algorithms for ATTR-CM detection in a heart failure population and assess the risk for harms due to model bias. METHODS: We identified patients in an integrated health system from 2010 to 2022 with ATTR-CM and age- and sex-matched them to controls with heart failure to target 5% prevalence. We compared the performance of a claims-based random forest model (Huda et al model), a regression-based score (Mayo ATTR-CM), and 2 deep learning echo models (EchoNet-LVH and EchoGo Amyloidosis). We evaluated for bias using standard fairness metrics. RESULTS: The analytical cohort included 176 confirmed cases of ATTR-CM and 3,192 control patients with 79.2% self-identified as White and 9.0% as Black. The Huda et al model performed poorly (AUC: 0.49). Both deep learning echo models had a higher AUC when compared to the Mayo ATTR-CM Score (EchoNet-LVH 0.88; EchoGo Amyloidosis 0.92; Mayo ATTR-CM Score 0.79; DeLong P < 0.001 for both). Bias auditing met fairness criteria for equal opportunity among patients who identified as Black. CONCLUSIONS: Deep learning, echo-based models to detect ATTR-CM demonstrated best overall discrimination when compared to 2 other models in external validation with low risk of harms due to racial bias.",
      "authors": "Hourmozdi Jonathan; Easton Nicholas; Benigeri Simon; Thomas James D; Narang Akhil; Ouyang David; Duffy Grant; Upton Ross; Hawkes Will; Akerman Ashley; Okwuosa Ike; Kline Adrienne; Kho Abel N; Luo Yuan; Shah Sanjiv J; Ahmad Faraz S",
      "year": "2025",
      "journal": "JACC. Advances",
      "doi": "10.1016/j.jacadv.2025.101901",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40616933/",
      "mesh_terms": "",
      "keywords": "amyloidosis; artificial intelligence; health care disparities; heart failure; machine learning",
      "pub_types": "Journal Article",
      "pmcid": "PMC12272418",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38160296",
      "title": "Quantifying Health Outcome Disparity in Invasive Methicillin-Resistant Staphylococcus aureus Infection using Fairness Algorithms on Real-World Data.",
      "abstract": "This study quantifies health outcome disparities in invasive Methicillin-Resistant Staphylococcus aureus (MRSA) infections by leveraging a novel artificial intelligence (AI) fairness algorithm, the Fairness-Aware Causal paThs (FACTS) decomposition, and applying it to real-world electronic health record (EHR) data. We spatiotemporally linked 9 years of EHRs from a large healthcare provider in Florida, USA, with contextual social determinants of health (SDoH). We first created a causal structure graph connecting SDoH with individual clinical measurements before/upon diagnosis of invasive MRSA infection, treatments, side effects, and outcomes; then, we applied FACTS to quantify outcome potential disparities of different causal pathways including SDoH, clinical and demographic variables. We found moderate disparity with respect to demographics and SDoH, and all the top ranked pathways that led to outcome disparities in age, gender, race, and income, included comorbidity. Prior kidney impairment, vancomycin use, and timing were associated with racial disparity, while income, rurality, and available healthcare facilities contributed to gender disparity. From an intervention standpoint, our results highlight the necessity of devising policies that consider both clinical factors and SDoH. In conclusion, this work demonstrates a practical utility of fairness AI methods in public health settings.",
      "authors": "Jun Inyoung; Ser Sarah E; Cohen Scott A; Xu Jie; Lucero Robert J; Bian Jiang; Prosperi Mattia",
      "year": "2024",
      "journal": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing",
      "doi": "10.15620/cdc:82532",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38160296/",
      "mesh_terms": "Humans; Methicillin-Resistant Staphylococcus aureus; Staphylococcal Infections; Artificial Intelligence; Community-Acquired Infections; Computational Biology; Algorithms; Outcome Assessment, Health Care; Anti-Bacterial Agents",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC10795837",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "33484133",
      "title": "The risk of racial bias while tracking influenza-related content on social media using machine learning.",
      "abstract": "OBJECTIVE: Machine learning is used to understand and track influenza-related content on social media. Because these systems are used at scale, they have the potential to adversely impact the people they are built to help. In this study, we explore the biases of different machine learning methods for the specific task of detecting influenza-related content. We compare the performance of each model on tweets written in Standard American English (SAE) vs African American English (AAE). MATERIALS AND METHODS: Two influenza-related datasets are used to train 3 text classification models (support vector machine, convolutional neural network, bidirectional long short-term memory) with different feature sets. The datasets match real-world scenarios in which there is a large imbalance between SAE and AAE examples. The number of AAE examples for each class ranges from 2% to 5% in both datasets. We also evaluate each model's performance using a balanced dataset via undersampling. RESULTS: We find that all of the tested machine learning methods are biased on both datasets. The difference in false positive rates between SAE and AAE examples ranges from 0.01 to 0.35. The difference in the false negative rates ranges from 0.01 to 0.23. We also find that the neural network methods generally has more unfair results than the linear support vector machine on the chosen datasets. CONCLUSIONS: The models that result in the most unfair predictions may vary from dataset to dataset. Practitioners should be aware of the potential harms related to applying machine learning to health-related social media data. At a minimum, we recommend evaluating fairness along with traditional evaluation metrics.",
      "authors": "Lwowski Brandon; Rios Anthony",
      "year": "2021",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "doi": "10.1093/jamia/ocaa326",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33484133/",
      "mesh_terms": "Black or African American; Datasets as Topic; Humans; Influenza Vaccines; Influenza, Human; Machine Learning; Neural Networks, Computer; Racism; Social Media; Support Vector Machine",
      "keywords": "classification; deep learning; fairness; machine learning; social network",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC7973478",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41141904",
      "title": "Assessment of demographic bias in retinal age prediction machine learning models.",
      "abstract": "The retinal age gap, defined as the difference between the predicted retinal age and chronological age, is an emerging biomarker for many eye conditions and even non-ocular diseases. Machine learning (ML) models are commonly used for retinal age prediction. However, biases in ML models may lead to unfair predictions for some demographic groups, potentially exacerbating health disparities. This retrospective cross-sectional study evaluated demographic biases related to sex and ethnicity in retinal age prediction models using retinal imaging data (color fundus photography [CFP], optical coherence tomography [OCT], and combined CFP\u202f+\u202fOCT) from 9,668 healthy individuals (mean age 56.8\u202fyears; 52% female) in the UK Biobank. The RETFound foundation model was fine-tuned to predict retinal age, and bias was assessed by comparing mean absolute error (MAE) and retinal age gaps across demographic groups. The combined CFP\u202f+\u202fOCT model achieved the lowest MAE (3.01\u202fyears), outperforming CFP-only (3.40\u202fyears) and OCT-only (4.37\u202fyears) models. Significant sex differences were observed only in the CFP model (p\u202f<\u202f0.001), while significant ethnicity differences appeared only in the OCT model (p\u202f<\u202f0.001). No significant sex/ethnicity differences were observed in the combined model. These results demonstrate that retinal age prediction models can exhibit biases, and that these biases, along with model accuracy, are influenced by the choice of imaging modality (CFP, OCT, or combined). Identifying and addressing sources of bias is essential for safe and reliable clinical implementation. Our results emphasize the importance of comprehensive bias assessments and prospective validation, ensuring that advances in machine learning and artificial intelligence benefit all patient populations.",
      "authors": "Nielsen Christopher; Stanley Emma A M; Wilms Matthias; Forkert Nils D",
      "year": "2025",
      "journal": "Frontiers in artificial intelligence",
      "doi": "10.3389/frai.2025.1653153",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41141904/",
      "mesh_terms": "",
      "keywords": "bias; machine learning; multimodal imaging; retinal age prediction; retinal imaging",
      "pub_types": "Journal Article",
      "pmcid": "PMC12549555",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38723025",
      "title": "Development and preliminary testing of Health Equity Across the AI Lifecycle (HEAAL): A framework for healthcare delivery organizations to mitigate the risk of AI solutions worsening health inequities.",
      "abstract": "The use of data-driven technologies such as Artificial Intelligence (AI) and Machine Learning (ML) is growing in healthcare. However, the proliferation of healthcare AI tools has outpaced regulatory frameworks, accountability measures, and governance standards to ensure safe, effective, and equitable use. To address these gaps and tackle a common challenge faced by healthcare delivery organizations, a case-based workshop was organized, and a framework was developed to evaluate the potential impact of implementing an AI solution on health equity. The Health Equity Across the AI Lifecycle (HEAAL) is co-designed with extensive engagement of clinical, operational, technical, and regulatory leaders across healthcare delivery organizations and ecosystem partners in the US. It assesses 5 equity assessment domains-accountability, fairness, fitness for purpose, reliability and validity, and transparency-across the span of eight key decision points in the AI adoption lifecycle. It is a process-oriented framework containing 37 step-by-step procedures for evaluating an existing AI solution and 34 procedures for evaluating a new AI solution in total. Within each procedure, it identifies relevant key stakeholders and data sources used to conduct the procedure. HEAAL guides how healthcare delivery organizations may mitigate the potential risk of AI solutions worsening health inequities. It also informs how much resources and support are required to assess the potential impact of AI solutions on health inequities.",
      "authors": "Kim Jee Young; Hasan Alifia; Kellogg Katherine C; Ratliff William; Murray Sara G; Suresh Harini; Valladares Alexandra; Shaw Keo; Tobey Danny; Vidal David E; Lifson Mark A; Patel Manesh; Raji Inioluwa Deborah; Gao Michael; Knechtle William; Tang Linda; Balu Suresh; Sendak Mark P",
      "year": "2024",
      "journal": "PLOS digital health",
      "doi": "10.1371/journal.pdig.0000390",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38723025/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC11081364",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39671594",
      "title": "Survival After Radical Cystectomy for Bladder Cancer: Development of a Fair Machine Learning Model.",
      "abstract": "BACKGROUND: Prediction models based on machine learning (ML) methods are being increasingly developed and adopted in health care. However, these models may be prone to bias and considered unfair if they demonstrate variable performance in population subgroups. An unfair model is of particular concern in bladder cancer, where disparities have been identified in sex and racial subgroups. OBJECTIVE: This study aims (1) to develop a ML model to predict survival after radical cystectomy for bladder cancer and evaluate for potential model bias in sex and racial subgroups; and (2) to compare algorithm unfairness mitigation techniques to improve model fairness. METHODS: We trained and compared various ML classification algorithms to predict 5-year survival after radical cystectomy using the National Cancer Database. The primary model performance metric was the F1-score. The primary metric for model fairness was the equalized odds ratio (eOR). We compared 3 algorithm unfairness mitigation techniques to improve eOR. RESULTS: We identified 16,481 patients; 23.1% (n=3800) were female, and 91.5% (n=15,080) were \"White,\" 5% (n=832) were \"Black,\" 2.3% (n=373) were \"Hispanic,\" and 1.2% (n=196) were \"Asian.\" The 5-year mortality rate was 75% (n=12,290). The best naive model was extreme gradient boosting (XGBoost), which had an F1-score of 0.860 and eOR of 0.619. All unfairness mitigation techniques increased the eOR, with correlation remover showing the highest increase and resulting in a final eOR of 0.750. This mitigated model had F1-scores of 0.86, 0.904, and 0.824 in the full, Black male, and Asian female test sets, respectively. CONCLUSIONS: The ML model predicting survival after radical cystectomy exhibited bias across sex and racial subgroups. By using algorithm unfairness mitigation techniques, we improved algorithmic fairness as measured by the eOR. Our study highlights the role of not only evaluating for model bias but also actively mitigating such disparities to ensure equitable health care delivery. We also deployed the first web-based fair ML model for predicting survival after radical cystectomy.",
      "authors": "Carbunaru Samuel; Neshatvar Yassamin; Do Hyungrok; Murray Katie; Ranganath Rajesh; Nayan Madhur",
      "year": "2024",
      "journal": "JMIR medical informatics",
      "doi": "10.2196/63289",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39671594/",
      "mesh_terms": "Humans; Urinary Bladder Neoplasms; Cystectomy; Machine Learning; Female; Male; Aged; Middle Aged; Algorithms",
      "keywords": "algorithmic fairness; bias; bladder cancer; fairness; health equity; healthcare disparities; machine learning; model; mortality rate; prediction; radical cystectomy; survival",
      "pub_types": "Journal Article",
      "pmcid": "PMC11694706",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39220818",
      "title": "Evaluating precision medicine tools in cystic fibrosis for racial and ethnic fairness.",
      "abstract": "INTRODUCTION: Patients with cystic fibrosis (CF) experience frequent episodes of acute decline in lung function called pulmonary exacerbations (PEx). An existing clinical and place-based precision medicine algorithm that accurately predicts PEx could include racial and ethnic biases in clinical and geospatial training data, leading to unintentional exacerbation of health inequities. METHODS: We estimated receiver operating characteristic curves based on predictions from a nonstationary Gaussian stochastic process model for PEx within 3, 6, and 12 months among 26,392 individuals aged 6 years and above (2003-2017) from the US CF Foundation Patient Registry. We screened predictors to identify reasons for discriminatory model performance. RESULTS: The precision medicine algorithm performed worse predicting a PEx among Black patients when compared with White patients or to patients of another race for all three prediction horizons. There was little to no difference in prediction accuracies among Hispanic and non-Hispanic patients for the same prediction horizons. Differences in F508del, smoking households, secondhand smoke exposure, primary and secondary road densities, distance and drive time to the CF center, and average number of clinical evaluations were key factors associated with race. CONCLUSIONS: Racial differences in prediction accuracies from our PEx precision medicine algorithm exist. Misclassification of future PEx was attributable to several underlying factors that correspond to race: CF mutation, location where the patient lives, and clinical awareness. Associations of our proxies with race for CF-related health outcomes can lead to systemic racism in data collection and in prediction accuracies from precision medicine algorithms constructed from it.",
      "authors": "Colegate Stephen P; Palipana Anushka; Gecili Emrah; Szczesniak Rhonda D; Brokamp Cole",
      "year": "2024",
      "journal": "Journal of clinical and translational science",
      "doi": "10.1017/cts.2024.532",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39220818/",
      "mesh_terms": "",
      "keywords": "Cystic fibrosis; group fairness; lung function; medical monitoring; precision medicine; pulmonary function tests",
      "pub_types": "Journal Article",
      "pmcid": "PMC11362628",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "34833435",
      "title": "Hybrid Feature Selection Framework for the Parkinson Imbalanced Dataset Prediction Problem.",
      "abstract": "Background and Objectives: Recently, many studies have focused on the early detection of Parkinson's disease (PD). This disease belongs to a group of neurological problems that immediately affect brain cells and influence the movement, hearing, and various cognitive functions. Medical data sets are often not equally distributed in their classes and this gives a bias in the classification of patients. We performed a Hybrid feature selection framework that can deal with imbalanced datasets like PD. Use the SOMTE algorithm to deal with unbalanced datasets. Removing the contradiction from the features in the dataset and decrease the processing time by using Recursive Feature Elimination (RFE), and Principle Component Analysis (PCA). Materials and Methods: PD acoustic datasets and the characteristics of control subjects were used to construct classification models such as Bagging, K-nearest neighbour (KNN), multilayer perceptron, and the support vector machine (SVM). In the prepressing stage, the synthetic minority over-sampling technique (SMOTE) with two-feature selection RFE and PCA were used. The PD dataset comprises a large difference between the numbers of the infected and uninfected patients, which causes the classification bias problem. Therefore, SMOTE was used to resolve this problem. Results: For model evaluation, the train-test split technique was used for the experiment. All the models were Grid-search tuned, the evaluation results of the SVM model showed the highest accuracy of 98.2%, and the KNN model exhibited the highest specificity of 99%. Conclusions: the proposed method is compared with the current modern methods of detecting Parkinson's disease and other methods for medical diseases, it was noted that our developed system could treat data bias and reach a high prediction of PD and this can be beneficial for health organizations to properly prioritize assets.",
      "authors": "Qasim Hayder Mohammed; Ata Oguz; Ansari Mohammad Azam; Alomary Mohammad N; Alghamdi Saad; Almehmadi Mazen",
      "year": "2021",
      "journal": "Medicina (Kaunas, Lithuania)",
      "doi": "10.3390/medicina57111217",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34833435/",
      "mesh_terms": "Algorithms; Cluster Analysis; Humans; Neural Networks, Computer; Parkinson Disease; Support Vector Machine",
      "keywords": "PCA; Parkinson detection; RFE; SMOTE; machine learning",
      "pub_types": "Journal Article",
      "pmcid": "PMC8619928",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40056436",
      "title": "Large language models are less effective at clinical prediction tasks than locally trained machine learning models.",
      "abstract": "OBJECTIVES: To determine the extent to which current large language models (LLMs) can serve as substitutes for traditional machine learning (ML) as clinical predictors using data from electronic health records (EHRs), we investigated various factors that can impact their adoption, including overall performance, calibration, fairness, and resilience to privacy protections that reduce data fidelity. MATERIALS AND METHODS: We evaluated GPT-3.5, GPT-4, and traditional ML (as gradient-boosting trees) on clinical prediction tasks in EHR data from Vanderbilt University Medical Center (VUMC) and MIMIC IV. We measured predictive performance with area under the receiver operating characteristic (AUROC) and model calibration using Brier Score. To evaluate the impact of data privacy protections, we assessed AUROC when demographic variables are generalized. We evaluated algorithmic fairness using equalized odds and statistical parity across race, sex, and age of patients. We also considered the impact of using in-context learning by incorporating labeled examples within the prompt. RESULTS: Traditional ML [AUROC: 0.847, 0.894 (VUMC, MIMIC)] substantially outperformed GPT-3.5 (AUROC: 0.537, 0.517) and GPT-4 (AUROC: 0.629, 0.602) (with and without in-context learning) in predictive performance and output probability calibration [Brier Score (ML vs GPT-3.5 vs GPT-4): 0.134 vs 0.384 vs 0.251, 0.042 vs 0.06 vs 0.219)]. DISCUSSION: Traditional ML is more robust than GPT-3.5 and GPT-4 in generalizing demographic information to protect privacy. GPT-4 is the fairest model according to our selected metrics but at the cost of poor model performance. CONCLUSION: These findings suggest that non-fine-tuned LLMs are less effective and robust than locally trained ML for clinical prediction tasks, but they are improving across releases.",
      "authors": "Brown Katherine E; Yan Chao; Li Zhuohang; Zhang Xinmeng; Collins Benjamin X; Chen You; Clayton Ellen Wright; Kantarcioglu Murat; Vorobeychik Yevgeniy; Malin Bradley A",
      "year": "2025",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "doi": "10.1093/jamia/ocaf038",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40056436/",
      "mesh_terms": "Machine Learning; Humans; Electronic Health Records; Female; ROC Curve; Male; Algorithms; Middle Aged; Large Language Models",
      "keywords": "clinical prediction models; fairness; large language models; privacy",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "pmcid": "PMC12012369",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38962581",
      "title": "Challenges in Reducing Bias Using Post-Processing Fairness for Breast Cancer Stage Classification with Deep Learning.",
      "abstract": "Breast cancer is the most common cancer affecting women globally. Despite the significant impact of deep learning models on breast cancer diagnosis and treatment, achieving fairness or equitable outcomes across diverse populations remains a challenge when some demographic groups are underrepresented in the training data. We quantified the bias of models trained to predict breast cancer stage from a dataset consisting of 1000 biopsies from 842 patients provided by AIM-Ahead (Artificial Intelligence/Machine Learning Consortium to Advance Health Equity and Researcher Diversity). Notably, the majority of data (over 70%) were from White patients. We found that prior to post-processing adjustments, all deep learning models we trained consistently performed better for White patients than for non-White patients. After model calibration, we observed mixed results, with only some models demonstrating improved performance. This work provides a case study of bias in breast cancer medical imaging models and highlights the challenges in using post-processing to attempt to achieve fairness.",
      "authors": "Soltan Armin; Washington Peter",
      "year": "2024",
      "journal": "Algorithms",
      "doi": "10.3390/a17040141",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38962581/",
      "mesh_terms": "",
      "keywords": "algorithmic fairness; breast cancer; deep learning; equalized odds; equalized opportunity; post-processing method",
      "pub_types": "Journal Article",
      "pmcid": "PMC11221567",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "34573790",
      "title": "The Problem of Fairness in Synthetic Healthcare Data.",
      "abstract": "Access to healthcare data such as electronic health records (EHR) is often restricted by laws established to protect patient privacy. These restrictions hinder the reproducibility of existing results based on private healthcare data and also limit new research. Synthetically-generated healthcare data solve this problem by preserving privacy and enabling researchers and policymakers to drive decisions and methods based on realistic data. Healthcare data can include information about multiple in- and out- patient visits of patients, making it a time-series dataset which is often influenced by protected attributes like age, gender, race etc. The COVID-19 pandemic has exacerbated health inequities, with certain subgroups experiencing poorer outcomes and less access to healthcare. To combat these inequities, synthetic data must \"fairly\" represent diverse minority subgroups such that the conclusions drawn on synthetic data are correct and the results can be generalized to real data. In this article, we develop two fairness metrics for synthetic data, and analyze all subgroups defined by protected attributes to analyze the bias in three published synthetic research datasets. These covariate-level disparity metrics revealed that synthetic data may not be representative at the univariate and multivariate subgroup-levels and thus, fairness should be addressed when developing data generation methods. We discuss the need for measuring fairness in synthetic healthcare data to enable the development of robust machine learning models to create more equitable synthetic healthcare datasets.",
      "authors": "Bhanot Karan; Qi Miao; Erickson John S; Guyon Isabelle; Bennett Kristin P",
      "year": "2021",
      "journal": "Entropy (Basel, Switzerland)",
      "doi": "10.3390/e23091165",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34573790/",
      "mesh_terms": "",
      "keywords": "covariate; disparate impact; fairness; health inequities; healthcare; synthetic data; temporal; time-series",
      "pub_types": "Journal Article",
      "pmcid": "PMC8468495",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "33910923",
      "title": "Equity in essence: a call for operationalising fairness in machine learning for healthcare.",
      "abstract": "",
      "authors": "Wawira Gichoya Judy; McCoy Liam G; Celi Leo Anthony; Ghassemi Marzyeh",
      "year": "2021",
      "journal": "BMJ health & care informatics",
      "doi": "10.1136/bmjhci-2020-100289",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33910923/",
      "mesh_terms": "Delivery of Health Care; Humans; Machine Learning",
      "keywords": "BMJ health informatics",
      "pub_types": "Journal Article",
      "pmcid": "PMC8733939",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38740316",
      "title": "A roadmap to artificial intelligence (AI): Methods for designing and building AI ready data to promote fairness.",
      "abstract": "OBJECTIVES: We evaluated methods for preparing electronic health record data to reduce bias before applying artificial intelligence (AI). METHODS: We created methods for transforming raw data into a data framework for applying machine learning and natural language processing techniques for predicting falls and fractures. Strategies such as inclusion and reporting for multiple races, mixed data sources such as outpatient, inpatient, structured codes, and unstructured notes, and addressing missingness were applied to raw data to promote a reduction in bias. The raw data was carefully curated using validated definitions to create data variables such as age, race, gender, and healthcare utilization. For the formation of these variables, clinical, statistical, and data expertise were used. The research team included a variety of experts with diverse professional and demographic backgrounds to include diverse perspectives. RESULTS: For the prediction of falls, information extracted from radiology reports was converted to a matrix for applying machine learning. The processing of the data resulted in an input of 5,377,673 reports to the machine learning algorithm, out of which 45,304 were flagged as positive and 5,332,369 as negative for falls. Processed data resulted in lower missingness and a better representation of race and diagnosis codes. For fractures, specialized algorithms extracted snippets of text around keywork \"femoral\" from dual x-ray absorptiometry (DXA) scans to identify femoral neck T-scores that are important for predicting fracture risk. The natural language processing algorithms yielded 98% accuracy and 2% error rate The methods to prepare data for input to artificial intelligence processes are reproducible and can be applied to other studies. CONCLUSION: The life cycle of data from raw to analytic form includes data governance, cleaning, management, and analysis. When applying artificial intelligence methods, input data must be prepared optimally to reduce algorithmic bias, as biased output is harmful. Building AI-ready data frameworks that improve efficiency can contribute to transparency and reproducibility. The roadmap for the application of AI involves applying specialized techniques to input data, some of which are suggested here. This study highlights data curation aspects to be considered when preparing data for the application of artificial intelligence to reduce bias.",
      "authors": "Kidwai-Khan Farah; Wang Rixin; Skanderson Melissa; Brandt Cynthia A; Fodeh Samah; Womack Julie A",
      "year": "2024",
      "journal": "Journal of biomedical informatics",
      "doi": "10.1016/j.jbi.2024.104654",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38740316/",
      "mesh_terms": "Humans; Electronic Health Records; Artificial Intelligence; Natural Language Processing; Accidental Falls; Algorithms; Machine Learning; Fractures, Bone; Female",
      "keywords": "Algorithms; Artificial Intelligence; Data preparation; Diversity; Fairness; Inclusion",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC11144439",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "37609241",
      "title": "Enhancing Fairness in Disease Prediction by Optimizing Multiple Domain Adversarial Networks.",
      "abstract": "Predictive models in biomedicine need to ensure equitable and reliable outcomes for the populations they are applied to. Unfortunately, biases in medical predictions can lead to unfair treatment and widening disparities, underscoring the need for effective techniques to address these issues. To enhance fairness, we introduce a framework based on a Multiple Domain Adversarial Neural Network (MDANN), which incorporates multiple adversarial components. In an MDANN, an adversarial module is applied to learn a fair pattern by negative gradients back-propagating across multiple sensitive features (i.e., characteristics of individuals that should not be used to discriminate unfairly between individuals when making predictions or decisions.) We leverage loss functions based on the Area Under the Receiver Operating Characteristic Curve (AUC) to address the class imbalance, promoting equitable classification performance for minority groups (e.g., a subset of the population that is underrepresented or disadvantaged.) Moreover, we utilize pre-trained convolutional autoencoders (CAEs) to extract deep representations of data, aiming to enhance prediction accuracy and fairness. Combining these mechanisms, we alleviate biases and disparities to provide reliable and equitable disease prediction. We empirically demonstrate that the MDANN approach leads to better accuracy and fairness in predicting disease progression using brain imaging data for Alzheimer's Disease and Autism populations than state-of-the-art techniques.",
      "authors": "Li Bin; Shi Xinghua; Gao Hongchang; Jiang Xiaoqian; Zhang Kai; Harmanci Arif O; Malin Bradley",
      "year": "2023",
      "journal": "bioRxiv : the preprint server for biology",
      "doi": "10.1101/2023.08.04.551906",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37609241/",
      "mesh_terms": "",
      "keywords": "Fairness; adversarial training; biases; health disparities; machine learning",
      "pub_types": "Journal Article; Preprint",
      "pmcid": "PMC10441334",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41001459",
      "title": "Racial and Ethnic Disparities in Brain Age Algorithm Performance: Investigating Bias Across Six Popular Methods.",
      "abstract": "Brain age algorithms, which estimate biological aging from neuroimaging data, are increasingly used as biomarkers for health and disease. However, most algorithms are trained on datasets with limited racial and ethnic diversity, raising concerns about potential algorithmic bias that could exacerbate health disparities. To probe this potential, we evaluated six popular brain age algorithms using data from the Health and Aging Brain Study-Health Disparities (HABS-HD), comprising 1,123 White American, 1,107 Hispanic American, and 678 African American participants, ages \u226550. Comparing correlations between brain age and chronological age across racial/ethnic groups, relations were consistently weaker for African American participants compared to White and Hispanic American participants across most algorithms (ranging from r=0.51-0.85 for African Americans vs. r=0.57-0.89 for other groups). We also examined error for brain age v. chronological age and found significant differences in median errors across racial/ethnic groups, though specific patterns varied by algorithm. Sensitivity models weighting for age, sex, and scan quality noted similar patterns, with all algorithms maintaining significant differences in correlation or median prediction error between groups. Our findings reveal systematic performance differences in brain age algorithms across racial and ethnic groups, with most algorithms consistently showing reduced algorithm accuracy for African American and/or Hispanic-American participants. These biases, which are likely introduced at multiple stages of algorithm development, could impact clinical utility and diagnostic accuracy. Results highlight the urgent need for more inclusive algorithm development and validation to ensure equitable healthcare applications of neuroimaging biomarkers.",
      "authors": "Adkins Dorthea J; Hanson Jamie L",
      "year": "2025",
      "journal": "medRxiv : the preprint server for health sciences",
      "doi": "10.1101/2025.09.18.25336117",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41001459/",
      "mesh_terms": "",
      "keywords": "algorithm; brain age; chronological age; correlation; ethnicity; median error; race",
      "pub_types": "Journal Article; Preprint",
      "pmcid": "PMC12458499",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40354171",
      "title": "Mitigating Bias in Machine Learning Models with Ethics-Based Initiatives: The Case of Sepsis.",
      "abstract": "This paper discusses ethics-based strategies for mitigating bias in machine learning models used to predict sepsis onset. The first part discusses how various kinds of bias and their potential synergies can reduce predictive accuracy, especially as those biases derive from social determinants of health (SDOHs) and from the design and construction of the predictive model. The second part of the essay discusses how certain ethically-based strategies might mitigate the potential for disparate or unfair treatment produced by these models, not only as they might apply to sepsis but to any syndrome that witnesses the impact of adverse SDOHs on socioeconomically disadvantaged or marginalized populations.",
      "authors": "D Banja John; Xie Yao; R Smith Jeffrey; Rana Shaheen; L Holder Andre",
      "year": "2026",
      "journal": "The American journal of bioethics : AJOB",
      "doi": "10.1080/15265161.2025.2497971",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40354171/",
      "mesh_terms": "Humans; Machine Learning; Sepsis; Social Determinants of Health; Bias",
      "keywords": "Bias; equity; ethics; machine learning; sepsis; social determinants of health",
      "pub_types": "Journal Article",
      "pmcid": "PMC12353398",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39953146",
      "title": "Improving medical machine learning models with generative balancing for equity and excellence.",
      "abstract": "Applying machine learning to clinical outcome prediction is challenging due to imbalanced datasets and sensitive tasks that contain rare yet critical outcomes and where equitable treatment across diverse patient groups is essential. Despite attempts, biases in predictions persist, driven by disparities in representation and exacerbated by the scarcity of positive labels, perpetuating health inequities. This paper introduces FairPlay, a synthetic data generation approach leveraging large language models, to address these issues. FairPlay enhances algorithmic performance and reduces bias by creating realistic, anonymous synthetic patient data that improves representation and augments dataset patterns while preserving privacy. Through experiments on multiple datasets, we demonstrate that FairPlay boosts mortality prediction performance across diverse subgroups, achieving up to a 21% improvement in F1 Score without requiring additional data or altering downstream training pipelines. Furthermore, FairPlay consistently reduces subgroup performance gaps, as shown by universal improvements in performance and fairness metrics across four experimental setups.",
      "authors": "Theodorou Brandon; Danek Benjamin; Tummala Venkat; Kumar Shivam Pankaj; Malin Bradley; Sun Jimeng",
      "year": "2025",
      "journal": "NPJ digital medicine",
      "doi": "10.1038/s41746-025-01438-z",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39953146/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC11828851",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "33856478",
      "title": "Comparison of Methods to Reduce Bias From Clinical Prediction Models of Postpartum Depression.",
      "abstract": "IMPORTANCE: The lack of standards in methods to reduce bias for clinical algorithms presents various challenges in providing reliable predictions and in addressing health disparities. OBJECTIVE: To evaluate approaches for reducing bias in machine learning models using a real-world clinical scenario. DESIGN, SETTING, AND PARTICIPANTS: Health data for this cohort study were obtained from the IBM MarketScan Medicaid Database. Eligibility criteria were as follows: (1) Female individuals aged 12 to 55 years with a live birth record identified by delivery-related codes from January 1, 2014, through December 31, 2018; (2) greater than 80% enrollment through pregnancy to 60 days post partum; and (3) evidence of coverage for depression screening and mental health services. Statistical analysis was performed in 2020. EXPOSURES: Binarized race (Black individuals and White individuals). MAIN OUTCOMES AND MEASURES: Machine learning models (logistic regression [LR], random forest, and extreme gradient boosting) were trained for 2 binary outcomes: postpartum depression (PPD) and postpartum mental health service utilization. Risk-adjusted generalized linear models were used for each outcome to assess potential disparity in the cohort associated with binarized race (Black or White). Methods for reducing bias, including reweighing, Prejudice Remover, and removing race from the models, were examined by analyzing changes in fairness metrics compared with the base models. Baseline characteristics of female individuals at the top-predicted risk decile were compared for systematic differences. Fairness metrics of disparate impact (DI, 1 indicates fairness) and equal opportunity difference (EOD, 0 indicates fairness). RESULTS: Among 573\u202f634 female individuals initially examined for this study, 314\u202f903 were White (54.9%), 217\u202f899 were Black (38.0%), and the mean (SD) age was 26.1 (5.5) years. The risk-adjusted odds ratio comparing White participants with Black participants was 2.06 (95% CI, 2.02-2.10) for clinically recognized PPD and 1.37 (95% CI, 1.33-1.40) for postpartum mental health service utilization. Taking the LR model for PPD prediction as an example, reweighing reduced bias as measured by improved DI and EOD metrics from 0.31 and -0.19 to 0.79 and 0.02, respectively. Removing race from the models had inferior performance for reducing bias compared with the other methods (PPD: DI\u2009=\u20090.61; EOD\u2009=\u2009-0.05; mental health service utilization: DI\u2009=\u20090.63; EOD\u2009=\u2009-0.04). CONCLUSIONS AND RELEVANCE: Clinical prediction models trained on potentially biased data may produce unfair outcomes on the basis of the chosen metrics. This study's results suggest that the performance varied depending on the model, outcome label, and method for reducing bias. This approach toward evaluating algorithmic bias can be used as an example for the growing number of researchers who wish to examine and address bias in their data and models.",
      "authors": "Park Yoonyoung; Hu Jianying; Singh Moninder; Sylla Issa; Dankwa-Mullan Irene; Koski Eileen; Das Amar K",
      "year": "2021",
      "journal": "JAMA network open",
      "doi": "10.1001/jamanetworkopen.2021.3909",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33856478/",
      "mesh_terms": "Adolescent; Adult; Algorithms; Cohort Studies; Depression, Postpartum; Female; Humans; Middle Aged; Models, Statistical; Odds Ratio; Patient-Specific Modeling; Postpartum Period; Pregnancy; Prognosis; Retrospective Studies; Risk Assessment; Risk Factors; United States; Young Adult",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC8050742",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40100898",
      "title": "What makes clinical machine learning fair? A practical ethics framework.",
      "abstract": "Machine learning (ML) can offer a tremendous contribution to medicine by streamlining decision-making, reducing mistakes, improving clinical accuracy and ensuring better patient outcomes. The prospects of a widespread and rapid integration of machine learning in clinical workflow have attracted considerable attention including due to complex ethical implications-algorithmic bias being among the most frequently discussed ML models. Here we introduce and discuss a practical ethics framework inductively-generated via normative analysis of the practical challenges in developing an actual clinical ML model (see case study). The framework is usable to identify, measure and address bias in clinical machine learning models, thus improving fairness as to both model performance and health outcomes. We detail a proportionate approach to ML bias by defining the demands of fair ML in light of what is ethically justifiable and, at the same time, technically feasible in light of inevitable trade-offs. Our framework enables ethically robust and transparent decision-making both in the design and the context-dependent aspects of ML bias mitigation, thus improving accountability for both developers and clinical users.",
      "authors": "Hoche Marine; Mineeva Olga; R\u00e4tsch Gunnar; Vayena Effy; Blasimme Alessandro",
      "year": "2025",
      "journal": "PLOS digital health",
      "doi": "10.1371/journal.pdig.0000728",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40100898/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC11918422",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39677419",
      "title": "Not the Models You Are Looking For: Traditional ML Outperforms LLMs in Clinical Prediction Tasks.",
      "abstract": "OBJECTIVES: To determine the extent to which current Large Language Models (LLMs) can serve as substitutes for traditional machine learning (ML) as clinical predictors using data from electronic health records (EHRs), we investigated various factors that can impact their adoption, including overall performance, calibration, fairness, and resilience to privacy protections that reduce data fidelity. MATERIALS AND METHODS: We evaluated GPT-3.5, GPT-4, and ML (as gradient-boosting trees) on clinical prediction tasks in EHR data from Vanderbilt University Medical Center and MIMIC IV. We measured predictive performance with AUROC and model calibration using Brier Score. To evaluate the impact of data privacy protections, we assessed AUROC when demographic variables are generalized. We evaluated algorithmic fairness using equalized odds and statistical parity across race, sex, and age of patients. We also considered the impact of using in-context learning by incorporating labeled examples within the prompt. RESULTS: Traditional ML (AUROC: 0.847, 0.894 (VUMC, MIMIC)) substantially outperformed GPT-3.5 (AUROC: 0.537, 0.517) and GPT-4 (AUROC: 0.629, 0.602) (with and without in-context learning) in predictive performance and output probability calibration (Brier Score (ML vs GPT-3.5 vs GPT-4): 0.134 versus 0.384 versus 0.251, 0.042 versus 0.06 versus 0.219). Traditional ML is more robust than GPT-3.5 and GPT-4 to generalizing demographic information to protect privacy. GPT-4 is the fairest model according to our selected metrics but at the cost of poor model performance. CONCLUSION: These findings suggest that LLMs are much less effective and robust than locally-trained ML for clinical prediction tasks, but they are getting better over time.",
      "authors": "Brown Katherine E; Yan Chao; Li Zhuohang; Zhang Xinmeng; Collins Benjamin X; Chen You; Clayton Ellen Wright; Kantarcioglu Murat; Vorobeychik Yevgeniy; Malin Bradley A",
      "year": "2024",
      "journal": "medRxiv : the preprint server for health sciences",
      "doi": "10.1101/2024.12.03.24318400",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39677419/",
      "mesh_terms": "",
      "keywords": "Large language models; clinical prediction models; fairness; privacy",
      "pub_types": "Journal Article; Preprint",
      "pmcid": "PMC11643212",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41296741",
      "title": "Evaluating algorithmic fairness of machine learning models in predicting underweight, overweight, and adiposity across socioeconomic and caste groups in India: evidence from the longitudinal ageing study in India.",
      "abstract": "Machine learning (ML) models are increasingly applied to predict body mass index (BMI) and related outcomes, yet their fairness across socioeconomic and caste groups remains uncertain, particularly in contexts of structural inequality. Using nationally representative data from more than 55,000 adults aged 45 years and older in the Longitudinal Ageing Study in India (LASI), we evaluated the accuracy and fairness of multiple ML algorithms-including Random Forest, XGBoost, Gradient Boosting, LightGBM, Deep Neural Networks, and Deep Cross Networks-alongside logistic regression for predicting underweight, overweight, and central adiposity. Models were trained on 80% of the data and tested on 20%, with performance assessed using AUROC, accuracy, sensitivity, specificity, and precision. Fairness was evaluated through subgroup analyses across socioeconomic and caste groups and equity-based metrics such as Equalized Odds and Demographic Parity. Feature importance was examined using SHAP values, and bias-mitigation methods were implemented at pre-processing, in-processing, and post-processing stages. Tree-based models, particularly LightGBM and Gradient Boosting, achieved the highest AUROC values (0.79-0.84). Incorporating socioeconomic and health-related variables improved prediction, but fairness gaps persisted: performance declined for scheduled tribes and lower socioeconomic groups. SHAP analyses identified grip strength, gender, and residence as key drivers of prediction differences. Among mitigation strategies, Reject Option Classification and Equalized Odds Post-processing moderately reduced subgroup disparities but sometimes decreased overall performance, whereas other approaches yielded minimal gains. ML models can effectively predict obesity and adiposity risk in India, but addressing bias is essential for equitable application. Continued refinement of fairness-aware ML methods is needed to support inclusive and effective public-health decision-making.",
      "authors": "Lee John Tayu; Hsu Sheng Hui; Li Vincent Cheng-Sheng; Anindya Kanya; Chen Meng-Huan; Wang Charlotte; Shen Toby Kai-Bo; Liu Valerie Tzu Ning; Chen Hsiao-Hui; Atun Rifat",
      "year": "2025",
      "journal": "PLOS digital health",
      "doi": "10.1371/journal.pdig.0000951",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41296741/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC12654920",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41292658",
      "title": "Develop and Validate A Fair Machine Learning Model to Indentify Patients with High Care-Continuity in Electronic Health Records Data.",
      "abstract": "OBJECTIVES: Electronic health record (EHR) data often missed care outside a given health system, resulting in data discontinuity. We aimed to: (1) quantify misclassification across levels of EHR data discontinuity and identify an optimal continuity threshold. (2) develop a machine learning (ML) model to predict EHR continuity and optimize fairness across racial and ethnic groups, and (3) externally validate the EHR continuity prediction model using an independent dataset. MATERIALS AND METHODS: We used linked OneFlorida+ EHR-Medicaid claims data for model development and REACHnet EHR-Louisiana Blue Cross Blue Shield (LABlue) claims data for external validation. A novel Harmonized Encounter Proportion Score (HEPS) was applied to quantify patient-level EHR data continuity and the impact on misclassification of 42 clinical variables. ML models were trained using routinely available demographic, clinical, and healthcare utilization features derived from structured EHR data. RESULTS: Higher EHR data continuity was associated with lower rates of misclassification. A HEPS threshold of approximately 30% effectively distinguished patients with sufficient data continuity. ML models demonstrated strong performance in predicting high continuity (AUROC=0.77). Fairness assessments showed bias against Hispanic group, which was substantially improved following bias mitigation procedures. Model performance remained robust and fair in the external validation. DISCUSSION: Our study offers a practical metric for quantifying care continuity in EHR networks. The current ML model incorporating EHR-routinely collected information can accurately identify patients with high care continuity. CONCLUSIONS: We developed a generalizable care-continuity classification tool that can be easily applied across EHR systems, strengthening the rigor of EHR-based research.",
      "authors": "Lee Yao An; Tang Tiange; Huang Yu; Bian Jiang; Shi Lizheng; Guo Jingchuan",
      "year": "2025",
      "journal": "medRxiv : the preprint server for health sciences",
      "doi": "10.1101/2025.11.11.25339938",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41292658/",
      "mesh_terms": "",
      "keywords": "Data continuity; EHR; Machine learning prediction",
      "pub_types": "Journal Article; Preprint",
      "pmcid": "PMC12642717",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40318249",
      "title": "Building competency in artificial intelligence and bias mitigation for nurse scientists and aligned health researchers.",
      "abstract": "Healthcare systems are increasingly integrating artificial intelligence and machine learning (AI/ML) tools into patient care, potentially influencing clinical decisions for millions. However, concerns are growing about these tools reinforcing systemic inequities. To address bias in AI/ML tools and promote equitable outcomes, guidelines for mitigating this bias and comprehensive workforce training programs are necessary. In response, we developed the multifaceted Human-Centered Use of Multidisciplinary AI for Next-Gen Education and Research (HUMAINE), informed by a comprehensive scoping review, training workshops, and a research symposium. The curriculum, which focuses on structural inequities in algorithms that contribute to health disparities, is designed to equip scientists with AI/ML competencies that allow them to effectively address these structural inequities and promote health equity. The curriculum incorporates the perspectives of clinicians, biostatisticians, engineers, and policymakers to harness AI's transformative potential, with the goal of building an inclusive ecosystem where cutting-edge technology and ethical AI governance converge to create a more equitable healthcare future for all.",
      "authors": "Cary Michael P; Grady Siobahn D; McMillian-Bohler Jacquelyn; Bessias Sophia; Silcox Christina; Silva Susan; Guilamo-Ramos Vincent; McCall Jonathan; Sperling Jessica; Goldstein Benjamin A",
      "year": "2025",
      "journal": "Nursing outlook",
      "doi": "10.1016/j.outlook.2025.102395",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40318249/",
      "mesh_terms": "Humans; Artificial Intelligence; Research Personnel; Curriculum",
      "keywords": "Algorithmic bias; Artificial intelligence; Ethics in AI; Health equity; Healthcare disparities; Machine learning; Social determinants of health; Structural racism; Workforce training",
      "pub_types": "Journal Article",
      "pmcid": "PMC12178818",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41256428",
      "title": "A Systematic Fairness Evaluation of Racial Bias in Alzheimer's Disease Diagnosis Using Machine Learning Models.",
      "abstract": "INTRODUCTION: Alzheimer's disease (AD) is a major global health concern, expected to affect 12.7 million Americans by 2050. Machine learning (ML) algorithms have been developed for AD diagnosis and progression prediction, but the lack of racial diversity in clinical datasets raises concerns about their generalizability across demographic groups, particularly underrepresented populations. Studies show ML algorithms can inherit biases from data, leading to biased AD predictions. METHODS: This study investigates the fairness of ML models in AD diagnosis. We hypothesize that models trained on a single racial group perform well within that group but poorly in others. We employ feature selection and model training techniques to improve fairness. RESULTS: Our findings support our hypothesis that ML models trained on one group underperform on others. We also demonstrated that applying fairness techniques to ML models reduces their bias. DISCUSSION: This study highlights the need for racial diversity in datasets and fair models for AD prediction.",
      "authors": "Baddam Neha Goud; Pijani Bizhan Alipour; Bozdag Serdar",
      "year": "2025",
      "journal": "bioRxiv : the preprint server for biology",
      "doi": "10.1101/2025.09.30.678854",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41256428/",
      "mesh_terms": "",
      "keywords": "Algorithmic Bias; Alzheimer\u2019s Disease; Fairness; Machine Learning",
      "pub_types": "Journal Article; Preprint",
      "pmcid": "PMC12622001",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41036091",
      "title": "Evaluating the impact of data biases on algorithmic fairness and clinical utility of machine learning models for prolonged opioid use prediction.",
      "abstract": "OBJECTIVES: The growing use of machine learning (ML) in healthcare raises concerns about how data biases affect real-world model performance. While existing frameworks evaluate algorithmic fairness, they often overlook the impact of bias on generalizability and clinical utility, which are critical for safe deployment. Building on prior methods, this study extends bias analysis to include clinical utility, addressing a key gap between fairness evaluation and decision-making. MATERIALS AND METHODS: We applied a 3-phase evaluation to a previously developed model predicting prolonged opioid use (POU), validated on Veterans Health Administration (VHA) data. The analysis included internal and external validation, model retraining on VHA data, and subgroup evaluation across demographic, vulnerable, risk, and comorbidity groups. We assessed performance using area under the receiver operating characteristic curve (AUROC), calibration, and decision curve analysis, incorporating standardized net-benefits to evaluate clinical utility alongside fairness and generalizability. RESULTS: The internal cohort (N\u2009=\u200941\u2009929) had a 14.7% POU prevalence, compared to 34.3% in the external VHA cohort (N\u2009=\u2009397\u2009150). The model's AUROC decreased from 0.74 in the internal test cohort to 0.70 in the full external cohort. Subgroup-level performance averaged 0.69 (SD\u2009=\u20090.01), showing minimal deviation from the external cohort overall. Retraining on VHA data improved AUROCs to 0.82. Clinical utility analysis showed systematic shifts in net-benefit across threshold probabilities. DISCUSSION: While the POU model showed generalizability and fairness internally, external validation and retraining revealed performance and utility shifts across subgroups. CONCLUSION: Population-specific biases affect clinical utility-an often-overlooked dimension in fairness evaluation-a key need to ensure equitable benefits across diverse patient groups.",
      "authors": "Naderalvojoud Behzad; Curtin Catherine; Asch Steven M; Humphreys Keith; Hernandez-Boussard Tina",
      "year": "2025",
      "journal": "JAMIA open",
      "doi": "10.1093/jamiaopen/ooaf115",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41036091/",
      "mesh_terms": "",
      "keywords": "algorithmic fairness; clinical utility; data bias; generalizability; machine learning",
      "pub_types": "Journal Article",
      "pmcid": "PMC12483547",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36573157",
      "title": "\"Just\" accuracy? Procedural fairness demands explainability in AI-based medical resource allocations.",
      "abstract": "The increasing application of artificial intelligence (AI) to healthcare raises both hope and ethical concerns. Some advanced machine learning methods provide accurate clinical predictions at the expense of a significant lack of explainability. Alex John London has defended that accuracy is a more important value than explainability in AI medicine. In this article, we locate the trade-off between accurate performance and explainable algorithms in the context of distributive justice. We acknowledge that accuracy is cardinal from outcome-oriented justice because it helps to maximize patients' benefits and optimizes limited resources. However, we claim that the opaqueness of the algorithmic black box and its absence of explainability threatens core commitments of procedural fairness such as accountability, avoidance of bias, and transparency. To illustrate this, we discuss liver transplantation as a case of critical medical resources in which the lack of explainability in AI-based allocation algorithms is procedurally unfair. Finally, we provide a number of ethical recommendations for when considering the use of unexplainable algorithms in the distribution of health-related resources.",
      "authors": "Rueda Jon; Rodr\u00edguez Janet Delgado; Jounou Iris Parra; Hortal-Carmona Joaqu\u00edn; Aus\u00edn Txetxu; Rodr\u00edguez-Arias David",
      "year": "2022",
      "journal": "AI & society",
      "doi": "10.1007/s00146-022-01614-9",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36573157/",
      "mesh_terms": "",
      "keywords": "Artificial intelligence; Distributive justice; Explainability; Medical AI; Procedural fairness",
      "pub_types": "Journal Article",
      "pmcid": "PMC9769482",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41283058",
      "title": "Development of machine learning models with explainable AI for frailty risk prediction and their web-based application in community public health.",
      "abstract": "BACKGROUND: Frailty is a public health concern linked to falls, disability, and mortality. Early screening and tailored interventions can mitigate adverse outcomes, but community settings require tools that are accurate and explainable. Korea is entering a super-aged phase, yet few approaches have used nationally representative survey data. OBJECTIVE: This study aimed to identify key predictors of frailty risk using the K-FRAIL scale using explainable machine learning (ML), based on data from the 2023 National Survey of Older Koreans (NSOK). It also sought to develop and internally validate prediction models. To demonstrate the potential applicability of these models in community public health and clinical practice, a web-based application was implemented. METHODS: Data from 10,078 older adults were analyzed, with frailty defined by the K-FRAIL scale (robust\u202f=\u202f0, pre-frail\u202f=\u202f1-2, and frail\u202f=\u202f3-5). A total of 132 candidate variables were constructed through selection and derivation. Using CatBoost with out-of-fold (OOF) SHapley Additive exPlanations (SHAP, a game-theoretic approach to quantify feature contributions), 15 key predictors were identified and applied across 10 algorithms under nested cross-validation (CV). Model performance was evaluated using receiver operating characteristic-area under the curve (ROC-AUC), precision-recall area under the curve (PR-AUC), F1-score, balanced accuracy, and the Brier score. To assess feasibility, a single-page bilingual web application was developed, integrating the CatBoost inference pipeline for offline use. RESULTS: SHAP analysis identified depression score, age, instrumental activities of daily living (IADL) count, sleep quality, and cognition as the leading predictors, followed by smartphone use, number of medications, province, driving status, hospital use, physical activity, osteoporosis, eating alone, digital adaptation difficulty, and sex, yielding 15 key predictors across the mental, functional, lifestyle, social, and digital domains. Using these predictors, boosting models outperformed other algorithms, with CatBoost achieving the best performance (ROC-AUC\u202f=\u202f0.813\u202f\u00b1\u202f0.014; PR-AUC\u202f=\u202f0.748\u202f\u00b1\u202f0.019). CONCLUSION: An explainable machine learning model with strong discrimination performance and adequate calibration was developed, accompanied by a lightweight web application for potential use in community and clinical settings. However, external validation, recalibration, and subgroup fairness assessments are needed to ensure generalizability and clinical adoption.",
      "authors": "Kim Seungmi; Choi Byung Kwan; Cho Jeong Su; Huh Up; Shin Myung-Jun; Obradovic Zoran; Rubin Daniel J; Lee Jae Il; Park Jong-Hwan",
      "year": "2025",
      "journal": "Frontiers in public health",
      "doi": "10.3389/fpubh.2025.1698062",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41283058/",
      "mesh_terms": "Humans; Machine Learning; Aged; Male; Republic of Korea; Female; Frailty; Public Health; Aged, 80 and over; Internet; Risk Assessment; Geriatric Assessment; Frail Elderly",
      "keywords": "SHAP; digital health; explainable AI; frailty; machine learning; prediction model",
      "pub_types": "Journal Article",
      "pmcid": "PMC12629939",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "30288421",
      "title": "Efficient and Fair Heart Allocation Policies for Transplantation.",
      "abstract": "Background: The optimal allocation of limited donated hearts to patients on the waiting list is one of the top priorities in heart transplantation management. We developed a simulation model of the US waiting list for heart transplantation to investigate the potential impacts of allocation policies on several outcomes such as pre- and posttransplant mortality. Methods: We used data from the United Network for Organ Sharing (UNOS) and the Scientific Registry of Transplant Recipient (SRTR) to simulate the heart allocation system. The model is validated by comparing the outcomes of the simulation with historical data. We also adapted fairness schemes studied in welfare economics to provide a framework to assess the fairness of allocation policies for transplantation. We considered three allocation policies, each a modification to the current UNOS allocation policy, and analyzed their performance via simulation. The first policy broadens the geographical allocation zones, the second modifies the health status order for receiving hearts, and the third prioritizes patients according to their waiting time. Results: Our results showed that the allocation policy similar to the current UNOS practice except that it aggregates the three immediate geographical allocation zones, improves the health outcomes, and is \"closer\" to an optimal fair policy compared to all other policies considered in this study. Specifically, this policy could have saved 319 total deaths (out of 3738 deaths) during the 2006 to 2014 time horizon, in average. This policy slightly differs from the current UNOS allocation policy and allows for easy implementation. Conclusion: We developed a model to compare the outcomes of heart allocation policies. Combining the three immediate geographical zones in the current allocation algorithm could potentially reduce mortality rate and is closer to an optimal fair policy.",
      "authors": "Hasankhani Farhad; Khademi Amin",
      "year": "2017",
      "journal": "MDM policy & practice",
      "doi": "10.1177/2381468317709475",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30288421/",
      "mesh_terms": "",
      "keywords": "allocation policy; fairness; heart failure; simulation; survival; transplantation",
      "pub_types": "Journal Article",
      "pmcid": "PMC6125046",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40880105",
      "title": "Algorithms to Improve Fairness in Medicare Risk Adjustment.",
      "abstract": "IMPORTANCE: Payment system design creates incentives that affect health care spending, access, and outcomes. With Medicare Advantage accounting for more than half of Medicare spending, changes to its risk adjustment algorithm have the potential for broad consequences. OBJECTIVE: To assess the potential for algorithmic tools to achieve more equitable plan payment for Medicare risk adjustment while maintaining current levels of performance, flexibility, feasibility, transparency, and interpretability. DESIGN, SETTING, AND PARTICIPANTS: This diagnostic study included a retrospective analysis of traditional Medicare enrollment and claims data generated between January 1, 2017, and December 31, 2020, from a random 20% sample of non-dual-eligible Medicare beneficiaries with documented residence in the US or Puerto Rico. Race and ethnicity were designated using the Research Triangle Institute enhanced indicator. Diagnoses in claims were mapped to hierarchical condition categories. Algorithms used demographic indicators and hierarchical condition categories from 1 calendar year to predict Medicare spending in the subsequent year. Data analysis was conducted between August 16, 2023, and January 27, 2025. MAIN OUTCOMES AND MEASURES: The main outcome was prospective health care spending by Medicare. Overall performance was measured by payment system fit and mean absolute error. Net compensation was used to assess group-level fairness. RESULTS: The main analysis of Medicare risk adjustment algorithms included 4\u202f398\u202f035 Medicare beneficiaries with a mean (SD) age of 75.2 (7.4) years and mean (SD) annual Medicare spending of $8345 ($18\u202f581); 44% were men; fewer than 1% were American Indian or Alaska Native, 2% were Asian or Other Pacific Islander, 6% were Black, 3% were Hispanic, 86% were non-Hispanic White, and 1% were part of an additional group (termed as other in the Centers for Medicare & Medicaid Services data). Out-of-sample payment system fit for the baseline regression was 12.7%. Constrained regression and postprocessing both achieved fair spending targets while maintaining payment system fit (constrained regression, 12.6%; postprocessing, 12.7%). Whereas postprocessing increased mean payments for beneficiaries in minoritized racial and ethnic groups (American Indian or Alaska Native, Asian or Other Pacific Islander, Black, and Hispanic individuals) only, constrained regression increased mean payments for beneficiaries in minoritized racial and ethnic groups and beneficiaries in other groups residing in counties with greater exposure to socioeconomic factors that can adversely affect health outcomes. CONCLUSIONS AND RELEVANCE: Results of this study suggest that constrained regression and postprocessing can incorporate fairness objectives into the Medicare risk adjustment algorithm with minimal reduction in overall fit. These feasible changes to the Medicare risk adjustment algorithm could be considered by policymakers aiming to address health care disparities through payment system reform.",
      "authors": "Reitsma Marissa B; McGuire Thomas G; Rose Sherri",
      "year": "2025",
      "journal": "JAMA health forum",
      "doi": "10.1001/jamahealthforum.2025.2640",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40880105/",
      "mesh_terms": "Humans; United States; Algorithms; Risk Adjustment; Medicare; Male; Retrospective Studies; Female; Aged; Health Expenditures; Aged, 80 and over; Medicare Part C",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC12397885",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40598240",
      "title": "Implicit bias in ICU electronic health record data: measurement frequencies and missing data rates of clinical variables.",
      "abstract": "BACKGROUND: Systematic disparities in data collection within electronic health records (EHRs), defined as non-random patterns in the measurement and recording of clinical variables across demographic groups, can be reflective of underlying implicit bias and may affect patient outcome. Identifying and mitigating these biases is critical for ensuring equitable healthcare. This study aims to develop an analytical framework for measurement patterns, defined as the combination of measurement frequency (how often variables are collected) and missing data rates (the frequency of missing recordings), evaluate the association between them and demographic factors, and assess their impact on in-hospital mortality prediction. METHODS: We conducted a retrospective cohort study using the Medical Information Mart for Intensive Care III (MIMIC-III) database, which includes data on over 40,000 ICU patients from Beth Israel Deaconess Medical Center (2001-2012). Adult patients with ICU stays longer than 24\u00a0h were included. Measurement patterns, including missing data rates and measurement frequencies, were derived from EHR data and analyzed. Targeted Machine Learning (TML) methods were used to assess potential systematic disparities in measurement patterns across demographic factors (age, gender, race/ethnicity) while controlling for confounders such as other demographics and disease severity. The predictive power of measurement patterns on in-hospital mortality was evaluated. RESULTS: Among 23,426 patients, significant demographic systematic disparities were observed in the first 24\u00a0h of ICU stays. Elderly patients (\u2265\u200965 years) had more frequent temperature measurements compared to younger patients, while males had slightly fewer missing temperature measurements than females. Racial disparities were notable: White patients had more frequent blood pressure and oxygen saturation (SpO2) measurements compared to Black and Hispanic patients. Measurement patterns were associated with ICU mortality, with models based solely on these patterns achieving an area under the receiver operating characteristic curve (AUC) of 0.76 (95% CI: 0.74-0.77). CONCLUSIONS: This study underscores the significance of measurement patterns in ICU EHR data, which are associated with patient demographics and ICU mortality. Analyzing patterns of missing data and measurement frequencies provides valuable insights into patient monitoring practices and potential systemic disparities in healthcare delivery. Understanding these disparities is critical for improving the fairness of healthcare delivery and developing more accurate predictive models in critical care settings. CLINICAL TRIAL NUMBER: Not applicable.",
      "authors": "Shi Junming; Hubbard Alan E; Fong Nicholas; Pirracchio Romain",
      "year": "2025",
      "journal": "BMC medical informatics and decision making",
      "doi": "10.1186/s12911-025-03058-9",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40598240/",
      "mesh_terms": "Humans; Electronic Health Records; Female; Male; Intensive Care Units; Middle Aged; Retrospective Studies; Aged; Hospital Mortality; Adult; Bias; Machine Learning",
      "keywords": "Bias detection; Critical care; Data completeness; Electronic health records (EHRs); Health equity; Healthcare applications; Implicit bias; MIMIC-III dataset; Measurement frequency; Systematic disparities",
      "pub_types": "Journal Article",
      "pmcid": "PMC12220764",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39979842",
      "title": "Optimized machine learning framework for cardiovascular disease diagnosis: a novel ethical perspective.",
      "abstract": "Alignment of advanced cutting-edge technologies such as Artificial Intelligence (AI) has emerged as a significant driving force to achieve greater precision and timeliness in identifying cardiovascular diseases (CVDs). However, it is difficult to achieve high accuracy and reliability in CVD diagnostics due to complex clinical data and the selection and modeling process of useful features. Therefore, this paper studies advanced AI-based feature selection techniques and the application of AI technologies in the CVD classification. It uses methodologies such as Chi-square, Info Gain, Forward Selection, and Backward Elimination as an essence of cardiovascular health indicators into a refined eight-feature subset. This study emphasizes ethical considerations, including transparency, interpretability, and bias mitigation. This is achieved by employing unbiased datasets, fair feature selection techniques, and rigorous validation metrics to ensure fairness and trustworthiness in the AI-based diagnostic process. In addition, the integration of various Machine Learning (ML) models, encompassing Random Forest (RF), XGBoost, Decision Trees (DT), and Logistic Regression (LR), facilitates a comprehensive exploration of predictive performance. Among this diverse range of models, XGBoost stands out as the top performer, achieving exceptional scores with a 99% accuracy rate, 100% recall, 99% F1-measure, and 99% precision. Furthermore, we venture into dimensionality reduction, applying Principal Component Analysis (PCA) to the eight-feature subset, effectively refining it to a compact six-attribute feature subset. Once again, XGBoost shines as the model of choice, yielding outstanding results. It achieves accuracy, recall, F1-measure, and precision scores of 98%, 100%, 98%, and 97%, respectively, when applied to the feature subset derived from the combination of Chi-square and Forward Selection methods.",
      "authors": "Alwakid Ghadah; Ul Haq Farman; Tariq Noshina; Humayun Mamoona; Shaheen Momina; Alsadun Marwa",
      "year": "2025",
      "journal": "BMC cardiovascular disorders",
      "doi": "10.1186/s12872-025-04550-w",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39979842/",
      "mesh_terms": "Humans; Machine Learning; Cardiovascular Diseases; Predictive Value of Tests; Reproducibility of Results; Diagnosis, Computer-Assisted; Decision Support Techniques; Decision Trees",
      "keywords": "Artificial intelligence; Cardiovascular diseases; Chi-square; Decision trees; Feature selection; K-nearest neighbours; Logistic regression; Machine learning; Principal component analysis; Random forest; XGBoost",
      "pub_types": "Journal Article",
      "pmcid": "PMC11844188",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41111097",
      "title": "Development and Validation of an Electronic Health Record-Derived Prediction Model for Preventing COVID-19 Hospitalization and Death.",
      "abstract": "Hospitalization and death following COVID-19 infection continue to pose a major public health concern and place strain on health system resources. Outpatient antiviral medication can reduce the risk of COVID-19 hospitalization and death for those at risk of poor outcomes, but identifying high-risk populations who may benefit most from treatment is challenging. The objective of this study was to develop and validate a prediction model for the composite outcome of hospitalization or death in the 14\u00a0days following COVID-19 infection. Our sample included 67,530 COVID-19 infections documented in outpatient care and occurring between April 1, 2020, and November 1, 2022, for 64,529 Kaiser Permanente Washington patients who did not receive outpatient antiviral treatment; 1378 (2.0%) of these infections resulted in hospitalization or death. Our prediction model, estimated using logistic regression with LASSO variable selection and ridge penalization, included 19 risk factors and showed high performance, including an area under the curve of 0.825 (95% confidence interval 0.813-0.836). Among the 10% of infections with the highest risk predictions, the true positive rate was 48% (46-51%) and the positive predictive value was 9.9% (9.2-10.6%). Supplemental analyses confirmed strong model performance across racial and ethnic subgroups and over time. We also present our process for selecting a risk threshold above which to recommend antiviral treatment and discuss considerations for prospective clinical implementation. This project demonstrates that machine learning tools can be used by health systems to deliver timely, targeted secondary prevention to reduce the risk of serious illness or death.",
      "authors": "Coley R Yates; Hays Rachel; Pardee Roy E; Fuller Sharon; Rogers Kristine; Allen Claire L; Arterburn David E; Frazier Robert K; Kent Daniel J; Mun Sophia; Mwatha Tolani; Thottingal Paul; Westbrook Emily O",
      "year": "2025",
      "journal": "Prevention science : the official journal of the Society for Prevention Research",
      "doi": "10.1007/s11121-025-01844-5",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41111097/",
      "mesh_terms": "",
      "keywords": "Algorithm fairness; Clinical prediction model; Health equity; Learning health system; Nirmatrelvir/ritonavir (Paxlovid\u00ae); Prospective validation",
      "pub_types": "Journal Article",
      "pmcid": "8531997",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35353048",
      "title": "Leveraging Machine Learning to Understand How Emotions Influence Equity Related Education: Quasi-Experimental Study.",
      "abstract": "BACKGROUND: Teaching and learning about topics such as bias are challenging due to the emotional nature of bias-related discourse. However, emotions can be challenging to study in health professions education for numerous reasons. With the emergence of machine learning and natural language processing, sentiment analysis (SA) has the potential to bridge the gap. OBJECTIVE: To improve our understanding of the role of emotions in bias-related discourse, we developed and conducted a SA of bias-related discourse among health professionals. METHODS: We conducted a 2-stage quasi-experimental study. First, we developed a SA (algorithm) within an existing archive of interviews with health professionals about bias. SA refers to a mechanism of analysis that evaluates the sentiment of textual data by assigning scores to textual components and calculating and assigning a sentiment value to the text. Next, we applied our SA algorithm to an archive of social media discourse on Twitter that contained equity-related hashtags to compare sentiment among health professionals and the general population. RESULTS: When tested on the initial archive, our SA algorithm was highly accurate compared to human scoring of sentiment. An analysis of bias-related social media discourse demonstrated that health professional tweets (n=555) were less neutral than the general population (n=6680) when discussing social issues on professionally associated accounts (\u03c72 [2, n=555)]=35.455; P<.001), suggesting that health professionals attach more sentiment to their posts on Twitter than seen in the general population. CONCLUSIONS: The finding that health professionals are more likely to show and convey emotions regarding equity-related issues on social media has implications for teaching and learning about sensitive topics related to health professions education. Such emotions must therefore be considered in the design, delivery, and evaluation of equity and bias-related education.",
      "authors": "Sukhera Javeed; Ahmed Hasan",
      "year": "2022",
      "journal": "JMIR medical education",
      "doi": "10.2196/33934",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35353048/",
      "mesh_terms": "",
      "keywords": "bias; education; emotion; equity; medical education; sentiment analysis",
      "pub_types": "Journal Article",
      "pmcid": "PMC9008524",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40905712",
      "title": "Detecting, Characterizing, and Mitigating Implicit and Explicit Racial Biases in Health Care Datasets With Subgroup Learnability: Algorithm Development and Validation Study.",
      "abstract": "BACKGROUND: The growing adoption of diagnostic and prognostic algorithms in health care has led to concerns about the perpetuation of algorithmic bias against disadvantaged groups of individuals. Deep learning methods to detect and mitigate bias have revolved around modifying models, optimization strategies, and threshold calibration with varying levels of success and tradeoffs. However, there have been limited substantive efforts to address bias at the level of the data used to generate algorithms in health care datasets. OBJECTIVE: The aim of this study is to create a simple metric (AEquity) that uses a learning curve approximation to distinguish and mitigate bias via guided dataset collection or relabeling. METHODS: We demonstrate this metric in 2 well-known examples, chest X-rays and health care cost utilization, and detect novel biases in the National Health and Nutrition Examination Survey. RESULTS: We demonstrated that using AEquity to guide data-centric collection for each diagnostic finding in the chest radiograph dataset decreased bias by between 29% and 96.5% when measured by differences in area under the curve. Next, we wanted to examine (1) whether AEquity worked on intersectional populations and (2) if AEquity is invariant to different types of fairness metrics, not just area under the curve. Subsequently, we examined the effect of AEquity on mitigating bias when measured by false negative rate, precision, and false discovery rate for Black patients on Medicaid. When we examined Black patients on Medicaid, at the intersection of race and socioeconomic status, we found that AEquity-based interventions reduced bias across a number of different fairness metrics including overall false negative rate by 33.3% (bias reduction absolute=1.88\u00d710-1, 95% CI 1.4\u00d710-1 to 2.5\u00d710-1; bias reduction of 33.3%, 95% CI 26.6%-40%; precision bias by 7.50\u00d710-2, 95% CI 7.48\u00d710-2 to 7.51\u00d710-2; bias reduction of 94.6%, 95% CI 94.5%-94.7%; false discovery rate by 94.5%; absolute bias reduction=3.50\u00d710-2, 95% CI 3.49\u00d710-2 to 3.50\u00d710-2). Similarly, AEquity-guided data collection demonstrated bias reduction of up to 80% on mortality prediction with the National Health and Nutrition Examination Survey (bias reduction absolute=0.08, 95% CI 0.07-0.09). Then, we wanted to compare AEquity to state-of-the-art data-guided debiasing measures such as balanced empirical risk minimization and calibration. Consequently, we benchmarked against balanced empirical risk minimization and calibration and showed that AEquity-guided data collection outperforms both standard approaches. Moreover, we demonstrated that AEquity works on fully connected networks; convolutional neural networks such as ResNet-50; transformer architectures such as VIT-B-16, a vision transformer with 86 million parameters; and nonparametric methods such as Light Gradient-Boosting Machine. CONCLUSIONS: In short, we demonstrated that AEquity is a robust tool by applying it to different datasets, algorithms, and intersectional analyses and measuring its effectiveness with respect to a range of traditional fairness metrics.",
      "authors": "Gulamali Faris; Sawant Ashwin Shreekant; Liharska Lora; Horowitz Carol; Chan Lili; Hofer Ira; Singh Karandeep; Richardson Lynne; Mensah Emmanuel; Charney Alexander; Reich David; Hu Jianying; Nadkarni Girish",
      "year": "2025",
      "journal": "Journal of medical Internet research",
      "doi": "10.2196/71757",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40905712/",
      "mesh_terms": "Bias; Humans; Datasets as Topic; Routinely Collected Health Data; Radiography, Thoracic; Health Care Costs; Black or African American; Medicaid; Race Factors; Social Class; Nutrition Surveys; Machine Learning; Data Curation; Mortality; United States",
      "keywords": "bias; data-centric artificial intelligence; fairness; machine learning",
      "pub_types": "Journal Article; Validation Study",
      "pmcid": "PMC12410029",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "29104450",
      "title": "Optimizing Variance-Bias Trade-off in the TWANG Package for Estimation of Propensity Scores.",
      "abstract": "While propensity score weighting has been shown to reduce bias in treatment effect estimation when selection bias is present, it has also been shown that such weighting can perform poorly if the estimated propensity score weights are highly variable. Various approaches have been proposed which can reduce the variability of the weights and the risk of poor performance, particularly those based on machine learning methods. In this study, we closely examine approaches to fine-tune one machine learning technique (generalized boosted models [GBM]) to select propensity scores that seek to optimize the variance-bias trade-off that is inherent in most propensity score analyses. Specifically, we propose and evaluate three approaches for selecting the optimal number of trees for the GBM in the twang package in R. Normally, the twang package in R iteratively selects the optimal number of trees as that which maximizes balance between the treatment groups being considered. Because the selected number of trees may lead to highly variable propensity score weights, we examine alternative ways to tune the number of trees used in the estimation of propensity score weights such that we sacrifice some balance on the pre-treatment covariates in exchange for less variable weights. We use simulation studies to illustrate these methods and to describe the potential advantages and disadvantages of each method. We apply these methods to two case studies: one examining the effect of dog ownership on the owner's general health using data from a large, population-based survey in California, and a second investigating the relationship between abstinence and a long-term economic outcome among a sample of high-risk youth.",
      "authors": "Parast Layla; McCaffrey Daniel F; Burgette Lane F; de la Guardia Fernando Hoces; Golinelli Daniela; Miles Jeremy N V; Griffin Beth Ann",
      "year": "2017",
      "journal": "Health services & outcomes research methodology",
      "doi": "10.1007/s10742-016-0168-2",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29104450/",
      "mesh_terms": "",
      "keywords": "causal inference; machine learning; propensity score",
      "pub_types": "Journal Article",
      "pmcid": "PMC5667923",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41107862",
      "title": "Navigating fairness aspects of clinical prediction models.",
      "abstract": "BACKGROUND: Algorithms are increasingly used in healthcare, yet most algorithms lack thorough evaluation and impact assessment across diverse populations. This absence of comprehensive scrutiny introduces a significant risk of inequitable clinical outcomes, particularly between different demographic and socioeconomic groups. MAIN BODY: Societal biases-rooted in structural inequalities and systemic discrimination-often shape the data used to develop these algorithms. When such biases become embedded into predictive models, algorithms frequently favor privileged populations, further deepening existing inequalities. Without proactive efforts to identify and mitigate these biases, algorithms risk disproportionately harming already marginalized groups, widening the gap between advantaged and disadvantaged patients. Various statistical metrics are available to assess algorithmic fairness, each addressing different dimensions of disparity in predictive performance across population groups. However, understanding and applying these fairness metrics in real-world healthcare settings remains limited. Transparency in both the development and communication of algorithms is essential to building a more equitable healthcare system. Openly addressing fairness concerns fosters trust and accountability, ensuring that fairness considerations become an integral part of algorithm design and implementation rather than an afterthought. Using a participatory approach involving three clinicians and three patients with lived experience of type 2 diabetes, we developed a set of guiding questions to help healthcare professionals assess algorithms critically, challenge existing practices, and stimulate discussions. CONCLUSIONS: We aim to direct healthcare professionals on navigating the complexities of bias in healthcare algorithms by encouraging critical thinking about biases present in society, data, algorithms, and healthcare systems.",
      "authors": "Chakradeo Kaustubh; Huynh Inchuen; Balaganeshan Sedrah B; Dollerup Ole L; Gade-J\u00f8rgensen Hj\u00f8rdis; Laupstad Susanne K; Malham Mikkel; Nguyen Tri-Long; Hulman Adam; Varga Tibor V",
      "year": "2025",
      "journal": "BMC medicine",
      "doi": "10.1186/s12916-025-04340-3",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41107862/",
      "mesh_terms": "Humans; Algorithms; Diabetes Mellitus, Type 2; Healthcare Disparities",
      "keywords": "Algorithmic fairness; Algorithms; Clinical decision rules; Clinical decision-making; Delivery of healthcare; Fairness; Health inequities; Health personnel; Prediction algorithms; Risk factors",
      "pub_types": "Journal Article",
      "pmcid": "PMC12535043",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40718760",
      "title": "A fair machine learning model to predict flares of systemic lupus erythematosus.",
      "abstract": "OBJECTIVE: Systemic lupus erythematosus (SLE) is a chronic autoimmune disease that disproportionately affects women and racial/ethnic minority groups. Predicting disease flares is essential for improving patient outcomes, yet few studies integrate both clinical and social determinants of health (SDoH). We therefore developed FLAME (FLAre Machine learning prediction of SLE), a machine learning pipeline that uses electronic health records (EHRs) and contextual-level SDoH to predict 3-month flare risk, emphasizing explainability and fairness. MATERIALS AND METHODS: We conducted a retrospective cohort study of 28\u2009433 patients with SLE from the University of Florida Health (2011-2022), linked to 675 contextual-level SDoH variables. We used XGBoost and logistic regression models to predict 3-month flare risk, evaluating model performance using the area under the receiver operating characteristic (AUROC). We applied SHapley Additive exPlanations (SHAP) values and causal structure learning to identify key predictors. Fairness was assessed using the equality of opportunity metric, measured by the false-negative rate across racial/ethnic groups. RESULTS: The FLAME model, incorporating clinical and contextual-level SDoH, achieved an AUROC of 0.66. The clinical-only model performed slightly better (AUROC of 0.67), while the SDoH-only model had lower performance (AUROC of 0.54). SHAP analysis identified headache, organic brain syndrome, and pyuria as key predictors. Causal learning revealed interactions between clinical factors and contextual-level SDoH. Fairness assessments showed no significant biases across groups. DISCUSSION: FLAME offers a fair and interpretable approach to predicting SLE flares, providing meaningful insights that may guide future clinical interventions. CONCLUSIONS: FLAME shows promise as an EHR-based tool to support personalized, equitable, and holistic SLE care.",
      "authors": "Li Yongqiu; Yao Lixia; Lee Yao An; Huang Yu; Merkel Peter A; Vina Ernest; Yeh Ya-Yun; Li Yujia; Allen John M; Bian Jiang; Guo Jingchuan",
      "year": "2025",
      "journal": "JAMIA open",
      "doi": "10.1093/jamiaopen/ooaf072",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40718760/",
      "mesh_terms": "",
      "keywords": "fairness; machine learning; prediction; social determinants of health; systemic lupus erythematosus",
      "pub_types": "Journal Article",
      "pmcid": "PMC12296391",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39252056",
      "title": "Clinician voices on ethics of LLM integration in healthcare: a thematic analysis of ethical concerns and implications.",
      "abstract": "OBJECTIVES: This study aimed to explain and categorize key ethical concerns about integrating large language models (LLMs) in healthcare, drawing particularly from the perspectives of clinicians in online discussions. MATERIALS AND METHODS: We analyzed 3049 posts and comments extracted from a self-identified clinician subreddit using unsupervised machine learning via Latent Dirichlet Allocation and a structured qualitative analysis methodology. RESULTS: Analysis uncovered 14 salient themes of ethical implications, which we further consolidated into 4 overarching domains reflecting ethical issues around various clinical applications of LLM in healthcare, LLM coding, algorithm, and data governance, LLM's role in health equity and the distribution of public health services, and the relationship between users (human) and LLM systems (machine). DISCUSSION: Mapping themes to ethical frameworks in literature illustrated multifaceted issues covering transparent LLM decisions, fairness, privacy, access disparities, user experiences, and reliability. CONCLUSION: This study emphasizes the need for ongoing ethical review from stakeholders to ensure responsible innovation and advocates for tailored governance to enhance LLM use in healthcare, aiming to improve clinical outcomes ethically and effectively.",
      "authors": "Mirzaei Tala; Amini Leila; Esmaeilzadeh Pouyan",
      "year": "2024",
      "journal": "BMC medical informatics and decision making",
      "doi": "10.1186/s12911-024-02656-3",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39252056/",
      "mesh_terms": "Humans; Attitude of Health Personnel; Delivery of Health Care; Qualitative Research",
      "keywords": "Artificial Intelligence; Ethics; LLM; Thematic analysis; Theme",
      "pub_types": "Journal Article",
      "pmcid": "PMC11382443",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40721333",
      "title": "Predicting Missed Appointments in Primary Care: A Personalized Machine Learning Approach.",
      "abstract": "PURPOSE: Factors influencing missed appointments are complex and difficult to anticipate and intervene against. To optimize appointment adherence, we aimed to use personalized machine learning and big data analytics to predict the risk of and contributing factors for no-shows and late cancellations in primary care practices. METHODS: We conducted a retrospective longitudinal study leveraging geolinked clinical, care utilization, socioeconomic, and climate data from 15 family medicine clinics at a regional academic health center in Pennsylvania from January 2019 to June 2023. We developed multiclass machine learning models using gradient boost, random forest, neural network, and logistic regression to predict appointment outcomes, followed by feature importance analysis to identify contributing factors for no-shows or late cancellations at the population and patient levels. We performed stratified analysis to evaluate the prediction performance by sex and race/ethnicity to ensure the fairness of the final model among sensitive features. RESULTS: The analysis consisted of 109,328 patients and 1,118,236 appointments, including 77,322 (6.9%) no-shows and 75,545 (6.8%) late cancellations. The gradient boost model achieved the best performance with an area under the receiver operating characteristic curve of 0.852 for predicting no-shows and 0.921 for late cancellations. No bias against patient characteristics was detected. Schedule lead time was identified as the most important predictor of missed appointments. CONCLUSIONS: Missed appointments remain a challenge for primary care. This study provided a practical and robust framework to predict missed appointments, laying the foundation for developing personalized strategies to improve patients' adherence to primary care appointments.",
      "authors": "Tuan Wen-Jan; Yan Yifang; Abou Al Ardat Bilal; Felix Todd; Chen Qiushi",
      "year": "2025",
      "journal": "Annals of family medicine",
      "doi": "10.1370/afm.240316",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40721333/",
      "mesh_terms": "Humans; Machine Learning; Primary Health Care; Male; Retrospective Studies; Female; Appointments and Schedules; Middle Aged; No-Show Patients; Longitudinal Studies; Pennsylvania; Adult; Aged; Logistic Models",
      "keywords": "continuity of care; health disparity; late cancellation; machine learning; no-show",
      "pub_types": "Journal Article",
      "pmcid": "PMC12306982",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38531676",
      "title": "Preparing for the bedside-optimizing a postpartum depression risk prediction model for clinical implementation in a health system.",
      "abstract": "OBJECTIVE: We developed and externally validated a machine-learning model to predict postpartum depression (PPD) using data from electronic health records (EHRs). Effort is under way to implement the PPD prediction model within the EHR system for clinical decision support. We describe the pre-implementation evaluation process that considered model performance, fairness, and clinical appropriateness. MATERIALS AND METHODS: We used EHR data from an academic medical center (AMC) and a clinical research network database from 2014 to 2020 to evaluate the predictive performance and net benefit of the PPD risk model. We used area under the curve and sensitivity as predictive performance and conducted a decision curve analysis. In assessing model fairness, we employed metrics such as disparate impact, equal opportunity, and predictive parity with the White race being the privileged value. The model was also reviewed by multidisciplinary experts for clinical appropriateness. Lastly, we debiased the model by comparing 5 different debiasing approaches of fairness through blindness and reweighing. RESULTS: We determined the classification threshold through a performance evaluation that prioritized sensitivity and decision curve analysis. The baseline PPD model exhibited some unfairness in the AMC data but had a fair performance in the clinical research network data. We revised the model by fairness through blindness, a debiasing approach that yielded the best overall performance and fairness, while considering clinical appropriateness suggested by the expert reviewers. DISCUSSION AND CONCLUSION: The findings emphasize the need for a thorough evaluation of intervention-specific models, considering predictive performance, fairness, and appropriateness before clinical implementation.",
      "authors": "Liu Yifan; Joly Rochelle; Reading Turchioe Meghan; Benda Natalie; Hermann Alison; Beecy Ashley; Pathak Jyotishman; Zhang Yiye",
      "year": "2024",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "doi": "10.1093/jamia/ocae056",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38531676/",
      "mesh_terms": "Humans; Depression, Postpartum; Female; Electronic Health Records; Machine Learning; Risk Assessment; Decision Support Systems, Clinical",
      "keywords": "electronic health record; health equity; machine learning implementation; postpartum depression",
      "pub_types": "Journal Article",
      "pmcid": "PMC11105144",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "34812384",
      "title": "Bias Analysis on Public X-Ray Image Datasets of Pneumonia and COVID-19 Patients.",
      "abstract": "Chest X-ray images are useful for early COVID-19 diagnosis with the advantage that X-ray devices are already available in health centers and images are obtained immediately. Some datasets containing X-ray images with cases (pneumonia or COVID-19) and controls have been made available to develop machine-learning-based methods to aid in diagnosing the disease. However, these datasets are mainly composed of different sources coming from pre-COVID-19 datasets and COVID-19 datasets. Particularly, we have detected a significant bias in some of the released datasets used to train and test diagnostic systems, which might imply that the results published are optimistic and may overestimate the actual predictive capacity of the techniques proposed. In this article, we analyze the existing bias in some commonly used datasets and propose a series of preliminary steps to carry out before the classic machine learning pipeline in order to detect possible biases, to avoid them if possible and to report results that are more representative of the actual predictive power of the methods under analysis.",
      "authors": "Catala Omar Del Tejo; Igual Ismael Salvador; Perez-Benito Francisco Javier; Escriva David Millan; Castello Vicent Ortiz; Llobet Rafael; Perez-Cortes Juan-Carlos",
      "year": "2021",
      "journal": "IEEE access : practical innovations, open solutions",
      "doi": "10.1109/ACCESS.2021.3065456",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34812384/",
      "mesh_terms": "",
      "keywords": "COVID-19; Deep learning; bias; chest X-ray; convolutional neural networks; saliency map; segmentation",
      "pub_types": "Journal Article",
      "pmcid": "PMC8545228",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35401342",
      "title": "From Cognitive Bias Toward Advanced Computational Intelligence for Smart Infrastructure Monitoring.",
      "abstract": "Visual inspections have been typically used in condition assessment of infrastructure. However, they are based on human judgment and their interpretation of data can differ from acquired results. In psychology, this difference is called cognitive bias which directly affects Structural Health Monitoring (SHM)-based decision making. Besides, the confusion between condition state and safety of a bridge is another example of cognitive bias in bridge monitoring. Therefore, integrated computer-based approaches as powerful tools can be significantly applied in SHM systems. This paper explores the relationship between the use of advanced computational intelligence and the development of SHM solutions through conducting an infrastructure monitoring methodology. Artificial Intelligence (AI)-based algorithms, i.e., Artificial Neural Network (ANN), hybrid ANN-based Imperial Competitive Algorithm, and hybrid ANN-based Genetic Algorithm, are developed for damage assessment using a lab-scale composite bridge deck structure. Based on the comparison of the results, the employed evolutionary algorithms could improve the prediction error of the pre-developed network by enhancing the learning procedure of the ANN.",
      "authors": "Gordan Meisam; Chao Ong Zhi; Sabbagh-Yazdi Saeed-Reza; Wee Lai Khin; Ghaedi Khaled; Ismail Zubaidah",
      "year": "2022",
      "journal": "Frontiers in psychology",
      "doi": "10.3389/fpsyg.2022.846610",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35401342/",
      "mesh_terms": "",
      "keywords": "artificial intelligence; bridge monitoring; cognitive bias; infrastructure health monitoring; safety",
      "pub_types": "Journal Article",
      "pmcid": "PMC8990332",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35396247",
      "title": "Evaluating algorithmic fairness in the presence of clinical guidelines: the case of atherosclerotic cardiovascular disease risk estimation.",
      "abstract": "OBJECTIVES: The American College of Cardiology and the American Heart Association guidelines on primary prevention of atherosclerotic cardiovascular disease (ASCVD) recommend using 10-year ASCVD risk estimation models to initiate statin treatment. For guideline-concordant decision-making, risk estimates need to be calibrated. However, existing models are often miscalibrated for race, ethnicity and sex based subgroups. This study evaluates two algorithmic fairness approaches to adjust the risk estimators (group recalibration and equalised odds) for their compatibility with the assumptions underpinning the guidelines' decision rules.MethodsUsing an updated pooled cohorts data set, we derive unconstrained, group-recalibrated and equalised odds-constrained versions of the 10-year ASCVD risk estimators, and compare their calibration at guideline-concordant decision thresholds. RESULTS: We find that, compared with the unconstrained model, group-recalibration improves calibration at one of the relevant thresholds for each group, but exacerbates differences in false positive and false negative rates between groups. An equalised odds constraint, meant to equalise error rates across groups, does so by miscalibrating the model overall and at relevant decision thresholds. DISCUSSION: Hence, because of induced miscalibration, decisions guided by risk estimators learned with an equalised odds fairness constraint are not concordant with existing guidelines. Conversely, recalibrating the model separately for each group can increase guideline compatibility, while increasing intergroup differences in error rates. As such, comparisons of error rates across groups can be misleading when guidelines recommend treating at fixed decision thresholds. CONCLUSION: The illustrated tradeoffs between satisfying a fairness criterion and retaining guideline compatibility underscore the need to evaluate models in the context of downstream interventions.",
      "authors": "Foryciarz Agata; Pfohl Stephen R; Patel Birju; Shah Nigam",
      "year": "2022",
      "journal": "BMJ health & care informatics",
      "doi": "10.1136/bmjhci-2021-100460",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35396247/",
      "mesh_terms": "American Heart Association; Atherosclerosis; Cardiology; Cardiovascular Diseases; Humans; Hydroxymethylglutaryl-CoA Reductase Inhibitors; United States",
      "keywords": "BMJ Health Informatics; clinical; decision support systems; health equity; machine learning; medical informatics",
      "pub_types": "Journal Article",
      "pmcid": "PMC8996004",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40026843",
      "title": "Equitable hospital length of stay prediction for patients with learning disabilities and multiple long-term conditions using machine learning.",
      "abstract": "PURPOSE: Individuals with learning disabilities (LD) often face higher rates of premature mortality and prolonged hospital stays compared to the general population. Predicting the length of stay (LOS) for patients with LD and multiple long-term conditions (MLTCs) is critical for improving patient care and optimising medical resource allocation. However, there is limited research on the application of machine learning (ML) models to this population. Furthermore, approaches designed for the general population often lack generalisability and fairness, particularly when applied across sensitive groups within their cohort. METHOD: This study analyses hospitalisations of 9,618 patients with LD in Wales using electronic health records (EHR) from the SAIL Databank. A Random Forest (RF) ML model was developed to predict hospital LOS, incorporating demographics, medication history, lifestyle factors, and 39 long-term conditions. To address fairness concerns, two bias mitigation techniques were applied: a post-processing threshold optimiser and an in-processing reductions method using an exponentiated gradient. These methods aimed to minimise performance discrepancies across ethnic groups while ensuring robust model performance. RESULTS: The RF model outperformed other state-of-the-art models, achieving an area under the curve of 0.759 for males and 0.756 for females, a false negative rate of 0.224 for males and 0.229 for females, and a balanced accuracy of 0.690 for males and 0.689 for females. Bias mitigation algorithms reduced disparities in prediction performance across ethnic groups, with the threshold optimiser yielding the most notable improvements. Performance metrics, including false positive rate and balanced accuracy, showed significant enhancements in fairness for the male cohort. CONCLUSION: This study demonstrates the feasibility of applying ML models to predict LOS for patients with LD and MLTCs, while addressing fairness through bias mitigation techniques. The findings highlight the potential for equitable healthcare predictions using EHR data, paving the way for improved clinical decision-making and resource management.",
      "authors": "Abakasanga Emeka; Kousovista Rania; Cosma Georgina; Akbari Ashley; Zaccardi Francesco; Kaur Navjot; Fitt Danielle; Jun Gyuchan Thomas; Kiani Reza; Gangadharan Satheesh",
      "year": "2025",
      "journal": "Frontiers in digital health",
      "doi": "10.3389/fdgth.2025.1538793",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40026843/",
      "mesh_terms": "",
      "keywords": "bias mitigation; exponentiated gradient; learning disabilities; length of stay; threshold optimiser",
      "pub_types": "Journal Article",
      "pmcid": "PMC11868268",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40577645",
      "title": "A Responsible Framework for Assessing, Selecting, and Explaining Machine Learning Models in Cardiovascular Disease Outcomes Among People With Type 2 Diabetes: Methodology and Validation Study.",
      "abstract": "BACKGROUND: Building machine learning models that are interpretable, explainable, and fair is critical for their trustworthiness in clinical practice. Interpretability, which refers to how easily a human can comprehend the mechanism by which a model makes predictions, is often seen as a primary consideration when adopting a machine learning model in health care. However, interpretability alone does not necessarily guarantee explainability, which offers stakeholders insights into a model's predicted outputs. Moreover, many existing frameworks for model evaluation focus primarily on maximizing predictive accuracy, overlooking the broader need for interpretability, fairness, and explainability. OBJECTIVE: This study proposes a 3-stage machine learning framework for responsible model development through model assessment, selection, and explanation. We demonstrate the application of this framework for predicting cardiovascular disease (CVD) outcomes, specifically myocardial infarction (MI) and stroke, among people with type 2 diabetes (T2D). METHODS: We extracted participant data comprised of people with T2D from the ACCORD (Action to Control Cardiovascular Risk in Diabetes) dataset (N=9635), including demographic, clinical, and biomarker records. Then, we applied hold-out cross-validation to develop several interpretable machine learning models (linear, tree-based, and ensemble) to predict the risks of MI and stroke among patients with diabetes. Our 3-stage framework first assesses these models via predictive accuracy and fairness metrics. Then, in the model selection stage, we quantify the trade-off between accuracy and fairness using area under the curve (AUC) and Relative Parity of Performance Scores (RPPS), wherein RPPS measures the greatest deviation of all subpopulations compared with the population-wide AUC. Finally, we quantify the explainability of the chosen models using methods such as SHAP (Shapley Additive Explanations) and partial dependence plots to investigate the relationship between features and model outputs. RESULTS: Our proposed framework demonstrates that the GLMnet model offers the best balance between predictive performance and fairness for both MI and stroke. For MI, GLMnet achieves the highest RPPS (0.979 for gender and 0.967 for race), indicating minimal performance disparities, while maintaining a high AUC of 0.705. For stroke, GLMnet has a relatively high AUC of 0.705 and the second-highest RPPS (0.961 for gender and 0.979 for race), suggesting it is effective across both subgroups. Our model explanation method further highlights that the history of CVD and age are the key predictors of MI, while HbA1c and systolic blood pressure significantly influence stroke classification. CONCLUSIONS: This study establishes a responsible framework for assessing, selecting, and explaining machine learning models, emphasizing accuracy-fairness trade-offs in predictive modeling. Key insights include: (1) simple models perform comparably to complex ensembles; (2) models with strong accuracy may harbor substantial differences in accuracy across demographic groups; and (3) explanation methods reveal the relationships between features and risk for MI and stroke. Our results underscore the need for holistic approaches that consider accuracy, fairness, and explainability in interpretable model design and selection, potentially enhancing health care technology adoption.",
      "authors": "Yang Yang; Liao Che-Yi; Keyvanshokooh Esmaeil; Shao Hui; Weber Mary Beth; Pasquel Francisco J; Garcia Gian-Gabriel P",
      "year": "2025",
      "journal": "JMIR medical informatics",
      "doi": "10.2196/66200",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40577645/",
      "mesh_terms": "Humans; Machine Learning; Diabetes Mellitus, Type 2; Cardiovascular Diseases; Male; Female; Middle Aged; Aged; Risk Assessment",
      "keywords": "MI; T2D; cardiology; cardiovascular; cardiovascular disease; clinical practice; diabetes; explainability; fairness; interpretable machine learning; machine learning; myocardial infarction; prediction; responsible framework; stroke; type 2 diabetes",
      "pub_types": "Journal Article; Validation Study",
      "pmcid": "PMC12256707",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40726749",
      "title": "Empirical Comparison of Post-processing Debiasing Methods for Machine Learning Classifiers in Healthcare.",
      "abstract": "UNLABELLED: Machine learning classifiers in healthcare tend to reproduce or exacerbate existing health disparities due to inherent biases in training data. This relevant issue has brought the attention of researchers in both healthcare and other domains, proposing techniques that deal with it in different stages of the machine learning process. Post-processing methods adjust model predictions to ensure fairness without interfering in the learning process nor requiring access to the original training data, preserving privacy and enabling the application to any trained model. This study rigorously compares state-of-the-art debiasing methods within the family of post-processing techniques across a wide range of synthetic and real-world (healthcare) datasets, by means of different performance and fairness metrics. Our experiments reveal the strengths and weaknesses of each method, examining the trade-offs between group fairness and predictive performance, as well as among different notions of group fairness. Additionally, we analyze the impact on untreated attributes to ensure overall bias mitigation. Our comprehensive evaluation provides insights into how these debiasing methods can be optimally implemented in healthcare settings to balance accuracy and fairness. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007/s41666-025-00196-7.",
      "authors": "Dang Vien Ngoc; Campello V\u00edctor M; Hern\u00e1ndez-Gonz\u00e1lez Jer\u00f3nimo; Lekadir Karim",
      "year": "2025",
      "journal": "Journal of healthcare informatics research",
      "doi": "10.1007/s41666-025-00196-7",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40726749/",
      "mesh_terms": "",
      "keywords": "Algorithmic bias; Fairness; Healthcare; Machine learning classifiers; Post-processing",
      "pub_types": "Journal Article",
      "pmcid": "PMC12290158",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39974004",
      "title": "Algorithms to Improve Fairness in Medicare Risk Adjustment.",
      "abstract": "IMPORTANCE: Payment system design creates incentives that impact healthcare spending, access, and outcomes. With Medicare Advantage accounting for more than half of Medicare spending, changes to its risk adjustment algorithm have the potential for broad consequences. OBJECTIVE: To develop risk adjustment algorithms that can achieve fair spending targets, and compare their performance to a baseline that emulates the least squares regression approach used by the Centers for Medicare and Medicaid Services. DESIGN: Retrospective analysis of Traditional Medicare enrollment and claims data between January 2017 and December 2020. Diagnoses in claims were mapped to Hierarchical Condition Categories (HCCs). Algorithms used demographic indicators and HCCs from one calendar year to predict Medicare spending in the subsequent year. SETTING: Data from Medicare beneficiaries with documented residence in the United States or Puerto Rico. PARTICIPANTS: A random 20% sample of beneficiaries enrolled in Traditional Medicare. Included beneficiaries were aged 65 years and older, and did not have Medicaid dual eligibility. Race/ethnicity was assigned using the Research Triangle Institute enhanced indicator. MAIN OUTCOME AND MEASURES: Prospective healthcare spending by Medicare. Overall performance was measured by payment system fit and mean absolute error. Net compensation was used to assess group-level fairness. RESULTS: The main analysis included 4,398,035 Medicare beneficiaries with a mean age of 75.2 years and mean annual Medicare spending of $8,345. Out-of-sample payment system fit for the baseline regression was 12.7%. Constrained regression and post-processing both achieved fair spending targets, while maintaining payment system fit values of 12.6% and 12.7%, respectively. Whereas post-processing only increased mean payments for beneficiaries in minoritized racial/ethnic groups, constrained regression increased mean payments for beneficiaries in minoritized racial/ethnic groups and beneficiaries in other groups residing in counties with greater exposure to socioeconomic factors that can adversely affect health outcomes. CONCLUSIONS AND RELEVANCE: Constrained regression and post-processing can incorporate fairness objectives in the Medicare risk adjustment algorithm with minimal reduction in overall fit.",
      "authors": "Reitsma Marissa B; McGuire Thomas G; Rose Sherri",
      "year": "2025",
      "journal": "medRxiv : the preprint server for health sciences",
      "doi": "10.1101/2025.01.25.25321057",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39974004/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article; Preprint",
      "pmcid": "PMC11838972",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39441784",
      "title": "Conceptualizing bias in EHR data: A case study in performance disparities by demographic subgroups for a pediatric obesity incidence classifier.",
      "abstract": "Electronic Health Records (EHRs) are increasingly used to develop machine learning models in predictive medicine. There has been limited research on utilizing machine learning methods to predict childhood obesity and related disparities in classifier performance among vulnerable patient subpopulations. In this work, classification models are developed to recognize pediatric obesity using temporal condition patterns obtained from patient EHR data in a U.S. study population. We trained four machine learning algorithms (Logistic Regression, Random Forest, Gradient Boosted Trees, and Neural Networks) to classify cases and controls as obesity positive or negative, and optimized hyperparameter settings through a bootstrapping methodology. To assess the classifiers for bias, we studied model performance by population subgroups then used permutation analysis to identify the most predictive features for each model and the demographic characteristics of patients with these features. Mean AUC-ROC values were consistent across classifiers, ranging from 0.72-0.80. Some evidence of bias was identified, although this was through the models performing better for minority subgroups (African Americans and patients enrolled in Medicaid). Permutation analysis revealed that patients from vulnerable population subgroups were over-represented among patients with the most predictive diagnostic patterns. We hypothesize that our models performed better on under-represented groups because the features more strongly associated with obesity were more commonly observed among minority patients. These findings highlight the complex ways that bias may arise in machine learning models and can be incorporated into future research to develop a thorough analytical approach to identify and mitigate bias that may arise from features and within EHR datasets when developing more equitable models.",
      "authors": "Campbell Elizabeth A; Bose Saurav; Masino Aaron J",
      "year": "2024",
      "journal": "PLOS digital health",
      "doi": "10.1371/journal.pdig.0000642",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39441784/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC11498669",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40825541",
      "title": "Efficient Detection of Stigmatizing Language in Electronic Health Records via In-Context Learning: Comparative Analysis and Validation Study.",
      "abstract": "BACKGROUND: The presence of stigmatizing language within electronic health records (EHRs) poses significant risks to patient care by perpetuating biases. While numerous studies have explored the use of supervised machine learning models to detect stigmatizing language automatically, these models require large, annotated datasets, which may not always be readily available. In-context learning (ICL) has emerged as a data-efficient alternative, allowing large language models to adapt to tasks using only instructions and examples. OBJECTIVE: We aimed to investigate the efficacy of ICL in detecting stigmatizing language within EHRs under data-scarce conditions. METHODS: We analyzed 5043 sentences from the Medical Information Mart for Intensive Care-IV dataset, which contains EHRs from patients admitted to the emergency department at the Beth Israel Deaconess Medical Center. We compared ICL with zero-shot (textual entailment), few-shot (SetFit), and supervised fine-tuning approaches. The ICL approach used 4 prompting strategies: generic, chain of thought, clue and reasoning prompting, and a newly introduced stigma detection guided prompt. Model fairness was evaluated using the equal performance criterion, measuring true positive rate, false positive rate, and F1-score disparities across protected attributes, including sex, age, and race. RESULTS: In the zero-shot setting, the best-performing ICL model, GEMMA-2, achieved a mean F1-score of 0.858 (95% CI 0.854-0.862), showing an 18.7% improvement over the best textual entailment model, DEBERTA-M (mean F1-score 0.723, 95% CI 0.718-0.728; P<.001). In the few-shot setting, the top ICL model, LLAMA-3, outperformed the leading SetFit models by 21.2%, 21.4%, and 12.3% with 4, 8, and 16 annotations per class, respectively (P<.001). Using 32 labeled instances, the best ICL model achieved a mean F1-score of 0.901 (95% CI 0.895-0.907), only 3.2% lower than the best supervised fine-tuning model, ROBERTA (mean F1-score 0.931, 95% CI 0.924-0.938), which was trained on 3543 labeled instances. Under the conditions tested, fairness evaluation revealed that supervised fine-tuning models exhibited greater bias compared with ICL models in the zero-shot, 4-shot, 8-shot, and 16-shot settings, as measured by true positive rate, false positive rate, and F1-score disparities. CONCLUSIONS: ICL offers a robust and flexible solution for detecting stigmatizing language in EHRs, offering a more data-efficient and equitable alternative to conventional machine learning methods. These findings suggest that ICL could enhance bias detection in clinical documentation while reducing the reliance on extensive labeled datasets.",
      "authors": "Chen Hongbo; Alfred Myrtede; Cohen Eldan",
      "year": "2025",
      "journal": "JMIR medical informatics",
      "doi": "10.2196/68955",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40825541/",
      "mesh_terms": "Humans; Electronic Health Records; Machine Learning; Language; Social Stigma; Stereotyping; Male; Female",
      "keywords": "artificial intelligence; electronic health record; fairness; few-shot; in-context learning; large language model; machine learning; prompting strategy; stigmatizing language; text classification; zero-shot",
      "pub_types": "Journal Article; Comparative Study; Validation Study",
      "pmcid": "PMC12402740",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40152140",
      "title": "Predicting postoperative chronic opioid use with fair machine learning models integrating multi-modal data sources: a demonstration of ethical machine learning in healthcare.",
      "abstract": "OBJECTIVE: Building upon our previous work on predicting chronic opioid use using electronic health records (EHR) and wearable data, this study leveraged the Health Equity Across the AI Lifecycle (HEAAL) framework to (a) fine tune the previously built model with genomic data and evaluate model performance in predicting chronic opioid use and (b) apply IBM's AIF360 pre-processing toolkit to mitigate bias related to gender and race and evaluate the model performance using various fairness metrics. MATERIALS AND METHODS: Participants included approximately 271 All of Us Research Program subjects with EHR, wearable, and genomic data. We fine-tuned 4 machine learning models on the new dataset. The SHapley Additive exPlanations (SHAP) technique identified the best-performing predictors. A preprocessing toolkit boosted fairness by gender and race. RESULTS: The genetic data enhanced model performance from the prior model, with the area under the curve improving from 0.90 (95% CI, 0.88-0.92) to 0.95 (95% CI, 0.89-0.95). Key predictors included Dopamine D1 Receptor (DRD1) rs4532, general type of surgery, and time spent in physical activity. The reweighing preprocessing technique applied to the stacking algorithm effectively improved the model's fairness across racial and gender groups without compromising performance. CONCLUSION: We leveraged 2 dimensions of the HEAAL framework to build a fair artificial intelligence (AI) solution. Multi-modal datasets (including wearable and genetic data) and applying bias mitigation strategies can help models to more fairly and accurately assess risk across diverse populations, promoting fairness in AI in healthcare.",
      "authors": "Soley Nidhi; Rattsev Ilia; Speed Traci J; Xie Anping; Ferryman Kadija S; Taylor Casey Overby",
      "year": "2025",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "doi": "10.1093/jamia/ocaf053",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40152140/",
      "mesh_terms": "Humans; Machine Learning; Electronic Health Records; Male; Female; Opioid-Related Disorders; Postoperative Pain; Wearable Electronic Devices; Analgesics, Opioid; Middle Aged; Adult; Information Sources",
      "keywords": "All of Us; chronic opioid use; ethical machine learning; multimodal dataset; responsible AI",
      "pub_types": "Journal Article",
      "pmcid": "PMC12089784",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38725085",
      "title": "Preoperative prediction model for risk of readmission after total joint replacement surgery: a random forest approach leveraging NLP and unfairness mitigation for improved patient care and cost-effectiveness.",
      "abstract": "BACKGROUND: The Center for Medicare and Medicaid Services (CMS) imposes payment penalties for readmissions following total joint replacement surgeries. This study focuses on total hip, knee, and shoulder arthroplasty procedures as they account for most joint replacement surgeries. Apart from being a burden to healthcare systems, readmissions are also troublesome for patients. There are several studies which only utilized structured data from Electronic Health Records (EHR) without considering any gender and payor bias adjustments. METHODS: For this study, dataset of 38,581 total knee, hip, and shoulder replacement surgeries performed from 2015 to 2021 at Novant Health was gathered. This data was used to train a random forest machine learning model to predict the combined endpoint of emergency department (ED) visit or unplanned readmissions within 30 days of discharge or discharge to Skilled Nursing Facility (SNF) following the surgery. 98 features of laboratory results, diagnoses, vitals, medications, and utilization history were extracted. A natural language processing (NLP) model finetuned from Clinical BERT was used to generate an NLP risk score feature for each patient based on their clinical notes. To address societal biases, a feature bias analysis was performed in conjunction with propensity score matching. A threshold optimization algorithm from the Fairlearn toolkit was used to mitigate gender and payor biases to promote fairness in predictions. RESULTS: The model achieved an Area Under the Receiver Operating characteristic Curve (AUROC) of 0.738 (95% confidence interval, 0.724 to 0.754) and an Area Under the Precision-Recall Curve (AUPRC) of 0.406 (95% confidence interval, 0.384 to 0.433). Considering an outcome prevalence of 16%, these metrics indicate the model's ability to accurately discriminate between readmission and non-readmission cases within the context of total arthroplasty surgeries while adjusting patient scores in the model to mitigate bias based on patient gender and payor. CONCLUSION: This work culminated in a model that identifies the most predictive and protective features associated with the combined endpoint. This model serves as a tool to empower healthcare providers to proactively intervene based on these influential factors without introducing bias towards protected patient classes, effectively mitigating the risk of negative outcomes and ultimately improving quality of care regardless of socioeconomic factors.",
      "authors": "Digumarthi Varun; Amin Tapan; Kanu Samuel; Mathew Joshua; Edwards Bryan; Peterson Lisa A; Lundy Matthew E; Hegarty Karen E",
      "year": "2024",
      "journal": "Journal of orthopaedic surgery and research",
      "doi": "10.1186/s13018-024-04774-0",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38725085/",
      "mesh_terms": "Humans; Patient Readmission; Female; Male; Machine Learning; Aged; Cost-Benefit Analysis; Natural Language Processing; Middle Aged; Arthroplasty, Replacement, Knee; Arthroplasty, Replacement, Hip; Arthroplasty, Replacement; Risk Assessment; Preoperative Period; Aged, 80 and over; Quality Improvement; Random Forest",
      "keywords": "Classification; Fairlearn; Natural language processing; Orthopedic; Predictive model",
      "pub_types": "Journal Article",
      "pmcid": "PMC11084055",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41479444",
      "title": "Predicting Metabolic Dysfunction-Associated Steatotic Liver Disease using Machine Learning Methods.",
      "abstract": "BACKGROUND: Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD) affects ~33% of U.S. adults and is the most common chronic liver disease. Although often asymptomatic, progression can lead to cirrhosis. Early detection is important, as lifestyle interventions can prevent disease progression. We developed a fair, rigorous, and reproducible MASLD prediction model and compared it to prior methods using a large electronic health record database. METHODS: We evaluated LASSO logistic regression, random forest, XGBoost, and a neural network for MASLD prediction using clinical feature subsets, including the top 10 SHAP-ranked features. To reduce disparities in true positive rates across racial and ethnic subgroups, we applied an equal opportunity postprocessing method. RESULTS: This study included 59,492 patients in the training data, 24,198 in the validating data, and 25,188 in the testing data. The LASSO logistic regression model with the top 10 features was selected for its interpretability and comparable performance. Before fairness adjustment, the model achieved AUROC of 0.84, accuracy of 78%, sensitivity of 72%, specificity of 79%, and F1-score of 0.617. After equal opportunity postprocessing, accuracy modestly increased to 81% and specificity to 94%, while sensitivity decreased to 41% and F1-score to 0.515, reflecting the fairness trade-off. CONCLUSIONS: We developed the MASER prediction model (MASLD Static EHR Risk Prediction), a LASSO logistic regression model which achieved competitive performance for MASLD prediction (AUROC 0.836, accuracy 77.6%), comparable to previously reported ensemble and tree-based models. Overall, this approach demonstrates that interpretable models can achieve a balance of predictive performance and fairness in diverse patient populations.",
      "authors": "An Mary Elena; Griffin Paul; Stine Jonathan G; Balakrishnan Ramakrishna; Kumara Soundar",
      "year": "2025",
      "journal": "ArXiv",
      "doi": "10.3350/cmh.2024.0431",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41479444/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article; Preprint",
      "pmcid": "PMC12755246",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40940655",
      "title": "FanFAIR: sensitive data sets semi-automatic fairness assessment.",
      "abstract": "BACKGROUND: Research has shown how data sets convey social bias in Artificial Intelligence systems, especially those based on machine learning. A biased data set is not representative of reality and might contribute to perpetuate societal biases within the model. To tackle this problem, it is important to understand how to avoid biases, errors, and unethical practices while creating the data sets. In order to provide guidance for the use of data sets in contexts of critical decision-making, such as health decisions, we identified six fundamental data set features (balance, numerosity, unevenness, compliance, quality, incompleteness) that could affect model fairness. These features were the foundation for the FanFAIR framework. RESULTS: We extended the FanFAIR framework for the semi-automated evaluation of fairness in data sets, by combining statistical information on data with qualitative features. In particular, we present an improved version of FanFAIR which introduces novel outlier detection capabilities working in multivariate fashion, using two state-of-the-art methods: the Empirical Cumulative-distribution Outlier Detection (ECOD) and Isolation Forest. We also introduce a novel metric for data set balance, based on an entropy measure. CONCLUSION: We addressed the issue of how much (un)fairness can be included in a data set used for machine learning research, focusing on classification issues. We developed a rule-based approach based on fuzzy logic that combines these characteristics into a single score and enables a semi-automatic evaluation of a data set in algorithmic fairness research. Our tool produces a detailed visual report about the fairness of the data set. We show the effectiveness of FanFAIR by applying the method on two open data sets.",
      "authors": "Gallese Chiara; Scantamburlo Teresa; Manzoni Luca; Giannerini Simone; Nobile Marco S",
      "year": "2025",
      "journal": "BMC medical informatics and decision making",
      "doi": "10.1186/s12911-025-03184-4",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40940655/",
      "mesh_terms": "",
      "keywords": "Data bias; Fairness; Fuzzy logic; Trustworthy artificial intelligence",
      "pub_types": "Journal Article",
      "pmcid": "PMC12427094",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39901158",
      "title": "The Data Artifacts Glossary: a community-based repository for bias on health datasets.",
      "abstract": "BACKGROUND: The deployment of Artificial Intelligence (AI) in healthcare has the potential to transform patient care through improved diagnostics, personalized treatment plans, and more efficient resource management. However, the effectiveness and fairness of AI are critically dependent on the data it learns from. Biased datasets can lead to AI outputs that perpetuate disparities, particularly affecting social minorities and marginalized groups. OBJECTIVE: This paper introduces the \"Data Artifacts Glossary\", a dynamic, open-source framework designed to systematically document and update potential biases in healthcare datasets. The aim is to provide a comprehensive tool that enhances the transparency and accuracy of AI applications in healthcare and contributes to understanding and addressing health inequities. METHODS: Utilizing a methodology inspired by the Delphi method, a diverse team of experts conducted iterative rounds of discussions and literature reviews. The team synthesized insights to develop a comprehensive list of bias categories and designed the glossary's structure. The Data Artifacts Glossary was piloted using the MIMIC-IV dataset to validate its utility and structure. RESULTS: The Data Artifacts Glossary adopts a collaborative approach modeled on successful open-source projects like Linux and Python. Hosted on GitHub, it utilizes robust version control and collaborative features, allowing stakeholders from diverse backgrounds to contribute. Through a rigorous peer review process managed by community members, the glossary ensures the continual refinement and accuracy of its contents. The implementation of the Data Artifacts Glossary with the MIMIC-IV dataset illustrates its utility. It categorizes biases, and facilitates their identification and understanding. CONCLUSION: The Data Artifacts Glossary serves as a vital resource for enhancing the integrity of AI applications in healthcare by providing a mechanism to recognize and mitigate dataset biases before they impact AI outputs. It not only aids in avoiding bias in model development but also contributes to understanding and addressing the root causes of health disparities.",
      "authors": "Gameiro Rodrigo R; Woite Naira Link; Sauer Christopher M; Hao Sicheng; Fernandes Chrystinne Oliveira; Premo Anna E; Teixeira Alice Rangel; Resli Isabelle; Wong An-Kwok Ian; Celi Leo Anthony",
      "year": "2025",
      "journal": "Journal of biomedical science",
      "doi": "10.1186/s12929-024-01106-6",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39901158/",
      "mesh_terms": "Humans; Artificial Intelligence; Datasets as Topic; Bias",
      "keywords": "Artificial intelligence; Bias; Data Artifacts Glossary; Dataset; Health equity; Machine learning",
      "pub_types": "Journal Article",
      "pmcid": "PMC11792693",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39371141",
      "title": "Evaluating and Reducing Subgroup Disparity in AI Models: An Analysis of Pediatric COVID-19 Test Outcomes.",
      "abstract": "Artificial Intelligence (AI) fairness in healthcare settings has attracted significant attention due to the concerns to propagate existing health disparities. Despite ongoing research, the frequency and extent of subgroup fairness have not been sufficiently studied. In this study, we extracted a nationally representative pediatric dataset (ages 0-17, n=9,935) from the US National Health Interview Survey (NHIS) concerning COVID-19 test outcomes. For subgroup disparity assessment, we trained 50 models using five machine learning algorithms. We assessed the models' area under the curve (AUC) on 12 small (<15% of the total n) subgroups defined using social economic factors versus the on the overall population. Our results show that subgroup disparities were prevalent (50.7%) in the models. Subgroup AUCs were generally lower, with a mean difference of 0.01, ranging from -0.29 to +0.41. Notably, the disparities were not always statistically significant, with four out of 12 subgroups having statistically significant disparities across models. Additionally, we explored the efficacy of synthetic data in mitigating identified disparities. The introduction of synthetic data enhanced subgroup disparity in 57.7% of the models. The mean AUC disparities for models with synthetic data decreased on average by 0.03 via resampling and 0.04 via generative adverbial network methods.",
      "authors": "Libin Alexander; Treitler Jonah T; Vasaitis Tadas; Shao Yijun",
      "year": "2024",
      "journal": "medRxiv : the preprint server for health sciences",
      "doi": "10.1101/2024.09.18.24313889",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39371141/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article; Preprint",
      "pmcid": "PMC11451670",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40463575",
      "title": "Leveraging neighborhood-level Information to Improve Model Fairness in Predicting Prenatal Depression.",
      "abstract": "IMPORTANCE: Perinatal depression (PND) affects 10-20% of pregnant women, with significant racial disparities in prevalence, screening, and treatment. Neighborhood-level factors significantly influence PND risk, particularly among women of color, but current machine learning models using electronic medical records (EMRs) rarely incorporate neighborhood characteristics. OBJECTIVE: To determine whether integrating neighborhood-level information with EMRs improves fairness in PND prediction while identifying key neighborhood factors influencing model bias across racial/ethnic groups. DESIGN SETTING AND PARTICIPANTS: Study of 6,137 pregnant women who received care at a large urban academic hospital from 2010-2019, comprising 58% Non-Hispanic Black (NHB), 10% Non-Hispanic White (NHW), and 28% Hispanic (H) individuals, with depression status determined by PHQ-9 scores. EXPOSURES: 125 neighborhood-level factors from Chicago Health Atlas merged with 61 EMR features based on residential location. MAIN OUTCOMES AND MEASURES: Model performance (ROCAUC, PRAUC) and fairness metrics (disparate impact, equal opportunity difference, equalized odds). Feature importance analyzed using Shapley values and the impact of each neighborhood factor on model bias were evaluated. Results Models integrating neighborhood-level measures showed moderate predictive performance (ROCAUC: NHB 55%, NHW 57%, H 58%) while significantly improving fairness metrics compared to EMR-only models (p<0.05). Factors, such as suicide mortality rate and neighborhood safety rate, helped reduce bias. NHB women showed stronger correlations between PND risk factors and neighborhood variables compared to other groups. Most neighborhood factors had differential impacts across racial/ethnic groups, increasing bias for NHB women while reducing it for Hispanic women. CONCLUSIONS AND RELEVANCE: Incorporating neighborhood-level information enhances fairness in PND prediction while maintaining predictive capability. The differential impact of neighborhood factors across racial/ethnic groups highlights the importance of considering neighborhood context in clinical risk assessment to reduce disparities in prenatal depression care.",
      "authors": "Huang Yongchao; Alvernaz Suzanne; Kim Sage J; Maki Pauline M; Boyd Andrew D; Dai Yang; Bernab\u00e9 Beatriz Pe\u00f1alver",
      "year": "2025",
      "journal": "medRxiv : the preprint server for health sciences",
      "doi": "10.1101/2025.05.12.25327329",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40463575/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article; Preprint",
      "pmcid": "PMC12132113",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36601036",
      "title": "Imputation Strategies Under Clinical Presence: Impact on Algorithmic Fairness.",
      "abstract": "Biases have marked medical history, leading to unequal care affecting marginalised groups. The patterns of missingness in observational data often reflect these group discrepancies, but the algorithmic fairness implications of group-specific missingness are not well understood. Despite its potential impact, imputation is too often an overlooked preprocessing step. When explicitly considered, attention is placed on overall performance, ignoring how this preprocessing can reinforce groupspecific inequities. Our work questions this choice by studying how imputation affects downstream algorithmic fairness. First, we provide a structured view of the relationship between clinical presence mechanisms and groupspecific missingness patterns. Then, through simulations and real-world experiments, we demonstrate that the imputation choice influences marginalised group performance and that no imputation strategy consistently reduces disparities. Importantly, our results show that current practices may endanger health equity as similarly performing imputation strategies at the population level can affect marginalised groups differently. Finally, we propose recommendations for mitigating inequities that may stem from a neglected step of the machine learning pipeline.",
      "authors": "Jeanselme Vincent; De-Arteaga Maria; Zhang Zhe; Barrett Jessica; Tom Brian",
      "year": "2022",
      "journal": "Proceedings of machine learning research",
      "doi": "",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36601036/",
      "mesh_terms": "",
      "keywords": "Clinical Presence; Fairness; Imputation",
      "pub_types": "Journal Article",
      "pmcid": "PMC7614014",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40666330",
      "title": "Auditor Models to Suppress Poor AI Predictions Can Improve Human-AI Collaborative Performance.",
      "abstract": "OBJECTIVE: Healthcare decisions are increasingly made with the assistance of machine learning (ML). ML has been known to have unfairness - inconsistent outcomes across subpopulations. Clinicians interacting with these systems can perpetuate such unfairness by overreliance. Recent work exploring ML suppression - silencing predictions based on auditing the ML - shows promise in mitigating performance issues originating from overreliance. This study aims to evaluate the impact of suppression on collaboration fairness and evaluate ML uncertainty as desiderata to audit the ML. MATERIALS AND METHODS: We used data from the Vanderbilt University Medical Center electronic health record (n = 58,817) and the MIMIC-IV-ED dataset (n = 363,145) to predict likelihood of death or ICU transfer and likelihood of 30-day readmission. Our simulation study used gradient-boosted trees as well as an artificially high-performing oracle model. We derived clinician decisions directly from the dataset and simulated clinician acceptance of ML predictions based on previous empirical work on acceptance of CDS alerts. We measured performance as area under the receiver operating characteristic curve and algorithmic fairness using absolute averaged odds difference. RESULTS: When the ML outperforms humans, suppression outperforms the human alone (p < 0.034) and at least does not degrade fairness. When the human outperforms the ML, suppression outperforms the human (p < 5.2 \u00d7 10-5) but the human is fairer than suppression (p < 0.0019). Finally, incorporating uncertainty quantification into suppression approaches can improve performance. CONCLUSION: Suppression of poor-quality ML predictions through an auditor model shows promise in improving collaborative human-AI performance and fairness.",
      "authors": "Brown Katherine E; Wrenn Jesse O; Jackson Nicholas J; Cauley Michael R; Collins Benjamin; Novak Laurie Lovett; Malin Bradley A; Ancker Jessica S",
      "year": "2025",
      "journal": "medRxiv : the preprint server for health sciences",
      "doi": "10.1101/2025.06.24.25330212",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40666330/",
      "mesh_terms": "",
      "keywords": "artificial intelligence; human-AI collaboration; machine learning",
      "pub_types": "Journal Article; Preprint",
      "pmcid": "PMC12262782",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40866555",
      "title": "Quantifying device type and handedness biases in a remote Parkinson's disease AI-powered assessment.",
      "abstract": "We investigate issues pertaining to algorithmic fairness and digital health equity within the context of using machine learning to predict Parkinson's Disease (PD) with data recorded from structured assessments of finger and hand movements. We evaluate the impact of demographic bias and bias related to device type and handedness. We collected data from 251 participants (99 with PD or suspected PD, 152 without PD or any suspicion of PD). Using a random forest model, we observe 92% accuracy, 94% AUROC, 86% sensitivity, 92% specificity, and 84% F1-score. When examining only F1-score differences across groups, no significant bias appears. However, a closer look reveals bias regarding positive prediction and error rates. While we find that sex and ethnicity have no statistically significant impact on PD predictions, biases exist regarding device type and dominant hand, as evidenced by disparate impact and equalized odds. Our findings suggest that remote digital health diagnostics may exhibit underrecognized biases related to handedness and device characteristics, the latter of which can act as a proxy for socioeconomic factors.",
      "authors": "Tumpa Zerin Nasrin; Zawad Md Rahat Shahriar; Sollis Lydia; Parab Shubham; Chen Irene Y; Washington Peter",
      "year": "2025",
      "journal": "NPJ digital medicine",
      "doi": "10.1038/s41746-025-01934-2",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40866555/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC12391457",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41030569",
      "title": "Predicting Childhood Anaemia in Nigeria: A Machine Learning Approach to Uncover Key Risk Factors.",
      "abstract": "BACKGROUND: Childhood anaemia is a major public health challenge in Nigeria, with high prevalence among children under five. This study identifies key determinants and develops a predictive model using advanced machine learning technique. METHODS: A total of 13,136 children aged 6-59 months from the 2018 National Demographic and Health Survey (NDHS) were analysed. Sixteen machine learning algorithms were evaluated on the basis of their ability to predict childhood anaemia using a wide range of individual, community and environmental factors. The Extra Trees (ET) classifier, demonstrating the highest predictive performance, was used to identify the top 10 predictors of childhood anaemia. A fairness and demographic bias assessment framework was incorporated to evaluate the model's performance across different regions, wealth index categories, ethnic groups and gender. RESULTS: The ET classifier achieved an area under the curve (AUC) of 0.8319, an accuracy of 0.7565 and a recall of 0.7565. The top 10 predictors identified by the model included the number of under-five children in the household, birth order, child age, media access, maternal health-seeking behaviour, child gender, proximity to water, money problems, day land surface temperature and all population count. The demographic bias assessment revealed variations in model performance across different subgroups, with the lowest AUCs observed in the north-east region (0.79), the poorest wealth index category (0.80) and the Hausa/Fulani ethnic group (0.81). CONCLUSION: This study shows that machine learning can accurately predict childhood anaemia in Nigeria and identify key risk factors, supporting targeted interventions. Future work should focus on refining models and integrating AI-based interventions to reduce anaemia.",
      "authors": "Ja'afar Ibrahim Khalil; Uthman Olalekan A",
      "year": "2025",
      "journal": "Public health challenges",
      "doi": "10.1002/puh2.70135",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41030569/",
      "mesh_terms": "",
      "keywords": "Nigeria; childhood anaemia; demographic bias; machine learning; predictive modelling; risk factors",
      "pub_types": "Journal Article",
      "pmcid": "PMC12477793",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39386055",
      "title": "The Impact of Race, Ethnicity, and Sex on Fairness in Artificial Intelligence for Glaucoma Prediction Models.",
      "abstract": "OBJECTIVE: Despite advances in artificial intelligence (AI) in glaucoma prediction, most works lack multicenter focus and do not consider fairness concerning sex, race, or ethnicity. This study aims to examine the impact of these sensitive attributes on developing fair AI models that predict glaucoma progression to necessitating incisional glaucoma surgery. DESIGN: Database study. PARTICIPANTS: Thirty-nine thousand ninety patients with glaucoma, as identified by International Classification of Disease codes from 7 academic eye centers participating in the Sight OUtcomes Research Collaborative. METHODS: We developed XGBoost models using 3 approaches: (1) excluding sensitive attributes as input features, (2) including them explicitly as input features, and (3) training separate models for each group. Model input features included demographic details, diagnosis codes, medications, and clinical information (intraocular pressure, visual acuity, etc.), from electronic health records. The models were trained on patients from 5 sites (N\u00a0=\u00a027\u00a0999) and evaluated on a held-out internal test set (N\u00a0=\u00a03499) and 2 external test sets consisting of N\u00a0=\u00a01550 and N\u00a0=\u00a02542 patients. MAIN OUTCOMES AND MEASURES: Area under the receiver operating characteristic curve (AUROC) and equalized odds on the test set and external sites. RESULTS: Six thousand six hundred eighty-two (17.1%) of 39\u00a0090 patients underwent glaucoma surgery with a mean age of 70.1 (standard deviation 14.6) years, 54.5% female, 62.3% White, 22.1% Black, and 4.7% Latinx/Hispanic. We found that not including the sensitive attributes led to better classification performance (AUROC: 0.77-0.82) but worsened fairness when evaluated on the internal test set. However, on external test sites, the opposite was true: including sensitive attributes resulted in better classification performance (AUROC: external #1 - [0.73-0.81], external #2 - [0.67-0.70]), but varying degrees of fairness for sex and race as measured by equalized odds. CONCLUSIONS: Artificial intelligence models predicting whether patients with glaucoma progress to surgery demonstrated bias with respect to sex, race, and ethnicity. The effect of sensitive attribute inclusion and exclusion on fairness and performance varied based on internal versus external test sets. Prior to deployment, AI models should be evaluated for fairness on the target population. FINANCIAL DISCLOSURES: Proprietary or commercial disclosure may be found in the Footnotes and Disclosures at the end of this article.",
      "authors": "Ravindranath Rohith; Stein Joshua D; Hernandez-Boussard Tina; Fisher A Caroline; Wang Sophia Y",
      "year": "2025",
      "journal": "Ophthalmology science",
      "doi": "10.1016/j.xops.2024.100596",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39386055/",
      "mesh_terms": "",
      "keywords": "Bias; Fairness; Glaucoma; Health disparities; Machine learning",
      "pub_types": "Journal Article",
      "pmcid": "PMC11462200",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39904407",
      "title": "Evaluating the Bias, type I error and statistical power of the prior Knowledge-Guided integrated likelihood estimation (PIE) for bias reduction in EHR based association studies.",
      "abstract": "OBJECTIVES: Binary outcomes in electronic health records (EHR) derived using automated phenotype algorithms may suffer from phenotyping error, resulting in bias in association estimation. Huang et al. [1] proposed the Prior Knowledge-Guided Integrated Likelihood Estimation (PIE) method to mitigate the estimation bias, however, their investigation focused on point estimation without statistical inference, and the evaluation of PIE therein using simulation was a proof-of-concept with only a limited scope of scenarios. This study aims to comprehensively assess PIE's performance including (1) how well PIE performs under a wide spectrum of operating characteristics of phenotyping algorithms under real-world scenarios (e.\u00a0g., low prevalence, low sensitivity, high specificity); (2) beyond point estimation, how much variation of the PIE estimator was introduced by the prior distribution; and (3) from a hypothesis testing point of view, if PIE improves type I error and statistical power relative to the na\u00efve method (i.e., ignoring the phenotyping error). METHODS: Synthetic data and use-case analysis were utilized to evaluate PIE. The synthetic data were generated under diverse outcome prevalence, phenotyping algorithm sensitivity, and association effect sizes. Simulation studies compared PIE under different prior distributions with the na\u00efve method, assessing bias, variance, type I error, and power. Use-case analysis compared the performance of PIE and the na\u00efve method in estimating the association of multiple predictors with COVID-19 infection. RESULTS: PIE exhibited reduced bias compared to the na\u00efve method across varied simulation settings, with comparable type I error and power. As the effect size became larger, the bias reduced by PIE was larger. PIE has superior performance when prior distributions aligned closely with true phenotyping algorithm characteristics. Impact of prior quality was minor for low-prevalence outcomes but large for common outcomes. In use-case analysis, PIE maintains a relatively accurate estimation across different scenarios, particularly outperforming the na\u00efve approach under large effect sizes. CONCLUSION: PIE effectively mitigates estimation bias in a wide spectrum of real-world settings, particularly with accurate prior information. Its main benefit lies in bias reduction rather than hypothesis testing. The impact of the prior is small for low-prevalence outcomes.",
      "authors": "Jing Naimin; Lu Yiwen; Tong Jiayi; Weaver James; Ryan Patrick; Xu Hua; Chen Yong",
      "year": "2025",
      "journal": "Journal of biomedical informatics",
      "doi": "10.1016/j.jbi.2025.104787",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39904407/",
      "mesh_terms": "Electronic Health Records; Humans; Algorithms; Likelihood Functions; Bias; COVID-19; Phenotype; SARS-CoV-2; Computer Simulation",
      "keywords": "Association study; Bias reduction; Electronic health record; Phenotyping error",
      "pub_types": "Journal Article",
      "pmcid": "PMC12180398",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "24525488",
      "title": "Evaluating treatment effectiveness under model misspecification: A comparison of targeted maximum likelihood estimation with bias-corrected matching.",
      "abstract": "Statistical approaches for estimating treatment effectiveness commonly model the endpoint, or the propensity score, using parametric regressions such as generalised linear models. Misspecification of these models can lead to biased parameter estimates. We compare two approaches that combine the propensity score and the endpoint regression, and can make weaker modelling assumptions, by using machine learning approaches to estimate the regression function and the propensity score. Targeted maximum likelihood estimation is a double-robust method designed to reduce bias in the estimate of the parameter of interest. Bias-corrected matching reduces bias due to covariate imbalance between matched pairs by using regression predictions. We illustrate the methods in an evaluation of different types of hip prosthesis on the health-related quality of life of patients with osteoarthritis. We undertake a simulation study, grounded in the case study, to compare the relative bias, efficiency and confidence interval coverage of the methods. We consider data generating processes with non-linear functional form relationships, normal and non-normal endpoints. We find that across the circumstances considered, bias-corrected matching generally reported less bias, but higher variance than targeted maximum likelihood estimation. When either targeted maximum likelihood estimation or bias-corrected matching incorporated machine learning, bias was much reduced, compared to using misspecified parametric models.",
      "authors": "Kreif No\u00e9mi; Gruber Susan; Radice Rosalba; Grieve Richard; Sekhon Jasjeet S",
      "year": "2016",
      "journal": "Statistical methods in medical research",
      "doi": "10.1177/0962280214521341",
      "url": "https://pubmed.ncbi.nlm.nih.gov/24525488/",
      "mesh_terms": "Aged; Bias; Computer Simulation; Confidence Intervals; Data Interpretation, Statistical; Hip Prosthesis; Humans; Likelihood Functions; Machine Learning; Male; Models, Statistical; Osteoarthritis; Quality of Life; Treatment Outcome",
      "keywords": "bias-corrected matching; double robustness; machine learning; model misspecification; targeted maximum likelihood estimation; treatment effectiveness",
      "pub_types": "Comparative Study; Journal Article",
      "pmcid": "PMC5051604",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41062083",
      "title": "Reinforcement Learning to Prevent Acute Care Events Among Medicaid Populations: Mixed Methods Study.",
      "abstract": "BACKGROUND: Multidisciplinary care management teams must rapidly prioritize interventions for patients with complex medical and social needs. Current approaches rely on individual training, judgment, and experience, missing opportunities to learn from longitudinal trajectories and prevent adverse outcomes through recommender systems. OBJECTIVE: This study aims to evaluate whether a reinforcement learning approach could outperform standard care management practices in recommending optimal interventions for patients with complex needs. METHODS: Using data from 3175 Medicaid beneficiaries in care management programs across 2 states from 2023 to 2024, we compared alternative approaches for recommending \"next best step\" interventions: the standard experience-based approach (status quo) and a state-action-reward-state-action (SARSA) reinforcement learning model. We evaluated performance using clinical impact metrics, conducted counterfactual causal inference analyses to estimate reductions in acute care events, assessed fairness across demographic subgroups, and performed qualitative chart reviews where the models differed. RESULTS: In counterfactual analyses, SARSA-guided care management reduced acute care events by 12 percentage points (95% CI 2.2-21.8 percentage points, a 20.7% relative reduction; P=.02) compared to the status quo approach, with a number needed to treat of 8.3 (95% CI 4.6-45.2) to prevent 1 acute event. The approach showed improved fairness across demographic groups, including gender (3.8% vs 5.3% disparity in acute event rates, reduction 1.5%, 95% CI 0.3%-2.7%) and race and ethnicity (5.6% vs 8.9% disparity, reduction 3.3%, 95% CI 1.1%-5.5%). In qualitative reviews, the SARSA model detected and recommended interventions for specific medical-social interactions, such as respiratory issues associated with poor housing quality or food insecurity in individuals with diabetes. CONCLUSIONS: SARSA-guided care management shows potential to reduce acute care use compared to standard practice. The approach demonstrates how reinforcement learning can improve complex decision-making in situations where patients face concurrent clinical and social factors while maintaining safety and fairness across demographic subgroups.",
      "authors": "Basu Sanjay; Muralidharan Bhairavi; Sheth Parth; Wanek Dan; Morgan John; Patel Sadiq",
      "year": "2025",
      "journal": "JMIR AI",
      "doi": "10.2196/74264",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41062083/",
      "mesh_terms": "",
      "keywords": "AI; artificial intelligence; care management; clinical decision support; community health worker; machine learning; reinforcement learning; social determinants of health",
      "pub_types": "Journal Article",
      "pmcid": "PMC12547335",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40994240",
      "title": "Racing Against the Algorithm: Leveraging Inclusive AI as an Antiracist Tool for Brain Health.",
      "abstract": "Artificial intelligence (AI) is transforming medicine, including neurology and mental health. Yet without equity-centered design, AI risks reinforcing systemic racism. This article explores how algorithmic bias and phenotypic exclusion disproportionately affect marginalized communities in brain health. Drawing on lived experience and scientific evidence, the essay outlines five design principles-centered on inclusion, transparency, and accountability-to ensure AI promotes equity. By reimagining AI as a tool for justice, we can reshape translational science to serve all populations.",
      "authors": "Ekuta Victor",
      "year": "2025",
      "journal": "Clinical and translational science",
      "doi": "10.1111/cts.70364",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40994240/",
      "mesh_terms": "Humans; Artificial Intelligence; Algorithms; Systemic Racism; Mental Health; Health Equity; Brain",
      "keywords": "artificial intelligence; brain health; clinical algorithms; health equity; inclusive design; machine learning; racial disparities; translational science",
      "pub_types": "Journal Article",
      "pmcid": "PMC12461116",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41273675",
      "title": "Equity-promoting integer programming approaches for medical resident rotation scheduling.",
      "abstract": "Motivated by our collaboration with a residency program at an academic health system, we propose new integer programming (IP) approaches for the resident-to-rotation assignment problem (RRAP). Given sets of residents, resident classes, and departments, as well as a block structure for each class, staffing needs, rotation requirements for each class, program rules, and resident vacation requests, the RRAP involves finding a feasible year-long rotation schedule that specifies resident assignments to rotations and vacation times. We first present an IP formulation for the RRAP, which mimics the manual method for generating rotation schedules in practice and can be easily implemented and efficiently solved using off-the-shelf optimization software. However, it can lead to disparities in satisfying vacation requests among residents. To mitigate such disparities, we derive an equity-promoting counterpart that finds an optimal rotation schedule, maximizing the number of satisfied vacation requests while minimizing a measure of disparity in satisfying these requests. Then, we propose a computationally efficient Pareto Search Algorithm capable of finding the complete set of Pareto optimal solutions to the equity-promoting IP within a time that is suitable for practical implementation. Additionally, we present a user-friendly tool that implements the proposed models to automate the generation of the rotation schedule. Finally, we construct diverse RRAP instances based on data from our collaborator and conduct extensive experiments to illustrate the potential practical benefits of our proposed approaches. Our results demonstrate the computational efficiency and implementability of our approaches and underscore their potential to enhance fairness in resident rotation scheduling.",
      "authors": "Li Shutian; Shehadeh Karmel S; Curtis Frank E; Hochman Beth R",
      "year": "2025",
      "journal": "Health care management science",
      "doi": "10.1007/s10729-025-09736-4",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41273675/",
      "mesh_terms": "Internship and Residency; Humans; Personnel Staffing and Scheduling; Algorithms; Academic Medical Centers",
      "keywords": "Fairness; Integer programming; Operations management; Operations research; Optimization; Resident scheduling",
      "pub_types": "Journal Article",
      "pmcid": "PMC12743667",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41146192",
      "title": "Reducing inequalities using an unbiased machine learning approach to identify births with the highest risk of preventable neonatal deaths.",
      "abstract": "BACKGROUND: Despite contemporaneous declines in neonatal mortality, recent studies show the existence of left-behind populations that continue to have higher mortality rates than the national averages. Additionally, many of these deaths are from preventable causes. This reality creates the need for more precise methods to identify high-risk births, allowing policymakers to target them more effectively. This study fills this gap by developing unbiased machine-learning approaches to more accurately identify births with a high risk of neonatal deaths from preventable causes. METHODS: We link administrative databases from the Brazilian health ministry to obtain birth and death records in the country from 2015 to 2017. The final dataset comprises 8,797,968 births, of which 59,615 newborns died before reaching 28 days alive (neonatal deaths). These neonatal deaths are categorized into preventable deaths (42,290) and non-preventable deaths (17,325). Our analysis identifies the death risk of the former group, as they are amenable to policy interventions. We train six machine-learning algorithms, test their performance on unseen data, and evaluate them using a new policy-oriented metric. To avoid biased policy recommendations, we also investigate how our approach impacts disadvantaged populations. RESULTS: XGBoost was the best-performing algorithm for our task, with the 5% of births identified as highest risk by the model accounting for over 85% of the observed deaths. Furthermore, the risk predictions exhibit no statistical differences in the proportion of actual preventable deaths from disadvantaged populations, defined by race, education, marital status, and maternal age. These results are similar for other threshold levels. CONCLUSIONS: We show that, by using publicly available administrative data sets and ML methods, it is possible to identify the births with the highest risk of preventable deaths with a high degree of accuracy. This is useful for policymakers as they can target health interventions to those who need them the most and where they can be effective without producing bias against disadvantaged populations. Overall, our approach can guide policymakers in reducing neonatal mortality rates and their health inequalities. Finally, it can be adapted for use in other developing countries.",
      "authors": "Ramos Antonio P; Caldieraro Fabio; Nascimento Marcus L; Saldanha Raphael",
      "year": "2025",
      "journal": "Population health metrics",
      "doi": "10.1186/s12963-025-00420-x",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41146192/",
      "mesh_terms": "Humans; Machine Learning; Infant, Newborn; Brazil; Infant Mortality; Female; Infant; Perinatal Death; Socioeconomic Factors; Male; Cause of Death; Algorithms; Databases, Factual; Risk Factors",
      "keywords": "Algorithmic bias; Health inequality; Machine learning; Neonatal mortality; Program targeting",
      "pub_types": "Journal Article",
      "pmcid": "PMC12557940",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41531745",
      "title": "Fairness-aware K-means clustering in digital mental health for higher education students: a generalizable framework for equitable clustering.",
      "abstract": "OBJECTIVES: Higher education students, particularly those from underrepresented backgrounds, experience heightened levels of anxiety, depression, and burnout. Clinical informatics approaches leveraging K-means clustering can aid in mental health risk stratification, yet they often exacerbate disparities. We present a socially fair clustering framework that ensures equitable clustering costs across demographic groups while minimizing within-cluster variability. MATERIALS AND METHODS: Our framework compares standard and socially fair K-means clustering to assess the impact of demographic disparities. It identifies factors affecting clustering across demographics using omnibus and post hoc statistical tests. Subsequently, it quantifies the influence of statistically significant factors on cluster development. We illustrate our approach by identifying racially equitable clusters of mental health among students surveyed by the Healthy Minds Network. RESULTS: The socially fair clustering approach reduces disparities in clustering costs by as much as 30% across racial groups while maintaining consistency with standard K-means solutions in socioeconomically homogenous populations. Discrimination experiences were the strongest indicator of poorer mental health, whereas stable financial conditions and robust social engagement promoted resilience. DISCUSSION: Integrating fairness constraints into clustering algorithms reduces disparities in risk stratification and provides insights into socioeconomic drivers of student well-being. Our findings suggest that standard models may overpathologize middle-risk cohorts, whereas fairness-aware clustering yields partitions that better capture disparities. CONCLUSION: Our work demonstrates how integrating fairness-aware objectives into clustering algorithms can enhance equity in partitioning systems. The framework we present is broadly applicable to clustering problems across various biomedical informatics domains.",
      "authors": "Alluri Priyanshu; Chen Zequn; Thesen Thomas; Jacobson Nicholas C; Marrero Wesley J",
      "year": "2026",
      "journal": "JAMIA open",
      "doi": "10.1093/jamiaopen/ooaf174",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41531745/",
      "mesh_terms": "",
      "keywords": "cluster analysis; machine learning; medical informatics; mental health; risk assessment",
      "pub_types": "Journal Article",
      "pmcid": "PMC12794019",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "37318804",
      "title": "Racial and Ethnic Bias in Risk Prediction Models for Colorectal Cancer Recurrence When Race and Ethnicity Are Omitted as Predictors.",
      "abstract": "IMPORTANCE: Including race and ethnicity as a predictor in clinical risk prediction algorithms has received increased scrutiny, but there continues to be a lack of empirical studies addressing whether simply omitting race and ethnicity from the algorithms will ultimately affect decision-making for patients of minoritized racial and ethnic groups. OBJECTIVE: To examine whether including race and ethnicity as a predictor in a colorectal cancer recurrence risk algorithm is associated with racial bias, defined as racial and ethnic differences in model accuracy that could potentially lead to unequal treatment. DESIGN, SETTING, AND PARTICIPANTS: This retrospective prognostic study was conducted using data from a large integrated health care system in Southern California for patients with colorectal cancer who received primary treatment between 2008 and 2013 and follow-up until December 31, 2018. Data were analyzed from January 2021 to June 2022. MAIN OUTCOMES AND MEASURES: Four Cox proportional hazards regression prediction models were fitted to predict time from surveillance start to cancer recurrence: (1) a race-neutral model that explicitly excluded race and ethnicity as a predictor, (2) a race-sensitive model that included race and ethnicity, (3) a model with 2-way interactions between clinical predictors and race and ethnicity, and (4) separate models by race and ethnicity. Algorithmic fairness was assessed using model calibration, discriminative ability, false-positive and false-negative rates, positive predictive value (PPV), and negative predictive value (NPV). RESULTS: The study cohort included 4230 patients (mean [SD] age, 65.3 [12.5] years; 2034 [48.1%] female; 490 [11.6%] Asian, Hawaiian, or Pacific Islander; 554 [13.1%] Black or African American; 937 [22.1%] Hispanic; and 2249 [53.1%] non-Hispanic White). The race-neutral model had worse calibration, NPV, and false-negative rates among racial and ethnic minority subgroups than non-Hispanic White individuals (eg, false-negative rate for Hispanic patients: 12.0% [95% CI, 6.0%-18.6%]; for non-Hispanic White patients: 3.1% [95% CI, 0.8%-6.2%]). Adding race and ethnicity as a predictor improved algorithmic fairness in calibration slope, discriminative ability, PPV, and false-negative rates (eg, false-negative rate for Hispanic patients: 9.2% [95% CI, 3.9%-14.9%]; for non-Hispanic White patients: 7.9% [95% CI, 4.3%-11.9%]). Inclusion of race interaction terms or using race-stratified models did not improve model fairness, likely due to small sample sizes in subgroups. CONCLUSIONS AND RELEVANCE: In this prognostic study of the racial bias in a cancer recurrence risk algorithm, removing race and ethnicity as a predictor worsened algorithmic fairness in multiple measures, which could lead to inappropriate care recommendations for patients who belong to minoritized racial and ethnic groups. Clinical algorithm development should include evaluation of fairness criteria to understand the potential consequences of removing race and ethnicity for health inequities.",
      "authors": "Khor Sara; Haupt Eric C; Hahn Erin E; Lyons Lindsay Joe L; Shankaran Veena; Bansal Aasthaa",
      "year": "2023",
      "journal": "JAMA network open",
      "doi": "10.1001/jamanetworkopen.2023.18495",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37318804/",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Black or African American; Colorectal Neoplasms; Ethnicity; Hispanic or Latino; Minority Groups; Retrospective Studies; White; Asian American Native Hawaiian and Pacific Islander",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, P.H.S.",
      "pmcid": "PMC10273018",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36463866",
      "title": "Special Section on Inclusive Digital Health: Notable Papers on Addressing Bias, Equity, and Literacy to Strengthen Health Systems.",
      "abstract": "OBJECTIVE: To summarize significant research contributions on addressing bias, equity, and literacy in health delivery systems published in 2021. METHODS: An extensive search using PubMed and Scopus was conducted to identify peer-reviewed articles published in 2021 that examined ways that informatics methods, approaches, and tools could address bias, equity, and literacy in health systems and care delivery processes. The selection process comprised three steps: (1) 15 candidate best papers were first selected by the two section editors; (2) external reviewers from internationally renowned research teams reviewed each candidate best paper; and (3) the final selection of three best papers was conducted by the editorial committee of the Yearbook. RESULTS: Selected best papers represent studies that characterized significant challenges facing biomedical informatics with respect to equity and practices that support equity and literacy in the design of health information systems. Selected papers represent the full spectrum of this year's yearbook theme. In general, papers identified in the search fell into one of the following categories: (1) descriptive accounts of algorithmic bias in medical software or machine learning approaches; (2) enabling health information systems to appropriately encode for gender identity and sex; (3) approaches to support health literacy among individuals who interact with information systems and mobile applications; and (4) approaches to engage diverse populations in the use of health information systems and the biomedical informatics workforce CONCLUSIONS: : Although the selected papers are notable, our collective efforts as a biomedical informatics community to address equity, literacy, and bias remain nascent. More work is needed to ensure health information systems are just in their use of advanced computing approaches and all persons have equal access to health care and informatics tools.",
      "authors": "Dixon Brian E; Holmes John H",
      "year": "2022",
      "journal": "Yearbook of medical informatics",
      "doi": "10.1055/s-0042-1742536",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36463866/",
      "mesh_terms": "Female; Humans; Male; Gender Identity; Bias; Health Literacy; Health Information Systems; Machine Learning",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC9719755",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36768092",
      "title": "Evaluation of AIML + HDR-A Course to Enhance Data Science Workforce Capacity for Hispanic Biomedical Researchers.",
      "abstract": "Artificial intelligence (AI) and machine learning (ML) facilitate the creation of revolutionary medical techniques. Unfortunately, biases in current AI and ML approaches are perpetuating minority health inequity. One of the strategies to solve this problem is training a diverse workforce. For this reason, we created the course \"Artificial Intelligence and Machine Learning applied to Health Disparities Research (AIML + HDR)\" which applied general Data Science (DS) approaches to health disparities research with an emphasis on Hispanic populations. Some technical topics covered included the Jupyter Notebook Framework, coding with R and Python to manipulate data, and ML libraries to create predictive models. Some health disparities topics covered included Electronic Health Records, Social Determinants of Health, and Bias in Data. As a result, the course was taught to 34 selected Hispanic participants and evaluated by a survey on a Likert scale (0-4). The surveys showed high satisfaction (more than 80% of participants agreed) regarding the course organization, activities, and covered topics. The students strongly agreed that the activities were relevant to the course and promoted their learning (3.71 \u00b1 0.21). The students strongly agreed that the course was helpful for their professional development (3.76 \u00b1 0.18). The open question was quantitatively analyzed and showed that seventy-five percent of the comments received from the participants confirmed their great satisfaction.",
      "authors": "Heredia-Negron Frances; Alamo-Rodriguez Natalie; Oyola-Velazquez Lenamari; Nieves Brenda; Carrasquillo Kelvin; Hochheiser Harry; Fristensky Brian; Daluz-Santana Istoni; Fernandez-Repollet Emma; Roche-Lima Abiel",
      "year": "2023",
      "journal": "International journal of environmental research and public health",
      "doi": "10.3390/ijerph20032726",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36768092/",
      "mesh_terms": "Humans; Artificial Intelligence; Data Science; Hispanic or Latino; Machine Learning; Workforce; Biomedical Research",
      "keywords": "artificial intelligence; data science; health disparities; hispanic biomedical research; machine learning",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC9914971",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39591156",
      "title": "EMSIG: Uncovering Factors Influencing COVID-19 Vaccination Across Different Subgroups Characterized by Embedding-Based Spatial Information Gain.",
      "abstract": "Background/Objectives: COVID-19 and its variants continue to pose significant threats to public health, with considerable uncertainty surrounding their impact. As of September 2024, the total number of deaths reached 8.8 million worldwide. Vaccination remains the most effective strategy for preventing COVID-19. However, vaccination rates in the Deep South, U.S., are notably lower than the national average due to various factors. Methods: To address this challenge, we developed the Embedding-based Spatial Information Gain (EMSIG) method, an innovative tool using machine learning techniques for subgroup modeling. EMSIG helps identify subgroups where participants share similar perceptions but exhibit high variance in COVID-19 vaccine doses. It introduces spatial information gain (SIG) to screen regions of interest (ROI) subgroups and reveals their specific concerns. Results: We analyzed survey data from 1020 participants in Alabama. EMSIG identified 16 factors encompassing COVID-19 hesitancy and trust in medical doctors, pharmacists, and public health authorities and revealed four distinct ROI subgroups. The five factors, including COVID-19 perceived detriment, fear, skepticism, side effects related to COVID-19, and communication with pharmacists, were commonly shared across at least three subgroups. A subgroup primarily composed of Democrats with a high flu-shot rate expressed concerns about pharmacist communication, government fairness, and responsibility. Another subgroup, characterized by older, white Republicans with a relatively low flu-shot rate, expressed concerns about doctor trust and the intelligence of public health authorities. Conclusions: EMSIG enhances our understanding of specific concerns across different demographics, characterizes these demographics, and informs targeted interventions to increase vaccination uptake and ensure equitable prevention strategies.",
      "authors": "Yue Zongliang; McCormick Nicholas P; Ezeala Oluchukwu M; Durham Spencer H; Westrick Salisa C",
      "year": "2024",
      "journal": "Vaccines",
      "doi": "10.3390/vaccines12111253",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39591156/",
      "mesh_terms": "",
      "keywords": "COVID-19; embedding; factor; hesitancy; machine learning; spatial information gain; subgroup; uptake; vaccination",
      "pub_types": "Journal Article",
      "pmcid": "PMC11599077",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "32819343",
      "title": "In Defence of informed consent for health record research\u00a0- why arguments from 'easy rescue', 'no harm' and 'consent bias' fail.",
      "abstract": "BACKGROUND: Health data holds great potential for improved treatments. Big data research and machine learning models have been shown to hold great promise for improved diagnostics and treatment planning. The potential is tied, however, to the availability of personal health data. In recent years, it has been argued that data from health records should be available for health research, and that individuals have a duty to make the data available for such research. A central point of debate is whether such secondary use of health data requires informed consent. MAIN BODY: In response to recent writings this paper argues that a requirement of informed consent for health record research must be upheld. It does so by exploring different contrasting notions of the duty of easy rescue and arguing that none of them entail a perfect duty to participate in health record research. In part because the costs of participation cannot be limited to 1) the threat of privacy breaches, but includes 2) the risk of reduced trust and 3) suboptimal treatment, 4) stigmatization and 5) medicalisation, 6) further stratification of solidarity and 7) increased inequality in access to treatment and medicine. And finally, it defends the requirement of informed consent by arguing that the mere possibility of consent bias provides a rather weak reason for making research participation mandatory, and that there are strong, independent reasons for making. CONCLUSION: Arguments from the duty of easy rescue in combination with claims about little risk of harm and potential consent bias fail to establish not only a perfect duty to participate in health record research, but also that participation in such research should be mandatory. On the contrary, an analysis of these arguments indicates that the duty to participate in research is most adequately construed as an imperfect duty, and reveals a number of strong reasons for insisting that participation in health records research is based on informed consent.",
      "authors": "Ploug Thomas",
      "year": "2020",
      "journal": "BMC medical ethics",
      "doi": "10.1186/s12910-020-00519-w",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32819343/",
      "mesh_terms": "Dissent and Disputes; Humans; Informed Consent; Trust",
      "keywords": "Consent bias; Duty of easy rescue; Equality in health; Harm; Health data; Informed consent; Medicalization; Privacy; Solidarity; Stigmatization; Trust",
      "pub_types": "Journal Article",
      "pmcid": "PMC7441538",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39767481",
      "title": "AI in Biomedicine-A Forward-Looking Perspective on Health Equity.",
      "abstract": "As new artificial intelligence (AI) tools are being developed and as AI continues to revolutionize healthcare, its potential to advance health equity is increasingly recognized. The 2024 Research Centers in Minority Institutions (RCMI) Consortium National Conference session titled \"Artificial Intelligence: Safely, Ethically, and Responsibly\" brought together experts from diverse institutions to explore AI's role and challenges in advancing health equity. This report summarizes presentations and discussions from the conference focused on AI's potential and its challenges, particularly algorithmic bias, transparency, and the under-representation of minority groups in AI datasets. Key topics included AI's predictive and generative capabilities in healthcare, ethical governance, and key national initiatives, like AIM-AHEAD. The session highlighted the critical role of RCMI institutions in fostering diverse AI/machine learning research and in developing culturally competent AI tools. Other discussions included AI's capacity to improve patient outcomes, especially for underserved communities, and underscored the necessity for robust ethical standards, a diverse AI and scientific workforce, transparency, and inclusive data practices. The engagement of RCMI institutions is critical to ensure practices in AI development and deployment which prioritize health equity, thus paving the way for a more inclusive AI-driven healthcare system.",
      "authors": "Kumar Deepak; Malin Bradley A; Vishwanatha Jamboor K; Wu Lang; Hedges Jerris R",
      "year": "2024",
      "journal": "International journal of environmental research and public health",
      "doi": "10.3390/ijerph21121642",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39767481/",
      "mesh_terms": "Health Equity; Artificial Intelligence; Humans; Biomedical Research",
      "keywords": "RCMI; artificial intelligence; augmented intelligence; health disparities; health equity; health ethics; machine learning",
      "pub_types": "Journal Article; Conference Proceedings",
      "pmcid": "PMC11675414",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39399154",
      "title": "Predicting Prenatal Depression and Assessing Model Bias Using Machine Learning Models.",
      "abstract": "BACKGROUND: Perinatal depression is one of the most common medical complications during pregnancy and postpartum period, affecting 10% to 20% of pregnant individuals, with higher rates among Black and Latina women who are also less likely to be diagnosed and treated. Machine learning (ML) models based on electronic medical records (EMRs) have effectively predicted postpartum depression in middle-class White women but have rarely included sufficient proportions of racial/ethnic minorities, which has contributed to biases in ML models. Our goal is to determine whether ML models could predict depression in early pregnancy in racial/ethnic minority women by leveraging EMR data. METHODS: We extracted EMRs from a large U.S. urban hospital serving mostly low-income Black and Hispanic women (n\u00a0= 5875). Depressive symptom severity was assessed using the Patient Health Questionnaire-9 self-report questionnaire. We investigated multiple ML classifiers using Shapley additive explanations for model interpretation and determined prediction bias with 4 metrics: disparate impact, equal opportunity difference, and equalized odds (standard deviations of true positives and false positives). RESULTS: Although the best-performing ML model's (elastic net) performance was low (area under the receiver operating characteristic curve\u00a0= 0.61), we identified known perinatal depression risk factors such as unplanned pregnancy and being single and underexplored factors such as self-reported pain, lower prenatal vitamin intake, asthma, carrying a male fetus, and lower platelet levels. Despite the sample comprising mostly low-income minority women (54% Black, 27% Latina), the model performed worse for these communities (area under the receiver operating characteristic curve: 57% Black, 59% Latina women vs. 64% White women). CONCLUSIONS: EMR-based ML models could moderately predict early pregnancy depression but exhibited biased performance against low-income minority women.",
      "authors": "Huang Yongchao; Alvernaz Suzanne; Kim Sage J; Maki Pauline; Dai Yang; Pe\u00f1alver Bernab\u00e9 Beatriz",
      "year": "2024",
      "journal": "Biological psychiatry global open science",
      "doi": "10.1016/j.bpsgos.2024.100376",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39399154/",
      "mesh_terms": "",
      "keywords": "Electronic medical records; Health disparities; Machine learning; Model performance bias; Perinatal depression",
      "pub_types": "Journal Article",
      "pmcid": "PMC11470166",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41260014",
      "title": "Knowledge-informed deep learning to mitigate bias in joint air pollutant prediction.",
      "abstract": "Accurate prediction of atmospheric air pollutants is critical for public health protection and environmental management. Traditional machine learning (ML) methods achieve high spatial resolution but lack physicochemical constraints, leading to systematic biases that compromise exposure estimates for epidemiological studies. Chemical transport models incorporate atmospheric physics but require expensive parameterization and often fail to capture local-scale variability crucial for health impact assessment. This gap between data-driven accuracy and physical realism presents a major obstacle to advancing air quality science. We address this challenge through a novel physics-informed deep learning framework that integrates advection-diffusion equations and fluid dynamics constraints directly into neural network architectures for multi-pollutant prediction. Our approach models air pollutant pairs across geographically distinct domains (NO2/NOx for California; PM2.5/PM10 for mainland China), providing a comprehensive framework for physics-constrained atmospheric modeling at high resolution. Through an efficient framework, our methodology demonstrates that incorporating proxy advection and diffusion fields as physical constraints fundamentally alters learning dynamics, reducing generalization error and eliminating systematic bias inherent in data-driven approaches while improving computational efficiency compared to graph networks. Site-based validation reveals unprecedented bias reduction: 21%-42% for nitrogen oxides and 16%-17% for particulate matter compared to the baseline deep learning methods. Our methodology uniquely generates physically interpretable parameters while providing explicit uncertainty quantification through ensemble techniques. The substantial bias reduction coupled with physically interpretable parameters has immediate implications for improving air pollutant exposure assessment and understanding in epidemiological research, potentially transforming health effect evaluations that rely on accurate spatial predictions.",
      "authors": "Li Lianfa; Khalili Roxana; Lurmann Frederick; Pavlovic Nathan; Wu Jun; Xu Yan; Liu Yisi; O'Sharkey Karl; Ritz Beate; Oman Luke; Franklin Meredith; Bastain Theresa; Farzan Shohreh F; Breton Carrie; Habre Rima",
      "year": "2025",
      "journal": "Environment international",
      "doi": "10.1016/j.envint.2025.109915",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41260014/",
      "mesh_terms": "Deep Learning; Air Pollutants; Air Pollution; Particulate Matter; Environmental Monitoring; California; China; Nitrogen Oxides",
      "keywords": "Air pollution; Bias mitigation; Deep learning; Joint prediction; Knowledge fusion; Physics-informed modeling",
      "pub_types": "Journal Article",
      "pmcid": "PMC12810719",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35044842",
      "title": "Negative Patient Descriptors: Documenting Racial Bias In The Electronic Health Record.",
      "abstract": "Little is known about how racism and bias may be communicated in the medical record. This study used machine learning to analyze electronic health records (EHRs) from an urban academic medical center and to investigate whether providers' use of negative patient descriptors varied by patient race or ethnicity. We analyzed a sample of 40,113 history and physical notes (January 2019-October 2020) from 18,459 patients for sentences containing a negative descriptor (for example, resistant or noncompliant) of the patient or the patient's behavior. We used mixed effects logistic regression to determine the odds of finding at least one negative descriptor as a function of the patient's race or ethnicity, controlling for sociodemographic and health characteristics. Compared with White patients, Black patients had 2.54 times the odds of having at least one negative descriptor in the history and physical notes. Our findings raise concerns about stigmatizing language in the EHR and its potential to exacerbate racial and ethnic health care disparities.",
      "authors": "Sun Michael; Oliwa Tomasz; Peek Monica E; Tung Elizabeth L",
      "year": "2022",
      "journal": "Health affairs (Project Hope)",
      "doi": "10.1377/hlthaff.2021.01423",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35044842/",
      "mesh_terms": "Black People; Electronic Health Records; Ethnicity; Healthcare Disparities; Humans; Racism",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC8973827",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36639799",
      "title": "Ethics and governance of trustworthy medical artificial intelligence.",
      "abstract": "BACKGROUND: The growing application of artificial intelligence (AI) in healthcare has brought technological breakthroughs to traditional diagnosis and treatment, but it is accompanied by many risks and challenges. These adverse effects are also seen as ethical issues and affect trustworthiness in medical AI and need to be managed through identification, prognosis and monitoring. METHODS: We adopted a multidisciplinary approach and summarized five subjects that influence the trustworthiness of medical AI: data quality, algorithmic bias, opacity, safety and security, and responsibility attribution, and discussed these factors from the perspectives of technology, law, and healthcare stakeholders and institutions. The ethical framework of ethical values-ethical principles-ethical norms is used to propose corresponding ethical governance countermeasures for trustworthy medical AI from the ethical, legal, and regulatory aspects. RESULTS: Medical data are primarily unstructured, lacking uniform and standardized annotation, and data quality will directly affect the quality of medical AI algorithm models. Algorithmic bias can affect AI clinical predictions and exacerbate health disparities. The opacity of algorithms affects patients' and doctors' trust in medical AI, and algorithmic errors or security vulnerabilities can pose significant risks and harm to patients. The involvement of medical AI in clinical practices may threaten doctors 'and patients' autonomy and dignity. When accidents occur with medical AI, the responsibility attribution is not clear. All these factors affect people's trust in medical AI. CONCLUSIONS: In order to make medical AI trustworthy, at the ethical level, the ethical value orientation of promoting human health should first and foremost be considered as the top-level design. At the legal level, current medical AI does not have moral status and humans remain the duty bearers. At the regulatory level, strengthening data quality management, improving algorithm transparency and traceability to reduce algorithm bias, and regulating and reviewing the whole process of the AI industry to control risks are proposed. It is also necessary to encourage multiple parties to discuss and assess AI risks and social impacts, and to strengthen international cooperation and communication.",
      "authors": "Zhang Jie; Zhang Zong-Ming",
      "year": "2023",
      "journal": "BMC medical informatics and decision making",
      "doi": "10.1186/s12911-023-02103-9",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36639799/",
      "mesh_terms": "Humans; Artificial Intelligence; Algorithms; Delivery of Health Care; Prognosis; Data Management",
      "keywords": "Algorithms; Artificial intelligence; Data; Ethics; Governance; Healthcare; Regulation; Responsibility attribution",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC9840286",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40298901",
      "title": "External validation of a proprietary risk model for 1-year mortality in community-dwelling adults aged 65 years or older.",
      "abstract": "OBJECTIVE: To examine the discrimination, calibration, and algorithmic fairness of the Epic End of Life Care Index (EOL-CI). MATERIALS AND METHODS: We assessed the EOL-CI's performance by estimating area under the receiver operating characteristic curve (AUC), sensitivity, and positive and negative predictive values in community-dwelling adults \u226565 years of age in a single health system in the Southeastern United States. Algorithmic fairness was examined by comparing the model's performance across sex, race, and ethnicity subgroups. Using a machine learning approach, we also explored local re-calibration of the EOL-CI considering additional information on past hospitalizations and frailty. RESULTS: Among 215\u00a0731 patients (median age\u2009=\u200974 years, 57% female, 12% of Black race), 10% were classified as medium risk (15-44) and 3% as high risk (\u226545) by the EOL-CI. The observed 1-year mortality rate was 3%. The EOL-CI had an AUC 0.82 for 1-year mortality, with a positive predictive value of 22%. Predictive performance was generally similar across sex and race subgroups, though the EOL-CI displayed better performance with increasing age and in older adults with 2 or more outpatient encounters in the past 24 months. Local re-calibration of the EOL-CI was required to provide absolute estimates of mortality risk, and calibration was further improved when the EOL-CI was augmented with data on inpatient hospitalizations and frailty. DISCUSSION: The EOL-CI demonstrates reasonable discrimination, albeit with better performance in older adults and in those with greater health system contact. CONCLUSION: Local refinement and calibration of the EOL-CI score is required to provide direct estimates of prognosis, with the goal of making the EOL-CI a more a valuable tool at the point of care for identifying patients who would benefit from targeted palliative care interventions and proactive care planning.",
      "authors": "Frechman Erica; Jaeger Byron C; Kowalkowski Marc; Williamson Jeff D; Lenoir Kristin M; Palakshappa Jessica A; Wells Brian J; Callahan Kathryn E; Pajewski Nicholas M; Gabbard Jennifer L",
      "year": "2025",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "doi": "10.1093/jamia/ocaf062",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40298901/",
      "mesh_terms": "Humans; Aged; Female; Male; Risk Assessment; Aged, 80 and over; Mortality; Independent Living; ROC Curve; Terminal Care; Algorithms; Machine Learning; Area Under Curve; Southeastern United States",
      "keywords": "clinical decision-making; electronic health records; palliative care; prognosis",
      "pub_types": "Journal Article; Validation Study",
      "pmcid": "PMC12199354",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38551630",
      "title": "The Implementation of Recommender Systems for Mental Health Recovery Narratives: Evaluation of Use and Performance.",
      "abstract": "BACKGROUND: Recommender systems help narrow down a large range of items to a smaller, personalized set. NarraGive is a first-in-field hybrid recommender system for mental health recovery narratives, recommending narratives based on their content and narrator characteristics (using content-based filtering) and on narratives beneficially impacting other similar users (using collaborative filtering). NarraGive is integrated into the Narrative Experiences Online (NEON) intervention, a web application providing access to the NEON Collection of recovery narratives. OBJECTIVE: This study aims to analyze the 3 recommender system algorithms used in NarraGive to inform future interventions using recommender systems for lived experience narratives. METHODS: Using a recently published framework for evaluating recommender systems to structure the analysis, we compared the content-based filtering algorithm and collaborative filtering algorithms by evaluating the accuracy (how close the predicted ratings are to the true ratings), precision (the proportion of the recommended narratives that are relevant), diversity (how diverse the recommended narratives are), coverage (the proportion of all available narratives that can be recommended), and unfairness (whether the algorithms produce less accurate predictions for disadvantaged participants) across gender and ethnicity. We used data from all participants in 2 parallel-group, waitlist control clinical trials of the NEON intervention (NEON trial: N=739; NEON for other [eg, nonpsychosis] mental health problems [NEON-O] trial: N=1023). Both trials included people with self-reported mental health problems who had and had not used statutory mental health services. In addition, NEON trial participants had experienced self-reported psychosis in the previous 5 years. Our evaluation used a database of Likert-scale narrative ratings provided by trial participants in response to validated narrative feedback questions. RESULTS: Participants from the NEON and NEON-O trials provided 2288 and 1896 narrative ratings, respectively. Each rated narrative had a median of 3 ratings and 2 ratings, respectively. For the NEON trial, the content-based filtering algorithm performed better for coverage; the collaborative filtering algorithms performed better for accuracy, diversity, and unfairness across both gender and ethnicity; and neither algorithm performed better for precision. For the NEON-O trial, the content-based filtering algorithm did not perform better on any metric; the collaborative filtering algorithms performed better on accuracy and unfairness across both gender and ethnicity; and neither algorithm performed better for precision, diversity, or coverage. CONCLUSIONS: Clinical population may be associated with recommender system performance. Recommender systems are susceptible to a wide range of undesirable biases. Approaches to mitigating these include providing enough initial data for the recommender system (to prevent overfitting), ensuring that items can be accessed outside the recommender system (to prevent a feedback loop between accessed items and recommended items), and encouraging participants to provide feedback on every narrative they interact with (to prevent participants from only providing feedback when they have strong opinions).",
      "authors": "Slade Emily; Rennick-Egglestone Stefan; Ng Fiona; Kotera Yasuhiro; Llewellyn-Beardsley Joy; Newby Chris; Glover Tony; Keppens Jeroen; Slade Mike",
      "year": "2024",
      "journal": "JMIR mental health",
      "doi": "10.2196/45754",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38551630/",
      "mesh_terms": "Humans; Mental Health Recovery; Neon; Algorithms; Software; Narration",
      "keywords": "NEON trial; Narrative Experiences Online trial; fairness across users; intralist diversity; item space coverage; lived experience narrative; mean absolute error; precision; psychosis; recommender system; recovery story",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC11015364",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40444224",
      "title": "The effectiveness, equity and explainability of health service resource allocation-with applications in kidney transplantation & family planning.",
      "abstract": "INTRODUCTION: Halfway to the deadline of the 2030 agenda, humankind continues to face long-standing yet urgent policy and management challenges to address resource shortages and deliver on Sustainable Development Goal 3; health and well-being for all at all ages. More than half of the global population lacks access to essential health services. Additional resources are required and need to be allocated effectively and equitably. Resource allocation models, however, have struggled to accurately predict effects and to present optimal allocations, thus hampering effectiveness and equity improvement. The current advances in machine learning present opportunities to better predict allocation effects and to prescribe solutions that better balance effectiveness and equity. The most advanced of these models tend to be \"black box\" models that lack explainability. This lack of explainability is problematic as it can clash with professional values and hide biases that negatively impact effectiveness and equity. METHODS: Through a novel theoretical framework and two diverse case studies, this manuscript explores the trade-offs between effectiveness, equity, and explainability. The case studies consider family planning in a low income country and kidney allocation in a high income country. RESULTS: Both case studies find that the least explainable models hardly offer improvements in effectiveness and equity over explainable alternatives. DISCUSSION: As this may more widely apply to health resource allocation decisions, explainable analytics, which are more likely to be trusted and used, might better enable progress towards SDG3 for now. Future research on explainability, also in relation to equity and fairness of allocation policies, can help deliver on the promise of advanced predictive and prescriptive analytics.",
      "authors": "van de Klundert Joris; de Vries Harwin; P\u00e9rez-Galarce Francisco; Valdes Nieves; Simon Felipe",
      "year": "2025",
      "journal": "Frontiers in health services",
      "doi": "10.3389/frhs.2025.1545864",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40444224/",
      "mesh_terms": "",
      "keywords": "effectiveness; equity; explainability; explainable AI; family planning; healthcare analytics; kidney allocation",
      "pub_types": "Journal Article",
      "pmcid": "PMC12119484",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38685924",
      "title": "Health equity assessment of machine learning performance (HEAL): a framework and dermatology AI model case study.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) has repeatedly been shown to encode historical inequities in healthcare. We aimed to develop a framework to quantitatively assess the performance equity of health AI technologies and to illustrate its utility via a case study. METHODS: Here, we propose a methodology to assess whether health AI technologies prioritise performance for patient populations experiencing worse outcomes, that is complementary to existing fairness metrics. We developed the Health Equity Assessment of machine Learning performance (HEAL) framework designed to quantitatively assess the performance equity of health AI technologies via a four-step interdisciplinary process to understand and quantify domain-specific criteria, and the resulting HEAL metric. As an illustrative case study (analysis conducted between October 2022 and January 2023), we applied the HEAL framework to a dermatology AI model. A set of 5420 teledermatology cases (store-and-forward cases from patients of 20 years or older, submitted from primary care providers in the USA and skin cancer clinics in Australia), enriched for diversity in age, sex and race/ethnicity, was used to retrospectively evaluate the AI model's HEAL metric, defined as the likelihood that the AI model performs better for subpopulations with worse average health outcomes as compared to others. The likelihood that AI performance was anticorrelated to pre-existing health outcomes was estimated using bootstrap methods as the probability that the negated Spearman's rank correlation coefficient (i.e., \"R\") was greater than zero. Positive values of R suggest that subpopulations with poorer health outcomes have better AI model performance. Thus, the HEAL metric, defined as p (R\u00a0>0), measures how likely the AI technology is to prioritise performance for subpopulations with worse average health outcomes as compared to others (presented as a percentage below). Health outcomes were quantified as disability-adjusted life years (DALYs) when grouping by sex and age, and years of life lost (YLLs) when grouping by race/ethnicity. AI performance was measured as top-3 agreement with the reference diagnosis from a panel of 3 dermatologists per case. FINDINGS: Across all dermatologic conditions, the HEAL metric was 80.5% for prioritizing AI performance of racial/ethnic subpopulations based on YLLs, and 92.1% and 0.0% respectively for prioritizing AI performance of sex and age subpopulations based on DALYs. Certain dermatologic conditions were significantly associated with greater AI model performance compared to a reference category of less common conditions. For skin cancer conditions, the HEAL metric was 73.8% for prioritizing AI performance of age subpopulations based on DALYs. INTERPRETATION: Analysis using the proposed HEAL framework showed that the dermatology AI model prioritised performance for race/ethnicity, sex (all conditions) and age (cancer conditions) subpopulations with respect to pre-existing health disparities. More work is needed to investigate ways of promoting equitable AI performance across age for non-cancer conditions and to better understand how AI models can contribute towards improving equity in health outcomes. FUNDING: Google LLC.",
      "authors": "Schaekermann Mike; Spitz Terry; Pyles Malcolm; Cole-Lewis Heather; Wulczyn Ellery; Pfohl Stephen R; Martin Donald; Jaroensri Ronnachai; Keeling Geoff; Liu Yuan; Farquhar Stephanie; Xue Qinghan; Lester Jenna; Hughes C\u00edan; Strachan Patricia; Tan Fraser; Bui Peggy; Mermel Craig H; Peng Lily H; Matias Yossi; Corrado Greg S; Webster Dale R; Virmani Sunny; Semturs Christopher; Liu Yun; Horn Ivor; Cameron Chen Po-Hsuan",
      "year": "2024",
      "journal": "EClinicalMedicine",
      "doi": "10.1016/j.eclinm.2024.102479",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38685924/",
      "mesh_terms": "",
      "keywords": "Artificial intelligence; Dermatology; Health equity; Machine learning",
      "pub_types": "Journal Article",
      "pmcid": "PMC11056401",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40927497",
      "title": "Beyond the Algorithm: A Perspective on Tackling Bias and Cultural Sensitivity in AI-Guided Aesthetic Standards for Cosmetic Surgery in the Middle East and North Africa (MENA) Region.",
      "abstract": "Artificial intelligence (AI) is increasingly reshaping cosmetic surgery by enhancing surgical planning, predicting outcomes, and enabling objective aesthetic assessment. Through narrative synthesis of existing literature and case studies, this perspective paper explores the issue of algorithmic bias in AI-powered aesthetic technologies and presents a framework for culturally sensitive application within cosmetic surgery practices in the Middle East and North Africa (MENA) region. Existing AI systems are predominantly trained on datasets that underrepresent MENA phenotypes, resulting in aesthetic recommendations that disproportionately reflect Western beauty ideals. The MENA region, however, encompasses a broad spectrum of beauty standards that merge traditional cultural aesthetics with modern global trends, posing unique challenges for AI integration. To ensure ethical and clinically relevant deployment, AI systems must undergo fundamental changes in algorithm design, including the incorporation of culturally diverse datasets with adequate MENA representation, implementation of cultural competency principles, and active collaboration with regional healthcare professionals. The framework outlines concrete criteria for evaluating cultural representativeness in AI training data and outcome assessments, supporting future empirical validation. Developing culturally aware AI tools is both a moral obligation and a clinical priority. This framework provides both a moral imperative and clinical pathway for ensuring AI serves to support, rather than homogenize, the region's diverse aesthetic traditions.",
      "authors": "Makhseed Abdulrahman; Arian Husain; Shuaib Ali",
      "year": "2025",
      "journal": "Clinical, cosmetic and investigational dermatology",
      "doi": "10.2147/CCID.S543045",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40927497/",
      "mesh_terms": "",
      "keywords": "MENA region; aesthetic medicine; algorithmic bias; artificial intelligence; cosmetic surgery; cultural competency; facial analysis; health equity; medical ethics",
      "pub_types": "Journal Article",
      "pmcid": "PMC12416507",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39753967",
      "title": "Evaluating generalizability of oncology trial results to real-world patients using machine learning-based trial emulations.",
      "abstract": "Randomized controlled trials (RCTs) evaluating anti-cancer agents often lack generalizability to real-world oncology patients. Although restrictive eligibility criteria contribute to this issue, the role of selection bias related to prognostic risk remains unclear. In this study, we developed TrialTranslator, a framework designed to systematically evaluate the generalizability of RCTs for oncology therapies. Using a nationwide database of electronic health records from Flatiron Health, this framework emulates RCTs across three prognostic phenotypes identified through machine learning models. We applied this approach to 11 landmark RCTs that investigated anti-cancer regimens considered standard of care for the four most prevalent advanced solid malignancies. Our analyses reveal that patients in low-risk and medium-risk phenotypes exhibit survival times and treatment-associated survival benefits similar to those observed in RCTs. In contrast, high-risk phenotypes show significantly lower survival times and treatment-associated survival benefits compared to RCTs. Our results were corroborated by a comprehensive robustness assessment, including examinations of specific patient subgroups, holdout validation and semi-synthetic data simulation. These findings suggest that the prognostic heterogeneity among real-world oncology patients plays a substantial role in the limited generalizability of RCT results. Machine learning frameworks may facilitate individual patient-level decision support and estimation of real-world treatment benefits to guide trial design.",
      "authors": "Orcutt Xavier; Chen Kan; Mamtani Ronac; Long Qi; Parikh Ravi B",
      "year": "2025",
      "journal": "Nature medicine",
      "doi": "10.1038/s41591-024-03352-5",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39753967/",
      "mesh_terms": "Humans; Machine Learning; Neoplasms; Randomized Controlled Trials as Topic; Prognosis; Electronic Health Records; Female; Antineoplastic Agents; Male; Medical Oncology",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC11835724",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38240671",
      "title": "Disparities in the Demographic Composition of The Cancer Imaging Archive.",
      "abstract": "Purpose To characterize the demographic distribution of The Cancer Imaging Archive (TCIA) studies and compare them with those of the U.S. cancer population. Materials and Methods In this retrospective study, data from TCIA studies were examined for the inclusion of demographic information. Of 189 studies in TCIA up until April 2023, a total of 83 human cancer studies were found to contain supporting demographic data. The median patient age and the sex, race, and ethnicity proportions of each study were calculated and compared with those of the U.S. cancer population, provided by the Surveillance, Epidemiology, and End Results Program and the Centers for Disease Control and Prevention U.S. Cancer Statistics Data Visualizations Tool. Results The median age of TCIA patients was found to be 6.84 years lower than that of the U.S. cancer population (P = .047) and contained more female than male patients (53% vs 47%). American Indian and Alaska Native, Black or African American, and Hispanic patients were underrepresented in TCIA studies by 47.7%, 35.8%, and 14.7%, respectively, compared with the U.S. cancer population. Conclusion The results demonstrate that the patient demographics of TCIA data sets do not reflect those of the U.S. cancer population, which may decrease the generalizability of artificial intelligence radiology tools developed using these imaging data sets. Keywords: Ethics, Meta-Analysis, Health Disparities, Cancer Health Disparities, Machine Learning, Artificial Intelligence, Race, Ethnicity, Sex, Age, Bias Published under a CC BY 4.0 license.",
      "authors": "Dulaney Aidan; Virostko John",
      "year": "2024",
      "journal": "Radiology. Imaging cancer",
      "doi": "10.1148/rycan.230100",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38240671/",
      "mesh_terms": "Female; Humans; Male; Artificial Intelligence; Ethnicity; Neoplasms; Retrospective Studies; Racial Groups; Datasets as Topic",
      "keywords": "Age; Artificial Intelligence; Bias; Cancer Health Disparities; Ethics; Ethnicity; Health Disparities; Machine Learning; Meta-Analysis; Race; Sex",
      "pub_types": "Journal Article",
      "pmcid": "PMC10825717",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38214974",
      "title": "Enhancing Health Equity by Predicting Missed Appointments in Health Care: Machine Learning Study.",
      "abstract": "BACKGROUND: The phenomenon of patients missing booked appointments without canceling them-known as Did Not Show (DNS), Did Not Attend (DNA), or Failed To Attend (FTA)-has a detrimental effect on patients' health and results in massive health care resource wastage. OBJECTIVE: Our objective was to develop machine learning (ML) models and evaluate their performance in predicting the likelihood of DNS for hospital outpatient appointments at the MidCentral District Health Board (MDHB) in New Zealand. METHODS: We sourced 5 years of MDHB outpatient records (a total of 1,080,566 outpatient visits) to build the ML prediction models. We developed 3 ML models using logistic regression, random forest, and Extreme Gradient Boosting (XGBoost). Subsequently, 10-fold cross-validation and hyperparameter tuning were deployed to minimize model bias and boost the algorithms' prediction strength. All models were evaluated against accuracy, sensitivity, specificity, and area under the receiver operating characteristic (AUROC) curve metrics. RESULTS: Based on 5 years of MDHB data, the best prediction classifier was XGBoost, with an area under the curve (AUC) of 0.92, sensitivity of 0.83, and specificity of 0.85. The patients' DNS history, age, ethnicity, and appointment lead time significantly contributed to DNS prediction. An ML system trained on a large data set can produce useful levels of DNS prediction. CONCLUSIONS: This research is one of the very first published studies that use ML technologies to assist with DNS management in New Zealand. It is a proof of concept and could be used to benchmark DNS predictions for the MDHB and other district health boards. We encourage conducting additional qualitative research to investigate the root cause of DNS issues and potential solutions. Addressing DNS using better strategies potentially can result in better utilization of health care resources and improve health equity.",
      "authors": "Yang Yi; Madanian Samaneh; Parry David",
      "year": "2024",
      "journal": "JMIR medical informatics",
      "doi": "10.2196/48273",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38214974/",
      "mesh_terms": "",
      "keywords": "Did Not Attend; Did Not Show; appointment nonadherence; data analytics; decision support system; health care operation; health equity; machine learning; patients no-show; prediction; predictive modeling",
      "pub_types": "Journal Article",
      "pmcid": "PMC10818230",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40206564",
      "title": "AI's ongoing impact: Implications of AI's effects on health equity for women's healthcare providers.",
      "abstract": "OBJECTIVE: To assess the effects of the current use of artificial intelligence (AI) in women's health on health equity, specifically in primary and secondary prevention efforts among women. METHODS: Two databases, Scopus and PubMed, were used to conduct this narrative review. The keywords included \"artificial intelligence,\" \"machine learning,\" \"women's health,\" \"screen,\" \"risk factor,\" and \"prevent,\" and papers were filtered only to include those about AI models that general practitioners may use. RESULTS: Of the 18 articles reviewed, 8 articles focused on risk factor modeling under primary prevention, and 10 articles focused on screening tools under secondary prevention. Gaps were found in the ability of AI models to train using large, diverse datasets that were reflective of the population it is intended for. Lack of these datasets was frequently identified as a limitation in the papers reviewed (n = 7). CONCLUSIONS: Minority, low-income women have poor access to health care and are, therefore, not well represented in the datasets AI uses to train, which risks introducing bias in its output. To mitigate this, more datasets should be developed to validate AI models, and AI in women's health should expand to include conditions that affect men and women to provide a gendered lens on these conditions. Public health, medical, and technology entities need to collaborate to regulate the development and use of AI in health care at a standard that reduces bias.",
      "authors": "Vadlamani Suman; Wachira Elizabeth",
      "year": "2025",
      "journal": "Revista panamericana de salud publica = Pan American journal of public health",
      "doi": "10.26633/RPSP.2025.19",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40206564/",
      "mesh_terms": "",
      "keywords": "Artificial intelligence; ethics; primary prevention; secondary prevention; women\u2019s health",
      "pub_types": "Journal Article",
      "pmcid": "PMC11980523",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35511151",
      "title": "An objective framework for evaluating unrecognized bias in medical AI models predicting COVID-19 outcomes.",
      "abstract": "OBJECTIVE: The increasing translation of artificial intelligence (AI)/machine learning (ML) models into clinical practice brings an increased risk of direct harm from modeling bias; however, bias remains incompletely measured in many medical AI applications. This article aims to provide a framework for objective evaluation of medical AI from multiple aspects, focusing on binary classification models. MATERIALS AND METHODS: Using data from over 56\u00a0000 Mass General Brigham (MGB) patients with confirmed severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), we evaluate unrecognized bias in 4 AI models developed during the early months of the pandemic in Boston, Massachusetts that predict risks of hospital admission, ICU admission, mechanical ventilation, and death after a SARS-CoV-2 infection purely based on their pre-infection longitudinal medical records. Models were evaluated both retrospectively and prospectively using model-level metrics of discrimination, accuracy, and reliability, and a novel individual-level metric for error. RESULTS: We found inconsistent instances of model-level bias in the prediction models. From an individual-level aspect, however, we found most all models performing with slightly higher error rates for older patients. DISCUSSION: While a model can be biased against certain protected groups (ie, perform worse) in certain tasks, it can be at the same time biased towards another protected group (ie, perform better). As such, current bias evaluation studies may lack a full depiction of the variable effects of a model on its subpopulations. CONCLUSION: Only a holistic evaluation, a diligent search for unrecognized bias, can provide enough information for an unbiased judgment of AI bias that can invigorate follow-up investigations on identifying the underlying roots of bias and ultimately make a change.",
      "authors": "Estiri Hossein; Strasser Zachary H; Rashidian Sina; Klann Jeffrey G; Wagholikar Kavishwar B; McCoy Thomas H; Murphy Shawn N",
      "year": "2022",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "doi": "10.1093/jamia/ocac070",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35511151/",
      "mesh_terms": "Artificial Intelligence; COVID-19; Humans; Reproducibility of Results; Retrospective Studies; SARS-CoV-2",
      "keywords": "COVID-19; bias; electronic health records; medical AI; predictive model",
      "pub_types": "Journal Article",
      "pmcid": "PMC9277645",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "31665002",
      "title": "Key challenges for delivering clinical impact with artificial intelligence.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) research in healthcare is accelerating rapidly, with potential applications being demonstrated across various domains of medicine. However, there are currently limited examples of such techniques being successfully deployed into clinical practice. This article explores the main challenges and limitations of AI in healthcare, and considers the steps required to translate these potentially transformative technologies from research to clinical practice. MAIN BODY: Key challenges for the translation of AI systems in healthcare include those intrinsic to the science of machine learning, logistical difficulties in implementation, and consideration of the barriers to adoption as well as of the necessary sociocultural or pathway changes. Robust peer-reviewed clinical evaluation as part of randomised controlled trials should be viewed as the gold standard for evidence generation, but conducting these in practice may not always be appropriate or feasible. Performance metrics should aim to capture real clinical applicability and be understandable to intended users. Regulation that balances the pace of innovation with the potential for harm, alongside thoughtful post-market surveillance, is required to ensure that patients are not exposed to dangerous interventions nor deprived of access to beneficial innovations. Mechanisms to enable direct comparisons of AI systems must be developed, including the use of independent, local and representative test sets. Developers of AI algorithms must be vigilant to potential dangers, including dataset shift, accidental fitting of confounders, unintended discriminatory bias, the challenges of generalisation to new populations, and the unintended negative consequences of new algorithms on health outcomes. CONCLUSION: The safe and timely translation of AI research into clinically validated and appropriately regulated systems that can benefit everyone is challenging. Robust clinical evaluation, using metrics that are intuitive to clinicians and ideally go beyond measures of technical accuracy to include quality of care and patient outcomes, is essential. Further work is required (1) to identify themes of algorithmic bias and unfairness while developing mitigations to address these, (2) to reduce brittleness and improve generalisability, and (3) to develop methods for improved interpretability of machine learning predictions. If these goals can be achieved, the benefits for patients are likely to be transformational.",
      "authors": "Kelly Christopher J; Karthikesalingam Alan; Suleyman Mustafa; Corrado Greg; King Dominic",
      "year": "2019",
      "journal": "BMC medicine",
      "doi": "10.1186/s12916-019-1426-2",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31665002/",
      "mesh_terms": "Algorithms; Artificial Intelligence; Delivery of Health Care; Humans; Peer Review",
      "keywords": "Algorithms; Artificial intelligence; Evaluation; Machine learning; Regulation; Translation",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC6821018",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "29895513",
      "title": "Mobile Phone Cognitive Bias Modification Research Platform for Substance Use Disorders: Protocol for a Feasibility Study.",
      "abstract": "BACKGROUND: Cognitive biases refer to automatic attentional and interpretational tendencies, which could be retained by cognitive bias modification interventions. Cristea et al and Jones et al have published reviews (in 2016 and 2017 respectively) on the effectiveness of such interventions. The advancement of technologies such as electronic health (eHealth) and mobile health (mHealth) has led to them being harnessed for the delivery of cognitive bias modification. To date, at least eight studies have demonstrated the feasibility of mobile technologies for the delivery of cognitive bias modification. Most of the studies are limited to a description of the conventional cognitive bias modification methodology that has been adopted. None of the studies shared the developmental process for the methodology involved, such that future studies could adopt it in the cost-effective replication of such interventions. OBJECTIVE: It is important to have a common platform that could facilitate the design and customization of cognitive bias modification interventions for a variety of psychiatric and addictive disorders. It is the aim of the current research protocol to describe the design of a research platform that allows for customization of cognitive bias modification interventions for addictive disorders. METHODS: A multidisciplinary team of 2 addiction psychiatrists, a psychologist with expertise in cognitive bias modification, and a computer engineer, were involved in the development of the intervention. The proposed platform would comprise of a mobile phone version of the cognitive bias task which is controlled by a server that could customize the algorithm for the tasks and collate the reaction-time data in realtime. The server would also allow the researcher to program the specific set of images that will be present in the task. The mobile phone app would synchronize with the backend server in real-time. An open-sourced cross-platform gaming software from React Native was used in the current development. RESULTS: Multimedia Appendix 1 contains a video demonstrating the operation of the app, as well as a sample dataset of the reaction times (used for the computation of attentional biases) captured by the app. CONCLUSIONS: The current design can be utilized for cognitive bias modification across a spectrum of disorders and is not limited to one disorder. It will be of value for future research to utilize the above platform and compare the efficacy of mHealth approaches, such as the one described in this study, with conventional Web-based approaches in the delivery of attentional bias modification interventions. REGISTERED REPORT IDENTIFIER: RR1-10.2196/9740.",
      "authors": "Zhang Melvyn; Ying JiangBo; Song Guo; Fung Daniel Ss; Smith Helen",
      "year": "2018",
      "journal": "JMIR research protocols",
      "doi": "10.2196/resprot.9740",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29895513/",
      "mesh_terms": "",
      "keywords": "attention bias modification; development; eHealth; mHealth",
      "pub_types": "Journal Article",
      "pmcid": "PMC6019844",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39747461",
      "title": "Predicting pediatric patient rehabilitation outcomes after spinal deformity surgery with artificial intelligence.",
      "abstract": "BACKGROUND: Adolescent idiopathic scoliosis (AIS) is the most common type of scoliosis, affecting 1-4% of adolescents. The Scoliosis Research Society-22R (SRS-22R), a health-related quality-of-life instrument for AIS, has allowed orthopedists to measure subjective patient outcomes before and after corrective surgery beyond objective radiographic measurements. However, research has revealed that there is no significant correlation between the correction rate in major radiographic parameters and improvements in patient-reported outcomes (PROs), making it difficult to incorporate PROs into personalized surgical planning. METHODS: The objective of this study is to develop an artificial intelligence (AI)-enabled surgical planning and counseling support system for post-operative patient rehabilitation outcomes prediction in order to facilitate personalized AIS patient care. A unique multi-site cohort of 455 pediatric patients undergoing spinal fusion surgery at two Shriners Children's hospitals from 2010 is investigated in our analysis. In total, 171 pre-operative clinical features are used to train six machine-learning models for post-operative outcomes prediction. We further employ explainability analysis to quantify the contribution of pre-operative radiographic and questionnaire parameters in predicting patient surgical outcomes. Moreover, we enable responsible AI by calibrating model confidence for human intervention and mitigating health disparities for algorithm fairness. RESULTS: The best prediction model achieves an area under receiver operating curve (AUROC) performance of 0.86, 0.85, and 0.83 for individual SRS-22R question response prediction over three-time horizons from pre-operation to 6-month, 1-year, and 2-year post-operation, respectively. Additionally, we demonstrate the efficacy of our proposed prediction method to predict other patient rehabilitation outcomes based on minimal clinically important differences (MCID) and correction rates across all three-time horizons. CONCLUSIONS: Based on the relationship analysis, we suggest additional attention to sagittal parameters (e.g., lordosis, sagittal vertical axis) and patient self-image beyond major Cobb angles to improve surgical decision-making for AIS patients. In the age of personalized medicine, the proposed responsible AI-enabled clinical decision-support system may facilitate pre-operative counseling and shared decision-making within real-world clinical settings.",
      "authors": "Shi Wenqi; Giuste Felipe O; Zhu Yuanda; Tamo Ben J; Nnamdi Micky C; Hornback Andrew; Carpenter Ashley M; Hilton Coleman; Iwinski Henry J; Wattenbarger J Michael; Wang May D",
      "year": "2025",
      "journal": "Communications medicine",
      "doi": "10.1038/s43856-024-00726-1",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39747461/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC11697361",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39803613",
      "title": "Validation, bias assessment, and optimization of the UNAFIED 2-year risk prediction model for undiagnosed atrial fibrillation using national electronic health data.",
      "abstract": "BACKGROUND: Prediction models for atrial fibrillation (AF) may enable earlier detection and guideline-directed treatment decisions. However, model bias may lead to inaccurate predictions and unintended consequences. OBJECTIVE: The purpose of this study was to validate, assess bias, and improve generalizability of \"UNAFIED-10,\" a 2-year, 10-variable predictive model of undiagnosed AF in a national data set (originally developed using the Indiana Network for Patient Care regional data). METHODS: UNAFIED-10 was validated and optimized using Optum de-identified electronic health record data set. AF diagnoses were recorded in the January 2018-December 2019 period (outcome period), with January 2016-December 2017 as the baseline period. Validation cohorts (patients with AF and non-AF controls, aged \u226540 years) comprised the full imbalanced and randomly sampled balanced data sets. Model performance and bias in patient subpopulations based on sex, insurance, race, and region were evaluated. RESULTS: Of the 6,058,657 eligible patients (mean age 60 \u00b1 12 years), 4.1% (n = 246,975) had their first AF diagnosis within the outcome period. The validated UNAFIED-10 model achieved a higher C-statistic (0.85 [95% confidence interval 0.85-0.86] vs 0.81 [0.80-0.81]) and sensitivity (86% vs 74%) but lower specificity (66% vs 74%) than the original UNAFIED-10 model. During retraining and optimization, the variables insurance, shock, and albumin were excluded to address bias and improve generalizability. This generated an 8-variable model (UNAFIED-8) with consistent performance. CONCLUSION: UNAFIED-10, developed using regional patient data, displayed consistent performance in a large national data set. UNAFIED-8 is more parsimonious and generalizable for using advanced analytics for AF detection. Future directions include validation on additional data sets.",
      "authors": "Ateya Mohammad; Aristeridou Danai; Sands George H; Zielinski Jessica; Grout Randall W; Colavecchia A Carmine; Wazni Oussama; Haque Saira N",
      "year": "2024",
      "journal": "Heart rhythm O2",
      "doi": "10.1016/j.hroo.2024.09.010",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39803613/",
      "mesh_terms": "",
      "keywords": "Atrial fibrillation; Electronic health record; Machine learning; Predictive model; Screening",
      "pub_types": "Journal Article",
      "pmcid": "PMC11721729",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36465087",
      "title": "Personalised depression forecasting using mobile sensor data and ecological momentary assessment.",
      "abstract": "INTRODUCTION: Digital health interventions are an effective way to treat depression, but it is still largely unclear how patients' individual symptoms evolve dynamically during such treatments. Data-driven forecasts of depressive symptoms would allow to greatly improve the personalisation of treatments. In current forecasting approaches, models are often trained on an entire population, resulting in a general model that works overall, but does not translate well to each individual in clinically heterogeneous, real-world populations. Model fairness across patient subgroups is also frequently overlooked. Personalised models tailored to the individual patient may therefore be promising. METHODS: We investigate different personalisation strategies using transfer learning, subgroup models, as well as subject-dependent standardisation on a newly-collected, longitudinal dataset of depression patients undergoing treatment with a digital intervention ( N = 65 patients recruited). Both passive mobile sensor data as well as ecological momentary assessments were available for modelling. We evaluated the models' ability to predict symptoms of depression (Patient Health Questionnaire-2; PHQ-2) at the end of each day, and to forecast symptoms of the next day. RESULTS: In our experiments, we achieve a best mean-absolute-error (MAE) of 0.801 (25% improvement) for predicting PHQ-2 values at the end of the day with subject-dependent standardisation compared to a non-personalised baseline ( MAE = 1.062 ). For one day ahead-forecasting, we can improve the baseline of 1.539 by 12 % to a MAE of 1.349 using a transfer learning approach with shared common layers. In addition, personalisation leads to fairer models at group-level. DISCUSSION: Our results suggest that personalisation using subject-dependent standardisation and transfer learning can improve predictions and forecasts, respectively, of depressive symptoms in participants of a digital depression intervention. We discuss technical and clinical limitations of this approach, avenues for future investigations, and how personalised machine learning architectures may be implemented to improve existing digital interventions for depression.",
      "authors": "Kathan Alexander; Harrer Mathias; K\u00fcster Ludwig; Triantafyllopoulos Andreas; He Xiangheng; Milling Manuel; Gerczuk Maurice; Yan Tianhao; Rajamani Srividya Tirunellai; Heber Elena; Grossmann Inga; Ebert David D; Schuller Bj\u00f6rn W",
      "year": "2022",
      "journal": "Frontiers in digital health",
      "doi": "10.3389/fdgth.2022.964582",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36465087/",
      "mesh_terms": "",
      "keywords": "depression; forecasting; mHealth; machine learning; mental illness; personalised models",
      "pub_types": "Journal Article",
      "pmcid": "PMC9715619",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39312289",
      "title": "Evaluating the Bias in Hospital Data: Automatic Preprocessing of Patient Pathways Algorithm Development and Validation Study.",
      "abstract": "BACKGROUND: The optimization of patient care pathways is crucial for hospital managers in the context of a scarcity of medical resources. Assuming unlimited capacities, the pathway of a patient would only be governed by pure medical logic to meet at best the patient's needs. However, logistical limitations (eg, resources such as inpatient beds) are often associated with delayed treatments and may ultimately affect patient pathways. This is especially true for unscheduled patients-when a patient in the emergency department needs to be admitted to another medical unit without disturbing the flow of planned hospitalizations. OBJECTIVE: In this study, we proposed a new framework to automatically detect activities in patient pathways that may be unrelated to patients' needs but rather induced by logistical limitations. METHODS: The scientific contribution lies in a method that transforms a database of historical pathways with bias into 2 databases: a labeled pathway database where each activity is labeled as relevant (related to a patient's needs) or irrelevant (induced by logistical limitations) and a corrected pathway database where each activity corresponds to the activity that would occur assuming unlimited resources. The labeling algorithm was assessed through medical expertise. In total, 2 case studies quantified the impact of our method of preprocessing health care data using process mining and discrete event simulation. RESULTS: Focusing on unscheduled patient pathways, we collected data covering 12 months of activity at the Groupe Hospitalier Bretagne Sud in France. Our algorithm had 87% accuracy and demonstrated its usefulness for preprocessing traces and obtaining a clean database. The 2 case studies showed the importance of our preprocessing step before any analysis. The process graphs of the processed data had, on average, 40% (SD 10%) fewer variants than the raw data. The simulation revealed that 30% of the medical units had >1 bed difference in capacity between the processed and raw data. CONCLUSIONS: Patient pathway data reflect the actual activity of hospitals that is governed by medical requirements and logistical limitations. Before using these data, these limitations should be identified and corrected. We anticipate that our approach can be generalized to obtain unbiased analyses of patient pathways for other hospitals.",
      "authors": "Uhl Laura; Augusto Vincent; Dalmas Benjamin; Alexandre Youenn; Bercelli Paolo; Jardinaud Fanny; Aloui Saber",
      "year": "2024",
      "journal": "JMIR medical informatics",
      "doi": "10.2196/58978",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39312289/",
      "mesh_terms": "Humans; Algorithms; Critical Pathways; Data Mining; Bias; Emergency Service, Hospital; Databases, Factual",
      "keywords": "bed management; framework; health care data; patient pathway; preprocessing",
      "pub_types": "Journal Article; Validation Study",
      "pmcid": "PMC11459108",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40072927",
      "title": "Data Obfuscation Through Latent Space Projection for Privacy-Preserving AI Governance: Case Studies in Medical Diagnosis and Finance Fraud Detection.",
      "abstract": "BACKGROUND: The increasing integration of artificial intelligence (AI) systems into critical societal sectors has created an urgent demand for robust privacy-preserving methods. Traditional approaches such as differential privacy and homomorphic encryption often struggle to maintain an effective balance between protecting sensitive information and preserving data utility for AI applications. This challenge has become particularly acute as organizations must comply with evolving AI governance frameworks while maintaining the effectiveness of their AI systems. OBJECTIVE: This paper aims to introduce and validate data obfuscation through latent space projection (LSP), a novel privacy-preserving technique designed to enhance AI governance and ensure responsible AI compliance. The primary goal is to develop a method that can effectively protect sensitive data while maintaining essential features necessary for AI model training and inference, thereby addressing the limitations of existing privacy-preserving approaches. METHODS: We developed LSP using a combination of advanced machine learning techniques, specifically leveraging autoencoder architectures and adversarial training. The method projects sensitive data into a lower-dimensional latent space, where it separates sensitive from nonsensitive information. This separation enables precise control over privacy-utility trade-offs. We validated LSP through comprehensive experiments on benchmark datasets and implemented 2 real-world case studies: a health care application focusing on cancer diagnosis and a financial services application analyzing fraud detection. RESULTS: LSP demonstrated superior performance across multiple evaluation metrics. In image classification tasks, the method achieved 98.7% accuracy while maintaining strong privacy protection, providing 97.3% effectiveness against sensitive attribute inference attacks. This performance significantly exceeded that of traditional anonymization and privacy-preserving methods. The real-world case studies further validated LSP's effectiveness, showing robust performance in both health care and financial applications. Additionally, LSP demonstrated strong alignment with global AI governance frameworks, including the General Data Protection Regulation, the California Consumer Privacy Act, and the Health Insurance Portability and Accountability Act. CONCLUSIONS: LSP represents a significant advancement in privacy-preserving AI, offering a promising approach to developing AI systems that respect individual privacy while delivering valuable insights. By embedding privacy protection directly within the machine learning pipeline, LSP contributes to key principles of fairness, transparency, and accountability. Future research directions include developing theoretical privacy guarantees, exploring integration with federated learning systems, and enhancing latent space interpretability. These developments position LSP as a crucial tool for advancing ethical AI practices and ensuring responsible technology deployment in privacy-sensitive domains.",
      "authors": "Vaijainthymala Krishnamoorthy Mahesh",
      "year": "2025",
      "journal": "JMIRx med",
      "doi": "10.2196/70100",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40072927/",
      "mesh_terms": "",
      "keywords": "AI governance; GDPR; General Data Protection Regulation; HIPAA; Health Insurance Portability and Accountability Act; artificial intelligence; compliance; data obfuscation; data utility; differential privacy; k-anonymity; latent space projection; machine learning privacy; medical imaging privacy; privacy-preserving AI; privacy-utility trade-off; responsible AI; secure data sharing",
      "pub_types": "Journal Article",
      "pmcid": "PMC11922095",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39625723",
      "title": "Evaluating Bias-Mitigated Predictive Models of Perinatal Mood and Anxiety Disorders.",
      "abstract": "IMPORTANCE: Machine learning for augmented screening of perinatal mood and anxiety disorders (PMADs) requires thorough consideration of clinical biases embedded in electronic health records (EHRs) and rigorous evaluations of model performance. OBJECTIVE: To mitigate bias in predictive models of PMADs trained on commonly available EHRs. DESIGN, SETTING, AND PARTICIPANTS: This diagnostic study collected data as part of a quality improvement initiative from 2020 to 2023 at Cedars-Sinai Medical Center in Los Angeles, California. The study inclusion criteria were birthing patients aged 14 to 59 years with live birth records and admission to the postpartum unit or the maternal-fetal care unit after delivery. EXPOSURE: Patient-reported race and ethnicity (7 levels) obtained through EHRs. MAIN OUTCOMES AND MEASURES: Logistic regression, random forest, and extreme gradient boosting models were trained to predict 2 binary outcomes: moderate to high-risk (positive) screen assessed using the 9-item Patient Health Questionnaire (PHQ-9), and the Edinburgh Postnatal Depression Scale (EPDS). Each model was fitted with or without reweighing data during preprocessing and evaluated through repeated K-fold cross validation. In every iteration, each model was evaluated on its area under the receiver operating curve (AUROC) and on 2 fairness metrics: demographic parity (DP), and difference in false negatives between races and ethnicities (relative to non-Hispanic White patients). RESULTS: Among 19\u202f430 patients in this study, 1402 (7%) identified as African American or Black, 2371 (12%) as Asian American and Pacific Islander; 1842 (10%) as Hispanic White, 10\u202f942 (56.3%) as non-Hispanic White, 606 (3%) as multiple races, 2146 (11%) as other (not further specified), and 121 (<1%) did not provide this information. The mean (SD) age was 34.1 (4.9) years, and all patients identified as female. Racial and ethnic minority patients were significantly more likely than non-Hispanic White patients to screen positive on both the PHQ-9 (odds ratio, 1.47 [95% CI, 1.23-1.77]) and the EPDS (odds ratio, 1.38 [95% CI, 1.20-1.57]). Mean AUROCs ranged from 0.610 to 0.635 without reweighing (baseline), and from 0.602 to 0.622 with reweighing. Baseline models predicted significantly greater prevalence of postpartum depression for patients who were not non-Hispanic White relative to those who were (mean DP, 0.238 [95% CI, 0.231-0.244]; P\u2009<\u2009.001) and displayed significantly lower false-negative rates (mean difference, -0.184 [95% CI, -0.195 to -0.174]; P\u2009<\u2009.001). Reweighing significantly reduced differences in DP (mean DP with reweighing, 0.022 [95% CI, 0.017-0.026]; P\u2009<\u2009.001) and false-negative rates (mean difference with reweighing, 0.018 [95% CI, 0.008-0.028]; P\u2009<\u2009.001) between racial and ethnic groups. CONCLUSIONS AND RELEVANCE: In this diagnostic study of predictive models of postpartum depression, clinical prediction models trained to predict psychometric screening results from commonly available EHRs achieved modest performance and were less likely to widen existing health disparities in PMAD diagnosis and potentially treatment. These findings suggest that is critical for researchers and physicians to consider their model design (eg, desired target and predictor variables) and evaluate model bias to minimize health disparities.",
      "authors": "Wong Emily F; Saini Anil K; Accortt Eynav E; Wong Melissa S; Moore Jason H; Bright Tiffani J",
      "year": "2024",
      "journal": "JAMA network open",
      "doi": "10.1001/jamanetworkopen.2024.38152",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39625723/",
      "mesh_terms": "Humans; Female; Adult; Pregnancy; Anxiety Disorders; Mood Disorders; Adolescent; Young Adult; Electronic Health Records; Machine Learning; Bias; Pregnancy Complications; Middle Aged; Logistic Models",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC11615713",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38026836",
      "title": "Your robot therapist is not your therapist: understanding the role of AI-powered mental health chatbots.",
      "abstract": "Artificial intelligence (AI)-powered chatbots have the potential to substantially increase access to affordable and effective mental health services by supplementing the work of clinicians. Their 24/7 availability and accessibility through a mobile phone allow individuals to obtain help whenever and wherever needed, overcoming financial and logistical barriers. Although psychological AI chatbots have the ability to make significant improvements in providing mental health care services, they do not come without ethical and technical challenges. Some major concerns include providing inadequate or harmful support, exploiting vulnerable populations, and potentially producing discriminatory advice due to algorithmic bias. However, it is not always obvious for users to fully understand the nature of the relationship they have with chatbots. There can be significant misunderstandings about the exact purpose of the chatbot, particularly in terms of care expectations, ability to adapt to the particularities of users and responsiveness in terms of the needs and resources/treatments that can be offered. Hence, it is imperative that users are aware of the limited therapeutic relationship they can enjoy when interacting with mental health chatbots. Ignorance or misunderstanding of such limitations or of the role of psychological AI chatbots may lead to a therapeutic misconception (TM) where the user would underestimate the restrictions of such technologies and overestimate their ability to provide actual therapeutic support and guidance. TM raises major ethical concerns that can exacerbate one's mental health contributing to the global mental health crisis. This paper will explore the various ways in which TM can occur particularly through inaccurate marketing of these chatbots, forming a digital therapeutic alliance with them, receiving harmful advice due to bias in the design and algorithm, and the chatbots inability to foster autonomy with patients.",
      "authors": "Khawaja Zoha; B\u00e9lisle-Pipon Jean-Christophe",
      "year": "2023",
      "journal": "Frontiers in digital health",
      "doi": "10.3389/fdgth.2023.1278186",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38026836/",
      "mesh_terms": "",
      "keywords": "AI ethics; artificial intelligence; chatbot; mental health services; therapeutic misconception",
      "pub_types": "Journal Article",
      "pmcid": "PMC10663264",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "37289496",
      "title": "Gender Bias When Using Artificial Intelligence to Assess Anorexia Nervosa on Social Media: Data-Driven Study.",
      "abstract": "BACKGROUND: Social media sites are becoming an increasingly important source of information about mental health disorders. Among them, eating disorders are complex psychological problems that involve unhealthy eating habits. In particular, there is evidence showing that signs and symptoms of anorexia nervosa can be traced in social media platforms. Knowing that input data biases tend to be amplified by artificial intelligence algorithms and, in particular, machine learning, these methods should be revised to mitigate biased discrimination in such important domains. OBJECTIVE: The main goal of this study was to detect and analyze the performance disparities across genders in algorithms trained for the detection of anorexia nervosa on social media posts. We used a collection of automated predictors trained on a data set in Spanish containing cases of 177 users that showed signs of anorexia (471,262 tweets) and 326 control cases (910,967 tweets). METHODS: We first inspected the predictive performance differences between the algorithms for male and female users. Once biases were detected, we applied a feature-level bias characterization to evaluate the source of such biases and performed a comparative analysis of such features and those that are relevant for clinicians. Finally, we showcased different bias mitigation strategies to develop fairer automated classifiers, particularly for risk assessment in sensitive domains. RESULTS: Our results revealed concerning predictive performance differences, with substantially higher false negative rates (FNRs) for female samples (FNR=0.082) compared with male samples (FNR=0.005). The findings show that biological processes and suicide risk factors were relevant for classifying positive male cases, whereas age, emotions, and personal concerns were more relevant for female cases. We also proposed techniques for bias mitigation, and we could see that, even though disparities can be mitigated, they cannot be eliminated. CONCLUSIONS: We concluded that more attention should be paid to the assessment of biases in automated methods dedicated to the detection of mental health issues. This is particularly relevant before the deployment of systems that are thought to assist clinicians, especially considering that the outputs of such systems can have an impact on the diagnosis of people at risk.",
      "authors": "Solans Noguero David; Ram\u00edrez-Cifuentes Diana; R\u00edssola Esteban Andr\u00e9s; Freire Ana",
      "year": "2023",
      "journal": "Journal of medical Internet research",
      "doi": "10.2196/45184",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37289496/",
      "mesh_terms": "Female; Humans; Male; Anorexia Nervosa; Artificial Intelligence; Sexism; Social Media; Feeding and Eating Disorders",
      "keywords": "anorexia nervosa; artificial intelligence; gender bias; social media",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC10288345",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "32188481",
      "title": "A geographic identifier assignment algorithm with Bayesian variable selection to identify neighborhood factors associated with emergency department visit disparities for asthma.",
      "abstract": "BACKGROUND: Ecologic health studies often rely on outcomes from health service utilization data that are limited by relatively coarse spatial resolutions and missing geographic information, particularly neighborhood level identifiers. When fine-scale geographic data are missing, the ramifications and strategies for addressing them are not well researched or developed. This study illustrates a novel spatio-temporal framework that combines a geographic identifier assignment (i.e., geographic imputation) algorithm with predictive Bayesian variable selection to identify neighborhood factors associated with disparities in emergency department (ED) visits for asthma. METHODS: ED visit records with missing fine-scale spatial identifiers (~\u200920%) were geocoded using information from known, coarser, misaligned spatial units using an innovative geographic identifier assignment algorithm. We then employed systematic variable selection in a spatio-temporal Bayesian hierarchical model (BHM) predictive framework within the NIMBLE package in R. Our novel methodology is illustrated in an ecologic case study aimed at identifying neighborhood-level predictors of asthma ED visits in South Carolina, United States, from 1999 to 2015. The health outcome was annual ED visit counts in small areas (i.e., census tracts) with primary diagnoses of asthma (ICD9 codes 493.XX) among children ages 5 to 19\u00a0years. RESULTS: We maintained 96% of ED visit records for this analysis. When the algorithm used areal proportions as probabilities for assignment, which addressed differential missingness of census tract identifiers in rural areas, variable selection consistently identified significant neighborhood-level predictors of asthma ED visit risk including pharmacy proximity, average household size, and carbon monoxide interactions. Contrasted with common solutions of removing geographically incomplete records or scaling up analyses, our methodology identified critical differences in parameters estimated, predictors selected, and inferences. We posit that the differences were attributable to improved data resolution, resulting in greater power and less bias. Importantly, without this methodology, we would have inaccurately identified predictors of risk for asthma ED visits, particularly in rural areas. CONCLUSIONS: Our approach innovatively addressed several issues in ecologic health studies, including missing small-area geographic information, multiple correlated neighborhood covariates, and multiscale unmeasured confounding factors. Our methodology could be widely applied to other small-area studies, useful to a range of researchers throughout the world.",
      "authors": "Bozigar Matthew; Lawson Andrew; Pearce John; King Kathryn; Svendsen Erik",
      "year": "2020",
      "journal": "International journal of health geographics",
      "doi": "10.1186/s12942-020-00203-7",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32188481/",
      "mesh_terms": "Adolescent; Algorithms; Asthma; Bayes Theorem; Child; Child, Preschool; Emergency Service, Hospital; Geographic Information Systems; Geography; Health Status Disparities; Humans; Residence Characteristics; South Carolina; Young Adult",
      "keywords": "Air pollution; Bayesian spatio-temporal modeling; Geographic imputation; Hospitalization record data; Respiratory diseases; Rural health; SEA-AIR Study; Social determinants of health; Urban health",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC7081565",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "37185650",
      "title": "Predictive care: a protocol for a computational ethnographic approach to building fair models of inpatient violence in emergency psychiatry.",
      "abstract": "INTRODUCTION: Managing violence or aggression is an ongoing challenge in emergency psychiatry. Many patients identified as being at risk do not go on to become violent or aggressive. Efforts to automate the assessment of risk involve training machine learning (ML) models on data from electronic health records (EHRs) to predict these behaviours. However, no studies to date have examined which patient groups may be over-represented in false positive predictions, despite evidence of social and clinical biases that may lead to higher perceptions of risk in patients defined by intersecting features (eg, race, gender). Because risk assessment can impact psychiatric care (eg, via coercive measures, such as restraints), it is unclear which patients might be underserved or harmed by the application of ML. METHODS AND ANALYSIS: We pilot a computational ethnography to study how the integration of ML into risk assessment might impact acute psychiatric care, with a focus on how EHR data is compiled and used to predict a risk of violence or aggression. Our objectives include: (1) evaluating an ML model trained on psychiatric EHRs to predict violent or aggressive incidents for intersectional bias; and (2) completing participant observation and qualitative interviews in an emergency psychiatric setting to explore how social, clinical and structural biases are encoded in the training data. Our overall aim is to study the impact of ML applications in acute psychiatry on marginalised and underserved patient groups. ETHICS AND DISSEMINATION: The project was approved by the research ethics board at The Centre for Addiction and Mental Health (053/2021). Study findings will be presented in peer-reviewed journals, conferences and shared with service users and providers.",
      "authors": "Sikstrom Laura; Maslej Marta M; Findlay Zoe; Strudwick Gillian; Hui Katrina; Zaheer Juveria; Hill Sean L; Buchman Daniel Z",
      "year": "2023",
      "journal": "BMJ open",
      "doi": "10.1136/bmjopen-2022-069255",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37185650/",
      "mesh_terms": "Humans; Inpatients; Violence; Aggression; Anthropology, Cultural; Psychiatry",
      "keywords": "ethnography; health equity; machine learning; psychiatry; risk assessment",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC10151964",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35089868",
      "title": "Precision Public Health and Structural Racism in the United States: Promoting Health Equity in the COVID-19 Pandemic Response.",
      "abstract": "The COVID-19 pandemic has revealed deeply entrenched structural inequalities that resulted in an excess of mortality and morbidity in certain racial and ethnic groups in the United States. Therefore, this paper examines from the US perspective how structural racism and defective data collection on racial and ethnic minorities can negatively influence the development of precision public health (PPH) approaches to tackle the ongoing COVID-19 pandemic. Importantly, the effects of structural and data racism on the development of fair and inclusive data-driven components of PPH interventions are discussed, such as with the use of machine learning algorithms to predict public health risks. The objective of this viewpoint is thus to inform public health policymaking with regard to the development of ethically sound PPH interventions against COVID-19. Particular attention is given to components of structural racism (eg, hospital segregation, implicit and organizational bias, digital divide, and sociopolitical influences) that are likely to hinder such approaches from achieving their social justice and health equity goals.",
      "authors": "Genevi\u00e8ve Lester Darryl; Martani Andrea; Wangmo Tenzin; Elger Bernice Simone",
      "year": "2022",
      "journal": "JMIR public health and surveillance",
      "doi": "10.2196/33277",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35089868/",
      "mesh_terms": "COVID-19; Data Collection; Health Equity; Humans; Pandemics; Public Health; SARS-CoV-2; Systemic Racism; United States",
      "keywords": "COVID-19; SARS-CoV-2; discrimination; disparity; equity; health equity; inequality; morbidity; mortality; pandemic; precision health; precision public health; public health; racism; social justice; stigma; structural racism",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC8900917",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38360543",
      "title": "Calibration and XGBoost reweighting to reduce coverage and non-response biases in overlapping panel surveys: application to the Healthcare and Social Survey.",
      "abstract": "BACKGROUND: Surveys have been used worldwide to provide information on the COVID-19 pandemic impact so as to prepare and deliver an effective Public Health response. Overlapping panel surveys allow longitudinal estimates and more accurate cross-sectional estimates to be obtained thanks to the larger sample size. However, the problem of non-response is particularly aggravated in the case of panel surveys due to population fatigue with repeated surveys. OBJECTIVE: To develop a new reweighting method for overlapping panel surveys affected by non-response. METHODS: We chose the Healthcare and Social Survey which has an overlapping panel survey design with measurements throughout 2020 and 2021, and random samplings stratified by province and degree of urbanization. Each measurement comprises two samples: a longitudinal sample taken from previous measurements and a new sample taken at each measurement. RESULTS: Our reweighting methodological approach is the result of a two-step process: the original sampling design weights are corrected by modelling non-response with respect to the longitudinal sample obtained in a previous measurement using machine learning techniques, followed by calibration using the auxiliary information available at the population level. It is applied to the estimation of totals, proportions, ratios, and differences between measurements, and to gender gaps in the variable of self-perceived general health. CONCLUSION: The proposed method produces suitable estimators for both cross-sectional and longitudinal samples. For addressing future health crises such as COVID-19, it is therefore necessary to reduce potential coverage and non-response biases in surveys by means of utilizing reweighting techniques as proposed in this study.",
      "authors": "Castro Luis; Rueda Mar\u00eda Del Mar; S\u00e1nchez-Cantalejo Carmen; Ferri Ram\u00f3n; Cabrera-Le\u00f3n Andr\u00e9s",
      "year": "2024",
      "journal": "BMC medical research methodology",
      "doi": "10.1186/s12874-024-02171-z",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38360543/",
      "mesh_terms": "Humans; Cross-Sectional Studies; Calibration; Pandemics; Surveys and Questionnaires; COVID-19; Bias; Delivery of Health Care",
      "keywords": "COVID-19; Machine learning; Non-response bias; Panel surveys; Public health; Sampling",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC10868104",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40745627",
      "title": "Using a large language model (ChatGPT) to assess risk of bias in randomized controlled trials of medical interventions: protocol for a pilot study of interrater agreement with human reviewers.",
      "abstract": "BACKGROUND: Risk of bias (RoB) assessment is an essential part of systematic reviews that requires reading and understanding each eligible trial and RoB tools. RoB assessment is subject to human error and is time-consuming. Machine learning-based tools have been developed to automate RoB assessment using simple models trained on limited corpuses. ChatGPT is a conversational agent based on a large language model (LLM) that was trained on an internet-scale corpus and has demonstrated human-like abilities in multiple areas including healthcare. LLMs might be able to support systematic reviewing tasks such as assessing RoB. We aim to assess interrater agreement in overall (rather than domain-level) RoB assessment between human reviewers and ChatGPT, in randomized controlled trials of interventions within medical interventions. METHODS: We will randomly select 100 individually- or cluster-randomized, parallel, two-arm trials of medical interventions from recent Cochrane systematic reviews that have been assessed using the RoB1 or RoB2 family of tools. We will exclude reviews and trials that were performed under emergency conditions (e.g.,\u00a0COVID-19), as well as public health and welfare interventions. We will use 25 of the trials and human RoB assessments to engineer a ChatGPT prompt for assessing overall RoB, based on trial methods text. We will obtain ChatGPT assessments of RoB for the remaining 75 trials and human assessments. We will then estimate interrater agreement using Cohen's \u03ba. RESULTS: The primary outcome for this study is overall human-ChatGPT interrater agreement. We will report observed agreement with an exact 95% confidence interval, expected agreement under random assessment, Cohen's \u03ba, and a p-value testing the null hypothesis of no difference in agreement. Several other analyses are also planned. CONCLUSIONS: This study is likely to provide the first evidence on interrater agreement between human RoB assessments and those provided by LLMs and will inform subsequent research in this area.",
      "authors": "Rose Christopher James; Bidonde Julia; Ringsten Martin; Glanville Julie; Berg Rigmor C; Cooper Chris; Muller Ashley Elizabeth; Bergsund Hans Bugge; Meneses-Echavez Jose F; Potrebny Thomas",
      "year": "2025",
      "journal": "BMC medical research methodology",
      "doi": "10.1186/s12874-025-02631-0",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40745627/",
      "mesh_terms": "Humans; Bias; Generative Artificial Intelligence; Large Language Models; Machine Learning; Observer Variation; Pilot Projects; Randomized Controlled Trials as Topic; Research Design; Risk Assessment; Systematic Reviews as Topic",
      "keywords": "Artificial intelligence; ChatGPT; Large language model; Machine learning; Risk of bias; Systematic reviewing",
      "pub_types": "Journal Article",
      "pmcid": "PMC12315198",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38105749",
      "title": "Some key questions: Pregnancy intention screening by community health workers.",
      "abstract": "BACKGROUND: Unintended pregnancy contributes to a high burden of maternal and fetal morbidity in the United States, and pregnancy intention screening offers a key strategy to improve preconception health and reproductive health equity. The One Key Question\u00a9 is a pregnancy intention screening tool that asks a single question, \"Would you like to become pregnant in the next year?\" to all reproductive-age women. This study explored the perspectives of community health workers on using One Key Question in community-based settings. OBJECTIVES: This study aimed to identify barriers and facilitators to the use of the One Key Question pregnancy intention screening tool by community health workers who serve reproductive-age women in Salt Lake City, Utah. DESIGN: Using reproductive justice as a guiding conceptual framework, this study employed a qualitative descriptive design. Participants were asked to identify barriers and facilitators to the One Key Question, with open-ended discussion to explore community health workers' knowledge and perceptions about pregnancy intention screening. METHODS: We conducted focus groups with 43 community health workers in Salt Lake City, Utah, from December 2017 through January 2018. Participants were trained on the One Key Question algorithm and asked to identify barriers and facilitators to implementation. All focus groups occurred face-to-face in community settings and used a semi-structured facilitation guide developed by the study Principal Investigator with input from community partners. RESULTS: Pregnancy intention screening is perceived positively by community health workers. Barriers identified include traditional cultural beliefs about modesty and sex, lack of trust in health care providers, and female bias in the One Key Question algorithm. Facilitators include the simplicity of the One Key Question algorithm and the flexibility of One Key Question responses. CONCLUSION: One Key Question is an effective pregnancy intention screening tool in primary care settings but is limited in its capacity to reach those outside the health system. Community-based pregnancy intention screening offers an alternative avenue for implementation of One Key Question that could address many of these barriers and reduce disparities for underserved populations.",
      "authors": "St Clair Stephanie; Dearden Susan; Clark Lauren; Simonsen Sara E",
      "year": "2023",
      "journal": "Women's health (London, England)",
      "doi": "10.1177/17455057231213735",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38105749/",
      "mesh_terms": "Pregnancy; Female; Humans; United States; Intention; Community Health Workers; Prenatal Care",
      "keywords": "community health workers; health equity; pregnancy intention screening; primary care; women\u2019s health",
      "pub_types": "Journal Article",
      "pmcid": "PMC10729636",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39901187",
      "title": "Accounting for racial bias and social determinants of health in a model of hypertension control.",
      "abstract": "BACKGROUND: Hypertension control remains a critical problem and most of the existing literature views it from a clinical perspective, overlooking the role of sociodemographic factors. This study aims to identify patients with not well-controlled hypertension using readily available demographic and socioeconomic features and elucidate important predictive variables. METHODS: In this retrospective cohort study, records from 1/1/2012 to 1/1/2020 at the Boston Medical Center were used. Patients with either a hypertension diagnosis or related records (\u2265\u2009130\u00a0mmHg systolic or\u2009\u2265\u200990\u00a0mmHg diastolic, n\u2009=\u2009164,041) were selected. Models were developed to predict which patients had uncontrolled hypertension defined as systolic blood pressure (SBP) records exceeding 160\u00a0mmHg. RESULTS: The predictive model of high SBP reached an Area Under the Receiver Operating Characteristic Curve of 74.49%\u2009\u00b1\u20090.23%. Age, race, Social Determinants of Health (SDoH), mental health, and cigarette use were predictive of high SBP. Being Black or having critical social needs led to higher probability of uncontrolled SBP. To mitigate model bias and elucidate differences in predictive variables, two separate models were trained for Black and White patients. Black patients face a 4.7 \u00d7 higher False Positive Rate (FPR) and a 0.58 \u00d7 lower False Negative Rate (FNR) compared to White patients. Decision threshold differentiation was implemented to equalize FNR. Race-specific models revealed different sets of social variables predicting high SBP, with Black patients being affected by structural barriers (e.g., food and transportation) and White patients by personal and demographic factors (e.g., marital status). CONCLUSIONS: Models using non-clinical factors can predict which patients exhibit poorly controlled hypertension. Racial and SDoH variables are significant predictors but lead to biased predictive models. Race-specific models are not sufficient to resolve such biases and require further decision threshold tuning. A host of structural socioeconomic factors are identified to be targeted to reduce disparities in hypertension control.",
      "authors": "Hu Yang; Cordella Nicholas; Mishuris Rebecca G; Paschalidis Ioannis Ch",
      "year": "2025",
      "journal": "BMC medical informatics and decision making",
      "doi": "10.1186/s12911-025-02873-4",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39901187/",
      "mesh_terms": "Humans; Hypertension; Social Determinants of Health; Female; Middle Aged; Male; Retrospective Studies; Aged; Racism; Adult; Boston; Black or African American; White",
      "keywords": "Hypertension; Machine learning; Racial bias; Social determinants of health",
      "pub_types": "Journal Article",
      "pmcid": "PMC11792567",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39367027",
      "title": "An intelligent learning system based on electronic health records for unbiased stroke prediction.",
      "abstract": "Stroke has a negative impact on people's lives and is one of the leading causes of death and disability worldwide. Early detection of symptoms can significantly help predict stroke and promote a healthy lifestyle. Researchers have developed several methods to predict strokes using machine learning (ML) techniques. However, the proposed systems have suffered from the following two main problems. The first problem is that the machine learning models are biased due to the uneven distribution of classes in the dataset. Recent research has not adequately addressed this problem, and no preventive measures have been taken. Synthetic Minority Oversampling (SMOTE) has been used to remove bias and balance the training of the proposed ML model. The second problem is to solve the problem of lower classification accuracy of machine learning models. We proposed a learning system that combines an autoencoder with a linear discriminant analysis (LDA) model to increase the accuracy of the proposed ML model for stroke prediction. Relevant features are extracted from the feature space using the autoencoder, and the extracted subset is then fed into the LDA model for stroke classification. The hyperparameters of the LDA model are found using a grid search strategy. However, the conventional accuracy metric does not truly reflect the performance of ML models. Therefore, we employed several evaluation metrics to validate the efficiency of the proposed model. Consequently, we evaluated the proposed model's accuracy, sensitivity, specificity, area under the curve (AUC), and receiver operator characteristic (ROC). The experimental results show that the proposed model achieves a sensitivity and specificity of 98.51% and 97.56%, respectively, with an accuracy of 99.24% and a balanced accuracy of 98.00%.",
      "authors": "Saleem Muhammad Asim; Javeed Ashir; Akarathanawat Wasan; Chutinet Aurauma; Suwanwela Nijasri Charnnarong; Kaewplung Pasu; Chaitusaney Surachai; Deelertpaiboon Sunchai; Srisiri Wattanasak; Benjapolakul Watit",
      "year": "2024",
      "journal": "Scientific reports",
      "doi": "10.1038/s41598-024-73570-x",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39367027/",
      "mesh_terms": "Humans; Stroke; Machine Learning; Electronic Health Records; Female; Male; Aged; Middle Aged; Discriminant Analysis",
      "keywords": "Feature extraction; Imbalance classes; Machine learning; Stroke",
      "pub_types": "Journal Article",
      "pmcid": "PMC11452373",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40124313",
      "title": "Artificial Intelligence Models to Identify Patients with High Probability of Glaucoma Using Electronic Health Records.",
      "abstract": "PURPOSE: Early detection of glaucoma allows for timely treatment to prevent severe vision loss, but screening requires resource-intensive examinations and imaging, which are challenging for large-scale implementation and evaluation. The purpose of this study was to develop artificial intelligence models that can utilize the wealth of data stored in electronic health records (EHRs) to identify patients who have high probability of developing glaucoma, without the use of any dedicated ophthalmic imaging or clinical data. DESIGN: Cohort study. PARTICIPANTS: A total of 64\u00a0735 participants who were \u226518 years of age and had \u22652 separate encounters with eye-related diagnoses recorded in their EHR records in the All of Us Research Program, a national multicenter cohort of patients contributing EHR and survey data, and who were enrolled from May 1, 2018, to July 1,\u00a02022. METHODS: We developed models to predict which patients had a diagnosis of glaucoma, using the following machine learning approaches: (1) penalized logistic regression, (2) XGBoost, and (3) a deep learning architecture that included a 1-dimensional convolutional neural network (1D-CNN) and stacked autoencoders. Model input features included demographics and only the nonophthalmic lab results, measurements, medications, and diagnoses available from structured EHR data. MAIN OUTCOME MEASURES: Evaluation metrics included area under the receiver operating characteristic curve (AUROC). RESULTS: Of 64\u00a0735 patients, 7268 (11.22%) had a glaucoma diagnosis. Overall, AUROC ranged from 0.796 to 0.863. The 1D-CNN model achieved the highest performance with an AUROC score of 0.863 (95% confidence interval [CI], 0.862-0.864). Investigation of 1D-CNN model performance stratified by race/ethnicity showed that AUROC ranged from 0.825 to 0.869 by subpopulation, with the highest performance of 0.869 (95% CI, 0.868-0.870) among the non-Hispanic White subpopulation. CONCLUSIONS: Machine and deep learning models were able to use the extensive systematic data within EHR to identify individuals with glaucoma, without the need for ophthalmic imaging or clinical data. These models could potentially automate identifying high-risk glaucoma patients in EHRs, aiding targeted screening referrals. Additional research is needed to investigate the impact of protected class characteristics such as race/ethnicity on model performance and fairness. FINANCIAL DISCLOSURES: The author(s) have no proprietary or commercial interest in any materials discussed in this article.",
      "authors": "Ravindranath Rohith; Wang Sophia Y",
      "year": "2025",
      "journal": "Ophthalmology science",
      "doi": "10.1016/j.xops.2024.100671",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40124313/",
      "mesh_terms": "",
      "keywords": "Deep learning; Electronic health records; Glaucoma screening; Machine learning",
      "pub_types": "Journal Article",
      "pmcid": "PMC11930135",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40445905",
      "title": "Towards robust medical machine olfaction: Debiasing GC-MS data enhances prostate cancer diagnosis from urine volatiles.",
      "abstract": "Prostate cancer (PCa) is a major, and increasingly global, health concern with current screening and diagnostic tools' severe limitations causing unnecessary, invasive biopsy procedures. While gas chromatography-mass spectrometry (GC-MS) has been used to detect urinary volatile organic compounds (VOCs) associated with PCa, efforts to identify consistent molecular biomarkers have failed to generalize across studies. Inspired by the olfactory diagnostic capabilities of medical detection dogs, we do not reduce chromatograms to a list of compounds and concentrations. Instead, we deploy a machine learning approach that bypasses molecular identification: PCa \"scent character\" signatures are extracted from raw time series data transformed into image representations for classification via convolutional neural networks. To address confounding factors such as sample-source bias, we implement a multi-step pre-processing and debiasing pipeline, including empirical Bayes correction, baseline drift removal, and domain adversarial learning. The resulting model achieves classification performance on par with similarly trained canines, achieving a recall of 88% and an F1-score of 0.78. These findings demonstrate that, at least in the context of PCa detection from urine, machine learning-based scent signature analysis can serve as a fully non-invasive diagnostic alternative, with these early results being also relevant to the wider emergent field of medical machine olfaction.",
      "authors": "Rotteveel Adan; Lee Wen-Yee; Kountouri Zoi; Stefanou Nikolas; Kivell Howard; Gluck Clifford; Zhang Shuguang; Mershin Andreas",
      "year": "2025",
      "journal": "PloS one",
      "doi": "10.1371/journal.pone.0314742",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40445905/",
      "mesh_terms": "Male; Prostatic Neoplasms; Volatile Organic Compounds; Gas Chromatography-Mass Spectrometry; Humans; Machine Learning; Dogs; Animals; Smell; Bayes Theorem; Neural Networks, Computer",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC12124533",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36653067",
      "title": "Evaluation of race/ethnicity-specific survival machine learning models for Hispanic and Black patients with breast cancer.",
      "abstract": "OBJECTIVES: Survival machine learning (ML) has been suggested as a useful approach for forecasting future events, but a growing concern exists that ML models have the potential to cause racial disparities through the data used to train them. This study aims to develop race/ethnicity-specific survival ML models for Hispanic and black women diagnosed with breast cancer to examine whether race/ethnicity-specific ML models outperform the general models trained with all races/ethnicity data. METHODS: We used the data from the US National Cancer Institute's Surveillance, Epidemiology and End Results programme registries. We developed the Hispanic-specific and black-specific models and compared them with the general model using the Cox proportional-hazards model, Gradient Boost Tree, survival tree and survival support vector machine. RESULTS: A total of 322\u2009348 female patients who had breast cancer diagnoses between 1 January 2000 and 31 December 2017 were identified. The race/ethnicity-specific models for Hispanic and black women consistently outperformed the general model when predicting the outcomes of specific race/ethnicity. DISCUSSION: Accurately predicting the survival outcome of a patient is critical in determining treatment options and providing appropriate cancer care. The high-performing models developed in this study can contribute to providing individualised oncology care and improving the survival outcome of black and Hispanic women. CONCLUSION: Predicting the individualised survival outcome of breast cancer can provide the evidence necessary for determining treatment options and high-quality, patient-centred cancer care delivery for under-represented populations. Also, the race/ethnicity-specific ML models can mitigate representation bias and contribute to addressing health disparities.",
      "authors": "Park Jung In; Bozkurt Selen; Park Jong Won; Lee Sunmin",
      "year": "2023",
      "journal": "BMJ health & care informatics",
      "doi": "10.1136/bmjhci-2022-100666",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36653067/",
      "mesh_terms": "Humans; Female; Ethnicity; Breast Neoplasms; Hispanic or Latino; Black People; Proportional Hazards Models",
      "keywords": "artificial intelligence; health equity; informatics; machine learning",
      "pub_types": "Journal Article",
      "pmcid": "PMC9853120",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "33981823",
      "title": "Comparing denominator sources for real-time disease incidence modeling: American Community Survey and WorldPop.",
      "abstract": "Across the United States public health community in 2020, in the midst of a pandemic and increased concern regarding racial/ethnic health disparities, there is widespread concern about our ability to accurately estimate small-area disease incidence rates due to the absence of a recent census to obtain reliable population denominators. 2010 decennial census data are likely outdated, and intercensal population estimates from the Census Bureau, which are less temporally misaligned with real-time disease incidence data, are not recommended for use with small areas. Machine learning-based population estimates are an attractive option but have not been validated for use in epidemiologic studies. Treating 2010 decennial census counts as a \"ground truth\", we conduct a case study to compare the performance of alternative small-area population denominator estimates from surrounding years for modeling real-time disease incidence rates. Our case study focuses on modeling health disparities in census tract incidence rates in Massachusetts, using population size estimates from the American Community Survey (ACS), the most commonly-used intercensal small-area population data in epidemiology, and WorldPop, a machine learning model for high-resolution population size estimation. Through simulation studies and an analysis of real premature mortality data, we evaluate whether WorldPop denominators can provide improved performance relative to ACS for quantifying disparities using both census tract-aggregate and race-stratified modeling approaches. We find that biases induced in parameter estimates due to temporally incompatible incidence and denominator data tend to be larger for race-stratified models than for area-aggregate models. In most scenarios considered here, WorldPop denominators lead to greater bias in estimates of health disparities than ACS denominators. These insights will assist researchers in intercensal years to select appropriate population size estimates for modeling disparities in real-time disease incidence. We highlight implications for health disparity studies in the coming decade, as 2020 census counts may introduce new sources of error.",
      "authors": "Nethery Rachel C; Rushovich Tamara; Peterson Emily; Chen Jarvis T; Waterman Pamela D; Krieger Nancy; Waller Lance; Coull Brent A",
      "year": "2021",
      "journal": "SSM - population health",
      "doi": "10.1016/j.ssmph.2021.100786",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33981823/",
      "mesh_terms": "",
      "keywords": "Health disparities; Population denominators; Real-time incidence modeling",
      "pub_types": "Journal Article",
      "pmcid": "PMC8081984",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "33779570",
      "title": "Digital Mental Health Challenges and the Horizon Ahead for Solutions.",
      "abstract": "The demand outstripping supply of mental health resources during the COVID-19 pandemic presents opportunities for digital technology tools to fill this new gap and, in the process, demonstrate capabilities to increase their effectiveness and efficiency. However, technology-enabled services have faced challenges in being sustainably implemented despite showing promising outcomes in efficacy trials since the early 2000s. The ongoing failure of these implementations has been addressed in reconceptualized models and frameworks, along with various efforts to branch out among disparate developers and clinical researchers to provide them with a key for furthering evaluative research. However, the limitations of traditional research methods in dealing with the complexities of mental health care warrant a diversified approach. The crux of the challenges of digital mental health implementation is the efficacy and evaluation of existing studies. Web-based interventions are increasingly used during the pandemic, allowing for affordable access to psychological therapies. However, a lagging infrastructure and skill base has limited the application of digital solutions in mental health care. Methodologies need to be converged owing to the rapid development of digital technologies that have outpaced the evaluation of rigorous digital mental health interventions and strategies to prevent mental illness. The functions and implications of human-computer interaction require a better understanding to overcome engagement barriers, especially with predictive technologies. Explainable artificial intelligence is being incorporated into digital mental health implementation to obtain positive and responsible outcomes. Investment in digital platforms and associated apps for real-time screening, tracking, and treatment offer the promise of cost-effectiveness in vulnerable populations. Although machine learning has been limited by study conduct and reporting methods, the increasing use of unstructured data has strengthened its potential. Early evidence suggests that the advantages outweigh the disadvantages of incrementing such technology. The limitations of an evidence-based approach require better integration of decision support tools to guide policymakers with digital mental health implementation. There is a complex range of issues with effectiveness, equity, access, and ethics (eg, privacy, confidentiality, fairness, transparency, reproducibility, and accountability), which warrant resolution. Evidence-informed policies, development of eminent digital products and services, and skills to use and maintain these solutions are required. Studies need to focus on developing digital platforms with explainable artificial intelligence-based apps to enhance resilience and guide the treatment decisions of mental health practitioners. Investments in digital mental health should ensure their safety and workability. End users should encourage the use of innovative methods to encourage developers to effectively evaluate their products and services and to render them a worthwhile investment. Technology-enabled services in a hybrid model of care are most likely to be effective (eg, specialists using these services among vulnerable, at-risk populations but not severe cases of mental ill health).",
      "authors": "Balcombe Luke; De Leo Diego",
      "year": "2021",
      "journal": "JMIR mental health",
      "doi": "10.2196/26811",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33779570/",
      "mesh_terms": "",
      "keywords": "COVID-19; challenges; digital mental health implementation; explainable artificial intelligence; human-computer interaction; hybrid model of care; resilience; technology",
      "pub_types": "Journal Article",
      "pmcid": "PMC8077937",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "23113916",
      "title": "The influence of measurement error on calibration, discrimination, and overall estimation of a risk prediction model.",
      "abstract": "BACKGROUND: Self-reported height and weight are commonly collected at the population level; however, they can be subject to measurement error. The impact of this error on predicted risk, discrimination, and calibration of a model that uses body mass index (BMI) to predict risk of diabetes incidence is not known. The objective of this study is to use simulation to quantify and describe the effect of random and systematic error in self-reported height and weight on the performance of a model for predicting diabetes. METHODS: Two general categories of error were examined: random (nondirectional) error and systematic (directional) error on an algorithm relating BMI in kg/m2 to probability of developing diabetes. The cohort used to develop the risk algorithm was derived from 23,403 Ontario residents that responded to the 1996/1997 National Population Health Survey linked to a population-based diabetes registry. The data and algorithm were then simulated to allow for estimation of the impact of these errors on predicted risk using the Hosmer-Lemeshow goodness-of-fit \u03c72 and C-statistic. Simulations were done 500 times with sample sizes of 9,177 for males and 10,618 for females. RESULTS: Simulation data successfully reproduced discrimination and calibration generated from population data. Increasing levels of random error in height and weight reduced the calibration and discrimination of the model. Random error biased the predicted risk upwards whereas systematic error biased predicted risk in the direction of the bias and reduced calibration; however, it did not affect discrimination. CONCLUSION: This study demonstrates that random and systematic errors in self-reported health data have the potential to influence the performance of risk algorithms. Further research that quantifies the amount and direction of error can improve model performance by allowing for adjustments in exposure measurements.",
      "authors": "Rosella Laura C; Corey Paul; Stukel Therese A; Mustard Cam; Hux Jan; Manuel Doug G",
      "year": "2012",
      "journal": "Population health metrics",
      "doi": "10.1186/1478-7954-10-20",
      "url": "https://pubmed.ncbi.nlm.nih.gov/23113916/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC3545925",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41200127",
      "title": "Predicting Child Development Across Literacy, Physical, Learning, and Social-Emotional Domains Using Supervised Machine Learning: A Cross-Sectional Study Based on MICS 2019 Bangladesh.",
      "abstract": "BACKGROUND AND AIMS: Early childhood development (ECD) plays a vital role in shaping a child's health and well-being, influenced by child, family, and environmental factors. To prevent long-term impairments, early detection and intervention are crucial. Using MICS 2019 data, this study applies supervised machine learning to predict ECD across four key domains and identify the most significant predictors and economic strategies. METHODS: In this study, using data of 9346 children obtained from Multiple Indicator Cluster Surveys (MICS) 2019, we evaluated and compared five classifiers: CART, Random Forest, XGBoost, Logistic Regression, and Support Vector Machines (SVM). We have addressed four early developmental domains as our target variables: literacy, numeracy, physics, learning, and social-emotional development of children. Five-fold cross-validation was used to ensure appropriate test error rate estimations and reduce bias. To handle the data imbalance, the Synthetic Minority Oversampling Technique (SMOTE) is used. RESULTS: The analysis shows that most children are developing normally in the learning (90.58%) and physical (98.70%) domains, while delays are highest in literacy-numeracy (71.37%) and social-emotional (27.57%) domains. Among the machine learning models evaluated, Random Forest consistently performed best across all domains, achieving the highest accuracy, particularly in learning (0.83) and physical (0.97) domains. Feature importance analysis identified maternal education, child age, regional location (Division), and socioeconomic status (Wealth Index) as key predictors. Early childhood education and books read at home also play important roles in cognitive and learning outcomes, guiding targeted interventions for child development. CONCLUSIONS: The results show notable differences in early childhood development, particularly in social-emotional and literacy-numeracy domains. Socioeconomic status, early learning experiences, and parental education are key predictors, while physical and social-emotional development are influenced by resources, regional factors, and nutrition. These findings can guide targeted interventions and policies for holistic child development.",
      "authors": "Islam Faizul; Suhel Golam Morshed; Afroz Mahmud; Apu Md Aminul I; Rana Md Jewel; Hossain Tofajjel; Ziba Zaibunnesa; Shariar Md Fahim; Hasan Mohammad Nayeem",
      "year": "2025",
      "journal": "Health science reports",
      "doi": "10.1002/hsr2.71434",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41200127/",
      "mesh_terms": "",
      "keywords": "algorithms; cognition; early childhood development; machine learning; psychomotor performance; social behavior; statistical models",
      "pub_types": "Journal Article",
      "pmcid": "PMC12586350",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35399912",
      "title": "Real-time data of COVID-19 detection with IoT sensor tracking using artificial neural network.",
      "abstract": "The coronavirus pandemic has affected people all over the world and posed a great challenge to international health systems. To aid early detection of coronavirus disease-2019 (COVID-19), this study proposes a real-time detection system based on the Internet of Things framework. The system collects real-time data from users to determine potential coronavirus cases, analyses treatment responses for people who have been treated, and accurately collects and analyses the datasets. Artificial intelligence-based algorithms are an alternative decision-making solution to extract valuable information from clinical data. This study develops a deep learning optimisation system that can work with imbalanced datasets to improve the classification of patients. A synthetic minority oversampling technique is applied to solve the problem of imbalance, and a recursive feature elimination algorithm is used to determine the most effective features. After data balance and extraction of features, the data are split into training and testing sets for validating all models. The experimental predictive results indicate good stability and compatibility of the models with the data, providing maximum accuracy of 98% and precision of 97%. Finally, the developed models are demonstrated to handle data bias and achieve high classification accuracy for patients with COVID-19. The findings of this study may be useful for healthcare organisations to properly prioritise assets.",
      "authors": "Mohammedqasem Roa'a; Mohammedqasim Hayder; Ata Oguz",
      "year": "2022",
      "journal": "Computers & electrical engineering : an international journal",
      "doi": "10.1016/j.compeleceng.2022.107971",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35399912/",
      "mesh_terms": "",
      "keywords": "ANN, Artificial Neural Network; AUC, Area Under Curve; CNN, Convolutional Neural Network; COVID-19; COVID-19, Coronavirus disease; DL, Deep learning; Imbalanced Dataset; Internet of Things; IoT, Internet of Things; ML, Machine learning; RFE, Recursive Feature Elimination; RNN, Recurrent Neural Network; Recursive feature elimination; SMOTE, Synthetic Minority Oversampling Technique; Synthetic minority oversampling technique",
      "pub_types": "Journal Article",
      "pmcid": "PMC8985446",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40631724",
      "title": "Disaggregating Health Differences and Disparities With Machine Learning and Observed-to-expected Ratios: Application to Major Lower Limb Amputation.",
      "abstract": "BACKGROUND: Major lower limb amputation is a devastating but preventable complication of peripheral artery disease. It is unclear whether racial and ethnic and rural differences in amputation rates are due to clinical, hospital, or structural factors. METHODS: We included all peripheral artery disease hospitalizations of patients \u226540 years old between 2017 and 2019 in Florida, Georgia, Maryland, Mississippi, or New York (HCUP State Inpatient Databases). We estimated the expected number of amputations using three models: (1) unadjusted, (2) adjusted for clinical factors, and (3) adjusted for clinical factors, hospital factors, and social determinants of health using least absolute shrinkage and selection operator (LASSO). We calculated and compared observed-to-expected ratios and quantified the role of these factors in amputation rates. RESULTS: Overall, 1,577,061 hospitalizations (990,152 unique patients) and 21,233 major lower limb amputations (1.4%) were included. After accounting for clinical differences, we observed amputation disparities among rural Black, Hispanic, Native American, and White patients and nonrural Black and Native American patients. After accounting for hospital factors and social determinants of health, disparities were no longer present among rural White adults (0.93, 95% confidence interval [CI]: 0.77, 1.09); however, disparities persisted among rural Black (1.26, 95% CI: 1.01, 1.51), Hispanic (1.50, 95% CI: 0.89, 2.12), and Native American patients (1.13, 95% CI: 0.68, 1.58) and nonrural Black (1.12, 95% CI: 1.09, 1.15) and Native American (1.15, 95% CI: 0.86, 1.44) patients. CONCLUSION: Clinical factors did not fully explain differences in amputation rates, and hospital factors and social determinants of health did not fully explain disparities. These findings provide additional evidence that implicit bias is associated with amputation disparities.",
      "authors": "Strassle Paula D; Minc Samantha D; Kalbaugh Corey A; Donneyong Macarius M; Ko Jamie S; McGinigle Katharine L",
      "year": "2025",
      "journal": "Epidemiology (Cambridge, Mass.)",
      "doi": "10.1097/EDE.0000000000001892",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40631724/",
      "mesh_terms": "Humans; Amputation, Surgical; Male; Female; Middle Aged; Aged; Peripheral Arterial Disease; Lower Extremity; Machine Learning; Healthcare Disparities; Health Status Disparities; Adult; United States; Social Determinants of Health; Hospitalization; Rural Population",
      "keywords": "Healthcare disparities; Machine learning; Major lower limb amputation; Peripheral artery disease; Rural health; Social determinants of health; Surgical",
      "pub_types": "Journal Article",
      "pmcid": "PMC12400468",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38844546",
      "title": "Assessing calibration and bias of a deployed machine learning malnutrition prediction model within a large healthcare system.",
      "abstract": "Malnutrition is a frequently underdiagnosed condition leading to increased morbidity, mortality, and healthcare costs. The Mount Sinai Health System (MSHS) deployed a machine learning model (MUST-Plus) to detect malnutrition upon hospital admission. However, in diverse patient groups, a poorly calibrated model may lead to misdiagnosis, exacerbating health care disparities. We explored the model's calibration across different variables and methods to improve calibration. Data from adult patients admitted to five MSHS hospitals from January 1, 2021 - December 31, 2022, were analyzed. We compared MUST-Plus prediction to the registered dietitian's formal assessment. Hierarchical calibration was assessed and compared between the recalibration sample (N\u2009=\u200949,562) of patients admitted between January 1, 2021 - December 31, 2022, and the hold-out sample (N\u2009=\u200917,278) of patients admitted between January 1, 2023 - September 30, 2023. Statistical differences in calibration metrics were tested using bootstrapping with replacement. Before recalibration, the overall model calibration intercept was -1.17 (95% CI: -1.20, -1.14), slope was 1.37 (95% CI: 1.34, 1.40), and Brier score was 0.26 (95% CI: 0.25, 0.26). Both weak and moderate measures of calibration were significantly different between White and Black patients and between male and female patients. Logistic recalibration significantly improved calibration of the model across race and gender in the hold-out sample. The original MUST-Plus model showed significant differences in calibration between White vs. Black patients. It also overestimated malnutrition in females compared to males. Logistic recalibration effectively reduced miscalibration across all patient subgroups. Continual monitoring and timely recalibration can improve model accuracy.",
      "authors": "Liou Lathan; Scott Erick; Parchure Prathamesh; Ouyang Yuxia; Egorova Natalia; Freeman Robert; Hofer Ira S; Nadkarni Girish N; Timsina Prem; Kia Arash; Levin Matthew A",
      "year": "2024",
      "journal": "NPJ digital medicine",
      "doi": "10.1038/s41746-024-01141-5",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38844546/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC11156633",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40933771",
      "title": "Bias correction for nonignorable missing counts of areal HIV new diagnosis.",
      "abstract": "Public health data, such as HIV new diagnoses, are often left-censored due to confidentiality issues. Standard analysis approaches that assume censored values as missing at random often lead to biased estimates and inferior predictions. Motivated by the Philadelphia areal counts of HIV new diagnosis for which all values less than or equal to 5 are suppressed, we propose two methods to reduce the adverse influence of missingness on predictions and imputation of areal HIV new diagnoses. One is the likelihood-based method that integrates the missing mechanism into the likelihood function, and the other is a nonparametric algorithm for matrix factorization imputation. Numerical studies and the Philadelphia data analysis demonstrate that the two proposed methods can significantly improve prediction and imputation based on left-censored HIV data. We also compare the two methods on their robustness to model misspecification and find that both methods appear to be robust for prediction, while their performance for imputation depends on model specification.",
      "authors": "Qu Tianyi; Li Bo; Chan Man-Pui Sally; Albarracin Dolores",
      "year": "2023",
      "journal": "Stat",
      "doi": "10.1002/sta4.555",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40933771/",
      "mesh_terms": "",
      "keywords": "left-censored; likelihood; matrix factorization; missing value; spatiotemporal data",
      "pub_types": "Journal Article",
      "pmcid": "PMC12419480",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "33651310",
      "title": "Excavating FAIR Data: the Case of the Multicenter Animal Spinal Cord Injury Study (MASCIS), Blood Pressure, and Neuro-Recovery.",
      "abstract": "Meta-analyses suggest that the published literature represents only a small minority of the total data collected in biomedical research, with most becoming 'dark data' unreported in the literature. Dark data is due to publication bias toward novel results that confirm investigator hypotheses and omission of data that do not. Publication bias contributes to scientific irreproducibility and failures in bench-to-bedside translation. Sharing dark data by making it Findable, Accessible, Interoperable, and Reusable (FAIR) may reduce the burden of irreproducible science by increasing transparency and support data-driven discoveries beyond the lifecycle of the original study. We illustrate feasibility of dark data sharing by recovering original raw data from the Multicenter Animal Spinal Cord Injury Study (MASCIS), an NIH-funded multi-site preclinical drug trial conducted in the 1990s that tested efficacy of several therapies after a spinal cord injury (SCI). The original drug treatments did not produce clear positive results and MASCIS data were stored in boxes for more than two decades. The goal of the present study was to independently confirm published machine learning findings that perioperative blood pressure is a major predictor of SCI neuromotor outcome (Nielson et al., 2015). We recovered, digitized, and curated the data from 1125 rats from MASCIS. Analyses indicated that high perioperative blood pressure at the time of SCI is associated with poorer health and worse neuromotor outcomes in more severe SCI, whereas low perioperative blood pressure is associated with poorer health and worse neuromotor outcome in moderate SCI. These findings confirm and expand prior results that a narrow window of blood-pressure control optimizes outcome, and demonstrate the value of recovering dark data for assessing reproducibility of findings with implications for precision therapeutic approaches.",
      "authors": "Almeida Carlos A; Torres-Espin Abel; Huie J Russell; Sun Dongming; Noble-Haeusslein Linda J; Young Wise; Beattie Michael S; Bresnahan Jacqueline C; Nielson Jessica L; Ferguson Adam R",
      "year": "2022",
      "journal": "Neuroinformatics",
      "doi": "10.1007/s12021-021-09512-z",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33651310/",
      "mesh_terms": "Animals; Blood Pressure; Rats; Reproducibility of Results; Spinal Cord Injuries",
      "keywords": "Autonomic; Data science; Hemodynamics; Metascience; Motor recovery; Neurotrauma; Reproducibility; Spinal contusion",
      "pub_types": "Journal Article; Multicenter Study; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.; Research Support, N.I.H., Extramural",
      "pmcid": "PMC9015816",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35185011",
      "title": "Use of a community advisory board to build equitable algorithms for participation in clinical trials: a protocol paper for HoPeNET.",
      "abstract": "INTRODUCTION: Participation from racial and ethnic minorities in clinical trials has been burdened by issues surrounding mistrust and access to healthcare. There is emerging use of machine learning (ML) in clinical trial recruitment and evaluation. However, for individuals from groups who are recipients of societal biases, utilisation of ML can lead to the creation and use of biased algorithms. To minimise bias, the design of equitable ML tools that advance health equity could be guided by community engagement processes. The Howard University Partnership with the National Institutes of Health for Equitable Clinical Trial Participation for Racial/Ethnic Communities Underrepresented in Research (HoPeNET) seeks to create an ML-based infrastructure from community advisory board (CAB) experiences to enhance participation of African-Americans/Blacks in clinical trials. METHODS AND ANALYSIS: This triphased cross-sectional study (24 months, n=56) will create a CAB of community members and research investigators. The three phases of the study include: (1) identification of perceived barriers/facilitators to clinical trial engagement through qualitative/quantitative methods and systems-based model building participation; (2) operation of CAB meetings and (3) development of a predictive ML tool and outcome evaluation. Identified predictors from the participant-derived systems-based map will be used for the ML tool development. ETHICS AND DISSEMINATION: We anticipate minimum risk for participants. Institutional review board approval and informed consent has been obtained and patient confidentiality ensured.",
      "authors": "Farmer Nicole; Osei Baah Foster; Williams Faustine; Ortiz-Chapparo Erika; Mitchell Valerie M; Jackson Latifa; Collins Billy; Graham Lennox; Wallen Gwenyth R; Powell-Wiley Tiffany M; Johnson Allan",
      "year": "2022",
      "journal": "BMJ health & care informatics",
      "doi": "10.1136/bmjhci-2021-100453",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35185011/",
      "mesh_terms": "Algorithms; Clinical Trials as Topic; Cross-Sectional Studies; Humans; Patient Selection",
      "keywords": "BMJ health informatics; artificial intelligence; health equity",
      "pub_types": "Journal Article",
      "pmcid": "PMC8860013",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39475765",
      "title": "Perceptions Toward Using Artificial Intelligence and Technology for Asthma Attack Risk Prediction: Qualitative Exploration of M\u0101ori Views.",
      "abstract": "BACKGROUND: Asthma is a significant global health issue, impacting over 500,000 individuals in New Zealand and disproportionately affecting M\u0101ori communities in New Zealand, who experience worse asthma symptoms and attacks. Digital technologies, including artificial intelligence (AI) and machine learning (ML) models, are increasingly popular for asthma risk prediction. However, these AI models may underrepresent minority ethnic groups and introduce bias, potentially exacerbating disparities. OBJECTIVE: This study aimed to explore the views and perceptions that M\u0101ori have toward using AI and ML technologies for asthma self-management, identify key considerations for developing asthma attack risk prediction models, and ensure M\u0101ori are represented in ML models without worsening existing health inequities. METHODS: Semistructured interviews were conducted with 20 M\u0101ori participants with asthma, 3 male and 17 female, aged 18-76 years. All the interviews were conducted one-on-one, except for 1 interview, which was conducted with 2 participants. Altogether, 10 web-based interviews were conducted, while the rest were kanohi ki te kanohi (face-to-face). A thematic analysis was conducted to identify the themes. Further, sentiment analysis was carried out to identify the sentiments using a pretrained Bidirectional Encoder Representations from Transformers model. RESULTS: We identified four key themes: (1) concerns about AI use, (2) interest in using technology to support asthma, (3) desired characteristics of AI-based systems, and (4) experience with asthma management and opportunities for technology to improve care. AI was relatively unfamiliar to many participants, and some of them expressed concerns about whether AI technology could be trusted, kanohi ki te kanohi interaction, and inadequate knowledge of AI and technology. These concerns are exacerbated by the M\u0101ori experience of colonization. Most of the participants were interested in using technology to support their asthma management, and we gained insights into user preferences regarding computer-based health care applications. Participants discussed their experiences, highlighting problems with health care quality and limited access to resources. They also mentioned the factors that trigger their asthma control level. CONCLUSIONS: The exploration revealed that there is a need for greater information about AI and technology for M\u0101ori communities and a need to address trust issues relating to the use of technology. Expectations in relation to computer-based applications for health purposes were expressed. The research outcomes will inform future investigations on AI and technology to enhance the health of people with asthma, in particular those designed for Indigenous populations in New Zealand.",
      "authors": "Jayamini Widana Kankanamge Darsha; Mirza Farhaan; Bidois-Putt Marie-Claire; Naeem M Asif; Chan Amy Hai Yan",
      "year": "2024",
      "journal": "JMIR formative research",
      "doi": "10.2196/59811",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39475765/",
      "mesh_terms": "Adolescent; Adult; Aged; Female; Humans; Male; Middle Aged; Young Adult; Artificial Intelligence; Asthma; Maori People; New Zealand; Qualitative Research; Risk Assessment",
      "keywords": "artificial intelligence; asthma risk prediction; health system development; machine learning; mobile phone; m\u0101ori perceptions",
      "pub_types": "Journal Article",
      "pmcid": "PMC11561449",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38638465",
      "title": "Optimal site selection strategies for urban parks green spaces under the joint perspective of spatial equity and social equity.",
      "abstract": "Urban park green spaces (UPGS) are a crucial element of social public resources closely related to the health and well-being of urban residents, and issues of equity have always been a focal point of concern. This study takes the downtown area of Nanchang as an example and uses more accurate point of interest (POI) and area of interest (AOI) data as analysis sources. The improved Gaussian two-step floating catchment area (G2SFCA) and spatial autocorrelation models are then used to assess the spatial and social equity in the study area, and the results of the two assessments were coupled to determine the optimization objective using the community as the smallest unit. Finally, the assessment results are combined with the k-means algorithm and particle swarm algorithm (PSO) to propose practical optimization strategies with the objectives of minimum walking distance and maximum fairness. The results indicate (1) There are significant differences in UPGS accessibility among residents with different walking distances, with the more densely populated Old Town and Honggu Tan areas having lower average accessibility and being the main areas of hidden blindness, while the fringe areas in the northern and south-western parts of the city are the main areas of visible blindness. (2) Overall, the UPGS accessibility in Nanchang City exhibits a spatial pattern of decreasing from the east, south, and west to the center. Nanchang City is in transition towards improving spatial and social equity while achieving basic regional equity. (3) There is a spatial positive correlation between socioeconomic level and UPGS accessibility, reflecting certain social inequity. (4) Based on the above research results, the UPGS layout optimization scheme was proposed, 29 new UPGS locations and regions were identified, and the overall accessibility was improved by 2.76. The research methodology and framework can be used as a tool to identify the underserved areas of UPGS and optimize the spatial and social equity of UPGS, which is in line with the current trend of urban development in the world and provides a scientific basis for urban infrastructure planning and spatial resource allocation.",
      "authors": "Zhao Youqiang; Gong Peng",
      "year": "2024",
      "journal": "Frontiers in public health",
      "doi": "10.3389/fpubh.2024.1310340",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38638465/",
      "mesh_terms": "Humans; Parks, Recreational; Cities; Spatial Analysis; Social Class; Blindness",
      "keywords": "Gaussian two-step floating catchment area; accessibility; park quality; social equity; spatial equity; urban park green spaces",
      "pub_types": "Journal Article",
      "pmcid": "PMC11024374",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "34083673",
      "title": "Automatic and unbiased segmentation and quantification of myofibers in skeletal muscle.",
      "abstract": "Skeletal muscle has the remarkable ability to regenerate. However, with age and disease muscle strength and function decline. Myofiber size, which is affected by injury and disease, is a critical measurement to assess muscle health. Here, we test and apply Cellpose, a recently developed deep learning algorithm, to automatically segment myofibers within murine skeletal muscle. We first show that tissue fixation is necessary to preserve cellular structures such as primary cilia, small cellular antennae, and adipocyte lipid droplets. However, fixation generates heterogeneous myofiber labeling, which impedes intensity-based segmentation. We demonstrate that Cellpose efficiently delineates thousands of individual myofibers outlined by a variety of markers, even within fixed tissue with highly uneven myofiber staining. We created a novel ImageJ plugin (LabelsToRois) that allows processing\u00a0of multiple Cellpose segmentation images in batch. The plugin also contains a semi-automatic erosion function to correct for the area bias introduced by the different stainings, thereby\u00a0identifying myofibers as accurately as human experts. We successfully applied our segmentation pipeline to uncover myofiber regeneration differences between two different muscle injury models, cardiotoxin and glycerol. Thus, Cellpose combined with LabelsToRois allows for fast, unbiased, and reproducible myofiber quantification for a variety of staining and fixation conditions.",
      "authors": "Waisman Ariel; Norris Alessandra Marie; El\u00edas Costa Mart\u00edn; Kopinke Daniel",
      "year": "2021",
      "journal": "Scientific reports",
      "doi": "10.1038/s41598-021-91191-6",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34083673/",
      "mesh_terms": "Algorithms; Animals; Computational Biology; Histocytochemistry; Image Processing, Computer-Assisted; Mice; Microscopy; Muscle Fibers, Skeletal; Muscle, Skeletal; Software",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC8175575",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40639839",
      "title": "Bias in vital signs? Machine learning models can learn patients' race or ethnicity from the values of vital signs alone.",
      "abstract": "OBJECTIVES: To investigate whether machine learning (ML) algorithms can learn racial or ethnic information from the vital signs alone. METHODS: A retrospective cohort study of critically ill patients between 2014 and 2015 from the multicentre eICU-CRD critical care database involving 335 intensive care units in 208 US hospitals, containing 200\u2009859 admissions. We extracted 10\u2009763 critical care admissions of patients aged 18 and over, alive during the first 24 hours after admission, with recorded race or ethnicity as well as at least two measurements of heart rate, oxygen saturation, respiratory rate and blood pressure. Pairs of subgroups were matched based on age, gender, admission diagnosis and disease severity. XGBoost, Random Forest and Logistic Regression algorithms were used to predict recorded race or ethnicity based on the values of vital signs. RESULTS: Models derived from only four vital signs can predict patients' recorded race or ethnicity with an area under the curve (AUC) of 0.74 (\u00b10.030) between White and Black patients, AUC of 0.74 (\u00b10.030) between Hispanic and Black patients and AUC of 0.67 (\u00b10.072) between Hispanic and White patients, even when controlling for known factors. There were very small, but statistically significant differences between heart rate, oxygen saturation and blood pressure, but not respiration rate and invasively measured oxygen saturation. DISCUSSION: ML algorithms can extract racial or ethnicity information from vital signs alone across diverse patient populations, even when controlling for known biases such as pulse oximetry variations and comorbidities. The model correctly classified the race or ethnicity in two out of three patients, indicating that this outcome is not random. CONCLUSION: Vital signs embed racial information that can be learnt by ML algorithms, posing a significant risk to equitable clinical decision-making. Mitigating measures might be challenging, considering the fundamental role of vital signs in clinical decision-making.",
      "authors": "Velichkovska Bojana; Gjoreski Hristijan; Denkovski Daniel; Kalendar Marija; Mullan Irene Dankwa; Wawira Gichoya Judy; Martinez Nicole; Celi Leo; Osmani Venet",
      "year": "2025",
      "journal": "BMJ health & care informatics",
      "doi": "10.1136/bmjhci-2024-101098",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40639839/",
      "mesh_terms": "Humans; Machine Learning; Vital Signs; Retrospective Studies; Male; Female; Middle Aged; Ethnicity; Aged; Racial Groups; Intensive Care Units; Adult; United States; Critical Illness; Algorithms; Bias; Respiratory Rate; Heart Rate",
      "keywords": "Artificial intelligence; Decision Support Systems, Clinical; Electronic Health Records; Health Equity",
      "pub_types": "Journal Article; Multicenter Study",
      "pmcid": "PMC12258377",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40144330",
      "title": "Artificial Intelligence to Promote Racial and Ethnic Cardiovascular Health Equity.",
      "abstract": "PURPOSE OF REVIEW: The integration of artificial intelligence (AI) in medicine holds promise for transformative advancements aimed at improving healthcare outcomes. Amidst this promise, AI has been envisioned as a tool to detect and mitigate racial and ethnic inequity known to plague current cardiovascular care. However, this enthusiasm is dampened by the recognition that AI itself can harbor and propagate biases, necessitating a careful approach to ensure equity. This review highlights topics in the landscape of AI in cardiology, its role in identifying and addressing healthcare inequities, promoting diversity in research, concerns surrounding its applications, and proposed strategies for fostering equitable utilization. RECENT FINDINGS: Artificial intelligence has proven to be a valuable tool for clinicians in diagnosing and mitigating racial and ethnic inequities in cardiology, as well as the promotion of diversity in research. This promise is counterbalanced by the cautionary reality that AI can inadvertently perpetuate existent biases stemming from limited diversity in training data, inherent biases within datasets, and inadequate bias detection and monitoring mechanisms. Recognizing these concerns, experts emphasize the need for rigorous efforts to address these limitations in the development and deployment of AI within medicine. SUMMARY: Implementing AI in cardiovascular care to identify and address racial and ethnic inequities requires careful design and execution, beginning with meticulous data collection and a thorough review of training datasets. Furthermore, ensuring equitable performance involves rigorous testing and continuous surveillance of algorithms. Lastly, the promotion of diversity in the AI workforce and engagement of stakeholders are crucial to the advancement of equity to ultimately realize the potential for artificial intelligence for cardiovascular health equity.",
      "authors": "Amponsah Daniel; Thamman Ritu; Brandt Eric; James Cornelius; Spector-Bagdady Kayte; Yong Celina M",
      "year": "2024",
      "journal": "Current cardiovascular risk reports",
      "doi": "10.1007/s12170-024-00745-6",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40144330/",
      "mesh_terms": "",
      "keywords": "Artificial Intelligence; Cardiovascular Equity; Disparities; Diversity; Machine Learning; Racial and Ethnic Inequity",
      "pub_types": "Journal Article",
      "pmcid": "PMC11938301",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "32298232",
      "title": "The Association Between State-Level Racial Attitudes Assessed From Twitter Data and Adverse Birth Outcomes: Observational Study.",
      "abstract": "BACKGROUND: In the United States, racial disparities in birth outcomes persist and have been widening. Interpersonal and structural racism are leading explanations for the continuing racial disparities in birth outcomes, but research to confirm the role of racism and evaluate trends in the impact of racism on health outcomes has been hampered by the challenge of measuring racism. Most research on discrimination relies on self-reported experiences of discrimination, and few studies have examined racial attitudes and bias at the US national level. OBJECTIVE: This study aimed to investigate the associations between state-level Twitter-derived sentiments related to racial or ethnic minorities and birth outcomes. METHODS: We utilized Twitter's Streaming application programming interface to collect 26,027,740 tweets from June 2015 to December 2017, containing at least one race-related term. Sentiment analysis was performed using support vector machine, a supervised machine learning model. We constructed overall indicators of sentiment toward minorities and sentiment toward race-specific groups. For each year, state-level Twitter-derived sentiment data were merged with birth data for that year. The study participants were women who had singleton births with no congenital abnormalities from 2015 to 2017 and for whom data were available on gestational age (n=9,988,030) or birth weight (n=9,985,402). The main outcomes were low birth weight (birth weight \u22642499 g) and preterm birth (gestational age <37 weeks). We estimated the incidence ratios controlling for individual-level maternal characteristics (sociodemographics, prenatal care, and health behaviors) and state-level demographics, using log binomial regression models. RESULTS: The accuracy for identifying negative sentiments on comparing the machine learning model to manually labeled tweets was 91%. Mothers living in states in the highest tertile for negative sentiment tweets referencing racial or ethnic minorities had greater incidences of low birth weight (8% greater, 95% CI 4%-13%) and preterm birth (8% greater, 95% CI 0%-14%) compared with mothers living in states in the lowest tertile. More negative tweets referencing minorities were associated with adverse birth outcomes in the total population, including non-Hispanic white people and racial or ethnic minorities. In stratified subgroup analyses, more negative tweets referencing specific racial or ethnic minority groups (black people, Middle Eastern people, and Muslims) were associated with poor birth outcomes for black people and minorities. CONCLUSIONS: A negative social context related to race was associated with poor birth outcomes for racial or ethnic minorities, as well as non-Hispanic white people.",
      "authors": "Nguyen Thu T; Adams Nikki; Huang Dina; Glymour M Maria; Allen Amani M; Nguyen Quynh C",
      "year": "2020",
      "journal": "JMIR public health and surveillance",
      "doi": "10.2196/17103",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32298232/",
      "mesh_terms": "Adult; Female; Geographic Mapping; Humans; Male; Pregnancy; Pregnancy Outcome; Racial Groups; Racism; Social Media; United States",
      "keywords": "birth outcomes; racial bias; racial or ethnic minorities; social media",
      "pub_types": "Journal Article; Observational Study; Research Support, N.I.H., Extramural",
      "pmcid": "PMC7381033",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40773445",
      "title": "Reducing bias in coronary heart disease prediction using Smote-ENN and PCA.",
      "abstract": "Coronary heart disease (CHD) is a major cardiovascular disorder that poses significant threats to global health and is increasingly affecting younger populations. Its treatment and prevention face challenges such as high costs, prolonged recovery periods, and limited efficacy of traditional methods. Additionally, the complexity of diagnostic indicators and the global shortage of medical professionals further complicate accurate diagnosis. This study employs machine learning techniques to analyze CHD-related pathogenic factors and proposes an efficient diagnostic and predictive framework. To address the data imbalance issue, SMOTE-ENN is utilized, and five machine learning algorithms-Decision Trees, KNN, SVM, XGBoost, and Random Forest-are applied for classification tasks. Principal Component Analysis (PCA) and Grid Search are used to optimize the models, with evaluation metrics including accuracy, precision, recall, F1-score, and AUC. According to the random forest model's optimization experiment, the initial unbalanced data's accuracy was 85.26%, and the F1-score was 12.58%. The accuracy increased to 92.16% and the F1-score reached 93.85% after using SMOTE-ENN for data balancing, which is an increase of 6.90% and 81.27%, respectively; the model accuracy increased to 97.91% and the F1-score increased to 97.88% after adding PCA feature dimensionality reduction processing, which is an increase of 5.75% and 4.03%, respectively, compared with the SMOTE-ENN stage. This indicates that combining data balancing and feature dimensionality reduction techniques significantly improves model accuracy and makes the random forest model the best model. This study provides an efficient diagnostic tool for CHD, alleviates the challenges posed by limited medical resources, and offers a scientific foundation for precise prevention and intervention strategies.",
      "authors": "Wei Xinyi; Shi Boyu",
      "year": "2025",
      "journal": "PloS one",
      "doi": "10.1371/journal.pone.0327569",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40773445/",
      "mesh_terms": "Humans; Coronary Disease; Principal Component Analysis; Machine Learning; Algorithms; Decision Trees; Male; Support Vector Machine; Bias",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC12331108",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40063843",
      "title": "Gamified Adaptive Approach Bias Modification in Individuals With Methamphetamine Use History From Communities in Sichuan: Pilot Randomized Controlled Trial.",
      "abstract": "BACKGROUND: Cognitive bias modification (CBM) programs have shown promise in treating psychiatric conditions, but they can be perceived as boring and repetitive. Incorporating gamified designs and adaptive algorithms in CBM training may address this issue and enhance engagement and effectiveness. OBJECTIVES: This study aims to gather preliminary data and assess the preliminary efficacy of an adaptive approach bias modification (A-ApBM) paradigm in reducing cue-induced craving in individuals with methamphetamine use history. METHODS: A randomized controlled trial with 3 arms was conducted. Individuals aged 18-60 years with methamphetamine dependence and at least 1 year of methamphetamine use were recruited from 12 community-based rehabilitation centers in Sichuan, China. Individuals with the inability to fluently operate a smartphone and the presence of mental health conditions other than methamphetamine use disorder were excluded. The A-ApBM group engaged in ApBM training using a smartphone app for 4 weeks. The A-ApBM used an adaptive algorithm to dynamically adjust the difficulty level based on individual performance. Cue-induced craving scores and relapses were assessed using a visual analogue scale at baseline, postintervention, and at week-16 follow-up. RESULTS: A total of 136 participants were recruited and randomized: 48 were randomized to the A-ApBM group, 48 were randomized to the static approach bias modification (S-ApBM) group, and 40 were randomized to the no-intervention control group. The A-ApBM group showed a significant reduction in cue-induced craving scores at postintervention compared with baseline (Cohen d=0.34; P<.01; 95% CI 0.03-0.54). The reduction remained significant at the week-16 follow-up (Cohen d=0.40; P=.01; 95% CI 0.18-0.57). No significant changes were observed in the S-ApBM and control groups. CONCLUSIONS: The A-ApBM paradigm with gamified designs and dynamic difficulty adjustments may be an effective intervention for reducing cue-induced craving in individuals with methamphetamine use history. This approach improves engagement and personalization, potentially enhancing the effectiveness of CBM programs. Further research is needed to validate these findings and explore the application of A-ApBM in other psychiatric conditions.",
      "authors": "Shen Danlin; Jiao Jianping; Zhang Liqun; Liu Yanru; Liu Xiang; Li Yuanhui; Zhang Tianjiao; Li Dai; Hao Wei",
      "year": "2025",
      "journal": "JMIR serious games",
      "doi": "10.2196/56978",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40063843/",
      "mesh_terms": "",
      "keywords": "cognitive bias modification; digital therapeutics; effectiveness; engagement; game; gamified design; methamphetamine; pilot RCT; psychiatric; randomized controlled trial; smartphone app; substance use disorder",
      "pub_types": "Journal Article",
      "pmcid": "PMC11931399",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40587474",
      "title": "Racial disparities in continuous glucose monitoring-based 60-min glucose predictions among people with type 1 diabetes.",
      "abstract": "Non-Hispanic white (White) populations are overrepresented in medical studies. Potential healthcare disparities can happen when machine learning models, used in diabetes technologies, are trained on data from primarily White patients. We aimed to evaluate algorithmic fairness in glucose predictions. This study utilized continuous glucose monitoring (CGM) data from 101 White and 104 Black participants with type 1 diabetes collected by the JAEB Center for Health Research, US. Long short-term memory (LSTM) deep learning models were trained on 11 datasets of different proportions of White and Black participants and tailored to each individual using transfer learning to predict glucose 60 minutes ahead based on 60-minute windows. Root mean squared errors (RMSE) were calculated for each participant. Linear mixed-effect models were used to investigate the association between racial composition and RMSE while accounting for age, sex, and training data size. A median of 9 weeks (IQR: 7, 10) of CGM data was available per participant. The divergence in performance (RMSE slope by proportion) was not statistically significant for either group. However, the slope difference (from 0% White and 100% Black to 100% White and 0% Black) between groups was statistically significant (p\u2009=\u20090.02), meaning the RMSE increased 0.04 [0.01, 0.08] mmol/L more for Black participants compared to White participants when the proportion of White participants increased from 0 to 100% in the training data. This difference was attenuated in the transfer learned models (RMSE: 0.02 [-0.01, 0.05] mmol/L, p\u2009=\u20090.20). The racial composition of training data created a small statistically significant difference in the performance of the models, which was not present after using transfer learning. This demonstrates the importance of diversity in datasets and the potential value of transfer learning for developing more fair prediction models.",
      "authors": "Thomsen Helene Bei; Li Livie Yumeng; Isaksen Anders Aasted; Lebiecka-Johansen Benjamin; Bour Charline; Fagherazzi Guy; van Doorn William P T M; Varga Tibor V; Hulman Adam",
      "year": "2025",
      "journal": "PLOS digital health",
      "doi": "10.1371/journal.pdig.0000918",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40587474/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC12208448",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41195704",
      "title": "Ensuring generalizability and clinical utility in mental health care applications: Robust artificial intelligence-based treatment predictions in diverse psychosis populations.",
      "abstract": "AIM: Artificial Intelligence (AI)-based prediction models of treatment response promise to revolutionize psychiatric care by enabling personalized treatment, but very few have been thoroughly tested in different samples or compared to current clinical standards. Here we present models predicting antipsychotic response and assess their clinical utility in a robust methodological framework. METHODS: Machine learning models were trained and cross-validated on clinical and sociodemographic data from 594 individuals with established schizophrenia (NCT00014001) and 323 individuals with first episode psychosis (NCT03510325). Models predicted four measures of antipsychotic response at 3\u2009months after baseline. Clinical utility was assessed using decision curve and calibration curve analyses. Model performance was tested in a reduced feature space and across sex, ethnicity, antipsychotic, and symptom change subgroups to investigate model fairness. RESULTS: Models predicting total symptom severity (r\u2009=\u20090.4-0.68) and symptomatic remission (BAC\u2009=\u200962.4%-69%) performed well in both samples and externally validated successfully in the opposing cohort (r\u2009=\u20090.4-0.5, BAC\u2009= 63.5%-65.7%). Performance remained significant when the models were reduced to 8-9 key variables (r\u2009=\u20090.53 for total symptom severity, BAC\u2009=\u200965.3% for symptomatic remission). Models predicting symptomatic remission had a net benefit across risk thresholds of 0.5-0.9 and were moderately well-calibrated (ECE\u2009=\u20090.16-0.18). Model performance different across sex, ethnicity and medication subgroups. CONCLUSIONS: We present a robust framework for training and assessing the clinical utility of prediction models in psychiatry. Our models generalize across different psychosis populations and show promising calibration and net benefit. However, performance disparities across demographic and treatment subgroups highlight the need for more diverse clinical samples to ensure equitable prediction.",
      "authors": "Coutts Fiona; Mena Sergio; Ucur Esin; Fleischhacker W Wolfgang; Kahn Rene; Lieberman Jeffrey; Hasan Alkomiet; Howes Oliver; Correll Christoph; Koutsouleris Nikolaos; Lalousis Paris Alexandros",
      "year": "2026",
      "journal": "Psychiatry and clinical neurosciences",
      "doi": "10.1111/pcn.13914",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41195704/",
      "mesh_terms": "Adolescent; Adult; Female; Humans; Male; Middle Aged; Young Adult; Antipsychotic Agents; Artificial Intelligence; Machine Learning; Outcome Assessment, Health Care; Psychotic Disorders; Schizophrenia; Randomized Controlled Trials as Topic; Multicenter Studies as Topic; Clinical Trials, Phase III as Topic; Clinical Trials, Phase IV as Topic",
      "keywords": "AI; antipsychotics; psychosis; translational; treatment response",
      "pub_types": "Journal Article",
      "pmcid": "PMC12757767",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35974092",
      "title": "A machine learning framework supporting prospective clinical decisions applied to risk prediction in oncology.",
      "abstract": "We present a general framework for developing a machine learning (ML) tool that supports clinician assessment of patient risk using electronic health record-derived real-world data and apply the framework to a quality improvement use case in an oncology setting to identify patients at risk for a near-term (60 day) emergency department (ED) visit who could potentially be eligible for a home-based acute care program. Framework steps include defining clinical quality improvement goals, model development and validation, bias assessment, retrospective and prospective validation, and deployment in clinical workflow. In the retrospective analysis for the use case, 8% of patient encounters were associated with a high risk (pre-defined as predicted probability \u226520%) for a near-term ED visit by the patient. Positive predictive value (PPV) and negative predictive value (NPV) for future ED events was 26% and 91%, respectively. Odds ratio (OR) of ED visit (high- vs. low-risk) was 3.5 (95% CI: 3.4-3.5). The model appeared to be calibrated across racial, gender, and ethnic groups. In the prospective analysis, 10% of patients were classified as high risk, 76% of whom were confirmed by clinicians as eligible for home-based acute care. PPV and NPV for future ED events was 22% and 95%, respectively. OR of ED visit (high- vs. low-risk) was 5.4 (95% CI: 2.6-11.0). The proposed framework for an ML-based tool that supports clinician assessment of patient risk is a stepwise development approach; we successfully applied the framework to an ED visit risk prediction use case.",
      "authors": "Coombs Lorinda; Orlando Abigail; Wang Xiaoliang; Shaw Pooja; Rich Alexander S; Lakhtakia Shreyas; Titchener Karen; Adamson Blythe; Miksad Rebecca A; Mooney Kathi",
      "year": "2022",
      "journal": "NPJ digital medicine",
      "doi": "10.1038/s41746-022-00660-3",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35974092/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC9380664",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39316436",
      "title": "Equity in Digital Mental Health Interventions in the United States: Where to Next?",
      "abstract": "Health care technologies have the ability to bridge or hinder equitable care. Advocates of digital mental health interventions (DMHIs) report that such technologies are poised to reduce the documented gross health care inequities that have plagued generations of people seeking care in the United States. This is due to a multitude of factors such as their potential to revolutionize access; mitigate logistical barriers to in-person mental health care; and leverage patient inputs to formulate tailored, responsive, and personalized experiences. Although we agree with the potential of DMHIs to advance health equity, we articulate several steps essential to mobilize and sustain meaningful forward progression in this endeavor, reflecting on decades of research and learnings drawn from multiple fields of expertise and real-world experience. First, DMHI manufacturers must build diversity, equity, inclusion, and belonging (DEIB) processes into the full spectrum of product evolution itself (eg, product design, evidence generation) as well as into the fabric of internal company practices (eg, talent recruitment, communication principles, and advisory boards). Second, awareness of the DEIB efforts-or lack thereof-in DMHI research trials is needed to refine and optimize future study design for inclusivity as well as proactively address potential barriers to doing so. Trials should incorporate thoughtful, inclusive, and creative approaches to recruitment, enrollment, and measurement of social determinants of health and self-identity, as well as a prioritization of planned and exploratory analyses examining outcomes across various groups of people. Third, mental health care advocacy, research funding policies, and local and federal legislation can advance these pursuits, with directives from the US Preventive Services Taskforce, National Institutes of Health, and Food and Drug Administration applied as poignant examples. For products with artificial intelligence/machine learning, maintaining a \"human in the loop\" as well as prespecified and adaptive analytic frameworks to monitor and remediate potential algorithmic bias can reduce the risk of increasing inequity. Last, but certainly not least, is a call for partnership and transparency within and across ecosystems (academic, industry, payer, provider, regulatory agencies, and value-based care organizations) to reliably build health equity into real-world DMHI product deployments and evidence-generation strategies. All these considerations should also extend into the context of an equity-informed commercial strategy for DMHI manufacturers and health care organizations alike. The potential to advance health equity in innovation with DMHI is apparent. We advocate the field's thoughtful and evergreen advancement in inclusivity, thereby redefining the mental health care experience for this generation and those to come.",
      "authors": "Robinson Athena; Flom Megan; Forman-Hoffman Valerie L; Histon Trina; Levy Monique; Darcy Alison; Ajayi Toluwalase; Mohr David C; Wicks Paul; Greene Carolyn; Montgomery Robert M",
      "year": "2024",
      "journal": "Journal of medical Internet research",
      "doi": "10.2196/59939",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39316436/",
      "mesh_terms": "Humans; United States; Mental Health Services; Mental Health; Health Equity; Telemedicine; Healthcare Disparities",
      "keywords": "Digital Mental Health Interventions; access to health care; health equity; health plan implementations; mental health",
      "pub_types": "Journal Article",
      "pmcid": "PMC11462105",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40857554",
      "title": "Comparing Multiple Imputation Methods to Address Missing Patient Demographics in Immunization Information Systems: Retrospective Cohort Study.",
      "abstract": "BACKGROUND: Immunization Information Systems (IIS) and surveillance data are essential for public health interventions and programming; however, missing data are often a challenge, potentially introducing bias and impacting the accuracy of vaccine coverage assessments, particularly in addressing disparities. OBJECTIVE: This study aimed to evaluate the performance of 3 multiple imputation methods, Stata's (StataCorp LLC) multiple imputation using chained equations (MICE), scikit-learn's Iterative-Imputer, and Python's miceforest package, in managing missing race and ethnicity data in large-scale surveillance datasets. We compared these methodologies in their ability to preserve demographic distribution, computational efficiency, and performed G-tests on contingency tables to obtain likelihood ratio statistics to assess the association between race and ethnicity and flu vaccination status. METHODS: In this retrospective cohort study, we analyzed 2021-2022 flu vaccination and demographic data from the West Virginia Immunization Information System (N=2,302,036), where race (15%) and ethnicity (34%) were missing. MICE, Iterative Imputer, and miceforest were used to impute missing variables, generating 15 datasets each. Computational efficiency, demographic distribution preservation, and spatial clustering patterns were assessed using G-statistics. RESULTS: After imputation, an additional 780,339 observations were obtained compared with complete case analysis. All imputation methods exhibited significant spatial clustering for race imputation (G-statistics: MICE=26,452.7, Iterative-Imputer=128,280.3, Miceforest=26,891.5; P<.001), while ethnicity imputation showed variable clustering patterns (G-statistics: MICE=1142.2, Iterative-Imputer=1.7, Miceforest=2185.0; P: MICE<.001, Iterative-Imputer=1.7, Miceforest<.001). MICE and miceforest best preserved the proportional distribution of demographics. Computational efficiency varied, with MICE requiring 14 hours, Iterative Imputer 2 minutes, and miceforest 10 minutes for 15 imputations. Postimputation estimates indicated a 0.87%-18% reduction in stratified flu vaccination coverage rates. Overall estimated flu vaccination rates decreased from 26% to 19% after imputations. CONCLUSIONS: Both MICE and Miceforest offer flexible and reliable approaches for imputing missing demographic data while mitigating bias compared with Iterative-Imputer. Our results also highlight that the imputation method can profoundly affect research findings. Though MICE and Miceforest had better effect sizes and reliability, MICE was much more computationally and time-expensive, limiting its use in large, surveillance datasets. Miceforest can use cloud-based computing, which further enhances efficiency by offloading resource-intensive tasks, enabling parallel execution, and minimizing processing delays. The significant decrease in vaccination coverage estimates validates how incomplete or missing data can eclipse real disparities. Our findings support regular application of imputation methods in immunization surveillance to improve health equity evaluations and shape targeted public health interventions and programming.",
      "authors": "Brown Sara; Kudia Ousswa; Kleine Kaye; Kidd Bryndan; Wines Robert; Meckes Nathanael",
      "year": "2025",
      "journal": "JMIR public health and surveillance",
      "doi": "10.2196/73916",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40857554/",
      "mesh_terms": "Retrospective Studies; Humans; Female; Male; Information Systems; Demography; Child, Preschool; Cohort Studies; Adolescent; Child; Immunization; Adult; Infant; Middle Aged",
      "keywords": "data science; immunization information system; imputation methods; machine learning; missing data; multiple imputation; statistical modeling",
      "pub_types": "Journal Article",
      "pmcid": "PMC12380239",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "38099504",
      "title": "Using artificial intelligence to promote equitable care for inpatients with language barriers and complex medical needs: clinical stakeholder perspectives.",
      "abstract": "OBJECTIVES: Inpatients with language barriers and complex medical needs suffer disparities in quality of care, safety, and health outcomes. Although in-person interpreters are particularly beneficial for these patients, they are underused. We plan to use machine learning predictive analytics to reliably identify patients with language barriers and complex medical needs to prioritize them for in-person interpreters. MATERIALS AND METHODS: This qualitative study used stakeholder engagement through semi-structured interviews to understand the perceived risks and benefits of artificial intelligence (AI) in this domain. Stakeholders included clinicians, interpreters, and personnel involved in caring for these patients or for organizing interpreters. Data were coded and analyzed using NVIVO software. RESULTS: We completed 49 interviews. Key perceived risks included concerns about transparency, accuracy, redundancy, privacy, perceived stigmatization among patients, alert fatigue, and supply-demand issues. Key perceived benefits included increased awareness of in-person interpreters, improved standard of care and prioritization for interpreter utilization; a streamlined process for accessing interpreters, empowered clinicians, and potential to overcome clinician bias. DISCUSSION: This is the first study that elicits stakeholder perspectives on the use of AI with the goal of improved clinical care for patients with language barriers. Perceived benefits and risks related to the use of AI in this domain, overlapped with known hazards and values of AI but some benefits were unique for addressing challenges with providing interpreter services to patients with language barriers. CONCLUSION: Artificial intelligence to identify and prioritize patients for interpreter services has the potential to improve standard of care and address healthcare disparities among patients with language barriers.",
      "authors": "Barwise Amelia K; Curtis Susan; Diedrich Daniel A; Pickering Brian W",
      "year": "2024",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "doi": "10.1093/jamia/ocad224",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38099504/",
      "mesh_terms": "Humans; Language; Inpatients; Artificial Intelligence; Communication Barriers; Allied Health Personnel",
      "keywords": "LEP; artificial intelligence; complex care; health equity; language barrier; non-English language preferred",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, P.H.S.",
      "pmcid": "PMC10873784",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "31840093",
      "title": "Eliminating biasing signals in lung cancer images for prognosis predictions with deep learning.",
      "abstract": "Deep learning has shown remarkable results for image analysis and is expected to aid individual treatment decisions in health care. Treatment recommendations are predictions with an inherently causal interpretation. To use deep learning for these applications in the setting of observational data, deep learning methods must be made compatible with the required causal assumptions. We present a scenario with real-world medical images (CT-scans of lung cancer) and simulated outcome data. Through the data simulation scheme, the images contain two distinct factors of variation that are associated with survival, but represent a collider (tumor size) and a prognostic factor (tumor heterogeneity), respectively. When a deep network would use all the information available in the image to predict survival, it would condition on the collider and thereby introduce bias in the estimation of the treatment effect. We show that when this collider can be quantified, unbiased individual prognosis predictions are attainable with deep learning. This is achieved by (1) setting a dual task for the network to predict both the outcome and the collider and (2) enforcing a form of linear independence of the activation distributions of the last layer. Our method provides an example of combining deep learning and structural causal models to achieve unbiased individual prognosis predictions. Extensions of machine learning methods for applications to causal questions are required to attain the long-standing goal of personalized medicine supported by artificial intelligence.",
      "authors": "van Amsterdam W A C; Verhoeff J J C; de Jong P A; Leiner T; Eijkemans M J C",
      "year": "2019",
      "journal": "NPJ digital medicine",
      "doi": "10.1038/s41746-019-0194-x",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31840093/",
      "mesh_terms": "",
      "keywords": "Computed tomography; Computer science; Epidemiology; Prognosis",
      "pub_types": "Journal Article",
      "pmcid": "PMC6904461",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40824638",
      "title": "Performance of 4 Methods to Assess Health-Related Social Needs.",
      "abstract": "IMPORTANCE: Organizations use health-related social needs (HRSN) information to identify patients in need of referrals, to increase clinician awareness, to improve analytics, and for quality reporting. OBJECTIVE: To contrast the performance of screening questionnaires, natural language processing (NLP) of clinical notes, rule-based computable phenotypes, and machine learning (ML) classification models in measuring HRSNs. DESIGN, SETTING, AND PARTICIPANTS: This cross-sectional study assessed 4 measurement approaches for 5 HRSNs in parallel. Each approach was treated as a screening test. Data included notes from adult patients treated at primary care clinics in 2 health systems in Indianapolis, Indiana, from January 2022 to June 2023. Data were analyzed from December 2024 to February 2025. EXPOSURES: Reference standard instruments measured food insecurity, housing instability, financial strain, transportation barriers, and history of legal problems. Participants completed the HRSN screening questions in the electronic health record (EHR). NLP algorithms, gradient-boosted decision tree ML classifiers, and refined versions of human-defined rule-based computable phenotypes were applied to participants' past 12 months EHR data. MAIN OUTCOMES AND MEASURES: Sensitivity, specificity, area under the curve (AUC), and positive predictive values (PPV) described performance of each approach against the reference standard measures. False-negative rates were used to explore fairness. RESULTS: Data from a total of 1252 adult patients (407 [32.51%] aged 30 to 49 years; 821 [65.58%] female) were assessed, including 94 (7.51%) who identified as Hispanic, 602 (48.08%) as non-Hispanic Black or African American, and 442 (35.30%) as non-Hispanic White. The screening questions method had the strongest overall performance for food insecurity (AUC, 0.94; 95% CI, 0.93-0.95), housing instability (AUC, 0.78; 95% CI, 0.75-0.80), transportation barriers (AUC, 0.77; 95% CI, 0.74-0.79), and legal problems (AUC, 0.81; 95% CI, 0.77-0.85). The screening questions had poor performance for financial strain (AUC, 0.62; 95% CI, 0.60-0.65). The PPV for screening tools ranged from 0.77 to 0.92, indicating utility for individual-level decision-making. NLP and rule-based computable phenotypes had poor performance. ML classification resulted in higher sensitivities than the other methods. False-negative rates indicated differential, unfair performance for all measurement approaches by gender, race and ethnicity, and age groups. CONCLUSIONS AND RELEVANCE: In this cross-sectional study of HRSN measurement, no approach performed strongly for every HRSN, and every approach had indication of unfair performance. These findings suggest that practitioners, health care and public health organizations, researchers, and policymakers who rely on a single method to collect HRSN data will likely underestimate patients' true social burden.",
      "authors": "Vest Joshua R; Wu Wei; Gregory Megan E; Kasturi Suranga N; Mendonca Eneida A; Bian Jiang; Magoc Tanja; Grannis Shaun; McNamee Cassidy; Harle Christopher A",
      "year": "2025",
      "journal": "JAMA network open",
      "doi": "10.1001/jamanetworkopen.2025.27426",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40824638/",
      "mesh_terms": "Humans; Cross-Sectional Studies; Female; Male; Middle Aged; Adult; Needs Assessment; Electronic Health Records; Machine Learning; Indiana; Surveys and Questionnaires; Natural Language Processing; Sensitivity and Specificity; Aged",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC12362220",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40969781",
      "title": "Using a Large Language Model (ChatGPT-4o) to Assess the Risk of Bias in Randomized Controlled Trials of Medical Interventions: Interrater Agreement With Human Reviewers.",
      "abstract": "BACKGROUND: Risk of bias (RoB) assessment is a highly skilled task that is time-consuming and subject to human error. RoB automation tools have previously used machine learning models built using relatively small task-specific training sets. Large language models (LLMs; e.g., ChatGPT) are complex models built using non-task-specific Internet-scale training sets. They demonstrate human-like abilities and might be able to support tasks like RoB assessment. METHODS: Following a published peer-reviewed protocol, we randomly sampled 100 Cochrane reviews. New or updated reviews that evaluated medical interventions, included \u2265\u20091 eligible trial, and presented human consensus assessments using Cochrane RoB1 or RoB2 were eligible. We excluded reviews performed under emergency conditions (e.g., COVID-19), and those on public health or welfare. We randomly sampled one trial from each review. Trials using individual- or cluster-randomized designs were eligible. We extracted human consensus RoB assessments of the trials from the reviews, and methods texts from the trials. We used 25 review-trial pairs to develop a ChatGPT prompt to assess RoB using trial methods text. We used the prompt and the remaining 75 review-trial pairs to estimate human-ChatGPT agreement for \"Overall RoB\" (primary outcome) and \"RoB due to the randomization process\", and ChatGPT-ChatGPT (intrarater) agreement for \"Overall RoB\". We used ChatGPT-4o (February 2025) throughout. RESULTS: The 75 reviews were sampled from 35 Cochrane review groups, and all used RoB1. The 75 trials spanned five decades, and all but one were published in English. Human-ChatGPT agreement for \"Overall RoB\" assessment was 50.7% (95% CI 39.3%-62.0%), substantially higher than expected by chance (p\u2009=\u20090.0015). Human-ChatGPT agreement for \"RoB due to the randomization process\" was 78.7% (95% CI 69.4%-88.0%; p\u2009<\u20090.001). ChatGPT-ChatGPT agreement was 74.7% (95% CI 64.8%-84.6%; p\u2009<\u20090.001). CONCLUSIONS: ChatGPT appears to have some ability to assess RoB and is unlikely to be guessing or \"hallucinating\". The estimated agreement for \"Overall RoB\" is well above estimates of agreement reported for some human reviewers, but below the highest estimates. LLM-based systems for assessing RoB may be able to help streamline and improve evidence synthesis production.",
      "authors": "Rose Christopher James; Bidonde Julia; Ringsten Martin; Glanville Julie; Potrebny Thomas; Cooper Chris; Muller Ashley Elizabeth; Bergsund Hans Bugge; Meneses-Echavez Jose F; Berg Rigmor C",
      "year": "2025",
      "journal": "Cochrane evidence synthesis and methods",
      "doi": "10.1002/cesm.70048",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40969781/",
      "mesh_terms": "",
      "keywords": "ChatGPT; LLM; RoB; artificial intelligence; evidence synthesis; large language model; risk of bias",
      "pub_types": "Journal Article",
      "pmcid": "PMC12442625",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "34895784",
      "title": "Willingness to vaccinate against SARS-CoV-2: The role of reasoning biases and conspiracist ideation.",
      "abstract": "UNLABELLED: BACKGR1OUND: Widespread vaccine hesitancy and refusal complicate containment of the SARS-CoV-2 pandemic. Extant research indicates that biased reasoning and conspiracist ideation discourage vaccination. However, causal pathways from these constructs to vaccine hesitancy and refusal remain underspecified, impeding efforts to intervene and increase vaccine uptake. METHOD: 554 participants who denied prior SARS-CoV-2 vaccination completed self-report measures of SARS-CoV-2 vaccine intentions, conspiracist ideation, and constructs from the Health Belief Model of medical decision-making (such as perceived vaccine dangerousness) along with tasks measuring reasoning biases (such as those concerning data gathering behavior). Cutting-edge machine learning algorithms (Greedy Fast Causal Inference) and psychometric network analysis were used to elucidate causal pathways to (and from) vaccine intentions. RESULTS: Results indicated that a bias toward reduced data gathering during reasoning may cause paranoia, increasing the perceived dangerousness of vaccines and thereby reducing willingness to vaccinate. Existing interventions that target data gathering and paranoia therefore hold promise for encouraging vaccination. Additionally, reduced willingness to vaccinate was identified as a likely cause of belief in conspiracy theories, subverting the common assumption that the opposite causal relation exists. Finally, perceived severity of SARS-CoV-2 infection and perceived vaccine dangerousness (but not effectiveness) were potential direct causes of willingness to vaccinate, providing partial support for the Health Belief Model's applicability to SARS-CoV-2 vaccine decisions. CONCLUSIONS: These insights significantly advance our understanding of the underpinnings of vaccine intentions and should scaffold efforts to prepare more effective interventions on hesitancy for deployment during future pandemics.",
      "authors": "Bronstein Michael V; Kummerfeld Erich; MacDonald Angus; Vinogradov Sophia",
      "year": "2022",
      "journal": "Vaccine",
      "doi": "10.1016/j.vaccine.2021.11.079",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34895784/",
      "mesh_terms": "Bias; COVID-19; COVID-19 Vaccines; Humans; SARS-CoV-2; Vaccination; Vaccination Hesitancy",
      "keywords": "COVID-19; Conspiracy theories; GFCI; Reasoning; SARS-CoV-2; Vaccines",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC8642163",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "30522989",
      "title": "Characterizing Tweet Volume and Content About Common Health Conditions Across Pennsylvania: Retrospective Analysis.",
      "abstract": "BACKGROUND: Tweets can provide broad, real-time perspectives about health and medical diagnoses that can inform disease surveillance in geographic regions. Less is known, however, about how much individuals post about common health conditions or what they post about. OBJECTIVE: We sought to collect and analyze tweets from 1 state about high prevalence health conditions and characterize the tweet volume and content. METHODS: We collected 408,296,620 tweets originating in Pennsylvania from 2012-2015 and compared the prevalence of 14 common diseases to the frequency of disease mentions on Twitter. We identified and corrected bias induced due to variance in disease term specificity and used the machine learning approach of differential language analysis to determine the content (words and themes) most highly correlated with each disease. RESULTS: Common disease terms were included in 226,802 tweets (174,381 tweets after disease term correction). Posts about breast cancer (39,156/174,381 messages, 22.45%; 306,127/12,702,379 prevalence, 2.41%) and diabetes (40,217/174,381 messages, 23.06%; 2,189,890/12,702,379 prevalence, 17.24%) were overrepresented on Twitter relative to disease prevalence, whereas hypertension (17,245/174,381 messages, 9.89%; 4,614,776/12,702,379 prevalence, 36.33%), chronic obstructive pulmonary disease (1648/174,381 messages, 0.95%; 1,083,627/12,702,379 prevalence, 8.53%), and heart disease (13,669/174,381 messages, 7.84%; 2,461,721/12,702,379 prevalence, 19.38%) were underrepresented. The content of messages also varied by disease. Personal experience messages accounted for 12.88% (578/4487) of prostate cancer tweets and 24.17% (4046/16,742) of asthma tweets. Awareness-themed tweets were more often about breast cancer (9139/39,156 messages, 23.34%) than asthma (1040/16,742 messages, 6.21%). Tweets about risk factors were more often about heart disease (1375/13,669 messages, 10.06%) than lymphoma (105/4927 messages, 2.13%). CONCLUSIONS: Twitter provides a window into the Web-based visibility of diseases and how the volume of Web-based content about diseases varies by condition. Further, the potential value in tweets is in the rich content they provide about individuals' perspectives about diseases (eg, personal experiences, awareness, and risk factors) that are not otherwise easily captured through traditional surveys or administrative data.",
      "authors": "Tufts Christopher; Polsky Daniel; Volpp Kevin G; Groeneveld Peter W; Ungar Lyle; Merchant Raina M; Pelullo Arthur P",
      "year": "2018",
      "journal": "JMIR public health and surveillance",
      "doi": "10.2196/10834",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30522989/",
      "mesh_terms": "",
      "keywords": "Twitter messaging; disease; prevalence; public health surveillance; social media",
      "pub_types": "Journal Article",
      "pmcid": "PMC6302232",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41148137",
      "title": "Incomplete Family History and Meeting Algorithmic Criteria for Genetic Evaluation of Hereditary Cancer.",
      "abstract": "IMPORTANCE: Incomplete electronic health record (EHR) documentation may limit the effectiveness of clinical decision support (CDS) algorithms designed to identify patients eligible for hereditary cancer genetic evaluation. OBJECTIVES: To determine whether a CDS algorithm can identify patients who meet criteria for hereditary cancer genetic evaluation when family history data are incompletely documented in the EHR, and to examine whether data missingness is associated with identification patterns across patient subgroups. DESIGN, SETTING, AND PARTICIPANTS: This cross-sectional study analyzed EHR data extracted in December 2020 from 2 large US health care systems: University of Utah Health (UHealth) and NYU Langone Health (NYULH). Eligible patients were adults aged 25 to 60 years who visited a primary care clinic within the previous 3 years and had some EHR documentation of cancer family history. Data analysis was conducted in August 2024. EXPOSURES: Patient demographic factors (age, sex, race and ethnicity, and language preference) and cancer family history characteristics (number of cancer history records, number of affected first- and second-degree relatives, relatives with rising mortality cancers, presence of hereditary cancer-related terms in comments, and completeness of documentation). MAIN OUTCOMES AND MEASURES: The primary outcome was meeting at least 1 CDS algorithm criterion for genetic evaluation of hereditary cancer risk based on National Comprehensive Cancer Network guidelines. Missing data patterns were assessed using the Little missing completely at random test, with analyses conducted using complete case analysis and multiple imputation. RESULTS: This study included 157\u202f207 patients: 55\u202f918 from UHealth and 101\u202f289 from NYULH. Their mean (SD) age was 43.5 (9.8) years, and most (65.7%) were female. A total of 5607 UHealth patients (10.0%) and 10 375 NYULH patients (10.2%) met CDS criteria for genetic evaluation. At UHealth, data appeared to be missing completely at random (\u03c7239\u2009=\u200939.09; P\u2009=\u2009.47), and complete case compared with multiple imputation analyses yielded similar results. At NYULH, data were not missing completely at random (\u03c7255\u2009=\u2009914.89; P\u2009<\u2009.001). Compared with multiple imputation, complete case analysis produced different association magnitudes for older age and having relatives with rising mortality cancers, suggesting bias when excluding incomplete records. CONCLUSIONS AND RELEVANCE: In this cross-sectional study, the magnitude of the association between incomplete family history documentation and identification of patients eligible for hereditary cancer genetic evaluation depended on whether data were missing randomly or systematically. These findings suggest that health care organizations implementing CDS algorithms should assess their specific missing data patterns and consider tailored approaches to handling incomplete family history information to ensure equitable identification of all patients who could benefit from genetic evaluation services.",
      "authors": "Harris Adrian; Bather Jemar R; Bradshaw Richard L; Kawamoto Kensaku; Del Fiol Guilherme; Kohlmann Wendy K; Chavez-Yenter Daniel; Monahan Rachel; Chambers Rachelle L; Sigireddi Meenakshi; Goodman Melody S; Kaphingst Kimberly A",
      "year": "2025",
      "journal": "JAMA network open",
      "doi": "10.1001/jamanetworkopen.2025.39870",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41148137/",
      "mesh_terms": "Humans; Male; Female; Cross-Sectional Studies; Middle Aged; Algorithms; Adult; Electronic Health Records; Genetic Testing; Neoplasms; Medical History Taking; Decision Support Systems, Clinical; Genetic Predisposition to Disease; Utah",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC12569706",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36414774",
      "title": "Mitigating the impact of biased artificial intelligence in emergency decision-making.",
      "abstract": "BACKGROUND: Prior research has shown that artificial intelligence (AI) systems often encode biases against minority subgroups. However, little work has focused on ways to mitigate the harm discriminatory algorithms can cause in high-stakes settings such as medicine. METHODS: In this study, we experimentally evaluated the impact biased AI recommendations have on emergency decisions, where participants respond to mental health crises by calling for either medical or police assistance. We recruited 438 clinicians and 516 non-experts to participate in our web-based experiment. We evaluated participant decision-making with and without advice from biased and unbiased AI systems. We also varied the style of the AI advice, framing it either as prescriptive recommendations or descriptive flags. RESULTS: Participant decisions are unbiased without AI advice. However, both clinicians and non-experts are influenced by prescriptive recommendations from a biased algorithm, choosing police help more often in emergencies involving African-American or Muslim men. Crucially, using descriptive flags rather than prescriptive recommendations allows respondents to retain their original, unbiased decision-making. CONCLUSIONS: Our work demonstrates the practical danger of using biased models in health contexts, and suggests that appropriately framing decision support can mitigate the effects of AI bias. These findings must be carefully considered in the many real-world clinical scenarios where inaccurate or biased models may be used to inform important decisions.",
      "authors": "Adam Hammaad; Balagopalan Aparna; Alsentzer Emily; Christia Fotini; Ghassemi Marzyeh",
      "year": "2022",
      "journal": "Communications medicine",
      "doi": "10.1038/s43856-022-00214-4",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36414774/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC9681767",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39199522",
      "title": "Leveraging Artificial Intelligence to Optimize Transcranial Direct Current Stimulation for Long COVID Management: A Forward-Looking Perspective.",
      "abstract": "Long COVID (Coronavirus disease), affecting millions globally, presents unprecedented challenges to healthcare systems due to its complex, multifaceted nature and the lack of effective treatments. This perspective review explores the potential of artificial intelligence (AI)-guided transcranial direct current stimulation (tDCS) as an innovative approach to address the urgent need for effective Long COVID management. The authors examine how AI could optimize tDCS protocols, enhance clinical trial design, and facilitate personalized treatment for the heterogeneous manifestations of Long COVID. Key areas discussed include AI-driven personalization of tDCS parameters based on individual patient characteristics and real-time symptom fluctuations, the use of machine learning for patient stratification, and the development of more sensitive outcome measures in clinical trials. This perspective addresses ethical considerations surrounding data privacy, algorithmic bias, and equitable access to AI-enhanced treatments. It also explores challenges and opportunities for implementing AI-guided tDCS across diverse healthcare settings globally. Future research directions are outlined, including the need for large-scale validation studies and investigations of long-term efficacy and safety. The authors argue that while AI-guided tDCS shows promise for addressing the complex nature of Long COVID, significant technical, ethical, and practical challenges remain. They emphasize the importance of interdisciplinary collaboration, patient-centered approaches, and a commitment to global health equity in realizing the potential of this technology. This perspective article provides a roadmap for researchers, clinicians, and policymakers involved in developing and implementing AI-guided neuromodulation therapies for Long COVID and potentially other neurological and psychiatric conditions.",
      "authors": "Rudroff Thorsten; Rainio Oona; Kl\u00e9n Riku",
      "year": "2024",
      "journal": "Brain sciences",
      "doi": "10.3390/brainsci14080831",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39199522/",
      "mesh_terms": "",
      "keywords": "artificial intelligence; brain stimulation; long COVID; neuroimaging",
      "pub_types": "Journal Article",
      "pmcid": "PMC11353063",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "32568726",
      "title": "Racial and Ethnic Digital Divides in Posting COVID-19 Content on Social Media Among US Adults: Secondary Survey Analysis.",
      "abstract": "BACKGROUND: Public health surveillance experts are leveraging user-generated content on social media to track the spread and effects of COVID-19. However, racial and ethnic digital divides, which are disparities among people who have internet access and post on social media, can bias inferences. This bias is particularly problematic in the context of the COVID-19 pandemic because due to structural inequalities, members of racial and ethnic minority groups are disproportionately vulnerable to contracting the virus and to the deleterious economic and social effects from mitigation efforts. Further, important demographic intersections with race and ethnicity, such as gender and age, are rarely investigated in work characterizing social media users; however, they reflect additional axes of inequality shaping differential exposure to COVID-19 and its effects. OBJECTIVE: The aim of this study was to characterize how the race and ethnicity of US adults are associated with their odds of posting COVID-19 content on social media and how gender and age modify these odds. METHODS: We performed a secondary analysis of a survey conducted by the Pew Research Center from March 19 to 24, 2020, using a national probability sample (N=10,510). Respondents were recruited from an online panel, where panelists without an internet-enabled device were given one to keep at no cost. The binary dependent variable was responses to an item asking whether respondents \"used social media to share or post information about the coronavirus.\" We used survey-weighted logistic regressions to estimate the odds of responding in the affirmative based on the race and ethnicity of respondents (white, black, Latino, other race/ethnicity), adjusted for covariates measuring sociodemographic background and COVID-19 experiences. We examined how gender (female, male) and age (18 to 30 years, 31 to 50 years, 51 to 64 years, and 65 years and older) intersected with race and ethnicity by estimating interactions. RESULTS: Respondents who identified as black (odds ratio [OR] 1.29, 95% CI 1.02-1.64; P=.03), Latino (OR 1.66, 95% CI 1.36-2.04; P<.001), or other races/ethnicities (OR 1.33, 95% CI 1.02-1.72; P=.03) had higher odds than respondents who identified as white of reporting that they posted COVID-19 content on social media. Women had higher odds of posting than men regardless of race and ethnicity (OR 1.58, 95% CI 1.39-1.80; P<.001). Among men, respondents who identified as black, Latino, or members of other races/ethnicities were significantly more likely to post than respondents who identified as white. Older adults (65 years or older) had significantly lower odds (OR 0.73, 95% CI 0.57-0.94; P=.01) of posting compared to younger adults (18-29 years), particularly among those identifying as other races/ethnicities. Latino respondents were the most likely to report posting across all age groups. CONCLUSIONS: In the United States, members of racial and ethnic minority groups are most likely to contribute to COVID-19 content on social media, particularly among groups traditionally less likely to use social media (older adults and men). The next step is to ensure that data collection procedures capture this diversity by encompassing a breadth of search criteria and social media platforms.",
      "authors": "Campos-Castillo Celeste; Laestadius Linnea I",
      "year": "2020",
      "journal": "Journal of medical Internet research",
      "doi": "10.2196/20472",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32568726/",
      "mesh_terms": "Adolescent; Black or African American; Age Factors; Aged; Betacoronavirus; COVID-19; Coronavirus Infections; Digital Divide; Ethnicity; Female; Hispanic or Latino; Humans; Male; Middle Aged; Minority Groups; Odds Ratio; Pandemics; Pneumonia, Viral; Racial Groups; SARS-CoV-2; Sex Factors; Social Media; Socioeconomic Factors; Surveys and Questionnaires; United States; White People; Young Adult",
      "keywords": "COVID-19; algorithm bias; bias; digital divides; ethnicity; public health; race; social media; surveillance; user characteristics",
      "pub_types": "Journal Article",
      "pmcid": "PMC7340161",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "41282840",
      "title": "Nonfasting, Telehealth-Ready LDL-C Testing With Machine Learning to Improve Cardiovascular Access and Equity.",
      "abstract": "IMPORTANCE: Current LDL-C testing requires 9-12 hour fasting and in-person visits, creating an access crisis: 40% of lipid panels occur outside fasting windows (yielding unreliable results), 60% of US counties lack cardiology services, and millions of patients with diabetes cannot safely fast. Meanwhile, telehealth infrastructure expanded 38-fold post-COVID, yet lipid workflows remain anchored to 1970s protocols. This mismatch drives ~20 million unnecessary repeat visits annually, disproportionately burdening Medicaid populations, essential workers, and rural communities. OBJECTIVE: To demonstrate that machine learning can transform lipid testing from a fasting-dependent, clinic-based bottleneck into an accurate, equitable, telehealth-ready service-eliminating three structural barriers simultaneously: fasting requirements, in-person visits, and racial algorithmic bias. DESIGN SETTING AND PARTICIPANTS: Cross-sectional analysis of All of Us Research Program (n=3,477; test n=696). Crucially, 40.1% were tested outside traditional fasting windows, reflecting real-world practice. We evaluated performance stratified by fasting status, telehealth feasibility (labs-only configuration), racial equity metrics, and economic impact. MAIN OUTCOMES AND MEASURES: Primary: MAE and calibration in non-fasting states. Secondary: Labs-only non-inferiority (\u00b10.5 mg dL-1margin); racial equity (Black-White performance gap); economic savings from eliminated repeat visits; and classification accuracy at treatment thresholds (70, 100, 130 mg dL-1). RESULTS: The ML system demonstrated paradoxical superiority in non-fasting conditions-precisely when needed most. While equations deteriorated (Friedewald MAE 29.1 vs 25.9 mg dL-1fasting, slopes 0.58-0.61), ML maintained accuracy (24.0 vs 23.2 mg dL-1, slopes 0.99-1.07), achieving 17.2% improvement over Friedewald when non-fasting vs 10.4% fasting. Labs-only configuration proved non-inferior (MAE=-0.12, p<0.001), enabling national retail-pharmacy and home-testing workflows. The system achieved racial equity without race input (Black-White gap -0.19 mg dL-1, CI includes zero) while providing greatest improvement for Black patients (19% vs 11% for White). Economically, eliminating 4,000 repeat visits per 10,000 tests helps address an estimated $2 billion annual repeat-testing cost burden and yields $815,000 total savings per 10,000 tests ($245,000 direct healthcare, $570,000 patient costs), with break-even at just 750 tests. CONCLUSIONS AND RELEVANCE: This ML approach helps address an estimated $2 billion annual problem of repeat testing while tackling three critical quality gaps in cardiovascular prevention: delayed treatment initiation, poor monitoring adherence, and specialty access barriers. By enabling accurate non-fasting, telehealth-compatible, race-free LDL-C estimation, it transforms lipid testing from an access barrier into an access enabler-particularly for the Medicaid, Medicare Advantage, and rural populations who drive both cost and outcomes in value-based care. From a technical standpoint, implementation requires only routine labs and <100 ms computation, making deployment feasible with existing infrastructure.",
      "authors": "Doku Ronald; Osafo Nana Yaw; Kwagyan John; Southerland William M",
      "year": "2025",
      "journal": "medRxiv : the preprint server for health sciences",
      "doi": "10.1101/2025.10.27.25338909",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41282840/",
      "mesh_terms": "",
      "keywords": "cardiovascular quality improvement; health equity; healthcare delivery; low-density lipoprotein cholesterol; machine learning; non-fasting lipid panel; telehealth; value-based care",
      "pub_types": "Journal Article; Preprint",
      "pmcid": "PMC12636691",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "39470636",
      "title": "Electronic Health Record Phenotyping of Pediatric Suicide-Related Emergency Department Visits.",
      "abstract": "IMPORTANCE: Suicide is a leading cause of death among young people. Accurate detection of self-injurious thoughts and behaviors (SITB) underpins equity in youth suicide prevention. OBJECTIVES: To compare methods of detecting SITB using structured electronic health information and measure algorithmic performance across demographics. DESIGN, SETTING, AND PARTICIPANTS: This cross-sectional study used medical records among youths aged 6 to 17 years with at least 1 mental health-related emergency department (ED) visit in 2017 to 2019 to an academic health system in Southern California serving 787\u202f000 unique individuals each year. Analyses were conducted between January and September 2023. EXPOSURES: Multiexpert electronic health record review ascertained the presence of SITB using the Columbia Classification Algorithm of Suicide Assessment. Random forest classifiers with nested cross-validation were developed using (1) International Statistical Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM) codes for nonfatal suicide attempt and self-harm and chief concern and (2) all available structured data, including diagnoses, medications, and laboratory tests. MAIN OUTCOME AND MEASURES: Detection performance was assessed overall and stratified by age group, sex, and race and ethnicity. RESULTS: The sample comprised 2702 unique youths with an MH-related ED visit (1384 youths who identified as female [51.2%]; 131 Asian [4.8%], 266 Black [9.8%], 719 Hispanic [26.6%], 1319 White [48.8%], and 233 other race [8.6%]; median [IQR] age, 14 [12-16] years), including 898 children and 1804 adolescents. Approximately half of visits were related to SITB (1286 visits [47.6%]). Sensitivity of SITB detection using only codes and chief concern varied by age group and increased until age 15 years (6-9 years: 59.3% [95% CI, 48.5%-69.5%]; 10-12 years: 69.0% [95% CI, 63.8%-73.9%]; 13-15 years: 88.4% [95% CI, 85.1%-91.2%]; 16-17 years: 83.1% [95% CI, 79.1%-86.6%]), while specificity remained constant. The area under the receiver operating characteristic curve (AUROC) was lower among preadolescents (0.841 [95% CI, 0.815-0.867]) and male (0.869 [95% CI, 0.848-0.890]), Black (0.859 [95% CI, 0.813-0.905]), and Hispanic (0.861 [95% CI, 0.831-0.891]) youths compared with adolescents (0.925 [95% CI, 0.912-0.938]), female youths (0.923 [95% CI, 0.909-0.937]), and youths of other races and ethnicities (eg, White: 0.901 [95% CI, 0.884-0.918]). Augmented classification (ie, using all available structured data) outperformed classification with codes and chief concern alone (AUROC, 0.975 [95% CI, 0.968-0.980] vs 0.894 [95% CI, 0.882-0.905]; P\u2009<\u2009.001). CONCLUSIONS AND RELEVANCE: In this study, diagnostic codes and chief concern underestimated SITB prevalence, particularly among minoritized youths. These results suggest that priority on algorithmic fairness in suicide prevention strategies must extend to accurate detection of youths with suicide-related emergencies.",
      "authors": "Edgcomb Juliet Beni; Olde Loohuis Loes; Tseng Chi-Hong; Klomhaus Alexandra M; Choi Kristen R; Ponce Chrislie G; Zima Bonnie T",
      "year": "2024",
      "journal": "JAMA network open",
      "doi": "10.1001/jamanetworkopen.2024.42091",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39470636/",
      "mesh_terms": "Humans; Adolescent; Male; Female; Emergency Service, Hospital; Child; Cross-Sectional Studies; Electronic Health Records; California; Suicide, Attempted; Self-Injurious Behavior; Suicide; Phenotype; Emergency Room Visits",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC11522940",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "40266658",
      "title": "Development and Validation of a Dynamic Real-Time Risk Prediction Model for Intensive Care Units Patients Based on Longitudinal Irregular Data: Multicenter Retrospective Study.",
      "abstract": "BACKGROUND: Timely and accurate prediction of short-term mortality is critical in intensive care units (ICUs), where patients' conditions change rapidly. Traditional scoring systems, such as the Simplified Acute Physiology Score and Acute Physiology and Chronic Health Evaluation, rely on static variables collected within the first 24 hours of admission and do not account for continuously evolving clinical states. These systems lack real-time adaptability, interpretability, and generalizability. With the increasing availability of high-frequency electronic medical record (EMR) data, machine learning (ML) approaches have emerged as powerful tools to model complex temporal patterns and support dynamic clinical decision-making. However, existing models are often limited by their inability to handle irregular sampling and missing values, and many lack rigorous external validation across institutions. OBJECTIVE: We aimed to develop a real-time, interpretable risk prediction model that continuously assesses ICU patient mortality using irregular, longitudinal EMR data, with improved performance and generalizability over traditional static scoring systems. METHODS: A time-aware bidirectional attention-based long short-term memory (TBAL) model was developed using EMR data from the MIMIC-IV (Medical Information Mart for Intensive Care) and eICU Collaborative Research Database (eICU-CRD) databases, comprising 176,344 ICU stays. The model incorporated dynamic variables, including vital signs, laboratory results, and medication data, updated hourly, to perform static and continuous mortality risk assessments. External cross-validation and subgroup sensitivity analyses were conducted to evaluate robustness and fairness. Model performance was assessed using the area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve (AUPRC), accuracy, and F1-score. Interpretability was enhanced using integrated gradients to identify key predictors. RESULTS: For the static 12-hour to 1-day mortality prediction task, the TBAL model achieved AUROCs of 95.9 (95% CI 94.2-97.5) and 93.3 (95% CI 91.5-95.3) and AUPRCs of 48.5 and 21.6 in MIMIC-IV and eICU-CRD, respectively. Accuracy and F1-scores reached 94.1 and 46.7 in MIMIC-IV and 92.2 and 28.1 in eICU-CRD. In dynamic prediction tasks, AUROCs reached 93.6 (95% CI 93.2-93.9) and 91.9 (95% CI 91.6-92.1), with AUPRCs of 41.3 and 50, respectively. The model maintained high recall for positive cases (82.6% and 79.1% in MIMIC-IV and eICU-CRD). Cross-database validation yielded AUROCs of 81.3 and 76.1, confirming generalizability. Subgroup analysis showed stable performance across age, sex, and severity strata, with top predictors including lactate, vasopressor use, and Glasgow Coma Scale score. CONCLUSIONS: The TBAL model offers a robust, interpretable, and generalizable solution for dynamic real-time mortality risk prediction in ICU patients. Its ability to adapt to irregular temporal patterns and to provide hourly updated predictions positions it as a promising decision-support tool. Future work should validate its utility in prospective clinical trials and investigate its integration into real-world ICU workflows to enhance patient outcomes.",
      "authors": "Zheng Zhuo; Luo Jiawei; Zhu Yingchao; Du Lei; Lan Lan; Zhou Xiaobo; Yang Xiaoyan; Huang Shixin",
      "year": "2025",
      "journal": "Journal of medical Internet research",
      "doi": "10.2196/69293",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40266658/",
      "mesh_terms": "Humans; Intensive Care Units; Retrospective Studies; Electronic Health Records; Risk Assessment; Female; Male; Middle Aged; Machine Learning; Aged; Longitudinal Studies; Hospital Mortality",
      "keywords": "continuous prediction; in-hospital mortality; intensive care units; machine learning; model interpretability",
      "pub_types": "Journal Article; Multicenter Study; Validation Study",
      "pmcid": "PMC12059492",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "36791660",
      "title": "Algorithmic encoding of protected characteristics in chest X-ray disease detection models.",
      "abstract": "BACKGROUND: It has been rightfully emphasized that the use of AI for clinical decision making could amplify health disparities. An algorithm may encode protected characteristics, and then use this information for making predictions due to undesirable correlations in the (historical) training data. It remains unclear how we can establish whether such information is actually used. Besides the scarcity of data from underserved populations, very little is known about how dataset biases manifest in predictive models and how this may result in disparate performance. This article aims to shed some light on these issues by exploring methodology for subgroup analysis in image-based disease detection models. METHODS: We utilize two publicly available chest X-ray datasets, CheXpert and MIMIC-CXR, to study performance disparities across race and biological sex in deep learning models. We explore test set resampling, transfer learning, multitask learning, and model inspection to assess the relationship between the encoding of protected characteristics and disease detection performance across subgroups. FINDINGS: We confirm subgroup disparities in terms of shifted true and false positive rates which are partially removed after correcting for population and prevalence shifts in the test sets. We find that transfer learning alone is insufficient for establishing whether specific patient information is used for making predictions. The proposed combination of test-set resampling, multitask learning, and model inspection reveals valuable insights about the way protected characteristics are encoded in the feature representations of deep neural networks. INTERPRETATION: Subgroup analysis is key for identifying performance disparities of AI models, but statistical differences across subgroups need to be taken into account when analyzing potential biases in disease detection. The proposed methodology provides a comprehensive framework for subgroup analysis enabling further research into the underlying causes of disparities. FUNDING: European Research Council Horizon 2020, UK Research and Innovation.",
      "authors": "Glocker Ben; Jones Charles; Bernhardt M\u00e9lanie; Winzeck Stefan",
      "year": "2023",
      "journal": "EBioMedicine",
      "doi": "10.1016/j.ebiom.2023.104467",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36791660/",
      "mesh_terms": "Humans; X-Rays; Deep Learning; Neural Networks, Computer; Algorithms; Radiography",
      "keywords": "Algorithmic bias; Artificial intelligence; Image-based disease detection; Subgroup disparities",
      "pub_types": "Journal Article",
      "pmcid": "PMC10025760",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "35623797",
      "title": "Development and multimodal validation of a substance misuse algorithm for referral to treatment using artificial intelligence (SMART-AI): a retrospective deep learning study.",
      "abstract": "BACKGROUND: Substance misuse is a heterogeneous and complex set of behavioural conditions that are highly prevalent in hospital settings and frequently co-occur. Few hospital-wide solutions exist to comprehensively and reliably identify these conditions to prioritise care and guide treatment. The aim of this study was to apply natural language processing (NLP) to clinical notes collected in the electronic health record (EHR) to accurately screen for substance misuse. METHODS: The model was trained and developed on a reference dataset derived from a hospital-wide programme at Rush University Medical Center (RUMC), Chicago, IL, USA, that used structured diagnostic interviews to manually screen admitted patients over 27 months (between Oct 1, 2017, and Dec 31, 2019; n=54\u2008915). The Alcohol Use Disorder Identification Test and Drug Abuse Screening Tool served as reference standards. The first 24 h of notes in the EHR were mapped to standardised medical vocabulary and fed into single-label, multilabel, and multilabel with auxillary-task neural network models. Temporal validation of the model was done using data from the subsequent 12 months on a subset of RUMC patients (n=16\u2008917). External validation was done using data from Loyola University Medical Center, Chicago, IL, USA between Jan 1, 2007, and Sept 30, 2017 (n=1991 adult patients). The primary outcome was discrimination for alcohol misuse, opioid misuse, or non-opioid drug misuse. Discrimination was assessed by the area under the receiver operating characteristic curve (AUROC). Calibration slope and intercept were measured with the unreliability index. Bias assessments were performed across demographic subgroups. FINDINGS: The model was trained on a cohort that had 3\u00b75% misuse (n=1\u2008921) with any type of substance. 220 (11%) of 1921 patients with substance misuse had more than one type of misuse. The multilabel convolutional neural network classifier had a mean AUROC of 0\u00b797 (95% CI 0\u00b796-0\u00b798) during temporal validation for all types of substance misuse. The model was well calibrated and showed good face validity with model features containing explicit mentions of aberrant drug-taking behaviour. A false-negative rate of 0\u00b718-0\u00b719 and a false-positive rate of 0\u00b703 between non-Hispanic Black and non-Hispanic White groups occurred. In external validation, the AUROCs for alcohol and opioid misuse were 0\u00b788 (95% CI 0\u00b786-0\u00b790) and 0\u00b794 (0\u00b792-0\u00b795), respectively. INTERPRETATION: We developed a novel and accurate approach to leveraging the first 24 h of EHR notes for screening multiple types of substance misuse. FUNDING: National Institute On Drug Abuse, National Institutes of Health.",
      "authors": "Afshar Majid; Sharma Brihat; Dligach Dmitriy; Oguss Madeline; Brown Randall; Chhabra Neeraj; Thompson Hale M; Markossian Talar; Joyce Cara; Churpek Matthew M; Karnik Niranjan S",
      "year": "2022",
      "journal": "The Lancet. Digital health",
      "doi": "10.1016/S2589-7500(22)00041-3",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35623797/",
      "mesh_terms": "Adult; Alcoholism; Artificial Intelligence; Deep Learning; Humans; Opioid-Related Disorders; Referral and Consultation; Retrospective Studies; United States",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "pmcid": "PMC9159760",
      "ft_status": "Included (2-stage)"
    },
    {
      "pmid": "34207713",
      "title": "Propensity Score Analysis Assessing the Burden of Non-Communicable Diseases among the Transgender Population in the United States Using the Behavioral Risk Factor Surveillance System (2017-2019).",
      "abstract": "Research to assess the burden of non-communicable diseases (NCDs) among the transgender population needs to be prioritized given the high prevalence of chronic conditions and associated risk factors in this group. Previous cross-sectional studies utilized unmatched samples with a significant covariate imbalance resulting in a selection bias. Therefore, this cross-sectional study attempts to assess and compare the burden of NCDs among propensity score-matched transgender and cisgender population groups. This study analyzed Behavioral Risk Factor Surveillance System data (2017-2019) using complex weighting procedures to generate nationally representative samples. Logistic regression was fit to estimate propensity scores. Transgender and cisgender groups were matched by sociodemographic variables using a 1:1 nearest neighbor matching algorithm. McNemar, univariate, and multivariate logistic regression analyses were conducted among matched cohorts using R and SPSS version 26 software. Compared with the cisgender group, the transgender group was significantly more likely to have hypertension (31.3% vs. 27.6%), hypercholesteremia (30.8% vs. 23.7%), prediabetes (17.3% vs. 10.3%), and were heavy drinkers (6.7% vs. 6.0%) and smokers (22.4% vs. 20.0%). Moreover, the transgender group was more than twice as likely to have depression (aOR: 2.70, 95% CI 2.62-2.72), stroke (aOR: 2.52 95% CI 2.50-2.55), coronary heart disease (aOR: 2.77, 95% CI 2.74-2.81), and heart attack (aOR: 2.90, 95% CI 2.87-2.94). Additionally, the transgender group was 1.2-1.7 times more likely to have metabolic and malignant disorders. Differences were also found between transgender subgroups compared with the cisgender group. This study provides a clear picture of the NCD burden among the transgender population. These findings offer an evidence base to build health equity models to reduce disparities among transgender groups.",
      "authors": "Pharr Jennifer R; Batra Kavita",
      "year": "2021",
      "journal": "Healthcare (Basel, Switzerland)",
      "doi": "10.3390/healthcare9060696",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34207713/",
      "mesh_terms": "",
      "keywords": "Behavioral Risk Factor Surveillance System; non-communicable diseases; propensity score matching; transgender",
      "pub_types": "Journal Article",
      "pmcid": "PMC8226537",
      "ft_status": "Included (2-stage)"
    }
  ],
  "ft_excluded": [
    {
      "pmid": "20378467",
      "title": "N4ITK: improved N3 bias correction.",
      "abstract": "A variant of the popular nonparametric nonuniform intensity normalization (N3) algorithm is proposed for bias field correction. Given the superb performance of N3 and its public availability, it has been the subject of several evaluation studies. These studies have demonstrated the importance of certain parameters associated with the B-spline least-squares fitting. We propose the substitution of a recently developed fast and robust B-spline approximation routine and a modified hierarchical optimization scheme for improved bias field correction over the original N3 algorithm. Similar to the N3 algorithm, we also make the source code, testing, and technical documentation of our contribution, which we denote as \"N4ITK,\" available to the public through the Insight Toolkit of the National Institutes of Health. Performance assessment is demonstrated using simulated data from the publicly available Brainweb database, hyperpolarized (3)He lung image data, and 9.4T postmortem hippocampus data.",
      "authors": "Tustison Nicholas J; Avants Brian B; Cook Philip A; Zheng Yuanjie; Egan Alexander; Yushkevich Paul A; Gee James C",
      "year": "2010",
      "journal": "IEEE transactions on medical imaging",
      "doi": "10.1109/TMI.2010.2046908",
      "url": "https://pubmed.ncbi.nlm.nih.gov/20378467/",
      "mesh_terms": "Algorithms; Artifacts; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Reproducibility of Results; Sensitivity and Specificity",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC3071855",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "38552451",
      "title": "Minimizing bias when using artificial intelligence in critical care medicine.",
      "abstract": "",
      "authors": "Ranard Benjamin L; Park Soojin; Jia Yugang; Zhang Yiye; Alwan Fatima; Celi Leo Anthony; Lusczek Elizabeth R",
      "year": "2024",
      "journal": "Journal of critical care",
      "doi": "10.1016/j.jcrc.2024.154796",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38552451/",
      "mesh_terms": "Humans; Artificial Intelligence; Critical Care; Bias",
      "keywords": "Artificial intelligence; Bias; Critical Care; Disparities; Fairness; Health equity; Machine learning",
      "pub_types": "Editorial",
      "pmcid": "PMC11139594",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "39282853",
      "title": "Characterizing Veteran suicide decedents that were not classified as high-suicide-risk.",
      "abstract": "BACKGROUND: Although the Department of Veterans Affairs (VA) has made important suicide prevention advances, efforts primarily target high-risk patients with documented suicide risk, such as suicidal ideation, prior suicide attempts, and recent psychiatric hospitalization. Approximately 90% of VA patients that go on to die by suicide do not meet these high-risk criteria and therefore do not receive targeted suicide prevention services. In this study, we used national VA data to focus on patients that were not classified as high-risk, but died by suicide. METHODS: Our sample included all VA patients who died by suicide in 2017 or 2018. We determined whether patients were classified as high-risk using the VA's machine learning risk prediction algorithm. After excluding these patients, we used principal component analysis to identify moderate-risk and low-risk patients and investigated demographics, service-usage, diagnoses, and social determinants of health differences across high-, moderate-, and low-risk subgroups. RESULTS: High-risk (n = 452) patients tended to be younger, White, unmarried, homeless, and have more mental health diagnoses compared to moderate- (n = 2149) and low-risk (n = 2209) patients. Moderate- and low-risk patients tended to be older, married, Black, and Native American or Pacific Islander, and have more physical health diagnoses compared to high-risk patients. Low-risk patients had more missing data than higher-risk patients. CONCLUSIONS: Study expands epidemiological understanding about non-high-risk suicide decedents, historically understudied and underserved populations. Findings raise concerns about reliance on machine learning risk prediction models that may be biased by relative underrepresentation of racial/ethnic minorities within health system.",
      "authors": "Levis Maxwell; Dimambro Monica; Levy Joshua; Dufort Vincent; Fraade Abby; Winer Max; Shiner Brian",
      "year": "2024",
      "journal": "Psychological medicine",
      "doi": "10.1017/S0033291724001296",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39282853/",
      "mesh_terms": "Humans; Veterans; Male; Female; Middle Aged; United States; Aged; Adult; Suicide; United States Department of Veterans Affairs; Risk Factors; Risk Assessment; Mental Disorders; Machine Learning; Suicide, Attempted; Suicidal Ideation",
      "keywords": "machine learning; machine learning bias; suicide prevention; veterans and military",
      "pub_types": "Journal Article",
      "pmcid": "PMC11839400",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "32574353",
      "title": "Latent bias and the implementation of artificial intelligence in medicine.",
      "abstract": "Increasing recognition of biases in artificial intelligence (AI) algorithms has motivated the quest to build fair models, free of biases. However, building fair models may be only half the challenge. A seemingly fair model could involve, directly or indirectly, what we call \"latent biases.\" Just as latent errors are generally described as errors \"waiting to happen\" in complex systems, latent biases are biases waiting to happen. Here we describe 3 major challenges related to bias in AI algorithms and propose several ways of managing them. There is an urgent need to address latent biases before the widespread implementation of AI algorithms in clinical practice.",
      "authors": "DeCamp Matthew; Lindvall Charlotta",
      "year": "2020",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "doi": "10.1093/jamia/ocaa094",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32574353/",
      "mesh_terms": "Algorithms; Artificial Intelligence; Bias; Decision Support Systems, Clinical; Humans; Prejudice",
      "keywords": "artificial intelligence; bias; clinical decision support; health informatics; machine learning",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC7727353",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "38960729",
      "title": "Fair prediction of 2-year stroke risk in patients with atrial fibrillation.",
      "abstract": "OBJECTIVE: This study aims to develop machine learning models that provide both accurate and equitable predictions of 2-year stroke risk for patients with atrial fibrillation across diverse racial groups. MATERIALS AND METHODS: Our study utilized structured electronic health records (EHR) data from the All of Us Research Program. Machine learning models (LightGBM) were utilized to capture the relations between stroke risks and the predictors used by the widely recognized CHADS2 and CHA2DS2-VASc scores. We mitigated the racial disparity by creating a representative tuning set, customizing tuning criteria, and setting binary thresholds separately for subgroups. We constructed a hold-out test set that not only supports temporal validation but also includes a larger proportion of Black/African Americans for fairness validation. RESULTS: Compared to the original CHADS2 and CHA2DS2-VASc scores, significant improvements were achieved by modeling their predictors using machine learning models (Area Under the Receiver Operating Characteristic curve from near 0.70 to above 0.80). Furthermore, applying our disparity mitigation strategies can effectively enhance model fairness compared to the conventional cross-validation approach. DISCUSSION: Modeling CHADS2 and CHA2DS2-VASc risk factors with LightGBM and our disparity mitigation strategies achieved decent discriminative performance and excellent fairness performance. In addition, this approach can provide a complete interpretation of each predictor. These highlight its potential utility in clinical practice. CONCLUSIONS: Our research presents a practical example of addressing clinical challenges through the All of Us Research Program data. The disparity mitigation framework we proposed is adaptable across various models and data modalities, demonstrating broad potential in clinical informatics.",
      "authors": "Gao Jifan; Mar Philip; Tang Zheng-Zheng; Chen Guanhua",
      "year": "2024",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "doi": "10.1093/jamia/ocae170",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38960729/",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Atrial Fibrillation; Black or African American; Electronic Health Records; Machine Learning; Risk Assessment; Risk Factors; ROC Curve; Stroke; Racial Groups",
      "keywords": "atrial fibrillation; bias; fairness; machine learning; stroke",
      "pub_types": "Journal Article",
      "pmcid": "PMC11631105",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "37992791",
      "title": "A transformer-based deep learning approach for fairly predicting post-liver transplant risk factors.",
      "abstract": "Liver transplantation is a life-saving procedure for patients with end-stage liver disease. There are two main challenges in liver transplant: finding the best matching patient for a donor and ensuring transplant equity among different subpopulations. The current MELD scoring system evaluates a patient's mortality risk if not receiving an organ within 90\u00a0days. However, the donor-patient matching should also consider post-transplant risk factors, such as cardiovascular disease, chronic rejection, etc., which are all common complications after transplant. Accurate prediction of these risk scores remains a significant challenge. In this study, we used predictive models to solve the above challenges. Specifically, we proposed a deep learning model to predict multiple risk factors after a liver transplant. By formulating it as a multi-task learning problem, the proposed deep neural network was trained to simultaneously predict the five post-transplant risks and achieve equal good performance by exploiting task-balancing techniques. We also proposed a novel fairness-achieving algorithm to ensure prediction fairness across different subpopulations. We used electronic health records of 160,360 liver transplant patients, including demographic information, clinical variables, and laboratory values, collected from the liver transplant records of the United States from 1987 to 2018. The model's performance was evaluated using various performance metrics such as AUROC and AUPRC. Our experiment results highlighted the success of our multi-task model in achieving task balance while maintaining accuracy. The model significantly reduced the task discrepancy by 39\u00a0%. Further application of the fairness-achieving algorithm substantially reduced fairness disparity among all sensitive attributes (gender, age group, and race/ethnicity) in each risk factor. It underlined the potency of integrating fairness considerations into the task-balancing framework, ensuring robust and fair predictions across multiple tasks and diverse demographic groups.",
      "authors": "Li Can; Jiang Xiaoqian; Zhang Kai",
      "year": "2024",
      "journal": "Journal of biomedical informatics",
      "doi": "10.1016/j.jbi.2023.104545",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37992791/",
      "mesh_terms": "Humans; United States; Liver Transplantation; Deep Learning; Tissue Donors; Neural Networks, Computer; Risk Factors",
      "keywords": "Fairness; Liver transplantation; Multi-task learning; Risk prediction",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "pmcid": "PMC11619923",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "37827839",
      "title": "Ethical Considerations for Artificial Intelligence in Medical Imaging: Data Collection, Development, and Evaluation.",
      "abstract": "The development of artificial intelligence (AI) within nuclear imaging involves several ethically fraught components at different stages of the machine learning pipeline, including during data collection, model training and validation, and clinical use. Drawing on the traditional principles of medical and research ethics, and highlighting the need to ensure health justice, the AI task force of the Society of Nuclear Medicine and Molecular Imaging has identified 4 major ethical risks: privacy of data subjects, data quality and model efficacy, fairness toward marginalized populations, and transparency of clinical performance. We provide preliminary recommendations to developers of AI-driven medical devices for mitigating the impact of these risks on patients and populations.",
      "authors": "Herington Jonathan; McCradden Melissa D; Creel Kathleen; Boellaard Ronald; Jones Elizabeth C; Jha Abhinav K; Rahmim Arman; Scott Peter J H; Sunderland John J; Wahl Richard L; Zuehlsdorff Sven; Saboury Babak",
      "year": "2023",
      "journal": "Journal of nuclear medicine : official publication, Society of Nuclear Medicine",
      "doi": "10.2967/jnumed.123.266080",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37827839/",
      "mesh_terms": "Humans; Artificial Intelligence; Data Collection; Machine Learning; Advisory Committees; Molecular Imaging",
      "keywords": "AI as medical device; AI ethics; health disparity; socioeconomic determinants of health; software as medical device",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC10690124",
      "ft_status": "Excluded: Insufficient evidence (approach_count=2, bias_title=False)"
    },
    {
      "pmid": "35308985",
      "title": "Data and Model Biases in Social Media Analyses: A Case Study of COVID-19 Tweets.",
      "abstract": "During the coronavirus disease pandemic (COVID-19), social media platforms such as Twitter have become a venue for individuals, health professionals, and government agencies to share COVID-19 information. Twitter has been a popular source of data for researchers, especially for public health studies. However, the use of Twitter data for research also has drawbacks and barriers. Biases appear everywhere from data collection methods to modeling approaches, and those biases have not been systematically assessed. In this study, we examined six different data collection methods and three different machine learning (ML) models-commonly used in social media analysis-to assess data collection bias and measure ML models' sensitivity to data collection bias. We showed that (1) publicly available Twitter data collection endpoints with appropriate strategies can collect data that is reasonably representative of the Twitter universe; and (2) careful examinations of ML models' sensitivity to data collection bias are critical.",
      "authors": "Zhao Yunpeng; Yin Pengfei; Li Yongqiu; He Xing; Du Jingcheng; Tao Cui; Guo Yi; Prosperi Mattia; Veltri Pierangelo; Yang Xi; Wu Yonghui; Bian Jiang",
      "year": "2021",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "doi": "10.1145/3400806.3400839",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35308985/",
      "mesh_terms": "Bias; COVID-19; Data Collection; Humans; Machine Learning; Social Media",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "pmcid": "PMC8861742",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "32585698",
      "title": "Patient safety and quality improvement: Ethical principles for a regulatory approach to bias in healthcare machine learning.",
      "abstract": "Accumulating evidence demonstrates the impact of bias that reflects social inequality on the performance of machine learning (ML) models in health care. Given their intended placement within healthcare decision making more broadly, ML tools require attention to adequately quantify the impact of bias and reduce its potential to exacerbate inequalities. We suggest that taking a patient safety and quality improvement approach to bias can support the quantification of bias-related effects on ML. Drawing from the ethical principles underpinning these approaches, we argue that patient safety and quality improvement lenses support the quantification of relevant performance metrics, in order to minimize harm while promoting accountability, justice, and transparency. We identify specific methods for operationalizing these principles with the goal of attending to bias to support better decision making in light of controllable and uncontrollable factors.",
      "authors": "McCradden Melissa D; Joshi Shalmali; Anderson James A; Mazwi Mjaye; Goldenberg Anna; Zlotnik Shaul Randi",
      "year": "2020",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "doi": "10.1093/jamia/ocaa085",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32585698/",
      "mesh_terms": "Artificial Intelligence; Data Collection; Government Regulation; Healthcare Disparities; Humans; Patient Safety; Prejudice; Quality Improvement; Social Determinants of Health",
      "keywords": "healthcare delivery; machine learning; patient safety; quality improvement; systematic bias",
      "pub_types": "Journal Article",
      "pmcid": "PMC7727331",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "40672707",
      "title": "Misattribution Bias of COVID-19 Hospitalizations in Alberta Using an Admission Algorithm.",
      "abstract": "BACKGROUND: With initial waves of COVID-19, many public health systems assumed each COVID-19 positive hospitalization was a direct cause from COVID-19 infection. Since January 2022, Alberta Health Services Communicable Disease Control Hospitalization Team (CDC-HT) implemented an admission criteria algorithm to adjudicate COVID-19 as a direct, contributing, or unrelated cause for all COVID-19 admissions in Alberta. METHODS: This quality improvement initiative sought to improve the admission algorithm's precision in reporting COVID-19 admissions. Patient hospitalization records from January-February 2022 with a positive COVID-19 test in the last 30 days were proportionally sampled in a geographically stratified manner across Alberta health zones. 261 patient records were sampled and determination of COVID-19 attribution by CDC-HT algorithm was compared to adjudication by a panel of infectious diseases physicians with extensive COVID-19 clinical experience. RESULTS: Of 261 sampled COVID-19 admissions, blinded physician adjudication determined 39.9% were direct-cause, 17.2% contributing-cause, and 37.6% unrelated-cause. Within the same cohort the CDC-HT admission algorithm adjudicated 42.9% direct-cause, 24.5% contributing-cause, and 30.3% unrelated-cause. Cohen's kappa was 0.475, providing only moderate agreement. The majority of discrepancy was from over-attribution of unrelated hospitalizations as contributing cause. Implementation of this algorithm in Alberta throughout 2022 showed a fluctuating proportion of direct plus contributing COVID-19 hospitalizations as low as 40%. CONCLUSION: There was misattribution bias in COVID-19 hospitalization determination using the admission algorithm. The findings from this analysis led to improvements in the algorithm to improve precision. Public health jurisdictions should review their COVID-19 hospitalization reporting approaches to ensure validity and consideration of incidental cases.",
      "authors": "Dinh Tri; Ross Jordan; James Samantha; Klein Kristin; Chandran A Uma; Larios Oscar; Strong David; Conly John M",
      "year": "2024",
      "journal": "Journal of the Association of Medical Microbiology and Infectious Disease Canada = Journal officiel de l'Association pour la microbiologie medicale et l'infectiologie Canada",
      "doi": "10.3138/jammi-2024-0011",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40672707/",
      "mesh_terms": "",
      "keywords": "COVID-19; SARS-COV-2; admission; hospitalization; incidental; misattribution bias",
      "pub_types": "Journal Article",
      "pmcid": "PMC12258636",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "37615359",
      "title": "Toward Advancing Precision Environmental Health: Developing a Customized Exposure Burden Score to PFAS Mixtures to Enable Equitable Comparisons Across Population Subgroups, Using Mixture Item Response Theory.",
      "abstract": "Quantifying a person's cumulative exposure burden to per- and polyfluoroalkyl substances (PFAS) mixtures is important for risk assessment, biomonitoring, and reporting of results to participants. However, different people may be exposed to different sets of PFASs due to heterogeneity in the exposure sources and patterns. Applying a single measurement model for the entire population (e.g., by summing concentrations of all PFAS analytes) assumes that each PFAS analyte is equally informative to PFAS exposure burden for all individuals. This assumption may not hold if PFAS exposure sources systematically differ within the population. However, the sociodemographic, dietary, and behavioral characteristics that underlie systematic exposure differences may not be known, or may be due to a combination of these factors. Therefore, we used mixture item response theory, an unsupervised psychometrics and data science method, to develop a customized PFAS exposure burden scoring algorithm. This scoring algorithm ensures that PFAS burden scores can be equitably compared across population subgroups. We applied our methods to PFAS biomonitoring data from the United States National Health and Nutrition Examination Survey (2013-2018). Using mixture item response theory, we found that participants with higher household incomes had higher PFAS burden scores. Asian Americans had significantly higher PFAS burden compared with non-Hispanic Whites and other race/ethnicity groups. However, some disparities were masked when using summed PFAS concentrations as the exposure metric. This work demonstrates that our summary PFAS burden metric, accounting for sources of exposure variation, may be a more fair and informative estimate of PFAS exposure.",
      "authors": "Liu Shelley H; Feuerstahler Leah; Chen Yitong; Braun Joseph M; Buckley Jessie P",
      "year": "2023",
      "journal": "Environmental science & technology",
      "doi": "10.1021/acs.est.3c00343",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37615359/",
      "mesh_terms": "Humans; United States; Environmental Pollutants; Alkanesulfonic Acids; Nutrition Surveys; Fluorocarbons; Environmental Health",
      "keywords": "algorithmic bias; chemical mixtures; fairness; item response theory; latent variables; per- and polyfluoroalkyl substances; precision environmental health; psychometrics",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC11106720",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "40417536",
      "title": "Does Cohort Selection Affect Machine Learning from Clinical Data?",
      "abstract": "This study investigates cohort selection and its effects on the quality of machine learning (ML) models trained on clinical data, focusing on measurements taken within the first 48 hours of hospital admission. It discusses the potential repercussions of making arbitrary decisions during data processing prior to applying ML methods. Experiments are performed within the framework of the National COVID Cohort Collaborative (N3C) dataset. The research aims to unravel biases and assess the fairness of machine learning models used to predict outcomes for hospitalized patients. Detailed discussions cover the data, decision-making processes, and the resulting impact on model predictions regarding patient outcomes. An experiment is conducted in which four arbitrary decisions are made, resulting in 16 distinct datasets characterized by varying sizes and properties. The findings demonstrate significant differences in the obtained datasets and indicate a high potential for bias based on inclusion or exclusion decisions. The results also confirm significant differences in the performance of models constructed on different cohorts, especially when cross-compared between ones based on different inclusion criteria. The study specifically chose to analyze gender, race, and ethnicity as these social determinants of health played a significant role in COVID-19 outcomes.",
      "authors": "Haghighathoseini Atefehsadat; Wojtusiak Janusz; Min Hua; Leslie Timothy; Frankenfeld Cara; Menon Nirup M",
      "year": "2024",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "doi": "",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40417536/",
      "mesh_terms": "Machine Learning; Humans; COVID-19; Cohort Studies; SARS-CoV-2; Female; Male",
      "keywords": "Data Processing; Machine Learning; National COVID Cohort Collaborative (N3C); Prediction; Selection Bias",
      "pub_types": "Journal Article",
      "pmcid": "PMC12099332",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "36865610",
      "title": "Identification of Social and Racial Disparities in Risk of HIV Infection in Florida using Causal AI Methods.",
      "abstract": "Florida -the 3rd most populous state in the USA-has the highest rates of Human Immunodeficiency Virus (HIV) infections and of unfavorable HIV outcomes, with marked social and racial disparities. In this work, we leveraged large-scale, real-world data, i.e., statewide surveillance records and publicly available data resources encoding social determinants of health (SDoH), to identify social and racial disparities contributing to individuals' risk of HIV infection. We used the Florida Department of Health's Syndromic Tracking and Reporting System (STARS) database (including 100,000+ individuals screened for HIV infection and their partners), and a novel algorithmic fairness assessment method -the Fairness-Aware Causal paThs decompoSition (FACTS)- merging causal inference and artificial intelligence. FACTS deconstructs disparities based on SDoH and individuals' characteristics, and can discover novel mechanisms of inequity, quantifying to what extent they could be reduced by interventions. We paired the deidentified demographic information (age, gender, drug use) of 44,350 individuals in STARS -with non-missing data on interview year, county of residence, and infection status- to eight SDoH, including access to healthcare facilities, % uninsured, median household income, and violent crime rate. Using an expert-reviewed causal graph, we found that the risk of HIV infection for African Americans was higher than for non- African Americans (both in terms of direct and total effect), although a null effect could not be ruled out. FACTS identified several paths leading to racial disparity in HIV risk, including multiple SDoH: education, income, violent crime, drinking, smoking, and rurality.",
      "authors": "Prosperi Mattia; Xu Jie; Guo Jingchuan Serena; Bian Jiang; Chen Wei-Han William; Canidate Shantrel; Marini Simone; Wang Mo",
      "year": "2022",
      "journal": "Proceedings. IEEE International Conference on Bioinformatics and Biomedicine",
      "doi": "10.1109/bibm55620.2022.9995662",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36865610/",
      "mesh_terms": "",
      "keywords": "artificial intelligence; causal inference; disparity; epidemiology; human immunodeficiency virus; machine learning; real-world data; social determinants of health; surveillance",
      "pub_types": "Journal Article",
      "pmcid": "PMC9977319",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "40339458",
      "title": "Comparing the influence of social risk factors on machine learning model performance across racial and ethnic groups in home healthcare.",
      "abstract": "This study examined the impact of social risk factors on machine learning model performance for predicting hospitalization and emergency department visits in home healthcare. Using retrospective data from one U.S. home healthcare agency, four models were developed with unstructured social information documented in clinical notes. Performance was compared with and without social factors. A subgroup analyses was conducted by race and ethnicity to assess for fairness. LightGBM performed best overall. Social factors had a modest effect, but findings highlight the feasibility of integrating unstructured social information into machine learning models and the importance of fairness evaluation in home healthcare.",
      "authors": "Hobensack Mollie; Davoudi Anahita; Song Jiyoun; Cato Kenrick; Bowles Kathryn H; Topaz Maxim",
      "year": "2025",
      "journal": "Nursing outlook",
      "doi": "10.1016/j.outlook.2025.102431",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40339458/",
      "mesh_terms": "Humans; Machine Learning; Female; Male; Retrospective Studies; Ethnicity; Home Care Services; Middle Aged; Risk Factors; Aged; Racial Groups; United States; Hospitalization; Aged, 80 and over; Adult; Emergency Service, Hospital",
      "keywords": "Home healthcare; Hospitalization; Machine learning; Nursing informatics; Social determinants of health",
      "pub_types": "Journal Article; Comparative Study",
      "pmcid": "PMC12178806",
      "ft_status": "Excluded: Insufficient evidence (approach_count=1, bias_title=False)"
    },
    {
      "pmid": "36190605",
      "title": "Scheduling mobile dental clinics: A heuristic approach considering fairness among school districts.",
      "abstract": "Mobile dental clinics (MDCs) are suitable solutions for servicing people living in rural and urban areas that require dental healthcare. MDCs can provide dental care to the most vulnerable high-school students. However, scheduling MDCs to visit patients is critical to developing efficient dental programs. Here, we study a mobile dental clinic scheduling problem that arises from the real-life logistics management challenge faced by a school-based mobile dental care program in Southern Chile. This problem involves scheduling MDCs to treat high-school students at public schools while considering a fairness constraint among districts. Schools are circumscribed into districts, and by program regulations, at least 50% of the students in each district must receive dental care during the first semester. Fairness prevents some districts from waiting more time to receive dental care than others. We model the problem as a parallel machine scheduling problem with sequence-dependent setup costs and batch due dates and propose a mathematical model and a genetic algorithm-based solution to solve the problem. Our computational results demonstrate the effectiveness of our approaches in obtaining near-optimal solutions. Finally, dental program managers can use the methodologies presented in this work to schedule mobile dental clinics and improve their operations.",
      "authors": "Sep\u00falveda Ignacio A; Aguayo Maichel M; De la Fuente Rodrigo; Latorre-N\u00fa\u00f1ez Guillermo; Obreque Carlos; Orrego Camila V\u00e1squez",
      "year": "2024",
      "journal": "Health care management science",
      "doi": "10.1007/s10729-022-09612-5",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36190605/",
      "mesh_terms": "Humans; Dental Clinics; Heuristics; Delivery of Health Care; Students; Costs and Cost Analysis",
      "keywords": "Dental care; Health care management; Mobile dental clinic; Operations research; Scheduling",
      "pub_types": "Journal Article",
      "pmcid": "8184733",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "32322144",
      "title": "Gender and Parity in Statistical Prediction of Anterior Carry Hand-Loads from Inertial Sensor Data.",
      "abstract": "The objective of this study was to examine potential gender effects on the performance of a statistical algorithm for predicting hand-load levels that uses body-worn inertial sensor data. Torso and pelvic kinematic data was obtained from 11 men and 11 women in a laboratory experiment while they carried anterior hand-loads of 13.6 kg, and 22.7 kg, and during unloaded walking. Nine kinematic variables expressed as relative changes from unloaded gait were calculated and used as predictors in a statistical classification model predicting load-level (no-load, 13.6 kg, and 22.7 kg). To compare effects of gender on prediction accuracy, prediction models were built using both, gender-balanced gait data and gender-specific data (i.e., separate models for men and women) and evaluated using hold-out validation techniques. The gender-balanced model correctly classified load levels with an accuracy of 74.2% and 80.0% for men and women, respectively. The gender-specific models had accuracies of 68.3% and 85.0% for men and women, respectively. Findings indicated a lack of classification parity across gender, and possibly across other types of personal attributes such as age, ethnicity, and health condition. While preliminary, this study hopes to draw attention to challenges in algorithmic bias, parity and fairness, particularly as machine learning techniques gain popularity in ergonomics practice.",
      "authors": "Lim Sol; D'Souza Clive",
      "year": "2019",
      "journal": "Proceedings of the Human Factors and Ergonomics Society ... Annual Meeting. Human Factors and Ergonomics Society. Annual meeting",
      "doi": "10.1177/1071181319631193",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32322144/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC7176367",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "33989164",
      "title": "Authors' Reply to: Minimizing Selection and Classification Biases Comment on \"Clinical Characteristics and Prognostic Factors for Intensive Care Unit Admission of Patients With COVID-19: Retrospective Study Using Machine Learning and Natural Language Processing\".",
      "abstract": "",
      "authors": "Izquierdo Jose Luis; Soriano Joan B",
      "year": "2021",
      "journal": "Journal of medical Internet research",
      "doi": "10.2196/29405",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33989164/",
      "mesh_terms": "Bias; COVID-19; Humans; Intensive Care Units; Machine Learning; Natural Language Processing; Prognosis; Retrospective Studies; SARS-CoV-2",
      "keywords": "COVID-19; SARS-CoV-2; artificial intelligence; big data; classification bias; critical care; electronic health records; predictive model; prognosis; tachypnea",
      "pub_types": "Letter; Comment",
      "pmcid": "PMC8190644",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "33989163",
      "title": "Minimizing Selection and Classification Biases. Comment on \"Clinical Characteristics and Prognostic Factors for Intensive Care Unit Admission of Patients With COVID-19: Retrospective Study Using Machine Learning and Natural Language Processing\".",
      "abstract": "",
      "authors": "Martos P\u00e9rez Francisco; Gomez Huelgas Ricardo; Mart\u00edn Escalante Mar\u00eda Dolores; Casas Rojo Jos\u00e9 Manuel",
      "year": "2021",
      "journal": "Journal of medical Internet research",
      "doi": "10.2196/27142",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33989163/",
      "mesh_terms": "Bias; COVID-19; Humans; Intensive Care Units; Machine Learning; Natural Language Processing; Prognosis; Retrospective Studies; SARS-CoV-2",
      "keywords": "COVID-19; SARS-CoV-2; artificial intelligence; big data; classification bias; critical care; electronic health records; predictive model; prognosis; tachypnea",
      "pub_types": "Journal Article; Comment",
      "pmcid": "PMC8190647",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "38533919",
      "title": "Accurate, Robust, and Scalable Machine Abstraction of Mayo Endoscopic Subscores From Colonoscopy Reports.",
      "abstract": "BACKGROUND: The Mayo endoscopic subscore (MES) is an important quantitative measure of disease activity in ulcerative colitis. Colonoscopy reports in routine clinical care usually characterize ulcerative colitis disease activity using free text description, limiting their utility for clinical research and quality improvement. We sought to develop algorithms to classify colonoscopy reports according to their MES. METHODS: We annotated 500 colonoscopy reports from 2 health systems. We trained and evaluated 4 classes of algorithms. Our primary outcome was accuracy in identifying scorable reports (binary) and assigning an MES (ordinal). Secondary outcomes included learning efficiency, generalizability, and fairness. RESULTS: Automated machine learning models achieved 98% and 97% accuracy on the binary and ordinal prediction tasks, outperforming other models. Binary models trained on the University of California, San Francisco data alone maintained accuracy (96%) on validation data from Zuckerberg San Francisco General. When using 80% of the training data, models remained accurate for the binary task (97% [n = 320]) but lost accuracy on the ordinal task (67% [n = 194]). We found no evidence of bias by gender (P\u2005=\u2005.65) or area deprivation index (P\u2005=\u2005.80). CONCLUSIONS: We derived a highly accurate pair of models capable of classifying reports by their MES and recognizing when to abstain from prediction. Our models were generalizable on outside institution validation. There was no evidence of algorithmic bias. Our methods have the potential to enable retrospective studies of treatment effectiveness, prospective identification of patients meeting study criteria, and quality improvement efforts in inflammatory bowel diseases.",
      "authors": "Silverman Anna L; Bhasuran Balu; Mosenia Arman; Yasini Fatema; Ramasamy Gokul; Banerjee Imon; Gupta Saransh; Mardirossian Taline; Narain Rohan; Sewell Justin; Butte Atul J; Rudrapatna Vivek A",
      "year": "2025",
      "journal": "Inflammatory bowel diseases",
      "doi": "10.1093/ibd/izae068",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38533919/",
      "mesh_terms": "Humans; Colonoscopy; Female; Male; Machine Learning; Algorithms; Colitis, Ulcerative; Severity of Illness Index; Middle Aged; Adult",
      "keywords": "endoscopic disease activity scores; healthcare applied AI; natural language processing; ulcerative colitis",
      "pub_types": "Journal Article",
      "pmcid": "PMC11879245",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "39371143",
      "title": "Federated Multiple Imputation for Variables that Are Missing Not At Random in Distributed Electronic Health Records.",
      "abstract": "Large electronic health records (EHR) have been widely implemented and are available for research activities. The magnitude of such databases often requires storage and computing infrastructure that are distributed at different sites. Restrictions on data-sharing due to privacy concerns have been another driving force behind the development of a large class of distributed and/or federated machine learning methods. While missing data problem is also present in distributed EHRs, albeit potentially more complex, distributed multiple imputation (MI) methods have not received as much attention. An important advantage of distributed MI, as well as distributed analysis, is that it allows researchers to borrow information across data sites, mitigating potential fairness issues for minority groups that do not have enough volume at certain sites. In this paper, we propose a communication-efficient and privacy-preserving distributed MI algorithms for variables that are missing not at random.",
      "authors": "Lian Yi; Jiang Xiaoqian; Long Qi",
      "year": "2024",
      "journal": "medRxiv : the preprint server for health sciences",
      "doi": "10.1101/2024.09.15.24313479",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39371143/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article; Preprint",
      "pmcid": "PMC11451631",
      "ft_status": "Excluded: Insufficient evidence (approach_count=1, bias_title=False)"
    },
    {
      "pmid": "21906253",
      "title": "Rational rationing or discrimination: balancing equity and efficiency considerations in kidney allocation.",
      "abstract": "After 6 years of deliberation, the Organ Procurement and Transplantation Network recently released a concept document proposing changes to the kidney allocation algorithm, sparking a heated debate about priority-setting of scarce health resources and discrimination. Proponents of the proposal argue that it will result in an additional 15,223 life years following transplant annually for recipients, yet the benefit will not be equally distributed and will likely benefit younger patients. Critics argue that the new model will promote age discrimination and may lead to a further decrease in live kidney donation. If true, these concerns could undermine fairness and damage public trust in the organ allocation system. We address these objections and consider their merit, highlighting both benefits and shortcomings of the proposal. We argue that, despite weaknesses of the proposal and the importance of maintaining consistency in patient and provider expectations over time, the proposal represents a needed first step in balancing equity and efficiency.",
      "authors": "Ladin K; Hanto D W",
      "year": "2011",
      "journal": "American journal of transplantation : official journal of the American Society of Transplantation and the American Society of Transplant Surgeons",
      "doi": "10.1111/j.1600-6143.2011.03726.x",
      "url": "https://pubmed.ncbi.nlm.nih.gov/21906253/",
      "mesh_terms": "Age Factors; Health Care Rationing; Humans; Kidney Transplantation; Patient Selection; Resource Allocation; Tissue Donors; Tissue and Organ Procurement; Waiting Lists",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC3203330",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "41040729",
      "title": "Reducing Inequalities Using an Unbiased Machine Learning Approach to Identify Births with the Highest Risk of Preventable Neonatal Deaths.",
      "abstract": "BACKGROUND: Despite contemporaneous declines in neonatal mortality, recent studies show the existence of left-behind populations that continue to have higher mortality rates than the national averages. Additionally, many of these deaths are from preventable causes. This reality creates the need for more precise methods to identify high-risk births, allowing policymakers to target them more effectively. This study fills this gap by developing unbiased machine-learning approaches to more accurately identify births with a high risk of neonatal deaths from preventable causes. METHODS: We link administrative databases from the Brazilian health ministry to obtain birth and death records in the country from 2015 to 2017. The final dataset comprises 8,797,968 births, of which 59,615 newborns died before reaching 28 days alive (neonatal deaths). These neonatal deaths are categorized into preventable deaths (42,290) and non-preventable deaths (17,325). Our analysis identifies the death risk of the former group, as they are amenable to policy interventions. We train six machine-learning algorithms, test their performance on unseen data, and evaluate them using a new policy-oriented metric. To avoid biased policy recommendations, we also investigate how our approach impacts disadvantaged populations. RESULTS: XGBoost was the best-performing algorithm for our task, with the 5% of births identified as highest risk by the model accounting for over 85% of the observed deaths. Furthermore, the risk predictions exhibit no statistical differences in the proportion of actual preventable deaths from disadvantaged populations, defined by race, education, marital status, and maternal age. These results are similar for other threshold levels. CONCLUSIONS: We show that, by using publicly available administrative data sets and ML methods, it is possible to identify the births with the highest risk of preventable deaths with a high degree of accuracy. This is useful for policymakers as they can target health interventions to those who need them the most and where they can be effective without producing bias against disadvantaged populations. Overall, our approach can guide policymakers in reducing neonatal mortality rates and their health inequalities. Finally, it can be adapted for use in other developing countries.",
      "authors": "Ramos Antonio P; Caldieraro Fabio; Nascimento Marcus L; Saldanha Raphael",
      "year": "2025",
      "journal": "medRxiv : the preprint server for health sciences",
      "doi": "10.1101/2024.01.12.24301163",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41040729/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article; Preprint",
      "pmcid": "PMC12486022",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "40417515",
      "title": "Federated Multiple Imputation for Variables that Are Missing Not At Random in Distributed Electronic Health Records.",
      "abstract": "Large electronic health records (EHR) have been widely implemented and are available for research activities. The magnitude of such databases often requires storage and computing infrastructure that are distributed at different sites. Restrictions on data-sharing due to privacy concerns have been another driving force behind the development of a large class of distributed and/or federated machine learning methods. While missing data problem is also present in distributed EHRs, albeit potentially more complex, distributed multiple imputation (MI) methods have not received as much attention. An important advantage of distributed MI, as well as distributed analysis, is that it allows researchers to borrow information across data sites, mitigating potential fairness issues for minority groups that do not have enough volume at certain sites. In this paper, we propose a communication-efficient and privacy-preserving distributed MI algorithms for variables that are missing not at random.",
      "authors": "Lian Yi; Jiang Xiaoqian; Long Qi",
      "year": "2024",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "doi": "",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40417515/",
      "mesh_terms": "Electronic Health Records; Algorithms; Humans; Machine Learning; Confidentiality; Privacy",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC12099382",
      "ft_status": "Excluded: Insufficient evidence (approach_count=1, bias_title=False)"
    },
    {
      "pmid": "31360060",
      "title": "On the Bias of Precision Estimation Under Separate Sampling.",
      "abstract": "Observational case-control studies for biomarker discovery in cancer studies often collect data that are sampled separately from the case and control populations. We present an analysis of the bias in the estimation of the precision of classifiers designed on separately sampled data. The analysis consists of both theoretical and numerical results, which show that classifier precision estimates can display strong bias under separating sampling, with the bias magnitude depending on the difference between the true case prevalence in the population and the sample prevalence in the data. We show that this bias is systematic in the sense that it cannot be reduced by increasing sample size. If information about the true case prevalence is available from public health records, then a modified precision estimator that uses the known prevalence displays smaller bias, which can in fact be reduced to zero as sample size increases under regularity conditions on the classification algorithm. The accuracy of the theoretical analysis and the performance of the precision estimators under separate sampling are confirmed by numerical experiments using synthetic and real data from published observational case-control studies. The results with real data confirmed that under separately sampled data, the usual estimator produces larger, ie, more optimistic, precision estimates than the estimator using the true prevalence value.",
      "authors": "Xie Shuilian; Braga-Neto Ulisses M",
      "year": "2019",
      "journal": "Cancer informatics",
      "doi": "10.1177/1176935119860822",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31360060/",
      "mesh_terms": "",
      "keywords": "Precision; bias; classification; experimental design; observational study; recall",
      "pub_types": "Journal Article",
      "pmcid": "PMC6636226",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "41409534",
      "title": "Refined Algorithm for Identifying Recurrence Among Patients with Non-Metastatic Colorectal Cancer Based on Danish National Health Data Registries.",
      "abstract": "PURPOSE: In the Danish and other national health registries, colorectal cancer (CRC) recurrence is not routinely registered. Algorithms to label patients with recurrence in Denmark exist but produce cohorts with a risk of selection bias due to either pre- or postoperative exclusion criteria. In this study, we aimed to refine and increase the generalizability of an existing registry-based algorithm. PATIENTS AND METHODS: Data from 5077 patients from an institution and a regional database, encompassing several departments of surgery in Denmark, were retrieved. Patients with non-metastatic CRC were included from 2008 to 2019. Electronic health journal-based recurrence registration was used as reference for the algorithm. Patients were linked with data from the Danish Colorectal Cancer Group database, the Danish National Health Registry, the Danish Cancer Registry, and the Danish Pathology Registry. The algorithm utilized metastasis, chemotherapy, pathology, and local recurrence codes. Refinement of the algorithm included the addition of targeted and radiation therapy codes and including patients who died within 180 days after surgery, along with revising the pathology codes and removing any preoperative exclusion criteria. Performance metrics were evaluated in 10,000 bootstrapped runs, while all-stage and stage-specific cumulative incidence of recurrence and overall survival were estimated. RESULTS: The refined algorithm included more patients than the conventional algorithm (4388 vs 3684) and performed marginally better in terms of sensitivity (0.92 (95% CI 0.89-0.94) vs 0.90 (95% CI 0.87-0.92)) and specificity (0.97 (95% CI 0.97-0.98) vs 0.96 (95% CI 0.95-0.96). A significant difference in cumulative incidence of recurrence for UICC stage I was detected between the conventional algorithm and reference, which was not significant when using the refined algorithm. CONCLUSION: The refined algorithm improves identification of CRC recurrence in national data, enabling broader inclusion and better representation of population subgroups.",
      "authors": "G\u00f6genur Mikail; Br\u00e4uner Karoline Bendix; L\u00f6ffler Lea; Olsen Anna Sofie Friis; Gundestrup Anders Kierkegaard; Jakobsen Peter Cornelius Helbo; Kleif Jakob; Bertelsen Claus Anders; G\u00f6genur Ismail",
      "year": "2025",
      "journal": "Clinical epidemiology",
      "doi": "10.2147/CLEP.S532957",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41409534/",
      "mesh_terms": "",
      "keywords": "colorectal cancer; recurrence; registry-based algorithm",
      "pub_types": "Journal Article",
      "pmcid": "PMC12706160",
      "ft_status": "Excluded: Insufficient evidence (approach_count=1, bias_title=False)"
    },
    {
      "pmid": "34966957",
      "title": "Big Data/AI in Neurocritical Care: Maybe/Summary.",
      "abstract": "Big data (BD) and artificial intelligence (AI) have increasingly been used in neurocritical care. \"BD\" can be operationally defined as extremely large datasets that are so large and complex that they cannot be analyzed by using traditional statistical modeling. \"AI\" means the ability of machines to perform tasks similar to those performed by human intelligence. We present a brief overview of the most commonly applied AI techniques to perform BD analytics and discuss some of the recent promising examples in the field of neurocritical care. The latter include the following: cognitive motor dissociation in disorders of consciousness, hypoxic-ischemic injury following cardiac arrest, delayed cerebral ischemia and vasospasm after subarachnoid hemorrhage, and monitoring of intracranial pressure. It is imperative that we develop multicenter collaborations to tackle BD. These collaborations will allow us to share data, combine predictive algorithms, and analyze multiple and cumulative sources of data retrospectively and prospectively. Once AI algorithms are validated at multiple centers, they should be tested in randomized controlled trials investigating their impact on clinical outcome. The neurocritical care community must work to ensure that AI incorporates standards to ensure fairness and health equity rather than reflect our biases present in our collective conscience.",
      "authors": "Suarez Jose I",
      "year": "2022",
      "journal": "Neurocritical care",
      "doi": "10.1007/s12028-021-01422-x",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34966957/",
      "mesh_terms": "Algorithms; Artificial Intelligence; Big Data; Humans; Machine Learning; Retrospective Studies",
      "keywords": "Artificial intelligence; Big data; Deep learning; Machine learning; Precision medicine",
      "pub_types": "Journal Article; Multicenter Study",
      "pmcid": "3897241",
      "ft_status": "Excluded: No AI/ML + health in full text"
    },
    {
      "pmid": "27694664",
      "title": "HTA - Algorithm or Process? Comment on \"Expanded HTA: Enhancing Fairness and Legitimacy\".",
      "abstract": "Daniels, Porteny and Urrutia et al make a good case for the idea that that public decisions ought to be made not only \"in the light of\" evidence but also \"on the basis of\" budget impact, financial protection and equity. Health technology assessment (HTA) should, they say, be accordingly expanded to consider matters additional to safety and cost-effectiveness. They also complain that most HTA reports fail to develop ethical arguments and generally do not even mention ethical issues. This comment argues that some of these defects are more apparent than real and are not inherent in HTA - as distinct from being common characteristics found in poorly conducted HTAs. More generally, HTA does not need \"extension\" since (1) ethical issues are already embedded in HTA processes, not least in their scoping phases, and (2) HTA processes are already sufficiently flexible to accommodate evidence about a wide range of factors, and will not need fundamental change in order to accommodate the new forms of decision-relevant evidence about distributional impact and financial protection that are now starting to emerge. HTA and related techniques are there to support decision-makers who have authority to make decisions. Analysts like us are there to support and advise them (and not to assume the responsibilities for which they, and not we, are accountable). The required quality in HTA then becomes its effectiveness as a means of addressing the issues of concern to decision-makers. What is also required is adherence by competent analysts to a standard template of good analytical practice. The competencies include not merely those of the usual disciplines (particularly biostatistics, cognitive psychology, health economics, epidemiology, and ethics) but also the imaginative and interpersonal skills for exploring the \"real\" question behind the decision-maker's brief (actual or postulated) and eliciting the social values that necessarily pervade the entire analysis. The product of such exploration defines the authoritative scope of an HTA.",
      "authors": "Culyer Anthony J",
      "year": "2016",
      "journal": "International journal of health policy and management",
      "doi": "10.15171/ijhpm.2016.59",
      "url": "https://pubmed.ncbi.nlm.nih.gov/27694664/",
      "mesh_terms": "Algorithms; Biomedical Technology; Cost-Benefit Analysis; Decision Making; Decision Support Techniques; Health Policy; Humans; Morals; Social Responsibility; Technology Assessment, Biomedical",
      "keywords": "Deliberation; Economic Evaluation; Equity; Extended Cost-Effectiveness; HTA Processes; Quality-Adjusted Life-Year (QALY) Algorithm; Reference Case",
      "pub_types": "Journal Article; Comment",
      "pmcid": "PMC4968254",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "32935131",
      "title": "Reporting of demographic data and representativeness in machine learning models using electronic health records.",
      "abstract": "OBJECTIVE: The development of machine learning (ML) algorithms to address a variety of issues faced in clinical practice has increased rapidly. However, questions have arisen regarding biases in their development that can affect their applicability in specific populations. We sought to evaluate whether studies developing ML models from electronic health record (EHR) data report sufficient demographic data on the study populations to demonstrate representativeness and reproducibility. MATERIALS AND METHODS: We searched PubMed for articles applying ML models to improve clinical decision-making using EHR data. We limited our search to papers published between 2015 and 2019. RESULTS: Across the 164 studies reviewed, demographic variables were inconsistently reported and/or included as model inputs. Race/ethnicity was not reported in 64%; gender and age were not reported in 24% and 21% of studies, respectively. Socioeconomic status of the population was not reported in 92% of studies. Studies that mentioned these variables often did not report if they were included as model inputs. Few models (12%) were validated using external populations. Few studies (17%) open-sourced their code. Populations in the ML studies include higher proportions of White and Black yet fewer Hispanic subjects compared to the general US population. DISCUSSION: The demographic characteristics of study populations are poorly reported in the ML literature based on EHR data. Demographic representativeness in training data and model transparency is necessary to ensure that ML models are deployed in an equitable and reproducible manner. Wider adoption of reporting guidelines is warranted to improve representativeness and reproducibility.",
      "authors": "Bozkurt Selen; Cahan Eli M; Seneviratne Martin G; Sun Ran; Lossio-Ventura Juan A; Ioannidis John P A; Hernandez-Boussard Tina",
      "year": "2020",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "doi": "10.1093/jamia/ocaa164",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32935131/",
      "mesh_terms": "Demography; Electronic Health Records; Ethnicity; Female; Humans; Machine Learning; Male; Nutrition Surveys; Socioeconomic Factors",
      "keywords": "clinical decision support, bias, transparency; demographic data; electronic health record; machine learning",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC7727384",
      "ft_status": "Excluded: Insufficient evidence (approach_count=1, bias_title=False)"
    },
    {
      "pmid": "40099281",
      "title": "In-Hospital Mortality Prediction among Intensive Care Unit Patients with Acute Ischemic Stroke: A Machine Learning Approach.",
      "abstract": "Background: Acute ischemic stroke is a leading cause of death in the United States. Identifying patients with stroke at high risk of mortality is crucial for timely intervention and optimal resource allocation. This study aims to develop and validate machine learning-based models to predict in-hospital mortality risk for intensive care unit (ICU) patients with acute ischemic stroke and identify important associated factors. Methods: Our data include 3,489 acute ischemic stroke admissions to the ICU for patients not discharged or dead within 48 h from the Medical Information Mart for Intensive Care-IV (MIMIC-IV) database. Demographic, hospitalization type, procedure, medication, intake (intravenous and oral), laboratory, vital signs, and clinical assessment [e.g., Glasgow Coma Scale Scores (GCS)] during the initial 48 h of admissions were used to predict in-hospital mortality after 48 h of ICU admission. We explored 3 machine learning models (random forests, logistic regression, and XGBoost) and applied Bayesian optimization for hyperparameter tuning. Important features were identified using learned coefficients. Results: Experiments show that XGBoost tuned for area under the receiver operating characteristic curve (AUC ROC) was the best performing model (AUC ROC 0.86, F1 0.52), compared to random forests (AUC ROC 0.85, F1 0.47) and logistic regression (AUC ROC 0.75, F1 0.40). Top features include GCS, blood urea nitrogen, and Richmond RASS score. The model also demonstrates good fairness for males versus females and across racial/ethnic groups. Conclusions: Machine learning has shown great potential in predicting in-hospital mortality risk for people with acute ischemic stroke in the ICU setting. However, more ethical considerations need to be applied to ensure that performance differences across different racial/ethnic groups will not exacerbate existing health disparities and will not harm historically marginalized populations.",
      "authors": "Cummins Jack A; Gerber Ben S; Fukunaga Mayuko Ito; Henninger Nils; Kiefe Catarina I; Liu Feifan",
      "year": "2025",
      "journal": "Health data science",
      "doi": "10.34133/hds.0179",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40099281/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC11912875",
      "ft_status": "Excluded: Insufficient evidence (approach_count=2, bias_title=False)"
    },
    {
      "pmid": "41397987",
      "title": "Development and validation of a simplified time-dependent interpretable machine learning-based survival model for older adults with multimorbidity.",
      "abstract": "Multimorbidity elevates late-life mortality, yet existing tools remain complex. Using two nationally representative Chinese cohorts-the Chinese Longitudinal Healthy Longevity and Happiness Family Study (CLHLS-HF; n\u2009=\u20098675) and the China Health and Retirement Longitudinal Study (CHARLS, n\u2009=\u20094171)-we developed and externally validated a simplified, time-dependent, interpretable survival model. A four-stage feature-selection pipeline (univariate Cox, L1-penalized Cox, multi-model importance with 100 bootstraps, and cumulative performance) identified four routinely available predictors: age, BMI, and cooking and toileting abilities. Among five algorithms, a parsimonious Cox model performed best (C-index 0.7524 internal; 0.7104 external) with a favorable time-Brier Score (0.1417; 0.1157), good calibration, decision-curve net benefit, and subgroup fairness. Time-dependent permutation importance confirmed age as dominant, toileting ability as short-term, and cooking ability as mid- to long-term contributors, while BMI showed modest, stable effects. Implemented as the M-SAGE online tool, this four-item model enables rapid, interpretable mortality risk stratification and supports individualized interventions for older adults with multimorbidity.",
      "authors": "Zhu Junmin; Chen Huanglong; Duan Siyu; Wu Yafei; Fang Ya",
      "year": "2025",
      "journal": "npj aging",
      "doi": "10.1038/s41514-025-00308-y",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41397987/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC12816092",
      "ft_status": "Excluded: Insufficient evidence (approach_count=1, bias_title=False)"
    },
    {
      "pmid": "36101652",
      "title": "Validity of a Computational Linguistics-Derived Automated Health Literacy Measure Across Race/Ethnicity: Findings from The ECLIPPSE Project.",
      "abstract": "Limited health literacy (HL) partially mediates health disparities. Measurement constraints, including lack of validity assessment across racial/ethnic groups and administration challenges, have undermined the field and impeded scaling of HL interventions. We employed computational linguistics to develop an automated and novel HL measure, analyzing >300,000 messages sent by >9,000 diabetes patients via a patient portal to create a Literacy Profiles. We carried out stratified analyses among White/non-Hispanics, Black/non-Hispanics, Hispanics, and Asian/Pacific Islanders to determine if the Literacy Profile has comparable criterion and predictive validities. We discovered that criterion validity was consistently high across all groups (c-statistics 0.82-0.89). We observed consistent relationships across racial/ethnic groups between HL and outcomes, including communication, adherence, hypoglycemia, diabetes control, and ED utilization. While concerns have arisen regarding bias in AI, the automated Literacy Profile appears sufficiently valid across race/ethnicity, enabling HL measurement at a scale that could improve clinical care and population health among diverse populations.",
      "authors": "Schillinger Dean; Balyan Renu; Crossley Scott; McNamara Danielle; Karter Andrew",
      "year": "2021",
      "journal": "Journal of health care for the poor and underserved",
      "doi": "10.1353/hpu.2021.0067",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36101652/",
      "mesh_terms": "Diabetes Mellitus; Ethnicity; Health Literacy; Humans; Linguistics; Racial Groups",
      "keywords": "Health literacy; artificial intelligence; communication; computational linguistics; diabetes; health disparities; machine learning; validation study",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC9467454",
      "ft_status": "Excluded: Insufficient evidence (approach_count=1, bias_title=False)"
    },
    {
      "pmid": "37566454",
      "title": "Ethical Considerations of Using ChatGPT in Health Care.",
      "abstract": "ChatGPT has promising applications in health care, but potential ethical issues need to be addressed proactively to prevent harm. ChatGPT presents potential ethical challenges from legal, humanistic, algorithmic, and informational perspectives. Legal ethics concerns arise from the unclear allocation of responsibility when patient harm occurs and from potential breaches of patient privacy due to data collection. Clear rules and legal boundaries are needed to properly allocate liability and protect users. Humanistic ethics concerns arise from the potential disruption of the physician-patient relationship, humanistic care, and issues of integrity. Overreliance on artificial intelligence (AI) can undermine compassion and erode trust. Transparency and disclosure of AI-generated content are critical to maintaining integrity. Algorithmic ethics raise concerns about algorithmic bias, responsibility, transparency and explainability, as well as validation and evaluation. Information ethics include data bias, validity, and effectiveness. Biased training data can lead to biased output, and overreliance on ChatGPT can reduce patient adherence and encourage self-diagnosis. Ensuring the accuracy, reliability, and validity of ChatGPT-generated content requires rigorous validation and ongoing updates based on clinical practice. To navigate the evolving ethical landscape of AI, AI in health care must adhere to the strictest ethical standards. Through comprehensive ethical guidelines, health care professionals can ensure the responsible use of ChatGPT, promote accurate and reliable information exchange, protect patient privacy, and empower patients to make informed decisions about their health care.",
      "authors": "Wang Changyu; Liu Siru; Yang Hao; Guo Jiulin; Wu Yuxuan; Liu Jialin",
      "year": "2023",
      "journal": "Journal of medical Internet research",
      "doi": "10.2196/48009",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37566454/",
      "mesh_terms": "Humans; Artificial Intelligence; Reproducibility of Results; Data Collection; Disclosure; Patient Compliance",
      "keywords": "AI; ChatGPT; algorithm; artificial intelligence; artificial intelligence development; development; ethics; health care; large language models; patient privacy; patient safety; privacy; safety",
      "pub_types": "Journal Article",
      "pmcid": "PMC10457697",
      "ft_status": "Excluded: Insufficient evidence (approach_count=2, bias_title=False)"
    },
    {
      "pmid": "38501988",
      "title": "Identification of group differences in predictive anticipatory biasing of pain during uncertainty: preparing for the worst but hoping for the best.",
      "abstract": "Pain anticipation during conditions of uncertainty can unveil intrinsic biases, and understanding these biases can guide pain treatment interventions. This study used machine learning and functional magnetic resonance imaging to predict anticipatory responses in a pain anticipation experiment. One hundred forty-seven participants that included healthy controls (n = 57) and individuals with current and/or past mental health diagnosis (n = 90) received cues indicating upcoming pain stimuli: 2 cues predicted high and low temperatures, while a third cue introduced uncertainty. Accurate differentiation of neural patterns associated with specific anticipatory conditions was observed, involving activation in the anterior short gyrus of the insula and the nucleus accumbens. Three distinct response profiles emerged: subjects with a negative bias towards high pain anticipation, those with a positive bias towards low pain anticipation, and individuals whose predictions during uncertainty were unbiased. These profiles remained stable over one year, were consistent across diagnosed psychopathologies, and correlated with cognitive coping styles and underlying insula anatomy. The findings suggest that individualized and stable pain anticipation occurs in uncertain conditions.",
      "authors": "Strigo Irina A; Kadlec Molly; Mitchell Jennifer M; Simmons Alan N",
      "year": "2024",
      "journal": "Pain",
      "doi": "10.1097/j.pain.0000000000003207",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38501988/",
      "mesh_terms": "Humans; Uncertainty; Male; Female; Adult; Magnetic Resonance Imaging; Anticipation, Psychological; Pain; Young Adult; Middle Aged; Machine Learning; Brain Mapping; Cues; Pain Measurement",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC11247452",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "40895087",
      "title": "Improving the FAIRness and Sustainability of the NHGRI Resources Ecosystem.",
      "abstract": "In 2024, individuals funded by NHGRI to support genomic community resources completed a Self-Assessment Tool (SAT) to evaluate their application of the FAIR (Findable, Accessible, Interoperable, and Reusable) principles and assess their sustainability. By collecting insights from the self-administered questionnaires and conducting personal interviews, a valuable perspective was gained on the FAIRness and sustainability of the NHGRI resources. The results highlighted several challenges and key areas the NHGRI resource community could improve by working together to form recommendations to address these challenges. The next step was the formation of an Organizing Committee to identify which challenges could lead to best practices or guidelines for the community. The workshop's Organizing Committee comprised four members from the NHGRI resource community: Carol Bult, PhD, Chris Mungall, PhD, Heidi Rehm, PhD, and Michael Schatz, PhD. In December 2024, the Organizing Committee engaged with the NHGRI resource community to refine these challenges further, inviting feedback on potential focus areas for a future workshop. This collaborative approach led to two informative webinars in December 2024, highlighting specific challenges in data curation, data processing, metadata tools, and variant identifiers within the NHGRI resources. Throughout the workshop planning process, the four Organizing Committee members worked together to create and develop themes, design breakout sessions, and create a detailed agenda. The workshop's agenda was intentionally structured to ensure participants could generate implementable recommendations for the NHGRI resource community. The two-day workshop was held in Bethesda, MD, on March 3-4, 2025. The challenges received from NHGRI resources were classified into four key categories, forming the basis of the workshop. The four key categories are variant identifiers, data processing, data curation, and metadata tools. They are briefly described below, with greater details on their challenges and recommendations in subsequent sections. Metadata Tools:While metadata is vital for capturing context in genomic datasets, its usage and relevance can vary by domain, making it difficult to standardize usage. While various methods exist for annotating and extracting metadata, incomplete or inconsistent annotations often result in ineffective data sharing and interoperability, further reducing data usability and reproducibility.Data Curation:Curation of annotations for genomics data is critical for FAIR-ness. Scalable curation solutions are challenging because of the multiple components for curation, including harmonizing data sets, data cleaning, and annotation. The workshop focused on identifying which aspects of data curation could be streamlined using computational methods while considering the barriers to increased automation.Variant Identifiers:Variant identifiers are standardized representations of genetic variants, crucial for sharing and interpreting genomic data in research and clinical work. They ensure consistent referencing and enable data aggregation. Standardizing variant identifiers is difficult due to varied formats, complex data, and distinct environments for generating and disseminating data.Data Processing:Data processing is a necessary first step in a FAIR environment. As there are many variant workflows, streamlining this process will ensure greater accuracy, reproducibility, interoperability, and FAIRness, driving advancements in clinical research. The workshop focused on addressing these aspects with a key focus on improvements and best practices around data processing for an NHGRI resource. Several recommendations were made throughout the workshop's interactive sessions with the resources' participants. While many recommendations were specific to data processing, data curation, metadata tools, or variant identifiers, they can be grouped into core recommendations addressing common challenges within the NHGRI resource community. These core recommendations highlight the key themes that emerged across sessions and are listed in the nine recommendations below. Increase transparency to enable effective sharing/reproducibility (documenting, benchmarking, publishing, mapping)Develop entity schema and ontology mapping tools (between models, identifiers, etc.)Annotate tools using resources to increase findability and reuse (Examples: EDAM Ontology of Bioscientific data analysis and data management)Use standard nomenclature and identifiersMake workflows usable by researchers with limited programming expertiseImplement APIs to improve data connectivityPresent data in an interpretable manner, along with machine readabilityDevelop artificial intelligence/machine learning (AI/ML) methods for scaling curation processesAssess the impact of resources using an independent group that can assess return on investment and impact to health and scientific advancement. An additional key collaborative outcome was the development of Appendix A, which outlines ongoing and future efforts, including additional workshops, webinars, and meetings through the listed events provided by the NHGRI resource community. We hope that these activities will enable further advances in the implementation of FAIR standards and continue to foster collaboration and exchange across NHGRI resources and the global community.",
      "authors": "Babb Larry; Bult Carol; Carey Vincent J; Carroll Robert J; Hitz Benjamin C; Mungall Chris J; Rehm Heidi L; Schatz Michael C; Wagner Alex",
      "year": "2025",
      "journal": "ArXiv",
      "doi": "",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40895087/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article; Preprint",
      "pmcid": "PMC12393232",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "33746859",
      "title": "A Pictorial Dot Probe Task to Assess Food-Related Attentional Bias in Youth With and Without Obesity: Overview of Indices and Evaluation of Their Reliability.",
      "abstract": "Several versions of the dot probe detection task are frequently used to assess maladaptive attentional processes associated with a broad range of psychopathology and health behavior, including eating behavior and weight. However, there are serious concerns about the reliability of the indices derived from the paradigm as measurement of attentional bias toward or away from salient stimuli. The present paper gives an overview of different attentional bias indices used in psychopathology research and scrutinizes three types of indices (the traditional attentional bias score, the dynamic trial-level base scores, and the probability index) calculated from a pictorial version of the dot probe task to assess food-related attentional biases in children and youngsters with and without obesity. Correlational analyses reveal that dynamic scores (but not the traditional and probability indices) are dependent on general response speed. Reliability estimates are low for the traditional and probability indices. The higher reliability for the dynamic indices is at least partially explained by general response speed. No significant group differences between youth with and without obesity are found, and correlations with weight are also non-significant. Taken together, results cast doubt on the applicability of this specific task for both experimental and individual differences research on food-related attentional biases in youth. However, researchers are encouraged to make and test adaptations to the procedure or computational algorithm in an effort to increase psychometric quality of the task and to report psychometric characteristics of their version of the task for their specific sample.",
      "authors": "Vervoort Leentje; Braun Maya; De Schryver Maarten; Naets Tiffany; Koster Ernst H W; Braet Caroline",
      "year": "2021",
      "journal": "Frontiers in psychology",
      "doi": "10.3389/fpsyg.2021.644512",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33746859/",
      "mesh_terms": "",
      "keywords": "attentional bias; children and adolescent; dot probe paradigm; obesity; reliability",
      "pub_types": "Journal Article",
      "pmcid": "PMC7965983",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "41040699",
      "title": "Privacy-Enhancing Sequential Learning under Heterogeneous Selection Bias in Multi-Site EHR Data.",
      "abstract": "OBJECTIVE: To develop privacy-enhancing statistical methods for estimation of binary disease risk model association parameters across multiple electronic health record (EHR) sites with heterogeneous selection mechanisms, without sharing raw individual-level data. We illustrate their utility through a cross-biobank analysis of smoking and 97 cancer subtypes using data from the NIH All of Us (AOU) and the Michigan Genomics Initiative (MGI). MATERIALS AND METHODS: Large-scale biobanks often follow heterogeneous recruitment strategies and store data in separate cloud-based platforms, making centralized algorithms infeasible. To address this, we propose two decentralized sequential estimators namely, Sequential Pseudo-likelihood (SPL) and Sequential Augmented Inverse Probability Weighting (SAIPW) that leverage external population-level information to adjust for selection bias, with valid variance estimation. SAIPW additionally protects against misspecification of the selection model using flexible machine learning based auxiliary outcome models. We compare SPL and SAIPW with the existing Sequential Unweighted (SUW) estimator and with centralized and meta learning extensions of IPW and AIPW in simulations under both correctly specified and misspecified selection mechanisms. We apply the methods to harmonized data from MGI ( n = 50,935) and AOU ( n = 241,563) to estimate smoking-cancer associations. RESULTS: In simulations, SUW exhibited substantial bias and poor coverage. SPL and SAIPW yielded unbiased estimates with valid coverage probabilities under correct model specification, with SAIPW remaining robust under selection model misspecification. Both approaches showed no notable efficiency loss relative to centralized methods. Meta-learning methods were efficient for large sites but failed in settings with small cohort sizes and rare outcome prevalence. In real-data analysis, strong associations were consistently identified between smoking and cancers of the lung, bladder, and larynx, aligning with established epidemiological evidence. CONCLUSION: Our framework enables valid, privacy-enhancing inference across EHR cohorts with heterogeneous selection, supporting scalable, decentralized research using real-world data.",
      "authors": "Kundu Ritoban; Shi Xu; Patel Kumar Kshitij; Ohno-Machado Lucila; Salvatore Maxwell; Song Peter X K; Mukherjee Bhramar",
      "year": "2025",
      "journal": "medRxiv : the preprint server for health sciences",
      "doi": "10.1101/2025.09.26.25336642",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41040699/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article; Preprint",
      "pmcid": "PMC12486029",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "39018490",
      "title": "Applying natural language processing to patient messages to identify depression concerns in cancer patients.",
      "abstract": "OBJECTIVE: This study aims to explore and develop tools for early identification of depression concerns among cancer patients by leveraging the novel data source of messages sent through a secure patient portal. MATERIALS AND METHODS: We developed classifiers based on logistic regression (LR), support vector machines (SVMs), and 2 Bidirectional Encoder Representations from Transformers (BERT) models (original and Reddit-pretrained) on 6600 patient messages from a cancer center (2009-2022), annotated by a panel of healthcare professionals. Performance was compared using AUROC scores, and model fairness and explainability were examined. We also examined correlations between model predictions and depression diagnosis and treatment. RESULTS: BERT and RedditBERT attained AUROC scores of 0.88 and 0.86, respectively, compared to 0.79 for LR and 0.83 for SVM. BERT showed bigger differences in performance across sex, race, and ethnicity than RedditBERT. Patients who sent messages classified as concerning had a higher chance of receiving a depression diagnosis, a prescription for antidepressants, or a referral to the psycho-oncologist. Explanations from BERT and RedditBERT differed, with no clear preference from annotators. DISCUSSION: We show the potential of BERT and RedditBERT in identifying depression concerns in messages from cancer patients. Performance disparities across demographic groups highlight the need for careful consideration of potential biases. Further research is needed to address biases, evaluate real-world impacts, and ensure responsible integration into clinical settings. CONCLUSION: This work represents a significant methodological advancement in the early identification of depression concerns among cancer patients. Our work contributes to a route to reduce clinical burden while enhancing overall patient care, leveraging BERT-based models.",
      "authors": "van Buchem Marieke M; de Hond Anne A H; Fanconi Claudio; Shah Vaibhavi; Schuessler Max; Kant Ilse M J; Steyerberg Ewout W; Hernandez-Boussard Tina",
      "year": "2024",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "doi": "10.1093/jamia/ocae188",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39018490/",
      "mesh_terms": "Humans; Natural Language Processing; Neoplasms; Depression; Male; Female; Support Vector Machine; Logistic Models; Patient Portals; Middle Aged; Adult",
      "keywords": "machine learning; mental health; natural language processing; oncology",
      "pub_types": "Journal Article",
      "pmcid": "PMC11413442",
      "ft_status": "Excluded: Insufficient evidence (approach_count=1, bias_title=False)"
    },
    {
      "pmid": "38961161",
      "title": "Early detection of pediatric health risks using maternal and child health data.",
      "abstract": "Machine learning (ML)-driven diagnosis systems are particularly relevant in pediatrics given the well-documented impact of early-life health conditions on later-life outcomes. Yet, early identification of diseases and their subsequent impact on length of hospital stay for this age group has so far remained uncharacterized, likely because access to relevant health data is severely limited. Thanks to a confidential data use agreement with the California Department of Health Care Access and Information, we introduce Ped-BERT: a state-of-the-art deep learning model that accurately predicts the likelihood of 100+ conditions and the length of stay in a pediatric patient's next medical visit. We link mother-specific pre- and postnatal period health information to pediatric patient hospital discharge and emergency room visits. Our data set comprises 513.9K mother-baby pairs and contains medical diagnosis codes, length of stay, as well as temporal and spatial pediatric patient characteristics, such as age and residency zip code at the time of visit. Following the popular bidirectional encoder representations from the transformers (BERT) approach, we pre-train Ped-BERT via the masked language modeling objective to learn embedding features for the diagnosis codes contained in our data. We then continue to fine-tune our model to accurately predict primary diagnosis outcomes and length of stay for a pediatric patient's next visit, given the history of previous visits and, optionally, the mother's pre- and postnatal health information. We find that Ped-BERT generally outperforms contemporary and state-of-the-art classifiers when trained with minimum features. We also find that incorporating mother health attributes leads to significant improvements in model performance overall and across all patient subgroups in our data. Our most successful Ped-BERT model configuration achieves an area under the receiver operator curve (ROC AUC) of 0.927 and an average precision score (APS) of 0.408 for the diagnosis prediction task, and a ROC AUC of 0.855 and APS of 0.815 for the length of hospital stay task. Further, we examine Ped-BERT's fairness by determining whether prediction errors are evenly distributed across various subgroups of mother-baby demographics and health characteristics, or if certain subgroups exhibit a higher susceptibility to prediction errors.",
      "authors": "Ilin Cornelia",
      "year": "2024",
      "journal": "Scientific reports",
      "doi": "10.1038/s41598-024-65449-8",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38961161/",
      "mesh_terms": "Humans; Female; Child Health; Infant; Child, Preschool; Maternal Health; Child; Early Diagnosis; Length of Stay; Infant, Newborn; Male; Deep Learning; Machine Learning",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC11222373",
      "ft_status": "Excluded: Insufficient evidence (approach_count=2, bias_title=False)"
    },
    {
      "pmid": "41308403",
      "title": "Patient-reported postoperative pain and stigmatizing language in anesthesia notes: a cross-sectional study (2017-2019).",
      "abstract": "BACKGROUND: Stigmatizing language reflects provider bias. Researchers found that documentation of stigmatizing language in obstetric clinical notes differed by patient race and ethnicity. The purpose of this study was to examine associations between postoperative pain and stigmatizing language documented by anesthesiologists. METHODS: We studied the electronic health records of obstetric patients at two hospitals between 2017 and 2019 (n\u00a0=\u00a04383). Pain was defined as a verbal numerical pain score (VNPS) \u2265 1 following cesarean delivery or other operative procedure during the delivery hospitalization. Stigmatizing language was identified in the free-text narratives of postoperative anesthesia notes using a well-performing natural language processing algorithm. Multivariable logistic regression was employed to examine associations between pain and stigmatizing language. RESULTS: Stigmatizing language was found in 9.9% of postoperative notes. Patients with documented pain were significantly more likely to have any stigmatizing language documented by anesthesiologists compared with patients with no pain (adjusted odds ratio [aOR], 1.64; 95% confidence interval [CI], 1.26-2.13). Patients with pain were also significantly more likely to have language labeling them as 'difficult' (aOR, 1.81; 95% CI, 1.34-2.45). There were no significant differences between patients with and without postoperative pain in language related to marginalized language/identities, unilateral/authoritarian decisions, or questioning patient credibility categories. CONCLUSIONS: In this cross-sectional study, postpartum patients with pain had increased odds of stigmatizing language. Findings suggest anesthesiologists may perceive patients who report pain as being 'difficult.' Quality improvement studies should track inequities in pain management as patient-centered, bias-free care is crucial for improving perinatal equity.",
      "authors": "Harkins S E; Thomas C D; Hulchafo I I; Topaz M; Landau R; Barcelona V",
      "year": "2025",
      "journal": "International journal of obstetric anesthesia",
      "doi": "10.1016/j.ijoa.2025.104824",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41308403/",
      "mesh_terms": "",
      "keywords": "Clinician bias; Health equity; Natural language processing; Postpartum pain; Stigmatizing language",
      "pub_types": "Journal Article",
      "pmcid": "PMC12771282",
      "ft_status": "Excluded: Insufficient evidence (approach_count=1, bias_title=False)"
    },
    {
      "pmid": "27460603",
      "title": "State-level estimates of childhood obesity prevalence in the United States corrected for report bias.",
      "abstract": "BACKGROUND/OBJECTIVES: State-specific obesity prevalence data are critical to public health efforts to address the childhood obesity epidemic. However, few states administer objectively measured body mass index (BMI) surveillance programs. This study reports state-specific childhood obesity prevalence by age and sex correcting for parent-reported child height and weight bias. SUBJECTS/METHODS: As part of the Childhood Obesity Intervention Cost Effectiveness Study (CHOICES), we developed childhood obesity prevalence estimates for states for the period 2005-2010 using data from the 2010 US Census and American Community Survey (ACS), 2003-2004 and 2007-2008 National Survey of Children's Health (NSCH) (n=133\u2009213), and 2005-2010 National Health and Nutrition Examination Surveys (NHANES) (n=9377; ages 2-17). Measured height and weight data from NHANES were used to correct parent-report bias in NSCH using a non-parametric statistical matching algorithm. Model estimates were validated against surveillance data from five states (AR, FL, MA, PA and TN) that conduct censuses of children across a range of grades. RESULTS: Parent-reported height and weight resulted in the largest overestimation of childhood obesity in males ages 2-5 years (NSCH: 42.36% vs NHANES: 11.44%). The CHOICES model estimates for this group (12.81%) and for all age and sex categories were not statistically different from NHANES. Our modeled obesity prevalence aligned closely with measured data from five validation states, with a 0.64 percentage point mean difference (range: 0.23-1.39) and a high correlation coefficient (r=0.96, P=0.009). Estimated state-specific childhood obesity prevalence ranged from 11.0 to 20.4%. CONCLUSION: Uncorrected estimates of childhood obesity prevalence from NSCH vary widely from measured national data, from a 278% overestimate among males aged 2-5 years to a 44% underestimate among females aged 14-17 years. This study demonstrates the validity of the CHOICES matching methods to correct the bias of parent-reported BMI data and highlights the need for public release of more recent data from the 2011 to 2012 NSCH.",
      "authors": "Long M W; Ward Z J; Resch S C; Cradock A L; Wang Y C; Giles C M; Gortmaker S L",
      "year": "2016",
      "journal": "International journal of obesity (2005)",
      "doi": "10.1038/ijo.2016.130",
      "url": "https://pubmed.ncbi.nlm.nih.gov/27460603/",
      "mesh_terms": "Adolescent; Body Mass Index; Child; Child, Preschool; Female; Humans; Male; Nutrition Surveys; Parents; Pediatric Obesity; Policy Making; Prevalence; Public Health; Public Health Surveillance; Self Report; United States",
      "keywords": "",
      "pub_types": "Journal Article; Validation Study",
      "pmcid": "PMC8966206",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "24049156",
      "title": "Disability and chronic disease among older adults in India: detecting vulnerable populations through the WHO SAGE Study.",
      "abstract": "Chronic noncommunicable diseases (NCDs) are now prevalent in many low- and middle-income countries and confer a heightened risk of disability. It is unclear how public health programs can identify the older adults at highest risk of disability related to NCDs within diverse developing country populations. We studied nationally representative survey data from 7,150 Indian adults older than 50 years of age who participated in the World Health Organization Study on Global Aging and Adult Health (2007-2010) to identify population subgroups who are highly disabled. Using machine-learning algorithms, we identified sociodemographic correlates of disability. Although having 2 or more symptomatic NCDs was a key correlate of disability, the prevalence of symptomatic, undiagnosed NCDs was highest among the lowest 2 wealth quintiles of Indian adults, contrary to prior hypotheses of increased NCDs with wealth. Women and persons from rural populations were also disproportionately affected by nondiagnosed NCDs, with high out-of-pocket health care expenditures increasing the probability of remaining symptomatic from NCDs. These findings also indicate that NCD prevalence surveillance studies in low- and middle-income countries should expand beyond self-reported diagnoses to include more extensive symptom- and examination-based surveys, given the likely high rate of surveillance bias due to barriers to diagnosis among vulnerable populations.",
      "authors": "Basu Sanjay; King Abby C",
      "year": "2013",
      "journal": "American journal of epidemiology",
      "doi": "10.1093/aje/kwt191",
      "url": "https://pubmed.ncbi.nlm.nih.gov/24049156/",
      "mesh_terms": "Aged; Aged, 80 and over; Algorithms; Artificial Intelligence; Chronic Disease; Cross-Sectional Studies; Developing Countries; Disability Evaluation; Female; Health Surveys; Humans; India; Male; Middle Aged; Population Surveillance; Prevalence; Risk Factors; Self Report; Socioeconomic Factors; Vulnerable Populations; World Health Organization",
      "keywords": "India; chronic disease; developing countries; disability; vulnerable populations",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC3842902",
      "ft_status": "Excluded: Insufficient evidence (approach_count=1, bias_title=False)"
    },
    {
      "pmid": "30805035",
      "title": "Extending Tests of Random Effects to Assess for Measurement Invariance in Factor Models.",
      "abstract": "Factor analysis models are widely used in health research to summarize hard to measure predictor or outcome variable constructs. For example, in the ELEMENT study, factor models are used to summarize lead exposure biomarkers which are thought to indirectly measure prenatal exposure to lead. Classic latent factor models are fitted assuming that factor loadings are constant across all covariate levels (e.g., maternal age in ELEMENT); that is, measurement invariance (MI) is assumed. When the MI is not met, measurement bias is introduced. Traditionally, MI is examined by defining subgroups of the data based on covariates, fitting multi-group factor analysis, and testing differences in factor loadings across covariate groups. In this paper, we develop novel tests of measurement invariance by modeling the factor loadings as varying coeffcients, i.e., letting the factor loading vary across continuous covariate values instead of groups. These varying coeffcients are estimated using penalized splines, where spline coeffcients are penalized by treating them as random coeffcients. The test of MI is then carried out by conducting a likelihood ratio test for the null hypothesis that the variance of the random spline coeffcients equals zero. We use a Monte-Carlo EM algorithm for estimation, and obtain the likelihood using Monte-Carlo in tegration. Using simulations, we compare the Type I error and power of our testing approach and the multi-group testing method. We apply the proposed methods to to summarize data on prenatal biomarkers of lead exposure from the ELEMENT study and find violations of MI due to maternal age.",
      "authors": "Zhang Zhenzhen; Braun Thomas M; Peterson Karen E; Hu Howard; T\u00e9llez-Rojo Martha M; S\u00e1nchez Brisa N",
      "year": "2018",
      "journal": "Statistics in biosciences",
      "doi": "10.1007/s12561-018-9222-7",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30805035/",
      "mesh_terms": "",
      "keywords": "Measurement invariance; Monte-Carlo EM algorithm; Testing variance components",
      "pub_types": "Journal Article",
      "pmcid": "PMC6385881",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "34312839",
      "title": "Use of behavioral health care in Medicaid managed care carve-out versus carve-in arrangements.",
      "abstract": "OBJECTIVE: To evaluate differences in access to behavioral health services for Medicaid enrollees covered by a Medicaid entity that integrated the financing of behavioral and physical health care (\"carve-in group\") versus a Medicaid entity that separated this financing (\"carve-out group\"). DATA SOURCES/STUDY SETTING: Medicaid claims data from two Medicaid entities in the Portland, Oregon tri-county area in 2016. STUDY DESIGN: In this cross-sectional study, we compared differences across enrollees in the carve-in versus carve-out group, using a machine learning approach to incorporate a large set of covariates and minimize potential selection bias. Our primary outcomes included behavioral health visits for a variety of different provider types. Secondary outcomes included inpatient, emergency department, and primary care visits. DATA COLLECTION: We used Medicaid claims, including adults with at least 9 months of enrollment. PRINCIPAL FINDINGS: The study population included 45,786 adults with mental health conditions. Relative to the carve-out group, individuals in the carve-in group were more likely to access outpatient behavioral health (2.39 percentage points, p\u00a0<\u20090.0001, with a baseline rate of approximately 73%). The carve-in group was also more likely to access primary care physicians, psychologists, and social workers and less likely to access psychiatrists and behavioral health specialists. Access to outpatient behavioral health visits was more likely in the carve-in arrangement among individuals with mild or moderate mental health conditions (compared to individuals with severe mental illness) and among black enrollees (compared to white enrollees). CONCLUSIONS: Financial integration of physical and behavioral health in Medicaid managed care was associated with greater access to behavioral health services, particularly for individuals with mild or moderate mental health conditions and for black enrollees. Recent changes to incentivize financial integration should be monitored to assess differential impacts by illness severity, race and ethnicity, provider types, and other factors.",
      "authors": "Charlesworth Christina J; Zhu Jane M; Horvitz-Lennon Marcela; McConnell K John",
      "year": "2021",
      "journal": "Health services research",
      "doi": "10.1111/1475-6773.13703",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34312839/",
      "mesh_terms": "Adolescent; Adult; Contracts; Cross-Sectional Studies; Female; Health Care Rationing; Health Services Accessibility; Humans; Male; Managed Care Programs; Medicaid; Mental Health Services; Middle Aged; Oregon; Primary Health Care; Referral and Consultation; Reimbursement, Incentive; Sociodemographic Factors; United States; Young Adult",
      "keywords": "Medicaid; managed care; mental health",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC8522561",
      "ft_status": "Excluded: Insufficient evidence (approach_count=2, bias_title=False)"
    },
    {
      "pmid": "32064914",
      "title": "Assessing and Mitigating Bias in Medical Artificial Intelligence: The Effects of Race and Ethnicity on a Deep Learning Model for ECG Analysis.",
      "abstract": "BACKGROUND: Deep learning algorithms derived in homogeneous populations may be poorly generalizable and have the potential to reflect, perpetuate, and even exacerbate racial/ethnic disparities in health and health care. In this study, we aimed to (1) assess whether the performance of a deep learning algorithm designed to detect low left ventricular ejection fraction using the 12-lead ECG varies by race/ethnicity and to (2) determine whether its performance is determined by the derivation population or by racial variation in the ECG. METHODS: We performed a retrospective cohort analysis that included 97 829 patients with paired ECGs and echocardiograms. We tested the model performance by race/ethnicity for convolutional neural network designed to identify patients with a left ventricular ejection fraction \u226435% from the 12-lead ECG. RESULTS: The convolutional neural network that was previously derived in a homogeneous population (derivation cohort, n=44 959; 96.2% non-Hispanic white) demonstrated consistent performance to detect low left ventricular ejection fraction across a range of racial/ethnic subgroups in a separate testing cohort (n=52 870): non-Hispanic white (n=44 524; area under the curve [AUC], 0.931), Asian (n=557; AUC, 0.961), black/African American (n=651; AUC, 0.937), Hispanic/Latino (n=331; AUC, 0.937), and American Indian/Native Alaskan (n=223; AUC, 0.938). In secondary analyses, a separate neural network was able to discern racial subgroup category (black/African American [AUC, 0.84], and white, non-Hispanic [AUC, 0.76] in a 5-class classifier), and a network trained only in non-Hispanic whites from the original derivation cohort performed similarly well across a range of racial/ethnic subgroups in the testing cohort with an AUC of at least 0.930 in all racial/ethnic subgroups. CONCLUSIONS: Our study demonstrates that while ECG characteristics vary by race, this did not impact the ability of a convolutional neural network to predict low left ventricular ejection fraction from the ECG. We recommend reporting of performance among diverse ethnic, racial, age, and sex groups for all new artificial intelligence tools to ensure responsible use of artificial intelligence in medicine.",
      "authors": "Noseworthy Peter A; Attia Zachi I; Brewer LaPrincess C; Hayes Sharonne N; Yao Xiaoxi; Kapa Suraj; Friedman Paul A; Lopez-Jimenez Francisco",
      "year": "2020",
      "journal": "Circulation. Arrhythmia and electrophysiology",
      "doi": "10.1161/CIRCEP.119.007988",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32064914/",
      "mesh_terms": "Artificial Intelligence; Deep Learning; Electrocardiography; Ethnicity; Female; Follow-Up Studies; Heart Ventricles; Humans; Male; Middle Aged; Racial Groups; Retrospective Studies; Ventricular Function, Left",
      "keywords": "United States; artificial intelligence; electrocardiography; humans; machine learning",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC7158877",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "41299259",
      "title": "Prevalence of headache disorders in Norway: results from the population based PopHEAD study.",
      "abstract": "BACKGROUND: Reliable and up-to-date prevalence estimates of headache disorders are essential for public health planning. Despite previous large-scale studies, there is a lack of validated and up-to-date population-representative prevalence estimates from Europe. Here, we aimed to estimate the one-year prevalence of the major headache disorders in Norway using a validated diagnostic tool. METHODS: PopHEAD is a population-based Norwegian cross-sectional study. A random sample of 28,753 individuals aged 18\u201370 years was invited to complete a digital version of the Headache-Attributed Restriction, Disability, Social Handicap and Impaired Participation (HARDSHIP) questionnaire, adapted and translated into Norwegian. Headache diagnoses were made using a standardized algorithm based on the International Classification of Headache Disorders (ICHD-3) criteria and validated by telephone interview in a sub-sample. Prevalence estimates were calculated as crude proportions with 95% confidence intervals and sequentially adjusted for age and sex, measurement error and selection bias. Associations with demographic variables were investigated. RESULTS: A total of 8,265 participants (3,344 men and 4,921 women; mean age 47.3 years) responded. The crude one-year prevalence was 29.6% for migraine (36.5% in women, 19.4% in men), 52.7% for tension-type headache (TTH) (51.4% in women, 54.6% in men), and 5.1% for probable medication-overuse headache (pMOH) (6.5% in women, 3.1% in men). After adjusting for age, sex, measurement error, and selection bias, the estimated prevalence was 20.3% for migraine, 47.9% for TTH, and 5.9% for pMOH. Migraine prevalence was highest among participants with low income and low education, while TTH prevalence was highest in participants with high socioeconomic status. pMOH prevalence was highest in participants aged 26\u201345 years and in participants with low education. CONCLUSION: The PopHEAD study provides updated, validated and bias-adjusted prevalence estimates for migraine, TTH and pMOH in the Norwegian adult population. These data may inform health resource allocation for headache management in similar populations.",
      "authors": "Argren Maria Bengtson; Engstrand Helene; Hus\u00f8y Andreas Kattem; Kristoffersen Espen Saxhaug; Toft Mathias; Pripp Are Hugo; Zwart John-Anker; Winsvold Bendik Slagsvold",
      "year": "2025",
      "journal": "The journal of headache and pain",
      "doi": "10.1186/s10194-025-02216-8",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41299259/",
      "mesh_terms": "",
      "keywords": "Epidemiology; HARDSHIP questionnaire; Medication-overuse headache; Migraine; Norway; Population-based study; Prevalence; Tension-type headache",
      "pub_types": "Journal Article",
      "pmcid": "PMC12659259",
      "ft_status": "Excluded: Insufficient evidence (approach_count=1, bias_title=False)"
    },
    {
      "pmid": "30362919",
      "title": "Evaluating the Presence of Cognitive Biases in Health Care Decision Making: A Survey of U.S. Formulary Decision Makers.",
      "abstract": "BACKGROUND: Behavioral economics is a field of economics that draws on insights from psychology to understand and identify patterns of decision making. Cognitive biases are psychological tendencies to process information in predictable patterns that result in deviations from rational decision making. Previous research has not evaluated the influence of cognitive biases on decision making in a managed care setting. OBJECTIVE: To assess the presence of cognitive biases in formulary decision making. METHODS: An online survey was conducted with a panel of U.S. pharmacy and medical directors who worked at managed care organizations and served on pharmacy and therapeutics committees. Survey questions assessed 4 cognitive biases: relative versus absolute framing effect, risk aversion, zero-risk bias, and delay discounting. Simulated data were presented in various scenarios related to adverse event profiles, drug safety and efficacy, and drug pricing for new hypothetical oncology products. Survey questions prompted participants to select a preferred drug based on the information provided. Survey answers were analyzed to identify decision patterns that could be explained by the cognitive biases. Likelihood of bias was analyzed via chi-square tests for framing effect, risk aversion, and zero-risk bias. The delay discounting section used a published algorithm to characterize discounting patterns. RESULTS: A total of 35 pharmacy directors and 19 medical directors completed the survey. In the framing effect section, 80% of participants selected the suboptimal choice in the relative risk frame, compared with 38.9% in the absolute risk frame (P < 0.0001). When assessing risk aversion, 42.6% and 61.1% of participants displayed risk aversion in the cost- and efficacy-based scenarios, respectively, but these were not statistically significant (P = 0.27 and P = 0.10, respectively). In the zero-risk bias section, results from each scenario diverged. In the first zero-risk bias scenario, 90.7% of participants selected the drug with zero risk (P < 0.001), but in the second scenario, only 32.1% chose the zero-risk option (P < 0.01). In the section assessing delay discounting, 54% of survey participants favored a larger delayed rebate over a smaller immediate discount. A shallow delay discounting curve was produced, which indicated participants discounted delayed rewards to a minimal degree. CONCLUSIONS: Pharmacy and medical directors, like other decision makers, appear to be susceptible to some cognitive biases. Directors demonstrated a tendency to underestimate risks when they were presented in relative risk terms but made more accurate appraisals when information was presented in absolute risk terms. Delay discounting also may be applicable to directors when choosing immediate discounts over delayed rebates. However, directors neither displayed a statistically significant bias for risk aversion when assessing scenarios related to drug pricing or clinical efficacy nor were there significant conclusions for zero-risk biases. Further research with larger samples using real-world health care decisions is necessary to validate these findings. DISCLOSURES: This research was funded by Xcenda. Mezzio, Nguyen, and O'Day are employees of Xcenda. Kiselica was employed by Xcenda at the time the study was conducted. The authors have nothing to disclose. A portion of the preliminary data was presented as posters at the 2017 AMCP Managed Care & Specialty Pharmacy Annual Meeting; March 27-30, 2017; in Denver, CO, and the 2017 International Society for Pharmacoeconomics and Outcomes Research 22nd Annual International Meeting; May 20-24, 2017; in Boston, MA.",
      "authors": "Mezzio Dylan J; Nguyen Victor B; Kiselica Andrew; O'Day Ken",
      "year": "2018",
      "journal": "Journal of managed care & specialty pharmacy",
      "doi": "10.18553/jmcp.2018.24.11.1173",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30362919/",
      "mesh_terms": "Cognition; Decision Making; Economics, Pharmaceutical; Humans; Likelihood Functions; Managed Care Programs; Outcome Assessment, Health Care; Pharmacy; Physician Executives; Prejudice; Risk Assessment; Surveys and Questionnaires",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC10397589",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "33893525",
      "title": "Race and Gender Disparity in the Surgical Management of Hepatocellular Cancer: Analysis of the Surveillance, Epidemiology, and End Results (SEER) Program Registry.",
      "abstract": "BACKGROUND: The existence of race and gender disparity has been described in numerous areas of medicine. The management of hepatocellular cancer is no different, but in no other area of medicine, is the treatment algorithm more complicated by local, regional, and national health care distribution policy. METHODS: Multivariate logistic regression and Cox-regression were utilized to analyze the treatment of patients with hepatocellular cancer registered in SEER between 1999 and 2013 to determine the incidence and effects of racial and gender disparity. Odd ratios (OR) are relative to Caucasian males, SEER region, and tumor characteristics. RESULTS: The analysis of 57,449 patients identified the minority were female (25.31%) and African-American (16.26%). All tumor interventions were protective (p\u2009<\u20090.001) with respect to survival. The mean survival for all registered patients was 13.01\u00a0months with conditional analysis, confirming that African-American men were less likely to undergo ablation, resection, or transplantation (p\u2009<\u20090.001). Women were more likely to undergo resection (p\u2009<\u20090.001). African-American women had an equivalent OR for resection but had a significantly lower transplant rate (p\u2009<\u20090.001). CONCLUSIONS: Utilizing SEER data as a surrogate for patient navigation in the treatment of hepatocellular cancer, our study identified not only race but gender bias with African-American women suffering the greatest. This is underscored by the lack of navigation of African-Americans to any therapy and a significant bias to navigate female patients to resection potentially limiting subsequent access to definitive therapy namely transplantation.",
      "authors": "Darden Michael; Parker Geoffrey; Monlezun Dominique; Anderson Edward; Buell Joseph F",
      "year": "2021",
      "journal": "World journal of surgery",
      "doi": "10.1007/s00268-021-06091-7",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33893525/",
      "mesh_terms": "Carcinoma, Hepatocellular; Female; Humans; Liver Neoplasms; Male; Registries; SEER Program; Sexism; United States",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "6556690",
      "ft_status": "Excluded: No AI/ML + health in full text"
    },
    {
      "pmid": "40681191",
      "title": "Curating a knowledge base for patients with neurosyphilis: a study protocol of a DEep learning Framework for pErsonalized prediction of Adverse prognosTic events in NeuroSyphilis (DEFEAT-NS).",
      "abstract": "INTRODUCTION: Adverse prognostic events (APE) of neurosyphilis include ongoing syphilitic meningitis, meningovascular syphilis, parenchymatous neurosyphilis and death. Its complexity and rarity have the potential to result in the underestimated true burden of neurosyphilis worldwide, due to lack of recognition and under-reporting. The unmet need for a modern method of refined and targeted treatment of neurosyphilis is strengthened by the currently various distinct diagnostic criteria. The DEep learning Framework for pErsonalized prediction of Adverse prognosTic events in NeuroSyphilis study will develop and validate prediction models for personalised prediction of APE after initial diagnosis in neurosyphilis to aid shared decision-making and stratify care of patients with neurosyphilis at high risk of severe prognostic course. METHODS AND ANALYSIS: We conducted formative research to conceptualise and design a robust and clinically acceptable deep learning framework. We will conduct a deep learning framework development and validation study using a retrospective, multicentre, longitudinal cohort design and applying unsupervised, semi-supervised machine learning and deep learning. It will be conducted following expert guidance for model development and validation and our previous research experience. This study design consists of six parts: development, calibration, validation, subgroup bias evaluation, clinical utility evaluation and explanation. ETHICS AND DISSEMINATION: This study will be conducted according to the Declaration of Helsinki and the Harmonised Tripartite Guideline for Good Clinical Practice of the International Conference on Harmonisation. No patient will be directly involved in developing the study's research question, design and implementation. This study will be a retrospective analysis of already anonymised data; therefore, ethical approval and informed consent were waived by the institutional review board of School of Public Health (Shenzhen), Sun Yat-sen University. The results will be disseminated through a peer-reviewed publication.",
      "authors": "Lu Zhen; Yang Liuqing; Li Jun; Wang Junfeng; Wu Weibo; Fu Leiwen; Wang Bingyi; Tian Tian; Zhang Hanlin; Peng Zhipeng; Liu Siyang; Zou Jun; Zou Huachun",
      "year": "2025",
      "journal": "BMJ open",
      "doi": "10.1136/bmjopen-2024-092248",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40681191/",
      "mesh_terms": "Humans; Deep Learning; Prognosis; Neurosyphilis; Retrospective Studies; Knowledge Bases; Research Design; Longitudinal Studies; Multicenter Studies as Topic",
      "keywords": "Artificial Intelligence; Machine Learning; Prognosis; Syphilis",
      "pub_types": "Journal Article",
      "pmcid": "PMC12273070",
      "ft_status": "Excluded: Insufficient evidence (approach_count=2, bias_title=False)"
    },
    {
      "pmid": "25316533",
      "title": "Posttreatment attrition and its predictors, attrition bias, and treatment efficacy of the anxiety online programs.",
      "abstract": "BACKGROUND: Although relatively new, the field of e-mental health is becoming more popular with more attention given to researching its various aspects. However, there are many areas that still need further research, especially identifying attrition predictors at various phases of assessment and treatment delivery. OBJECTIVE: The present study identified the predictors of posttreatment assessment completers based on 24 pre- and posttreatment demographic and personal variables and 1 treatment variable, their impact on attrition bias, and the efficacy of the 5 fully automated self-help anxiety treatment programs for generalized anxiety disorder (GAD), social anxiety disorder (SAD), panic disorder with or without agoraphobia (PD/A), obsessive-compulsive disorder (OCD), and posttraumatic stress disorder (PTSD). METHODS: A complex algorithm was used to diagnose participants' mental disorders based on the criteria of the Diagnostic and Statistical Manual of Mental Disorders (Fourth Edition, Text Revision; DSM-IV-TR). Those who received a primary or secondary diagnosis of 1 of 5 anxiety disorders were offered an online 12-week disorder-specific treatment program. A total of 3199 individuals did not formally drop out of the 12-week treatment cycle, whereas 142 individuals formally dropped out. However, only 347 participants who completed their treatment cycle also completed the posttreatment assessment measures. Based on these measures, predictors of attrition were identified and attrition bias was examined. The efficacy of the 5 treatment programs was assessed based on anxiety-specific severity scores and 5 additional treatment outcome measures. RESULTS: On average, completers of posttreatment assessment measures were more likely to be seeking self-help online programs; have heard about the program from traditional media or from family and friends; were receiving mental health assistance; were more likely to learn best by reading, hearing and doing; had a lower pretreatment Kessler-6 total score; and were older in age. Predicted probabilities resulting from these attrition variables displayed no significant attrition bias using Heckman's method and thus allowing for the use of completer analysis. Six treatment outcome measures (Kessler-6 total score, number of diagnosed disorders, self-confidence in managing mental health issues, quality of life, and the corresponding pre- and posttreatment severity for each program-specific anxiety disorder and for major depressive episode) were used to assess the efficacy of the 5 anxiety treatment programs. Repeated measures MANOVA revealed a significant multivariate time effect for all treatment outcome measures for each treatment program. Follow-up repeated measures ANOVAs revealed significant improvements on all 6 treatment outcome measures for GAD and PTSD, 5 treatment outcome measures were significant for SAD and PD/A, and 4 treatment outcome measures were significant for OCD. CONCLUSIONS: Results identified predictors of posttreatment assessment completers and provided further support for the efficacy of self-help online treatment programs for the 5 anxiety disorders. TRIAL REGISTRATION: Australian and New Zealand Clinical Trials Registry ACTRN121611000704998; http://www.anzctr.org.au/trial_view.aspx?ID=336143 (Archived by WebCite at http://www.webcitation.org/618r3wvOG).",
      "authors": "Al-Asadi Ali M; Klein Britt; Meyer Denny",
      "year": "2014",
      "journal": "Journal of medical Internet research",
      "doi": "10.2196/jmir.3513",
      "url": "https://pubmed.ncbi.nlm.nih.gov/25316533/",
      "mesh_terms": "Adolescent; Adult; Aged; Aged, 80 and over; Anxiety; Bias; Female; Humans; Internet; Male; Middle Aged; Models, Psychological; Outcome Assessment, Health Care; Quality of Life; Randomized Controlled Trials as Topic; Treatment Outcome; Young Adult",
      "keywords": "Internet interventions; Web treatment; cognitive behavioral therapy; e-mental health; fully automated; generalized anxiety disorder; obsessive compulsive disorder; online therapy; posttreatment attrition; posttreatment predictors; self-help; treatment efficacy",
      "pub_types": "Journal Article",
      "pmcid": "PMC4211028",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "26820188",
      "title": "Multivariate analysis of the population representativeness of related clinical studies.",
      "abstract": "OBJECTIVE: To develop a multivariate method for quantifying the population representativeness across related clinical studies and a computational method for identifying and characterizing underrepresented subgroups in clinical studies. METHODS: We extended a published metric named Generalizability Index for Study Traits (GIST) to include multiple study traits for quantifying the population representativeness of a set of related studies by assuming the independence and equal importance among all study traits. On this basis, we compared the effectiveness of GIST and multivariate GIST (mGIST) qualitatively. We further developed an algorithm called \"Multivariate Underrepresented Subgroup Identification\" (MAGIC) for constructing optimal combinations of distinct value intervals of multiple traits to define underrepresented subgroups in a set of related studies. Using Type 2 diabetes mellitus (T2DM) as an example, we identified and extracted frequently used quantitative eligibility criteria variables in a set of clinical studies. We profiled the T2DM target population using the National Health and Nutrition Examination Survey (NHANES) data. RESULTS: According to the mGIST scores for four example variables, i.e., age, HbA1c, BMI, and gender, the included observational T2DM studies had superior population representativeness than the interventional T2DM studies. For the interventional T2DM studies, Phase I trials had better population representativeness than Phase III trials. People at least 65years old with HbA1c value between 5.7% and 7.2% were particularly underrepresented in the included T2DM trials. These results confirmed well-known knowledge and demonstrated the effectiveness of our methods in population representativeness assessment. CONCLUSIONS: mGIST is effective at quantifying population representativeness of related clinical studies using multiple numeric study traits. MAGIC identifies underrepresented subgroups in clinical studies. Both data-driven methods can be used to improve the transparency of design bias in participation selection at the research community level.",
      "authors": "He Zhe; Ryan Patrick; Hoxha Julia; Wang Shuang; Carini Simona; Sim Ida; Weng Chunhua",
      "year": "2016",
      "journal": "Journal of biomedical informatics",
      "doi": "10.1016/j.jbi.2016.01.007",
      "url": "https://pubmed.ncbi.nlm.nih.gov/26820188/",
      "mesh_terms": "Algorithms; Biomedical Research; Clinical Trials as Topic; Databases, Factual; Demography; Diabetes Mellitus, Type 2; Humans; Medical Informatics Computing; Multivariate Analysis; Nutrition Surveys; Observational Studies as Topic; Patient Selection; Selection Bias",
      "keywords": "Clinical trial; Knowledge representation; Selection bias",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "PMC4837055",
      "ft_status": "Excluded: Insufficient evidence (approach_count=1, bias_title=False)"
    },
    {
      "pmid": "41220002",
      "title": "Sex-specific machine learning classification models improve outcome prediction for abdominal aortic aneurysms.",
      "abstract": "BACKGROUND: Abdominal aortic aneurysm (AAA) is an abnormal dilation of the abdominal aorta that carries up to a 90% mortality rate when ruptured. Although male patients experience AAA at a higher rate than females, female patients experience AAA rupture at a rate three- to four-fold higher that of their male counterparts. The current standard clinicians use for determining when to surgically intervene is maximum transverse diameter of the AAA perpendicular to the axis of flow. However, some aneurysms below these diameter thresholds rupture. Machine learning (ML) classification models have been previously shown to predict patient outcomes with more discriminability than the diameter criterion. However, these models do not consider sex-based differences. In this proof-of-concept study, we investigate how creating sex-specific ML models impacts patient outcome prediction as compared to a general model encompassing all patients (sex agnostic). METHODS: Computed tomography image sets were acquired from 537 patients (n\u2009=\u2009159 female, n\u2009=\u2009378 male) at the University of Pittsburgh Medical Center (UPMC) and Mayo Clinic Health Systems. Features used as input to the ML models were categorized as clinical, biomechanical, and morphological data. Prior to ML model training, patient data were randomly split for 20% holdout testing. ML models encompassing all patients (general model), only male patients (male-specific model), and only female patients (female-specific model) were trained and tested. RESULTS: A female-specific model and male-specific model both had a higher maximum area under the receiver-operating characteristic curve values than a general model for female patients and male patients, respectively. Equalizing the sample sizes of female and male patients in the model led to improved outcomes for female patients without decreasing performance for male patients. CONCLUSION: ML classification models show promise in improving predictions of patient outcomes for AAA. The higher AAA prevalence rate for males leads to female patients being underrepresented in AAA datasets. In this proof-of-concept study, we demonstrated that sex-specific models outperformed a general model in predicting patient outcomes. Additionally, equalizing sample sizes within the dataset improved predictions for female patients without compromising overall performance of the model. As ML applications in medicine continue to grow, it is important to consider population representation within datasets to reduce model bias.",
      "authors": "Kerr Katherine E; Sen Indrani; Gueldner Pete H; Tallarita Tiziano; Wildenberg Joseph C; Liang Nathan L; Vorp David A; Chung Timothy K",
      "year": "2025",
      "journal": "Biology of sex differences",
      "doi": "10.1186/s13293-025-00765-w",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41220002/",
      "mesh_terms": "Humans; Aortic Aneurysm, Abdominal; Machine Learning; Male; Female; Sex Characteristics; Aged; Middle Aged; Tomography, X-Ray Computed; Aged, 80 and over",
      "keywords": "AI explainability; Abdominal aortic aneurysm; Biomechanics; Machine learning; Morphology; Outcome prediction; Sex-based differences; Shape analysis; Stress analysis; Vascular surgery",
      "pub_types": "Journal Article",
      "pmcid": "PMC12607067",
      "ft_status": "Excluded: Insufficient evidence (approach_count=1, bias_title=False)"
    },
    {
      "pmid": "33313606",
      "title": "Frequent Causal Pattern Mining: A Computationally Efficient Framework For Estimating Bias-Corrected Effects.",
      "abstract": "Our aging population increasingly suffers from multiple chronic diseases simultaneously, necessitating the comprehensive treatment of these conditions. Finding the optimal set of drugs for a combinatorial set of diseases is a combinatorial pattern exploration problem. Association rule mining is a popular tool for such problems, but the requirement of health care for finding causal, rather than associative, patterns renders association rule mining unsuitable. To address this issue, we propose a novel framework based on the Rubin-Neyman causal model for extracting causal rules from observational data, correcting for a number of common biases. Specifically, given a set of interventions and a set of items that define subpopulations (e.g., diseases), we wish to find all subpopulations in which effective intervention combinations exist and in each such subpopulation, we wish to find all intervention combinations such that dropping any intervention from this combination will reduce the efficacy of the treatment. A key aspect of our framework is the concept of closed intervention sets which extend the concept of quantifying the effect of a single intervention to a set of concurrent interventions. Closed intervention sets also allow for a pruning strategy that is strictly more efficient than the traditional pruning strategy used by the Apriori algorithm. To implement our ideas, we introduce and compare five methods of estimating causal effect from observational data and rigorously evaluate them on synthetic data to mathematically prove (when possible) why they work. We also evaluated our causal rule mining framework on the Electronic Health Records (EHR) data of a large cohort of 152000 patients from Mayo Clinic and showed that the patterns we extracted are sufficiently rich to explain the controversial findings in the medical literature regarding the effect of a class of cholesterol drugs on Type-II Diabetes Mellitus (T2DM).",
      "authors": "Yadav Pranjul; Caraballo Pedro J; Steinbach Michael; Kumar Vipin; Castro M Regina; Simon Gyorgy",
      "year": "2019",
      "journal": "Proceedings : ... IEEE International Conference on Big Data. IEEE International Conference on Big Data",
      "doi": "10.1109/bigdata47090.2019.9005977",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33313606/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC7730315",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "41088416",
      "title": "PURE: policy-guided unbiased REpresentations for structure-constrained molecular generation.",
      "abstract": "Structure-constrained molecular generation (SCMG) generates novel molecules that are structurally similar to a given molecule and have optimized properties. Deep learning solutions for SCMG are limited in that they are predisposed towards existing knowledge, and they suffer from a natural impedance mismatch problem due to the discrete nature of molecules, while deep learning methods for SCMG often operate in continuous space. Moreover, many task-specific evaluation metrics used during training often bias the model towards a particular metric -\"metric-leakage\". To overcome these shortcomings, we propose Policy-guided Unbiased REpresentations (PURE) for SCMG that learns within a framework simulating molecular transformations for drug synthesis. PURE combines self-supervised learning with a policy-based reinforcement\u00a0learning (RL) framework, thereby avoiding the need for external molecular metrics while learning high-quality representations that incorporate an inherent notion of similarity specific to the given task. Along with a semi-supervised training design, PURE utilizes template-based molecular simulations to better explore and navigate the discrete molecular search space. Despite the lack of metric biases, PURE achieves competitive or superior performance to state-of-the-art methods on multiple benchmarks. Our study emphasizes the importance of reevaluating current approaches for SCMG and developing strategies that naturally align with the problem. Finally, we illustrate how our methodology can be applied to combat drug resistance by identifying sorafenib-like compounds as a case study.",
      "authors": "Gupta Abhor; Lenin Barathi; Current Sean; Batra Rohit; Ravindran Balaraman; Raman Karthik; Parthasarathy Srinivasan",
      "year": "2025",
      "journal": "Journal of cheminformatics",
      "doi": "10.1186/s13321-025-01090-5",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41088416/",
      "mesh_terms": "",
      "keywords": "Cancer drugs; Deep learning; Drug discovery; Drug resistance; Drug synthesis; Human health; Lead optimization; Machine learning; Product innovation; Reinforcement learning",
      "pub_types": "Journal Article",
      "pmcid": "PMC12522651",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "38240915",
      "title": "Quantifying bias due to missing data in quality of life surveys of advanced-stage cancer patients.",
      "abstract": "PURPOSE: Many studies on cancer patients investigate the impact of treatment on health-related quality of life (QoL). Typically, QoL is measured longitudinally, at baseline and at predefined timepoints thereafter. The question is whether, at a given timepoint, patients who return their questionnaire (available cases, AC) have a different QoL than those who do not return their questionnaire (non-AC). METHODS: We employed augmented inverse probability weighting (AIPW) to estimate the average QoL of non-AC in two studies on advanced-stage cancer patients. The AIPW estimator assumed data to be missing at random (MAR) and used machine learning (ML)-based methods to estimate answering probabilities of individuals at given timepoints as well as their reported QoL, as a function of auxiliary variables. These auxiliary variables were selected by medical oncologists based on domain expertise. We aggregated results both by timepoint and by time until death and compared AIPW estimates to the AC averages. Additionally, we used a pattern mixture model (PMM) to check sensitivity of our AIPW estimates against violation of the MAR assumption. RESULTS: Our study included 1927 patients with advanced pancreatic and 797 patients with advanced breast cancer. The AIPW estimate for average QoL of non-AC was below the average QoL of AC when aggregated by timepoint. The difference vanished when aggregated by time until death. PMM estimates were below AIPW estimates. CONCLUSIONS: Our results indicate that non-AC have a lower average QoL than AC. However, estimates for QoL of non-AC are subject to unverifiable assumptions about the missingness mechanism.",
      "authors": "Haug Nina; J\u00e4nicke Martina; Kasenda Benjamin; Marschner Norbert; Frank Melanie",
      "year": "2024",
      "journal": "Quality of life research : an international journal of quality of life aspects of treatment, care and rehabilitation",
      "doi": "10.1007/s11136-023-03588-7",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38240915/",
      "mesh_terms": "Humans; Female; Quality of Life; Breast Neoplasms; Surveys and Questionnaires; Bias",
      "keywords": "Augmented inverse probability weighting; Breast cancer; Double robust methods; Longitudinal studies; Missing data; Oncology; Pancreatic cancer",
      "pub_types": "Journal Article",
      "pmcid": "8789297",
      "ft_status": "Excluded: No AI/ML + health in full text"
    },
    {
      "pmid": "22092021",
      "title": "Examining multiple sources of differential item functioning on the Clinician & Group CAHPS\u00ae survey.",
      "abstract": "OBJECTIVE: To evaluate psychometric properties of a widely used patient experience survey. DATA SOURCES: English-language responses to the Clinician & Group Consumer Assessment of Healthcare Providers and Systems (CG-CAHPS\u00ae) survey (n = 12,244) from a 2008 quality improvement initiative involving eight southern California medical groups. METHODS: We used an iterative hybrid ordinal logistic regression/item response theory differential item functioning (DIF) algorithm to identify items with DIF related to patient sociodemographic characteristics, duration of the physician-patient relationship, number of physician visits, and self-rated physical and mental health. We accounted for all sources of DIF and determined its cumulative impact. PRINCIPAL FINDINGS: The upper end of the CG-CAHPS\u00ae performance range is measured with low precision. With sensitive settings, some items were found to have DIF. However, overall DIF impact was negligible, as 0.14 percent of participants had salient DIF impact. Latinos who spoke predominantly English at home had the highest prevalence of salient DIF impact at 0.26 percent. CONCLUSIONS: The CG-CAHPS\u00ae functions similarly across commercially insured respondents from diverse backgrounds. Consequently, previously documented racial and ethnic group differences likely reflect true differences rather than measurement bias. The impact of low precision at the upper end of the scale should be clarified.",
      "authors": "Rodriguez Hector P; Crane Paul K",
      "year": "2011",
      "journal": "Health services research",
      "doi": "10.1111/j.1475-6773.2011.01299.x",
      "url": "https://pubmed.ncbi.nlm.nih.gov/22092021/",
      "mesh_terms": "Adult; Age Factors; Algorithms; Body Mass Index; Educational Status; Ethnicity; Female; Health Status; Humans; Language; Logistic Models; Male; Mental Health; Middle Aged; Patient Satisfaction; Physician-Patient Relations; Primary Health Care; Psychometrics; Racial Groups; Sex Factors; Surveys and Questionnaires",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC3393020",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "40993641",
      "title": "Association between geriatric nutritional risk index (GNRI) and asthma in elderly individuals aged 60 and above: a cross-sectional study of the NHANES 2005-2018.",
      "abstract": "OBJECTIVE: The geriatric nutritional risk index (GNRI) is a promising tool for predicting nutrition-related complications in older adults. This study aimed to explore the association between GNRI and asthma in individuals aged 60 and above. METHODS: A retrospective cohort study was conducted using the National Health and Nutrition Examination Survey (NHANES) database. Propensity score matching was used to manage observational data to minimize clinical data bias and confounding variables. Weighted logistic regression with subgroup and sensitivity analyses was used to analyze the potential relationship between GNRI and asthma in elderly individuals aged 60 and above. RESULTS: The study population consisted of individuals aged 60 and above. After adjusting for race, education, emphysema, and chronic bronchitis, the odds ratio (OR) for asthma in relation to the GNRI was 1.021 (95% confidence interval [CI]: 1.016-1.026, P\u2009<\u20090.001), indicating that a lower GNRI is associated with a higher risk of asthma in elderly individuals.The GNRI quartile analysis revealed a significant upward trend (Q4 versus Q1, OR: 1.666, 95% CI: 1.41-1.972, P\u2009<\u20090.001). The significance of the selected factors was assessed using the XGBoost machine learning model, which demonstrated that the GNRI was one of the top five variables influencing the risk of asthma in elderly individuals. Subgroup analysis confirmed the association between GNRI and factors such as gender, race, smoking, alcohol consumption, education level, poverty income ratio, emphysema, and chronic bronchitis. Furthermore, GNRI levels were associated with increased eosinophils, basophils, white blood cells, red blood cells, neutrophils, monocytes, and albumin levels. CONCLUSION: This study demonstrates that GNRI levels are significantly associated with asthma in the elderly.",
      "authors": "Wang Jue; Wang ZiMeng; Zhang Qi; Yu Shiting",
      "year": "2025",
      "journal": "BMC pulmonary medicine",
      "doi": "10.1186/s12890-025-03830-7",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40993641/",
      "mesh_terms": "Humans; Asthma; Male; Female; Aged; Retrospective Studies; Cross-Sectional Studies; Nutrition Surveys; Middle Aged; Geriatric Assessment; Risk Factors; Nutritional Status; Nutrition Assessment; Propensity Score; Aged, 80 and over; Logistic Models; Risk Assessment; United States",
      "keywords": "Asthma; Cohort analysis; Geriatric nutritional risk index (GNRI); NHANES; Over 60\u00a0years old; XGBoost machine learning",
      "pub_types": "Journal Article",
      "pmcid": "PMC12462012",
      "ft_status": "Excluded: Insufficient evidence (approach_count=2, bias_title=False)"
    },
    {
      "pmid": "38286672",
      "title": "Improving our understanding of the social determinants of mental health: a data linkage study of mental health records and the 2011 UK census.",
      "abstract": "OBJECTIVES: To address the lack of individual-level socioeconomic information in electronic healthcare records, we linked the 2011 census of England and Wales to patient records from a large mental healthcare provider. This paper describes the linkage process and methods for mitigating bias due to non-matching. SETTING: South London and Maudsley NHS Foundation Trust (SLaM), a mental healthcare provider in Southeast London. DESIGN: Clinical records from SLaM were supplied to the Office of National Statistics for linkage to the census through a deterministic matching algorithm. We examined clinical (International Classification of Disease-10 diagnosis, history of hospitalisation, frequency of service contact) and socio-demographic (age, gender, ethnicity, deprivation) information recorded in Clinical Record Interactive Search (CRIS) as predictors of linkage success with the 2011 census. To assess and adjust for potential biases caused by non-matching, we evaluated inverse probability weighting for mortality associations. PARTICIPANTS: Individuals of all ages in contact with SLaM up until December 2019 (N=459\u2009374). OUTCOME MEASURES: Likelihood of mental health records' linkage to census. RESULTS: 220\u2009864 (50.4%) records from CRIS linked to the 2011 census. Young adults (prevalence ratio (PR) 0.80, 95%\u2009CI 0.80 to 0.81), individuals living in more deprived areas (PR 0.78, 95% CI 0.78 to 0.79) and minority ethnic groups (eg, Black African, PR 0.67, 0.66 to 0.68) were less likely to match to census. After implementing inverse probability weighting, we observed little change in the strength of association between clinical/demographic characteristics and mortality (eg, presence of any psychiatric disorder: unweighted PR 2.66, 95%\u2009CI 2.52 to 2.80; weighted PR 2.70, 95%\u2009CI 2.56 to 2.84). CONCLUSIONS: Lower response rates to the 2011 census among people with psychiatric disorders may have contributed to lower match rates, a potential concern as the census informs service planning and allocation of resources. Due to its size and unique characteristics, the linked data set will enable novel investigations into the relationship between socioeconomic factors and psychiatric disorders.",
      "authors": "Cybulski Lukasz; Chilman Natasha; Jewell Amelia; Dewey Michael; Hildersley Rosanna; Morgan Craig; Huck Rachel; Hotopf Matthew; Stewart Robert; Pritchard Megan; Wuerth Milena; Das-Munshi Jayati",
      "year": "2024",
      "journal": "BMJ open",
      "doi": "10.1136/bmjopen-2023-073582",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38286672/",
      "mesh_terms": "Young Adult; Humans; Mental Health; Censuses; Social Determinants of Health; England; London; Information Storage and Retrieval; Electronic Health Records",
      "keywords": "mental health; psychiatry; schizophrenia & psychotic disorders",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC10826590",
      "ft_status": "Excluded: Insufficient evidence (approach_count=2, bias_title=False)"
    },
    {
      "pmid": "38426381",
      "title": "Analysis of agreement between measures of subjective cognitive impairment and probable dementia in the National Health and Aging Trends Study.",
      "abstract": "BACKGROUND: Subjective cognitive impairment (SCI) measures in population-based surveys offer potential for dementia surveillance, yet their validation against established dementia measures is lacking. METHODS: We assessed agreement between SCI and a validated probable dementia algorithm in a random one-third sample (n\u00a0=\u00a01936) of participants in the 2012 National Health and Aging Trends Study (NHATS). RESULTS: SCI was more prevalent than probable dementia (12.2%\u00a0vs 8.4%). Agreement between measures was 90.0% and of substantial strength. Misclassification rates were higher among older and less-educated subgroups due to higher prevalence of false-positive misclassification but did not vary by sex or race and ethnicity. DISCUSSION: SCI sensitivity (63.4%) and specificity (92.5%) against dementia were comparable with similar metrics for the NHATS probable dementia measure against the \"gold-standard\" Aging, Demographics, and Memory Study-based dementia criteria, implying that population-based surveys may afford cost-effective opportunities for dementia surveillance to assess risk and inform policy. HIGHLIGHTS: The prevalence of subjective cognitive impairment (SCI) is generally higher than that of a validated measure of probable dementia, particularly within the youngest age group, females, Whites, and persons with a college or higher degree. Percent agreement between SCI and a validated measure of probable dementia was 90.0% and of substantial strength (prevalence- and bias-adjusted kappa, 0.80). Agreement rates were higher in older and less-educated subgroups, driven by the higher prevalence of false-positive disagreement, but did not vary significantly by sex or race and ethnicity. SCI's overall sensitivity and specificity were 63.4% and 92.5%, respectively, against a validated measure of probable dementia, suggesting utility as a low-cost option for dementia surveillance. Heterogeneity in agreement quality across subpopulations warrants caution in its use for subgroup analyses.",
      "authors": "Chyr Linda C; Wolff Jennifer L; Zissimopoulos Julie M; Drabo Emmanuel F",
      "year": "2024",
      "journal": "Alzheimer's & dementia : the journal of the Alzheimer's Association",
      "doi": "10.1002/alz.13758",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38426381/",
      "mesh_terms": "Female; Humans; Aged; Cognition Disorders; Cognitive Dysfunction; Aging; Sensitivity and Specificity; Dementia",
      "keywords": "ADRD; Alzheimer's disease; NHATS; dementia; disability questionnaire; subjective cognitive impairment",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "pmcid": "PMC11032562",
      "ft_status": "Excluded: Insufficient evidence (approach_count=2, bias_title=False)"
    },
    {
      "pmid": "29046267",
      "title": "Identifying Sentiment of Hookah-Related Posts on Twitter.",
      "abstract": "BACKGROUND: The increasing popularity of hookah (or waterpipe) use in the United States and elsewhere has consequences for public health because it has similar health risks to that of combustible cigarettes. While hookah use rapidly increases in popularity, social media data (Twitter, Instagram) can be used to capture and describe the social and environmental contexts in which individuals use, perceive, discuss, and are marketed this tobacco product. These data may allow people to organically report on their sentiment toward tobacco products like hookah unprimed by a researcher, without instrument bias, and at low costs. OBJECTIVE: This study describes the sentiment of hookah-related posts on Twitter and describes the importance of debiasing Twitter data when attempting to understand attitudes. METHODS: Hookah-related posts on Twitter (N=986,320) were collected from March 24, 2015, to December 2, 2016. Machine learning models were used to describe sentiment on 20 different emotions and to debias the data so that Twitter posts reflected sentiment of legitimate human users and not of social bots or marketing-oriented accounts that would possibly provide overly positive or overly negative sentiment of hookah. RESULTS: From the analytical sample, 352,116 tweets (59.50%) were classified as positive while 177,537 (30.00%) were classified as negative, and 62,139 (10.50%) neutral. Among all positive tweets, 218,312 (62.00%) were classified as highly positive emotions (eg, active, alert, excited, elated, happy, and pleasant), while 133,804 (38.00%) positive tweets were classified as passive positive emotions (eg, contented, serene, calm, relaxed, and subdued). Among all negative tweets, 95,870 (54.00%) were classified as subdued negative emotions (eg, sad, unhappy, depressed, and bored) while the remaining 81,667 (46.00%) negative tweets were classified as highly negative emotions (eg, tense, nervous, stressed, upset, and unpleasant). Sentiment changed drastically when comparing a corpus of tweets with social bots to one without. For example, the probability of any one tweet reflecting joy was 61.30% from the debiased (or bot free) corpus of tweets. In contrast, the probability of any one tweet reflecting joy was 16.40% from the biased corpus. CONCLUSIONS: Social media data provide researchers the ability to understand public sentiment and attitudes by listening to what people are saying in their own words. Tobacco control programmers in charge of risk communication may consider targeting individuals posting positive messages about hookah on Twitter or designing messages that amplify the negative sentiments. Posts on Twitter communicating positive sentiment toward hookah could add to the normalization of hookah use and is an area of future research. Findings from this study demonstrated the importance of debiasing data when attempting to understand attitudes from Twitter data.",
      "authors": "Allem Jon-Patrick; Ramanujam Jagannathan; Lerman Kristina; Chu Kar-Hai; Boley Cruz Tess; Unger Jennifer B",
      "year": "2017",
      "journal": "JMIR public health and surveillance",
      "doi": "10.2196/publichealth.8133",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29046267/",
      "mesh_terms": "",
      "keywords": "Twitter; big data; bots; hookah; sentiment; social media; waterpipe",
      "pub_types": "Journal Article",
      "pmcid": "PMC5667930",
      "ft_status": "Excluded: Insufficient evidence (approach_count=2, bias_title=False)"
    },
    {
      "pmid": "40601199",
      "title": "Incident Atherosclerotic Cardiovascular Disease Among Veterans by Gender Identity: A Cohort Study.",
      "abstract": "BACKGROUND: Transgender and gender diverse (trans) populations are at elevated risk for atherosclerotic cardiovascular disease (ASCVD). OBJECTIVE: Measure the association of gender identity and gender-affirming hormone therapy (GAHT) with ASCVD outcomes. DESIGN: Cohort study. PARTICIPANTS: Over 1 million veterans receiving care in the Veterans Health Administration. MAIN MEASURES: Gender identity was identified via a validated natural language processing (NLP) algorithm. Incident ASCVD (acute myocardial infarction, ischemic stroke, or revascularization after the baseline date) was identified via International Classification of Diseases diagnosis codes among veterans without prevalent ASCVD. We calculated sample statistics stratified by gender identity and used Cox proportional hazard regression to assess associations of gender identity and GAHT with incident ASCVD. KEY RESULTS: Among 1,105,082 veterans, 42,149 were classified as trans (8013 transfeminine, 7127 transmasculine, and 27,009 uncategorized trans) while 918,843 were cisgender men and 144,090 were cisgender women. During a median follow-up of 9.39\u00a0years, 92,910 veterans had incident ASCVD (2806 among trans veterans). Adjusting for age, race, Hispanic ethnicity, and sexual orientation, trans veterans had 1.52 [1.45, 1.59] and 0.92 [0.89, 0.96] times the hazard of ASCVD compared to cisgender women and cisgender men, respectively. Compared to trans veterans not receiving GAHT, GAHT among trans veterans assigned female at birth was significantly associated a reduced hazard of ASCVD (0.89 [0.80, 0.98]); GAHT was not associated with ASCVD among trans veterans assigned male at birth (0.99 [0.89, 1.09]). LIMITATIONS: With NLP, there is potential for selection bias as clinicians may preferentially document the gender identity for trans more than cisgender veterans. CONCLUSIONS: This is one of the first studies to examine the association of both gender identity and GAHT with incident ASCVD in veterans. Future research must comprehensively evaluate ASCVD outcomes and the effects of gender-affirming care (including hormone therapy) in trans populations.",
      "authors": "Streed Carl G; Duncan Meredith S; Heier Kory R; Workman T Elizabeth; Beach Lauren B; Jasuja Guneet K; Wolfe Hill L; Hughes Landon D; O'Leary John R; Skanderson Melissa; Goulet Joseph L",
      "year": "2025",
      "journal": "Journal of general internal medicine",
      "doi": "10.1007/s11606-025-09701-5",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40601199/",
      "mesh_terms": "Humans; Male; Female; Veterans; Middle Aged; Atherosclerosis; Cohort Studies; United States; Gender Identity; Incidence; Aged; Adult; Transgender Persons",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "PMC12508330",
      "ft_status": "Excluded: No approach indicators in full text"
    },
    {
      "pmid": "35731224",
      "title": "Retracted: Exposure to Per- and Polyfluoroalkyl Substances and Mortality in U.S. Adults: A Population-Based Cohort Study.",
      "abstract": "BACKGROUND: Per- and polyfluoroalkyl substances (PFAS) are widespread environmental contaminants associated with diseases such as cancer and dyslipidemia. However, few studies have investigated the association between PFAS mixture exposure and mortality in general populations. OBJECTIVES: This study aimed to explore the association between PFAS mixture, perfluorooctanoic acid (PFOA), and perfluorooctane sulfonic acid (PFOS) and mortality in U.S. adults by a nationally representative cohort. METHODS: Adults \u226518\u2009years of age who were enrolled in the National Health and Nutrition Examination Survey (NHANES) (1999-2014) were included in our study. Baseline serum concentrations of seven PFAS were measured and individuals were followed up to 31 December 2015. Hazard ratios (HRs) and confidence intervals (CIs) were estimated using Cox proportional hazards models. Association between PFAS mixture exposure and mortality was analyzed using the k-means method by clustering PFAS mixtures into subgroups. Association between PFOA/PFOS exposure and mortality was subsequently analyzed in both continuous and categorical models. RESULTS: During the follow-up period, 1,251 participants died. In the mixture analysis, the k-means algorithm clustered participants into low-, medium-, and high-exposure groups. Compared with the low-exposure group, participants in the high-exposure group showed significantly higher risks for all-cause mortality (HR=1.38; 95% CI: 1.07, 1.80), heart disease mortality (HR=1.58; 95% CI: 1.05, 2.51), and cancer mortality (HR=1.70; 95% CI: 1.08, 2.84). In single PFAS analysis, PFOS was found to be positively associated with all-cause mortality (third vs. first tertile HR=1.57; 95% CI: 1.22, 2.07), heart disease mortality (third vs. first tertile HR=1.65; 95% CI: 1.09, 2.57), and cancer mortality (third vs. first tertile HR=1.75; 95% CI: 1.10, 2.83), whereas PFOA exposure had no significant association with mortality. Assuming the observed association is causal, the number of deaths associated with PFOS exposure (\u226517.1 vs. <7.9\u2009ng/mL) was \u223c382,000 (95% CI: 176,000, 588,000) annually between 1999 and 2015, and it decreased to 69,000 (95% CI: 28,000, 119,000) annually between 2015 and 2018. The association between PFOS and mortality was stronger among women and people without diabetes. DISCUSSION: We observed a positive association between PFAS mixture exposure and mortality among U.S. adults. Limitations of this study include the potential for unmeasured confounding, selection bias, a relatively small number of deaths, and only measuring PFAS at one point in time. Further studies with serial measures of PFAS concentrations and longer follow-ups are necessary to elucidate the association between PFAS and mortality from specific causes. https://doi.org/10.1289/EHP10393.",
      "authors": "Wen Xue; Wang Mei; Xu Xuewen; Li Tao",
      "year": "2022",
      "journal": "Environmental health perspectives",
      "doi": "10.1289/EHP10393",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35731224/",
      "mesh_terms": "Adult; Alkanesulfonic Acids; Cohort Studies; Environmental Pollutants; Female; Fluorocarbons; Heart Diseases; Humans; Nutrition Surveys; Research",
      "keywords": "",
      "pub_types": "Journal Article; Retracted Publication",
      "pmcid": "PMC9215707",
      "ft_status": "Excluded: Insufficient evidence (approach_count=1, bias_title=False)"
    }
  ],
  "no_fulltext": [
    {
      "pmid": "31649194",
      "title": "Dissecting racial bias in an algorithm used to manage the health of populations.",
      "abstract": "Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.",
      "authors": "Obermeyer Ziad; Powers Brian; Vogeli Christine; Mullainathan Sendhil",
      "year": "2019",
      "journal": "Science (New York, N.Y.)",
      "doi": "10.1126/science.aax2342",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31649194/",
      "mesh_terms": "Black or African American; Algorithms; Bias; Chronic Disease; Health Care Costs; Health Status Disparities; Humans; Medical Records; Racism; Risk Assessment; United States; White People",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "35130064",
      "title": "The Potential For Bias In Machine Learning And Opportunities For Health Insurers To Address It.",
      "abstract": "As the use of machine learning algorithms in health care continues to expand, there are growing concerns about equity, fairness, and bias in the ways in which machine learning models are developed and used in clinical and business decisions. We present a guide to the data ecosystem used by health insurers to highlight where bias can arise along machine learning pipelines. We suggest mechanisms for identifying and dealing with bias and discuss challenges and opportunities to increase fairness through analytics in the health insurance industry.",
      "authors": "Gervasi Stephanie S; Chen Irene Y; Smith-McLallen Aaron; Sontag David; Obermeyer Ziad; Vennera Michael; Chawla Ravi",
      "year": "2022",
      "journal": "Health affairs (Project Hope)",
      "doi": "10.1377/hlthaff.2021.01287",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35130064/",
      "mesh_terms": "Algorithms; Bias; Ecosystem; Humans; Insurance Carriers; Machine Learning",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "36578121",
      "title": "Implicit Bias and Machine Learning in Health Care.",
      "abstract": "",
      "authors": "Zaidi Danish; Miller Taylor",
      "year": "2023",
      "journal": "Southern medical journal",
      "doi": "10.14423/SMJ.0000000000001489",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36578121/",
      "mesh_terms": "Humans; Bias, Implicit; Attitude of Health Personnel; Curriculum; Machine Learning",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "35049447",
      "title": "On Algorithmic Fairness in Medical Practice.",
      "abstract": "The application of machine-learning technologies to medical practice promises to enhance the capabilities of healthcare professionals in the assessment, diagnosis, and treatment, of medical conditions. However, there is growing concern that algorithmic bias may perpetuate or exacerbate existing health inequalities. Hence, it matters that we make precise the different respects in which algorithmic bias can arise in medicine, and also make clear the normative relevance of these different kinds of algorithmic bias for broader questions about justice and fairness in healthcare. In this paper, we provide the building blocks for an account of algorithmic bias and its normative relevance in medicine.",
      "authors": "Grote Thomas; Keeling Geoff",
      "year": "2022",
      "journal": "Cambridge quarterly of healthcare ethics : CQ : the international journal of healthcare ethics committees",
      "doi": "10.1017/S0963180121000839",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35049447/",
      "mesh_terms": "Data Collection; Delivery of Health Care; Humans; Machine Learning; Social Justice",
      "keywords": "algorithmic bias; discrimination; fairness; machine learning; medical practice",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "39049285",
      "title": "Fairness in Classifying and Grouping Health Equity Information.",
      "abstract": "This paper explores the balance between fairness and performance in machine learning classification, predicting the likelihood of a patient receiving anti-microbial treatment using structured data in community nursing wound care electronic health records. The data includes two important predictors (gender and language) of the social determinants of health, which we used to evaluate the fairness of the classifiers. At the same time, the impact of various groupings of language codes on classifiers' performance and fairness is analyzed. Most common statistical learning-based classifiers are evaluated. The findings indicate that while K-Nearest Neighbors offers the best fairness metrics among different grouping settings, the performance of all classifiers is generally consistent across different language code groupings. Also, grouping more variables tends to improve the fairness metrics over all classifiers while maintaining their performance.",
      "authors": "Jin Ruinan; Li Xiaoxiao; Block Lorraine J; Beschastnikh Ivan; Currie Leanne M; Ronquillo Charlene E",
      "year": "2024",
      "journal": "Studies in health technology and informatics",
      "doi": "10.3233/SHTI240171",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39049285/",
      "mesh_terms": "Health Equity; Electronic Health Records; Machine Learning; Humans; Social Determinants of Health",
      "keywords": "Electronic Health Record; Fairness and Bias; Feature Engineering",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "40775971",
      "title": "AI Bias and Confounding Risk in Health Feature Engineering for Machine Learning Classification Task.",
      "abstract": "Recent advancements in machine learning bring unique opportunities in health fields but also pose considerable challenges. Due to stringent ethical considerations and resource constraints, health data can vary in scope, population coverage, and collection granularity, prone to different AI bias and confounding risks in the performance of a classification task. This experimental study explored the impact on hidden confounding risk of model performance in a cardiovascular readmission prediction task using real-life health data from 'Data-derived Risk assessment using the Electronic medical record through Application of Machine Learning' (DREAM). Five commonly used machine learning models-k-nearest neighbors (KNN), random forest (RF), decision tree (DT), Catboost and Xgboost-were selected for this task. Model performance was assessed via the area under the receiver operating characteristics curve (AUC) and F1 score, both before and after propensity score adjustment. Based on density plot comparison of the adjustment, the difference mainly contributed from patients aged 20 and 40. High fluctuation on the model performance has been noted by including and excluding patients under this age group. After reasoning, high-risk pregnant females may serve as a confounding factor in the original model generation. The pregnancy rate in the non-readmitted group is significantly higher than that in the readmitted group (x2 = 10.2, p < 0.001). However, pregnant status required additional information query from a different hospital system. Without carefully consideration of confounding risks, traditional pipeline may generate a less robotic classifier in the clinical setting. Incorporating propensity score matching could be a solution to randomise invisible confounding factors between the classes.",
      "authors": "Guo Ruihua; Ritchie Angus; Smith Ross; Lu Yang; Min Haeri; Poon Simon K",
      "year": "2025",
      "journal": "Studies in health technology and informatics",
      "doi": "10.3233/SHTI250953",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40775971/",
      "mesh_terms": "Machine Learning; Humans; Electronic Health Records; Female; Risk Assessment; Bias; Adult; Patient Readmission; Cardiovascular Diseases; Pregnancy; Confounding Factors, Epidemiologic",
      "keywords": "AI bias; Classification; Confounding bias; Feature Engineering; Machine Learning; Quality Control; Readmission risk prediction",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "37729839",
      "title": "Cardiometabolic risk estimation using exposome data and machine learning.",
      "abstract": "BACKGROUND: The human exposome encompasses all exposures that individuals encounter throughout their lifetime. It is now widely acknowledged that health outcomes are influenced not only by genetic factors but also by the interactions between these factors and various exposures. Consequently, the exposome has emerged as a significant contributor to the overall risk of developing major diseases, such as cardiovascular disease (CVD) and diabetes. Therefore, personalized early risk assessment based on exposome attributes might be a promising tool for identifying high-risk individuals and improving disease prevention. OBJECTIVE: Develop and evaluate a novel and fair machine learning (ML) model for CVD and type 2 diabetes (T2D) risk prediction based on a set of readily available exposome factors. We evaluated our model using internal and external validation groups from a multi-center cohort. To be considered fair, the model was required to demonstrate consistent performance across different sub-groups of the cohort. METHODS: From the UK Biobank, we identified 5,348 and 1,534 participants who within 13 years from the baseline visit were diagnosed with CVD and T2D, respectively. An equal number of participants who did not develop these pathologies were randomly selected as the control group. 109 readily available exposure variables from six different categories (physical measures, environmental, lifestyle, mental health events, sociodemographics, and early-life factors) from the participant's baseline visit were considered. We adopted the XGBoost ensemble model to predict individuals at risk of developing the diseases. The model's performance was compared to that of an integrative ML model which is based on a set of biological, clinical, physical, and sociodemographic variables, and, additionally for CVD, to the Framingham risk score. Moreover, we assessed the proposed model for potential bias related to sex, ethnicity, and age. Lastly, we interpreted the model's results using SHAP, a state-of-the-art explainability method. RESULTS: The proposed ML model presents a comparable performance to the integrative ML model despite using solely exposome information, achieving a ROC-AUC of 0.78\u00b10.01 and 0.77\u00b10.01 for CVD and T2D, respectively. Additionally, for CVD risk prediction, the exposome-based model presents an improved performance over the traditional Framingham risk score. No bias in terms of key sensitive variables was identified. CONCLUSIONS: We identified exposome factors that play an important role in identifying patients at risk of CVD and T2D, such as naps during the day, age completed full-time education, past tobacco smoking, frequency of tiredness/unenthusiasm, and current work status. Overall, this work demonstrates the potential of exposome-based machine learning as a fair CVD and T2D risk assessment tool.",
      "authors": "Atehort\u00faa Ang\u00e9lica; Gkontra Polyxeni; Camacho Marina; Diaz Oliver; Bulgheroni Maria; Simonetti Valentina; Chadeau-Hyam Marc; Felix Janine F; Sebert Sylvain; Lekadir Karim",
      "year": "2023",
      "journal": "International journal of medical informatics",
      "doi": "10.1016/j.ijmedinf.2023.105209",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37729839/",
      "mesh_terms": "Humans; Exposome; Diabetes Mellitus, Type 2; Risk Factors; Cardiovascular Diseases; Machine Learning",
      "keywords": "Cardiovascular disease; Explainability; Exposure data; Fairness; Type 2 diabetes; XGBoost",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "39495385",
      "title": "Evaluating machine learning model bias and racial disparities in non-small cell lung cancer using SEER registry data.",
      "abstract": "BACKGROUND: Despite decades of pursuing health equity, racial and ethnic disparities persist in healthcare in America. For cancer specifically, one of the leading observed disparities is worse mortality among non-Hispanic Black patients compared to non-Hispanic White patients across the cancer care continuum. These real-world disparities are reflected in the data used to inform the decisions made to alleviate such inequities. Failing to account for inherently biased data underlying these observations could intensify racial cancer disparities and lead to misguided efforts that fail to appropriately address the real causes of health inequity. OBJECTIVE: Estimate the racial/ethnic bias of machine learning models in predicting two-year survival and surgery treatment recommendation for non-small cell lung cancer (NSCLC) patients. METHODS: A Cox survival model, and a LOGIT model as well as three other machine learning models for predicting surgery recommendation were trained using SEER data from NSCLC patients diagnosed from 2000-2018. Models were trained with a 70/30 train/test split (both including and excluding race/ethnicity) and evaluated using performance and fairness metrics. The effects of oversampling the training data were also evaluated. RESULTS: The survival models show disparate impact towards non-Hispanic Black patients regardless of whether race/ethnicity is used as a predictor. The models including race/ethnicity amplified the disparities observed in the data. The exclusion of race/ethnicity as a predictor in the survival and surgery recommendation models improved fairness metrics without degrading model performance. Stratified oversampling strategies reduced disparate impact while reducing the accuracy of the model. CONCLUSION: NSCLC disparities are complex and multifaceted. Yet, even when accounting for age and stage at diagnosis, non-Hispanic Black patients with NSCLC are less often recommended to have surgery than non-Hispanic White patients. Machine learning models amplified the racial/ethnic disparities across the cancer care continuum (which are reflected in the data used to make model decisions). Excluding race/ethnicity lowered the bias of the models but did not affect disparate impact. Developing analytical strategies to improve fairness would in turn improve the utility of machine learning approaches analyzing population-based cancer data.",
      "authors": "Trentz Cameron; Engelbart Jacklyn; Semprini Jason; Kahl Amanda; Anyimadu Eric; Buatti John; Casavant Thomas; Charlton Mary; Canahuate Guadalupe",
      "year": "2024",
      "journal": "Health care management science",
      "doi": "10.1007/s10729-024-09691-6",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39495385/",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Bias; Black or African American; Carcinoma, Non-Small-Cell Lung; Ethnicity; Healthcare Disparities; Lung Neoplasms; Machine Learning; Proportional Hazards Models; Racial Groups; SEER Program; United States; White",
      "keywords": "Fairness in AI; Health disparities; Non-small cell lung cancer survival; Racial disparities",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "38820189",
      "title": "Equity and AI governance at academic medical centers.",
      "abstract": "OBJECTIVES: To understand whether and how equity is considered in artificial intelligence/machine learning governance processes at academic medical centers. STUDY DESIGN: Qualitative analysis of interview data. METHODS: We created a database of academic medical centers from the full list of Association of American Medical Colleges hospital and health system members in 2022. Stratifying by census region and restricting to nonfederal and nonspecialty centers, we recruited chief medical informatics officers and similarly positioned individuals from academic medical centers across the country. We created and piloted a semistructured interview guide focused on (1) how academic medical centers govern artificial intelligence and prediction and (2) to what extent equity is considered in these processes. A total of 17 individuals representing 13 institutions across 4 census regions of the US were interviewed. RESULTS: A minority of participants reported considering inequity, racism, or bias in governance. Most participants conceptualized these issues as characteristics of a tool, using frameworks such as algorithmic bias or fairness. Fewer participants conceptualized equity beyond the technology itself and asked broader questions about its implications for patients. Disparities in health information technology resources across health systems were repeatedly identified as a threat to health equity. CONCLUSIONS: We found a lack of consistent equity consideration among academic medical centers as they develop their governance processes for predictive technologies despite considerable national attention to the ways these technologies can cause or reproduce inequities. Health systems and policy makers will need to specifically prioritize equity literacy among health system leadership, design oversight policies, and promote critical engagement with these tools and their implications to prevent the further entrenchment of inequities in digital health care.",
      "authors": "Nong Paige; Hamasha Reema; Platt Jodyn",
      "year": "2024",
      "journal": "The American journal of managed care",
      "doi": "10.37765/ajmc.2024.89555",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38820189/",
      "mesh_terms": "Academic Medical Centers; Humans; United States; Artificial Intelligence; Qualitative Research; Health Equity; Interviews as Topic; Racism",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "39126673",
      "title": "Predictive roles of cognitive biases in health anxiety: A machine learning approach.",
      "abstract": "Prior work suggests that cognitive biases may contribute to health anxiety. Yet there is little research investigating how biased attention, interpretation, and memory for health threats are collectively associated with health anxiety, as well as the relative importance of these cognitive processes in predicting health anxiety. This study aimed to build a prediction model for health anxiety with multiple cognitive biases as potential predictors and to identify the biased cognitive processes that best predict individual differences in health anxiety. A machine learning algorithm (elastic net) was performed to recognise the predictors of health anxiety, using various tasks of attention, interpretation, and memory measured across behavioural, self-reported, and computational modelling approaches. Participants were 196 university students with a range of health anxiety severity from mild to severe. The results showed that only the interpretation bias for illness and the attention bias towards symptoms significantly contributed to the prediction model of health anxiety, with both biases having positive weights and the former being the most important predictor. These findings underscore the central role of illness-related interpretation bias and suggest that combined cognitive bias modification may be a promising method for alleviating health anxiety.",
      "authors": "Shi Congrong; Du Xiayu; Chen Wenke; Ren Zhihong",
      "year": "2024",
      "journal": "Stress and health : journal of the International Society for the Investigation of Stress",
      "doi": "10.1002/smi.3463",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39126673/",
      "mesh_terms": "Humans; Machine Learning; Male; Female; Young Adult; Adult; Anxiety; Cognition; Adolescent; Attention",
      "keywords": "attention bias; health anxiety; interpretation bias; machine learning; memory bias",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "38269933",
      "title": "Equitable Machine Learning for Hypoglycaemia Risk Management.",
      "abstract": "We developed a machine learning (ML) model for the detection of patients with high risk of hypoglycaemic events during their hospital stay to improve the detection and management of hypoglycaemia. Our model was trained on data from a regional local health care district in Australia. The model was found to have good predictive performance in the general case (AUC 0.837). We conducted subgroup analysis to ensure that the model performed in a way that did not disadvantage population subgroups, in this case based on gender or indigenous status. We found that our specific problem domain assisted us in reducing unwanted bias within the model, because it did not rely on practice patterns or subjective judgements for the outcome measure. With careful analysis for equity there is great potential for ML models to automate the detection of high-risk cohorts and automate mitigation strategies to reduce preventable errors.",
      "authors": "Rodriguez Jhordany; Padilla Daniel; Bruce Lenert; Thow Ben; Pradhan Malcolm",
      "year": "2024",
      "journal": "Studies in health technology and informatics",
      "doi": "10.3233/SHTI231089",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38269933/",
      "mesh_terms": "Humans; Hypoglycemia; Hypoglycemic Agents; Australia; Machine Learning; Risk Management",
      "keywords": "AI; Machine learning; diabetes; emr; equity; fairness; hypoglycaemia",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "41039915",
      "title": "Getting Started on Artificial Intelligence in Health Care and Clinical Research: Includes Rigor Checklist for Authors and Reviewers.",
      "abstract": "Artificial intelligence (AI) is rapidly transforming biomedical research and health care, offering new paradigms for discovery, diagnosis, and decision-making. This article provides a roadmap for researchers, clinicians, and reviewers seeking to understand and apply AI with rigor and relevance. It begins with a historical anchor: the birth of AI in health care at the University of Pittsburgh in the 1970s, where the INTERNIST-1 system pioneered diagnostic reasoning through symbolic logic, a milestone that laid the foundation for today's intelligent systems. Structured into three tiers-foundations, core techniques, and applications-the article addresses the full spectrum of biomedical AI. It introduces foundational concepts such as data engineering and preprocessing, knowledge representation and reasoning, and symbolic AI, which together enable structured, interpretable intelligence. Core techniques including expert systems, machine learning, deep learning, and explainable AI are presented with clinical examples, highlighting their role in wound care, image analysis, and predictive modeling. The applications tier showcases natural language processing, non-machine learning computer vision, robotics and automation, and distributed AI/multi-agent systems, demonstrating how AI integrates into real-world workflows. Ethical considerations and bias mitigation strategies are addressed with emphasis on Institutional Review Board oversight and fairness frameworks. Crucially, the article emphasizes that successful AI adoption begins not with technology, but with people. It outlines a systematic approach to building a biomedical AI workforce from within, empowering clinicians, researchers, and staff to become AI-literate contributors and leaders. With rigor checklists, practical guidance, and a vision for human-AI collaboration, this article invites readers to move beyond hype and toward responsible, transformative innovation in health care and biomedical science. [Figure: see text].",
      "authors": "Sen Chandan K; DeMazumder Deeptankar",
      "year": "2025",
      "journal": "Advances in wound care",
      "doi": "10.1177/21621918251380217",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41039915/",
      "mesh_terms": "",
      "keywords": "artificial intelligence; machine learning; smart care",
      "pub_types": "Editorial",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "38938093",
      "title": "Mitigating Racial Bias in Health Care Algorithms: Improving Fairness in Access to Supportive Housing.",
      "abstract": "Algorithms for guiding health care decisions have come under increasing scrutiny for being unfair to certain racial and ethnic groups. The authors describe their multistep process, using data from 3,465 individuals, to reduce racial and ethnic bias in an algorithm developed to identify state Medicaid beneficiaries experiencing homelessness and chronic health needs who were eligible for coordinated health care and housing supports. Through an iterative process of adjusting inputs, reviewing outputs with diverse stakeholders, and performing quality assurance, the authors developed an algorithm that achieved racial and ethnic parity in the selection of eligible Medicaid beneficiaries.",
      "authors": "Noam Krista R; Schmutte Timothy; Bory Christopher; Plant Robert W",
      "year": "2024",
      "journal": "Psychiatric services (Washington, D.C.)",
      "doi": "10.1176/appi.ps.20230359",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38938093/",
      "mesh_terms": "Humans; United States; Algorithms; Ill-Housed Persons; Medicaid; Racism; Health Services Accessibility; Healthcare Disparities; Housing",
      "keywords": "health equity; racial-ethnic disparities; research design and methodology; responsible AI",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "41354230",
      "title": "Development and validation of a novel machine learning-based algorithm to predict incident atrial fibrillation: A multicohort analysis.",
      "abstract": "BACKGROUND: Existing atrial fibrillation (AF) risk prediction models incorporate race as a covariate, systematically underestimating AF risk in black individuals and potentially perpetuating health care disparities. OBJECTIVE: This study aimed to develop and validate machine learning (ML)-based race-agnostic risk scores to predict AF risk and assess differences in risk stratification and bias compared with the CHARGE-AF score. METHODS: The derivation cohort included 16,719 participants free of AF at baseline (Atherosclerosis Risk in Communities visit 5, 2011-2013; Cardiovascular Health Study baseline, 1989-1990), and the validation cohort included 13,928 (Multi-Ethnic Study of Atherosclerosis and Framingham Offspring and Generation 3 studies). The primary outcome was the incidence of AF within 5 years. Model performance was assessed using concordance index, Brier score, and index of prediction accuracy. Bias was evaluated using disparate impact, equal opportunity difference, and Theil index. Population-attributable risk percentage was calculated across racial groups. RESULTS: During the 5-year follow-up, incident AF occurred in 507 participants (3.0%) in the derivation cohort and 262 (1.9%) in the validation cohort. The ML model demonstrated superior performance compared with CHARGE-AF, with better discrimination (concordance index 0.83 [95% confidence interval 0.80-0.85] vs 0.77 [95% confidence interval 0.74-0.79]; P < .001) and improved calibration (Brier score 1.82 vs 1.92; P < .001). Key predictors included age, clinical factors (electrocardiographic parameters, cardiac biomarkers, and blood pressure), and education level. Population-attributable risk analysis demonstrated marked racial differences in AF risk contribution from age (non-Hispanic black 14.3% vs white participants 34.6%). The ML model reduced algorithmic bias vs CHARGE-AF across all metrics. CONCLUSION: Race-agnostic ML models demonstrated superior predictive performance and reduced bias compared with CHARGE-AF, potentially improving clinical risk stratification while promoting health equity.",
      "authors": "Segar Matthew W; Keshvani Neil; Jaeger Byron; Rosenblatt Anna; Razavi Mehdi; Saeed Mohammad; Rodriguez Carlos J; Khan Sadiya S; Kao David; Pandey Ambarish",
      "year": "2025",
      "journal": "Heart rhythm",
      "doi": "10.1016/j.hrthm.2025.12.008",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41354230/",
      "mesh_terms": "",
      "keywords": "Algorithmic bias; Atrial fibrillation; Machine learning; Race; Risk prediction",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "38548006",
      "title": "Participant flow diagrams for health equity in AI.",
      "abstract": "Selection bias can arise through many aspects of a study, including recruitment, inclusion/exclusion criteria, input-level exclusion and outcome-level exclusion, and often reflects the underrepresentation of populations historically disadvantaged in medical research. The effects of selection bias can be further amplified when non-representative samples are used in artificial intelligence (AI) and machine learning (ML) applications to construct clinical algorithms. Building on the \"Data Cards\" initiative for transparency in AI research, we advocate for the addition of a participant flow diagram for AI studies detailing relevant sociodemographic and/or clinical characteristics of excluded participants across study phases, with the goal of identifying potential algorithmic biases before their clinical implementation. We include both a model for this flow diagram as well as a brief case study explaining how it could be implemented in practice. Through standardized reporting of participant flow diagrams, we aim to better identify potential inequities embedded in AI applications, facilitating more reliable and equitable clinical algorithms.",
      "authors": "Ellen Jacob G; Matos Jo\u00e3o; Viola Martin; Gallifant Jack; Quion Justin; Anthony Celi Leo; Abu Hussein Nebal S",
      "year": "2024",
      "journal": "Journal of biomedical informatics",
      "doi": "10.1016/j.jbi.2024.104631",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38548006/",
      "mesh_terms": "Humans; Artificial Intelligence; Health Equity; Algorithms; Machine Learning; Biomedical Research",
      "keywords": "Data cards; Flow diagram; Health equity; Machine learning; Selection bias",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "30794127",
      "title": "Can AI Help Reduce Disparities in General Medical and Mental Health Care?",
      "abstract": "BACKGROUND: As machine learning becomes increasingly common in health care applications, concerns have been raised about bias in these systems' data, algorithms, and recommendations. Simply put, as health care improves for some, it might not improve for all. METHODS: Two case studies are examined using a machine learning algorithm on unstructured clinical and psychiatric notes to predict intensive care unit (ICU) mortality and 30-day psychiatric readmission with respect to race, gender, and insurance payer type as a proxy for socioeconomic status. RESULTS: Clinical note topics and psychiatric note topics were heterogenous with respect to race, gender, and insurance payer type, which reflects known clinical findings. Differences in prediction accuracy and therefore machine bias are shown with respect to gender and insurance type for ICU mortality and with respect to insurance policy for psychiatric 30-day readmission. CONCLUSIONS: This analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care.",
      "authors": "Chen Irene Y; Szolovits Peter; Ghassemi Marzyeh",
      "year": "2019",
      "journal": "AMA journal of ethics",
      "doi": "10.1001/amajethics.2019.167",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30794127/",
      "mesh_terms": "Adult; Aged; Aged, 80 and over; Artificial Intelligence; Delivery of Health Care; Female; Healthcare Disparities; Humans; Intensive Care Units; Male; Mental Health Services; Middle Aged; Mortality; Patient Readmission; Sex Factors",
      "keywords": "",
      "pub_types": "Comparative Study; Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "37130756",
      "title": "Addressing bias in artificial intelligence for public health surveillance.",
      "abstract": "Components of artificial intelligence (AI) for analysing social big data, such as natural language processing (NLP) algorithms, have improved the timeliness and robustness of health data. NLP techniques have been implemented to analyse large volumes of text from social media platforms to gain insights on disease symptoms, understand barriers to care and predict disease outbreaks. However, AI-based decisions may contain biases that could misrepresent populations, skew results or lead to errors. Bias, within the scope of this paper, is described as the difference between the predictive values and true values within the modelling of an algorithm. Bias within algorithms may lead to inaccurate healthcare outcomes and exacerbate health disparities when results derived from these biased algorithms are applied to health interventions. Researchers who implement these algorithms must consider when and how bias may arise. This paper explores algorithmic biases as a result of data collection, labelling and modelling of NLP algorithms. Researchers have a role in ensuring that efforts towards combating bias are enforced, especially when drawing health conclusions derived from social media posts that are linguistically diverse. Through the implementation of open collaboration, auditing processes and the development of guidelines, researchers may be able to reduce bias and improve NLP algorithms that improve health surveillance.",
      "authors": "Flores Lidia; Kim Seungjun; Young Sean D",
      "year": "2024",
      "journal": "Journal of medical ethics",
      "doi": "10.1136/jme-2022-108875",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37130756/",
      "mesh_terms": "Humans; Artificial Intelligence; Public Health Surveillance; Bias; Data Collection; Disease Outbreaks",
      "keywords": "decision making; ethics; ethics- medical; ethics- research; information technology",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "39176898",
      "title": "How Data Infrastructure Deals with Bias Problems in Medical Imaging.",
      "abstract": "The paper discusses biases in medical imaging analysis, particularly focusing on the challenges posed by the development of machine learning algorithms and generative models. It introduces a taxonomy of bias problems and addresses them through a data infrastructure initiative: the PADME (Platform for Analytics and Distributed Machine-Learning for Enterprises), which is a part of the National Research Data Infrastructure for Personal Health Data (NFDI4Health) project. The PADME facilitates the structuring and sharing of health data while ensuring privacy and adherence to FAIR principles. The paper presents experimental results that show that generative methods can be effective in data augmentation. Complying with PADME infrastructure, this work proposes a solution framework to deal with bias in the different data stations and preserve privacy when transferring images. It highlights the importance of standardized data infrastructure in mitigating biases and promoting FAIR, reusable, and privacy-preserving research environments in healthcare.",
      "authors": "Li Feifei; Kutafina Ekaterina; Schoneck Mirjam; Caldeira Liliana Lourenco; Beyan Oya",
      "year": "2024",
      "journal": "Studies in health technology and informatics",
      "doi": "10.3233/SHTI240517",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39176898/",
      "mesh_terms": "Diagnostic Imaging; Humans; Machine Learning; Bias; Algorithms; Confidentiality; Computer Security",
      "keywords": "Bias; Data Infrastructure; Differential Privacy; Federated Learning; Machine Learning; Medical Imaging",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "35685000",
      "title": "Designing Equitable Health Care Outreach Programs From Machine Learning Patient Risk Scores.",
      "abstract": "There is growing interest in ensuring equity and guarding against bias in the use of risk scores produced by machine learning and artificial intelligence models. Risk scores are used to select patients who will receive outreach and support. Inappropriate use of risk scores, however, can perpetuate disparities. Commonly advocated solutions to improve equity are nontrivial to implement and may not pass legal scrutiny. In this article, we introduce pragmatic tools that support better use of risk scores for more equitable outreach programs. Our model output charts allow modeling and care management teams to see the equity consequences of different threshold choices and to select the optimal risk thresholds to trigger outreach. For best results, as with any health equity tool, we recommend that these charts be used by a diverse team and shared with relevant stakeholders.",
      "authors": "Hane Christopher A; Wasserman Melanie",
      "year": "2023",
      "journal": "Medical care research and review : MCRR",
      "doi": "10.1177/10775587221098831",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35685000/",
      "mesh_terms": "Humans; Artificial Intelligence; Delivery of Health Care; Machine Learning",
      "keywords": "artificial intelligence; civil rights; health care disparities; health equity; health status disparities; structural inequity",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "35914194",
      "title": "Predicting Race And Ethnicity To Ensure Equitable Algorithms For Health Care Decision Making.",
      "abstract": "Algorithms are currently used to assist in a wide array of health care decisions. Despite the general utility of these health care algorithms, there is growing recognition that they may lead to unintended racially discriminatory practices, raising concerns about the potential for algorithmic bias. An intuitive precaution against such bias is to remove race and ethnicity information as an input to health care algorithms, mimicking the idea of \"race-blind\" decisions. However, we argue that this approach is misguided. Knowledge, not ignorance, of race and ethnicity is necessary to combat algorithmic bias. When race and ethnicity are observed, many methodological approaches can be used to enforce equitable algorithmic performance. When race and ethnicity information is unavailable, which is often the case, imputing them can expand opportunities to not only identify and assess algorithmic bias but also combat it in both clinical and nonclinical settings. A valid imputation method, such as Bayesian Improved Surname Geocoding, can be applied to standard data collected by public and private payers and provider entities. We describe two applications in which imputation of race and ethnicity can help mitigate potential algorithmic biases: equitable disease screening algorithms using machine learning and equitable pay-for-performance incentives.",
      "authors": "Cabreros Irineo; Agniel Denis; Martino Steven C; Damberg Cheryl L; Elliott Marc N",
      "year": "2022",
      "journal": "Health affairs (Project Hope)",
      "doi": "10.1377/hlthaff.2022.00095",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35914194/",
      "mesh_terms": "Algorithms; Bayes Theorem; Decision Making; Delivery of Health Care; Ethnicity; Humans; Reimbursement, Incentive",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "37579574",
      "title": "Algorithmic bias in artificial intelligence is a problem-And the root issue is power.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) in health care continues to expand at a rapid rate, impacting both nurses and communities we accompany in care. PURPOSE: We argue algorithmic bias is but a symptom of a more systemic and longstanding problem: power imbalances related to the creation, development, and use of health care technologies. METHODS: This commentary responds to Drs. O'Connor and Booth's 2022 article, \"Algorithmic bias in health care: Opportunities for nurses to improve equality in the age of artificial intelligence.\" DISCUSSION: Nurses need not 'reinvent the wheel' when it comes to AI policy, curricula, or ethics. We can and should follow the lead of communities already working 'from the margins' who provide ample guidance. CONCLUSION: Its neither feasible nor just to expect individual nurses to counter systemic injustice in health care through individual actions, more technocentric curricula, or industry partnerships. We need disciplinary supports for collective action to renegotiate power for AI tech.",
      "authors": "Walker Rae; Dillard-Wright Jess; Iradukunda Favorite",
      "year": "2023",
      "journal": "Nursing outlook",
      "doi": "10.1016/j.outlook.2023.102023",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37579574/",
      "mesh_terms": "Humans; Artificial Intelligence; Delivery of Health Care",
      "keywords": "Algorithms; Artificial intelligence; Bias; Machine learning; Nursing ethics; Racism",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "38876452",
      "title": "Identify and mitigate bias in electronic phenotyping: A comprehensive study from computational perspective.",
      "abstract": "Electronic phenotyping is a fundamental task that identifies the special group of patients, which plays an important role in precision medicine in the era of digital health. Phenotyping provides real-world evidence for other related biomedical research and clinical tasks, e.g., disease diagnosis, drug development, and clinical trials, etc. With the development of electronic health records, the performance of electronic phenotyping has been significantly boosted by advanced machine learning techniques. In the healthcare domain, precision and fairness are both essential aspects that should be taken into consideration. However, most related efforts are put into designing phenotyping models with higher accuracy. Few attention is put on the fairness perspective of phenotyping. The neglection of bias in phenotyping leads to subgroups of patients being underrepresented which will further affect the following healthcare activities such as patient recruitment in clinical trials. In this work, we are motivated to bridge this gap through a comprehensive experimental study to identify the bias existing in electronic phenotyping models and evaluate the widely-used debiasing methods' performance on these models. We choose pneumonia and sepsis as our phenotyping target diseases. We benchmark 9 kinds of electronic phenotyping methods spanning from rule-based to data-driven methods. Meanwhile, we evaluate the performance of the 5 bias mitigation strategies covering pre-processing, in-processing, and post-processing. Through the extensive experiments, we summarize several insightful findings from the bias identified in the phenotyping and key points of the bias mitigation strategies in phenotyping.",
      "authors": "Ding Sirui; Zhang Shenghan; Hu Xia; Zou Na",
      "year": "2024",
      "journal": "Journal of biomedical informatics",
      "doi": "10.1016/j.jbi.2024.104671",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38876452/",
      "mesh_terms": "Bias; Benchmarking; Data Mining; Electronic Health Records; Algorithms; Pneumonia; Sepsis; Cohort Studies; Machine Learning; Phenotype; Humans; Male; Female; Racial Groups; Sex Factors; Datasets as Topic",
      "keywords": "Algorithm fairness; Bias mitigation; Electronic phenotyping; Fairness in healthcare",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "40776043",
      "title": "Algorithmic Fairness in Machine Learning Prediction of Autism Using Electronic Health Records.",
      "abstract": "Efforts to improve early diagnosis of autism spectrum disorder (ASD) in children are beginning to use machine learning (ML) approaches applied to real-world clinical datasets, such as electronic health records (EHRs). However, sex-based disparities in ASD diagnosis highlight the need for fair prediction models that ensure equitable performance across demographic groups for ASD identification. This retrospective case-control study aimed to develop ML-based prediction models for ASD diagnosis using risk factors found in EHRs and assess their algorithmic fairness. The study cohorts included 70,803 children diagnosed with ASD and 212,409 matched controls without ASD. We built logistic regression and Xgboost models and evaluated their performance using standard metrics, including accuracy, recall, precision, F1-score, and area under the curve (AUC). To assess fairness, we examined model performance by sex and calculated fairness-specific metrics, such as equal opportunity (recall parity) and equalized odds, to identify potential biases in model predictions between boys and girls. Our results revealed significant fairness issues in ML models for ASD prediction using EHRs.",
      "authors": "Angell Amber M; Li Yongqiu; Bian Jiang; Parchment Camille; Yin Larry; Chamala Srikar; Hakimjavadi Hesamedin; Thompson Lindsay; Guo Yi",
      "year": "2025",
      "journal": "Studies in health technology and informatics",
      "doi": "10.3233/SHTI251025",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40776043/",
      "mesh_terms": "Humans; Electronic Health Records; Machine Learning; Male; Female; Retrospective Studies; Case-Control Studies; Child; Autism Spectrum Disorder; Algorithms; Child, Preschool",
      "keywords": "Autism; electronic health records; predictive modeling",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "33328054",
      "title": "Ethical limitations of algorithmic fairness solutions in health care machine learning.",
      "abstract": "",
      "authors": "McCradden Melissa D; Joshi Shalmali; Mazwi Mjaye; Anderson James A",
      "year": "2020",
      "journal": "The Lancet. Digital health",
      "doi": "10.1016/S2589-7500(20)30065-0",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33328054/",
      "mesh_terms": "Algorithms; Delivery of Health Care; Female; Health Equity; Humans; Machine Learning; Male; Models, Biological; Social Justice",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "39731446",
      "title": "De-biasing the bias: methods for improving disparity assessments with noisy group measurements.",
      "abstract": "Health care decisions are increasingly informed by clinical decision support algorithms, but these algorithms may perpetuate or increase racial and ethnic disparities in access to and quality of health care. Further complicating the problem, clinical data often have missing or poor quality racial and ethnic information, which can lead to misleading assessments of algorithmic bias. We present novel statistical methods that allow for the use of probabilities of racial/ethnic group membership in assessments of algorithm performance and quantify the statistical bias that results from error in these imputed group probabilities. We propose a sensitivity analysis approach to estimating the statistical bias that allows practitioners to assess disparities in algorithm performance under a range of assumed levels of group probability error. We also prove theoretical bounds on the statistical bias for a set of commonly used fairness metrics and describe real-world scenarios where our theoretical results are likely to apply. We present a case study using imputed race and ethnicity from the modified Bayesian Improved First and Surname Geocoding algorithm for estimation of disparities in a clinical decision support algorithm used to inform osteoporosis treatment. Our novel methods allow policymakers to understand the range of potential disparities under a given algorithm even when race and ethnicity information is missing and to make informed decisions regarding the implementation of machine learning for clinical decision support.",
      "authors": "Wastvedt Solvejg; Snoke Joshua; Agniel Denis; Lai Julie; Elliott Marc N; Martino Steven C",
      "year": "2024",
      "journal": "Biometrics",
      "doi": "10.1093/biomtc/ujae155",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39731446/",
      "mesh_terms": "Humans; Algorithms; Bias; Bayes Theorem; Healthcare Disparities; Ethnicity; Osteoporosis; Racial Groups; Decision Support Systems, Clinical; Biometry; Models, Statistical",
      "keywords": "Bayesian improved surname geocoding; algorithmic fairness; race imputation; sensitivity analysis",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "41482239",
      "title": "Interpretable machine learning for cognitive impairment screening: Development and external validation of a clinical prediction model based on NHANES data.",
      "abstract": "BACKGROUND: Cognitive impairment in older adults poses a growing public health challenge, yet accessible screening tools remain limited. We aimed to develop and validate an interpretable machine learning model for cognitive impairment prediction by routinely collecting clinical data. METHODS: We analyzed 1061 participants from the U.S. National Health and Nutrition Examination Survey (NHANES 2011-2014). Feature selection combined multivariable regression, restricted cubic splines, and the Boruta algorithm to identify 40 clinical, demographic, and socioeconomic variables. Twelve machine learning models (including Support Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), and Random Forest (RF)) were trained and externally validated on NHANES 2001-2002 (n\u00a0=\u00a0531). Model performance was evaluated by area under the receiver operating characteristic curve (AUC-ROC), calibration (Brier score), accuracy, and sensitivity. Additionally, an assessment of fairness was conducted across racial subgroups. Interpretability was enhanced via SHapley Additive exPlanations (SHAP). RESULTS: The SVM model demonstrated optimal generalizability, achieving an external validation AUC of 0.8265 (95\u00a0%CI: 0.7867-0.8582) with sustained calibration (Brier score\u00a0=\u00a00.1703). Subgroup analyses showed no statistically significant AUC differences (all P\u00a0>\u00a00.05). SHAP analysis identified socioeconomic factors, systemic inflammation indices, and metabolic markers as key predictors. LIMITATIONS: Generalizability may be limited to U.S. populations, and unmeasured biomarkers (e.g., amyloid-\u03b2) could affect prediction accuracy. Subgroup analyses for minorities were constrained by sample size. CONCLUSION: Our interpretable prediction strategy enables rapid cognitive risk assessment using routine clinical data, providing a cost-effective decision support tool adaptable to electronic health record systems.",
      "authors": "Chen Kang; Yu Guran; Li Hao",
      "year": "2026",
      "journal": "Experimental gerontology",
      "doi": "10.1016/j.exger.2025.113019",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41482239/",
      "mesh_terms": "Humans; Cognitive Dysfunction; Male; Female; Aged; Nutrition Surveys; Machine Learning; Middle Aged; United States; ROC Curve; Support Vector Machine; Mass Screening; Area Under Curve; Aged, 80 and over",
      "keywords": "Cognitive impairment; Machine learning; NHANES; Prediction model; SHAP",
      "pub_types": "Journal Article; Validation Study",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "39868946",
      "title": "AI for all: bridging data gaps in machine learning and health.",
      "abstract": "Artificial intelligence (AI) and its subset, machine learning, have tremendous potential to transform health care, medicine, and population health through improved diagnoses, treatments, and patient care. However, the effectiveness of these technologies hinges on the quality and diversity of the data used to train them. Many datasets currently used in machine learning are inherently biased and lack diversity, leading to inaccurate predictions that may perpetuate existing health disparities. This commentary highlights the challenges of biased datasets, the impact on marginalized communities, and the critical need for strategies to address these disparities throughout the research continuum. To overcome these challenges, it is essential to adopt more inclusive data collection practices, engage collaboratively with community stakeholders, and leverage innovative approaches like federated learning. These steps can help mitigate bias and enhance the accuracy and fairness of AI-assisted or informed\u00a0health care solutions. By addressing systemic biases embedded across research phases, we can build a better foundation for AI to enhance diagnostic and treatment capabilities and move society closer to the goal where improved health and\u00a0health care can be a fundamental right for all, and not just for some.",
      "authors": "Wang Monica L; Bertrand Kimberly A",
      "year": "2025",
      "journal": "Translational behavioral medicine",
      "doi": "10.1093/tbm/ibae075",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39868946/",
      "mesh_terms": "Humans; Machine Learning; Artificial Intelligence; Delivery of Health Care",
      "keywords": "artificial intelligence; data practices; disparities; health equity; inclusion; machine learning; medicine",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "37097792",
      "title": "Post-Processing Fairness Evaluation of Federated Models: An Unsupervised Approach in Healthcare.",
      "abstract": "Modern Healthcare cyberphysical systems have begun to rely more and more on distributed AI leveraging the power of Federated Learning (FL). Its ability to train Machine Learning (ML) and Deep Learning (DL) models for the wide variety of medical fields, while at the same time fortifying the privacy of the sensitive information that are present in the medical sector, makes the FL technology a necessary tool in modern health and medical systems. Unfortunately, due to the polymorphy of distributed data and the shortcomings of distributed learning, the local training of Federated models sometimes proves inadequate and thus negatively imposes the federated learning optimization process and in extend in the subsequent performance of the rest Federated models. Badly trained models can cause dire implications in the healthcare field due to their critical nature. This work strives to solve this problem by applying a post-processing pipeline to models used by FL. In particular, the proposed work ranks the model by finding how fair they are by discovering and inspecting micro-Manifolds that cluster each neural model's latent knowledge. The produced work applies a completely unsupervised both model and data agnostic methodology that can be leveraged for general model fairness discovery. The proposed methodology is tested against a variety of benchmark DL architectures and in the FL environment, showing an average 8.75% increase in Federated model accuracy in comparison with similar work.",
      "authors": "Siniosoglou Ilias; Argyriou Vasileios; Sarigiannidis Panagiotis; Lagkas Thomas; Sarigiannidis Antonios; Goudos Sotirios K; Wan Shaohua",
      "year": "2023",
      "journal": "IEEE/ACM transactions on computational biology and bioinformatics",
      "doi": "10.1109/TCBB.2023.3269767",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37097792/",
      "mesh_terms": "Benchmarking; Machine Learning; Delivery of Health Care",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "38554626",
      "title": "Comparing survival of older ovarian cancer patients treated with neoadjuvant chemotherapy versus primary cytoreductive surgery: Reducing bias through machine learning.",
      "abstract": "OBJECTIVE: To develop and evaluate a multidimensional comorbidity index (MCI) that identifies ovarian cancer patients at risk of early mortality more accurately than the Charlson Comorbidity Index (CCI) for use in health services research. METHODS: We utilized SEER-Medicare data to identify patients with stage IIIC and IV ovarian cancer, diagnosed in 2010-2015. We employed partial least squares regression, a supervised machine learning algorithm, to develop the MCI by extracting latent factors that optimally captured the variation in health insurance claims made in the year preceding cancer diagnosis, and 1-year mortality. We assessed the discrimination and calibration of the MCI for 1-year mortality and compared its performance to the commonly-used CCI. Finally, we evaluated the MCI's ability to reduce confounding in the association of neoadjuvant chemotherapy (NACT) and all-cause mortality. RESULTS: We included 4723 patients in the development cohort and 933 in the validation cohort. The MCI demonstrated good discrimination for 1-year mortality (c-index: 0.75, 95% CI: 0.72-0.79), while the CCI had poor discrimination (c-index: 0.59, 95% CI: 0.56-0.63). Calibration plots showed better agreement between predicted and observed 1-year mortality risk for the MCI compared with CCI. When comparing all-cause mortality between NACT with primary cytoreductive surgery, NACT was associated with a higher hazard of death (HR: 1.13, 95% CI: 1.04-1.23) after controlling for tumor characteristics, demographic factors, and the CCI. However, when controlling for the MCI instead of the CCI, there was no longer a significant difference (HR: 1.05, 95% CI: 0.96-1.14). CONCLUSIONS: The MCI outperformed the conventional CCI in predicting 1-year mortality, and reducing confounding due to differences in baseline health status in comparative effectiveness analysis of NACT versus primary surgery.",
      "authors": "Huang Yongmei; Rauh-Hain J Alejandro; McCoy Thomas H; Hou June Y; Hillyer Grace; Ferris Jennifer S; Hershman Dawn; Wright Jason D; Melamed Alexander",
      "year": "2024",
      "journal": "Gynecologic oncology",
      "doi": "10.1016/j.ygyno.2024.03.016",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38554626/",
      "mesh_terms": "Humans; Female; Cytoreduction Surgical Procedures; Neoadjuvant Therapy; Aged; Ovarian Neoplasms; Machine Learning; SEER Program; Aged, 80 and over; United States; Chemotherapy, Adjuvant; Bias; Carcinoma, Ovarian Epithelial; Neoplasm Staging; Medicare",
      "keywords": "All-cause mortality; Machine learning; Multidimensional comorbidity index; Neoadjuvant chemotherapy; Primary cytoreductive surgery",
      "pub_types": "Journal Article; Comparative Study; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "31278181",
      "title": "Benefits, Pitfalls, and Potential Bias in Health Care AI.",
      "abstract": "As the health care industry adopts artificial intelligence, machine learning, and other modeling techniques, it is seeing benefits to both patient outcomes and cost reduction; however, it needs to be cognizant of and ensure proper management of the risks, including bias. Lessons learned from other industries may provide a framework for acknowledging and managing data, machine, and human biases that arise while implementing AI.",
      "authors": "Hague Douglas C",
      "year": "2019",
      "journal": "North Carolina medical journal",
      "doi": "10.18043/ncm.80.4.219",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31278181/",
      "mesh_terms": "Artificial Intelligence; Bias; Data Analysis; Delivery of Health Care; Humans; Machine Learning",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "35728803",
      "title": "Reducing Nonresponse and Data Linkage Consent Bias in Large-Scale Panel Surveys.",
      "abstract": "Selection bias is an ongoing concern in large-scale panel surveys where the cumulative effects of unit nonresponse increase at each subsequent wave of\u00a0data collection. A second source of selection bias in panel studies is the inability to link respondents to supplementary administrative records, either because respondents do not consent to link or the matching algorithm fails to locate their administrative records. Both sources of selection bias can affect the validity of conclusions drawn from these data sources. In this article, I discuss recently proposed methods of reducing both sources of selection bias in panel studies, with a special emphasis on reducing selection bias in the US Health and Retirement Study.",
      "authors": "Sakshaug Joseph W",
      "year": "2022",
      "journal": "Forum for health economics & policy",
      "doi": "10.1515/fhep-2021-0060",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35728803/",
      "mesh_terms": "Bias; Surveys and Questionnaires; Selection Bias; Longitudinal Studies; Information Storage and Retrieval",
      "keywords": "health and retirement study; post-survey adjustments; questionnaire design; selection bias",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "37819812",
      "title": "Bipartite Ranking Fairness Through a Model Agnostic Ordering Adjustment.",
      "abstract": "Recently, with the applications of algorithms in various risky scenarios, algorithmic fairness has been a serious concern and received lots of interest in machine learning community. In this article, we focus on the bipartite ranking scenario, where the instances come from either the positive or negative class and the goal is to learn a ranking function that ranks positive instances higher than negative ones. We are interested in whether the learned ranking function can cause systematic disparity across different protected groups defined by sensitive attributes. While there could be a trade-off between fairness and performance, we propose a model agnostic post-processing framework xOrder for achieving fairness in bipartite ranking and maintaining the algorithm classification performance. In particular, we optimize a weighted sum of the utility as identifying an optimal warping path across different protected groups and solve it through a dynamic programming process. xOrder is compatible with various classification models and ranking fairness metrics, including supervised and unsupervised fairness metrics. In addition to binary groups, xOrder can be applied to multiple protected groups. We evaluate our proposed algorithm on four benchmark data sets and two real-world patient electronic health record repositories. xOrder consistently achieves a better balance between the algorithm utility and ranking fairness on a variety of datasets with different metrics. From the visualization of the calibrated ranking scores, xOrder mitigates the score distribution shifts of different groups compared with baselines. Moreover, additional analytical results verify that xOrder achieves a robust performance when faced with fewer samples and a bigger difference between training and testing ranking score distributions.",
      "authors": "Cui Sen; Pan Weishen; Zhang Changshui; Wang Fei",
      "year": "2023",
      "journal": "IEEE transactions on pattern analysis and machine intelligence",
      "doi": "10.1109/TPAMI.2023.3290949",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37819812/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "33315263",
      "title": "The Emerging Hazard of AI-Related Health Care Discrimination.",
      "abstract": "Artificial intelligence holds great promise for improved health-care outcomes. But it also poses substantial new hazards, including algorithmic discrimination. For example, an algorithm used to identify candidates for beneficial \"high risk care management\" programs routinely failed to select racial minorities. Furthermore, some algorithms deliberately adjust for race in ways that divert resources away from minority patients. To illustrate, algorithms have underestimated African Americans' risks of kidney stones and death from heart failure. Algorithmic discrimination can violate Title VI of the Civil Rights Act and Section 1557 of the Affordable Care Act when it unjustifiably disadvantages underserved populations. This article urges that both legal and technical tools be deployed to promote AI fairness. Plaintiffs should be able to assert disparate impact claims in health-care litigation, and Congress should enact an Algorithmic Accountability Act. In addition, fairness should be a key element in designing, implementing, validating, and employing AI.",
      "authors": "Hoffman Sharona",
      "year": "2021",
      "journal": "The Hastings Center report",
      "doi": "10.1002/hast.1203",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33315263/",
      "mesh_terms": "Artificial Intelligence; Civil Rights; Delivery of Health Care; Humans; Minority Groups; Patient Protection and Affordable Care Act; United States",
      "keywords": "algorithmic fairness; artificial intelligence; civil rights; discrimination; disparate impact",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "40550353",
      "title": "Dental services use prediction among adults in Southern Brazil: A gender and racial fairness-oriented machine learning approach.",
      "abstract": "OBJECTIVE: To develop machine learning models to predict the use of dental services among adults aged 18 and older. METHODS: This is a prospective cohort study that uses data from the survey \"EAI Pelotas?\". The sample consisted of individuals who participated in both the baseline and follow-up, totaling 3461 people. Predictors were collected as baseline and comprised 47 sociodemographic, behavioral, oral and general health characteristics. The outcome was dental service use in the last year assessed during the one-year follow-up. Data was divided into training (80 %) and test (20 %) sets. Five machine learning models were tested. Hyperparameter tuning was optimized through 10-fold cross-validation, utilizing 30 iterations. Model performance was assessed based on the area under the Receiver Operating Characteristic (ROC) curve (AUC), accuracy, recall, precision, and F1-score. RESULTS: The prevalence of dental service use in the follow-up was 47.2 % (95 % CI, 45.5 - 48.9). All models in the test set demonstrated an AUC-ROC between 0.76 and 0.77. The CatBoost Classifier model exhibited the highest performance in the test dataset among the models concerning the AUC metric (AUC = 0.77, CI95 %,[0.73-0.80]), displaying an accuracy = 0.69, recall = 0.69, precision = 0.68, and F1-score = 0.69. Fairness estimations for the best model indicated consistent performance across gender categories. However, disparities were observed among racial groups, AUC = 0.57 for individuals who self-reported mixed (\"pardos\") skin color. The explainability analysis shows that the most important features were the last dental visit at baseline and education level. CONCLUSION: Despite our findings suggesting a sufficient prediction of overall dental services' use, performance varied across racial groups. CLINICAL SIGNIFICANCE: Our findings highlight the potential of machine learning models to predict dental service use with good overall accuracy. However, the significantly lower performance for mixed-race individuals raises concerns about fairness and equity. Therefore, despite promising results, the model requires further refinement before it can be applied in real-world public health settings.",
      "authors": "Chisini Luiz Alexandre; Ara\u00fajo C\u00ednthia Fonseca; Delpino Felipe Mendes; Figueiredo L\u00edlian Munhoz; Filho Alexandre Dias Porto Chiavegatto; Schuch Helena Silveira; Nunes Bruno Pereira; Demarco Fl\u00e1vio Fernando",
      "year": "2025",
      "journal": "Journal of dentistry",
      "doi": "10.1016/j.jdent.2025.105929",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40550353/",
      "mesh_terms": "Humans; Adult; Female; Male; Machine Learning; Prospective Studies; Middle Aged; Brazil; Sex Factors; Young Adult; Dental Health Services; Adolescent; Aged; Dental Care; ROC Curve",
      "keywords": "Artificial intelligence; Dental health services; Longitudinal study; Machine learning; Oral health",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "29998588",
      "title": "Technology-induced bias in the theory of evidence-based medicine.",
      "abstract": "Designing trials and studies to minimize confounding and bias is central to evidence-based medicine (EBM). The widespread use of recent technologies such as machine learning, smartphones, and the World Wide Web to collect, analyse, and disseminate information can improve the efficiency, reliability, and availability of medical research. However, it also has the potential to introduce new sources of significant, technology-induced evidential bias. This paper assesses the extent of the impact by reviewing some of the methods by and principles according to which evidence is collected, analysed, and disseminated in EBM, supported by specific examples. It considers the effect of personal health tracking via smartphones, the current proliferation of research data and the influence of search engine \"filter bubbles\", the possibility of machine learning-driven study design, and the implications of using machine learning to seek patterns in large quantities of data, for example from observational studies and medical record databases. It concludes that new technology may introduce profound new sources of bias that current EBM frameworks do not accommodate. It also proposes new approaches that could be incorporated in to EBM theory to mitigate the most obvious risks, and suggests where further assessment of the practical implications is needed.",
      "authors": "Eustace Scott",
      "year": "2018",
      "journal": "Journal of evaluation in clinical practice",
      "doi": "10.1111/jep.12972",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29998588/",
      "mesh_terms": "Bias; Biomedical Research; Biomedical Technology; Evidence-Based Medicine; Humans; Information Dissemination; Medical Informatics; Philosophy, Medical; Reproducibility of Results; Research Design",
      "keywords": "data proliferation; evidence-based medicine; machine learning; philosophy of medicine; search engine; technology-induced bias",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "38925281",
      "title": "Assessing racial bias in healthcare predictive models: Practical lessons from an empirical evaluation of 30-day hospital readmission models.",
      "abstract": "OBJECTIVE: Despite increased availability of methodologies to identify algorithmic bias, the operationalization of bias evaluation for healthcare predictive models is still limited. Therefore, this study proposes a process for bias evaluation through an empirical assessment of common hospital readmission models. The process includes selecting bias measures, interpretation, determining disparity impact and potential mitigations. METHODS: This retrospective analysis evaluated racial bias of four common models predicting 30-day unplanned readmission (i.e., LACE Index, HOSPITAL Score, and the CMS readmission measure applied as is and retrained). The models were assessed using 2.4 million adult inpatient discharges in Maryland from 2016 to 2019. Fairness metrics that are model-agnostic, easy to compute, and interpretable were implemented and apprised to select the most appropriate bias measures. The impact of changing model's risk thresholds on these measures was further assessed to guide the selection of optimal thresholds to control and mitigate bias. RESULTS: Four bias measures were selected for the predictive task: zero-one-loss difference, false negative rate (FNR) parity, false positive rate (FPR) parity, and generalized entropy index. Based on these measures, the HOSPITAL score and the retrained CMS measure demonstrated the lowest racial bias. White patients showed a higher FNR while Black patients resulted in a higher FPR and zero-one-loss. As the models' risk threshold changed, trade-offs between models' fairness and overall performance were observed, and the assessment showed all models' default thresholds were reasonable for balancing accuracy and bias. CONCLUSIONS: This study proposes an Applied Framework to Assess Fairness of Predictive Models (AFAFPM) and demonstrates the process using 30-day hospital readmission model as the example. It suggests the feasibility of applying algorithmic bias assessment to determine optimized risk thresholds so that predictive models can be used more equitably and accurately. It is evident that a combination of qualitative and quantitative methods and a multidisciplinary team are necessary to identify, understand and respond to algorithm bias in real-world healthcare settings. Users should also apply multiple bias measures to ensure a more comprehensive, tailored, and balanced view. The results of bias measures, however, must be interpreted with caution and consider the larger operational, clinical, and policy context.",
      "authors": "Wang H Echo; Weiner Jonathan P; Saria Suchi; Lehmann Harold; Kharrazi Hadi",
      "year": "2024",
      "journal": "Journal of biomedical informatics",
      "doi": "10.1016/j.jbi.2024.104683",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38925281/",
      "mesh_terms": "Humans; Patient Readmission; Racism; Retrospective Studies; Male; Female; Middle Aged; Adult; Aged; Maryland; Algorithms; Healthcare Disparities",
      "keywords": "Algorithmic Bias; Algorithmic Fairness; Health Disparity; Hospital Readmission; Population Health Management; Predictive Models",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "41005257",
      "title": "Fairness in machine learning-based hand load estimation: A case study on load carriage tasks.",
      "abstract": "Predicting external hand load from sensor data is essential for ergonomic exposure assessments, as obtaining this information typically requires direct observation or supplementary data. While machine learning can estimate hand load from posture or force data, we found systematic bias tied to biological sex, with predictive disparities worsening in imbalanced training datasets. To address this, we developed a fair predictive model using a Variational Autoencoder with feature disentanglement, which separates sex-agnostic from sex-specific motion features. This enables predictions based only on sex-agnostic patterns. Our proposed algorithm outperformed conventional machine learning models, including k-Nearest Neighbors, Support Vector Machine, and Random Forest, achieving a mean absolute error of 3.42 and improving fairness metrics like statistical parity and positive and negative residual differences, even when trained on imbalanced sex datasets. These results underscore the importance of fairness-aware algorithms in avoiding health and safety disadvantages for specific worker groups in the workplace.",
      "authors": "Rahman Arafat; Lim Sol; Chung Seokhyun",
      "year": "2025",
      "journal": "Applied ergonomics",
      "doi": "10.1016/j.apergo.2025.104642",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41005257/",
      "mesh_terms": "",
      "keywords": "Algorithmic bias; Fairness; Gait kinematics; Load carriage; Machine learning",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "39541598",
      "title": "Reducing bias in healthcare artificial intelligence: A white paper.",
      "abstract": "Objective: Mitigation of racism in artificial intelligence (AI) is needed to improve health outcomes, yet no consensus exists on how this might be achieved. Methods: At an international conference in 2022, experts gathered to discuss strategies for reducing bias in healthcare AI. Results: This paper delineates these strategies along with their corresponding strengths and weaknesses and reviews the existing literature on these strategies. Conclusions: Five major themes resulted: reducing dataset bias, accurate modeling of existing data, transparency of artificial intelligence, regulation of artificial intelligence and the people who develop it, and bringing stakeholders to the table.",
      "authors": "Sun Carolyn; Harris Shannon L",
      "year": "2024",
      "journal": "Health informatics journal",
      "doi": "10.1177/14604582241291410",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39541598/",
      "mesh_terms": "Artificial Intelligence; Humans; Bias; Racism; Delivery of Health Care",
      "keywords": "AI; bias; healthcare; machine learning; policy",
      "pub_types": "Journal Article; Consensus Statement",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "41512215",
      "title": "Understanding algorithmic fairness for clinical prediction in terms of subgroup net benefit and health equity.",
      "abstract": "There are concerns about the fairness of clinical prediction models. 'Fair' models are defined as those for which their performance or predictions are not inappropriately influenced by protected attributes such as ethnicity, gender, or socio-economic status. Researchers have raised concerns that current algorithmic fairness paradigms enforce strict egalitarianism in healthcare, leveling down the performance of models in higher-performing subgroups instead of improving it in lower-performing ones. We propose assessing the fairness of a prediction model by expanding the concept of net benefit, using it to quantify and compare the clinical impact of a model in different subgroups. We use this to explore how a model distributes benefit across a population, its impact on health inequalities, and its role in the achievement of health equity. We show how resource constraints might introduce necessary trade-offs between health equity and other objectives of healthcare systems. We showcase our proposed approach with the development of two clinical prediction models: 1) a prognostic type 2 diabetes model used by clinicians to enrol patients into a preventive care lifestyle intervention programme, and 2) a lung cancer screening algorithm used to allocate diagnostic scans across the population. This approach helps modelers better understand if a model upholds health equity by considering its performance in a clinical and social context.",
      "authors": "Benitez-Aurioles Jose; Joules Alice; Brusini Irene; Peek Niels; Sperrin Matthew",
      "year": "2025",
      "journal": "Epidemiology (Cambridge, Mass.)",
      "doi": "10.1097/EDE.0000000000001949",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41512215/",
      "mesh_terms": "",
      "keywords": "Clinical prediction models; UK Biobank; fairness; health equity; net benefit",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "36396503",
      "title": "Algorithmic bias in health care: Opportunities for nurses to improve equality in the age of artificial intelligence.",
      "abstract": "",
      "authors": "O'Connor Siobhan; Booth Richard G",
      "year": "2022",
      "journal": "Nursing outlook",
      "doi": "10.1016/j.outlook.2022.09.003",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36396503/",
      "mesh_terms": "Humans; Artificial Intelligence; Bias; Delivery of Health Care",
      "keywords": "Algorithms; Artificial Intelligence; Bias; Health care; Machine learning; Natural language processing; Neural networks; Nursing",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "40484312",
      "title": "AI4CDI: Introducing a novel machine learning approach to demonstrate feasibility of timely and early identification of at-risk populations for Clostridioides difficile infections.",
      "abstract": "OBJECTIVE: We evaluated machine learning (ML) model feasibility to predict Clostridioides difficile infection (CDI) six months prior to onset and to identify early predictors over a longer period. METHODS: A retrospective analysis was performed using electronic health records data from US adults (Optum Market Clarity). Cases with CDI and non-CDI controls were identified. A 1:1 coarsened exact matching algorithm was applied, with final analysis cohorts of 4736 cases and 4732 controls. CDI-relevant features were identified from the published literature, and information was extracted for >900 features. The final model was trained on 597 mostly binary features. Feature information during the 6 months prior to date of first CDI diagnosis was hidden to the model to identify patients at risk for CDI with a longer time horizon. Sensitivity analysis was conducted on cases aged 65-80 years. RESULTS: Median age was 65 years (19-88) in case and control cohorts. The Gradient Boosted Trees ML model had an Area Under the Curve Receiver Operating Characteristic (AUC-ROC) of 0.79. Post-model bias evaluation revealed disparities in sensitivity (race). Long-term predictors included hospitalization days. While some predictors were exclusive to the 65-80 years model, others were more strongly associated with CDI in the overall model. CONCLUSIONS: We developed a ML model that can identify patient groups at increased risk for primary CDI. While the predictive capability of this ML model is promising, validation is needed before exploring its readiness for use in healthcare settings to inform preventive measures for CDI.",
      "authors": "Karatzia Anastasia; Aristeridou Danai; Kantz Wawi; Colavecchia A Carmine; Madhava Harish; Ateya Mohammad; Czudek Carole; Kelly Patrick H; Halsby Kate",
      "year": "2025",
      "journal": "Anaerobe",
      "doi": "10.1016/j.anaerobe.2025.102978",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40484312/",
      "mesh_terms": "Humans; Clostridium Infections; Machine Learning; Aged; Middle Aged; Retrospective Studies; Male; Female; Adult; Aged, 80 and over; Young Adult; Clostridioides difficile; Electronic Health Records; Feasibility Studies; Risk Factors; ROC Curve; Risk Assessment; Early Diagnosis",
      "keywords": "CDI; Classification algorithm; Clostridioides difficile infection; Machine learning; Predictive model",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "40865290",
      "title": "Machine learning-based prediction of suicide risk using adult attention-deficit/hyperactivity disorder symptoms and depression indicators: insights from a nationally representative south korean survey.",
      "abstract": "BACKGROUND: Suicide is a major global health issue. Evidence shows adult ADHD symptoms increase suicide risk, particularly with depression. We aimed to develop a predictive ML model. METHODS: We analyzed data from the 2021 Korean National Mental Health Survey. We defined suicide risk as serious suicidal ideation. Employing complex survey-weighted logistic regression and random forest (RF) classifier with age and sex covariates, we predicted suicidal ideation. We applied the synthetic minority over-sampling technique (SMOTE) to address class imbalance in model training. RESULTS: The SMOTE-balanced RF model exhibited higher recall (\u2248 0.76) for suicide risk than logistic regression (recall 0.08-0.48), with balanced overall performance and minimal bias between classes. Inattention symptoms demonstrated the strongest associations with suicidal ideation (odds ratio \u2248 3.2, p < 0.001). CONCLUSION: In a nationwide sample, adult ADHD symptoms-particularly inattention-and depression indicators were significantly correlated with suicidal ideation. Compared to traditional methods, an ML model substantially enhanced the identification of individuals exhibiting suicidal ideation, while maintaining fairness. These findings indicate that incorporating adult ADHD screening and ML-based models into suicide prevention strategies can facilitate the early detection of high-risk individuals in the general population.",
      "authors": "Kim Sunhae; Lee Kounseok",
      "year": "2025",
      "journal": "Psychiatry research",
      "doi": "10.1016/j.psychres.2025.116702",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40865290/",
      "mesh_terms": "Humans; Attention Deficit Disorder with Hyperactivity; Republic of Korea; Male; Female; Adult; Suicidal Ideation; Middle Aged; Machine Learning; Depression; Young Adult; Suicide; Health Surveys; Risk Assessment; Adolescent; Aged",
      "keywords": "Attention-deficit/hyperactivity disorder; Depression; Machine Learning; Suicidal ideation; Suicide Risk",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "39689864",
      "title": "Building a Time-Series Model to Predict Hospitalization Risks in Home Health Care: Insights Into Development, Accuracy, and Fairness.",
      "abstract": "OBJECTIVES: Home health care (HHC) serves more than 5 million older adults annually in the United States, aiming to prevent unnecessary hospitalizations and emergency department (ED) visits. Despite efforts, up to 25% of patients in HHC experience these adverse events. The underutilization of clinical notes, aggregated data approaches, and potential demographic biases have limited previous HHC risk prediction models. This study aimed to develop a time-series risk model to predict hospitalizations and ED visits in patients in HHC, examine model performance over various prediction windows, identify top predictive variables and map them to data standards, and assess model fairness across demographic subgroups. SETTING AND PARTICIPANTS: A total of 27,222 HHC episodes between 2015 and\u00a02017. METHODS: The study used health care process modeling of electronic health records, including clinical notes processed with natural language processing techniques and Medicare claims data. A Light Gradient Boosting Machine algorithm was used to develop the risk prediction model, with performance evaluated using 5-fold cross-validation. Model fairness was assessed across gender, race/ethnicity, and socioeconomic subgroups. RESULTS: The model achieved high predictive performance, with an F1 score of 0.84 for a 5-day prediction window. Twenty top predictive variables were identified, including novel indicators such as the length of nurse-patient visits and visit frequency. Eighty-five percent of these variables mapped completely to the US Core Data for Interoperability standard. Fairness assessment revealed performance disparities across demographic and socioeconomic groups, with lower model effectiveness for more historically underserved populations. CONCLUSIONS AND IMPLICATIONS: This study developed a robust time-series risk model for predicting adverse events in patients in HHC, incorporating diverse data types and demonstrating high predictive accuracy. The findings highlight the importance of considering established and novel risk factors in HHC. Importantly, the observed performance disparities across subgroups emphasize the need for fairness adjustments to ensure equitable risk prediction across all patient populations.",
      "authors": "Topaz Maxim; Davoudi Anahita; Evans Lauren; Sridharan Sridevi; Song Jiyoun; Chae Sena; Barr\u00f3n Yolanda; Hobensack Mollie; Scharp Danielle; Cato Kenrick; Rossetti Sarah Collins; Kapela Piotr; Xu Zidu; Gupta Pallavi; Zhang Zhihong; Mcdonald Margaret V; Bowles Kathryn H",
      "year": "2025",
      "journal": "Journal of the American Medical Directors Association",
      "doi": "10.1016/j.jamda.2024.105417",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39689864/",
      "mesh_terms": "Humans; Male; Female; Aged; Hospitalization; Home Care Services; United States; Risk Assessment; Aged, 80 and over; Emergency Service, Hospital; Electronic Health Records",
      "keywords": "Home health care service; model fairness; natural language processing; risk prediction",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "41032562",
      "title": "Enhancing Fairness and Accuracy in Diagnosing Type 2 Diabetes in Young Adult Population.",
      "abstract": "While type 2 diabetes is predominantly found in the elderly population, recent publications indicate an increasing prevalence in the young adult population. Failing to diagnose it in the minority younger age group could have significant adverse effects on their health. Several previous works acknowledge the bias of machine learning models towards different gender and race groups and propose various approaches to mitigate it. However, those works failed to propose any effective methodologies to diagnose diabetes in the young population, which is the minority group in the diabetic population. This is the first paper where we mention digital ageism towards the young adult population diagnosing diabetes. In this paper, we identify this deficiency in traditional machine learning models and propose an algorithm to mitigate the bias towards the young population when predicting diabetes. Deviating from the traditional concept of one-model-fits-all, we train customized machine-learning models for each age group. Our pipeline trains a separate machine learning model for every 5-year age band (i.e., age groups 30-34, 35-39, and 40-44). The proposed solution consistently improves recall of diabetes class by 26% to 40% in the young age group (30-44). Moreover, our technique outperforms 7 commonly used whole-group resampling techniques (i.e., random oversampling, random undersampling, SMOTE, ADASYN, Tomek-links, ENN, and Near Miss) by at least 36% in terms of diabetes recall in the young age group. Feature important analysis shows that the age attribute has a significant contribution to the decision of the original model, which was marginalized in the age-personalized model. Our method shows improved performance (e.g., balanced accuracy improved 7-12%) over multiple machine learning models and multiple sampling algorithms.",
      "authors": "Pias Tanmoy Sarkar; Su Yiqi; Tang Xuxin; Wang Haohui; Faghani Shahriar; Yao Danfeng",
      "year": "2025",
      "journal": "IEEE journal of biomedical and health informatics",
      "doi": "10.1109/JBHI.2025.3616312",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41032562/",
      "mesh_terms": "",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "40484798",
      "title": "Reimagining Resilience in Aging: Leveraging AI/ML, Big Data Analytics, and Systems Innovation.",
      "abstract": "As the aging population in the United States grows, the need for an integrated approach to support older adults has become increasingly urgent. The SUNSHINE framework, Seniors Uniting Nationwide to Support Health, INtegrated Care, and Evolution, offers a model for advancing resilience, defined as the capacity of individuals, families, systems, and communities to adapt and thrive in the face of adversity. SUNSHINE promotes this goal through the alignment of older and aging adults, families, healthcare systems, public health agencies, social services, and community resources. Using the Theory of Change modeling, SUNSHINE emphasizes whole-person health, interdisciplinary collaboration, and the strategic use of technology to address the evolving needs of aging populations. The framework promotes systems integration supported by research infrastructure and multi-sector collaboration to enhance the well-being of older adults and family caregivers. SUNSHINE places a strong emphasis on mental health, particularly depression, and highlights the importance of social connection and prevention in addressing health disparities and care gaps associated with aging. It conceptualizes resilience as both a desired outcome and a driver of transformation, guiding the redesign and evaluation of health and social systems. The framework also identifies opportunities to leverage artificial intelligence and machine learning (AI/ML) technologies, grounded in scientific evidence, to support personalized prevention, treatment, and care strategies. These technologies are critical for optimizing decision-making, improving care delivery, and enhancing system flexibility. Finally, SUNSHINE aspires to advance a future of aging that is healthy, resilient, and fair, guided by principles of equity, defined as fairness and impartiality in health opportunities and outcomes.",
      "authors": "Chen Jie; Maguire Teagan K; McCoy Rozalina G; Thomas Stephen; Reynolds Charles F",
      "year": "2025",
      "journal": "The American journal of geriatric psychiatry : official journal of the American Association for Geriatric Psychiatry",
      "doi": "10.1016/j.jagp.2025.05.007",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40484798/",
      "mesh_terms": "Humans; Resilience, Psychological; Aging; Artificial Intelligence; Big Data; Aged; United States; Data Science; Data Analytics",
      "keywords": "AI/ML; Aging health; Collaboration; Depression; Health disparities; Integration; Resilience",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "41543310",
      "title": "Machine Learning-Based Bias-Corrected Future Projections of Ozone Concentrations from a Chemistry-Climate Model.",
      "abstract": "Reliable projections of future surface ozone are crucial for air quality management and health risk assessment. However, potential biases in spatial distribution, magnitude, and trends in ozone simulated by global chemistry-climate models limit their applicability in regional evaluations. In this study, LightGBM, a machine learning (ML) algorithm, is applied to correct biases in CESM2-simulated ozone over China, the United States, and Europe and to calibrate future projections under two Shared Socioeconomic Pathways (SSP1-2.6 and SSP5-8.5) from 2020 to 2060. The ML-based correction significantly improves spatial distribution and reduces bias by 40 to 60%, also reversing the potentially incorrect trend under SSP1-2.6 in eastern China. When the ML-based correction is applied to CESM2 projections, the warm-season mean ozone shows substantial changes from 2020 to 2060. Under SSP1-2.6, corrected ozone decreases by 13.5, 17.9, and 13.7 \u03bcg/m3 in China, the United States, and Europe, respectively. In contrast, under SSP5-8.5, ozone increases over the same period by 9.4, 2.0, and 5.2 \u03bcg/m3 in these regions. The decomposition analysis shows that anthropogenic emission changes dominate future ozone trends, while a strong climate penalty occurs in polluted eastern China and climate benefits are found in western China, the United States, and Europe under SSP5-8.5. These findings demonstrate the value of combining ML with chemistry-climate models to produce more accurate air quality projections, indicating more effective and region-specific environmental protection strategies.",
      "authors": "Ni Yiqian; Yang Yang; Wang Hailong; Wang Pinya; Li Ke; Chen Lei; Zhu Jia; Li Baojie; Liao Hong",
      "year": "2026",
      "journal": "Environmental science & technology",
      "doi": "10.1021/acs.est.5c11992",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41543310/",
      "mesh_terms": "Ozone; Machine Learning; Climate Models; Air Pollutants; China; Climate; Air Pollution; United States",
      "keywords": "bias correction; future projection; machine learning; ozone",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "40965098",
      "title": "Advancing equity in generative AI dermatology requires representative data and transparent evaluation.",
      "abstract": "",
      "authors": "Kabakova Margaret; Joerg Lucie; Jagdeo Jared",
      "year": "2025",
      "journal": "Journal of the European Academy of Dermatology and Venereology : JEADV",
      "doi": "10.1111/jdv.70052",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40965098/",
      "mesh_terms": "",
      "keywords": "algorithmic bias; artificial intelligence; dermatology; health equity; machine learning; skin pigmentation",
      "pub_types": "Letter",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "27411847",
      "title": "Estimating causal contrasts involving intermediate variables in the presence of selection bias.",
      "abstract": "An important goal across the biomedical and social sciences is the quantification of the role of intermediate factors in explaining how an exposure exerts an effect on an outcome. Selection bias has the potential to severely undermine the validity of inferences on direct and indirect causal effects in observational as well as in randomized studies. The phenomenon of selection may arise through several mechanisms, and we here focus on instances of missing data. We study the sign and magnitude of selection bias in the estimates of direct and indirect effects when data on any of the factors involved in the analysis is either missing at random or not missing at random. Under some simplifying assumptions, the bias formulae can lead to nonparametric sensitivity analyses. These sensitivity analyses can be applied to causal effects on the risk difference and risk-ratio scales irrespectively of the estimation approach employed. To incorporate parametric assumptions, we also develop a sensitivity analysis for selection bias in mediation analysis in the spirit of the expectation-maximization algorithm. The approaches are applied to data from a health disparities study investigating the role of stage at diagnosis on racial disparities in colorectal cancer survival. Copyright \u00a9 2016 John Wiley & Sons, Ltd.",
      "authors": "Valeri Linda; Coull Brent A",
      "year": "2016",
      "journal": "Statistics in medicine",
      "doi": "10.1002/sim.7025",
      "url": "https://pubmed.ncbi.nlm.nih.gov/27411847/",
      "mesh_terms": "Bias; Humans; Randomized Controlled Trials as Topic; Risk; Selection Bias",
      "keywords": "EM algorithm; controlled direct effects; mediation analysis; missing at random; natural direct and indirect effects; not missing at random; selection bias; sensitivity analyses",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "17328979",
      "title": "Economic evaluation of services for a National Health scheme: the case for a fairness-based framework.",
      "abstract": "In this paper we argue that the usual framework for evaluating health services may need modification in the context of a National Health Scheme (NHS). Some costs and benefits may need to be ignored or discounted, others included at face value, and some transfer payments included in the decision algorithm. In contrast with the standard framework, we argue that economic evaluation in the context of an NHS should focus on 'social transfers' between taxpayers and beneficiaries, and that the nature and scope of these transfers is determined by the level of social generosity. Some of the implications of a modified framework are illustrated with a re-examination of (i) costs and transfer payments, (ii) unrelated future costs, (iii) moral hazard, and (iv) the rule that marginal costs should equal marginal benefits. We argue that an explicitly 'fairness-based' framework is needed for the evaluation of services in an NHS. In contrast, the usual welfare economic theoretic framework facilitates the sidelining of issues of fairness.",
      "authors": "Richardson Jeff; McKie John",
      "year": "2007",
      "journal": "Journal of health economics",
      "doi": "10.1016/j.jhealeco.2006.11.004",
      "url": "https://pubmed.ncbi.nlm.nih.gov/17328979/",
      "mesh_terms": "Evaluation Studies as Topic; Health Services Accessibility; Humans; National Health Programs; Social Justice; Victoria",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "41528321",
      "title": "Auditor models to suppress poor artificial intelligence predictions can improve human-artificial intelligence collaborative performance.",
      "abstract": "OBJECTIVE: Healthcare decisions are increasingly made with the assistance of machine learning (ML). ML has been known to have unfairness-inconsistent outcomes across subpopulations. Clinicians interacting with these systems can perpetuate such unfairness by overreliance. Recent work exploring ML suppression-silencing predictions based on auditing the ML-shows promise in mitigating performance issues originating from overreliance. This study aims to evaluate the impact of suppression on collaboration fairness and evaluate ML uncertainty as desiderata to audit the ML. MATERIALS AND METHODS: We used data from the Vanderbilt University Medical Center electronic health record (n\u2009=\u200958\u2009817) and the MIMIC-IV-ED dataset (n\u2009=\u2009363\u2009145) to predict likelihood of death or intensive care unit transfer and likelihood of 30-day readmission using gradient-boosted trees and an artificially high-performing oracle model. We derived clinician decisions directly from the dataset and simulated clinician acceptance of ML predictions based on previous empirical work on acceptance of clinical decision support alerts. We measured performance as area under the receiver operating characteristic curve and algorithmic fairness using absolute averaged odds difference. RESULTS: When the ML outperforms humans, suppression outperforms the human alone (P\u2009<\u20098.2\u2009\u00d7\u200910-6) and at least does not degrade fairness. When the human outperforms the ML, the human is either fairer than suppression (P\u2009<\u20098.2\u2009\u00d7\u200910-4) or there is no statistically significant difference in fairness. Incorporating uncertainty quantification into suppression approaches can improve performance. CONCLUSION: Suppression of poor-quality ML predictions through an auditor model shows promise in improving collaborative human-AI performance and fairness.",
      "authors": "Brown Katherine E; Wrenn Jesse O; Jackson Nicholas J; Cauley Michael R; Collins Benjamin X; Novak Laurie L; Malin Bradley A; Ancker Jessica S",
      "year": "2026",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "doi": "10.1093/jamia/ocaf235",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41528321/",
      "mesh_terms": "",
      "keywords": "artificial intelligence; human-AI collaboration; machine learning",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "40776262",
      "title": "FairFML: A Unified Approach to Algorithmic Fair Federated Learning with Applications to Reducing Gender Disparities in Cardiac Arrest Outcomes.",
      "abstract": "Addressing algorithmic bias in healthcare is crucial for ensuring equity in patient outcomes, particularly in cross-institutional collaborations where privacy constraints often limit data sharing. Federated learning (FL) offers a solution by enabling institutions to collaboratively train models without sharing sensitive data, but challenges related to fairness remain. To tackle this, we propose Fair Federated Machine Learning (FairFML), a model-agnostic framework designed to reduce algorithmic disparities while preserving patient privacy. Validated in a real-world study on gender disparities in cardiac arrest outcomes, FairFML improved fairness by up to 65% compared to centralized models, without compromising predictive performance.",
      "authors": "Li Siqi; Wu Qiming; Li Xin; Miao Di; Hong Chuan; Gu Wenjun; Ning Yilin; Shang Yuqing; Liu Nan",
      "year": "2025",
      "journal": "Studies in health technology and informatics",
      "doi": "10.3233/SHTI251245",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40776262/",
      "mesh_terms": "Humans; Machine Learning; Female; Male; Heart Arrest; Algorithms; Healthcare Disparities; Sex Factors; Federated Learning",
      "keywords": "Clinical decision-making; Demographic disparity; Electronic health records; Federated Learning; Model fairness",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "38635456",
      "title": "Understanding and Mitigating Bias in Imaging Artificial Intelligence.",
      "abstract": "Artificial intelligence (AI) algorithms are prone to bias at multiple stages of model development, with potential for exacerbating health disparities. However, bias in imaging AI is a complex topic that encompasses multiple coexisting definitions. Bias may refer to unequal preference to a person or group owing to preexisting attitudes or beliefs, either intentional or unintentional. However, cognitive bias refers to systematic deviation from objective judgment due to reliance on heuristics, and statistical bias refers to differences between true and expected values, commonly manifesting as systematic error in model prediction (ie, a model with output unrepresentative of real-world conditions). Clinical decisions informed by biased models may lead to patient harm due to action on inaccurate AI results or exacerbate health inequities due to differing performance among patient populations. However, while inequitable bias can harm patients in this context, a mindful approach leveraging equitable bias can address underrepresentation of minority groups or rare diseases. Radiologists should also be aware of bias after AI deployment such as automation bias, or a tendency to agree with automated decisions despite contrary evidence. Understanding common sources of imaging AI bias and the consequences of using biased models can guide preventive measures to mitigate its impact. Accordingly, the authors focus on sources of bias at stages along the imaging machine learning life cycle, attempting to simplify potentially intimidating technical terminology for general radiologists using AI tools in practice or collaborating with data scientists and engineers for AI tool development. The authors review definitions of bias in AI, describe common sources of bias, and present recommendations to guide quality control measures to mitigate the impact of bias in imaging AI. Understanding the terms featured in this article will enable a proactive approach to identifying and mitigating bias in imaging AI. Published under a CC BY 4.0 license. Test Your Knowledge questions for this article are available in the supplemental material. See the invited commentary by Rouzrokh and Erickson in this issue.",
      "authors": "Tejani Ali S; Ng Yee Seng; Xi Yin; Rayan Jesse C",
      "year": "2024",
      "journal": "Radiographics : a review publication of the Radiological Society of North America, Inc",
      "doi": "10.1148/rg.230067",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38635456/",
      "mesh_terms": "Humans; Artificial Intelligence; Algorithms; Automation; Machine Learning; Bias",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "3853482",
      "title": "A computer algorithm for the assessment of age reporting bias in censal population estimates using Myers' 'blended' method.",
      "abstract": "A population's age structure is widely used in the computation of many vital statistics. The importance of highly accurate vital statistics cannot be overemphasized--such statistics are used extensively by governments to determine the proper allocation of health resources and services, and by demographers, sociologists and epidemiologists to study secular trends. A computer program has been developed for use on an Apple II+ microcomputer for the analysis of population age profiles and determination of age reporting bias.",
      "authors": "Ayiomamitis A",
      "year": "1985",
      "journal": "Computer methods and programs in biomedicine",
      "doi": "10.1016/0169-2607(85)90069-0",
      "url": "https://pubmed.ncbi.nlm.nih.gov/3853482/",
      "mesh_terms": "Adolescent; Adult; Age Factors; Aged; Child; Computers; Demography; Female; Humans; Life Expectancy; Male; Microcomputers; Middle Aged; Software; Vital Statistics",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "41106548",
      "title": "Evaluating Long-Term Health Disparity Impacts of Clinical Algorithms Using a Patient-Level Simulation Framework.",
      "abstract": "OBJECTIVES: This study applies a simulation framework to evaluate the long-term effects of omitting race from a colon cancer decision algorithm for adjuvant chemotherapy, assessing impacts on health outcomes, costs, and disparities while accounting for measurement errors across racial groups. METHODS: We developed a patient-level state-transition model using electronic health records from a large Southern California health system to project outcomes for 4839 adults with stage II and III colon cancer after surgery. We compared 30-year quality-adjusted life-years (QALYs), healthcare costs, and QALY distribution among racial groups under 3 chemotherapy treatment scenarios: (1) current practice, (2) treatment guided by an algorithm that includes race, and (3) the same algorithm with race omitted. An additional health state addressed racial bias in cancer recurrence ascertainment, and probabilistic sensitivity analysis (PSA) assessed uncertainty. RESULTS: The clinical algorithm, compared with current practice, could improve average health by 0.048 QALYs and reduce racial health disparity by 0.20 QALYs at an incremental cost of $3221, with the disparity gap decreasing in 96% of PSA iterations. Omitting race showed minimal effects on overall health or costs but resulted in 13% fewer Black patients receiving treatment, decreasing their QALYs by 0.07 and widening the disparity gap by 0.13 QALY. Health disparity increased in 94% of PSA iterations. CONCLUSIONS: A cancer decision algorithm can improve population health and reduce health disparities, but omitting race may harm disadvantaged groups and limit reductions in disparities. Patient-level simulations can be routinely used to evaluate the potential health disparity impacts of algorithms before implementation.",
      "authors": "Khor Sara; Basu Anirban; Shankaran Veena; Lee Kyueun; Haupt Eric C; Hahn Erin E; Carlson Josh J; Bansal Aasthaa",
      "year": "2025",
      "journal": "Value in health : the journal of the International Society for Pharmacoeconomics and Outcomes Research",
      "doi": "10.1016/j.jval.2025.09.3066",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41106548/",
      "mesh_terms": "",
      "keywords": "clinical algorithms; health disparity; microsimulation; patient-level simulation; racial disparity",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "24913296",
      "title": "Bias in the proportionate mortality ratio analysis of small study populations: a case on analyses of radiation and mesothelioma.",
      "abstract": "UNLABELLED: Abstract Purpose: To quantify bias in the proportionate mortality ratio (PMR) analysis of small study populations and develop a bias correction methodology. MATERIALS AND METHODS: Bias in the PMR analysis of small study populations is quantified through algebraic derivation. A simulation procedure is developed to evaluate the relationship between bias and study population size. A recently published PMR analysis of radiation and mesothelioma among 329 deceased registrants in the United States Transuranium and Uranium Registries (USTUR) is used as an illustrated example. RESULTS: The proportionate mortality ratios are biased and overestimated in small population studies; the smaller the study population, the larger the overestimation. As such, the average overestimation of PMR for mesothelioma in the analyses of radiation and mesothelioma in USTUR is 7.2% (95% confidence interval = 5.1%, 9.7%); the PMR overestimation is 22.5% (95% confidence interval = 16.8%, 29.1%) when stratified by quartiles of radiation doses. CONCLUSIONS: The degree of PMR small sample bias is mainly determined by the sample size ratio, which is defined as the ratio of the sample size to the number of disease categories in the reference population. Correction for the bias is recommended when the sample size ratio is less than 5. The quantification and correction algorithm of the PMR small sample bias developed in this research supplements the PMR methodology.",
      "authors": "Zhou Joey Y",
      "year": "2014",
      "journal": "International journal of radiation biology",
      "doi": "10.3109/09553002.2014.931611",
      "url": "https://pubmed.ncbi.nlm.nih.gov/24913296/",
      "mesh_terms": "Global Health; Humans; Mesothelioma; Models, Statistical; Mortality; Neoplasms, Radiation-Induced; Occupational Diseases; Poisson Distribution; Radiation Dosage; Reproducibility of Results; Research Design; Risk Factors; Sample Size; Treatment Outcome; United States",
      "keywords": "Radiation health effect; mesothelioma; proportionate mortality ratio; small sample bias",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "34986109",
      "title": "Personalized On-Device E-Health Analytics With Decentralized Block Coordinate Descent.",
      "abstract": "Actuated by the growing attention to personal healthcare and the pandemic, the popularity of E-health is proliferating. Nowadays, enhancement on medical diagnosis via machine learning models has been highly effective in many aspects of e-health analytics. Nevertheless, in the classic cloud-based/centralized e-health paradigms, all the data will be centrally stored on the server to facilitate model training, which inevitably incurs privacy concerns and high time delay. Distributed solutions like Decentralized Stochastic Gradient Descent (D-SGD) are proposed to provide safe and timely diagnostic results based on personal devices. However, methods like D-SGD are subject to the gradient vanishing issue and usually proceed slowly at the early training stage, thereby impeding the effectiveness and efficiency of training. In addition, existing methods are prone to learning models that are biased towards users with dense data, compromising the fairness when providing E-health analytics for minority groups. In this paper, we propose a Decentralized Block Coordinate Descent (D-BCD) learning framework that can better optimize deep neural network-based models distributed on decentralized devices for E-health analytics. As a gradient-free optimization method, Block Coordinate Descent (BCD) mitigates the gradient vanishing issue and converges faster at the early stage compared with the conventional gradient-based optimization. To overcome the potential data scarcity issues for users' local data, we propose similarity-based model aggregation that allows each on-device model to leverage knowledge from similar neighbor models, so as to achieve both personalization and high accuracy for the learned models. Benchmarking experiments on three real-world datasets illustrate the effectiveness and practicality of our proposed D-BCD, where additional simulation study showcases the strong applicability of D-BCD in real-life E-health scenarios.",
      "authors": "Ye Guanhua; Yin Hongzhi; Chen Tong; Xu Miao; Nguyen Quoc Viet Hung; Song Jiangning",
      "year": "2022",
      "journal": "IEEE journal of biomedical and health informatics",
      "doi": "10.1109/JBHI.2022.3140455",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34986109/",
      "mesh_terms": "Computer Simulation; Humans; Machine Learning; Neural Networks, Computer; Privacy; Telemedicine",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "39397594",
      "title": "Accessing the Impact of TikTok's Algorithm on Regional Inequality in Health Information.",
      "abstract": "This study aims to audit the potential algorithmic bias in TikTok's health-related video recommendation toward geographically diverse groups in China. We employed 120 cloud phones and conducted two agent-based testing experiments simulating users' geographical locations and online behaviors. The results indicated significant regional inequality in video sources recommended by the TikTok algorithm, t(118)\u2009=\u20093.02, p\u2009=\u2009.003, with users from developed cities encountering a higher proportion of professional videos than those from underdeveloped cities. However, when users from both regions expressed a similar preference for the same type of information, an equal proportion of professional videos was recommended. Our findings suggest that widely used algorithms may covertly perpetuate social inequities and reinforce preexisting class-based inequalities, particularly affecting vulnerable population from low-income regions. This study also highlights the importance of enhancing eHealth literacy among disadvantaged users to mitigate problematic outcomes in the AI-based communication landscape.",
      "authors": "Li Jinhui; Shi Wen",
      "year": "2025",
      "journal": "Health communication",
      "doi": "10.1080/10410236.2024.2414882",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39397594/",
      "mesh_terms": "Humans; China; Algorithms; Socioeconomic Factors; Video Recording; Consumer Health Information",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "32107061",
      "title": "'Should I vaccinate my child?' comparing the displayed stances of vaccine information retrieved from Google, Facebook and YouTube.",
      "abstract": "Whether to vaccinate or not is currently a hot topic in social discourse. Despite the majority view that childhood vaccination is safe and effective, websites and social media content opposing such vaccination are common. In this study, we searched the internet platforms Google, Facebook and YouTube for childhood vaccine information. We made every attempt to minimise selection bias generated by internet algorithms. We compared the displayed stances of vaccine information retrieved. Most of the information had a clearly stated stance on vaccines or made some sort of recommendation on whether or not to vaccinate. Despite our careful attempt to search comprehensively and systematically for vaccine information with as little bias as possible, this search yielded a sizeable minority of vaccine negative information. This research shows that negative vaccine information persists and is readily accessible online despite algorithm and policy changes in recent years, even when searching in the least biased way possible. It is important that vaccine-promoting entities and agencies continue to make every effort to maximize their presence online so that parents searching the internet to answer the question 'should I vaccinate my child?' continue to receive vaccine positive information.",
      "authors": "Elkin Lucy E; Pullon Susan R H; Stubbe Maria H",
      "year": "2020",
      "journal": "Vaccine",
      "doi": "10.1016/j.vaccine.2020.02.041",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32107061/",
      "mesh_terms": "Child; Humans; Information Dissemination; Information Seeking Behavior; Internet; Parents; Selection Bias; Social Media; Vaccination; Vaccines",
      "keywords": "Health communication; Internet; Public health; Social media; Vaccination; Vaccine criticism",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "39701480",
      "title": "The misclassification of depression and anxiety disorders in the multiple sclerosis prodrome: A probabilistic bias analysis.",
      "abstract": "BACKGROUND: Studies suggest that depression/anxiety form part of the multiple sclerosis (MS) prodrome. However, several biases have not been addressed. We re-examined this association after correcting for: (i) misclassification of individuals not seeking healthcare, (ii) differential surveillance of depression/anxiety in the health system, and (iii) misclassified person-time from using the date of the first MS-related diagnostic claim (i.e., a demyelinating event) as a proxy for MS onset. METHODS: In this cohort study, we applied a validated algorithm to health administrative ('claims') data in British Columbia, Canada (1991-2020) to identify MS cases, and matched to general population controls. The neurologist-recorded date of MS symptom onset was available for a subset of the MS cases. We identified depression/anxiety in the 5-years preceding the first demyelinating claim using a validated algorithm. We compared the prevalence of depression/anxiety using modified Poisson regression. To account for misclassification and differential surveillance, we applied probabilistic bias analyses; for misclassified person-time, we applied time-distribution matching to the MS symptom onset date. RESULTS: Our cohort included 9929 MS cases and 49,574 controls. The prevalence ratio for depression/anxiety was 1.74 (95\u202f%CI: 1.66-1.81). Following correction for misclassification, differential surveillance using a detection ratio of 1.11, and misclassified person-time, the prevalence ratio increased to 3.25 (95\u202f%CI: 1.98-40.54). When the same correction was conducted, but a detection ratio of 1.16 was applied, the prevalence ratio increased to 3.13 (95\u202f%CI: 1.97-33.52). CONCLUSIONS: Previous conventional analyses were biased towards the null, leading to an under-estimation of the association between depression/anxiety and MS in the prodromal period. This first application of probabilistic quantitative bias analysis within MS research demonstrates both its feasibility and utility.",
      "authors": "Yusuf Fardowsa L A; Karim Mohammad Ehsanul; Gustafson Paul; Sutherland Jason M; Zhu Feng; Zhao Yinshan; Marrie Ruth Ann; Tremlett Helen",
      "year": "2025",
      "journal": "Annals of epidemiology",
      "doi": "10.1016/j.annepidem.2024.12.006",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39701480/",
      "mesh_terms": "Humans; Multiple Sclerosis; Female; Male; Adult; Middle Aged; Anxiety Disorders; British Columbia; Prevalence; Depression; Bias; Cohort Studies; Prodromal Symptoms; Algorithms",
      "keywords": "Health care utilization; Misclassification; Multiple sclerosis; Prodrome",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "34542183",
      "title": "Highlighting psychological pain avoidance and decision-making bias as key predictors of suicide attempt in major depressive disorder-A novel investigative approach using machine learning.",
      "abstract": "OBJECTIVE: Predicting suicide is notoriously difficult and complex, but a serious public health issue. An innovative approach utilizing machine learning (ML) that incorporates features of psychological mechanisms and decision-making characteristics related to suicidality could create an improved model for identifying suicide risk in patients with major depressive disorder (MDD). METHOD: Forty-four patients with MDD and past suicide attempts (MDD_SA, N\u2009=\u200944); 48 patients with MDD but without past suicide attempts (MDD_NS, N\u2009=\u200948-42 of whom with suicide ideation [MDD_SI, N\u2009=\u200942]), and healthy controls (HCs, N\u2009=\u200951) completed seven psychometric assessments including the Three-dimensional\u2002Psychological Pain Scale (TDPPS), and one behavioral assessment, the Balloon Analogue Risk Task (BART). Descriptive statistics, group comparisons, logistic regressions, and ML were used to explore and compare the groups and generate predictors of suicidal acts. RESULTS: MDD_SA and MDD_NS differed in TDPPS\u2002total score, pain arousal and avoidance subscale scores, suicidal ideation scores, and relevant decision-making indicators in BART. Logistic regression tests linked suicide attempts to psychological pain avoidance and a risk decision-making indicator. The resultant key ML model distinguished MDD_SA/MDD_NS with 88.2% accuracy. The model could also distinguish MDD_SA/MDD_SI with 81.25% accuracy. The ML model using hopelessness could classify MDD_SI/HC with 94.4% accuracy. CONCLUSION: ML analyses showed that motivation to avoid intolerable psychological pain, coupled with impaired decision-making bias toward under-valuing life's worth are highly predictive of suicide attempts. Analyses also demonstrated that suicidal ideation and attempts differed in potential mechanisms, as suicidal ideation was more related to hopelessness. ML algorithms show useful promises as a predictive instrument.",
      "authors": "Ji Xinlei; Zhao Jiahui; Fan Lejia; Li Huanhuan; Lin Pan; Zhang Panwen; Fang Shulin; Law Samuel; Yao Shuqiao; Wang Xiang",
      "year": "2022",
      "journal": "Journal of clinical psychology",
      "doi": "10.1002/jclp.23246",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34542183/",
      "mesh_terms": "Major Depressive Disorder; Humans; Machine Learning; Pain; Suicidal Ideation; Suicide, Attempted",
      "keywords": "machine learning; major depressive disorder; psychological pain; risk decision-making; suicide",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "24077093",
      "title": "Self-controlled case series and misclassification bias induced by case selection from administrative hospital databases: application to febrile convulsions in pediatric vaccine pharmacoepidemiology.",
      "abstract": "Vaccine safety studies are increasingly conducted by using administrative health databases and self-controlled case series designs that are based on cases only. Often, several criteria are available to define the cases, which may yield different positive predictive values, as well as different sensitivities, and therefore different numbers of selected cases. The question then arises as to which is the best case definition. This article proposes new methodology to guide this choice based on the bias of the relative incidence and the power of the test. We apply this methodology in a validation study of 4 nested algorithms for identifying febrile convulsions from the administrative databases of 10 French hospitals. We used a sample of 695 children aged 1 month to 3 years who were hospitalized in 2008-2009 with at least 1 diagnosis code of febrile convulsions. The positive predictive values of the algorithms ranged from 81% to 98%, and their sensitivities were estimated to be 47%-99% in data from 1 large hospital. When applying our proposed methods, the algorithm we selected used a restricted diagnosis code and position on the discharge abstract. These criteria, which resulted in the selection of 502 cases with a positive predictive value of 95%, provided the best compromise between high power and low relative bias.",
      "authors": "Quantin Catherine; Benzenine Eric; Velten Michel; Huet Fr\u00e9d\u00e9ric; Farrington C Paddy; Tubert-Bitter Pascale",
      "year": "2013",
      "journal": "American journal of epidemiology",
      "doi": "10.1093/aje/kwt207",
      "url": "https://pubmed.ncbi.nlm.nih.gov/24077093/",
      "mesh_terms": "Algorithms; Bias; Causality; Child, Preschool; Databases, Factual; Female; France; Hospital Administration; Humans; Infant; Male; Pharmacovigilance; Research Design; Seizures, Febrile; Vaccines",
      "keywords": "administrative data; bias; febrile convulsions; pharmacoepidemiology; positive predictive value; power; vaccines",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "41038065",
      "title": "Guidance to undertaking systematic evidence maps.",
      "abstract": "Systematic Evidence Maps (SEMs) are a form of evidence synthesis offering structured approaches to categorizing and organizing scientific evidence by identifying trends and gaps. SEMs support researchers and policymakers in navigating complex evidence landscapes. By synthesizing evidence, they lay the foundation for targeted systematic reviews and primary research, supporting evidence-informed decision-making. These outputs can be hosted on websites, providing an interactive tool. In environmental health, SEMs are systematically used to categorize evidence on topics such as pollution control measures, climate change impacts, and health disparities. The methodological framework for conducting SEMs involves defining the research scope, employing a systematic search strategy, screening studies systematically, optionally conducting critical appraisal (risk of bias assessment) when studies are categorized by effect direction or intended to inform subsequent syntheses, and coding data for synthesis and visualization. Narrative synthesis, heatmaps and network diagrams enhance SEMs usability. However, challenges remain, including methodological inconsistencies and the need for standardization. Advances in automation, machine learning, and stakeholder engagement can further refine SEMs methodologies. This commentary situates SEMs within the broader family of evidence synthesis, emphasizing their role in environmental health science. By enhancing methodological clarity and leveraging innovative tools, SEMs can support researchers and decision-makers in navigating complex evidence ecosystems and implementing evidence-based solutions for environmental scientists.",
      "authors": "Khalil H; Welsh V; Grainger M; Campbell F",
      "year": "2025",
      "journal": "Environment international",
      "doi": "10.1016/j.envint.2025.109827",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41038065/",
      "mesh_terms": "Environmental Health; Climate Change; Decision Making; Humans",
      "keywords": "Environmental health; Methodology; Public health; Reviews; Systematic evidence map",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "41410517",
      "title": "Multidisciplinary Perspectives on Artificial Intelligence in Aging Research and Education: Evolving Uses, Ethics, and Equity Considerations in Gerontology.",
      "abstract": "Artificial intelligence (AI) models and applications are proliferating rapidly throughout gerontological research and education. Machine learning has catapulted gerontological research in diagnosing and treating age-related health conditions. Students and educators have new tools for customized learning and innovation. Yet many of these developments come with persistent challenges, including bias, inaccuracy, and data security. As in other fields, engagement with AI models in gerontology is often siloed within disciplines. Exploring common opportunities and challenges in this space requires collaboration and conversations across disciplines. To fill this gap, the Gerontological Society of America (GSA)'s Public Policy Advisory Panel convened a multidisciplinary panel discussion of experts from the six GSA member groups and three advisory panels in November 2024 to discuss how AI is shaping various disciplines, and what ethical issues exist within or across disciplines. Several common themes emerged across disciplines: (1) human interaction remains critical to offset AI limitations in human experience, abstract reasoning, creativity, and bias; (2) AI provides opportunities for customized support across disciplines for older adults, care partners, practitioners, researchers, and students; (3) ongoing training is essential to navigate this rapidly evolving landscape; and (4) cross-disciplinary collaboration is needed to address overlapping challenges, limitations, and risks concerning AI.",
      "authors": "Perone Angela K; Abadir Peter M; Berlinger Nancy; Carey James R; Guest M Aaron; Hass Zachary J; Stephan Abigail T; Xie Bo",
      "year": "2025",
      "journal": "The Gerontologist",
      "doi": "10.1093/geront/gnaf314",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41410517/",
      "mesh_terms": "",
      "keywords": "geriatrics; interdisciplinary; technology; training",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "40696767",
      "title": "Minimizing Racial Algorithmic Bias when Predicting Electronic Health Record Data Completeness.",
      "abstract": "The previously developed algorithm for identifying subjects with high electronic health record (EHR)-continuity performed suboptimally in racially diverse populations. We aimed to improve the performance by optimizing the race modeling strategy. We randomly divided TriNetX claims-linked EHR dataset from 11 US-based healthcare organizations into training (70%) and testing data (30%) to develop and test models with and without race interactions and race-specific models. We held out a Medicaid-linked EHR dataset as validation data. Study subjects were \u226518\u2009years with \u2265365\u2009days of continuous insurance enrollment overlapping an EHR encounter. We used cross-validated least absolute shrinkage and selection operator (LASSO) to select predictors of high EHR-continuity. We compared the model performance using area under receiver operating curve (AUC). There were 550,859, 236,089, and 65,956 subjects in the training, testing, and validation datasets, respectively. In the validation set, the introduction of race-interaction terms resulted in improved model performance in Black (AUC 0.821 vs. 0.812, P\u2009<\u20090.001) and other non-White race (AUC 0.828 vs. 0.812, P\u2009<\u20090.001) subgroups. The performance of the race-specific models did not differ substantially from that of the models with race-interaction terms in the race subgroups. Using the race interactions model, subjects in the top 50% of predicted EHR-continuity had 2-3-fold lesser misclassification of 40 comparative effectiveness research (CER) relevant variables. The inclusion of race-interaction terms improved model performance in the race subgroups. Using the EHR-continuity prediction algorithm with race-interaction terms can potentially reduce algorithmic bias for racial minorities.",
      "authors": "Anand Priyanka; Jin Yinzhu; Liu Jun; Lii Joyce; Belitkar Shruti; Lin Kueiyu Joshua",
      "year": "2025",
      "journal": "Clinical pharmacology and therapeutics",
      "doi": "10.1002/cpt.3758",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40696767/",
      "mesh_terms": "Humans; Electronic Health Records; Algorithms; Male; Female; Adult; United States; Middle Aged; Racial Groups; Medicaid; Bias",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "35489789",
      "title": "Closing the Gaps in Racial Disparities in Critical Limb Ischemia Outcome and Amputation Rates: Proceedings from a Society of Interventional Radiology Foundation Research Consensus Panel.",
      "abstract": "Minority patients such as Blacks, Hispanics, and Native Americans are disproportionately impacted by critical limb ischemia and amputation due to multiple factors such as socioeconomic status, type or lack of insurance, lack of access to health care, capacity and expertise of local hospitals, prevalence of diabetes, and unconscious bias. The Society of Interventional Radiology Foundation recognizes that it is imperative to close the disparity gaps and funded a Research Consensus Panel to prioritize a research agenda. The following research priorities were ultimately prioritized: (a) randomized controlled trial with peripheral arterial disease screening of at-risk patients with oversampling of high-risk racial groups, (b) prospective trial with the introduction of an intervention to alter a social determinant of health, and (c) a prospective trial with the implementation of an algorithm that requires criteria be met prior to an amputation. This article presents the proceedings and recommendations from the panel.",
      "authors": "Bryce Yolanda; Katzen Barry; Patel Parag; Moreira Carla C; Fakorede Foluso A; Arya Shipra; D'Andrea Melissa; Mustapha Jihad; Rowe Vincent; Rosenfield Kenneth; Vedantham Suresh; Abi-Jaoudeh Nadine; Rochon Paul J",
      "year": "2022",
      "journal": "Journal of vascular and interventional radiology : JVIR",
      "doi": "10.1016/j.jvir.2022.02.010",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35489789/",
      "mesh_terms": "Amputation, Surgical; Chronic Limb-Threatening Ischemia; Humans; Peripheral Arterial Disease; Prospective Studies; Racial Groups; Radiology, Interventional; Research",
      "keywords": "",
      "pub_types": "Journal Article; Randomized Controlled Trial; Consensus Statement",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "33958261",
      "title": "Bias in a blink: Shedding light on implicit attitudes toward patients with a cleft lip.",
      "abstract": "INTRODUCTION: Previous studies have shown that patients with cleft lip and/or palate may be stigmatized in society. The objective of this study was to use an implicit association test to evaluate the subconscious biases of non-health care providers and orthodontists against patients with a repaired cleft lip (CL). METHODS: Respondents participated in an implicit association test. Pictures of patients with CL and controls were shown to participants, along with terms representing positive and negative attributes. Participants were prompted to match pictures to the attributes. The software algorithm detected whether the participants were more likely to associate CL with positive or negative terms than controls. Demographic information was collected to measure the association between some sociodemographic factors and implicit biases. RESULTS: Of 130 valid participants, 52 were orthodontists and 78 were non-health care providers. The entire sample displayed a significant implicit bias against CL (P\u00a0<0.001). Overall, orthodontists tended to exhibit slightly higher levels of implicit biases against CL than non-health care providers, but the difference was not significant when controlling for sociodemographic factors (P\u00a0=\u00a00.34). Females showed significantly lower implicit biases against CL than males (P\u00a0=\u00a00.046). Spearman correlations showed that older people and those who reported a more conservative political affiliation tended to show slightly higher levels of implicit biases against CL (P\u00a0<0.007). CONCLUSIONS: Orthodontists and non-health care providers showed moderate but significant levels of implicit biases against patients with clefts. Males, older age groups, and patients with a more conservative political affiliation tended to exhibit slightly higher levels of biases than females, younger people, and those with a more liberal political affiliation.",
      "authors": "Bous Rany M; Lyamichev Anthony; Kmentt Ashleigh; Valiathan Manish",
      "year": "2021",
      "journal": "American journal of orthodontics and dentofacial orthopedics : official publication of the American Association of Orthodontists, its constituent societies, and the American Board of Orthodontics",
      "doi": "10.1016/j.ajodo.2020.04.023",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33958261/",
      "mesh_terms": "Aged; Attitude of Health Personnel; Bias; Cleft Lip; Cleft Palate; Female; Humans; Male",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "31313407",
      "title": "Diffusion gradient nonlinearity bias correction reduces bias of breast cancer bone metastasis ADC values.",
      "abstract": "CONTRACT GRANT SPONSOR: Health Research Fund of Central Denmark Region. BACKGROUND: Diffusion gradient nonlinearity (DGNL) bias causes apparent diffusion coefficient (ADC) values to drop with increasing superior-inferior (SI) isocenter offset. This is a concern when performing quantitative diffusion-weighted imaging (DWI). PURPOSE/HYPOTHESIS: To investigate if DGNL ADC bias can be corrected in breast cancer bone metastases using a clinical DWI protocol and an online correction algorithm. STUDY TYPE: Prospective. SUBJECTS/PHANTOM: A diffusion phantom (Model 128, High Precision Devices, Boulder, CO) was used for in vitro validation. Twenty-three women with bone-metastasizing breast cancer were enrolled to assess DGNL correction in vivo. FIELD STRENGTH/SEQUENCE: DWI was performed on a 1.5T MRI system as single-shot, spin-echo, echo-planar imaging with short-tau inversion recovery (STIR) fat-saturation. ADC maps with and without DGNL correction were created from the b50 and b800 images. ASSESSMENT: Uncorrected and DGNL-corrected ADC values were measured in phantom and bone metastases by placing regions of interest on b800 images and copying them to the ADC map. The SI offset was recorded. STATISTICAL TESTS: In all, 79 bone metastases were assessed. ADC values with and without DGNL correction were compared at 14 cm SI offset using a two-tailed t-test. RESULTS: In the diffusion phantom, DGNL correction increased SI offset, where ADC bias was lower than 5%, from 7.3-13.8 cm. Of the 23 patients examined, six had no metastases in the covered regions. In the remaining patients, bias of uncorrected bone metastasis ADC values was 19.1% (95% confidence interval [CI]: 15.4-22.9%) at 14 cm SI offset. After DGNL correction, ADC bias was significantly reduced to 3.5% (95% CI: 0.7-6.3%, P\u2009<\u20090.001), thus reducing bias due to DGNL by 82%. DATA CONCLUSION: Online DGNL correction corrects DGNL ADC value bias and allows increased station lengths in the SI direction. LEVEL OF EVIDENCE: 2 Technical Efficacy: Stage 2 J. Magn. Reson. Imaging 2020;51:904-911.",
      "authors": "Buus Thomas W; Jensen Anders B; Pedersen Erik M",
      "year": "2020",
      "journal": "Journal of magnetic resonance imaging : JMRI",
      "doi": "10.1002/jmri.26873",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31313407/",
      "mesh_terms": "Breast Neoplasms; Diffusion Magnetic Resonance Imaging; Female; Humans; Image Interpretation, Computer-Assisted; Prospective Studies; Reproducibility of Results",
      "keywords": "bone marrow diseases; breast neoplasms; diffusion magnetic resonance imaging; software validation",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "38905988",
      "title": "Responsible AI for cardiovascular disease detection: Towards a privacy-preserving and interpretable model.",
      "abstract": "BACKGROUND AND OBJECTIVE: Cardiovascular disease (CD) is a major global health concern, affecting millions with symptoms like fatigue and chest discomfort. Timely identification is crucial due to its significant contribution to global mortality. In healthcare, artificial intelligence (AI) holds promise for advancing disease risk assessment and treatment outcome prediction. However, machine learning (ML) evolution raises concerns about data privacy and biases, especially in sensitive healthcare applications. The objective is to develop and implement a responsible AI model for CD prediction that prioritize patient privacy, security, ensuring transparency, explainability, fairness, and ethical adherence in healthcare applications. METHODS: To predict CD while prioritizing patient privacy, our study employed data anonymization involved adding Laplace noise to sensitive features like age and gender. The anonymized dataset underwent analysis using a differential privacy (DP) framework to preserve data privacy. DP ensured confidentiality while extracting insights. Compared with Logistic Regression (LR), Gaussian Na\u00efve Bayes (GNB), and Random Forest (RF), the methodology integrated feature selection, statistical analysis, and SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) for interpretability. This approach facilitates transparent and interpretable AI decision-making, aligning with responsible AI development principles. Overall, it combines privacy preservation, interpretability, and ethical considerations for accurate CD predictions. RESULTS: Our investigations from the DP framework with LR were promising, with an area under curve (AUC) of 0.848 \u00b1 0.03, an accuracy of 0.797 \u00b1 0.02, precision at 0.789 \u00b1 0.02, recall at 0.797 \u00b1 0.02, and an F1 score of 0.787 \u00b1 0.02, with a comparable performance with the non-privacy framework. The SHAP and LIME based results support clinical findings, show a commitment to transparent and interpretable AI decision-making, and aligns with the principles of responsible AI development. CONCLUSIONS: Our study endorses a novel approach in predicting CD, amalgamating data anonymization, privacy-preserving methods, interpretability tools SHAP, LIME, and ethical considerations. This responsible AI framework ensures accurate predictions, privacy preservation, and user trust, underscoring the significance of comprehensive and transparent ML models in healthcare. Therefore, this research empowers the ability to forecast CD, providing a vital lifeline to millions of CD patients globally and potentially preventing numerous fatalities.",
      "authors": "Ferdowsi Mahbuba; Hasan Md Mahmudul; Habib Wafa",
      "year": "2024",
      "journal": "Computer methods and programs in biomedicine",
      "doi": "10.1016/j.cmpb.2024.108289",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38905988/",
      "mesh_terms": "Humans; Cardiovascular Diseases; Artificial Intelligence; Machine Learning; Bayes Theorem; Female; Male; Privacy; Logistic Models; Confidentiality; Algorithms; Middle Aged; Data Anonymization; Risk Assessment",
      "keywords": "Cardiovascular disease; Differential privacy; Explainable machine learning; Responsible artificial intelligence",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "33665879",
      "title": "Accounting for selection bias due to death in estimating the effect of wealth shock on cognition for the Health and Retirement Study.",
      "abstract": "The Health and Retirement Study (HRS) is a longitudinal study of U.S. adults enrolled at age 50 and older. We were interested in investigating the effect of a sudden large decline in wealth on the cognitive ability of subjects measured using a dataset provided composite score. However, our analysis was complicated by the lack of randomization, time-dependent confounding, and a substantial fraction of the sample and population will die during follow-up leading to some of our outcomes being censored. The common method to handle this type of problem is marginal structural models (MSM). Although MSM produces valid estimates, this may not be the most appropriate method to reflect a useful real-world situation because MSM upweights subjects who are more likely to die to obtain a hypothetical population that over time, resembles that would have been obtained in the absence of death. A more refined and practical framework, principal stratification (PS), would be to restrict analysis to the strata of the population that would survive regardless of negative wealth shock experience. In this work, we propose a new algorithm for the estimation of the treatment effect under PS by imputing the counterfactual survival status and outcomes. Simulation studies suggest that our algorithm works well in various scenarios. We found no evidence that a negative wealth shock experience would affect the cognitive score of HRS subjects.",
      "authors": "Tan Yaoyuan Vincent; Flannagan Carol A C; Pool Lindsay R; Elliott Michael R",
      "year": "2021",
      "journal": "Statistics in medicine",
      "doi": "10.1002/sim.8921",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33665879/",
      "mesh_terms": "Humans; Middle Aged; Bias; Cognition; Longitudinal Studies; Retirement; Selection Bias",
      "keywords": "Bayesian additive regression trees; causal inference; longitudinal study; missing data; penalized spline of propensity methods in treatment comparisons; time-dependent confounding",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "38880237",
      "title": "Assessing inclusion and representativeness on digital platforms for health education: Evidence from YouTube.",
      "abstract": "BACKGROUND: Studies confirm that significant biases exist in online recommendation platforms, exacerbating pre-existing disparities and leading to less-than-optimal outcomes for underrepresented demographics. We study issues of bias in inclusion and representativeness in the context of healthcare information disseminated via videos on the YouTube social media platform, a widely used online channel for multi-media rich information. With one in three US adults using the Internet to learn about a health concern, it is critical to assess inclusivity and representativeness regarding how health information is disseminated by digital platforms such as YouTube. METHODS: Leveraging methods from fair machine learning (ML), natural language processing and voice and facial recognition methods, we examine inclusivity and representativeness of video content presenters using a large corpus of videos and their metadata on a chronic condition (diabetes) extracted from the YouTube platform. Regression models are used to determine whether presenter demographics impact video popularity, measured by the video's average daily view count. A video that generates a higher view count is considered to be more popular. RESULTS: The voice and facial recognition methods predicted the gender and race of the presenter with reasonable success. Gender is predicted through voice recognition (accuracy\u00a0=\u00a078%, AUC\u00a0=\u00a076%), while the gender and race predictions use facial recognition (accuracy\u00a0=\u00a093%, AUC\u00a0=\u00a092% and accuracy\u00a0=\u00a082%, AUC\u00a0=\u00a080%, respectively). The gender of the presenter is more significant for video views only when the face of the presenter is not visible while videos with male presenters with no face visibility have a positive relationship with view counts. Furthermore, videos with white and male presenters have a positive influence on view counts while videos with female and non - white group have high view counts. CONCLUSION: Presenters' demographics do have an influence on average daily view count of videos viewed on social media platforms as shown by advanced voice and facial recognition algorithms used for assessing inclusion and representativeness of the video content. Future research can explore short videos and those at the channel level because popularity of the channel name and the number of videos associated with that channel do have an influence on view counts.",
      "authors": "Pothugunta Krishna; Liu Xiao; Susarla Anjana; Padman Rema",
      "year": "2024",
      "journal": "Journal of biomedical informatics",
      "doi": "10.1016/j.jbi.2024.104669",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38880237/",
      "mesh_terms": "Humans; Social Media; Natural Language Processing; Machine Learning; Health Education; Male; Female; Video Recording; Adult",
      "keywords": "Inclusivity; Machine Learning; Natural Language Processing; Representativeness; Syntactic Analysis; Textual Analytics, Metadata; Voice and Facial Recognition",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "29698153",
      "title": "Health-Related Quality-of-Life Measures: Evidence from Tunisian Population Using the SF-12 Health Survey.",
      "abstract": "OBJECTIVE: To explore reporting differences related to sociodemographic characteristics affecting different health status indicators to assess their impact on the measurement of self-reported health status among the Tunisian population using the Tunisian version of the 12-item Short-Form Health Survey (SF-12). METHODS: Psychometric properties of the SF-12 were validated for a random sample of individuals (N = 3864) aged 18 years and older. The SF-12 summary scores were derived using the standard US algorithm. The principal-component analysis was used to confirm the hypothesized component structure of the SF-12 items. RESULTS: \"Known-subgroup\" comparisons showed that the SF-12 discriminated well between groups of respondents on the basis of sex, age, education, and socioeconomic status, providing evidence of construct validity. The results suggest the existence of reporting differences related to the sociodemographic characteristics affecting the health status indicators. For a given latent health status, women and oldest people are more likely to report physical activity limitations and chronic diseases. Mental health problems are overreported by divorced people and underreported by the oldest people. In addition, highly educated and socially advantaged people more often report social activities limitations due to the problems of physical and mental health. CONCLUSIONS: The findings showed that the Tunisian version of the SF-12 is a reliable and valid measure, and suggest its potential for measuring health-related quality of life in large-scale studies, specifically when overall physical and mental health are the outcomes of interest instead of the typical eight-scale profile.",
      "authors": "Younsi Moheddine",
      "year": "2015",
      "journal": "Value in health regional issues",
      "doi": "10.1016/j.vhri.2015.07.004",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29698153/",
      "mesh_terms": "",
      "keywords": "SF-12; Tunisia; health-related quality-of-life measures; reporting bias",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "41032683",
      "title": "Regulating AI in Nursing and Healthcare: Ensuring Safety, Equity, and Accessibility in the Era of Federal Innovation Policy.",
      "abstract": "The rapid integration of artificial intelligence in healthcare, accelerated by the Trump administration's 2025 AI Action Plan and private sector innovations from companies like Nvidia and Hippocratic AI, poses urgent challenges for nursing and health policy. This policy analysis examines the intersection of federal AI initiatives, emerging healthcare technologies, and nursing workforce implications through document analysis of regulatory frameworks, the federal AI Action Plan's 90+ initiatives, and insights from the American Academy of Nursing's November 2024 policy dialogue on AI transformation. The analysis reveals that while AI demonstrates measurable improvements in discrete clinical tasks-including 16% better medication assessment accuracy and 43% greater precision in identifying drug interactions at $9 per hour compared to nurses' median $41.38 hourly wage-current federal policy lacks critical healthcare-specific safeguards. The AI Action Plan's emphasis on rapid deployment and deregulation fails to address safety-net infrastructure needs, implementation pathways for vulnerable populations, or mechanisms ensuring health equity. Evidence from the Academy dialogue indicates that AI's \"technosocial reality\" fundamentally alters care delivery while potentially exacerbating disparities in underserved communities, as demonstrated by algorithmic bias in systems like Optum's care allocation algorithm. The findings suggest that achieving equitable AI integration requires comprehensive regulatory frameworks coordinating FDA, CMS, OCR, and HRSA oversight; community-centered governance approaches redistributing decision-making power to affected populations; and nursing leadership in AI development to preserve patient-centered care values. Without proactive nursing engagement in AI governance, healthcare risks adopting technologies that prioritize efficiency over the holistic, compassionate care fundamental to nursing practice.",
      "authors": "Yang Y Tony; Ricciardi Richard",
      "year": "2026",
      "journal": "Policy, politics & nursing practice",
      "doi": "10.1177/15271544251381228",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41032683/",
      "mesh_terms": "Artificial Intelligence; Humans; United States; Health Policy; Health Equity; Health Services Accessibility; Delivery of Health Care; Patient Safety",
      "keywords": "algorithms; artificial intelligence; health care delivery; health equity; health policy; nursing",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "11008073",
      "title": "Lung allocation in the United States, 1995-1997: an analysis of equity and utility.",
      "abstract": "BACKGROUND: Waiting time for organ transplantation varies widely between programs of different sizes and by geographic regions. The purpose of this study was to determine if the current lung-allocation policy is equitable for candidates waiting at various-sized centers, and to model how national allocation based solely on waiting time might affect patients and programs. METHODS: UNOS provided data on candidate registrations; transplants and outcomes; waiting times; and deaths while waiting for all U.S. lung-transplant programs during 1995-1997. Transplant centers were categorized based on average yearly volume: small (< or = 10 pounds sterling transplants/year; n = 46), medium (11-30 transplants/year; n = 29), or large (>30 transplants/year; n = 6). This data was used to model national organ allocation based solely on accumulated waiting time for candidates listed at the end of 1997. RESULTS: Median waiting time for patients transplanted was longest at large programs (724-848 days) compared to small and medium centers (371-552 days and 337-553 days, respectively) and increased at programs of all sizes during the study period. Wait-time-adjusted risk of death correlated inversely with program size (365 vs 261 vs 148 deaths per 1,000 patient-years-at-risk at small, medium, and large centers, respectively). Mortality as a percentage of new candidate registrations was similar for all program categories, ranging from 21 to 25%. Survival rates following transplantation were equivalent at medium-sized centers vs large centers (p = 0.50), but statistically lower when small centers were compared to either large- or medium-size centers (p < or = 0.05). Using waiting time as the primary criterion lung allocation would acutely shift 10 to 20% of lung-transplant activity from medium to large programs. CONCLUSIONS: 1) Waiting list mortality rates are not higher at large lung-transplant programs with long average waiting times. 2) A lung-allocation algorithm based primarily on waiting-list seniority would probably disadvantage candidates at medium-size centers without improving overall lung-transplant outcomes. 3) If fairness is measured by equal distribution of opportunity and risk, we conclude that the current allocation system is relatively equitable for patients currently entering the lung-transplant system.",
      "authors": "Pierson R N; Milstone A P; Loyd J E; Lewis B H; Pinson C W; Ely E W",
      "year": "2000",
      "journal": "The Journal of heart and lung transplantation : the official publication of the International Society for Heart Transplantation",
      "doi": "10.1016/s1053-2498(00)00151-0",
      "url": "https://pubmed.ncbi.nlm.nih.gov/11008073/",
      "mesh_terms": "Actuarial Analysis; Health Care Rationing; Humans; Lung Transplantation; Retrospective Studies; Tissue and Organ Procurement; United States; Waiting Lists",
      "keywords": "Empirical Approach; Health Care and Public Health",
      "pub_types": "Journal Article; Multicenter Study; Research Support, U.S. Gov't, Non-P.H.S.; Research Support, U.S. Gov't, P.H.S.",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "37036329",
      "title": "Craniofacial Soft-Tissue Anthropomorphic Database with Magnetic Resonance Imaging and Unbiased Diffeomorphic Registration.",
      "abstract": "BACKGROUND: Objective assessment of craniofacial surgery outcomes in a pediatric population is challenging because of the complexity of patient presentations, diversity of procedures performed, and rapid craniofacial growth. There is a paucity of robust methods to quantify anatomical measurements by age and objectively compare craniofacial dysmorphology and postoperative outcomes. Here, the authors present data in developing a racially and ethnically sensitive anthropomorphic database, providing plastic and craniofacial surgeons with \"normal\" three-dimensional anatomical parameters with which to appraise and optimize aesthetic and reconstructive outcomes. METHODS: Patients with normal craniofacial anatomy undergoing head magnetic resonance imaging (MRI) scans from 2008 to 2021 were included in this retrospective study. Images were used to construct composite (template) images with diffeomorphic image registration method using the Advanced Normalization Tools package. Composites were thresholded to generate binary three-dimensional segmentations used for anatomical measurements in Materalise Mimics. RESULTS: High-resolution MRI scans from 130 patients generated 12 composites from an average of 10 MRI sequences each: four 3-year-olds, four 4-year-olds, and four 5-year-olds (two male, two female, two Black, and two White). The average head circumference of 3-, 4-, and 5-year-old composites was 50.3, 51.5, and 51.7 cm, respectively, comparable to normative data published by the World Health Organization. CONCLUSIONS: Application of diffeomorphic registration-based image template algorithm to MRI is effective in creating composite templates to represent \"normal\" three-dimensional craniofacial and soft-tissue anatomy. Future research will focus on development of automated computational tools to characterize anatomical normality, generation of indices to grade preoperative severity, and quantification of postoperative results to reduce subjectivity bias.",
      "authors": "Villavisanis Dillan F; Khandelwal Pulkit; Zapatero Zachary D; Wagner Connor S; Blum Jessica D; Cho Daniel Y; Swanson Jordan W; Taylor Jesse A; Yushkevich Paul A; Bartlett Scott P",
      "year": "2024",
      "journal": "Plastic and reconstructive surgery",
      "doi": "10.1097/PRS.0000000000010526",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37036329/",
      "mesh_terms": "Humans; Child; Male; Female; Child, Preschool; Retrospective Studies; Image Processing, Computer-Assisted; Cephalometry; Algorithms; Magnetic Resonance Imaging; Imaging, Three-Dimensional",
      "keywords": "",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "35839913",
      "title": "Air pollution exposure during pregnancy and childhood, cognitive function, and emotional and behavioral problems in adolescents.",
      "abstract": "BACKGROUND: Exposure to air pollution may impact neurodevelopment during childhood, but current evidence on the association with cognitive function and mental health is inconclusive and primarily focusses on young children. Therefore, we aim to study the association of exposure to air pollution during pregnancy and childhood, with cognitive function and emotional and behavioral problems in adolescents. METHODS: We used data from 5170 participants of a birth cohort in Rotterdam, the Netherlands. Concentrations of fourteen air pollutants at participant's home addresses were estimated during pregnancy and childhood, using land use regression models. We included four cognitive domains (processing speed, working memory, fluid reasoning and verbal intelligence quotient (IQ)) and an estimated full-scale IQ. Internalizing, externalizing, and attention problems were self- and parent-reported. We used linear regression models to assess the association of each air pollutant, with cognitive function and emotional and behavioral problems, adjusting for socioeconomic status and lifestyle characteristics. Then, we performed multipollutant analyses using the Deletion/Substitution/Addition (DSA) algorithm. RESULTS: Air pollution exposure was not associated with full-scale IQ, working memory, or processing speed. Higher exposure to few air pollutants was associated with higher fluid reasoning and verbal IQ scores (e.g. 0.22 points of fluid reasoning (95%CI 0.00; 0.44) per 1\u00a0\u03bcg/m3 increase in organic carbon during pregnancy). Higher exposure to some air pollutants was also associated with less internalizing, externalizing, and attention problems (e.g. -0.27 internalizing problems (95% CI -0.52; -0.02) per each 5\u00a0ng/m3 increase in copper during pregnancy). CONCLUSIONS: Higher exposure to air pollution during pregnancy and childhood was not associated with lower cognitive function or more emotional and behavioral problems in adolescents. Based on previous literature and biological plausibility, the observed protective associations are probably explained by negative residual confounding, selection bias, or chance and do not represent a causal relationship.",
      "authors": "Kusters Michelle S W; Essers Esm\u00e9e; Muetzel Ryan; Ambr\u00f3s Albert; Tiemeier Henning; Guxens M\u00f2nica",
      "year": "2022",
      "journal": "Environmental research",
      "doi": "10.1016/j.envres.2022.113891",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35839913/",
      "mesh_terms": "Adolescent; Air Pollutants; Air Pollution; Child; Child, Preschool; Cognition; Environmental Exposure; Female; Humans; Particulate Matter; Pregnancy; Problem Behavior",
      "keywords": "Adolescents; Air pollution; Cognitive function; Environmental epidemiology; Mental health; Traffic",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "30841773",
      "title": "Bias-corrected estimates of reduction of post-surgery length of stay and corresponding cost savings through the widespread national implementation of fast-tracking after liver transplantation: a quasi-experimental study.",
      "abstract": "Background: Fast-tracking is an approach adopted by Mayo Clinic in Florida's (MCF) liver transplant (LT) program, which consists of early tracheal extubation and transfer of patients to surgical ward, eliminating a stay in the intensive care unit in select patients. Since adopting this approach in 2002, MCF has successfully fast-tracked 54.3% of patients undergoing LT. Objectives: This study evaluated the reduction in post-operative length of stay (LOS) that resulted from the fast-tracking protocol and assessed the potential cost saving in the case of nationwide implementation. Methods: A propensity score for fast-tracking was generated based on MCF liver transplant databases during 2011-2013. Various propensity score matching algorithms were used to form control groups from the United Network of Organ Sharing Standard Analysis and Research (STAR) file that had comparable demographic characteristics and health status to the treatment group identified in MCF. Multiple regression and matching estimators were employed for evaluation of the post-surgery LOS. The algorithm generated from the analysis was also applied to the STAR data to determine the proportion of patients in the US who could potentially be candidates for fast-tracking, and the potential savings. Results: The effect of the fast-tracking on the post-transplant LOS was estimated at approximately from 2.5 (p-value\u2009=\u20090.001) to 3.2 (p-value\u2009<\u20090.001) days based on various matching algorithms. The cost saving from a nationwide implementation of fast-tracking of liver transplant patients was estimated to be at least $78 million during the 2-year period. Conclusion: The fast-track program was found to be effective in reducing post-transplant LOS, although the reduction appeared to be less than previously reported. Nationwide implementation of fast-tracking could result in substantial cost savings without compromising the patient outcome.",
      "authors": "Loh Chung-Ping A; Croome Kristopher P; Burcin Taner C; Keaveny Andrew P",
      "year": "2019",
      "journal": "Journal of medical economics",
      "doi": "10.1080/13696998.2019.1592179",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30841773/",
      "mesh_terms": "Academic Medical Centers; Age Factors; Cohort Studies; Cost Savings; Databases, Factual; Early Ambulation; Female; Florida; Humans; Intensive Care Units; Length of Stay; Liver Transplantation; Logistic Models; Male; Middle Aged; Multivariate Analysis; Postoperative Care; Retrospective Studies; Risk Factors; Selection Bias",
      "keywords": "C40; C90; Fast-tracking; I11; I19; length of stay; liver transplant; matching; propensity score; quasi-experimental study",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    },
    {
      "pmid": "40767775",
      "title": "Delirium as a Precursor to Dementia in Elderly Type 2 Diabetes Mellitus Patients.",
      "abstract": "Purpose: This study aimed to investigate the association between delirium and incident dementia in elderly (\u226565 years) type 2 diabetes mellitus (T2DM) patients, addressing the heightened dementia risk in this population. Methods: We conducted a retrospective cohort study using data from the National Health Insurance Research Database (NHIRD) spanning January 1, 2005, to December 31, 2022. The study included elderly (\u226565 years) T2DM patients newly diagnosed between January 1, 2005, and December 31, 2007. Patients were categorized into delirium and no delirium groups. A rigorous propensity score matching algorithm was applied to ensure optimal balance of baseline covariates, thereby minimizing selection bias and confounding, and Cox regression models along with competing risk analyses assessed the risk of incident dementia. Results: The study included 5,128 elderly (\u226565 years) T2DM patients, with 2,564 patients in both the delirium and no delirium groups. Baseline covariates achieved balance, including age, sex, income levels, urbanization, duration of diabetes, types of antidiabetic medications, and comorbidities. The incidence of dementia was significantly higher in the delirium group (42.75%) compared to the no delirium group (22.66%), with a P value <.0001. The data reveal a clear dose-response pattern, wherein each additional delirium episode substantially amplifies dementia risk, underscoring the cumulative impact of repeated episodes on cognitive deterioration: no episodes (4.40 per 100 person-years), 1 episode (7.62 per 100 person-years), and 2 or more episodes (8.41 per 100 person-years). Conclusions: Our findings confirm a strong association between delirium and an increased risk of dementia in elderly (\u226565 years) T2DM patients, suggesting a potential causal link. Effective delirium management in elderly T2DM patients is imperative to mitigate dementia risk. These findings advocate for targeted interventions to alleviate the substantial cognitive burden in this vulnerable population.",
      "authors": "Sun Mingyang; Wang Xiaoling; Lu Zhongyuan; Yang Yitian; Lv Shuang; Miao Mengrong; Chen Wan-Ming; Wu Szu-Yuan; Zhang Jiaqiang",
      "year": "2025",
      "journal": "The Journal of clinical psychiatry",
      "doi": "10.4088/JCP.25m15798",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40767775/",
      "mesh_terms": "Humans; Diabetes Mellitus, Type 2; Delirium; Aged; Male; Female; Dementia; Retrospective Studies; Aged, 80 and over; Incidence; Risk Factors; Comorbidity; Taiwan",
      "keywords": "",
      "pub_types": "Journal Article",
      "pmcid": "",
      "ft_status": "No PMC full text"
    }
  ],
  "ft_failed": []
}