{
  "ft_included": [
    {
      "pmid": "39695057",
      "title": "The Algorithmic Divide: A Systematic Review on AI-Driven Racial Disparities in Healthcare.",
      "abstract": "INTRODUCTION: As artificial intelligence (AI) continues to permeate various sectors, concerns about disparities arising from its deployment have surfaced. AI's effectiveness correlates not only with the algorithm's quality but also with its training data's integrity. This systematic review investigates the racial disparities perpetuated by AI systems across diverse medical domains and the implications of deploying them, particularly in healthcare. METHODS: Six electronic databases (PubMed, Scopus, IEEE, Google Scholar, EMBASE, and Cochrane) were systematically searched on October 3, 2023. Inclusion criteria were peer-reviewed articles in English from 2013 to 2023 that examined instances of racial bias perpetuated by AI in healthcare. Studies conducted outside of healthcare settings or that addressed biases other than racial, as well as letters, opinions were excluded. The risk of bias was identified using CASP criteria for reviews and the Modified Newcastle Scale for observational studies. RESULTS: Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, 1272 articles were initially identified, from which 26 met eligibility criteria. Four articles were identified via snowballing, resulting in 30 articles in the analysis. Studies indicate a significant association between AI utilization and the exacerbation of racial disparities, especially in minority populations, including Blacks and Hispanics. Biased data, algorithm design, unfair deployment of algorithms, and historic/systemic inequities were identified as the causes. Study limitations stem from heterogeneity impeding broad comparisons and the preclusion of meta-analysis. CONCLUSION: To address racial disparities in healthcare outcomes, enhanced ethical considerations and regulatory frameworks are needed in AI healthcare applications. Comprehensive bias detection tools and mitigation strategies, coupled with active supervision by physicians, are essential to ensure AI becomes a tool for reducing racial disparities in healthcare outcomes.",
      "journal": "Journal of racial and ethnic health disparities",
      "year": "2026",
      "doi": "10.1007/s40615-024-02237-0",
      "authors": "Haider Syed Ali et al.",
      "keywords": "AI\u00a0Artificial Intelligence; Fairness; Healthcare disparities; ML\u00a0Machine Learning; Racial disparities",
      "mesh_terms": "Humans; Algorithms; Artificial Intelligence; Healthcare Disparities; Racism",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39695057/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: To address racial disparities in healthcare outcomes, enhanced ethical considerations and regulatory frameworks are needed in AI healthcare applications. Comprehensive bias detection tools and mitigation strategies, coupled with active supervision by physicians, are essential to ensure AI becomes a tool for reducing racial disparities in healthcare outcomes.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40788801",
      "title": "Adversarial Debiasing for Equitable and Fair Detection of Acute Coronary Syndrome Using 12-Lead ECG.",
      "abstract": "OBJECTIVE: Acute coronary syndrome (ACS) is a life-threatening condition requiring accurate diagnosis for better outcomes. However, variability in signs and symptoms among racial subgroups could cause disparities in diagnostic accuracy. In this study, we use machine learning models to diagnose ACS, focusing on mitigating disparities and ensuring fairness between Black and non-Black populations. METHODS: We built on a state-of-the-art random forest classifier to compare three mitigation strategies. The first two approaches involved resampling or partitioning the data prior to training, while the third approach proposed an innovative framework called adversarial debiasing. To evaluate our model performance, we used the receiver operating characteristic (ROC) curve and an operating point at 80% specificity for clinical importance. RESULTS: After mitigation with adversarial debiasing, the difference in sensitivities between the two subgroups decreased from 9.8% to 1.3%. Specifically, this approach achieved areas under the ROC of 0.810 and 0.817, and sensitivities of 70.1% and 71.4%, respectively for Black and non-Black subgroups. CONCLUSION: The proposed adversarial debiasing model outperformed the other two methods in both diagnostic accuracy and effectiveness in minimizing disparities. SIGNIFICANCE: We expect this framework to achieve fair diagnostic models across diverse demographic populations globally and be generalizable to other outcomes.",
      "journal": "IEEE transactions on bio-medical engineering",
      "year": "2026",
      "doi": "10.1109/TBME.2025.3597527",
      "authors": "Ji Rui Qi et al.",
      "keywords": "",
      "mesh_terms": "Humans; Acute Coronary Syndrome; Electrocardiography; Machine Learning; ROC Curve; Female; Male; Middle Aged; Aged; Signal Processing, Computer-Assisted; Diagnosis, Computer-Assisted; Sensitivity and Specificity; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40788801/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Random Forest",
      "health_domain": "Cardiology",
      "bias_axes": "Race/Ethnicity",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Adversarial Debiasing; Threshold Adjustment",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: The proposed adversarial debiasing model outperformed the other two methods in both diagnostic accuracy and effectiveness in minimizing disparities. SIGNIFICANCE: We expect this framework to achieve fair diagnostic models across diverse demographic populations globally and be generalizable to other outcomes.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40795063",
      "title": "Enhancing end-stage renal disease outcome prediction: a multisourced data-driven approach.",
      "abstract": "OBJECTIVES: To improve prediction of chronic kidney disease (CKD) progression to end-stage renal disease (ESRD) using machine learning (ML) and deep learning (DL) models applied to integrated clinical and claims data with varying observation windows, supported by explainable artificial intelligence (AI) to enhance interpretability and reduce bias. MATERIALS AND METHODS: We utilized data from 10\u00a0326 CKD patients, combining clinical and claims information from 2009 to 2018. After preprocessing, cohort identification, and feature engineering, we evaluated multiple statistical, ML and DL models using 5 distinct observation windows. Feature importance and SHapley Additive exPlanations (SHAP) analysis were employed to understand key predictors. Models were tested for robustness, clinical relevance, misclassification patterns, and bias. RESULTS: Integrated data models outperformed single data source models, with long short-term memory achieving the highest area under the receiver operating characteristic curve (AUROC) (0.93) and F1 score (0.65). A 24-month observation window optimally balanced early detection and prediction accuracy. The 2021 estimated glomerular filtration rate (eGFR)\u00a0equation improved prediction accuracy and reduced racial bias, particularly for African American patients. DISCUSSION: Improved prediction accuracy, interpretability, and bias mitigation strategies have the potential to enhance CKD management, support targeted interventions, and reduce health-care disparities. CONCLUSION: This study presents a robust framework for predicting ESRD outcomes, improving clinical decision-making through integrated multisourced data and advanced analytics. Future research will expand data integration and extend this framework to other chronic diseases.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2026",
      "doi": "10.1093/jamia/ocaf118",
      "authors": "Li Yubo et al.",
      "keywords": "chronic kidney disease; clinical and claims data integration; end-stage renal disease; machine learning; predictive modeling",
      "mesh_terms": "Humans; Kidney Failure, Chronic; Machine Learning; Disease Progression; Male; Female; Deep Learning; Middle Aged; Renal Insufficiency, Chronic; Aged; ROC Curve",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40795063/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning",
      "health_domain": "ICU/Critical Care; Nephrology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: This study presents a robust framework for predicting ESRD outcomes, improving clinical decision-making through integrated multisourced data and advanced analytics. Future research will expand data integration and extend this framework to other chronic diseases.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12758457"
    },
    {
      "pmid": "41202615",
      "title": "FairREAD: Re-fusing demographic attributes after disentanglement for fair medical image classification.",
      "abstract": "Recent advancements in deep learning have shown transformative potential in medical imaging, yet concerns about fairness persist due to performance disparities across demographic subgroups. Existing methods aim to address these biases by mitigating sensitive attributes in image data; however, these attributes often carry clinically relevant information, and their removal can compromise model performance-a highly undesirable outcome. To address this challenge, we propose Fair Re-fusion After Disentanglement (FairREAD), a novel, simple, and efficient framework that mitigates unfairness by re-integrating sensitive demographic attributes into fair image representations. FairREAD employs orthogonality constraints and adversarial training to disentangle demographic information while using a controlled re-fusion mechanism to preserve clinically relevant details. Additionally, subgroup-specific threshold adjustments ensure equitable performance across demographic groups. Comprehensive evaluations and out-of-distribution testing on large-scale clinical X-ray datasets demonstrate that, given demographic attributes of each patient, FairREAD is able to significantly reduce unfairness metrics while maintaining diagnostic accuracy. Our code is available at: https://github.com/Advanced-AI-in-Medicine-and-Physics-Lab/FairREAD/.",
      "journal": "Medical image analysis",
      "year": "2026",
      "doi": "10.1016/j.media.2025.103858",
      "authors": "Gao Yicheng et al.",
      "keywords": "Bias mitigation; Chest X-ray; Disentanglement; Fairness",
      "mesh_terms": "Humans; Deep Learning; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41202615/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Adversarial Debiasing; Threshold Adjustment; Fairness Metrics Evaluation; Representation Learning",
      "clinical_setting": "Not specified",
      "key_findings": "Comprehensive evaluations and out-of-distribution testing on large-scale clinical X-ray datasets demonstrate that, given demographic attributes of each patient, FairREAD is able to significantly reduce unfairness metrics while maintaining diagnostic accuracy. Our code is available at: https://github.com/Advanced-AI-in-Medicine-and-Physics-Lab/FairREAD/.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41380307",
      "title": "TransFair: Transferring fairness from ocular disease classification to progression prediction.",
      "abstract": "The use of artificial intelligence (AI) in automated disease classification significantly reduces healthcare costs and improves the accessibility of services. However, this transformation has given rise to concerns about the fairness of AI, which disproportionately affects certain groups, particularly patients from underprivileged populations. Recently, a number of methods and large-scale datasets have been proposed to address group performance disparities. Although these methods have shown effectiveness in disease classification tasks, they may fall short in ensuring fair prediction of disease progression, mainly because of limited longitudinal data with diverse demographics available for training a robust and equitable prediction model. In this paper, we introduce TransFair to enhance demographic fairness in progression prediction for ocular diseases. TransFair aims to transfer a fairness-enhanced disease classification model to the task of progression prediction with fairness preserved. First, we train a fairness-aware EfficientNet called FairEN using extensive data for ocular disease classification. Subsequently, this fair classification model is adapted to a fair progression prediction model through knowledge distillation, which minimizes the latent feature distances between classification and progression prediction models. We evaluate FairEN and TransFair for fairness-enhanced ocular disease classification and progression prediction using both two-dimensional (2D) and 3D retinal images. Extensive experiments and comparisons show that TransFair enhances group fairness in predicting ocular disease progression.",
      "journal": "Artificial intelligence in medicine",
      "year": "2026",
      "doi": "10.1016/j.artmed.2025.103331",
      "authors": "Shi Min et al.",
      "keywords": "AI fairness; Disease progression; OCT B-scans; Ocular disease; RNFLT maps",
      "mesh_terms": "Humans; Disease Progression; Artificial Intelligence; Eye Diseases",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41380307/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Ophthalmology; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "We evaluate FairEN and TransFair for fairness-enhanced ocular disease classification and progression prediction using both two-dimensional (2D) and 3D retinal images. Extensive experiments and comparisons show that TransFair enhances group fairness in predicting ocular disease progression.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41395651",
      "title": "Literature-informed ensemble machine learning for three-year diabetic kidney disease risk prediction in type 2 diabetes: Development, validation, and deployment of the PSMMC NephraRisk model.",
      "abstract": "INTRODUCTION: Diabetic kidney disease (DKD) and diabetic nephropathy (DN) affect around 40% of diabetic patients but lack accurate risk prediction tools that include social determinants and demographic complexity. We developed and validated an ensemble machine learning model for three-year DKD/DN risk prediction with deployment readiness. METHODS: We analysed 18\u2009742 eligible adult type 2 diabetic patients from Prince Sultan Military Medical City (PSMMC) registry between 2019 and 2024 in Riyadh, Saudi Arabia. Using temporal patient-level splitting, we developed a stacked ensemble model (LightGBM + CoxBoost) with several features including multiple literature-informed imputed variables including family history, non-steroidal anti-inflammatory drug (NSAID) use, socioeconomic deprivation, diabetic retinopathy severity, and antihypertensive medications, imputed via Bayesian multiple imputation by chained equations (MICE) with external study priors. Primary outcome was incident/progressive DKD/DN within 3\u2009years' timeframe. We assessed discrimination, calibration, model utilisation, and algorithmic fairness. RESULTS: The final model achieved excellent discrimination (receiver operating characteristic [AUROC] of 0.852, 95% CI 0.847-0.857) and near-perfect calibration (slope 0.98, intercept -0.012) on multi-trial validation. Decision curve evaluation demonstrated superior net benefit (+22 events prevented per 1000 patients at 10% threshold) compared to treat-all strategies. Bootstrap validation showed minimal optimism in discrimination (C-statistic optimism\u2009=\u20090.005). No algorithmic bias was detected across demographic subgroups (maximum |\u0394-AUROC|\u2009=\u20090.010). Prior sensitivity analysis confirmed validity and significance (AUROC variation \u22640.008). The model was engineered and deployed as an interactive web-based application (https://nephrarisk.streamlit.app/). CONCLUSIONS: Our developed and demonstrated model provided accurate and well-fair DKD/DN risk prediction with excellent calibration, allowing for better decision making with deployment as a web-based research tool and framework for future prospective clinical validation. Further validation and testing are warranted from different centres and healthcare systems to increase confidence and dissemination of our model findings for better utilisation purposes in the future.",
      "journal": "Diabetes, obesity & metabolism",
      "year": "2026",
      "doi": "10.1111/dom.70385",
      "authors": "Tourkmani Ayla M et al.",
      "keywords": "diabetes; diabetic kidney disease; diabetic nephropathy; glycaemic control; renal functions",
      "mesh_terms": "Humans; Diabetic Nephropathies; Diabetes Mellitus, Type 2; Machine Learning; Male; Female; Middle Aged; Risk Assessment; Adult; Risk Factors; Registries; Aged",
      "pub_types": "Journal Article; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41395651/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "XGBoost/Gradient Boosting; Ensemble Methods; Clinical Prediction Model",
      "health_domain": "Ophthalmology; Nephrology; Endocrinology/Diabetes",
      "bias_axes": "Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Data Preprocessing; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Calibration; Threshold Adjustment; Ensemble Methods",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Our developed and demonstrated model provided accurate and well-fair DKD/DN risk prediction with excellent calibration, allowing for better decision making with deployment as a web-based research tool and framework for future prospective clinical validation. Further validation and testing are warranted from different centres and healthcare systems to increase confidence and dissemination of our model findings for better utilisation purposes in the future.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12890761"
    },
    {
      "pmid": "41414851",
      "title": "Clinical validation of AI-assisted contouring in prostate radiation therapy treatment planning: Highlighting automation bias and the need for standardized quality assurance.",
      "abstract": "PURPOSE: This study evaluated the impact of a commercial AI-assisted contouring tool on intra- and inter-observer variability in prostate radiation therapy and assessed the dosimetric consequences of geometric contour differences. METHODS: Two experienced radiation oncologists independently delineated clinical target volume (CTV) and organs at risk (OARs) for prostate cancer patients. Manual contours (Cman) and AI-generated contours (CAI) were compared with adjusted AI contours (CAI,adj). A consensus reference (Cref) served as the benchmark. To evaluate clinical impact, treatment plans were recalculated and replanned on each contour set under identical beam geometries to assess dose-volume histogram (DVH) parameters. RESULTS: AI-assisted contouring significantly improved both intra- and inter-observer agreement. Inter-observer analysis revealed that the Dice similarity coefficient (DSCs) for CTV increased from 0.78 (\u00b1\u00a00.11) for Cman to 0.89 (\u00b1\u00a00.09) for CAI, adj. Similarly, intra-observer analysis revealed that both oncologists showed significantly higher DSCs for CAI, adj compared to Cman. A thorough geometric comparison to the Cref revealed that while adjustments to CAI improved accuracy, they generally did not surpass Cman for CTV and rectum. Dosimetric analyses demonstrated that, under fixed plan geometry, both Cman and CAI,adj contours yielded lower planning target volume (PTV) D95% values compared with Cref, whereas after replanning, all plans met institutional criteria with no clinically significant differences among contour sets. CONCLUSION: AI-assisted contouring in prostate radiotherapy reduced intra- and inter-observer variability and improved contouring consistency. However, CAI, adj did not consistently surpass Cman, especially for the CTV and rectum, where automation bias or selective clinical acceptance may have influenced edits. Fixed-plan recalculations revealed dose differences from minor geometric deviations. These findings underscore the importance of structured quality assurance (QA) and human oversight to mitigate automation bias while leveraging AI's efficiency. The single-institution design with two oncologists and one AI software limits generalizability, underscoring the need for multi-observer validation.",
      "journal": "Journal of applied clinical medical physics",
      "year": "2026",
      "doi": "10.1002/acm2.70425",
      "authors": "Arjmandi Najmeh et al.",
      "keywords": "Automation bias; Auto\u2010contouring; Deep learning; Inter\u2010observer variability; Intra\u2010observer variability; Quality assurance; Radiotherapy",
      "mesh_terms": "Humans; Radiotherapy Planning, Computer-Assisted; Prostatic Neoplasms; Male; Radiotherapy Dosage; Organs at Risk; Quality Assurance, Health Care; Radiotherapy, Intensity-Modulated; Artificial Intelligence; Automation",
      "pub_types": "Journal Article; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41414851/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Generative AI",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: AI-assisted contouring in prostate radiotherapy reduced intra- and inter-observer variability and improved contouring consistency. However, CAI, adj did not consistently surpass Cman, especially for the CTV and rectum, where automation bias or selective clinical acceptance may have influenced edits. Fixed-plan recalculations revealed dose differences from minor geometric deviations.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12715409"
    },
    {
      "pmid": "41458068",
      "title": "Conformal uncertainty quantification to evaluate predictive fairness of foundation AI model for skin lesion classes across patient demographics.",
      "abstract": "Deep learning based diagnostic AI systems based on medical images are starting to provide similar performance as human experts. However, these data-hungry complex systems are inherently black boxes and therefore slow to be adopted for high-risk applications like healthcare. This problem of lack of transparency is exacerbated in the case of recent large foundation models, which are trained in a self-supervised manner on millions of data points to provide robust generalisation across a range of downstream tasks. The embeddings generated from them happen through a process that is not interpretable, and hence not easily trustable for clinical applications. To address this timely issue, we deploy conformal analysis to quantify the predictive uncertainty of a vision transformer (ViT)-based foundation model across patient demographics with respect to sex, age, and ethnicity for the task of skin lesion classification using several public benchmark datasets. The significant advantage of this method is that conformal analysis is method independent, and it not only provides a coverage guarantee at the population level but also provides an uncertainty score for each individual. This is used to demonstrate the effectiveness of utilizing these embeddings for specialized tasks like diagnostic classification, meanwhile reducing computational costs. Secondly, the public benchmark datasets we used had severe class imbalance in terms of the number of samples in different classes. We used a model-agnostic dynamic F1-score-based sampling during model training, which helped to stabilize the class imbalance. We investigate the effects on uncertainty quantification (UQ) with or without this bias mitigation step. Thus, our results show how this can be used as a fairness metric to evaluate the robustness of the feature embeddings of the foundation model (Google DermFoundation), advancing the trustworthiness and fairness of clinical AI.",
      "journal": "Health information science and systems",
      "year": "2026",
      "doi": "10.1007/s13755-025-00412-z",
      "authors": "Bhattacharyya Swarnava et al.",
      "keywords": "Algorithmic fairness; Class imbalance; Conformal prediction; Foundation models; Skin lesion classification; Transparent trustworthy AI; Uncertainty quantification; Vision transformer (ViT)",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41458068/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; NLP/LLM; Foundation Model",
      "health_domain": "Dermatology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Representation Learning; Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We investigate the effects on uncertainty quantification (UQ) with or without this bias mitigation step. Thus, our results show how this can be used as a fairness metric to evaluate the robustness of the feature embeddings of the foundation model (Google DermFoundation), advancing the trustworthiness and fairness of clinical AI.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12738507"
    },
    {
      "pmid": "41482490",
      "title": "A bias field correction workflow based on generative adversarial network for abdominal cancers treated with 0.35T MR-LINAC.",
      "abstract": "PURPOSE: In this study, a bias field correction workflow was proposed to improve the flexibility and generalizability of the generative adversarial network (GAN) model for abdominal cancer patients treated with a 0.35T magnetic resonance imaging linear accelerator (MR-LINAC) system. METHODS: Model training was performed using brain MR images acquired on a 3T diagnostic scanner, while model testing was performed using abdominal MR images obtained using a 0.35T MR-LINAC system. The performance of the proposed workflow was first compared with the GAN model using root-mean-square error (RMSE), peak signal-to-noise ratio (PSNR), and structural similarity index measure (SSIM). To assess the impact of the workflow on image segmentation, it was also compared with the N4ITK algorithm. Segmentation was performed using the k-means clustering algorithm with three clusters corresponding to air, fat, and soft tissue. Segmentation accuracy was then evaluated using the Dice similarity coefficient (DSC). RESULTS: The RMSE values were 30.59, 12.06, 10.37 for the bias field-corrupted images (IIN), GAN-corrected images (IGAN), and images corrected with the proposed workflow (IOUT), respectively. Corresponding PSNR values were 42.34, 46.04, 47.04 dB, and SSIM values were 0.84, 0.96, 0.98. For segmentation accuracy, the mean DSC for air masks was 0.95, 0.97, and 0.97; for fat masks, 0.61, 0.71, and 0.74; and for soft tissue masks, 0.60, 0.68, and 0.69, corresponding to IIN, N4ITK-corrected images (IN4ITK), and IOUT, respectively CONCLUSION: By effectively mitigating bias field artifacts, the proposed workflow has the potential to strengthen the clinical utility of MRI-guided adaptive radiotherapy for abdominal cancers, ensuring safer and more accurate radiation delivery.",
      "journal": "Journal of applied clinical medical physics",
      "year": "2026",
      "doi": "10.1002/acm2.70448",
      "authors": "Yang Ching-Ching et al.",
      "keywords": "0.35T MR\u2010LINAC; bias field artifacts; generative adversarial network",
      "mesh_terms": "Humans; Abdominal Neoplasms; Workflow; Magnetic Resonance Imaging; Algorithms; Image Processing, Computer-Assisted; Radiotherapy Planning, Computer-Assisted; Particle Accelerators; Radiotherapy Dosage; Radiotherapy, Intensity-Modulated; Neural Networks, Computer; Generative Adversarial Networks",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41482490/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Computer Vision/Imaging AI; Generative AI; Clustering",
      "health_domain": "Radiology/Medical Imaging; Oncology; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: By effectively mitigating bias field artifacts, the proposed workflow has the potential to strengthen the clinical utility of MRI-guided adaptive radiotherapy for abdominal cancers, ensuring safer and more accurate radiation delivery.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12758996"
    },
    {
      "pmid": "41522832",
      "title": "Evaluation and improvement of algorithmic fairness for COVID-19 severity classification using Explainable Artificial Intelligence-based bias mitigation.",
      "abstract": "OBJECTIVES: The COVID-19 pandemic has highlighted the growing reliance on machine learning (ML) models for predicting disease severity, which is important for clinical decision-making and equitable resource allocation. While achieving high predictive accuracy is important, ensuring fairness in the prediction output of these models is equally important to prevent bias-driven disparities in healthcare. This study evaluates fairness in a machine learning-based COVID-19 severity classification model and proposes an Explainable AI (XAI)-based bias mitigation strategy to address sex-related bias. MATERIALS AND METHODS: Using data from the Quebec Biobank, we developed an XGBoost-based multi-class classification model. Fairness was assessed using Subset Accuracy Parity Difference (SAPD) and Label-wise Equal Opportunity Difference (LEOD) metrics. Four bias mitigation strategies were implemented and evaluated: Fair Representation Learning, Fair Classifier Using Constraints, Adversarial Debiasing, and our proposed XAI-based method utilizing SHapley Additive exPlanations (SHAP) method for feature importance analysis. RESULTS: The study cohort included 1642 COVID-19 positive older adults (mean age: 77.5), balance equally between males and females. The baseline (unmitigated) classification model achieved 90.68% accuracy but exhibited a 10.11% Subset Accuracy Parity Difference between sexes, indicating a relatively large bias. The introduced XAI-based method demonstrated a better trade-off between model performance and fairness compared to existing bias mitigation methods by identifying sex-sensitive feature interactions and integrating them into the model re-training. DISCUSSION: Traditional fairness interventions often compromise accuracy to a greater extent. Our XAI-based method achieves the best balance between classification performance and bias, enhancing its clinical applicability. CONCLUSION: The XAI-driven bias mitigation intervention effectively reduces sex-based disparities in COVID-19 severity prediction without the significant accuracy loss observed in traditional methods. This approach provides a framework for developing fair and accurate clinical decision support systems for older adults, which ensures equitable care in clinical risk stratification and resource allocation.",
      "journal": "JAMIA open",
      "year": "2026",
      "doi": "10.1093/jamiaopen/ooaf171",
      "authors": "Nejadshamsi Shayan et al.",
      "keywords": "COVID-19 severity; bias mitigation; explainable AI; fairness; machine learning classifier",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41522832/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "XGBoost/Gradient Boosting; Clinical Decision Support",
      "health_domain": "Pulmonology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Adversarial Debiasing; Fairness Metrics Evaluation; Representation Learning; Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: The XAI-driven bias mitigation intervention effectively reduces sex-based disparities in COVID-19 severity prediction without the significant accuracy loss observed in traditional methods. This approach provides a framework for developing fair and accurate clinical decision support systems for older adults, which ensures equitable care in clinical risk stratification and resource allocation.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12790453"
    },
    {
      "pmid": "41525640",
      "title": "Enhancing Fairness in Skin Lesion Classification for Medical Diagnosis Using Prune Learning.",
      "abstract": "Recent advances in deep learning have significantly improved the accuracy of skin lesion classification models, supporting medical diagnoses and promoting equitable healthcare. However, concerns remain about potential biases related to skin color, which can impact diagnostic outcomes. Ensuring fairness is challenging due to difficulties in classifying skin tones, high computational demands, and the complexity of objectively verifying fairness, given the continuous and context-dependent nature of skin tone and the dependence of fairness conclusions on metric choice and subgroup representation. To address these challenges, we propose a fairness algorithm for skin lesion classification that overcomes the challenges associated with achieving diagnostic fairness across varying skin tones. By calculating the skewness of the feature map in the convolution layer of the Visual Geometry Group network (VGG) and the patches and the heads of the Vision Transformer (ViT), our method reduces unnecessary channels related to skin tone, focusing instead on the lesion area. Application on VGG11 and ViT-B16, showed improved fairness metrics by 15-20% on average while maintaining accuracy and F1-score within 0.01 of the baseline. Additionally, the method reduced model size by 16% for VGG11 and decreased memory footprint for ViT-B16, without requiring skin tone labels at inference. Thus, the approach lowers computational costs and mitigates bias without relying on conventional statistical methods. It potentially reduces model size while maintaining fairness, making it more practical for real-world applications.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2026",
      "doi": "10.1109/JBHI.2026.3652910",
      "authors": "Paxton Kuniko et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41525640/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning; NLP/LLM",
      "health_domain": "Dermatology; ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "ICU",
      "key_findings": "Thus, the approach lowers computational costs and mitigates bias without relying on conventional statistical methods. It potentially reduces model size while maintaining fairness, making it more practical for real-world applications.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41531745",
      "title": "Fairness-aware K-means clustering in digital mental health for higher education students: a generalizable framework for equitable clustering.",
      "abstract": "OBJECTIVES: Higher education students, particularly those from underrepresented backgrounds, experience heightened levels of anxiety, depression, and burnout. Clinical informatics approaches leveraging K-means clustering can aid in mental health risk stratification, yet they often exacerbate disparities. We present a socially fair clustering framework that ensures equitable clustering costs across demographic groups while minimizing within-cluster variability. MATERIALS AND METHODS: Our framework compares standard and socially fair K-means clustering to assess the impact of demographic disparities. It identifies factors affecting clustering across demographics using omnibus and post hoc statistical tests. Subsequently, it quantifies the influence of statistically significant factors on cluster development. We illustrate our approach by identifying racially equitable clusters of mental health among students surveyed by the Healthy Minds Network. RESULTS: The socially fair clustering approach reduces disparities in clustering costs by as much as 30% across racial groups while maintaining consistency with standard K-means solutions in socioeconomically homogenous populations. Discrimination experiences were the strongest indicator of poorer mental health, whereas stable financial conditions and robust social engagement promoted resilience. DISCUSSION: Integrating fairness constraints into clustering algorithms reduces disparities in risk stratification and provides insights into socioeconomic drivers of student well-being. Our findings suggest that standard models may overpathologize middle-risk cohorts, whereas fairness-aware clustering yields partitions that better capture disparities. CONCLUSION: Our work demonstrates how integrating fairness-aware objectives into clustering algorithms can enhance equity in partitioning systems. The framework we present is broadly applicable to clustering problems across various biomedical informatics domains.",
      "journal": "JAMIA open",
      "year": "2026",
      "doi": "10.1093/jamiaopen/ooaf174",
      "authors": "Alluri Priyanshu et al.",
      "keywords": "cluster analysis; machine learning; medical informatics; mental health; risk assessment",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41531745/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Clustering",
      "health_domain": "Mental Health/Psychiatry; ICU/Critical Care; EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Constraints",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: Our work demonstrates how integrating fairness-aware objectives into clustering algorithms can enhance equity in partitioning systems. The framework we present is broadly applicable to clustering problems across various biomedical informatics domains.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12794019"
    },
    {
      "pmid": "41536573",
      "title": "Understanding and Addressing Bias in Artificial Intelligence Systems: A Primer for the Emergency Medicine Physician.",
      "abstract": "Artificial intelligence (AI) tools and technologies are increasingly being integrated into emergency medicine (EM) practice, not only offering potential benefits such as improved efficiency, better patient experience, and increased safety, but also resulting in potential risks including exacerbation of biases. These biases, inadvertently embedded in AI algorithms or training data, can adversely affect clinical decision making for diverse patient populations. Bias is a universal human attribute, subject to introduction into any human interaction. The risk with AI is magnification of, or even normalization of, patterns of biases across the health care ecosystem within tools that in time may be considered authoritative. This article, the work of members of the American College of Emergency Physicians (ACEP) AI Task Force, aims to equip emergency physicians (EPs) with a practical framework for understanding, identifying, and addressing bias in clinical and operational AI tools encountered in the emergency department (ED). For this publication, we have defined bias as a systematic flaw in a decision-making process that results in unfair or unintended outcomes that can be inadvertently embedded in AI algorithms or training data. This can result in adverse effects on clinical decision making for diverse patient populations. We begin by reviewing common sources of AI bias relevant to EM, including data, algorithmic, measurement, and human-interaction factors, and then, we discuss the potential pitfalls. Following this, we use illustrative examples from EM practice (eg, triage tools, risk stratification, and medical devices) to demonstrate how bias can manifest. We subsequently discuss the evolving regulatory landscape, structured assessment frameworks (including predeployment, continuous monitoring, and postdeployment steps), key principles (like sociotechnical perspectives and stakeholder engagement), and specific tools. Finally, this review outlines the EP's vital role in mitigation of AI-related biases through advocacy, local validation, clinical feedback, demanding transparency, and maintaining clinical judgment over automation.",
      "journal": "Journal of the American College of Emergency Physicians open",
      "year": "2026",
      "doi": "10.1016/j.acepjo.2025.100311",
      "authors": "Abbott Ethan E et al.",
      "keywords": "artificial intelligence; bias; emergency medicine",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41536573/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Emergency Medicine",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Emergency Department; Public Health/Population",
      "key_findings": "We subsequently discuss the evolving regulatory landscape, structured assessment frameworks (including predeployment, continuous monitoring, and postdeployment steps), key principles (like sociotechnical perspectives and stakeholder engagement), and specific tools. Finally, this review outlines the EP's vital role in mitigation of AI-related biases through advocacy, local validation, clinical feedback, demanding transparency, and maintaining clinical judgment over automation.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12797052"
    },
    {
      "pmid": "41540090",
      "title": "DermNet: integrative CNN-ViT architecture for bias mitigation in dermatological diagnostics using advanced unsupervised lesion segmentation.",
      "abstract": "In this paper, we propose a method for reducing the bias in skin disease identification for people of color with the aid of lesion only zero shot unsupervised approach that is then passed to the classifier Dermnet comprising of a hybrid Vision Transformer and Convolutional Neural Network, achieving robust validation accuracy of approximately 81%. Our Segmentation without training with labeled data as is the case with traditional U-Net has achieved an IOU of 90% across all skin colors in segmenting the lesion from skin effectively eradicating the impact of skin in the classification of disease.",
      "journal": "Scientific reports",
      "year": "2026",
      "doi": "10.1038/s41598-026-35697-x",
      "authors": "Imran Muhammad Huzaifa et al.",
      "keywords": "",
      "mesh_terms": "Humans; Neural Networks, Computer; Skin Diseases; Algorithms; Skin; Image Processing, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41540090/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning; NLP/LLM; Neural Network; Clustering",
      "health_domain": "Dermatology",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "In this paper, we propose a method for reducing the bias in skin disease identification for people of color with the aid of lesion only zero shot unsupervised approach that is then passed to the classifier Dermnet comprising of a hybrid Vision Transformer and Convolutional Neural Network, achieving robust validation accuracy of approximately 81%. Our Segmentation without training with labeled data as is the case with traditional U-Net has achieved an IOU of 90% across all skin colors in segmenti...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12880972"
    },
    {
      "pmid": "41542334",
      "title": "Ethical challenges and opportunities for integrating predictive analytics in community-based overdose prevention.",
      "abstract": "As predictive analytics become more widely integrated into local public health responses to the United States overdose epidemic, community-based substance use service providers have begun to adopt machine learning-based predictive tools to guide the allocation and delivery of overdose prevention services. While these tools hold promise for anticipating community overdose risk and enhancing the efficiency of overdose prevention resource distribution, outreach, and education efforts, their use in community settings raises substantial ethical and practical challenges. In this Viewpoint, we examine the application of predictive analytics to community-based overdose prevention through a public health ethics lens, drawing on principles of distributive justice, transparency, community participation, and implementation readiness. We outline five key ethical considerations for developers (i.e., institutional responsibility, oversimplification of complex social realities, data and algorithmic bias, community displacement in decision making, and equity trade-offs) and corresponding practical challenges for service providers. We offer five recommendations for developers, public health authorities, and frontline organizations to overcome challenges and ensure responsible, equity-driven implementation. As data-driven approaches to overdose prevention proliferate, ethical and participatory frameworks will be essential to ensure predictive tools strengthen, rather than undermine, community trust and health equity.",
      "journal": "Lancet regional health. Americas",
      "year": "2026",
      "doi": "10.1016/j.lana.2025.101345",
      "authors": "Allen Bennett et al.",
      "keywords": "Ethics; Machine learning; Overdose prevention; Predictive analytics; Public health",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41542334/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Generative AI",
      "health_domain": "Public Health",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We offer five recommendations for developers, public health authorities, and frontline organizations to overcome challenges and ensure responsible, equity-driven implementation. As data-driven approaches to overdose prevention proliferate, ethical and participatory frameworks will be essential to ensure predictive tools strengthen, rather than undermine, community trust and health equity.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12800477"
    },
    {
      "pmid": "41551000",
      "title": "A predictive model for cognitive decline using social determinants of health.",
      "abstract": "BACKGROUND: Early diagnosis of Alzheimer's disease and related dementias (AD/ADRD) is critical but often constrained by limited access to fluid and imaging biomarkers, particularly in low-resource settings. OBJECTIVE: To develop and evaluate a predictive model for cognitive decline using survey-based data, with attention to model interpretability and fairness. METHODS: Using data from the Mexican Health and Aging Study (MHAS), a nationally representative longitudinal survey of adults aged 50 and older (N = 4095), we developed a machine learning model to predict future cognitive scores. The model was trained on survey data from 2003 to 2012, encompassing demographic, lifestyle, and social determinants of health (SDoH) variables. A stacked ensemble approach combined five base models-Random Forest, LightGBM, XGBoost, Lasso, and K-Nearest Neighbors-with a Ridge regression meta-model. RESULTS: The model achieved a root-mean-square error (RMSE) of 39.25 (95 % CI: 38.12-40.52), representing 10.2 % of the cognitive score range, on a 20 % held-out test set. Features influencing predictions, included education level, age, reading behavior, floor material, mother's education level, social activity frequency, the interaction between the number of living children and age, and overall engagement in activities. Fairness analyses revealed model biases in underrepresented subgroups within the dataset, such as individuals with 7-9 years of education. DISCUSSION: These findings highlight the potential of using accessible, low-cost SDoH survey data for predicting risk of cognitive decline in aging populations. They also underscore the importance of incorporating fairness metrics into predictive modeling pipelines to ensure equitable performance across diverse groups.",
      "journal": "JAR life",
      "year": "2026",
      "doi": "10.1016/j.jarlif.2025.100056",
      "authors": "He Yingnan et al.",
      "keywords": "Aging; Bias analysis; Cognitive decline; Health disparities, predictive modeling; Interpretable models; Machine learning; Mexican health and aging study (MHAS); Social determinants of health (SDoH); Stacked model",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41551000/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Random Forest; XGBoost/Gradient Boosting; Ensemble Methods; Clinical Prediction Model",
      "health_domain": "ICU/Critical Care; Pediatrics; Neurology",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Geographic",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability; Ensemble Methods",
      "clinical_setting": "ICU; Public Health/Population; Safety-Net/Underserved",
      "key_findings": "RESULTS: The model achieved a root-mean-square error (RMSE) of 39.25 (95 % CI: 38.12-40.52), representing 10.2 % of the cognitive score range, on a 20 % held-out test set. Features influencing predictions, included education level, age, reading behavior, floor material, mother's education level, social activity frequency, the interaction between the number of living children and age, and overall engagement in activities. Fairness analyses revealed model biases in underrepresented subgroups withi...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12809124"
    },
    {
      "pmid": "41556569",
      "title": "Mapping Algorithmic Bias in AI-Powered Electrocardiogram Interpretation Across the AI Life Cycle: Protocol for a Scoping Review.",
      "abstract": "BACKGROUND: Artificial intelligence (AI)-powered analysis of electrocardiograms (ECGs) is reshaping cardiac diagnostics, offering faster and often more accurate detection of conditions such as arrhythmias and heart failure. However, growing evidence suggests that algorithmic bias, defined as performance disparities across patient subgroups, may undermine diagnostic equity. These biases can emerge at any stage of the AI life cycle, including data collection, model development, evaluation, deployment, and clinical use. If unaddressed, they risk exacerbating health disparities, particularly in underrepresented populations and low-resource settings. Early identification and mitigation of such bias are essential to ensuring diagnostic equity. OBJECTIVE: This scoping review protocol outlines a structured approach to mapping the evidence on algorithmic bias in AI-enabled ECG interpretation. Following the population-concept-context framework and PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) guidance, the planned review will systematically identify and categorize reported sources and types of bias, examine their effects on diagnostic performance across demographic and geographic subgroups, and document mitigation strategies applied throughout the AI life cycle. By synthesizing how bias and fairness considerations are handled in this field, this review aims to clarify existing evidence, highlight key gaps, and inform future efforts toward equitable and clinically trustworthy application of AI in cardiology. METHODS: We will conduct a comprehensive literature search across 5 electronic databases (PubMed, Embase, Cochrane CENTRAL, CINAHL, and IEEE Xplore) and gray literature sources. Eligible studies will include original research (2015-2025) evaluating the performance of AI-based ECG models across different subgroups or reporting on bias mitigation strategies. Two reviewers will independently screen studies, extract data using a standardized form, and resolve disagreements through consensus. This review will follow the PRISMA-ScR reporting framework. RESULTS: At the time of submission, study identification and screening has been completed. Database searches conducted in August and September 2025 yielded 430 records, with an additional 18 records identified through other sources. After duplicates removal, 398 unique records remained. Title and abstract screening led to the exclusion of 250 records, and 148 articles proceeded to full-text review. Following full-text assessment, 110 articles were evaluated for eligibility, of which 38 studies met the inclusion criteria and were included in the qualitative synthesis. The study selection process is summarized in a PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) flow diagram. Data extraction was conducted between November and December 2025. CONCLUSIONS: This review will be the first to comprehensively map the landscape of algorithmic bias in AI-powered ECG interpretation. By identifying patterns of inequity and evaluating proposed solutions, it will provide actionable insights for developers, clinicians, and policymakers aiming to promote fairness in AI-enabled cardiac care. INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): PRR1-10.2196/82486.",
      "journal": "JMIR research protocols",
      "year": "2026",
      "doi": "10.2196/82486",
      "authors": "Lawal Luqman et al.",
      "keywords": "AI; algorithmic bias; artificial intelligence; cardiovascular diagnostics; diagnostic fairness; electrocardiography; health equity; machine learning; scoping review",
      "mesh_terms": "Humans; Electrocardiography; Scoping Reviews as Topic; Algorithms; Artificial Intelligence; Bias; Research Design",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41556569/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Data Collection; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population; Safety-Net/Underserved",
      "key_findings": "CONCLUSIONS: This review will be the first to comprehensively map the landscape of algorithmic bias in AI-powered ECG interpretation. By identifying patterns of inequity and evaluating proposed solutions, it will provide actionable insights for developers, clinicians, and policymakers aiming to promote fairness in AI-enabled cardiac care. INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): PRR1-10.2196/82486.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12869145"
    },
    {
      "pmid": "41577988",
      "title": "Sex disparities in deep learning estimation of ejection fraction from cardiac magnetic resonance imaging.",
      "abstract": "The advent of artificial intelligence in cardiovascular imaging holds immense potential for earlier diagnoses, precision medicine, and improved disease management. However, the presence of sex-based disparities and strategies to mitigate biases in deep learning models for cardiac imaging remain understudied. In this study, we analyzed algorithmic bias in a foundation model that was pretrained on cardiac magnetic resonance imaging and radiology reports from multiple institutes and finetuned to estimate ejection fraction (EF) on the UK Biobank dataset. The model performed significantly worse in EF estimation for females than males in the diagnosis of reduced EF. Algorithmic fairness did not improve despite masking of protected attributes in radiology reports and data resampling, although explicit input of sex in model finetuning may improve EF estimation in some cases. The underdiagnosis of reduced EF among females holds critical implications for the exacerbation of existing sex-based disparities in cardiovascular health. We advise caution in the development of models for cardiovascular imaging to avoid such pitfalls.",
      "journal": "NPJ digital medicine",
      "year": "2026",
      "doi": "10.1038/s41746-025-02330-6",
      "authors": "Kaur Dhamanpreet et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41577988/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Foundation Model",
      "health_domain": "Radiology/Medical Imaging; Cardiology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling",
      "clinical_setting": "Not specified",
      "key_findings": "The underdiagnosis of reduced EF among females holds critical implications for the exacerbation of existing sex-based disparities in cardiovascular health. We advise caution in the development of models for cardiovascular imaging to avoid such pitfalls.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12894861"
    },
    {
      "pmid": "41589030",
      "title": "Mitigating Disparities in Prostate Cancer Survival Prediction Through Fairness-Aware Machine Learning Models.",
      "abstract": "PURPOSE: Prediction models can contribute to disparities in care by performing unequally across demographic groups. While fairness-aware methods have been explored for binary outcomes, applications to survival analysis remain limited. This study compares two fairness-aware deep learning survival models to mitigate racial disparities in predicting survival after radical prostatectomy for prostate cancer. METHODS: We used the National Cancer Database to train deep Cox proportional hazards models for overall survival. Two fairness-aware approaches, Fair Deep Cox Proportional Hazards Model (Fair DCPH) and Group Distributionally Robust Optimization Deep Cox Proportional Hazards Model (GroupDRO DCPH), were compared against a standard Deep Cox model (Baseline). Model fairness was assessed via cross-group and within-group concordance indices (C-index). RESULTS: Among 418,968 included patients, 78.5% were White, with smaller proportions of Black (13.2%), Hispanic (4.5%), Asian (1.9%), and Other (2.0%) patients. The baseline DCPH model achieved a cross-group C-index of 0.699 for White patients but showed reduced performance for Black (0.678) and Hispanic (0.689) patients. Fairness-aware models improved cross-group C-indices; for Black patients, cross-group C-index increased to 0.692 (Fair DCPH) and 0.696 (GroupDRO DCPH); for Hispanic patients, to 0.693 and 0.697, respectively. Cross-group C-index also improved in the Asian subgroup, where the C-index rose from 0.696 (Baseline DCPH) to 0.702 (Fair DCPH) and 0.707 (GroupDRO DCPH), with minimal performance loss observed for White patients. CONCLUSION: We benchmark two fairness-aware survival models that address racial disparities in post-prostatectomy survival prediction. These methods can be extended to other time-to-event models to ensure equitable care supported by fair prediction models.",
      "journal": "Cancer medicine",
      "year": "2026",
      "doi": "10.1002/cam4.71544",
      "authors": "Do Hyungrok et al.",
      "keywords": "bias; fairness; machine learning; prostate cancer; survival",
      "mesh_terms": "Humans; Male; Prostatic Neoplasms; Middle Aged; Aged; Prostatectomy; Healthcare Disparities; Machine Learning; Proportional Hazards Models; United States",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41589030/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Clinical Prediction Model; Survival Analysis",
      "health_domain": "Oncology",
      "bias_axes": "Race/Ethnicity",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: We benchmark two fairness-aware survival models that address racial disparities in post-prostatectomy survival prediction. These methods can be extended to other time-to-event models to ensure equitable care supported by fair prediction models.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12835780"
    },
    {
      "pmid": "41624564",
      "title": "Generative pre-trained transformer reinforces historical gender bias in diagnosing women's cardiovascular symptoms.",
      "abstract": "AIMS: Large language models (LLMs) such as GPT are increasingly used to generate clinical teaching cases and support diagnostic reasoning. However, biases in their training data may skew the portrayal and interpretation of cardiovascular symptoms in women, potentially leading to delayed or inaccurate diagnoses. We assessed GPT-4o's and GPT-4's gender representation in simulated cardiovascular cases and GPT-4o's diagnostic performance across genders using real patient notes. METHODS AND RESULTS: First, GPT-4o and GPT-4 were each prompted to generate 15 000 simulated cases spanning 15 cardiovascular conditions with known gender prevalence differences. The model's gender distributions were compared to U.S. prevalence data from large national datasets (Centers for Disease Control and Prevention and National Inpatient Sample) using FDR-corrected \u03c7\u00b2 tests, finding a significant deviation (P < 0.0001). In 14 GPT-4-generated conditions (93%), male patients were overrepresented compared to females by a mean of 30% (SD 8.6%). Second, fifty de-identified cardiovascular patient notes were extracted from the MIMIC-IV-Note database. Patient gender was systematically swapped in each note, and GPT-4o was asked to produce differential diagnoses for each version (10 000 total prompts). Diagnostic accuracy across genders was determined by comparing model outputs to actual discharge diagnoses via FDR-corrected Mann-Whitney U tests, revealing significant diagnostic accuracy differences in 11 cases (22%). Female patients received lower accuracy scores than males for key conditions like coronary artery disease (P < 0.01), abdominal aortic aneurysm (P < 1.0 \u00d7 10-9), and atrial fibrillation (P < 0.01). CONCLUSION: GPT-4o underrepresented women in simulated cardiovascular scenarios and less accurately diagnosed female patients with critical conditions. These biases risk reinforcing historical disparities in cardiovascular care. Future efforts should focus on bias detection and mitigation.",
      "journal": "European heart journal. Digital health",
      "year": "2026",
      "doi": "10.1093/ehjdh/ztaf131",
      "authors": "Krieger Katherine et al.",
      "keywords": "Cardiovascular Diagnosis; GPT; Gender Bias; Large Language Model; Medical Education",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41624564/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Cardiology",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "CONCLUSION: GPT-4o underrepresented women in simulated cardiovascular scenarios and less accurately diagnosed female patients with critical conditions. These biases risk reinforcing historical disparities in cardiovascular care. Future efforts should focus on bias detection and mitigation.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12853122"
    },
    {
      "pmid": "41627658",
      "title": "Bias and Oversight in Clinical AI: A Review of Decision Support Tools and Equity Frameworks.",
      "abstract": "Artificial intelligence (AI) decision support tools (DSTs) are increasingly used across clinical settings to improve efficiency and support decision-making. However, these tools risk perpetuating existing healthcare disparities if not designed and implemented with transparency, equity, and cultural sensitivity. This review explores how racial and ethnic biases manifest within AI-driven DSTs and evaluates the role of governance frameworks in mitigating such harms. It examines the implications of biased algorithms, presents case examples highlighting disparities in tool performance, and critically assesses the adequacy of current national and international regulatory guidance. The review reports that bias can stem from unrepresentative training datasets, exclusion of equity auditing in design, and the absence of mandated transparency in reporting. Although several frameworks exist to guide development and reporting, few are mandatory, and most do not include equity as a core criterion. The current UK and US regulatory models are decentralized and lack mechanisms to systematically detect or prevent bias. To prevent biased tools from entering practice, equity must be structurally embedded across the AI lifecycle. Embedding equity into AI tools requires standardized subgroup performance reporting, mandating fairness assessments, and establishing national and global governance standards to ensure AI tools serve all populations equitably.",
      "journal": "Journal of general internal medicine",
      "year": "2026",
      "doi": "10.1007/s11606-026-10229-5",
      "authors": "Adegunle Farrah et al.",
      "keywords": "algorithmic bias; artificial intelligence; clinical decision support tools; digital health; ethnic disparities; healthcare inequity; racial bias",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41627658/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Representation Learning; Bias Auditing Framework",
      "clinical_setting": "Public Health/Population",
      "key_findings": "To prevent biased tools from entering practice, equity must be structurally embedded across the AI lifecycle. Embedding equity into AI tools requires standardized subgroup performance reporting, mandating fairness assessments, and establishing national and global governance standards to ensure AI tools serve all populations equitably.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41632023",
      "title": "Fairness Correction in COVID-19 Predictive Models Using Demographic Optimization: Algorithm Development and Validation Study.",
      "abstract": "BACKGROUND: COVID-19 forecasting models have been used to inform decision-making around resource allocation and intervention decisions, such as hospital beds or stay-at-home orders. State-of-the-art forecasting models often use multimodal data, including mobility or sociodemographic data, to enhance COVID-19 case prediction models. Nevertheless, related work has revealed under-reporting bias in COVID-19 cases as well as sampling bias in mobility data for certain minority racial and ethnic groups, which affects the fairness of COVID-19 predictions across racial and ethnic groups. OBJECTIVE: This study aims to introduce a fairness correction method that works for forecasting COVID-19 cases at an aggregate geographic level. METHODS: We use hard and soft error parity analyses on existing fairness frameworks and demonstrate that our proposed method, Demographic Optimization (DemOpts), performs better in both scenarios. RESULTS: We first demonstrate that state-of-the-art COVID-19 deep learning models produce mean prediction errors that are significantly different across racial and ethnic groups at larger geographic scales. We then propose a novel debiasing method, DemOpts, to increase the fairness of deep learning-based forecasting models trained on potentially biased datasets. Our results show that DemOpts can achieve better error parity than other state-of-the-art debiasing approaches, thus effectively reducing the differences in the mean error distributions across racial and ethnic groups. CONCLUSIONS: We introduce DemOpts, which reduces error parity differences compared with other approaches and generates fairer forecasting models compared with other approaches in the literature.",
      "journal": "Online journal of public health informatics",
      "year": "2026",
      "doi": "10.2196/78235",
      "authors": "Awasthi Naman et al.",
      "keywords": "COVID-19 forecasting; deep learning model; fairness; regression; time series model",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41632023/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Clinical Prediction Model",
      "health_domain": "Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Socioeconomic Status; Geographic",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "CONCLUSIONS: We introduce DemOpts, which reduces error parity differences compared with other approaches and generates fairer forecasting models compared with other approaches in the literature.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12866456"
    },
    {
      "pmid": "41636665",
      "title": "PREVENT Equations in Young Adults: Fairness, Calibration, and Performance Across Racial and Ethnic Groups.",
      "abstract": "BACKGROUND: Cardiovascular disease (CVD) is increasing among young adults. The American Heart Association's PREVENT (Predicting Risk of Cardiovascular Disease Events) equations estimate risk of CVD, atherosclerotic cardiovascular disease (ASCVD), and heart failure (HF) for primary prevention. Augmented equations additionally include zip code-based social deprivation index (SDI) to address adverse social exposures. OBJECTIVES: We assessed performance and algorithmic fairness of base and SDI-augmented PREVENT equations in young adults aged 30 to 39 years, defining fairness as similar performance across racial and ethnic groups. An exploratory analysis was conducted among young adults aged 20 to 29 years. METHODS: We included Kaiser Permanente Southern California members aged 20 to 39 years without prior CVD between 2008 and 2009, followed through 2019. We compared 10-year predicted and observed CVD, ASCVD, and HF events for base and SDI-augmented PREVENT models. Performance (Harell's C, calibration slopes, mean calibration) and fairness (concordance imparity, fair calibration) were estimated by race and ethnicity and age group (30-39 years [primary analysis], 20-29 years [exploratory analysis]). RESULTS: Among 161,202 young adults aged 30 to 39 years (60.0% women; 51.7% Hispanic, 26.9% non-Hispanic White, 12.5% Asian/Pacific Islander, 8.9% non-Hispanic Black), 10-year CVD incidence was 0.7%. Race-specific Harrell's C-statistics for the base PREVENT CVD model ranged from 0.68 to 0.72, yielding low concordance imparity (0.04; 95% CI: 0.02-0.22) which implies fair discrimination. Mean calibration showed underprediction in non-Hispanic Black participants (0.54; 95% CI: 0.48-0.65) vs other groups (range: 0.96-1.07). In fair calibration testing, prediction errors differed across racial and ethnic groups. Results were similar for ASCVD and HF. Adding SDI did not improve performance or fairness despite disparities across groups. In exploratory analyses among 80,978 individuals aged 20 to 29 years, performance and fairness results were similar. CONCLUSIONS: This large, diverse cohort of young adults demonstrates how the PREVENT equations may perform when applied in real-world clinical settings, reflecting the true operational environment faced by large health systems. Applications of PREVENT in clinical patient care, eg, early initiation of preventive strategies, should consider variations in model performance across age, race, and ethnicity.",
      "journal": "Journal of the American College of Cardiology",
      "year": "2026",
      "doi": "10.1016/j.jacc.2025.12.019",
      "authors": "Gauen Abigail M et al.",
      "keywords": "algorithmic fairness; cardiovascular disease; epidemiology; primary prevention; risk prediction",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41636665/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: This large, diverse cohort of young adults demonstrates how the PREVENT equations may perform when applied in real-world clinical settings, reflecting the true operational environment faced by large health systems. Applications of PREVENT in clinical patient care, eg, early initiation of preventive strategies, should consider variations in model performance across age, race, and ethnicity.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41667212",
      "title": "Mechanistic interpretability of reinforcement learning in Medicaid care coordination.",
      "abstract": "OBJECTIVE: To expose reasoning pathways of a reinforcement learning policy for Medicaid care coordination, develop an error taxonomy and implement fairness-aware guardrails. DESIGN: Retrospective interpretability audit using attention analysis, Shapley explanations, sparse autoencoder feature discovery and blinded clinician adjudication. SETTING: Medicaid care coordination programmes in Washington, Virginia and Ohio (July 2023-June 2025). PARTICIPANTS: 250\u2009000 intervention decisions; 200 divergent cases reviewed by five clinicians. MAIN OUTCOME MEASURES: Calibrated harm prediction; algorithmic clearance and residual harm rates; error taxonomy frequencies; subgroup fairness metrics. RESULTS: The conformal model achieved area under the receiver operating characteristic curve of 0.80 (95% CI 0.78 to 0.82), clearing 89.5% (95% CI 88.9% to 90.1%) of decisions with 1.22% (95% CI 1.14% to 1.30%) residual harm versus 6.67% (95% CI 6.02% to 7.32%) for flagged decisions. Sparse autoencoders identified seven reasoning motifs linking social determinants to clinical cascades. The error taxonomy revealed premise errors (48%, 95%\u2009CI 41% to 55%), calibration failures (27%, 95%\u2009CI 21% to 33%) and contextual blind spots (25%, 95%\u2009CI 19% to 31%). Divergence was higher for telehealth visits (11.2%) and behavioural health patients (10.7% vs 6.9%, p<0.001). Fairness optimisation reduced race-group disparity by 37% (95% CI 22% to 48%) and sex-group disparity by 28% (95% CI 14% to 39%). Reviewers rated 23% (95% CI 17% to 29%) of overridden recommendations as well-matched, confirming appropriate human oversight. CONCLUSIONS: Mechanistic interpretability transforms opaque algorithmic assistance into auditable decision support, providing a governance scaffold for clinical artificial intelligence deployment.",
      "journal": "BMJ health & care informatics",
      "year": "2026",
      "doi": "10.1136/bmjhci-2025-101935",
      "authors": "Basu Sanjay et al.",
      "keywords": "BMJ Health Informatics; Decision Making, Computer-Assisted",
      "mesh_terms": "Humans; United States; Medicaid; Retrospective Studies; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41667212/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Reinforcement Learning",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Fairness Metrics Evaluation; Explainability/Interpretability; Bias Auditing Framework",
      "clinical_setting": "Telehealth/Remote",
      "key_findings": "CONCLUSIONS: Mechanistic interpretability transforms opaque algorithmic assistance into auditable decision support, providing a governance scaffold for clinical artificial intelligence deployment.",
      "ft_include": true,
      "ft_reason": "Included: substantial approach content in abstract",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41680855",
      "title": "AI/ML-based strategies for enhancing equity, diversity, and inclusion in randomized clinical trials.",
      "abstract": "This paper introduces a conceptual framework designed to embed equity, diversity, and inclusion (EDI) across all stages of the clinical trial lifecycle. Randomized clinical trials (RCTs) remain the most reliable method for evaluating medical treatments, yet persistent gaps in representation undermine their validity and fairness. Women, older adults, racial and ethnic minorities, and socioeconomically disadvantaged groups are often underrepresented, raising concerns about whether trial results can be generalized to all patients. This lack of inclusivity not only limits scientific rigor but also risks reinforcing existing health disparities. Recent advances in artificial intelligence (AI) and machine learning (ML) provide new opportunities to address these challenges. These technologies can support more inclusive study designs, enable targeted recruitment of underrepresented populations, and monitor diversity in real time throughout the trial process. They can also be applied to analyze outcomes with fairness-aware methods, helping ensure that results are meaningful across diverse subgroups. In this work, we propose an AI/ML-based framework aimed at operationalizing equity, diversity, and inclusion in clinical research. The framework integrates predictive modeling, adaptive trial designs, and continuous bias detection with ethical and legal safeguards to ensure responsible deployment. By embedding fairness into every stage of the trial lifecycle, this approach offers a pathway toward more representative and trustworthy evidence in medical science. Our analysis reveals the persistent gaps across demographic groups in current RCTs, demonstrating the urgent requirement for systematic intervention. This study also contributes a comprehensive AI/ML framework that operationalizes equity through predictive modeling, adaptive designs, and continuous bias monitoring, providing a structured pathway for researchers to enhance both the scientific validity and ethical integrity of clinical trials.",
      "journal": "Trials",
      "year": "2026",
      "doi": "10.1186/s13063-026-09537-2",
      "authors": "Abbidi Shashidar Reddy et al.",
      "keywords": "Artificial intelligence; Bias detection; Clinical trials; Diversity; Fairness; Health equity; Inclusion; Machine learning; Recruitment optimization",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41680855/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Representation Learning",
      "clinical_setting": "Public Health/Population; Clinical Trial",
      "key_findings": "Our analysis reveals the persistent gaps across demographic groups in current RCTs, demonstrating the urgent requirement for systematic intervention. This study also contributes a comprehensive AI/ML framework that operationalizes equity through predictive modeling, adaptive designs, and continuous bias monitoring, providing a structured pathway for researchers to enhance both the scientific validity and ethical integrity of clinical trials.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41682156",
      "title": "Federated Learning in Healthcare Ethics: A Systematic Review of Privacy-Preserving and Equitable Medical AI.",
      "abstract": "Background/Objectives: Federated learning (FL) offers a way for healthcare institutions to collaboratively train machine learning models without sharing sensitive patient data. This systematic review aims to comprehensively synthesize the ethical dimensions of FL in healthcare, integrating privacy preservation, algorithmic fairness, governance, and equitable access into a unified analytical framework. The application of FL in healthcare between January 2020 and December 2024 is examined, with a focus on ethical issues such as algorithmic fairness, privacy preservation, governance, and equitable access. Methods: Following PRISMA guidelines, six databases (PubMed, IEEE Xplore, Web of Science, Scopus, ACM Digital Library, and arXiv) were searched. The PROSPERO registration is CRD420251274110. Studies were selected if they described FL implementations in healthcare settings and explicitly discussed ethical considerations. Key data extracted included FL architectures, privacy-preserving mechanisms, such as differential privacy, secure multiparty computation, and encryption, as well as fairness metrics, governance models, and clinical application domains. Results: Out of 3047 records, 38 met the inclusion criteria. The most popular applications were found in medical imaging and electronic health records, especially in radiology and oncology. Through thematic analysis, four key ethical themes emerged: algorithmic fairness, which addresses differences between clients and attributes; privacy protection through formal guarantees and cryptographic techniques; governance models, which emphasize accountability, transparency, and stakeholder engagement; and equitable distribution of computing resources for institutions with limited resources. Considerable variation was observed in how fairness and privacy trade-offs were evaluated, and only a few studies reported real-world clinical deployment. Conclusions: FL has significant potential to promote ethical AI in healthcare, but advancement will require the development of common fairness standards, workable governance plans, and systems to guarantee fair benefit sharing. Future studies should develop standardized fairness metrics, implement multi-stakeholder governance frameworks, and prioritize real-world clinical validation beyond proof-of-concept implementations.",
      "journal": "Healthcare (Basel, Switzerland)",
      "year": "2026",
      "doi": "10.3390/healthcare14030306",
      "authors": "Mir Bilal Ahmad et al.",
      "keywords": "algorithmic fairness; bias mitigation; data governance; distributed machine learning; federated learning; health equity; healthcare ethics; medical AI; privacy preservation",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41682156/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Federated Learning; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Oncology; EHR/Health Informatics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Federated Learning",
      "clinical_setting": "Not specified",
      "key_findings": "Conclusions: FL has significant potential to promote ethical AI in healthcare, but advancement will require the development of common fairness standards, workable governance plans, and systems to guarantee fair benefit sharing. Future studies should develop standardized fairness metrics, implement multi-stakeholder governance frameworks, and prioritize real-world clinical validation beyond proof-of-concept implementations.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12896918"
    },
    {
      "pmid": "36823101",
      "title": "Practical, epistemic and normative implications of algorithmic bias in healthcare artificial intelligence: a qualitative study of multidisciplinary expert perspectives.",
      "abstract": "BACKGROUND: There is a growing concern about artificial intelligence (AI) applications in healthcare that can disadvantage already under-represented and marginalised groups (eg, based on gender or race). OBJECTIVES: Our objectives are to canvas the range of strategies stakeholders endorse in attempting to mitigate algorithmic bias, and to consider the ethical question of responsibility for algorithmic bias. METHODOLOGY: The study involves in-depth, semistructured interviews with healthcare workers, screening programme managers, consumer health representatives, regulators, data scientists and developers. RESULTS: Findings reveal considerable divergent views on three key issues. First, views on whether bias is a problem in healthcare AI varied, with most participants agreeing bias is a problem (which we call the bias-critical view), a small number believing the opposite (the bias-denial view), and some arguing that the benefits of AI outweigh any harms or wrongs arising from the bias problem (the bias-apologist view). Second, there was a disagreement on the strategies to mitigate bias, and who is responsible for such strategies. Finally, there were divergent views on whether to include or exclude sociocultural identifiers (eg, race, ethnicity or gender-diverse identities) in the development of AI as a way to mitigate bias. CONCLUSION/SIGNIFICANCE: Based on the views of participants, we set out responses that stakeholders might pursue, including greater interdisciplinary collaboration, tailored stakeholder engagement activities, empirical studies to understand algorithmic bias and strategies to modify dominant approaches in AI development such as the use of participatory methods, and increased diversity and inclusion in research teams and research participant recruitment and selection.",
      "journal": "Journal of medical ethics",
      "year": "2025",
      "doi": "10.1136/jme-2022-108850",
      "authors": "Aquino Yves Saint James et al.",
      "keywords": "Decision Making; Ethics; Information Technology; Policy",
      "mesh_terms": "Humans; Artificial Intelligence; Qualitative Research; Algorithms; Female; Male; Bias; Delivery of Health Care; Health Personnel; Attitude of Health Personnel; Interviews as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36823101/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "RESULTS: Findings reveal considerable divergent views on three key issues. First, views on whether bias is a problem in healthcare AI varied, with most participants agreeing bias is a problem (which we call the bias-critical view), a small number believing the opposite (the bias-denial view), and some arguing that the benefits of AI outweigh any harms or wrongs arising from the bias problem (the bias-apologist view). Second, there was a disagreement on the strategies to mitigate bias, and who is...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12171461"
    },
    {
      "pmid": "38953330",
      "title": "Bias in artificial intelligence for medical imaging: fundamentals, detection, avoidance, mitigation, challenges, ethics, and prospects.",
      "abstract": "Although artificial intelligence (AI) methods hold promise for medical imaging-based prediction tasks, their integration into medical practice may present a double-edged sword due to bias (i.e., systematic errors). AI algorithms have the potential to mitigate cognitive biases in human interpretation, but extensive research has highlighted the tendency of AI systems to internalize biases within their model. This fact, whether intentional or not, may ultimately lead to unintentional consequences in the clinical setting, potentially compromising patient outcomes. This concern is particularly important in medical imaging, where AI has been more progressively and widely embraced than any other medical field. A comprehensive understanding of bias at each stage of the AI pipeline is therefore essential to contribute to developing AI solutions that are not only less biased but also widely applicable. This international collaborative review effort aims to increase awareness within the medical imaging community about the importance of proactively identifying and addressing AI bias to prevent its negative consequences from being realized later. The authors began with the fundamentals of bias by explaining its different definitions and delineating various potential sources. Strategies for detecting and identifying bias were then outlined, followed by a review of techniques for its avoidance and mitigation. Moreover, ethical dimensions, challenges encountered, and prospects were discussed.",
      "journal": "Diagnostic and interventional radiology (Ankara, Turkey)",
      "year": "2025",
      "doi": "10.4274/dir.2024.242854",
      "authors": "Ko\u00e7ak Burak et al.",
      "keywords": "Artificial intelligence; bias; fairness; machine learning; medical imaging; radiology",
      "mesh_terms": "Humans; Artificial Intelligence; Diagnostic Imaging; Bias; Algorithms",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38953330/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Computer Vision/Imaging AI; Generative AI",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "Strategies for detecting and identifying bias were then outlined, followed by a review of techniques for its avoidance and mitigation. Moreover, ethical dimensions, challenges encountered, and prospects were discussed.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11880872"
    },
    {
      "pmid": "39075715",
      "title": "Empowering nurses to champion Health equity & BE FAIR: Bias elimination for fair and responsible AI in healthcare.",
      "abstract": "BACKGROUND: The concept of health equity by design encompasses a multifaceted approach that integrates actions aimed at eliminating biased, unjust, and correctable differences among groups of people as a fundamental element in the design of algorithms. As algorithmic tools are increasingly integrated into clinical practice at multiple levels, nurses are uniquely positioned to address challenges posed by the historical marginalization of minority groups and its intersections with the use of \"big data\" in healthcare settings; however, a coherent framework is needed to ensure that nurses receive appropriate training in these domains and are equipped to act effectively. PURPOSE: We introduce the Bias Elimination for Fair AI in Healthcare (BE FAIR) framework, a comprehensive strategic approach that incorporates principles of health equity by design, for nurses to employ when seeking to mitigate bias and prevent discriminatory practices arising from the use of clinical algorithms in healthcare. By using examples from a \"real-world\" AI governance framework, we aim to initiate a wider discourse on equipping nurses with the skills needed to champion the BE FAIR initiative. METHODS: Drawing on principles recently articulated by the Office of the National Coordinator for Health Information Technology, we conducted a critical examination of the concept of health equity by design. We also reviewed recent literature describing the risks of artificial intelligence (AI) technologies in healthcare as well as their potential for advancing health equity. Building on this context, we describe the BE FAIR framework, which has the potential to enable nurses to take a leadership role within health systems by implementing a governance structure to oversee the fairness and quality of clinical algorithms. We then examine leading frameworks for promoting health equity to inform the operationalization of BE FAIR within a local AI governance framework. RESULTS: The application of the BE FAIR framework within the context of a working governance system for clinical AI technologies demonstrates how nurses can leverage their expertise to support the development and deployment of clinical algorithms, mitigating risks such as bias and promoting ethical, high-quality care powered by big data and AI technologies. CONCLUSION AND RELEVANCE: As health systems learn how well-intentioned clinical algorithms can potentially perpetuate health disparities, we have an opportunity and an obligation to do better. New efforts empowering nurses to advocate for BE FAIR, involving them in AI governance, data collection methods, and the evaluation of tools intended to reduce bias, mark important steps in achieving equitable healthcare for all.",
      "journal": "Journal of nursing scholarship : an official publication of Sigma Theta Tau International Honor Society of Nursing",
      "year": "2025",
      "doi": "10.1111/jnu.13007",
      "authors": "Cary Michael P et al.",
      "keywords": "Health equity; artificial intelligence; ethics; nursing; social determinants of health",
      "mesh_terms": "Humans; Health Equity; Artificial Intelligence; Empowerment",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39075715/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "RESULTS: The application of the BE FAIR framework within the context of a working governance system for clinical AI technologies demonstrates how nurses can leverage their expertise to support the development and deployment of clinical algorithms, mitigating risks such as bias and promoting ethical, high-quality care powered by big data and AI technologies. CONCLUSION AND RELEVANCE: As health systems learn how well-intentioned clinical algorithms can potentially perpetuate health disparities, we...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11771545"
    },
    {
      "pmid": "39386055",
      "title": "The Impact of Race, Ethnicity, and Sex on Fairness in Artificial Intelligence for Glaucoma Prediction Models.",
      "abstract": "OBJECTIVE: Despite advances in artificial intelligence (AI) in glaucoma prediction, most works lack multicenter focus and do not consider fairness concerning sex, race, or ethnicity. This study aims to examine the impact of these sensitive attributes on developing fair AI models that predict glaucoma progression to necessitating incisional glaucoma surgery. DESIGN: Database study. PARTICIPANTS: Thirty-nine thousand ninety patients with glaucoma, as identified by International Classification of Disease codes from 7 academic eye centers participating in the Sight OUtcomes Research Collaborative. METHODS: We developed XGBoost models using 3 approaches: (1) excluding sensitive attributes as input features, (2) including them explicitly as input features, and (3) training separate models for each group. Model input features included demographic details, diagnosis codes, medications, and clinical information (intraocular pressure, visual acuity, etc.), from electronic health records. The models were trained on patients from 5 sites (N\u00a0=\u00a027\u00a0999) and evaluated on a held-out internal test set (N\u00a0=\u00a03499) and 2 external test sets consisting of N\u00a0=\u00a01550 and N\u00a0=\u00a02542 patients. MAIN OUTCOMES AND MEASURES: Area under the receiver operating characteristic curve (AUROC) and equalized odds on the test set and external sites. RESULTS: Six thousand six hundred eighty-two (17.1%) of 39\u00a0090 patients underwent glaucoma surgery with a mean age of 70.1 (standard deviation 14.6) years, 54.5% female, 62.3% White, 22.1% Black, and 4.7% Latinx/Hispanic. We found that not including the sensitive attributes led to better classification performance (AUROC: 0.77-0.82) but worsened fairness when evaluated on the internal test set. However, on external test sites, the opposite was true: including sensitive attributes resulted in better classification performance (AUROC: external #1 - [0.73-0.81], external #2 - [0.67-0.70]), but varying degrees of fairness for sex and race as measured by equalized odds. CONCLUSIONS: Artificial intelligence models predicting whether patients with glaucoma progress to surgery demonstrated bias with respect to sex, race, and ethnicity. The effect of sensitive attribute inclusion and exclusion on fairness and performance varied based on internal versus external test sets. Prior to deployment, AI models should be evaluated for fairness on the target population. FINANCIAL DISCLOSURES: Proprietary or commercial disclosure may be found in the Footnotes and Disclosures at the end of this article.",
      "journal": "Ophthalmology science",
      "year": "2025",
      "doi": "10.1016/j.xops.2024.100596",
      "authors": "Ravindranath Rohith et al.",
      "keywords": "Bias; Fairness; Glaucoma; Health disparities; Machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39386055/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "XGBoost/Gradient Boosting; Clinical Prediction Model",
      "health_domain": "Ophthalmology; EHR/Health Informatics; Surgery",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: Artificial intelligence models predicting whether patients with glaucoma progress to surgery demonstrated bias with respect to sex, race, and ethnicity. The effect of sensitive attribute inclusion and exclusion on fairness and performance varied based on internal versus external test sets. Prior to deployment, AI models should be evaluated for fairness on the target population.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11462200"
    },
    {
      "pmid": "39444135",
      "title": "Laboratory Data as a Potential Source of Bias in Healthcare Artificial Intelligence and Machine Learning Models.",
      "abstract": "Artificial intelligence (AI) and machine learning (ML) are anticipated to transform the practice of medicine. As one of the largest sources of digital data in healthcare, laboratory results can strongly influence AI and ML algorithms that require large sets of healthcare data for training. Embedded bias introduced into AI and ML models not only has disastrous consequences for quality of care but also may perpetuate and exacerbate health disparities. The lack of test harmonization, which is defined as the ability to produce comparable results and the same interpretation irrespective of the method or instrument platform used to produce the result, may introduce aggregation bias into algorithms with potential adverse outcomes for patients. Limited interoperability of laboratory results at the technical, syntactic, semantic, and organizational levels is a source of embedded bias that limits the accuracy and generalizability of algorithmic models. Population-specific issues, such as inadequate representation in clinical trials and inaccurate race attribution, not only affect the interpretation of laboratory results but also may perpetuate erroneous conclusions based on AI and ML models in the healthcare literature.",
      "journal": "Annals of laboratory medicine",
      "year": "2025",
      "doi": "10.3343/alm.2024.0323",
      "authors": "Luu Hung S",
      "keywords": "Aggregation bias; Artificial intelligence; Clinical pathology; Diagnostic error; Health information interoperability; Logical Observation Identifiers Names and Codes; Machine learning; SNOMED CT",
      "mesh_terms": "Humans; Machine Learning; Artificial Intelligence; Algorithms; Bias; Delivery of Health Care",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39444135/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Clinical Trial; Laboratory/Pathology",
      "key_findings": "Limited interoperability of laboratory results at the technical, syntactic, semantic, and organizational levels is a source of embedded bias that limits the accuracy and generalizability of algorithmic models. Population-specific issues, such as inadequate representation in clinical trials and inaccurate race attribution, not only affect the interpretation of laboratory results but also may perpetuate erroneous conclusions based on AI and ML models in the healthcare literature.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11609702"
    },
    {
      "pmid": "39499598",
      "title": "Bias Amplification to Facilitate the Systematic Evaluation of Bias Mitigation Methods.",
      "abstract": "The future of artificial intelligence (AI) safety is expected to include bias mitigation methods from development to application. The complexity and integration of these methods could grow in conjunction with advances in AI and human-AI interactions. Numerous methods are being proposed to mitigate bias, but without a structured way to compare their strengths and weaknesses. In this work, we present two approaches to systematically amplify subgroup performance bias. These approaches allow for the evaluation and comparison of the effectiveness of bias mitigation methods on AI models by varying the degrees of bias, and can be applied to any classification model. We used these approaches to compare four off-the-shelf bias mitigation methods. Both amplification approaches promote the development of learning shortcuts in which the model forms associations between patient attributes and AI output. We demonstrate these approaches in a case study, evaluating bias in the determination of COVID status from chest x-rays. The maximum achieved increase in performance bias, measured as a difference in predicted prevalence, was 72% and 32% for bias between subgroups related to patient sex and race, respectively. These changes in predicted prevalence were not accompanied by substantial changes in the differences in subgroup area under the receiver operating characteristic curves, indicating that the increased bias is due to the formation of learning shortcuts, not a difference in ability to distinguish positive and negative patients between subgroups.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2025",
      "doi": "10.1109/JBHI.2024.3491946",
      "authors": "Burgon Alexis et al.",
      "keywords": "",
      "mesh_terms": "Humans; COVID-19; Bias; Artificial Intelligence; SARS-CoV-2; Female; Male; Radiography, Thoracic",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39499598/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The maximum achieved increase in performance bias, measured as a difference in predicted prevalence, was 72% and 32% for bias between subgroups related to patient sex and race, respectively. These changes in predicted prevalence were not accompanied by substantial changes in the differences in subgroup area under the receiver operating characteristic curves, indicating that the increased bias is due to the formation of learning shortcuts, not a difference in ability to distinguish positive and n...",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39500799",
      "title": "Unbiased and reproducible liver MRI-PDFF estimation using a scan protocol-informed deep learning method.",
      "abstract": "OBJECTIVE: To estimate proton density fat fraction (PDFF) from chemical shift encoded (CSE) MR images using a deep learning (DL)-based method that is precise and robust to different MR scanners and acquisition echo times (TEs). METHODS: Variable echo times neural network (VET-Net) is a two-stage framework that first estimates nonlinear variables of the CSE-MR signal model, to posteriorly estimate water/fat signal components using the least-squares method. VET-Net incorporates a vector with TEs as an auxiliary input, therefore enabling PDFF calculation with any TE setting. A single-site liver CSE-MRI dataset (188 subjects, 4146 axial slices) was considered, which was split into training (150 subjects), validation (18), and testing (20) subsets. Testing subjects were scanned using several protocols with different TEs, which we then used to measure the PDFF reproducibility coefficient (RDC) at two regions of interest (ROIs): the right posterior and left hepatic lobes. An open-source multi-site and multi-vendor fat-water phantom dataset was also used for PDFF bias assessment. RESULTS: VET-Net showed RDCs of 1.71% and 1.04% on the right posterior and left hepatic lobes, respectively, across different TEs, which was comparable to a reference graph cuts-based method (RDCs\u2009=\u20091.71% and 0.86%). VET-Net also showed a smaller PDFF bias (-0.55%) than graph cuts (0.93%) when tested on a multi-site phantom dataset. Reproducibility (1.94% and 1.59%) and bias (-2.04%) were negatively affected when the auxiliary TE input was not considered. CONCLUSION: VET-Net provided unbiased and precise PDFF estimations using CSE-MR images from different hardware vendors and different TEs, outperforming conventional DL approaches. KEY POINTS: Question Reproducibility of liver PDFF DL-based approaches on different scan protocols or manufacturers is not validated. Findings VET-Net showed a PDFF bias of -0.55% on a multi-site phantom dataset, and RDCs of 1.71% and 1.04% at two liver ROIs. Clinical relevance VET-Net provides efficient, in terms of scan and processing times, and unbiased PDFF estimations across different MR scanners and scan protocols, and therefore it can be leveraged to expand the use of MRI-based liver fat quantification to assess hepatic steatosis.",
      "journal": "European radiology",
      "year": "2025",
      "doi": "10.1007/s00330-024-11164-x",
      "authors": "Meneses Juan P et al.",
      "keywords": "Biomarkers; Deep learning; Liver; Magnetic resonance imaging",
      "mesh_terms": "Humans; Deep Learning; Magnetic Resonance Imaging; Reproducibility of Results; Liver; Male; Female; Phantoms, Imaging; Middle Aged; Adult; Adipose Tissue; Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39500799/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: VET-Net provided unbiased and precise PDFF estimations using CSE-MR images from different hardware vendors and different TEs, outperforming conventional DL approaches. KEY POINTS: Question Reproducibility of liver PDFF DL-based approaches on different scan protocols or manufacturers is not validated. Findings VET-Net showed a PDFF bias of -0.55% on a multi-site phantom dataset, and RDCs of 1.71% and 1.04% at two liver ROIs.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39532254",
      "title": "Sociodemographic bias in clinical machine learning models: a scoping review of algorithmic bias instances and mechanisms.",
      "abstract": "BACKGROUND AND OBJECTIVES: Clinical machine learning (ML) technologies can sometimes be biased and their use could exacerbate health disparities. The extent to which bias is present, the groups who most frequently experience bias, and the mechanism through which bias is introduced in clinical ML applications is not well described. The objective of this study was to examine instances of bias in clinical ML models. We identified the sociodemographic subgroups PROGRESS that experienced bias and the reported mechanisms of bias introduction. METHODS: We searched MEDLINE, EMBASE, PsycINFO, and Web of Science for all studies that evaluated bias on sociodemographic factors within ML algorithms created for the purpose of facilitating clinical care. The scoping review was conducted according to the Joanna Briggs Institute guide and reported using the PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses) extension for scoping reviews. RESULTS: We identified 6448 articles, of which 760 reported on a clinical ML model and 91 (12.0%) completed a bias evaluation and met\u00a0all inclusion criteria. Most studies evaluated a single sociodemographic factor (n\u00a0=\u00a056, 61.5%). The most frequently evaluated sociodemographic factor was race (n\u00a0=\u00a059, 64.8%), followed by sex/gender (n\u00a0=\u00a041, 45.1%), and age (n\u00a0=\u00a024, 26.4%), with one study (1.1%) evaluating intersectional factors. Of all studies, 74.7% (n\u00a0=\u00a068) reported that bias was present, 18.7% (n\u00a0=\u00a017) reported bias was not present, and 6.6% (n\u00a0=\u00a06) did not state whether bias was present. When present, 87% of studies reported bias against groups with socioeconomic disadvantage. CONCLUSION: Most ML algorithms that were evaluated for bias demonstrated bias on sociodemographic factors. Furthermore, most bias evaluations concentrated on race, sex/gender, and age, while other sociodemographic factors and their intersection were infrequently assessed. Given potential health equity implications, bias assessments should be completed for all clinical ML models.",
      "journal": "Journal of clinical epidemiology",
      "year": "2025",
      "doi": "10.1016/j.jclinepi.2024.111606",
      "authors": "Colacci Michael et al.",
      "keywords": "Algorithmic bias; Bias; Clinical artificial intelligence; Clinical decision support; Machine learning; Sociodemographic",
      "mesh_terms": "Humans; Machine Learning; Bias; Algorithms; Sociodemographic Factors; Socioeconomic Factors; Female; Male",
      "pub_types": "Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39532254/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Intersectional",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Most ML algorithms that were evaluated for bias demonstrated bias on sociodemographic factors. Furthermore, most bias evaluations concentrated on race, sex/gender, and age, while other sociodemographic factors and their intersection were infrequently assessed. Given potential health equity implications, bias assessments should be completed for all clinical ML models.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39671751",
      "title": "Where, why, and how is bias learned in medical image analysis models? A study of bias encoding within convolutional networks using synthetic data.",
      "abstract": "BACKGROUND: Understanding the mechanisms of algorithmic bias is highly challenging due to the complexity and uncertainty of how various unknown sources of bias impact deep learning models trained with medical images. This study aims to bridge this knowledge gap by studying where, why, and how biases from medical images are encoded in these models. METHODS: We systematically studied layer-wise bias encoding in a convolutional neural network for disease classification using synthetic brain magnetic resonance imaging data with known disease and bias effects. We quantified the degree to which disease-related information, as well as morphology-based and intensity-based biases were represented within the learned features of the model. FINDINGS: Although biases were encoded throughout the model, a stronger encoding did not necessarily lead to the model using these biases as a shortcut for disease classification. We also observed that intensity-based effects had a greater influence on shortcut learning compared to morphology-based effects when multiple biases were present. INTERPRETATION: We believe that these results constitute an important first step towards a deeper understanding of algorithmic bias in deep learning models trained using medical imaging data. This study also showcases the benefits of utilising controlled, synthetic bias scenarios for objectively studying the mechanisms of shortcut learning. FUNDING: Alberta Innovates, Natural Sciences and Engineering Research Council of Canada, Killam Trusts, Parkinson Association of Alberta, River Fund at Calgary Foundation, Canada Research Chairs Program.",
      "journal": "EBioMedicine",
      "year": "2025",
      "doi": "10.1016/j.ebiom.2024.105501",
      "authors": "Stanley Emma A M et al.",
      "keywords": "Algorithmic bias; Artificial intelligence; Synthetic data",
      "mesh_terms": "Bias; Humans; Image Processing, Computer-Assisted; Convolutional Neural Networks; Diagnostic Imaging; Algorithms; Brain; Magnetic Resonance Imaging; Databases, Factual",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39671751/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; NLP/LLM; Neural Network; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Neurology",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "FINDINGS: Although biases were encoded throughout the model, a stronger encoding did not necessarily lead to the model using these biases as a shortcut for disease classification. We also observed that intensity-based effects had a greater influence on shortcut learning compared to morphology-based effects when multiple biases were present. INTERPRETATION: We believe that these results constitute an important first step towards a deeper understanding of algorithmic bias in deep learning models t...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11700256"
    },
    {
      "pmid": "39677375",
      "title": "Debiasing large language models: research opportunities.",
      "abstract": "Large language models (LLMs) are powerful decision-making tools widely adopted in healthcare, finance, and transportation. Embracing the opportunities and innovations of LLMs is inevitable. However, LLMs inherit stereotypes, misrepresentations, discrimination, and societies' biases from various sources-including training data, algorithm design, and user interactions-resulting in concerns about equality, diversity, and fairness. The bias problem has triggered increased research towards defining, detecting and quantifying bias and developing debiasing techniques. The predominant focus in tackling the bias problem is skewed towards resource-rich regions such as the US and Europe, resulting in a scarcity of research in other societies. As a small country with a unique history, culture and social composition, there is an opportunity for Aotearoa New Zealand's (NZ) research community to address this inadequacy. This paper presents an experimental evaluation of existing bias metrics and debiasing techniques in the NZ context. Research gaps derived from the study and a literature review are outlined, current and ongoing research in this space are discussed, and the suggested scope of research opportunities for NZ are presented.",
      "journal": "Journal of the Royal Society of New Zealand",
      "year": "2025",
      "doi": "10.1080/03036758.2024.2398567",
      "authors": "Yogarajan Vithya et al.",
      "keywords": "Large language models; New Zealand; bias; generative AI; responsible AI",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39677375/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Language; Geographic",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "This paper presents an experimental evaluation of existing bias metrics and debiasing techniques in the NZ context. Research gaps derived from the study and a literature review are outlined, current and ongoing research in this space are discussed, and the suggested scope of research opportunities for NZ are presented.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11639098"
    },
    {
      "pmid": "39689864",
      "title": "Building a Time-Series Model to Predict Hospitalization Risks in Home Health Care: Insights Into Development, Accuracy, and Fairness.",
      "abstract": "OBJECTIVES: Home health care (HHC) serves more than 5 million older adults annually in the United States, aiming to prevent unnecessary hospitalizations and emergency department (ED) visits. Despite efforts, up to 25% of patients in HHC experience these adverse events. The underutilization of clinical notes, aggregated data approaches, and potential demographic biases have limited previous HHC risk prediction models. This study aimed to develop a time-series risk model to predict hospitalizations and ED visits in patients in HHC, examine model performance over various prediction windows, identify top predictive variables and map them to data standards, and assess model fairness across demographic subgroups. SETTING AND PARTICIPANTS: A total of 27,222 HHC episodes between 2015 and\u00a02017. METHODS: The study used health care process modeling of electronic health records, including clinical notes processed with natural language processing techniques and Medicare claims data. A Light Gradient Boosting Machine algorithm was used to develop the risk prediction model, with performance evaluated using 5-fold cross-validation. Model fairness was assessed across gender, race/ethnicity, and socioeconomic subgroups. RESULTS: The model achieved high predictive performance, with an F1 score of 0.84 for a 5-day prediction window. Twenty top predictive variables were identified, including novel indicators such as the length of nurse-patient visits and visit frequency. Eighty-five percent of these variables mapped completely to the US Core Data for Interoperability standard. Fairness assessment revealed performance disparities across demographic and socioeconomic groups, with lower model effectiveness for more historically underserved populations. CONCLUSIONS AND IMPLICATIONS: This study developed a robust time-series risk model for predicting adverse events in patients in HHC, incorporating diverse data types and demonstrating high predictive accuracy. The findings highlight the importance of considering established and novel risk factors in HHC. Importantly, the observed performance disparities across subgroups emphasize the need for fairness adjustments to ensure equitable risk prediction across all patient populations.",
      "journal": "Journal of the American Medical Directors Association",
      "year": "2025",
      "doi": "10.1016/j.jamda.2024.105417",
      "authors": "Topaz Maxim et al.",
      "keywords": "Home health care service; model fairness; natural language processing; risk prediction",
      "mesh_terms": "Humans; Male; Female; Aged; Hospitalization; Home Care Services; United States; Risk Assessment; Aged, 80 and over; Emergency Service, Hospital; Electronic Health Records",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39689864/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM; XGBoost/Gradient Boosting; Clinical Prediction Model",
      "health_domain": "Emergency Medicine; EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Language; Insurance Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; Emergency Department; Public Health/Population; Safety-Net/Underserved",
      "key_findings": "RESULTS: The model achieved high predictive performance, with an F1 score of 0.84 for a 5-day prediction window. Twenty top predictive variables were identified, including novel indicators such as the length of nurse-patient visits and visit frequency. Eighty-five percent of these variables mapped completely to the US Core Data for Interoperability standard.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39701919",
      "title": "Tackling algorithmic bias and promoting transparency in health datasets: the STANDING Together consensus recommendations.",
      "abstract": "Without careful dissection of the ways in which biases can be encoded into artificial intelligence (AI) health technologies, there is a risk of perpetuating existing health inequalities at scale. One major source of bias is the data that underpins such technologies. The STANDING Together recommendations aim to encourage transparency regarding limitations of health datasets and proactive evaluation of their effect across population groups. Draft recommendation items were informed by a systematic review and stakeholder survey. The recommendations were developed using a Delphi approach, supplemented by a public consultation and international interview study. Overall, more than 350 representatives from 58 countries provided input into this initiative. 194 Delphi participants from 25 countries voted and provided comments on 32 candidate items across three electronic survey rounds and one in-person consensus meeting. The 29 STANDING Together consensus recommendations are presented here in two parts. Recommendations for Documentation of Health Datasets provide guidance for dataset curators to enable transparency around data composition and limitations. Recommendations for Use of Health Datasets aim to enable identification and mitigation of algorithmic biases that might exacerbate health inequalities. These recommendations are intended to prompt proactive inquiry rather than acting as a checklist. We hope to raise awareness that no dataset is free of limitations, so transparent communication of data limitations should be perceived as valuable, and absence of this information as a limitation. We hope that adoption of the STANDING Together recommendations by stakeholders across the AI health technology lifecycle will enable everyone in society to benefit from technologies which are safe and effective.",
      "journal": "The Lancet. Digital health",
      "year": "2025",
      "doi": "10.1016/S2589-7500(24)00224-3",
      "authors": "Alderman Joseph E et al.",
      "keywords": "",
      "mesh_terms": "Humans; Algorithms; Artificial Intelligence; Bias; Datasets as Topic; Delphi Technique",
      "pub_types": "Journal Article; Systematic Review; Research Support, N.I.H., Extramural; Consensus Statement",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39701919/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We hope to raise awareness that no dataset is free of limitations, so transparent communication of data limitations should be perceived as valuable, and absence of this information as a limitation. We hope that adoption of the STANDING Together recommendations by stakeholders across the AI health technology lifecycle will enable everyone in society to benefit from technologies which are safe and effective.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11668905"
    },
    {
      "pmid": "39737346",
      "title": "Algorithmic individual fairness and healthcare: a scoping review.",
      "abstract": "OBJECTIVES: Statistical and artificial intelligence algorithms are increasingly being developed for use in healthcare. These algorithms may reflect biases that magnify disparities in clinical care, and there is a growing need for understanding how algorithmic biases can be mitigated in pursuit of algorithmic fairness. We conducted a scoping review on algorithmic individual fairness (IF) to understand the current state of research in the metrics and methods developed to achieve IF and their applications in healthcare. MATERIALS AND METHODS: We searched four databases: PubMed, ACM Digital Library, IEEE Xplore, and medRxiv for algorithmic IF metrics, algorithmic bias mitigation, and healthcare applications. Our search was restricted to articles published between January 2013 and November 2024. We identified 2498 articles through database searches and seven additional articles, of which 32 articles were included in the review. Data from the selected articles were extracted, and the findings were synthesized. RESULTS: Based on the 32 articles in the review, we identified several themes, including philosophical underpinnings of fairness, IF metrics, mitigation methods for achieving IF, implications of achieving IF on group fairness and vice versa, and applications of IF in healthcare. DISCUSSION: We find that research of IF is still in their early stages, particularly in healthcare, as evidenced by the limited number of relevant articles published between 2013 and 2024. While healthcare applications of IF remain sparse, growth has been steady in number of publications since 2012. The limitations of group fairness further emphasize the need for alternative approaches like IF. However, IF itself is not without challenges, including subjective definitions of similarity and potential bias encoding from data-driven methods. These findings, coupled with the limitations of the review process, underscore the need for more comprehensive research on the evolution of IF metrics and definitions to advance this promising field. CONCLUSION: While significant work has been done on algorithmic IF in recent years, the definition, use, and study of IF remain in their infancy, especially in healthcare. Future research is needed to comprehensively apply and evaluate IF in healthcare.",
      "journal": "JAMIA open",
      "year": "2025",
      "doi": "10.1093/jamiaopen/ooae149",
      "authors": "Anderson Joshua W et al.",
      "keywords": "algorithmic fairness; health disparities; healthcare; individual fairness",
      "mesh_terms": "",
      "pub_types": "Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39737346/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: While significant work has been done on algorithmic IF in recent years, the definition, use, and study of IF remain in their infancy, especially in healthcare. Future research is needed to comprehensively apply and evaluate IF in healthcare.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11684587"
    },
    {
      "pmid": "39773888",
      "title": "Bias Mitigation in Primary Health Care Artificial Intelligence Models: Scoping Review.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) predictive models in primary health care have the potential to enhance population health by rapidly and accurately identifying individuals who should receive care and health services. However, these models also carry the risk of perpetuating or amplifying existing biases toward diverse groups. We identified a gap in the current understanding of strategies used to assess and mitigate bias in primary health care algorithms related to individuals' personal or protected attributes. OBJECTIVE: This study aimed to describe the attempts, strategies, and methods used to mitigate bias in AI models within primary health care, to identify the diverse groups or protected attributes considered, and to evaluate the results of these approaches on both bias reduction and AI model performance. METHODS: We conducted a scoping review following Joanna Briggs Institute (JBI) guidelines, searching Medline (Ovid), CINAHL (EBSCO), PsycINFO (Ovid), and Web of Science databases for studies published between January 1, 2017, and November 15, 2022. Pairs of reviewers independently screened titles and abstracts, applied selection criteria, and performed full-text screening. Discrepancies regarding study inclusion were resolved by consensus. Following reporting standards for AI in health care, we extracted data on study objectives, model features, targeted diverse groups, mitigation strategies used, and results. Using the mixed methods appraisal tool, we appraised the quality of the studies. RESULTS: After removing 585 duplicates, we screened 1018 titles and abstracts. From the remaining 189 full-text articles, we included 17 studies. The most frequently investigated protected attributes were race (or ethnicity), examined in 12 of the 17 studies, and sex (often identified as gender), typically classified as \"male versus female\" in 10 of the studies. We categorized bias mitigation approaches into four clusters: (1) modifying existing AI models or datasets, (2) sourcing data from electronic health records, (3) developing tools with a \"human-in-the-loop\" approach, and (4) identifying ethical principles for informed decision-making. Algorithmic preprocessing methods, such as relabeling and reweighing data, along with natural language processing techniques that extract data from unstructured notes, showed the greatest potential for bias mitigation. Other methods aimed at enhancing model fairness included group recalibration and the application of the equalized odds metric. However, these approaches sometimes exacerbated prediction errors across groups or led to overall model miscalibrations. CONCLUSIONS: The results suggest that biases toward diverse groups are more easily mitigated when data are open-sourced, multiple stakeholders are engaged, and during the algorithm's preprocessing stage. Further empirical studies that include a broader range of groups, such as Indigenous peoples in Canada, are needed to validate and expand upon these findings. TRIAL REGISTRATION: OSF Registry osf.io/9ngz5/; https://osf.io/9ngz5/. INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): RR2-10.2196/46684.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/60269",
      "authors": "Sasseville Maxime et al.",
      "keywords": "AI; algorithms; artificial intelligence; bias; community health services; decision support; expert system; health disparities; primary health care; scoping review; social equity",
      "mesh_terms": "Artificial Intelligence; Humans; Primary Health Care; Bias; Algorithms; Female; Male",
      "pub_types": "Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39773888/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "NLP/LLM; Clinical Prediction Model",
      "health_domain": "EHR/Health Informatics; Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Data Preprocessing; Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Calibration; Fairness Metrics Evaluation",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: The results suggest that biases toward diverse groups are more easily mitigated when data are open-sourced, multiple stakeholders are engaged, and during the algorithm's preprocessing stage. Further empirical studies that include a broader range of groups, such as Indigenous peoples in Canada, are needed to validate and expand upon these findings. TRIAL REGISTRATION: OSF Registry osf.io/9ngz5/; https://osf.io/9ngz5/.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11751650"
    },
    {
      "pmid": "39784101",
      "title": "Cognitive biases in osteopathic diagnosis: a mixed study among French osteopaths.",
      "abstract": "OBJECTIVES: Although cognitive biases are one of the most frequent causes of diagnostic errors, their influence remains underestimated in allied health professions, especially in osteopathy. Yet, a part of osteopathic clinical reasoning and diagnosis rely on the practitioner's intuition and subjective haptic perceptions. The aim of this study is to highlight links between the cognitive biases perceived by the practitioner to understand cognitive patterns during osteopathic diagnosis, and to suggest debiasing strategies. METHODS: A mixed method based on an explanatory sequential type is used. (QUAN\u2192QUAL). A quantitative cross-sectional survey of 272 French osteopaths and three focus groups including 24 osteopaths were carried out. The quantitative analysis includes multinominal logistic regression models and multiple correspondence analysis. The qualitative analysis is based on the framework method (within thematic analysis) and followed a step-by-step guide (Gale et\u00a0al.). RESULTS: Among 19 selected biases, osteopaths feel to be affected by 9.4\u00a0\u00b1\u00a00.28 biases (range [1-19], median=9). Some presumed biases would be associated, and socio-demographic (gender, age) and professional (experience and types of practice) factors would modify how practitioners perceive the presence of biases. Main debiasing solutions are supervision and transcultural clinical competences. CONCLUSIONS: Osteopaths believe their diagnosis is impaired by the presence of cognitive biases as observed in clinical reality. Some biases are shared with medical doctors, but others are more specific to osteopaths, such as confirmation bias. To reduce their effect, the practitioner needs to be aware of these cognitive patterns of clinical reasoning, understand the patient and himself better, and use objective tests.",
      "journal": "Diagnosis (Berlin, Germany)",
      "year": "2025",
      "doi": "10.1515/dx-2024-0144",
      "authors": "Siffert Cassandra et al.",
      "keywords": "clinical reasoning; cognitive bias; cognitive debiasing; decision-making; diagnostic errors; osteopathic diagnosis",
      "mesh_terms": "Humans; Female; Male; France; Cross-Sectional Studies; Osteopathic Physicians; Adult; Osteopathic Medicine; Middle Aged; Cognition; Attitude of Health Personnel; Focus Groups; Diagnostic Errors; Clinical Reasoning; Surveys and Questionnaires; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39784101/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Logistic Regression; Regression",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Osteopaths believe their diagnosis is impaired by the presence of cognitive biases as observed in clinical reality. Some biases are shared with medical doctors, but others are more specific to osteopaths, such as confirmation bias. To reduce their effect, the practitioner needs to be aware of these cognitive patterns of clinical reasoning, understand the patient and himself better, and use objective tests.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39825353",
      "title": "Mitigating bias in AI mortality predictions for minority populations: a transfer learning approach.",
      "abstract": "BACKGROUND: The COVID-19 pandemic has highlighted the crucial role of artificial intelligence (AI) in predicting mortality and guiding healthcare decisions. However, AI models may perpetuate or exacerbate existing health disparities due to demographic biases, particularly affecting racial and ethnic minorities. The objective of this study is to investigate the demographic biases in AI models predicting COVID-19 mortality and to assess the effectiveness of transfer learning in improving model fairness across diverse demographic groups. METHODS: This retrospective cohort study used a population-based dataset of COVID-19 cases from the Centers for Disease Control and Prevention (CDC), spanning the years 2020-2024. The study analyzed AI model performance across different racial and ethnic groups and employed transfer learning techniques to improve model fairness by adapting pre-trained models to the specific demographic and clinical characteristics of the population. RESULTS: Decision Tree (DT) and Random Forest (RF) models consistently showed improvements in accuracy, precision, and ROC-AUC scores for Non-Hispanic Black, Hispanic/Latino, and Asian populations. The most significant precision improvement was observed in the DT model for Hispanic/Latino individuals, which increased from 0.3805 to 0.5265. The precision for Asians or Pacific Islanders in the DT model increased from 0.4727 to 0.6071, and for Non-Hispanic Blacks, it rose from 0.5492 to 0.6657. Gradient Boosting Machines (GBM) produced mixed results, showing accuracy and precision improvements for Non-Hispanic Black and Asian groups, but declines for the Hispanic/Latino and American Indian groups, with the most significant decline in precision, which dropped from 0.4612 to 0.2406 in the American Indian group. Logistic Regression (LR) demonstrated minimal changes across all metrics and groups. For the Non-Hispanic American Indian group, most models showed limited benefits, with several performance metrics either remaining stable or declining. CONCLUSIONS: This study demonstrates the potential of AI in predicting COVID-19 mortality while also underscoring the critical need to address demographic biases. The application of transfer learning significantly improved the predictive performance of models across various racial and ethnic groups, suggesting these techniques are effective in mitigating biases and promoting fairness in AI models.",
      "journal": "BMC medical informatics and decision making",
      "year": "2025",
      "doi": "10.1186/s12911-025-02862-7",
      "authors": "Gu Tianshu et al.",
      "keywords": "Artificial intelligence; COVID-19; Demographic bias; Health disparities; Mortality prediction; Transfer learning",
      "mesh_terms": "Humans; COVID-19; Retrospective Studies; Artificial Intelligence; Minority Groups; United States; Male; Female; Middle Aged; Machine Learning; Adult; Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39825353/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Random Forest; Logistic Regression; XGBoost/Gradient Boosting; Decision Tree",
      "health_domain": "ICU/Critical Care; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Transfer Learning",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSIONS: This study demonstrates the potential of AI in predicting COVID-19 mortality while also underscoring the critical need to address demographic biases. The application of transfer learning significantly improved the predictive performance of models across various racial and ethnic groups, suggesting these techniques are effective in mitigating biases and promoting fairness in AI models.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11742213"
    },
    {
      "pmid": "39836496",
      "title": "Development of secure infrastructure for advancing generative artificial intelligence research in healthcare at an academic medical center.",
      "abstract": "BACKGROUND: Generative AI, particularly large language models (LLMs), holds great potential for improving patient care and operational efficiency in healthcare. However, the use of LLMs is complicated by regulatory concerns around data security and patient privacy. This study aimed to develop and evaluate a secure infrastructure that allows researchers to safely leverage LLMs in healthcare while ensuring HIPAA compliance and promoting equitable AI. MATERIALS AND METHODS: We implemented a private Azure OpenAI Studio deployment with secure API-enabled endpoints for researchers. Two use cases were explored, detecting falls from electronic health records (EHR) notes and evaluating bias in mental health prediction using fairness-aware prompts. RESULTS: The framework provided secure, HIPAA-compliant API access to LLMs, allowing researchers to handle sensitive data safely. Both use cases highlighted the secure infrastructure's capacity to protect sensitive patient data while supporting innovation. DISCUSSION AND CONCLUSION: This centralized platform presents a scalable, secure, and HIPAA-compliant solution for healthcare institutions aiming to integrate LLMs into clinical research.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2025",
      "doi": "10.1093/jamia/ocaf005",
      "authors": "Ng Madelena Y et al.",
      "keywords": "artificial intelligence; healthcare informatics infrastructure; large language models; large-scale research; privacy; security; technology adoption",
      "mesh_terms": "Artificial Intelligence; Academic Medical Centers; Electronic Health Records; Computer Security; Humans; Health Insurance Portability and Accountability Act; United States; Confidentiality; Generative Artificial Intelligence",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39836496/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "NLP/LLM; Generative AI",
      "health_domain": "Mental Health/Psychiatry; ICU/Critical Care; EHR/Health Informatics",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: This centralized platform presents a scalable, secure, and HIPAA-compliant solution for healthcare institutions aiming to integrate LLMs into clinical research.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11833461"
    },
    {
      "pmid": "39875475",
      "title": "Large language models improve the identification of emergency department visits for symptomatic kidney stones.",
      "abstract": "Recent advancements of large language models (LLMs) like generative pre-trained transformer 4 (GPT-4) have generated significant interest among the scientific community. Yet, the potential of these models to be utilized in clinical settings remains largely unexplored. In this study, we investigated the abilities of multiple LLMs and traditional machine learning models to analyze emergency department (ED) reports and determine if the corresponding visits were due to symptomatic kidney stones. Leveraging a dataset of manually annotated ED reports, we developed strategies to enhance LLMs including prompt optimization, zero- and few-shot prompting, fine-tuning, and prompt augmentation. Further, we implemented fairness assessment and bias mitigation methods to investigate the potential disparities by LLMs with respect to race and gender. A clinical expert manually assessed the explanations generated by GPT-4 for its predictions to determine if they were sound, factually correct, unrelated to the input prompt, or potentially harmful. The best results were achieved by GPT-4 (macro-F1\u2009=\u20090.833, 95% confidence interval [CI] 0.826-0.841) and GPT-3.5 (macro-F1\u2009=\u20090.796, 95% CI 0.796-0.796). Ablation studies revealed that the initial pre-trained GPT-3.5 model benefits from fine-tuning. Adding demographic information and prior disease history to the prompts allows LLMs to make better decisions. Bias assessment found that GPT-4 exhibited no racial or gender disparities, in contrast to GPT-3.5, which failed to effectively model racial diversity.",
      "journal": "Scientific reports",
      "year": "2025",
      "doi": "10.1038/s41598-025-86632-5",
      "authors": "Bejan Cosmin A et al.",
      "keywords": "GPT-3.5; GPT-4; Kidney stones; LLMs; Large language models; Llama-2; Nephrolithiasis",
      "mesh_terms": "Humans; Kidney Calculi; Emergency Service, Hospital; Female; Male; Machine Learning; Middle Aged; Emergency Room Visits; Large Language Models",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39875475/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Emergency Medicine; Nephrology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Transfer Learning",
      "clinical_setting": "Emergency Department; Public Health/Population",
      "key_findings": "Adding demographic information and prior disease history to the prompts allows LLMs to make better decisions. Bias assessment found that GPT-4 exhibited no racial or gender disparities, in contrast to GPT-3.5, which failed to effectively model racial diversity.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11775227"
    },
    {
      "pmid": "39901158",
      "title": "The Data Artifacts Glossary: a community-based repository for bias on health datasets.",
      "abstract": "BACKGROUND: The deployment of Artificial Intelligence (AI) in healthcare has the potential to transform patient care through improved diagnostics, personalized treatment plans, and more efficient resource management. However, the effectiveness and fairness of AI are critically dependent on the data it learns from. Biased datasets can lead to AI outputs that perpetuate disparities, particularly affecting social minorities and marginalized groups. OBJECTIVE: This paper introduces the \"Data Artifacts Glossary\", a dynamic, open-source framework designed to systematically document and update potential biases in healthcare datasets. The aim is to provide a comprehensive tool that enhances the transparency and accuracy of AI applications in healthcare and contributes to understanding and addressing health inequities. METHODS: Utilizing a methodology inspired by the Delphi method, a diverse team of experts conducted iterative rounds of discussions and literature reviews. The team synthesized insights to develop a comprehensive list of bias categories and designed the glossary's structure. The Data Artifacts Glossary was piloted using the MIMIC-IV dataset to validate its utility and structure. RESULTS: The Data Artifacts Glossary adopts a collaborative approach modeled on successful open-source projects like Linux and Python. Hosted on GitHub, it utilizes robust version control and collaborative features, allowing stakeholders from diverse backgrounds to contribute. Through a rigorous peer review process managed by community members, the glossary ensures the continual refinement and accuracy of its contents. The implementation of the Data Artifacts Glossary with the MIMIC-IV dataset illustrates its utility. It categorizes biases, and facilitates their identification and understanding. CONCLUSION: The Data Artifacts Glossary serves as a vital resource for enhancing the integrity of AI applications in healthcare by providing a mechanism to recognize and mitigate dataset biases before they impact AI outputs. It not only aids in avoiding bias in model development but also contributes to understanding and addressing the root causes of health disparities.",
      "journal": "Journal of biomedical science",
      "year": "2025",
      "doi": "10.1186/s12929-024-01106-6",
      "authors": "Gameiro Rodrigo R et al.",
      "keywords": "Artificial intelligence; Bias; Data Artifacts Glossary; Dataset; Health equity; Machine learning",
      "mesh_terms": "Humans; Artificial Intelligence; Datasets as Topic; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39901158/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: The Data Artifacts Glossary serves as a vital resource for enhancing the integrity of AI applications in healthcare by providing a mechanism to recognize and mitigate dataset biases before they impact AI outputs. It not only aids in avoiding bias in model development but also contributes to understanding and addressing the root causes of health disparities.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11792693"
    },
    {
      "pmid": "39919295",
      "title": "The Efficacy of Conversational AI in Rectifying the Theory-of-Mind and Autonomy Biases: Comparative Analysis.",
      "abstract": "BACKGROUND: The increasing deployment of conversational artificial intelligence (AI) in mental health interventions necessitates an evaluation of their efficacy in rectifying cognitive biases and recognizing affect in human-AI interactions. These biases are particularly relevant in mental health contexts as they can exacerbate conditions such as depression and anxiety by reinforcing maladaptive thought patterns or unrealistic expectations in human-AI interactions. OBJECTIVE: This study aimed to assess the effectiveness of therapeutic chatbots (Wysa and Youper) versus general-purpose language models (GPT-3.5, GPT-4, and Gemini Pro) in identifying and rectifying cognitive biases and recognizing affect in user interactions. METHODS: This study used constructed case scenarios simulating typical user-bot interactions to examine how effectively chatbots address selected cognitive biases. The cognitive biases assessed included theory-of-mind biases (anthropomorphism, overtrust, and attribution) and autonomy biases (illusion of control, fundamental attribution error, and just-world hypothesis). Each chatbot response was evaluated based on accuracy, therapeutic quality, and adherence to cognitive behavioral therapy principles using an ordinal scale to ensure consistency in scoring. To enhance reliability, responses underwent a double review process by 2 cognitive scientists, followed by a secondary review by a clinical psychologist specializing in cognitive behavioral therapy, ensuring a robust assessment across interdisciplinary perspectives. RESULTS: This study revealed that general-purpose chatbots outperformed therapeutic chatbots in rectifying cognitive biases, particularly in overtrust bias, fundamental attribution error, and just-world hypothesis. GPT-4 achieved the highest scores across all biases, whereas the therapeutic bot Wysa scored the lowest. Notably, general-purpose bots showed more consistent accuracy and adaptability in recognizing and addressing bias-related cues across different contexts, suggesting a broader flexibility in handling complex cognitive patterns. In addition, in affect recognition tasks, general-purpose chatbots not only excelled but also demonstrated quicker adaptation to subtle emotional nuances, outperforming therapeutic bots in 67% (4/6) of the tested biases. CONCLUSIONS: This study shows that, while therapeutic chatbots hold promise for mental health support and cognitive bias intervention, their current capabilities are limited. Addressing cognitive biases in AI-human interactions requires systems that can both rectify and analyze biases as integral to human cognition, promoting precision and simulating empathy. The findings reveal the need for improved simulated emotional intelligence in chatbot design to provide adaptive, personalized responses that reduce overreliance and encourage independent coping skills. Future research should focus on enhancing affective response mechanisms and addressing ethical concerns such as bias mitigation and data privacy to ensure safe, effective AI-based mental health support.",
      "journal": "JMIR mental health",
      "year": "2025",
      "doi": "10.2196/64396",
      "authors": "Rz\u0105deczka Marcin et al.",
      "keywords": "AI; affect recognition; artificial intelligence; bias rectification; chatbots; cognitive bias; conversational artificial intelligence; digital mental health",
      "mesh_terms": "Humans; Artificial Intelligence; Theory of Mind; Communication; Personal Autonomy; Adult; Male; Cognitive Behavioral Therapy; Female",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39919295/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Mental Health/Psychiatry; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSIONS: This study shows that, while therapeutic chatbots hold promise for mental health support and cognitive bias intervention, their current capabilities are limited. Addressing cognitive biases in AI-human interactions requires systems that can both rectify and analyze biases as integral to human cognition, promoting precision and simulating empathy. The findings reveal the need for improved simulated emotional intelligence in chatbot design to provide adaptive, personalized responses t...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11845887"
    },
    {
      "pmid": "39941776",
      "title": "Exploring Artificial Intelligence Biases in Predictive Models for Cancer Diagnosis.",
      "abstract": "The American Society of Clinical Oncology (ASCO) has released the principles for the responsible use of artificial intelligence (AI) in oncology emphasizing fairness, accountability, oversight, equity, and transparency. However, the extent to which these principles are followed is unknown. The goal of this study was to assess the presence of biases and the quality of studies on AI models according to the ASCO principles and examine their potential impact through citation analysis and subsequent research applications. A review of original research articles centered on the evaluation of predictive models for cancer diagnosis published in the ASCO journal dedicated to informatics and data science in clinical oncology was conducted. Seventeen potential bias criteria were used to evaluate the sources of bias in the studies, aligned with the ASCO's principles for responsible AI use in oncology. The CREMLS checklist was applied to assess the study quality, focusing on the reporting standards, and the performance metrics along with citation counts of the included studies were analyzed. Nine studies were included. The most common biases were environmental and life-course bias, contextual bias, provider expertise bias, and implicit bias. Among the ASCO principles, the least adhered to were transparency, oversight and privacy, and human-centered AI application. Only 22% of the studies provided access to their data. The CREMLS checklist revealed the deficiencies in methodology and evaluation reporting. Most studies reported performance metrics within moderate to high ranges. Additionally, two studies were replicated in the subsequent research. In conclusion, most studies exhibited various types of bias, reporting deficiencies, and failure to adhere to the principles for responsible AI use in oncology, limiting their applicability and reproducibility. Greater transparency, data accessibility, and compliance with international guidelines are recommended to improve the reliability of AI-based research in oncology.",
      "journal": "Cancers",
      "year": "2025",
      "doi": "10.3390/cancers17030407",
      "authors": "Smiley Aref et al.",
      "keywords": "artificial intelligence; bias; cancer",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39941776/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "In conclusion, most studies exhibited various types of bias, reporting deficiencies, and failure to adhere to the principles for responsible AI use in oncology, limiting their applicability and reproducibility. Greater transparency, data accessibility, and compliance with international guidelines are recommended to improve the reliability of AI-based research in oncology.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11816222"
    },
    {
      "pmid": "39953146",
      "title": "Improving medical machine learning models with generative balancing for equity and excellence.",
      "abstract": "Applying machine learning to clinical outcome prediction is challenging due to imbalanced datasets and sensitive tasks that contain rare yet critical outcomes and where equitable treatment across diverse patient groups is essential. Despite attempts, biases in predictions persist, driven by disparities in representation and exacerbated by the scarcity of positive labels, perpetuating health inequities. This paper introduces FairPlay, a synthetic data generation approach leveraging large language models, to address these issues. FairPlay enhances algorithmic performance and reduces bias by creating realistic, anonymous synthetic patient data that improves representation and augments dataset patterns while preserving privacy. Through experiments on multiple datasets, we demonstrate that FairPlay boosts mortality prediction performance across diverse subgroups, achieving up to a 21% improvement in F1 Score without requiring additional data or altering downstream training pipelines. Furthermore, FairPlay consistently reduces subgroup performance gaps, as shown by universal improvements in performance and fairness metrics across four experimental setups.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01438-z",
      "authors": "Theodorou Brandon et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39953146/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Data Augmentation; Fairness Metrics Evaluation; Diverse/Representative Data",
      "clinical_setting": "Not specified",
      "key_findings": "Through experiments on multiple datasets, we demonstrate that FairPlay boosts mortality prediction performance across diverse subgroups, achieving up to a 21% improvement in F1 Score without requiring additional data or altering downstream training pipelines. Furthermore, FairPlay consistently reduces subgroup performance gaps, as shown by universal improvements in performance and fairness metrics across four experimental setups.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11828851"
    },
    {
      "pmid": "39958549",
      "title": "Enhancing rectal cancer liver metastasis prediction: Magnetic resonance imaging-based radiomics, bias mitigation, and regulatory considerations.",
      "abstract": "In this article, we comment on the article by Long et al published in the recent issue of the World Journal of Gastrointestinal Oncology. Rectal cancer patients are at risk for developing metachronous liver metastasis (MLM), yet early prediction remains challenging due to variations in tumor heterogeneity and the limitations of traditional diagnostic methods. Therefore, there is an urgent need for non-invasive techniques to improve patient outcomes. Long et al's study introduces an innovative magnetic resonance imaging (MRI)-based radiomics model that integrates high-throughput imaging data with clinical variables to predict MLM. The study employed a 7:3 split to generate training and validation datasets. The MLM prediction model was constructed using the training set and subsequently validated on the validation set using area under the curve (AUC) and dollar-cost averaging metrics to assess performance, robustness, and generalizability. By employing advanced algorithms, the model provides a non-invasive solution to assess tumor heterogeneity for better metastasis prediction, enabling early intervention and personalized treatment planning. However, variations in MRI parameters, such as differences in scanning resolutions and protocols across facilities, patient heterogeneity (e.g., age, comorbidities), and external factors like carcinoembryonic antigen levels introduce biases. Additionally, confounding factors such as diagnostic staging methods and patient comorbidities require further validation and adjustment to ensure accuracy and generalizability. With evolving Food and Drug Administration regulations on machine learning models in healthcare, compliance and careful consideration of these regulatory requirements are essential to ensuring safe and effective implementation of this approach in clinical practice. In the future, clinicians may be able to utilize data-driven, patient-centric artificial intelligence (AI)-enhanced imaging tools integrated with clinical data, which would help improve early detection of MLM and optimize personalized treatment strategies. Combining radiomics, genomics, histological data, and demographic information can significantly enhance the accuracy and precision of predictive models.",
      "journal": "World journal of gastrointestinal oncology",
      "year": "2025",
      "doi": "10.4251/wjgo.v17.i2.102151",
      "authors": "Zhang Yuwei",
      "keywords": "Bias mitigation; Food and Drug Administration regulations; Machine learning; Magnetic resonance imaging variability; Metachronous liver metastasis; Predictive modeling; Radiomics; Rectal cancer",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39958549/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Radiology/Medical Imaging; Oncology; Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "In the future, clinicians may be able to utilize data-driven, patient-centric artificial intelligence (AI)-enhanced imaging tools integrated with clinical data, which would help improve early detection of MLM and optimize personalized treatment strategies. Combining radiomics, genomics, histological data, and demographic information can significantly enhance the accuracy and precision of predictive models.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11756008"
    },
    {
      "pmid": "39974139",
      "title": "Assessing Algorithm Fairness Requires Adjustment for Risk Distribution Differences: Re-Considering the Equal Opportunity Criterion.",
      "abstract": "The proliferation of algorithm-assisted decision making has prompted calls for careful assessment of algorithm fairness. One popular fairness metric, equal opportunity, demands parity in true positive rates (TPRs) across different population subgroups. However, we highlight a critical but overlooked weakness in this measure: at a given decision threshold, TPRs vary when the underlying risk distribution varies across subgroups, even if the model equally captures the underlying risks. Failure to account for variations in risk distributions may lead to misleading conclusions on performance disparity. To address this issue, we introduce a novel metric called adjusted TPR (aTPR), which modifies subgroup-specific TPRs to reflect performance relative to the risk distribution in a common reference subgroup. Evaluating fairness using aTPRs promotes equal treatment for equal risk by reflecting whether individuals with similar underlying risks have similar opportunities of being identified as high risk by the model, regardless of subgroup membership. We demonstrate our method through numerical experiments that explore a range of differential calibration relationships and in a real-world data set that predicts 6-month mortality risk in an in-patient sample in order to increase timely referrals for palliative care consultations.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.01.31.25321489",
      "authors": "Hegarty Sarah E et al.",
      "keywords": "Algorithm fairness; Clinical decision-making; Equal opportunity; High-risk identification; Risk distribution",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39974139/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Threshold Adjustment; Fairness Metrics Evaluation",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Evaluating fairness using aTPRs promotes equal treatment for equal risk by reflecting whether individuals with similar underlying risks have similar opportunities of being identified as high risk by the model, regardless of subgroup membership. We demonstrate our method through numerical experiments that explore a range of differential calibration relationships and in a real-world data set that predicts 6-month mortality risk in an in-patient sample in order to increase timely referrals for pall...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11838655"
    },
    {
      "pmid": "40001699",
      "title": "A Conceptual Framework for Applying Ethical Principles of AI to Medical Practice.",
      "abstract": "Artificial Intelligence (AI) is reshaping healthcare through advancements in clinical decision support and diagnostic capabilities. While human expertise remains foundational to medical practice, AI-powered tools are increasingly matching or exceeding specialist-level performance across multiple domains, paving the way for a new era of democratized healthcare access. These systems promise to reduce disparities in care delivery across demographic, racial, and socioeconomic boundaries by providing high-quality diagnostic support at scale. As a result, advanced healthcare services can be affordable to all populations, irrespective of demographics, race, or socioeconomic background. The democratization of such AI tools can reduce the cost of care, optimize resource allocation, and improve the quality of care. In contrast to humans, AI can potentially uncover complex relationships in the data from a large set of inputs and generate new evidence-based knowledge in medicine. However, integrating AI into healthcare raises several ethical and philosophical concerns, such as bias, transparency, autonomy, responsibility, and accountability. In this study, we examine recent advances in AI-enabled medical image analysis, current regulatory frameworks, and emerging best practices for clinical integration. We analyze both technical and ethical challenges inherent in deploying AI systems across healthcare institutions, with particular attention to data privacy, algorithmic fairness, and system transparency. Furthermore, we propose practical solutions to address key challenges, including data scarcity, racial bias in training datasets, limited model interpretability, and systematic algorithmic biases. Finally, we outline a conceptual algorithm for responsible AI implementations and identify promising future research and development directions.",
      "journal": "Bioengineering (Basel, Switzerland)",
      "year": "2025",
      "doi": "10.3390/bioengineering12020180",
      "authors": "Jha Debesh et al.",
      "keywords": "artificial intelligence (AI); ethical AI; philosophical AI; trustworthy AI",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40001699/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Decision Support",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "Furthermore, we propose practical solutions to address key challenges, including data scarcity, racial bias in training datasets, limited model interpretability, and systematic algorithmic biases. Finally, we outline a conceptual algorithm for responsible AI implementations and identify promising future research and development directions.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11851997"
    },
    {
      "pmid": "40011901",
      "title": "Evaluating and addressing demographic disparities in medical large language models: a systematic review.",
      "abstract": "BACKGROUND: Large language models are increasingly evaluated for use in healthcare. However, concerns about their impact on disparities persist. This study reviews current research on demographic biases in large language models to identify prevalent bias types, assess measurement methods, and evaluate mitigation strategies. METHODS: We conducted a systematic review, searching publications from January 2018 to July 2024 across five databases. We included peer-reviewed studies evaluating demographic biases in large language models, focusing on gender, race, ethnicity, age, and other factors. Study quality was assessed using the Joanna Briggs Institute Critical Appraisal Tools. RESULTS: Our review included 24 studies. Of these, 22 (91.7%) identified biases. Gender bias was the most prevalent, reported in 15 of 16 studies (93.7%). Racial or ethnic biases were observed in 10 of 11 studies (90.9%). Only two studies found minimal or no bias in certain contexts. Mitigation strategies mainly included prompt engineering, with varying effectiveness. However, these findings are tempered by a potential publication bias, as studies with negative results are less frequently published. CONCLUSION: Biases are observed in large language models across various medical domains. While bias detection is improving, effective mitigation strategies are still developing. As LLMs increasingly influence critical decisions, addressing these biases and their resultant disparities is essential for ensuring fair artificial intelligence systems. Future research should focus on a wider range of demographic factors, intersectional analyses, and non-Western cultural contexts.",
      "journal": "International journal for equity in health",
      "year": "2025",
      "doi": "10.1186/s12939-025-02419-0",
      "authors": "Omar Mahmud et al.",
      "keywords": "",
      "mesh_terms": "Humans; Healthcare Disparities; Demography; Ethnicity; Sexism; Female; Large Language Models",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40011901/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language; Intersectional",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Biases are observed in large language models across various medical domains. While bias detection is improving, effective mitigation strategies are still developing. As LLMs increasingly influence critical decisions, addressing these biases and their resultant disparities is essential for ensuring fair artificial intelligence systems.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11866893"
    },
    {
      "pmid": "40026843",
      "title": "Equitable hospital length of stay prediction for patients with learning disabilities and multiple long-term conditions using machine learning.",
      "abstract": "PURPOSE: Individuals with learning disabilities (LD) often face higher rates of premature mortality and prolonged hospital stays compared to the general population. Predicting the length of stay (LOS) for patients with LD and multiple long-term conditions (MLTCs) is critical for improving patient care and optimising medical resource allocation. However, there is limited research on the application of machine learning (ML) models to this population. Furthermore, approaches designed for the general population often lack generalisability and fairness, particularly when applied across sensitive groups within their cohort. METHOD: This study analyses hospitalisations of 9,618 patients with LD in Wales using electronic health records (EHR) from the SAIL Databank. A Random Forest (RF) ML model was developed to predict hospital LOS, incorporating demographics, medication history, lifestyle factors, and 39 long-term conditions. To address fairness concerns, two bias mitigation techniques were applied: a post-processing threshold optimiser and an in-processing reductions method using an exponentiated gradient. These methods aimed to minimise performance discrepancies across ethnic groups while ensuring robust model performance. RESULTS: The RF model outperformed other state-of-the-art models, achieving an area under the curve of 0.759 for males and 0.756 for females, a false negative rate of 0.224 for males and 0.229 for females, and a balanced accuracy of 0.690 for males and 0.689 for females. Bias mitigation algorithms reduced disparities in prediction performance across ethnic groups, with the threshold optimiser yielding the most notable improvements. Performance metrics, including false positive rate and balanced accuracy, showed significant enhancements in fairness for the male cohort. CONCLUSION: This study demonstrates the feasibility of applying ML models to predict LOS for patients with LD and MLTCs, while addressing fairness through bias mitigation techniques. The findings highlight the potential for equitable healthcare predictions using EHR data, paving the way for improved clinical decision-making and resource management.",
      "journal": "Frontiers in digital health",
      "year": "2025",
      "doi": "10.3389/fdgth.2025.1538793",
      "authors": "Abakasanga Emeka et al.",
      "keywords": "bias mitigation; exponentiated gradient; learning disabilities; length of stay; threshold optimiser",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40026843/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Random Forest",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Threshold Adjustment; Post-hoc Correction",
      "clinical_setting": "Hospital/Inpatient; ICU; Public Health/Population",
      "key_findings": "CONCLUSION: This study demonstrates the feasibility of applying ML models to predict LOS for patients with LD and MLTCs, while addressing fairness through bias mitigation techniques. The findings highlight the potential for equitable healthcare predictions using EHR data, paving the way for improved clinical decision-making and resource management.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11868268"
    },
    {
      "pmid": "40041672",
      "title": "Biases in Artificial Intelligence Application in Pain Medicine.",
      "abstract": "Artificial Intelligence (AI) has the potential to optimize personalized treatment tools and enhance clinical decision-making. However, biases in AI, arising from sex, race, socioeconomic status (SES), and statistical methods, can exacerbate disparities in pain management. This narrative review examines these biases and proposes strategies to mitigate them. A comprehensive literature search across databases such as PubMed, Google Scholar, and PsycINFO focused on AI applications in pain management and sources of biases. Sex and racial biases often stem from societal stereotypes, underrepresentation of females, overrepresentation of European ancestry patients in clinical trials, and unequal access to treatment caused by systemic racism, leading to inaccurate pain assessments and misrepresentation in clinical data. SES biases reflect differential access to healthcare resources and incomplete data for lower SES individuals, resulting in larger prediction errors. Statistical biases, including sampling and measurement biases, further affect the reliability of AI algorithms. To ensure equitable healthcare delivery, this review recommends employing specific fairness-aware techniques such as reweighting algorithms, adversarial debiasing, and other methods that adjust training data to minimize bias. Additionally, leveraging diverse perspectives-including insights from patients, clinicians, policymakers, and interdisciplinary collaborators-can enhance the development of fair and interpretable AI systems. Continuous monitoring and inclusive collaboration are essential for addressing biases and harnessing AI's potential to improve pain management outcomes across diverse populations.",
      "journal": "Journal of pain research",
      "year": "2025",
      "doi": "10.2147/JPR.S495934",
      "authors": "Jumreornvong Oranicha et al.",
      "keywords": "artificial intelligence; biases; gender; pain; race; socioeconomic status; statistical biases",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40041672/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Pain Management",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Development/Training; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Adversarial Debiasing; Explainability/Interpretability",
      "clinical_setting": "Public Health/Population; Clinical Trial",
      "key_findings": "Additionally, leveraging diverse perspectives-including insights from patients, clinicians, policymakers, and interdisciplinary collaborators-can enhance the development of fair and interpretable AI systems. Continuous monitoring and inclusive collaboration are essential for addressing biases and harnessing AI's potential to improve pain management outcomes across diverse populations.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11878133"
    },
    {
      "pmid": "40047796",
      "title": "Comparison of Machine Learning Algorithms Identifying Children at Increased Risk of Out-of-Home Placement: Development and Practical Considerations.",
      "abstract": "OBJECTIVE: To develop a machine learning (ML) algorithm capable of identifying children at risk of out-of-home placement among a Medicaid-insured population. STUDY SETTING AND DESIGN: The study population includes children enrolled in a Medicaid accountable care organization between 2018 and 2022 in two nonurban Ohio counties served by the Centers for Medicare and Medicaid Services-funded Integrated Care for Kids Model. Using a retrospective cohort, we developed and compared a set of ML algorithms to identify children at risk of out-of-home placement within one\u2009year. ML algorithms tested include least absolute shrinkage and selection operator (LASSO)-regularized logistic regression and eXtreme gradient-boosted trees (XGBoost). We compared both modeling approaches with and without race as a candidate predictor. Performance metrics included the area under the receiver operating characteristic curve (AUROC) and the corrected partial AUROC at specificities \u2265\u200990% (pAUROC90). Algorithmic bias was tested by comparing pAUROC90 across each model between Black and White children. DATA SOURCES AND ANALYTIC SAMPLE: The modeling dataset was comprised of Medicaid claims and patient demographics data from Partners For Kids, a pediatric accountable care organization. PRINCIPAL FINDINGS: Overall, XGBoost models outperformed LASSO models. When race was included in the model, XGBoost had an AUROC of 0.78 (95% confidence interval [CI]: 0.77-0.79) while the LASSO model had an AUROC of 0.75 (95% CI: 0.74-0.77). When race was excluded from the model, XGBoost had an AUROC of 0.76 (95% CI: 0.74-0.77) while LASSO had an AUROC of 0.73 (95% CI: 0.72-0.74). CONCLUSIONS: The more complex XGBoost outperformed the simpler LASSO in predicting out-of-home placement and had less evidence of racial bias. This study highlights the complexities of developing predictive models in systems with known racial disparities and illustrates what can be accomplished when ML developers and policy leaders collaborate to maximize data to meet the needs of children and families.",
      "journal": "Health services research",
      "year": "2025",
      "doi": "10.1111/1475-6773.14601",
      "authors": "Gorham Tyler J et al.",
      "keywords": "Medicaid; accountable care organization; machine learning; out\u2010of\u2010home placement; predictive modeling",
      "mesh_terms": "Humans; Machine Learning; Medicaid; United States; Retrospective Studies; Male; Female; Algorithms; Child; Child, Preschool; Ohio; Accountable Care Organizations; Foster Home Care; Infant; Logistic Models; ROC Curve; Adolescent; Risk Assessment",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40047796/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Logistic Regression; XGBoost/Gradient Boosting; Clinical Prediction Model; Generative AI",
      "health_domain": "Pediatrics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Geographic; Insurance Status",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Regularization",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: The more complex XGBoost outperformed the simpler LASSO in predicting out-of-home placement and had less evidence of racial bias. This study highlights the complexities of developing predictive models in systems with known racial disparities and illustrates what can be accomplished when ML developers and policy leaders collaborate to maximize data to meet the needs of children and families.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12277119"
    },
    {
      "pmid": "40056436",
      "title": "Large language models are less effective at clinical prediction tasks than locally trained machine learning models.",
      "abstract": "OBJECTIVES: To determine the extent to which current large language models (LLMs) can serve as substitutes for traditional machine learning (ML) as clinical predictors using data from electronic health records (EHRs), we investigated various factors that can impact their adoption, including overall performance, calibration, fairness, and resilience to privacy protections that reduce data fidelity. MATERIALS AND METHODS: We evaluated GPT-3.5, GPT-4, and traditional ML (as gradient-boosting trees) on clinical prediction tasks in EHR data from Vanderbilt University Medical Center (VUMC) and MIMIC IV. We measured predictive performance with area under the receiver operating characteristic (AUROC) and model calibration using Brier Score. To evaluate the impact of data privacy protections, we assessed AUROC when demographic variables are generalized. We evaluated algorithmic fairness using equalized odds and statistical parity across race, sex, and age of patients. We also considered the impact of using in-context learning by incorporating labeled examples within the prompt. RESULTS: Traditional ML [AUROC: 0.847, 0.894 (VUMC, MIMIC)] substantially outperformed GPT-3.5 (AUROC: 0.537, 0.517) and GPT-4 (AUROC: 0.629, 0.602) (with and without in-context learning) in predictive performance and output probability calibration [Brier Score (ML vs GPT-3.5 vs GPT-4): 0.134 vs 0.384 vs 0.251, 0.042 vs 0.06 vs 0.219)]. DISCUSSION: Traditional ML is more robust than GPT-3.5 and GPT-4 in generalizing demographic information to protect privacy. GPT-4 is the fairest model according to our selected metrics but at the cost of poor model performance. CONCLUSION: These findings suggest that non-fine-tuned LLMs are less effective and robust than locally trained ML for clinical prediction tasks, but they are improving across releases.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2025",
      "doi": "10.1093/jamia/ocaf038",
      "authors": "Brown Katherine E et al.",
      "keywords": "clinical prediction models; fairness; large language models; privacy",
      "mesh_terms": "Machine Learning; Humans; Electronic Health Records; Female; ROC Curve; Male; Algorithms; Middle Aged; Large Language Models",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40056436/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM; Clinical Prediction Model",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Fairness Metrics Evaluation; Transfer Learning",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: These findings suggest that non-fine-tuned LLMs are less effective and robust than locally trained ML for clinical prediction tasks, but they are improving across releases.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12012369"
    },
    {
      "pmid": "40079820",
      "title": "Emerging algorithmic bias: fairness drift as the next dimension of model maintenance and sustainability.",
      "abstract": "OBJECTIVES: While performance drift of clinical prediction models is well-documented, the potential for algorithmic biases to emerge post-deployment has had limited characterization. A better understanding of how temporal model performance may shift across subpopulations is required to incorporate fairness drift into model maintenance strategies. MATERIALS AND METHODS: We explore fairness drift in a national population over 11 years, with and without model maintenance aimed at sustaining population-level performance. We trained random forest models predicting 30-day post-surgical readmission, mortality, and pneumonia using 2013 data from US Department of Veterans Affairs facilities. We evaluated performance quarterly from 2014 to 2023 by self-reported race and sex. We estimated discrimination, calibration, and accuracy, and operationalized fairness using metric parity measured as the gap between disadvantaged and advantaged groups. RESULTS: Our cohort included 1\u00a0739\u00a0666 surgical cases. We observed fairness drift in both the original and temporally updated models. Model updating had a larger impact on overall performance than fairness gaps. During periods of stable fairness, updating models at the population level increased, decreased, or did not impact fairness gaps. During periods of fairness drift, updating models restored fairness in some cases and exacerbated fairness gaps in others. DISCUSSION: This exploratory study highlights that algorithmic fairness cannot be assured through one-time assessments during model development. Temporal changes in fairness may take multiple forms and interact with model updating strategies in unanticipated ways. CONCLUSION: Equitable and sustainable clinical artificial intelligence deployments will require novel methods to monitor algorithmic fairness, detect emerging bias, and adopt model updates that promote fairness.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2025",
      "doi": "10.1093/jamia/ocaf039",
      "authors": "Davis Sharon E et al.",
      "keywords": "algorithmic fairness; dataset shift; model updating; performance drift; predictive analytics",
      "mesh_terms": "Humans; Algorithms; United States; Female; Male; Bias; Middle Aged; Models, Statistical; Patient Readmission; Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40079820/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Random Forest; Clinical Prediction Model",
      "health_domain": "Surgery; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Calibration; Subgroup Analysis",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: Equitable and sustainable clinical artificial intelligence deployments will require novel methods to monitor algorithmic fairness, detect emerging bias, and adopt model updates that promote fairness.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12012346"
    },
    {
      "pmid": "40080818",
      "title": "Assessing Racial and Ethnic Bias in Text Generation by Large Language Models for Health Care-Related Tasks: Cross-Sectional Study.",
      "abstract": "BACKGROUND: Racial and ethnic bias in large language models (LLMs) used for health care tasks is a growing concern, as it may contribute to health disparities. In response, LLM operators implemented safeguards against prompts that are overtly seeking certain biases. OBJECTIVE: This study aims to investigate a potential racial and ethnic bias among 4 popular LLMs: GPT-3.5-turbo (OpenAI), GPT-4 (OpenAI), Gemini-1.0-pro (Google), and Llama3-70b (Meta) in generating health care consumer-directed text in the absence of overtly biased queries. METHODS: In this cross-sectional study, the 4 LLMs were prompted to generate discharge instructions for patients with HIV. Each patient's encounter deidentified metadata including race/ethnicity as a variable was passed over in a table format through a prompt 4 times, altering only the race/ethnicity information (African American, Asian, Hispanic White, and non-Hispanic White) each time, while keeping all other information constant. The prompt requested the model to write discharge instructions for each encounter without explicitly mentioning race or ethnicity. The LLM-generated instructions were analyzed for sentiment, subjectivity, reading ease, and word frequency by race/ethnicity. RESULTS: The only observed statistically significant difference between race/ethnicity groups was found in entity count (GPT-4, df=42, P=.047). However, post hoc chi-square analysis for GPT-4's entity counts showed no significant pairwise differences among race/ethnicity categories after Bonferroni correction. CONCLUSIONS: A total of 4 LLMs were relatively invariant to race/ethnicity in terms of linguistic and readability measures. While our study used proxy linguistic and readability measures to investigate racial and ethnic bias among 4 LLM responses in a health care-related task, there is an urgent need to establish universally accepted standards for measuring bias in LLM-generated responses. Further studies are needed to validate these results and assess their implications.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/57257",
      "authors": "Hanna John J et al.",
      "keywords": "ChatGPT; artificial intelligence; bias; consumer-directed; cross sectional; healthcare; human immunodeficiency virus; large language models; racism; reading ease; sentiment analysis; task; text generation; word frequency",
      "mesh_terms": "Cross-Sectional Studies; Humans; Ethnicity; Racism; Racial Groups; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40080818/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Genomics/Genetics; Infectious Disease",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: A total of 4 LLMs were relatively invariant to race/ethnicity in terms of linguistic and readability measures. While our study used proxy linguistic and readability measures to investigate racial and ethnic bias among 4 LLM responses in a health care-related task, there is an urgent need to establish universally accepted standards for measuring bias in LLM-generated responses. Further studies are needed to validate these results and assess their implications.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11950697"
    },
    {
      "pmid": "40081979",
      "title": "Sex bias consideration in healthcare machine-learning research: a systematic review in rheumatoid arthritis.",
      "abstract": "OBJECTIVE: To assess the acknowledgement and mitigation of sex bias within studies using supervised machine learning (ML) for improving clinical outcomes in rheumatoid arthritis (RA). DESIGN: A systematic review of original studies published in English between 2018 and November 2023. DATA SOURCES: PUBMED and EMBASE databases. STUDY SELECTION: Studies were selected based on their use of supervised ML in RA and their publication within the specified date range. DATA EXTRACTION AND SYNTHESIS: Papers were scored on whether they reported, attempted to mitigate or successfully mitigated various types of bias: training data bias, test data bias, input variable bias, output variable bias and analysis bias. The quality of ML research in all papers was also assessed. RESULTS: Out of 52 papers included in the review, 51 had a female skew in their study participants. However, 42 papers did not acknowledge any potential sex bias. Only three papers assessed bias in model performance by sex disaggregating their results. Potential sex bias in input variables was acknowledged in one paper, while six papers commented on sex bias in their output variables, predominantly disease activity scores. No paper attempted to mitigate any type of sex bias. CONCLUSIONS: The findings demonstrate the need for increased promotion of inclusive and equitable ML practices in healthcare to address unchecked sex bias in ML algorithms. PROSPERO REGISTRATION NUMBER: CRD42023431754.",
      "journal": "BMJ open",
      "year": "2025",
      "doi": "10.1136/bmjopen-2024-086117",
      "authors": "Talwar Anahita et al.",
      "keywords": "Health Equity; MEDICAL ETHICS; Machine Learning; RHEUMATOLOGY; STATISTICS & RESEARCH METHODS",
      "mesh_terms": "Humans; Arthritis, Rheumatoid; Sexism; Machine Learning; Female; Male",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40081979/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: The findings demonstrate the need for increased promotion of inclusive and equitable ML practices in healthcare to address unchecked sex bias in ML algorithms. PROSPERO REGISTRATION NUMBER: CRD42023431754.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11906982"
    },
    {
      "pmid": "40095575",
      "title": "Shaping the Future of Healthcare: Ethical Clinical Challenges and Pathways to Trustworthy AI.",
      "abstract": "Background/Objectives: Artificial intelligence (AI) is transforming healthcare, enabling advances in diagnostics, treatment optimization, and patient care. Yet, its integration raises ethical, regulatory, and societal challenges. Key concerns include data privacy risks, algorithmic bias, and regulatory gaps that struggle to keep pace with AI advancements. This study aims to synthesize a multidisciplinary framework for trustworthy AI in healthcare, focusing on transparency, accountability, fairness, sustainability, and global collaboration. It moves beyond high-level ethical discussions to provide actionable strategies for implementing trustworthy AI in clinical contexts. Methods: A structured literature review was conducted using PubMed, Scopus, and Web of Science. Studies were selected based on relevance to AI ethics, governance, and policy in healthcare, prioritizing peer-reviewed articles, policy analyses, case studies, and ethical guidelines from authoritative sources published within the last decade. The conceptual approach integrates perspectives from clinicians, ethicists, policymakers, and technologists, offering a holistic \"ecosystem\" view of AI. No clinical trials or patient-level interventions were conducted. Results: The analysis identifies key gaps in current AI governance and introduces the Regulatory Genome-an adaptive AI oversight framework aligned with global policy trends and Sustainable Development Goals. It introduces quantifiable trustworthiness metrics, a comparative analysis of AI categories for clinical applications, and bias mitigation strategies. Additionally, it presents interdisciplinary policy recommendations for aligning AI deployment with ethical, regulatory, and environmental sustainability goals. This study emphasizes measurable standards, multi-stakeholder engagement strategies, and global partnerships to ensure that future AI innovations meet ethical and practical healthcare needs. Conclusions: Trustworthy AI in healthcare requires more than technical advancements-it demands robust ethical safeguards, proactive regulation, and continuous collaboration. By adopting the recommended roadmap, stakeholders can foster responsible innovation, improve patient outcomes, and maintain public trust in AI-driven healthcare.",
      "journal": "Journal of clinical medicine",
      "year": "2025",
      "doi": "10.3390/jcm14051605",
      "authors": "Goktas Polat et al.",
      "keywords": "artificial intelligence; bias; ethics; health policy; large language model; machine learning; natural language processing; privacy; regulation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40095575/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Clinical Trial",
      "key_findings": "Conclusions: Trustworthy AI in healthcare requires more than technical advancements-it demands robust ethical safeguards, proactive regulation, and continuous collaboration. By adopting the recommended roadmap, stakeholders can foster responsible innovation, improve patient outcomes, and maintain public trust in AI-driven healthcare.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11900311"
    },
    {
      "pmid": "40100898",
      "title": "What makes clinical machine learning fair? A practical ethics framework.",
      "abstract": "Machine learning (ML) can offer a tremendous contribution to medicine by streamlining decision-making, reducing mistakes, improving clinical accuracy and ensuring better patient outcomes. The prospects of a widespread and rapid integration of machine learning in clinical workflow have attracted considerable attention including due to complex ethical implications-algorithmic bias being among the most frequently discussed ML models. Here we introduce and discuss a practical ethics framework inductively-generated via normative analysis of the practical challenges in developing an actual clinical ML model (see case study). The framework is usable to identify, measure and address bias in clinical machine learning models, thus improving fairness as to both model performance and health outcomes. We detail a proportionate approach to ML bias by defining the demands of fair ML in light of what is ethically justifiable and, at the same time, technically feasible in light of inevitable trade-offs. Our framework enables ethically robust and transparent decision-making both in the design and the context-dependent aspects of ML bias mitigation, thus improving accountability for both developers and clinical users.",
      "journal": "PLOS digital health",
      "year": "2025",
      "doi": "10.1371/journal.pdig.0000728",
      "authors": "Hoche Marine et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40100898/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "We detail a proportionate approach to ML bias by defining the demands of fair ML in light of what is ethically justifiable and, at the same time, technically feasible in light of inevitable trade-offs. Our framework enables ethically robust and transparent decision-making both in the design and the context-dependent aspects of ML bias mitigation, thus improving accountability for both developers and clinical users.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11918422"
    },
    {
      "pmid": "40111867",
      "title": "Easy ensemble classifier-group and intersectional fairness and threshold (EEC-GIFT): a fairness-aware machine learning framework for lung cancer screening eligibility using real-world data.",
      "abstract": "BACKGROUND: We use real-world data to develop a lung cancer screening (LCS) eligibility mechanism that is both accurate and free from racial bias. METHODS: Our data came from the Prostate, Lung, Colorectal, and Ovarian (PLCO) cancer screening trial. We built a systematic fairness-aware machine learning framework by integrating a Group and Intersectional Fairness and Threshold (GIFT) strategy with an easy ensemble classifier-(EEC-) or logistic regression-(LR-) based model. The best LCS eligibility mechanism EEC-GIFT* and LR-GIFT* were applied to the testing dataset and their performances were compared to the 2021 US Preventive Services Task Force (USPSTF) criteria and PLCOM2012 model. The equal opportunity difference (EOD) of developing lung cancer between Black and White smokers was used to evaluate mechanism fairness. RESULTS: The fairness of LR-GIFT* or EEC-GIFT* during training was notably greater than that of the LR or EEC models without greatly reducing their accuracy. During testing, the EEC-GIFT* (85.16% vs 78.08%, P\u2009<\u2009.001) and LR-GIFT* (85.98% vs 78.08%, P\u2009<\u2009.001) models significantly improved sensitivity without sacrificing specificity compared to the 2021 USPSTF criteria. The EEC-GIFT* (0.785 vs 0.788, P\u2009=\u2009.28) and LR-GIFT* (0.785 vs 0.788, P\u2009=\u2009.30) showed similar area under receiver operating characteristic curve values compared to the PLCOM2012 model. While the average EODs between Blacks and Whites were significant for the 2021 USPSTF criteria (0.0673, P\u2009<\u2009.001), PLCOM2012 (0.0566, P\u2009<\u2009.001), and LR-GIFT* (0.0081, P\u2009<\u2009.001), the EEC-GIFT* model was unbiased (0.0034, P\u2009=\u2009.07). CONCLUSION: Our EEC-GIFT* LCS eligibility mechanism can significantly mitigate racial biases in eligibility determination without compromising its predictive performance.",
      "journal": "JNCI cancer spectrum",
      "year": "2025",
      "doi": "10.1093/jncics/pkaf030",
      "authors": "Conahan Piyawan et al.",
      "keywords": "",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Black or African American; Early Detection of Cancer; Eligibility Determination; Logistic Models; Lung Neoplasms; Machine Learning; Mass Screening; White",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40111867/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Logistic Regression; Ensemble Methods",
      "health_domain": "Oncology; Pulmonology",
      "bias_axes": "Race/Ethnicity; Age; Intersectional",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment; Fairness Metrics Evaluation; Ensemble Methods",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Our EEC-GIFT* LCS eligibility mechanism can significantly mitigate racial biases in eligibility determination without compromising its predictive performance.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11986816"
    },
    {
      "pmid": "40111994",
      "title": "Evaluating fairness of machine learning prediction of prolonged wait times in Emergency Department with Interpretable eXtreme gradient boosting.",
      "abstract": "It is essential to evaluate performance and assess quality before applying artificial intelligence (AI) and machine learning (ML) models to clinical practice. This study utilized ML to predict patient wait times in the Emergency Department (ED), determine model performance accuracies, and conduct fairness evaluations to further assess ethnic disparities in using ML for wait time prediction among different patient populations in the ED. This retrospective observational study included adult patients (age \u226518 years) in the ED (n=173,856 visits) who were assigned an Emergency Severity Index (ESI) level of 3 at triage. Prolonged wait time was defined as waiting time \u226530 minutes. We employed extreme gradient boosting (XGBoost) for predicting prolonged wait times. Model performance was assessed with accuracy, recall, precision, F1 score, and false negative rate (FNR). To perform the global and local interpretation of feature importance, we utilized Shapley additive explanations (SHAP) to interpret the output from the XGBoost model. Fairness in ML models were evaluated across sensitive attributes (sex, race and ethnicity, and insurance status) at both subgroup and individual levels. We found that nearly half (48.43%, 84,195) of ED patient visits demonstrated prolonged ED wait times. XGBoost model exhibited moderate accuracy performance (AUROC=0.81). When fairness was evaluated with FNRs, unfairness existed across different sensitive attributes (male vs. female, Hispanic vs. Non-Hispanic White, and patients with insurances vs. without insurance). The predicted FNRs were lower among females, Hispanics, and patients without insurance compared to their counterparts. Therefore, XGBoost model demonstrated acceptable performance in predicting prolonged wait times in ED visits. However, disparities arise in predicting patients with different sex, race and ethnicity, and insurance status. To enhance the utility of ML model predictions in clinical practice, conducting performance assessments and fairness evaluations are crucial.",
      "journal": "PLOS digital health",
      "year": "2025",
      "doi": "10.1371/journal.pdig.0000751",
      "authors": "Wang Hao et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40111994/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "XGBoost/Gradient Boosting",
      "health_domain": "Emergency Medicine",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Emergency Department; Public Health/Population",
      "key_findings": "However, disparities arise in predicting patients with different sex, race and ethnicity, and insurance status. To enhance the utility of ML model predictions in clinical practice, conducting performance assessments and fairness evaluations are crucial.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11925291"
    },
    {
      "pmid": "40122892",
      "title": "Towards fairness-aware and privacy-preserving enhanced collaborative learning for healthcare.",
      "abstract": "The widespread integration of AI algorithms in healthcare has sparked ethical concerns, particularly regarding privacy and fairness. Federated Learning (FL) offers a promising solution to learn from a broad spectrum of patient data without directly accessing individual records, enhancing privacy while facilitating knowledge sharing across distributed data sources. However, healthcare institutions face significant variations in access to crucial computing resources, with resource budgets often linked to demographic and socio-economic factors, exacerbating unfairness in participation. While heterogeneous federated learning methods allow healthcare institutions with varying computational capacities to collaborate, they fail to address the performance gap between resource-limited and resource-rich institutions. As a result, resource-limited institutions may receive suboptimal models, further reinforcing disparities in AI-driven healthcare outcomes. Here, we propose a resource-adaptive framework for collaborative learning that dynamically adjusts to varying computational capacities, ensuring fair participation. Our approach enhances model accuracy, safeguards patient privacy, and promotes equitable access to trustworthy and efficient AI-driven healthcare solutions.",
      "journal": "Nature communications",
      "year": "2025",
      "doi": "10.1038/s41467-025-58055-3",
      "authors": "Zhang Feilong et al.",
      "keywords": "",
      "mesh_terms": "Humans; Privacy; Delivery of Health Care; Cooperative Behavior; Algorithms; Artificial Intelligence; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40122892/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Federated Learning",
      "health_domain": "ICU/Critical Care; Surgery",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Federated Learning",
      "clinical_setting": "ICU",
      "key_findings": "Here, we propose a resource-adaptive framework for collaborative learning that dynamically adjusts to varying computational capacities, ensuring fair participation. Our approach enhances model accuracy, safeguards patient privacy, and promotes equitable access to trustworthy and efficient AI-driven healthcare solutions.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11930927"
    },
    {
      "pmid": "40127903",
      "title": "PROBAST+AI: an updated quality, risk of bias, and applicability assessment tool for prediction models using regression or artificial intelligence methods.",
      "abstract": "The Prediction model Risk Of Bias ASsessment Tool (PROBAST) is used to assess the quality, risk of bias, and applicability of prediction models or algorithms and of prediction model/algorithm studies. Since PROBAST\u2019s introduction in 2019, much progress has been made in the methodology for prediction modelling and in the use of artificial intelligence, including machine learning, techniques. An update to PROBAST-2019 is thus needed. This article describes the development of PROBAST+AI. PROBAST+AI consists of two distinctive parts: model development and model evaluation. For model development, PROBAST+AI users assess quality and applicability using 16 targeted signalling questions. For model evaluation, PROBAST+AI users assess the risk of bias and applicability using 18 targeted signalling questions. Both parts contain four domains: participants and data sources, predictors, outcome, and analysis. Applicability of the prediction model is rated for the participants and data sources, predictors, and outcome domains. PROBAST+AI may replace the original PROBAST tool and allows all key stakeholders (eg, model developers, AI companies, researchers, editors, reviewers, healthcare professionals, guideline developers, and policy organisations) to examine the quality, risk of bias, and applicability of any type of prediction model in the healthcare sector, irrespective of whether regression modelling or AI techniques are used.",
      "journal": "BMJ (Clinical research ed.)",
      "year": "2025",
      "doi": "10.1136/bmj-2024-082505",
      "authors": "Moons Karel G M et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40127903/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Prediction Model; Generative AI; Regression",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Collection; Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Applicability of the prediction model is rated for the participants and data sources, predictors, and outcome domains. PROBAST+AI may replace the original PROBAST tool and allows all key stakeholders (eg, model developers, AI companies, researchers, editors, reviewers, healthcare professionals, guideline developers, and policy organisations) to examine the quality, risk of bias, and applicability of any type of prediction model in the healthcare sector, irrespective of whether regression modelli...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11931409"
    },
    {
      "pmid": "40138420",
      "title": "Demographic bias of expert-level vision-language foundation models in medical imaging.",
      "abstract": "Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit training annotations. However, it is crucial to ensure that these AI models do not mirror or amplify human biases, disadvantaging historically marginalized groups such as females or Black patients. In this study, we investigate the algorithmic fairness of state-of-the-art vision-language foundation models in chest x-ray diagnosis across five globally sourced datasets. Our findings reveal that compared to board-certified radiologists, these foundation models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups such as Black female patients. Such biases present over a wide range of pathologies and demographic attributes. Further analysis of the model embedding uncovers its substantial encoding of demographic information. Deploying medical AI systems with biases can intensify preexisting care disparities, posing potential challenges to equitable healthcare access and raising ethical questions about their clinical applications.",
      "journal": "Science advances",
      "year": "2025",
      "doi": "10.1126/sciadv.adq0305",
      "authors": "Yang Yuzhe et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Artificial Intelligence; Diagnostic Imaging; Algorithms; Male; Demography",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40138420/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Computer Vision/Imaging AI; Foundation Model",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language; Intersectional",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Representation Learning",
      "clinical_setting": "Not specified",
      "key_findings": "Further analysis of the model embedding uncovers its substantial encoding of demographic information. Deploying medical AI systems with biases can intensify preexisting care disparities, posing potential challenges to equitable healthcare access and raising ethical questions about their clinical applications.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11939055"
    },
    {
      "pmid": "40139775",
      "title": "False hope of a single generalisable AI sepsis prediction model: bias and proposed mitigation strategies for improving performance based on a retrospective multisite cohort study.",
      "abstract": "OBJECTIVE: To identify bias in using a single machine learning (ML) sepsis prediction model across multiple hospitals and care locations; evaluate the impact of six different bias mitigation strategies and propose a generic modelling approach for developing best-performing models. METHODS: We developed a baseline ML model to predict sepsis using retrospective data on patients in emergency departments (EDs) and wards across nine hospitals. We set model sensitivity at 70% and determined the number of alerts required to be evaluated (number needed to evaluate (NNE), 95%\u2009CI) for each case of true sepsis and the number of hours between the first alert and timestamped outcomes meeting sepsis-3 reference criteria (HTS3). Six bias mitigation models were compared with the baseline model for impact on NNE and HTS3. RESULTS: Across 969\u2009292 admissions, mean NNE for the baseline model was significantly lower for EDs (6.1 patients, 95% CI 6 to 6.2) than for wards (7.5 patients, 95% CI 7.4 to 7.5). Across all sites, median HTS3 was 20 hours (20-21) for wards vs 5 (5-5) for EDs. Bias mitigation models significantly impacted NNE but not HTS3. Compared with the baseline model, the best-performing models for NNE with reduced interhospital variance were those trained separately on data from ED patients or from ward patients across all sites. These models generated the lowest NNE results for all care locations in seven of nine hospitals. CONCLUSIONS: Implementing a single sepsis prediction model across all sites and care locations within multihospital systems may be unacceptable given large variances in NNE across multiple sites. Bias mitigation methods can identify models demonstrating improved performance across most sites in reducing alert burden but with no impact on the length of the prediction window.",
      "journal": "BMJ quality & safety",
      "year": "2025",
      "doi": "10.1136/bmjqs-2024-018328",
      "authors": "Schnetler Rudolf et al.",
      "keywords": "Decision support, computerized; Hospital medicine; Information technology; Patient Safety",
      "mesh_terms": "Humans; Sepsis; Retrospective Studies; Machine Learning; Emergency Service, Hospital; Bias; Female; Male; Middle Aged; Aged",
      "pub_types": "Journal Article; Multicenter Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40139775/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Emergency Medicine; ICU/Critical Care",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; Emergency Department",
      "key_findings": "CONCLUSIONS: Implementing a single sepsis prediction model across all sites and care locations within multihospital systems may be unacceptable given large variances in NNE across multiple sites. Bias mitigation methods can identify models demonstrating improved performance across most sites in reducing alert burden but with no impact on the length of the prediction window.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40150706",
      "title": "Simplatab: An Automated Machine Learning Framework for Radiomics-Based Bi-Parametric MRI Detection of Clinically Significant Prostate Cancer.",
      "abstract": "BACKGROUND: Prostate cancer (PCa) diagnosis using MRI is often challenged by lesion variability. METHODS: This study introduces Simplatab, an open-source automated machine learning (AutoML) framework designed for, but not limited to, automating the entire machine Learning pipeline to facilitate the detection of clinically significant prostate cancer (csPCa) using radiomics features. Unlike existing AutoML tools such as Auto-WEKA, Auto-Sklearn, ML-Plan, ATM, Google AutoML, and TPOT, Simplatab offers a comprehensive, user-friendly framework that integrates data bias detection, feature selection, model training with hyperparameter optimization, explainable AI (XAI) analysis, and post-training model vulnerabilities detection. Simplatab requires no coding expertise, provides detailed performance reports, and includes robust data bias detection, making it particularly suitable for clinical applications. RESULTS: Evaluated on a large pan-European cohort of 4816 patients from 12 clinical centers, Simplatab supports multiple machine learning algorithms. The most notable features that differentiate Simplatab include ease of use, a user interface accessible to those with no coding experience, comprehensive reporting, XAI integration, and thorough bias assessment, all provided in a human-understandable format. CONCLUSIONS: Our findings indicate that Simplatab can significantly enhance the usability, accountability, and explainability of machine learning in clinical settings, thereby increasing trust and accessibility for AI non-experts.",
      "journal": "Bioengineering (Basel, Switzerland)",
      "year": "2025",
      "doi": "10.3390/bioengineering12030242",
      "authors": "Zaridis Dimitrios I et al.",
      "keywords": "AutoML; MRI; artificial intelligence; automated machine learning framework; open source; prostate cancer; radiomics",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40150706/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Oncology; ICU/Critical Care",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSIONS: Our findings indicate that Simplatab can significantly enhance the usability, accountability, and explainability of machine learning in clinical settings, thereby increasing trust and accessibility for AI non-experts.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11939345"
    },
    {
      "pmid": "40152140",
      "title": "Predicting postoperative chronic opioid use with fair machine learning models integrating multi-modal data sources: a demonstration of ethical machine learning in healthcare.",
      "abstract": "OBJECTIVE: Building upon our previous work on predicting chronic opioid use using electronic health records (EHR) and wearable data, this study leveraged the Health Equity Across the AI Lifecycle (HEAAL) framework to (a) fine tune the previously built model with genomic data and evaluate model performance in predicting chronic opioid use and (b) apply IBM's AIF360 pre-processing toolkit to mitigate bias related to gender and race and evaluate the model performance using various fairness metrics. MATERIALS AND METHODS: Participants included approximately 271 All of Us Research Program subjects with EHR, wearable, and genomic data. We fine-tuned 4 machine learning models on the new dataset. The SHapley Additive exPlanations (SHAP) technique identified the best-performing predictors. A preprocessing toolkit boosted fairness by gender and race. RESULTS: The genetic data enhanced model performance from the prior model, with the area under the curve improving from 0.90 (95% CI, 0.88-0.92) to 0.95 (95% CI, 0.89-0.95). Key predictors included Dopamine D1 Receptor (DRD1) rs4532, general type of surgery, and time spent in physical activity. The reweighing preprocessing technique applied to the stacking algorithm effectively improved the model's fairness across racial and gender groups without compromising performance. CONCLUSION: We leveraged 2 dimensions of the HEAAL framework to build a fair artificial intelligence (AI) solution. Multi-modal datasets (including wearable and genetic data) and applying bias mitigation strategies can help models to more fairly and accurately assess risk across diverse populations, promoting fairness in AI in healthcare.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2025",
      "doi": "10.1093/jamia/ocaf053",
      "authors": "Soley Nidhi et al.",
      "keywords": "All of Us; chronic opioid use; ethical machine learning; multimodal dataset; responsible AI",
      "mesh_terms": "Humans; Machine Learning; Electronic Health Records; Male; Female; Opioid-Related Disorders; Postoperative Pain; Wearable Electronic Devices; Analgesics, Opioid; Middle Aged; Adult; Information Sources",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40152140/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Ensemble Methods",
      "health_domain": "Mental Health/Psychiatry; EHR/Health Informatics; Surgery; Genomics/Genetics; Pain Management; Wearables/Remote Monitoring",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Fairness Metrics Evaluation; Transfer Learning; Explainability/Interpretability",
      "clinical_setting": "Public Health/Population; Telehealth/Remote",
      "key_findings": "CONCLUSION: We leveraged 2 dimensions of the HEAAL framework to build a fair artificial intelligence (AI) solution. Multi-modal datasets (including wearable and genetic data) and applying bias mitigation strategies can help models to more fairly and accurately assess risk across diverse populations, promoting fairness in AI in healthcare.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12089784"
    },
    {
      "pmid": "40171241",
      "title": "Utilizing large language models for gastroenterology research: a conceptual framework.",
      "abstract": "Large language models (LLMs) transform healthcare by assisting clinicians with decision-making, research, and patient management. In gastroenterology, LLMs have shown potential in clinical decision support, data extraction, and patient education. However, challenges such as bias, hallucinations, integration with clinical workflows, and regulatory compliance must be addressed for safe and effective implementation. This manuscript presents a structured framework for integrating LLMs into gastroenterology, using Hepatitis C treatment as a real-world application. The framework outlines key steps to ensure accuracy, safety, and clinical relevance while mitigating risks associated with artificial intelligence (AI)-driven healthcare tools. The framework includes defining clinical goals, assembling a multidisciplinary team, data collection and preparation, model selection, fine-tuning, calibration, hallucination mitigation, user interface development, integration with electronic health records, real-world validation, and continuous improvement. Retrieval-augmented generation and fine-tuning approaches are evaluated for optimizing model adaptability. Bias detection, reinforcement learning from human feedback, and structured prompt engineering are incorporated to enhance reliability. Ethical and regulatory considerations, including the Health Insurance Portability and Accountability Act, General Data Protection Regulation, and AI-specific guidelines (DECIDE-AI, SPIRIT-AI, CONSORT-AI), are addressed to ensure responsible AI deployment. LLMs have the potential to enhance decision-making, research efficiency, and patient care in gastroenterology, but responsible deployment requires bias mitigation, transparency, and ongoing validation. Future research should focus on multi-institutional validation and AI-assisted clinical trials to establish LLMs as reliable tools in gastroenterology. How large language models could transform gastroenterology: a framework for future research and care Artificial intelligence (AI) is transforming healthcare by helping doctors make better decisions, analyze research faster, and improve patient care. Large language models (LLMs) are a type of AI that process and generate human-like text, making them useful in gastroenterology. This paper presents a structured framework for safely using LLMs in clinical practice, using Hepatitis C treatment as an example. The framework begins by setting clear goals, such as improving Hepatitis C treatment recommendations or making patient education easier to understand. A team of doctors, AI specialists, and data experts is assembled to ensure the model is medically accurate and practical. Next, relevant medical data from electronic health records (EHRs), clinical guidelines, and research studies is gathered and prepared to improve AI, ensuring it provides useful and fair recommendations. The right AI model is then chosen and improved to specialize in gastroenterology. To make sure the model is reliable and makes correct suggestions, its performance is checked and adjusted before use. A user-friendly interface is created so doctors can access AI-generated recommendations directly in EHRs and decision-support tools, making it easy to integrate into daily practice. Before full use, the AI is tested in real-world settings, where gastroenterologists review its recommendations for safety and accuracy. Once in use, ongoing updates based on doctor feedback help improve its performance. Ethical and legal safeguards, such as protecting patient privacy and ensuring fairness, guide its responsible use. Findings are then shared with the medical community, allowing for further testing and broader adoption. By following this framework, LLMs can help doctors make better decisions, personalize treatments, and improve efficiency, ultimately leading to better patient outcomes in gastroenterology.",
      "journal": "Therapeutic advances in gastroenterology",
      "year": "2025",
      "doi": "10.1177/17562848251328577",
      "authors": "Berry Parul et al.",
      "keywords": "artificial intelligence; framework; generative artificial intelligence; healthcare",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40171241/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM; Reinforcement Learning; Clinical Decision Support",
      "health_domain": "EHR/Health Informatics; Infectious Disease",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Language; Insurance Status",
      "lifecycle_stage": "Data Collection; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Transfer Learning",
      "clinical_setting": "Public Health/Population; Clinical Trial",
      "key_findings": "Findings are then shared with the medical community, allowing for further testing and broader adoption. By following this framework, LLMs can help doctors make better decisions, personalize treatments, and improve efficiency, ultimately leading to better patient outcomes in gastroenterology.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11960180"
    },
    {
      "pmid": "40171421",
      "title": "Integrating health equity in artificial intelligence for public health in Canada: a rapid narrative review.",
      "abstract": "INTRODUCTION: The application of artificial intelligence (AI) in public health is rapidly evolving, offering promising advancements in various public health settings across Canada. AI has the potential to enhance the effectiveness, precision, decision-making, and scalability of public health initiatives. However, to leverage AI in public health without exacerbating inequities, health equity considerations must be addressed. This rapid narrative review aims to synthesize health equity considerations related to AI application in public health. METHODS: A rapid narrative review methodology was used to identify and synthesize literature on health equity considerations for AI application in public health. After conducting title/abstract and full-text screening of articles, and consensus decision on study inclusion, the data extraction process proceeded using an extraction template. Data synthesis included the identification of challenges and opportunities for strengthening health equity in AI application for public health. RESULTS: The review included 54 peer-review articles and grey literature sources. Several health equity considerations for applying AI in public health were identified, including gaps in AI epistemology, algorithmic bias, accessibility of AI technologies, ethical and privacy concerns, unrepresentative training datasets, lack of transparency and interpretability of AI models, and challenges in scaling technical skills. CONCLUSION: While AI has the potential to advance public health in Canada, addressing equity is critical to preventing inequities. Opportunities to strengthen health equity in AI include implementing diverse AI frameworks, ensuring human oversight, using advanced modeling techniques to mitigate biases, fostering intersectoral collaboration for equitable AI development, and standardizing ethical and privacy guidelines to enhance AI governance.",
      "journal": "Frontiers in public health",
      "year": "2025",
      "doi": "10.3389/fpubh.2025.1524616",
      "authors": "Ghanem Samantha et al.",
      "keywords": "AI biases; AI ethics; artificial intelligence; health equity; public health",
      "mesh_terms": "Artificial Intelligence; Humans; Health Equity; Canada; Public Health",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40171421/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: While AI has the potential to advance public health in Canada, addressing equity is critical to preventing inequities. Opportunities to strengthen health equity in AI include implementing diverse AI frameworks, ensuring human oversight, using advanced modeling techniques to mitigate biases, fostering intersectoral collaboration for equitable AI development, and standardizing ethical and privacy guidelines to enhance AI governance.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11958991"
    },
    {
      "pmid": "40196288",
      "title": "Navigating Fairness in AI-based Prediction Models: Theoretical Constructs and Practical Applications.",
      "abstract": "Artificial Intelligence (AI)-based prediction models, including risk scoring systems and decision support systems, are increasingly adopted in healthcare. Addressing AI fairness is essential to fighting health disparities and achieving equitable performance and patient outcomes. Numerous and conflicting definitions of fairness complicate this effort. This paper aims to structure the transition of AI fairness from theory to practical application with appropriate fairness metrics. For 27 definitions of fairness identified in the recent literature, we assess the relation with the model's intended use, type of decision influenced and ethical principles of distributive justice. We advocate that due to limitations in some notions of fairness, clinical utility, performance-based metrics (area under the receiver operating characteristic curve), calibration, and statistical parity are the most relevant group-based metrics for medical applications. Through two use cases, we demonstrate that different metrics may be applicable depending on the intended use and ethical framework. Our approach provides a foundation for AI developers and assessors by assessing model fairness and the impact of bias mitigation strategies, hence promoting more equitable AI-based implementations.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.03.24.25324500",
      "authors": "van der Meijden S L et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40196288/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Prediction Model; Clinical Decision Support",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "Through two use cases, we demonstrate that different metrics may be applicable depending on the intended use and ethical framework. Our approach provides a foundation for AI developers and assessors by assessing model fairness and the impact of bias mitigation strategies, hence promoting more equitable AI-based implementations.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11974802"
    },
    {
      "pmid": "40198849",
      "title": "Revisiting Technical Bias Mitigation Strategies.",
      "abstract": "Efforts to mitigate bias and enhance fairness in the artificial intelligence (AI) community have predominantly focused on technical solutions. While numerous reviews have addressed bias in AI, this review uniquely focuses on the practical limitations of technical solutions in healthcare settings, providing a structured analysis across five key dimensions affecting their real-world implementation: who defines bias and fairness, which mitigation strategy to use and prioritize among dozens that are inconsistent and incompatible, when in the AI development stages the solutions are most effective, for which populations, and the context for which the solutions are designed. We illustrate each limitation with empirical studies focusing on healthcare and biomedical applications. Moreover, we discuss how value-sensitive AI, a framework derived from technology design, can engage stakeholders and ensure that their values are embodied in bias and fairness mitigation solutions. Finally, we discuss areas that require further investigation and provide practical recommendations to address the limitations covered in the study.",
      "journal": "Annual review of biomedical data science",
      "year": "2025",
      "doi": "10.1146/annurev-biodatasci-103123-095737",
      "authors": "Mahamadou Abdoul Jalil Djiberou et al.",
      "keywords": "AI bias; bias mitigation; ethics; stakeholder engagement; value-sensitive design",
      "mesh_terms": "Artificial Intelligence; Humans; Bias; Delivery of Health Care",
      "pub_types": "Journal Article; Review; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40198849/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Moreover, we discuss how value-sensitive AI, a framework derived from technology design, can engage stakeholders and ensure that their values are embodied in bias and fairness mitigation solutions. Finally, we discuss areas that require further investigation and provide practical recommendations to address the limitations covered in the study.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40199877",
      "title": "Achieving flexible fairness metrics in federated medical imaging.",
      "abstract": "The rapid adoption of Artificial Intelligence (AI) in medical imaging raises fairness and privacy concerns across demographic groups, especially in diagnosis and treatment decisions. While federated learning (FL) offers decentralized privacy preservation, current frameworks often prioritize collaboration fairness over group fairness, risking healthcare disparities. Here we present FlexFair, an innovative FL framework designed to address both fairness and privacy challenges. FlexFair incorporates a flexible regularization term to facilitate the integration of multiple fairness criteria, including equal accuracy, demographic parity, and equal opportunity. Evaluated across four clinical applications (polyp segmentation, fundus vascular segmentation, cervical cancer segmentation, and skin disease diagnosis), FlexFair outperforms state-of-the-art methods in both fairness and accuracy. Moreover, we curate a multi-center dataset for cervical cancer segmentation that includes 678 patients from four hospitals. This diverse dataset allows for a more comprehensive analysis of model performance across different population groups, ensuring the findings are applicable to a broader range of patients.",
      "journal": "Nature communications",
      "year": "2025",
      "doi": "10.1038/s41467-025-58549-0",
      "authors": "Xing Huijun et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Artificial Intelligence; Diagnostic Imaging; Uterine Cervical Neoplasms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40199877/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Federated Learning; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Dermatology; Oncology; Ophthalmology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Federated Learning; Diverse/Representative Data; Regularization",
      "clinical_setting": "Hospital/Inpatient; Public Health/Population",
      "key_findings": "Moreover, we curate a multi-center dataset for cervical cancer segmentation that includes 678 patients from four hospitals. This diverse dataset allows for a more comprehensive analysis of model performance across different population groups, ensuring the findings are applicable to a broader range of patients.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11978761"
    },
    {
      "pmid": "40218157",
      "title": "Deep Learning-Based Glaucoma Detection Using Clinical Notes: A Comparative Study of Long Short-Term Memory and Convolutional Neural Network Models.",
      "abstract": "Background/Objectives: Glaucoma is the second-leading cause of irreversible blindness globally. Retinal images such as color fundus photography have been widely used to detect glaucoma. However, little is known about the effectiveness of using raw clinical notes generated by glaucoma specialists in detecting glaucoma. This study aims to investigate the capability of deep learning approaches to detect glaucoma from clinical notes based on a real-world dataset including 10,000 patients. Different popular models are explored to predict the binary glaucomatous status defined from a comprehensive vision function assessment. Methods: We compared multiple deep learning architectures, including Long Short-Term Memory (LSTM) networks, Convolutional Neural Networks (CNNs), and transformer-based models BERT and BioBERT. LSTM exploits temporal feature dependencies within the clinical notes, while CNNs focus on extracting local textual features, and transformer-based models leverage self-attention to capture rich contextual information and feature correlations. We also investigated the group disparities of deep learning for glaucoma detection in various demographic groups. Results: The experimental results indicate that the CNN model achieved an Overall AUC of 0.80, slightly outperforming LSTM by 0.01. Both models showed disparities and biases in performance across different racial groups. However, the CNN showed reduced group disparities compared to LSTM across Asian, Black, and White groups, meaning it has the advantage of achieving more equitable outcomes. Conclusions: This study demonstrates the potential of deep learning models to detect glaucoma from clinical notes and highlights the need for fairness-aware modeling to address health disparities.",
      "journal": "Diagnostics (Basel, Switzerland)",
      "year": "2025",
      "doi": "10.3390/diagnostics15070807",
      "authors": "Mohammadjafari Ali et al.",
      "keywords": "AI healthcare; CNN; LSTM; clinical notes; deep learning; fairness-aware modeling; glaucoma detection",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40218157/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; NLP/LLM; Neural Network",
      "health_domain": "Ophthalmology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Conclusions: This study demonstrates the potential of deep learning models to detect glaucoma from clinical notes and highlights the need for fairness-aware modeling to address health disparities.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11988537"
    },
    {
      "pmid": "40270986",
      "title": "An Updated Systematic Review on Asthma Exacerbation Risk Prediction Models Between 2017 and 2023: Risk of Bias and Applicability.",
      "abstract": "BACKGROUND: Accurate risk prediction of exacerbations in asthma patients promotes personalized asthma management. OBJECTIVE: This systematic review aimed to provide an update and critically appraise the quality and usability of asthma exacerbation prediction models which were developed since 2017. METHODS: In the Embase and PubMed databases, we performed a systematic search for studies published in English between May 2017 and August 2023, and identified peer-reviewed publications regarding the development of prognostic prediction models for the risk of asthma exacerbations in adult patients with asthma. We then applied the Prediction Risk of Bias Assessment tool (PROBAST) to assess the risk of bias and applicability of the included models. RESULTS: Of 415 studies screened, 10 met eligibility criteria, comprising 41 prediction models. Among them, 7 (70%) studies used real-world data (RWD) and 3 (30%) were based on trial data to derive the models, 7 (70%) studies applied machine learning algorithms, and 2 (20%) studies included biomarkers like blood eosinophil count and fractional exhaled nitric oxide in the model. PROBAST indicated a generally high risk of bias (80%) in these models, which mainly originated from the sample selection (\"Participant\" domain, 6 studies) and statistical analysis (\"Analysis\" domain, 7 studies). Meanwhile, 5 (50%) studies were rated as having a high concern in applicability due to model complexity. CONCLUSION: Despite the use of big health data and advanced ML, asthma risk prediction models from 2017-2023\u00a0had high risk of bias and limited practical use. Future efforts should enhance generalizability and practicality for real-world implementation.",
      "journal": "Journal of asthma and allergy",
      "year": "2025",
      "doi": "10.2147/JAA.S509260",
      "authors": "Liu Anqi et al.",
      "keywords": "adults; asthma; exacerbation; prediction model; risk",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40270986/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Pulmonology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Despite the use of big health data and advanced ML, asthma risk prediction models from 2017-2023\u00a0had high risk of bias and limited practical use. Future efforts should enhance generalizability and practicality for real-world implementation.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12017270"
    },
    {
      "pmid": "40318249",
      "title": "Building competency in artificial intelligence and bias mitigation for nurse scientists and aligned health researchers.",
      "abstract": "Healthcare systems are increasingly integrating artificial intelligence and machine learning (AI/ML) tools into patient care, potentially influencing clinical decisions for millions. However, concerns are growing about these tools reinforcing systemic inequities. To address bias in AI/ML tools and promote equitable outcomes, guidelines for mitigating this bias and comprehensive workforce training programs are necessary. In response, we developed the multifaceted Human-Centered Use of Multidisciplinary AI for Next-Gen Education and Research (HUMAINE), informed by a comprehensive scoping review, training workshops, and a research symposium. The curriculum, which focuses on structural inequities in algorithms that contribute to health disparities, is designed to equip scientists with AI/ML competencies that allow them to effectively address these structural inequities and promote health equity. The curriculum incorporates the perspectives of clinicians, biostatisticians, engineers, and policymakers to harness AI's transformative potential, with the goal of building an inclusive ecosystem where cutting-edge technology and ethical AI governance converge to create a more equitable healthcare future for all.",
      "journal": "Nursing outlook",
      "year": "2025",
      "doi": "10.1016/j.outlook.2025.102395",
      "authors": "Cary Michael P et al.",
      "keywords": "Algorithmic bias; Artificial intelligence; Ethics in AI; Health equity; Healthcare disparities; Machine learning; Social determinants of health; Structural racism; Workforce training",
      "mesh_terms": "Humans; Artificial Intelligence; Research Personnel; Curriculum",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40318249/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "The curriculum, which focuses on structural inequities in algorithms that contribute to health disparities, is designed to equip scientists with AI/ML competencies that allow them to effectively address these structural inequities and promote health equity. The curriculum incorporates the perspectives of clinicians, biostatisticians, engineers, and policymakers to harness AI's transformative potential, with the goal of building an inclusive ecosystem where cutting-edge technology and ethical AI ...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12178818"
    },
    {
      "pmid": "40323319",
      "title": "Debiased machine learning for ultra-high dimensional mediation analysis.",
      "abstract": "MOTIVATION: In ultra-high dimensional mediation analysis, confounding variables can influence both mediators and outcomes through complex functional forms. While machine learning (ML) approaches are effective at modeling such complex relationships, they can introduce bias when estimating mediation effects. In this article, we propose a debiased ML framework that mitigates this bias, enabling accurate identification of key mediators and precise estimation and inference of their respective contributions. RESULTS: We construct an orthogonalized score function and use cross-fitting to reduce bias introduced by ML. To tackle ultra-high dimensional potential mediators, we implement screening and regularization techniques for variable selection and effect estimation. For statistical inference of the mediators' contributions, we use an adjusted Sobel-type test. Simulation results demonstrate the superior performance of the proposed method in handling complex confounding. Applying this method to Alzheimer's Disease Neuroimaging Initiative data, we identify several cytosine-phosphate-guanine sites where DNA methylation mediates the effect of body mass index on Alzheimer's Disease. AVAILABILITY AND IMPLEMENTATION: The R function DML_HDMA implementing the proposed methods is available online at https://github.com/Wei-Kecheng/DML_HDMA.",
      "journal": "Bioinformatics (Oxford, England)",
      "year": "2025",
      "doi": "10.1093/bioinformatics/btaf282",
      "authors": "Wei Kecheng et al.",
      "keywords": "",
      "mesh_terms": "Machine Learning; Humans; Alzheimer Disease; DNA Methylation; Algorithms; Neuroimaging; Body Mass Index",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40323319/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Regularization",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: We construct an orthogonalized score function and use cross-fitting to reduce bias introduced by ML. To tackle ultra-high dimensional potential mediators, we implement screening and regularization techniques for variable selection and effect estimation. For statistical inference of the mediators' contributions, we use an adjusted Sobel-type test.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12198499"
    },
    {
      "pmid": "40343639",
      "title": "Shortcut learning leads to sex bias in deep learning models for photoacoustic tomography.",
      "abstract": "PURPOSE: Shortcut learning has been identified as a source of algorithmic unfairness in medical imaging artificial intelligence (AI), but its impact on photoacoustic tomography (PAT), particularly concerning sex bias, remains underexplored. This study investigates this issue using peripheral artery disease (PAD) diagnosis as a specific clinical application. METHODS: To examine the potential for sex bias due to shortcut learning in convolutional neural network (CNNs) and assess how such biases might affect diagnostic predictions, we created training and test datasets with varying PAD prevalence between sexes. Using these datasets, we explored (1) whether CNNs can classify the sex from imaging data, (2) how sex-specific prevalence shifts impact PAD diagnosis performance and underdiagnosis disparity between sexes, and (3) how similarly CNNs encode sex and PAD features. RESULTS: Our study with 147 individuals demonstrates that CNNs can classify the sex from calf muscle PAT images, achieving an AUROC of 0.75. For PAD diagnosis, models trained on data with imbalanced sex-specific disease prevalence experienced significant performance drops (up to 0.21 AUROC) when applied to balanced test sets. Additionally, greater imbalances in sex-specific prevalence within the training data exacerbated underdiagnosis disparities between sexes. Finally, we identify evidence of shortcut learning by demonstrating the effective reuse of learned feature representations between PAD diagnosis and sex classification tasks. CONCLUSION: CNN-based models trained on PAT data may engage in shortcut learning by leveraging sex-related features, leading to biased and unreliable diagnostic predictions. Addressing demographic-specific prevalence imbalances and preventing shortcut learning is critical for developing models in the medical field that are both accurate and equitable across diverse patient populations.",
      "journal": "International journal of computer assisted radiology and surgery",
      "year": "2025",
      "doi": "10.1007/s11548-025-03370-9",
      "authors": "Knopp Marcel et al.",
      "keywords": "Peripheral artery disease (PAD); Photoacoustic tomography (PAT); Sex Bias in AI; Shortcut learning",
      "mesh_terms": "Humans; Deep Learning; Male; Female; Photoacoustic Techniques; Peripheral Arterial Disease; Sexism; Aged; Middle Aged; Tomography",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40343639/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Neural Network; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: CNN-based models trained on PAT data may engage in shortcut learning by leveraging sex-related features, leading to biased and unreliable diagnostic predictions. Addressing demographic-specific prevalence imbalances and preventing shortcut learning is critical for developing models in the medical field that are both accurate and equitable across diverse patient populations.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12226672"
    },
    {
      "pmid": "40351508",
      "title": "Evaluating algorithmic bias on biomarker classification of breast cancer pathology reports.",
      "abstract": "OBJECTIVES: This work evaluated algorithmic bias in biomarkers classification using electronic pathology reports from female breast cancer cases. Bias was assessed across 5 subgroups: cancer registry, race, Hispanic ethnicity, age at diagnosis, and socioeconomic status. MATERIALS AND METHODS: We utilized 594\u00a0875 electronic pathology reports from 178\u00a0121 tumors diagnosed in Kentucky, Louisiana, New Jersey, New Mexico, Seattle, and Utah to train 2 deep-learning algorithms to classify breast cancer patients using their biomarkers test results. We used balanced error rate (BER), demographic parity (DP), equalized odds (EOD), and equal opportunity (EOP) to assess bias. RESULTS: We found differences in predictive accuracy between registries, with the highest accuracy in the registry that contributed the most data (Seattle Registry, BER ratios for all registries >1.25). BER showed no significant algorithmic bias in extracting biomarkers (estrogen receptor, progesterone receptor, human epidermal growth factor receptor 2) for race, Hispanic ethnicity, age at diagnosis, or socioeconomic subgroups (BER ratio <1.25). DP, EOD, and EOP all showed insignificant results. DISCUSSION: We observed significant differences in BER by registry, but no significant bias using the DP, EOD, and EOP metrics for socio-demographic or racial categories. This highlights the importance of employing a diverse set of metrics for a comprehensive evaluation of model fairness. CONCLUSION: A thorough evaluation of algorithmic biases that may affect equality in clinical care is a critical step before deploying algorithms in the real world. We found little evidence of algorithmic bias in our biomarker classification tool. Artificial intelligence tools to expedite information extraction from clinical records could accelerate clinical trial matching and improve care.",
      "journal": "JAMIA open",
      "year": "2025",
      "doi": "10.1093/jamiaopen/ooaf033",
      "authors": "Tschida Jordan et al.",
      "keywords": "algorithmic bias; artificial intelligence; biomarkers; breast cancer; population-level",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40351508/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; Pathology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "Clinical Trial; Laboratory/Pathology",
      "key_findings": "CONCLUSION: A thorough evaluation of algorithmic biases that may affect equality in clinical care is a critical step before deploying algorithms in the real world. We found little evidence of algorithmic bias in our biomarker classification tool. Artificial intelligence tools to expedite information extraction from clinical records could accelerate clinical trial matching and improve care.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12063583"
    },
    {
      "pmid": "40354109",
      "title": "Evaluation and Bias Analysis of Large Language Models in Generating Synthetic Electronic Health Records: Comparative Study.",
      "abstract": "BACKGROUND: Synthetic electronic health records (EHRs) generated by large language models (LLMs) offer potential for clinical education and model training while addressing privacy concerns. However, performance variations and demographic biases in these models remain underexplored, posing risks to equitable health care. OBJECTIVE: This study aimed to systematically assess the performance of various LLMs in generating synthetic EHRs and to critically evaluate the presence of gender and racial biases in the generated outputs. We focused on assessing the completeness and representativeness of these EHRs across 20 diseases with varying demographic prevalence. METHODS: A framework was developed to generate 140,000 synthetic EHRs using 10 standardized prompts across 7 LLMs. The electronic health record performance score (EPS) was introduced to quantify completeness, while the statistical parity difference (SPD) was proposed to assess the degree and direction of demographic bias. Chi-square tests were used to evaluate the presence of bias across demographic groups. RESULTS: Larger models exhibited superior performance but heightened biases. The Yi-34B achieved the highest EPS (96.8), while smaller models (Qwen-1.8B: EPS=63.35) underperformed. Sex polarization emerged: female-dominated diseases (eg, multiple sclerosis) saw amplified female representation in outputs (Qwen-14B: 973/1000, 97.3% female vs 564,424/744,778, 75.78% real; SPD=+21.50%), while balanced diseases and male-dominated diseases skewed the male group (eg, hypertension Llama 2-13 B: 957/1000, 95.7% male vs 79,540,040/152,466,669, 52.17% real; SPD=+43.50%). Racial bias patterns revealed that some models overestimated the representation of White (eg, Yi-6B: mean SPD +14.40%, SD 16.22%) or Black groups (eg, Yi-34B: mean SPD +14.90%, SD 27.16%), while most models systematically underestimated the representation of Hispanic (average SPD across 7 models is -11.93%, SD 8.36%) and Asian groups (average SPD across 7 models is -0.77%, SD 11.99%). CONCLUSIONS: Larger models, such as Yi-34B, Qwen-14B, and Llama 2 to 13 B, showed improved performance in generating more comprehensive EHRs, as reflected in higher EPS values. However, this increased performance was accompanied by a notable escalation in both gender and racial biases, highlighting a performance-bias trade-off. The study identified 4 key findings as follows: (1) as model size increased, EHR generation improved, but demographic biases also became more pronounced; (2) biases were observed across all models, not just the larger ones; (3) gender bias closely aligned with real-world disease prevalence, while racial bias was evident in only a subset of diseases; and (4) racial biases varied, with some diseases showing overrepresentation of White or Black populations and underrepresentation of Hispanic and Asian groups. These findings underline the need for effective bias mitigation strategies and the development of benchmarks to ensure fairness in artificial intelligence applications for health care.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/65317",
      "authors": "Huang Ruochen et al.",
      "keywords": "artificial intelligence; electronic health records; gender bias; large language models; performance evaluation; racial bias",
      "mesh_terms": "Electronic Health Records; Humans; Female; Male; Bias; Large Language Models",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40354109/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: Larger models, such as Yi-34B, Qwen-14B, and Llama 2 to 13 B, showed improved performance in generating more comprehensive EHRs, as reflected in higher EPS values. However, this increased performance was accompanied by a notable escalation in both gender and racial biases, highlighting a performance-bias trade-off. The study identified 4 key findings as follows: (1) as model size increased, EHR generation improved, but demographic biases also became more pronounced; (2) biases were ...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12107208"
    },
    {
      "pmid": "40380414",
      "title": "Bias Detection in Histology Images Using Explainable AI and Image Darkness Assessment.",
      "abstract": "The study underscores the importance of addressing biases in medical AI models to improve fairness, generalizability, and clinical utility. In this paper, we present a novel framework that combines Explainable AI (XAI) with image darkness assessment to detect and mitigate bias in cervical histology image classification. Four deep learning architectures were employed-AlexNet, ResNet-50, EfficientNet-B0, and DenseNet-121-with EfficientNet-B0 demonstrating the highest accuracy post-mitigation. Grad-CAM and saliency maps were used to identify biases in the models' predictions. After applying brightness normalisation and synthetic data augmentation, the models shifted focus toward clinically relevant features, improving both accuracy and fairness. Statistical analysis using ANOVA confirmed a reduction in the influence of image darkness on model predictions after mitigation, as evidenced by a decrease in the F-statistic from 120.79 to 14.05, indicating improved alignment of the models with clinically relevant features.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI250302",
      "authors": "Skarga-Bandurova Inna et al.",
      "keywords": "Explainable AI (XAI); bias detection; histopathology; image darkness",
      "mesh_terms": "Humans; Deep Learning; Female; Bias; Uterine Cervical Neoplasms; Artificial Intelligence; Image Interpretation, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40380414/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Computer Vision/Imaging AI",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Data Augmentation; Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "After applying brightness normalisation and synthetic data augmentation, the models shifted focus toward clinically relevant features, improving both accuracy and fairness. Statistical analysis using ANOVA confirmed a reduction in the influence of image darkness on model predictions after mitigation, as evidenced by a decrease in the F-statistic from 120.79 to 14.05, indicating improved alignment of the models with clinically relevant features.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40382499",
      "title": "Fair ultrasound diagnosis via adversarial protected attribute aware perturbations on latent embeddings.",
      "abstract": "Deep learning techniques have significantly enhanced the convenience and precision of ultrasound image diagnosis, particularly in the crucial step of lesion segmentation. However, recent studies reveal that both train-from-scratch models and pre-trained models often exhibit performance disparities across sex and age attributes, leading to biased diagnoses for different subgroups. In this paper, we propose APPLE, a novel approach designed to mitigate unfairness without altering the parameters of the base model. APPLE achieves this by learning fair perturbations in the latent space through a generative adversarial network. Extensive experiments on both a publicly available dataset and an in-house ultrasound image dataset demonstrate that our method improves segmentation and diagnostic fairness across all sensitive attributes and various backbone architectures compared to the base models. Through this study, we aim to highlight the critical importance of fairness in medical segmentation and contribute to the development of a more equitable healthcare system.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01641-y",
      "authors": "Xu Zikang et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40382499/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning; Generative AI",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "Extensive experiments on both a publicly available dataset and an in-house ultrasound image dataset demonstrate that our method improves segmentation and diagnostic fairness across all sensitive attributes and various backbone architectures compared to the base models. Through this study, we aim to highlight the critical importance of fairness in medical segmentation and contribute to the development of a more equitable healthcare system.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12085594"
    },
    {
      "pmid": "40392801",
      "title": "AI-driven healthcare: Fairness in AI healthcare: A survey.",
      "abstract": "Artificial intelligence (AI) is rapidly advancing in healthcare, enhancing the efficiency and effectiveness of services across various specialties, including cardiology, ophthalmology, dermatology, emergency medicine, etc. AI applications have significantly improved diagnostic accuracy, treatment personalization, and patient outcome predictions by leveraging technologies such as machine learning, neural networks, and natural language processing. However, these advancements also introduce substantial ethical and fairness challenges, particularly related to biases in data and algorithms. These biases can lead to disparities in healthcare delivery, affecting diagnostic accuracy and treatment outcomes across different demographic groups. This review paper examines the integration of AI in healthcare, highlighting critical challenges related to bias and exploring strategies for mitigation. We emphasize the necessity of diverse datasets, fairness-aware algorithms, and regulatory frameworks to ensure equitable healthcare delivery. The paper concludes with recommendations for future research, advocating for interdisciplinary approaches, transparency in AI decision-making, and the development of innovative and inclusive AI applications.",
      "journal": "PLOS digital health",
      "year": "2025",
      "doi": "10.1371/journal.pdig.0000864",
      "authors": "Chinta Sribala Vidyadhari et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40392801/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "NLP/LLM; Neural Network",
      "health_domain": "Dermatology; Cardiology; Ophthalmology; Emergency Medicine; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Diverse/Representative Data",
      "clinical_setting": "ICU",
      "key_findings": "We emphasize the necessity of diverse datasets, fairness-aware algorithms, and regulatory frameworks to ensure equitable healthcare delivery. The paper concludes with recommendations for future research, advocating for interdisciplinary approaches, transparency in AI decision-making, and the development of innovative and inclusive AI applications.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12091740"
    },
    {
      "pmid": "40424948",
      "title": "Automated motor-leg scoring in stroke via a stable graph causality debiasing model.",
      "abstract": "Difficulty in resisting gravity is a common leg motor impairment in stroke patients, significantly impacting daily life. Automated clinical-level quantification of motor-leg videos based on the National Institutes of Health Stroke Scale is crucial for consistent and timely stroke diagnosis and assessment. However, real-world applications are challenged by interference impacting motion representation and decision-making, leading to performance instability. To address this, we propose a causality debiasing graph convolutional network. This model systematically reduces interference in both motor and non-motor body parts, extracting causal representations from human skeletons to ensure reliable decision-making. Specifically, an intra-class causality enhancement module is first proposed to resolve instability in motor-leg representations. This involves separating skeletal graphs with the same score, generating unbiased samples with similar discriminative features, and improving causal consistency. Subsequently, an inter-class non-causality suppression module is designed to handle biases in non-motor body parts. By decoupling skeletal graphs with different scores, this module constructs biased samples and enhances decision stability despite non-causal factors. Extensive validation on the clinical video dataset highlights the strong performance of our method for motor-leg scoring, achieving an impressive correlation above 0.82 with clinical scores, while independent testing at two additional hospitals further reinforces its stability. Furthermore, performance on another motor-arm scoring task and an additional Parkinsonian gait assessment task also successfully confirmed the method's reliability. Even when faced with potential real-world interferences, our approach consistently shows substantial value, offering both clinical significance and credibility. In summary, this work provides new insights for daily stroke assessment and telemedicine, with significant potential for widespread clinical adoption.",
      "journal": "Medical image analysis",
      "year": "2025",
      "doi": "10.1016/j.media.2025.103643",
      "authors": "Guo Rui et al.",
      "keywords": "Causality debiasing; Graph convolutional network; Stroke; Video-based assessment",
      "mesh_terms": "Humans; Stroke; Video Recording; Leg; Image Interpretation, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40424948/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Neurology",
      "bias_axes": "Gender/Sex; Disability",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; ICU; Telehealth/Remote",
      "key_findings": "Even when faced with potential real-world interferences, our approach consistently shows substantial value, offering both clinical significance and credibility. In summary, this work provides new insights for daily stroke assessment and telemedicine, with significant potential for widespread clinical adoption.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40435158",
      "title": "Is there a competitive advantage to using multivariate statistical or machine learning methods over the Bross formula in the hdPS framework for bias and variance estimation?",
      "abstract": "PURPOSE: We aim to evaluate various proxy selection methods within the context of high-dimensional propensity score (hdPS) analysis. This study aimed to systematically evaluate and compare the performance of traditional statistical methods and machine learning approaches within the hdPS framework, focusing on key metrics such as bias, standard error (SE), and coverage, under various exposure and outcome prevalence scenarios. METHODS: We conducted a plasmode simulation study using data from the National Health and Nutrition Examination Survey (NHANES) cycles from 2013 to 2018. We compared methods including the kitchen sink model, Bross-based hdPS, Hybrid hdPS, LASSO, Elastic Net, Random Forest, XGBoost, and Genetic Algorithm (GA). The performance of each inverse probability weighted method was assessed based on bias, MSE, coverage probability, and SE estimation across three epidemiological scenarios: frequent exposure and outcome, rare exposure and frequent outcome, and frequent exposure and rare outcome. RESULTS: XGBoost consistently demonstrated strong performance in terms of MSE and coverage, making it effective for scenarios prioritizing precision. However, it exhibited higher bias, particularly in rare exposure scenarios, suggesting it is less suited when minimizing bias is critical. In contrast, GA showed significant limitations, with consistently high bias and MSE, making it the least reliable method. Bross-based hdPS, and Hybrid hdPS methods provided a balanced approach, with low bias and moderate MSE, though coverage varied depending on the scenario. Rare outcome scenarios generally resulted in lower MSE and better precision, while rare exposure scenarios were associated with higher bias and MSE. Notably, traditional statistical approaches such as forward selection and backward elimination performed comparably to more sophisticated machine learning methods in terms of bias and coverage, suggesting that these simpler approaches may be viable alternatives due to their computational efficiency. CONCLUSION: The results highlight the importance of selecting hdPS methods based on the specific characteristics of the data, such as exposure and outcome prevalence. While advanced machine learning methods such as XGBoost can enhance precision, simpler methods such as forward selection or backward elimination may offer similar performance in terms of bias and coverage with fewer computational demands. Tailoring the choice of method to the epidemiological scenario is essential for optimizing the balance between bias reduction and precision.",
      "journal": "PloS one",
      "year": "2025",
      "doi": "10.1371/journal.pone.0324639",
      "authors": "Ehsanul Karim Mohammad et al.",
      "keywords": "",
      "mesh_terms": "Machine Learning; Humans; Bias; Propensity Score; Nutrition Surveys; Algorithms; Multivariate Analysis; Models, Statistical; Computer Simulation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40435158/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Random Forest; XGBoost/Gradient Boosting",
      "health_domain": "ICU/Critical Care; Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: The results highlight the importance of selecting hdPS methods based on the specific characteristics of the data, such as exposure and outcome prevalence. While advanced machine learning methods such as XGBoost can enhance precision, simpler methods such as forward selection or backward elimination may offer similar performance in terms of bias and coverage with fewer computational demands. Tailoring the choice of method to the epidemiological scenario is essential for optimizing the...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12118903"
    },
    {
      "pmid": "40439482",
      "title": "So You've Got a High AUC, Now What? An Overview of Important Considerations when Bringing Machine-Learning Models from Computer to Bedside.",
      "abstract": "Machine-learning (ML) models have the potential to transform health care by enabling more personalized and data-driven clinical decision making. However, their successful implementation in clinical practice requires careful consideration of factors beyond predictive accuracy. We provide an overview of essential considerations for developing clinically applicable ML models, including methods for assessing and improving calibration, selecting appropriate decision thresholds, enhancing model explainability, identifying and mitigating bias, as well as methods for robust validation. We also discuss strategies for improving accessibility to ML models and performing real-world testing.HighlightsThis tutorial provides clinicians with a comprehensive guide to implementing machine-learning classification models in clinical practice.Key areas covered include model calibration, threshold selection, explainability, bias mitigation, validation, and real-world testing, all of which are essential for the clinical deployment of machine-learning models.Following these guidance can help clinicians bridge the gap between machine-learning model development and real-world application and enhance patient care outcomes.",
      "journal": "Medical decision making : an international journal of the Society for Medical Decision Making",
      "year": "2025",
      "doi": "10.1177/0272989X251343082",
      "authors": "Deng Jiawen et al.",
      "keywords": "bedside deployment; external validation; health informatics; machine learning; model calibration",
      "mesh_terms": "Machine Learning; Humans; Area Under Curve; Clinical Decision-Making",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40439482/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Threshold Adjustment; Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "We provide an overview of essential considerations for developing clinically applicable ML models, including methods for assessing and improving calibration, selecting appropriate decision thresholds, enhancing model explainability, identifying and mitigating bias, as well as methods for robust validation. We also discuss strategies for improving accessibility to ML models and performing real-world testing.HighlightsThis tutorial provides clinicians with a comprehensive guide to implementing mac...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12260203"
    },
    {
      "pmid": "40440013",
      "title": "Intersectional and Marginal Debiasing in Prediction Models for Emergency Admissions.",
      "abstract": "IMPORTANCE: Fair clinical prediction models are crucial for achieving equitable health outcomes. Intersectionality has been applied to develop algorithms that address discrimination among intersections of protected attributes (eg, Black women rather than Black persons or women separately), yet most fair algorithms default to marginal debiasing, optimizing performance across simplified patient subgroups. OBJECTIVE: To assess the extent to which simplifying patient subgroups during training is associated with intersectional subgroup performance in emergency department (ED) admission models. DESIGN, SETTING, AND PARTICIPANTS: This prognostic study of admission prediction models used retrospective data from ED visits to Beth Israel Deaconess Medical Center Medical Information Mart for Intensive Care IV (MIMIC-IV; n\u2009=\u2009160\u202f016) from January 1, 2011, to December 31, 2019, and Boston Children's Hospital (BCH; n\u2009=\u200922\u202f222) from June 1 through August 13, 2019. Statistical analysis was conducted from January 2022 to August 2024. MAIN OUTCOMES AND MEASURES: The primary outcome was admission to an in-patient service. The accuracy of admission predictions among intersectional subgroups was measured under variations on model training with respect to optimizing for group level performance. Under different fairness definitions (calibration, error rate balance) and modeling methods (linear, nonlinear), overall performance and subgroup performance of marginal debiasing approaches were compared with intersectional debiasing approaches. Subgroups were defined by self-reported race and ethnicity and gender. Measures include area under the receiver operator characteristic curve (AUROC), area under the precision recall curve, subgroup calibration error, and false-negative rates. RESULTS: The MIMIC-IV cohort included 160\u202f016 visits (mean [SD] age, 53.0 [19.3] years; 57.4% female patients; 0.3% American Indian or Alaska Native patients, 3.7% Asian patients, 26.2% Black patients, 10.0% Hispanic or Latino patients, and 59.7% White patients; 29.5% admitted) and the BCH cohort included 22\u202f222 visits (mean [SD] age, 8.2 [6.8] years; 52.1% male patients; 0.1% American Indian or Alaska Native patients, 4.0% Asian patients, 19.7% Black patients, 30.6% Hispanic or Latino patients, 0.2% Native Hawaiian or Pacific Islander patients, 37.7% White patients; 16.3% admitted). Among MIMIC-IV groups, intersectional debiasing was associated with a reduced subgroup calibration error from 0.083 to 0.065 (22.3%), while marginal fairness debiasing was associated with a reduced subgroup calibration error from 0.083 to 0.074 (11.3%; difference, 11.1%); among BCH groups, intersectional debiasing was associated with a reduced subgroup calibration error from 0.111 to 0.080 (28.3%), while marginal fairness debiasing was associated with a reduced subgroup calibration error from 0.111 to 0.086 (22.6%; difference, 5.7%). Among MIMIC-IV groups, intersectional debiasing was associated with lowered subgroup false-negative rates from 0.142 to 0.125 (11.9%), while marginal debiasing was associated with lowered subgroup false-negative rates from 0.142 to 0.132 (6.8%; difference, 5.1%). Fairness improvements did not decrease overall accuracy compared with baseline models (eg, MIMIC-IV: mean [SD] AUROC, 0.85 [0.00], both models). Intersectional debiasing was associated with lowered error rates in several intersectional subpopulations compared with other strategies. CONCLUSIONS AND RELEVANCE: This study suggests that intersectional debiasing better mitigates performance disparities across intersecting groups than marginal debiasing for admission prediction. Intersectionally debiased models were associated with reduced group-specific errors without compromising overall accuracy. Clinical risk prediction models should consider incorporating intersectional debiasing into their development.",
      "journal": "JAMA network open",
      "year": "2025",
      "doi": "10.1001/jamanetworkopen.2025.12947",
      "authors": "Lett Elle et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Emergency Service, Hospital; Male; Retrospective Studies; Patient Admission; Adult; Middle Aged; Boston; Algorithms; Models, Statistical",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40440013/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Emergency Medicine; ICU/Critical Care; Pediatrics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Intersectional",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Subgroup Analysis",
      "clinical_setting": "Hospital/Inpatient; ICU; Emergency Department; Public Health/Population",
      "key_findings": "RESULTS: The MIMIC-IV cohort included 160\u202f016 visits (mean [SD] age, 53.0 [19.3] years; 57.4% female patients; 0.3% American Indian or Alaska Native patients, 3.7% Asian patients, 26.2% Black patients, 10.0% Hispanic or Latino patients, and 59.7% White patients; 29.5% admitted) and the BCH cohort included 22\u202f222 visits (mean [SD] age, 8.2 [6.8] years; 52.1% male patients; 0.1% American Indian or Alaska Native patients, 4.0% Asian patients, 19.7% Black patients, 30.6% Hispanic or Latino patients,...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12123471"
    },
    {
      "pmid": "40445905",
      "title": "Towards robust medical machine olfaction: Debiasing GC-MS data enhances prostate cancer diagnosis from urine volatiles.",
      "abstract": "Prostate cancer (PCa) is a major, and increasingly global, health concern with current screening and diagnostic tools' severe limitations causing unnecessary, invasive biopsy procedures. While gas chromatography-mass spectrometry (GC-MS) has been used to detect urinary volatile organic compounds (VOCs) associated with PCa, efforts to identify consistent molecular biomarkers have failed to generalize across studies. Inspired by the olfactory diagnostic capabilities of medical detection dogs, we do not reduce chromatograms to a list of compounds and concentrations. Instead, we deploy a machine learning approach that bypasses molecular identification: PCa \"scent character\" signatures are extracted from raw time series data transformed into image representations for classification via convolutional neural networks. To address confounding factors such as sample-source bias, we implement a multi-step pre-processing and debiasing pipeline, including empirical Bayes correction, baseline drift removal, and domain adversarial learning. The resulting model achieves classification performance on par with similarly trained canines, achieving a recall of 88% and an F1-score of 0.78. These findings demonstrate that, at least in the context of PCa detection from urine, machine learning-based scent signature analysis can serve as a fully non-invasive diagnostic alternative, with these early results being also relevant to the wider emergent field of medical machine olfaction.",
      "journal": "PloS one",
      "year": "2025",
      "doi": "10.1371/journal.pone.0314742",
      "authors": "Rotteveel Adan et al.",
      "keywords": "",
      "mesh_terms": "Male; Prostatic Neoplasms; Volatile Organic Compounds; Gas Chromatography-Mass Spectrometry; Humans; Machine Learning; Dogs; Animals; Smell; Bayes Theorem; Neural Networks, Computer",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40445905/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Neural Network; Generative AI",
      "health_domain": "Oncology; Pathology",
      "bias_axes": "Gender/Sex; Age; Intersectional",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Adversarial Debiasing",
      "clinical_setting": "Not specified",
      "key_findings": "The resulting model achieves classification performance on par with similarly trained canines, achieving a recall of 88% and an F1-score of 0.78. These findings demonstrate that, at least in the context of PCa detection from urine, machine learning-based scent signature analysis can serve as a fully non-invasive diagnostic alternative, with these early results being also relevant to the wider emergent field of medical machine olfaction.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12124533"
    },
    {
      "pmid": "40458639",
      "title": "Ethical framework for responsible foundational models in medical imaging.",
      "abstract": "The emergence of foundational models represents a paradigm shift in medical imaging, offering extraordinary capabilities in disease detection, diagnosis, and treatment planning. These large-scale artificial intelligence systems, trained on extensive multimodal and multi-center datasets, demonstrate remarkable versatility across diverse medical applications. However, their integration into clinical practice presents complex ethical challenges that extend beyond technical performance metrics. This study examines the critical ethical considerations at the intersection of healthcare and artificial intelligence. Patient data privacy remains a fundamental concern, particularly given these models' requirement for extensive training data and their potential to inadvertently memorize sensitive information. Algorithmic bias poses a significant challenge in healthcare, as historical disparities in medical data collection may perpetuate or exacerbate existing healthcare inequities across demographic groups. The complexity of foundational models presents significant challenges regarding transparency and explainability in medical decision-making. We propose a comprehensive ethical framework that addresses these challenges while promoting responsible innovation. This framework emphasizes robust privacy safeguards, systematic bias detection and mitigation strategies, and mechanisms for maintaining meaningful human oversight. By establishing clear guidelines for development and deployment, we aim to harness the transformative potential of foundational models while preserving the fundamental principles of medical ethics and patient-centered care.",
      "journal": "Frontiers in medicine",
      "year": "2025",
      "doi": "10.3389/fmed.2025.1544501",
      "authors": "Jha Debesh et al.",
      "keywords": "ethical AI; fairness; foundational models; medical imaging; responsible AI",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40458639/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "This framework emphasizes robust privacy safeguards, systematic bias detection and mitigation strategies, and mechanisms for maintaining meaningful human oversight. By establishing clear guidelines for development and deployment, we aim to harness the transformative potential of foundational models while preserving the fundamental principles of medical ethics and patient-centered care.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12128638"
    },
    {
      "pmid": "40463575",
      "title": "Leveraging neighborhood-level Information to Improve Model Fairness in Predicting Prenatal Depression.",
      "abstract": "IMPORTANCE: Perinatal depression (PND) affects 10-20% of pregnant women, with significant racial disparities in prevalence, screening, and treatment. Neighborhood-level factors significantly influence PND risk, particularly among women of color, but current machine learning models using electronic medical records (EMRs) rarely incorporate neighborhood characteristics. OBJECTIVE: To determine whether integrating neighborhood-level information with EMRs improves fairness in PND prediction while identifying key neighborhood factors influencing model bias across racial/ethnic groups. DESIGN SETTING AND PARTICIPANTS: Study of 6,137 pregnant women who received care at a large urban academic hospital from 2010-2019, comprising 58% Non-Hispanic Black (NHB), 10% Non-Hispanic White (NHW), and 28% Hispanic (H) individuals, with depression status determined by PHQ-9 scores. EXPOSURES: 125 neighborhood-level factors from Chicago Health Atlas merged with 61 EMR features based on residential location. MAIN OUTCOMES AND MEASURES: Model performance (ROCAUC, PRAUC) and fairness metrics (disparate impact, equal opportunity difference, equalized odds). Feature importance analyzed using Shapley values and the impact of each neighborhood factor on model bias were evaluated. Results Models integrating neighborhood-level measures showed moderate predictive performance (ROCAUC: NHB 55%, NHW 57%, H 58%) while significantly improving fairness metrics compared to EMR-only models (p<0.05). Factors, such as suicide mortality rate and neighborhood safety rate, helped reduce bias. NHB women showed stronger correlations between PND risk factors and neighborhood variables compared to other groups. Most neighborhood factors had differential impacts across racial/ethnic groups, increasing bias for NHB women while reducing it for Hispanic women. CONCLUSIONS AND RELEVANCE: Incorporating neighborhood-level information enhances fairness in PND prediction while maintaining predictive capability. The differential impact of neighborhood factors across racial/ethnic groups highlights the importance of considering neighborhood context in clinical risk assessment to reduce disparities in prenatal depression care.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.05.12.25327329",
      "authors": "Huang Yongchao et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40463575/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry; ICU/Critical Care; EHR/Health Informatics; Obstetrics/Maternal Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "Hospital/Inpatient; ICU",
      "key_findings": "CONCLUSIONS AND RELEVANCE: Incorporating neighborhood-level information enhances fairness in PND prediction while maintaining predictive capability. The differential impact of neighborhood factors across racial/ethnic groups highlights the importance of considering neighborhood context in clinical risk assessment to reduce disparities in prenatal depression care.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12132113"
    },
    {
      "pmid": "40467886",
      "title": "Racial bias in AI-mediated psychiatric diagnosis and treatment: a qualitative comparison of four large language models.",
      "abstract": "Artificial intelligence (AI), particularly large language models (LLMs), is increasingly integrated into mental health care. This study examined racial bias in psychiatric diagnosis and treatment across four leading LLMs: Claude, ChatGPT, Gemini, and NewMes-15 (a local, medical-focused LLaMA 3 variant). Ten psychiatric patient cases representing five diagnoses were presented to these models under three conditions: race-neutral, race-implied, and race-explicitly stated (i.e., stating patient is African American). The models' diagnostic recommendations and treatment plans were qualitatively evaluated by a clinical psychologist and a social psychologist, who scored 120 outputs for bias by comparing responses generated under race-neutral, race-implied, and race-explicit conditions. Results indicated that LLMs often proposed inferior treatments when patient race was explicitly or implicitly indicated, though diagnostic decisions demonstrated minimal bias. NewMes-15 exhibited the highest degree of racial bias, while Gemini showed the least. These findings underscore critical concerns about the potential for AI to perpetuate racial disparities in mental healthcare, emphasizing the necessity of rigorous bias assessment in algorithmic medical decision support systems.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01746-4",
      "authors": "Bouguettaya Ayoub et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40467886/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM; Clinical Decision Support",
      "health_domain": "Mental Health/Psychiatry; ICU/Critical Care; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "NewMes-15 exhibited the highest degree of racial bias, while Gemini showed the least. These findings underscore critical concerns about the potential for AI to perpetuate racial disparities in mental healthcare, emphasizing the necessity of rigorous bias assessment in algorithmic medical decision support systems.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12137607"
    },
    {
      "pmid": "40472465",
      "title": "Mitigating medical dataset bias by learning adaptive agreement from a biased council.",
      "abstract": "Dataset bias in images is an important yet less explored topic in medical images. Deep learning could be prone to learning spurious correlation raised by dataset bias, resulting in inaccurate, unreliable, and unfair models, which impedes its adoption in real-world clinical applications. Despite its significance, there is a dearth of research in the medical image classification domain to address dataset bias. Furthermore, the bias labels are often agnostic, as identifying biases can be laborious and depend on post-hoc interpretation. This paper proposes learning Adaptive Agreement from a Biased Council (Ada-ABC), a debiasing framework that does not rely on explicit bias labels to tackle dataset bias in medical images. Ada-ABC develops a biased council consisting of multiple classifiers optimized with generalized cross entropy loss to learn the dataset bias. A debiasing model is then simultaneously trained under the guidance of the biased council. Specifically, the debiasing model is required to learn adaptive agreement with the biased council by agreeing on the correctly predicted samples and disagreeing on the wrongly predicted samples by the biased council. In this way, the debiasing model could learn the target attribute on the samples without spurious correlations while also avoiding ignoring the rich information in samples with spurious correlations. We theoretically demonstrated that the debiasing model could learn the target features when the biased model successfully captures dataset bias. Moreover, we constructed the first medical debiasing benchmark focusing on addressing spurious correlation from four datasets containing seven different bias scenarios. Our extensive experiments practically showed that our proposed Ada-ABC outperformed competitive approaches, verifying its effectiveness in mitigating dataset bias for medical image classification. The codes and organized benchmark datasets can be accessed via https://github.com/LLYXC/Ada-ABC.",
      "journal": "Medical image analysis",
      "year": "2025",
      "doi": "10.1016/j.media.2025.103629",
      "authors": "Luo Luyang et al.",
      "keywords": "Dataset bias; Shortcut learning; Trustworthy artificial intelligence",
      "mesh_terms": "Humans; Deep Learning; Bias; Datasets as Topic; Algorithms; Image Interpretation, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40472465/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Computer Vision/Imaging AI; Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Post-hoc Correction",
      "clinical_setting": "Not specified",
      "key_findings": "Our extensive experiments practically showed that our proposed Ada-ABC outperformed competitive approaches, verifying its effectiveness in mitigating dataset bias for medical image classification. The codes and organized benchmark datasets can be accessed via https://github.com/LLYXC/Ada-ABC.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40473916",
      "title": "Identifying and mitigating algorithmic bias in the safety net.",
      "abstract": "Algorithmic bias occurs when predictive model performance varies meaningfully across sociodemographic classes, exacerbating systemic healthcare disparities. NYC Health + Hospitals, an urban safety net system, assessed bias in two binary classification models in our electronic medical record: one predicting acute visits for asthma and one predicting unplanned readmissions. We evaluated differences in subgroup performance across race/ethnicity, sex, language, and insurance using equal opportunity difference (EOD), a metric comparing false negative rates. The most biased classes (race/ethnicity for asthma, insurance for readmission) were targeted for mitigation using threshold adjustment, which adjusts subgroup thresholds to minimize EOD, and reject option classification, which re-classifies scores near the threshold by subgroup. Successful mitigation was defined as 1) absolute subgroup EODs <5 percentage points, 2) accuracy reduction <10%, and 3) alert rate change <20%. Threshold adjustment met these criteria; reject option classification did not. We introduce a Supplementary Playbook outlining our approach for low-resource bias mitigation.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01732-w",
      "authors": "Mackin Shaina et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40473916/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Language; Geographic; Insurance Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment; Fairness Metrics Evaluation; Post-hoc Correction",
      "clinical_setting": "Hospital/Inpatient; Safety-Net/Underserved",
      "key_findings": "Threshold adjustment met these criteria; reject option classification did not. We introduce a Supplementary Playbook outlining our approach for low-resource bias mitigation.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12141433"
    },
    {
      "pmid": "40479961",
      "title": "Multi-instance curriculum learning for histopathology image classification with bias reduction.",
      "abstract": "Multi-instance learning (MIL) exhibits advanced and surpassed capabilities in understanding and recognizing complex patterns within gigapixel histopathological images. However, current MIL methods for the analysis of the histopathological images still give rise to two main concerns. On one hand, vanilla MIL methods intuitively focus on identifying salient instances (easy-to-classify instances) without considering hard-to-classify instances, which is biased and prone to produce false positive instances. On the other hand, since the positive tissue occupies only a small fraction of histopathological images, it is commonly suffer from class imbalance between positive and negative instances, causing the MIL model to overly focus on the majority class in training instances classifier. In light of these issues of bias learning, we propose a multi-instance curriculum learning method that collaboratively incorporates hard negative instance mining and positive instance augmentation to improve classification performance of the model. Specifically, we first initialize the MIL model using easy-to-classify instances, then we mine the hard negative instances (hard-to-classify instances) and augment the positive instances via the diffusion model. Finally, the MIL model is retrained with memory rehearsal method by combining the mined negative instances and the augmented positive instances. Technically, the diffusion model is first designed to generate lesion instances, which optimally augment diverse features to reflect realistic positive samples with post screening scenario. Extensive experimental results show that the proposed method alleviates model bias in MIL and improves model interpretability.",
      "journal": "Medical image analysis",
      "year": "2025",
      "doi": "10.1016/j.media.2025.103647",
      "authors": "Mi Zihao et al.",
      "keywords": "Curriculum learning; Diffusion model; Hard negative instance mining; Histopathology image; Multi-instance learning; Positive instance augmentation",
      "mesh_terms": "Humans; Image Interpretation, Computer-Assisted; Machine Learning; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40479961/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "ICU/Critical Care; Pathology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Laboratory/Pathology",
      "key_findings": "Technically, the diffusion model is first designed to generate lesion instances, which optimally augment diverse features to reflect realistic positive samples with post screening scenario. Extensive experimental results show that the proposed method alleviates model bias in MIL and improves model interpretability.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40487862",
      "title": "Gender Disparities in Artificial Intelligence-Generated Images of Hospital Leadership in the United States.",
      "abstract": "OBJECTIVE: To evaluate demographic representation in artificial intelligence (AI)-generated images of hospital leadership roles and compare them with real-world data from US hospitals. PATIENTS AND METHODS: This cross-sectional study, conducted from October 1, 2024 to October 31, 2024, analyzed images generated by 3 AI text-to-image models: Midjourney 6.0, OpenAI ChatGPT DALL-E 3, and Google Gemini Imagen 3. Standardized prompts were used to create 1200 images representing 4 key leadership roles: chief executive officers, chief medical officers, chief nursing officers, and chief financial officers. Real-world demographic data from 4397 US hospitals showed that chief executive officers were 73.2% men; chief financial officers, 65.2% men; chief medical officers, 85.7% men; and chief nursing officers, 9.4% men (overall: 60.1% men). The primary outcome was gender representation, with secondary outcomes including race/ethnicity and age. Two independent reviewers assessed images, with interrater reliability evaluated using Cohen \u03ba. RESULTS: Interrater agreement was high for gender (\u03ba=0.998) and moderate for race/ethnicity (\u03ba=0.670) and age (\u03ba=0.605). DALL-E overrepresented men (86.5%) and White individuals (94.5%). Midjourney showed improved gender balance (69.5% men) but overrepresented White individuals (75.0%). Imagen achieved near gender parity (50.3% men) but remained predominantly White (51.5%). Statistically significant differences were observed across models and between models and real-world demographics. CONCLUSION: Artificial intelligence text-to-image models reflect and amplify systemic biases, overrepresenting men and White leaders, while underrepresenting diversity. Ethical AI practices, including diverse training data sets and fairness-aware algorithms, are essential to ensure equitable representation in health care leadership.",
      "journal": "Mayo Clinic proceedings. Digital health",
      "year": "2025",
      "doi": "10.1016/j.mcpdig.2025.100218",
      "authors": "Gisselbaek Mia et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40487862/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "CONCLUSION: Artificial intelligence text-to-image models reflect and amplify systemic biases, overrepresenting men and White leaders, while underrepresenting diversity. Ethical AI practices, including diverse training data sets and fairness-aware algorithms, are essential to ensure equitable representation in health care leadership.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12140938"
    },
    {
      "pmid": "40495929",
      "title": "Evaluating artificial intelligence bias in nephrology: the role of diversity, equity, and inclusion in AI-driven decision-making and ethical regulation.",
      "abstract": "BACKGROUND: The integration of Artificial Intelligence (AI) in nephrology has raised concerns regarding bias, fairness, and ethical decision-making, particularly in the context of Diversity, Equity, and Inclusion (DEI). AI-driven models, including Large Language Models (LLMs) like ChatGPT, may unintentionally reinforce existing disparities in patient care and workforce recruitment. This study investigates how AI models (ChatGPT 3.5 and 4.0) handle DEI-related ethical considerations in nephrology, highlighting the need for improved regulatory oversight to ensure equitable AI deployment. METHODS: The study was conducted in March 2024 using ChatGPT 3.5 and 4.0. Eighty simulated cases were developed to assess ChatGPT's decision-making across diverse nephrology topics. ChatGPT was instructed to respond to questions considering factors such as age, sex, gender identity, race, ethnicity, religion, cultural beliefs, socioeconomic status, education level, family structure, employment, insurance, geographic location, disability, mental health, language proficiency, and technology access. RESULTS: ChatGPT 3.5 provided a response to all scenario questions and did not refuse to make decisions under any circumstances. This contradicts the essential DEI principle of avoiding decisions based on potentially discriminatory criteria. In contrast, ChatGPT 4.0 declined to make decisions based on potentially discriminatory criteria in 13 (16.3%) scenarios during the first round and in 5 (6.3%) during the second round. CONCLUSION: While ChatGPT 4.0 shows improvement in ethical AI decision-making, its limited recognition of bias and DEI considerations underscores the need for robust AI regulatory frameworks in nephrology. AI governance must incorporate structured DEI guidelines, ongoing bias detection mechanisms, and ethical oversight to prevent AI-driven disparities in clinical practice and workforce recruitment. This study emphasizes the importance of transparency, fairness, and inclusivity in AI development, calling for collaborative efforts between AI developers, nephrologists, policymakers, and patient communities to ensure AI serves as an equitable tool in nephrology.",
      "journal": "Frontiers in artificial intelligence",
      "year": "2025",
      "doi": "10.3389/frai.2025.1525937",
      "authors": "Balakrishnan Suryanarayanan et al.",
      "keywords": "ChatGPT; artificial intelligence; bias detection; clinical implications; decision-making; diversity, equity, and inclusion; ethical AI regulation; nephrology",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40495929/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Mental Health/Psychiatry; ICU/Critical Care; Nephrology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Language; Disability; Geographic; Insurance Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: While ChatGPT 4.0 shows improvement in ethical AI decision-making, its limited recognition of bias and DEI considerations underscores the need for robust AI regulatory frameworks in nephrology. AI governance must incorporate structured DEI guidelines, ongoing bias detection mechanisms, and ethical oversight to prevent AI-driven disparities in clinical practice and workforce recruitment. This study emphasizes the importance of transparency, fairness, and inclusivity in AI development,...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12150873"
    },
    {
      "pmid": "40502572",
      "title": "Evaluating the performance and potential bias of predictive models for detection of transthyretin cardiac amyloidosis.",
      "abstract": "BACKGROUND: Delays in the diagnosis of transthyretin amyloid cardiomyopathy (ATTR-CM) contribute to the significant morbidity of the condition, especially in the era of disease-modifying therapies. Screening for ATTR-CM with AI and other algorithms may improve timely diagnosis, but these algorithms have not been directly compared. OBJECTIVES: The aim of this study was to compare the performance of four algorithms for ATTR-CM detection in a heart failure population and assess the risk for harms due to model bias. METHODS: We identified patients in an integrated health system from 2010-2022 with ATTR-CM and age- and sex-matched them to controls with heart failure to target 5% prevalence. We compared the performance of a claims-based random forest model (Huda et al. model), a regression-based score (Mayo ATTR-CM), and two deep learning echo models (EchoNet-LVH and EchoGo \u00ae Amyloidosis). We evaluated for bias using standard fairness metrics. RESULTS: The analytical cohort included 176 confirmed cases of ATTR-CM and 3192 control patients with 79.2% self-identified as White and 9.0% as Black. The Huda et al. model performed poorly (AUC 0.49). Both deep learning echo models had a higher AUC when compared to the Mayo ATTR-CM Score (EchoNet-LVH 0.88; EchoGo Amyloidosis 0.92; Mayo ATTR-CM Score 0.79; DeLong P<0.001 for both). Bias auditing met fairness criteria for equal opportunity among patients who identified as Black. CONCLUSIONS: Deep learning, echo-based models to detect ATTR-CM demonstrated best overall discrimination when compared to two other models in external validation with low risk of harms due to racial bias.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2024.10.09.24315202",
      "authors": "Hourmozdi Jonathan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40502572/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Random Forest; Clinical Prediction Model",
      "health_domain": "Cardiology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation; Bias Auditing Framework",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: Deep learning, echo-based models to detect ATTR-CM demonstrated best overall discrimination when compared to two other models in external validation with low risk of harms due to racial bias.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12155028"
    },
    {
      "pmid": "40514386",
      "title": "Uncovering ethical biases in publicly available fetal ultrasound datasets.",
      "abstract": "We explore biases present in publicly available fetal ultrasound (US) imaging datasets, currently at the disposal of researchers to train deep learning (DL) algorithms for prenatal diagnostics. As DL increasingly permeates the field of medical imaging, the urgency to critically evaluate the fairness of benchmark public datasets used to train them grows. Our thorough investigation reveals a multifaceted bias problem, encompassing issues such as lack of demographic representativeness, limited diversity in clinical conditions depicted, and variability in US technology used across datasets. We argue that these biases may significantly influence DL model performance, which may lead to inequities in healthcare outcomes. To address these challenges, we recommend a multilayered approach. This includes promoting practices that ensure data inclusivity, such as diversifying data sources and populations, and refining model strategies to better account for population variances. These steps will enhance the trustworthiness of DL algorithms in fetal US analysis.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01739-3",
      "authors": "Fiorentino Maria Chiara et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40514386/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Obstetrics/Maternal Health",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "This includes promoting practices that ensure data inclusivity, such as diversifying data sources and populations, and refining model strategies to better account for population variances. These steps will enhance the trustworthiness of DL algorithms in fetal US analysis.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12166052"
    },
    {
      "pmid": "40517148",
      "title": "A scoping review and evidence gap analysis of clinical AI fairness.",
      "abstract": "The ethical integration of artificial intelligence (AI) in healthcare necessitates addressing fairness. AI fairness involves mitigating biases in AI and leveraging AI to promote equity. Despite advancements, significant disconnects persist between technical solutions and clinical applications. Through evidence gap analysis, this review systematically pinpoints the gaps at the intersection of healthcare contexts-including medical fields, healthcare datasets, and bias-relevant attributes (e.g., gender/sex)-and AI fairness techniques for bias detection, evaluation, and mitigation. We highlight the scarcity of AI fairness research in medical domains, the narrow focus on bias-relevant attributes, the dominance of group fairness centering on model performance equality, and the limited integration of clinician-in-the-loop to improve AI fairness. To bridge the gaps, we propose actionable strategies for future research to accelerate the development of AI fairness in healthcare, ultimately advancing equitable healthcare delivery.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01667-2",
      "authors": "Liu Mingxuan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40517148/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We highlight the scarcity of AI fairness research in medical domains, the narrow focus on bias-relevant attributes, the dominance of group fairness centering on model performance equality, and the limited integration of clinician-in-the-loop to improve AI fairness. To bridge the gaps, we propose actionable strategies for future research to accelerate the development of AI fairness in healthcare, ultimately advancing equitable healthcare delivery.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12167363"
    },
    {
      "pmid": "40563902",
      "title": "AI-Driven Transcriptome Prediction in Human Pathology: From Molecular Insights to Clinical Applications.",
      "abstract": "Gene expression regulation underpins cellular function and disease progression, yet its complexity and the limitations of conventional detection methods hinder clinical translation. In this review, we define \"predict\" as the AI-driven inference of gene expression levels and regulatory mechanisms from non-invasive multimodal data (e.g., histopathology images, genomic sequences, and electronic health records) instead of direct molecular assays. We systematically examine and analyze the current approaches for predicting gene expression and diagnosing diseases, highlighting their respective advantages and limitations. Machine learning algorithms and deep learning models excel in extracting meaningful features from diverse biomedical modalities, enabling tools like PathChat and Prov-GigaPath to improve cancer subtyping, therapy response prediction, and biomarker discovery. Despite significant progress, persistent challenges-such as data heterogeneity, noise, and ethical issues including privacy and algorithmic bias-still limit broad clinical adoption. Emerging solutions like cross-modal pretraining frameworks, federated learning, and fairness-aware model design aim to overcome these barriers. Case studies in precision oncology illustrate AI's ability to decode tumor ecosystems and predict treatment outcomes. By harmonizing multimodal data and advancing ethical AI practices, this field holds immense potential to propel personalized medicine forward, although further innovation is needed to address the issues of scalability, interpretability, and equitable deployment.",
      "journal": "Biology",
      "year": "2025",
      "doi": "10.3390/biology14060651",
      "authors": "Chen Xiaoya et al.",
      "keywords": "artificial intelligence; deep learning; multimodal data fusion; precision medicine; transcriptome prediction",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40563902/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Federated Learning",
      "health_domain": "Oncology; EHR/Health Informatics; Pathology; Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Federated Learning; Explainability/Interpretability",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "Case studies in precision oncology illustrate AI's ability to decode tumor ecosystems and predict treatment outcomes. By harmonizing multimodal data and advancing ethical AI practices, this field holds immense potential to propel personalized medicine forward, although further innovation is needed to address the issues of scalability, interpretability, and equitable deployment.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12189417"
    },
    {
      "pmid": "40564772",
      "title": "Risk of Bias Assessment of Diagnostic Accuracy Studies Using QUADAS 2 by Large Language Models.",
      "abstract": "Background/Objectives: Diagnostic accuracy studies are essential for the evaluation of the performance of medical tests. The risk of bias (RoB) for these studies is commonly assessed using the Quality Assessment of Diagnostic Accuracy Studies (QUADAS) tool. This study aimed to assess the capabilities and reasoning accuracy of large language models (LLMs) in evaluating the RoB in diagnostic accuracy studies, using QUADAS 2, compared to human experts. Methods: Four LLMs were used for the AI assessment: ChatGPT 4o model, X.AI Grok 3 model, Gemini 2.0 flash model, and DeepSeek V3 model. Ten recent open-access diagnostic accuracy studies were selected. Each article was independently assessed by human experts and by LLMs using QUADAS 2. Results: Out of 110 signaling questions assessments (11 questions for each of the 10 articles) by the four AI models, and the mean percentage of correct assessments of all the models was 72.95%. The most accurate model was Grok 3, followed by ChatGPT 4o, DeepSeek V3, and Gemini 2.0 Flash, with accuracies ranging from 74.45% to 67.27%. When analyzed by domain, the most accurate responses were for \"flow and timing\", followed by \"index test\", and then similarly for \"patient selection\" and \"reference standard\". An extensive list of reasoning errors was documented. Conclusions: This study demonstrates that LLMs can achieve a moderate level of accuracy in evaluating the RoB in diagnostic accuracy studies. However, they are not yet a substitute for expert clinical and methodological judgment. LLMs may serve as complementary tools in systematic reviews, with compulsory human supervision.",
      "journal": "Diagnostics (Basel, Switzerland)",
      "year": "2025",
      "doi": "10.3390/diagnostics15121451",
      "authors": "Leucu\u021ba Daniel-Corneliu et al.",
      "keywords": "artificial intelligence; diagnostic accuracy; evidence-based medicine; large language models; risk of bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40564772/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Conclusions: This study demonstrates that LLMs can achieve a moderate level of accuracy in evaluating the RoB in diagnostic accuracy studies. However, they are not yet a substitute for expert clinical and methodological judgment. LLMs may serve as complementary tools in systematic reviews, with compulsory human supervision.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12191753"
    },
    {
      "pmid": "40595988",
      "title": "Beyond accuracy: a framework for evaluating algorithmic bias and performance, applied to automated sleep scoring.",
      "abstract": "Recent advancements in artificial intelligence (AI) have significantly improved sleep-scoring algorithms, bringing their performance close to the theoretical limit of approximately 80%, which aligns with inter-scorer agreement levels. While this suggests the problem is technically solved, clinical adoption remains challenging due to ethical and regulatory requirements for rigorous validation, fairness, and human oversight. Existing validation methods, such as Bland-Altman analysis, often rely on simple correlation metrics, overlooking potential non-linear influences of external factors (e.g., demographic or clinical variables) on systematic predictive errors (biases) in derived clinical markers. Additionally, performance metrics are typically reported as the mean of on-subject results, neglecting critical scenarios-such as different quantiles-that could better convey the algorithm's capabilities and limitations to clinicians as end-users. To address this gap, we propose a universal framework for quantifying both performance metrics and biases in predictive algorithmic tools. Our approach extends conventional validation methods by analyzing how external factors shape the entire distribution of predictive performance and errors, rather than just the expected mean. Applying it to the widely recognized U-Sleep and YASA sleep-scoring algorithms, we identify biases-such as age-related shifts-indicating missing input information or imbalances in training data. Despite these biases, we illustrate that both algorithms maintain non-inferior performance in the risk assessment of sleep apnea based on prediction-derived markers, highlighting the potential and clinical utility of algorithmic insights.",
      "journal": "Scientific reports",
      "year": "2025",
      "doi": "10.1038/s41598-025-06019-4",
      "authors": "Bechny Michal et al.",
      "keywords": "",
      "mesh_terms": "Humans; Algorithms; Polysomnography; Sleep; Artificial Intelligence; Male; Female; Adult; Middle Aged; Bias; Reproducibility of Results",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40595988/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Applying it to the widely recognized U-Sleep and YASA sleep-scoring algorithms, we identify biases-such as age-related shifts-indicating missing input information or imbalances in training data. Despite these biases, we illustrate that both algorithms maintain non-inferior performance in the risk assessment of sleep apnea based on prediction-derived markers, highlighting the potential and clinical utility of algorithmic insights.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12215420"
    },
    {
      "pmid": "40599875",
      "title": "Privacy, ethics, transparency, and accountability in AI systems for wearable devices.",
      "abstract": "The integration of artificial intelligence (AI) and machine learning (ML) into wearable sensor technologies has substantially advanced health data science, enabling continuous monitoring, personalised interventions, and predictive analytics. However, the fast advancement of these technologies has raised critical ethical and regulatory concerns, particularly around data privacy, algorithmic bias, informed consent, and the opacity of automated decision-making. This study undertakes a systematic examination of these challenges, highlighting the risks posed by unregulated data aggregation, biased model training, and inadequate transparency in AI-powered health applications. Through an analysis of current privacy frameworks and empirical assessment of publicly available datasets, the study identifies significant disparities in model performance across demographic groups and exposes vulnerabilities in both technical design and ethical governance. To address these issues, this article introduces a data-driven methodological framework that embeds transparency, accountability, and regulatory alignment across all stages of AI development. The framework operationalises ethical principles through concrete mechanisms, including explainable AI, bias mitigation techniques, and consent-aware data processing pipelines, while aligning with legal standards such as the GDPR, the UK Data Protection Act, and the EU AI Act. By incorporating transparency as a structural and procedural requirement, the framework presented in this article offers a replicable model for the responsible development of AI systems in wearable healthcare. In doing so, the study advocates for a regulatory paradigm that balances technological innovation with the protection of individual rights, fostering fair, secure, and trustworthy AI-driven health monitoring.",
      "journal": "Frontiers in digital health",
      "year": "2025",
      "doi": "10.3389/fdgth.2025.1431246",
      "authors": "Radanliev Petar",
      "keywords": "artificial intelligence; data privacy; digital identity systems; ethical considerations; health data science; machine learning; wearable technology",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40599875/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Wearables/Remote Monitoring",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Telehealth/Remote",
      "key_findings": "By incorporating transparency as a structural and procedural requirement, the framework presented in this article offers a replicable model for the responsible development of AI systems in wearable healthcare. In doing so, the study advocates for a regulatory paradigm that balances technological innovation with the protection of individual rights, fostering fair, secure, and trustworthy AI-driven health monitoring.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12209263"
    },
    {
      "pmid": "40615561",
      "title": "Assessing risk of bias in toxicological studies in the era of artificial intelligence.",
      "abstract": "Risk of bias is a critical factor influencing the reliability and validity of toxicological studies, impacting evidence synthesis and decision-making in regulatory and public health contexts. The traditional approaches for assessing risk of bias are often subjective and time-consuming. Recent advancements in artificial intelligence (AI) offer promising solutions for automating and enhancing bias detection and evaluation. This article reviews key types of biases-such as selection, performance, detection, attrition, and reporting biases-in in vivo, in vitro, and in silico studies. It further discusses specialized tools, including the SYRCLE and OHAT frameworks, designed to address such biases. The integration of AI-based tools into risk of bias assessments can significantly improve the efficiency, consistency, and accuracy of evaluations. However, AI models are themselves susceptible to algorithmic and data biases, necessitating robust validation and transparency in their development. The article highlights the need for standardized, AI-enabled risk of bias assessment methodologies, training, and policy implementation to mitigate biases in AI-driven analyses. The strategies for leveraging AI to screen studies, detect anomalies, and support systematic reviews are explored. By adopting these advanced methodologies, toxicologists and regulators can enhance the quality and reliability of toxicological evidence, promoting evidence-based practices and ensuring more informed decision-making. The way forward includes fostering interdisciplinary collaboration, developing bias-resilient AI models, and creating a research culture that actively addresses bias through transparent and rigorous practices.",
      "journal": "Archives of toxicology",
      "year": "2025",
      "doi": "10.1007/s00204-025-03978-5",
      "authors": "Hartung Thomas et al.",
      "keywords": "AI bias; Artificial intelligence; Evidence-based toxicology; OHAT; Regulatory toxicology; Risk of bias; SYRCLE; Systematic review; ToxRTool; Toxicology",
      "mesh_terms": "Artificial Intelligence; Humans; Bias; Toxicology; Animals; Toxicity Tests; Reproducibility of Results; Risk Assessment; Research Design",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40615561/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "By adopting these advanced methodologies, toxicologists and regulators can enhance the quality and reliability of toxicological evidence, promoting evidence-based practices and ensuring more informed decision-making. The way forward includes fostering interdisciplinary collaboration, developing bias-resilient AI models, and creating a research culture that actively addresses bias through transparent and rigorous practices.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12367879"
    },
    {
      "pmid": "40616933",
      "title": "Evaluating the Performance and Potential Bias of Predictive Models for Detection of Transthyretin Cardiac Amyloidosis.",
      "abstract": "BACKGROUND: Delays in the diagnosis of transthyretin amyloid cardiomyopathy (ATTR-CM) contribute to the significant morbidity of the condition, especially in the era of disease-modifying therapies. Screening for ATTR-CM with artificial intelligence and other algorithms may improve timely diagnosis, but these algorithms have not been directly compared. OBJECTIVES: The aim of this study was to compare the performance of 4 algorithms for ATTR-CM detection in a heart failure population and assess the risk for harms due to model bias. METHODS: We identified patients in an integrated health system from 2010 to 2022 with ATTR-CM and age- and sex-matched them to controls with heart failure to target 5% prevalence. We compared the performance of a claims-based random forest model (Huda et al model), a regression-based score (Mayo ATTR-CM), and 2 deep learning echo models (EchoNet-LVH and EchoGo Amyloidosis). We evaluated for bias using standard fairness metrics. RESULTS: The analytical cohort included 176 confirmed cases of ATTR-CM and 3,192 control patients with 79.2% self-identified as White and 9.0% as Black. The Huda et al model performed poorly (AUC: 0.49). Both deep learning echo models had a higher AUC when compared to the Mayo ATTR-CM Score (EchoNet-LVH 0.88; EchoGo Amyloidosis 0.92; Mayo ATTR-CM Score 0.79; DeLong P < 0.001 for both). Bias auditing met fairness criteria for equal opportunity among patients who identified as Black. CONCLUSIONS: Deep learning, echo-based models to detect ATTR-CM demonstrated best overall discrimination when compared to 2 other models in external validation with low risk of harms due to racial bias.",
      "journal": "JACC. Advances",
      "year": "2025",
      "doi": "10.1016/j.jacadv.2025.101901",
      "authors": "Hourmozdi Jonathan et al.",
      "keywords": "amyloidosis; artificial intelligence; health care disparities; heart failure; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40616933/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Random Forest; Clinical Prediction Model",
      "health_domain": "Cardiology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation; Bias Auditing Framework",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: Deep learning, echo-based models to detect ATTR-CM demonstrated best overall discrimination when compared to 2 other models in external validation with low risk of harms due to racial bias.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12272418"
    },
    {
      "pmid": "40624414",
      "title": "Fairness of machine learning readmission predictions following open ventral hernia repair.",
      "abstract": "INTRODUCTION: Few models have predicted readmission following open ventral hernia repair (VHR), and none have assessed fairness. Fairness evaluation assesses whether predictive performance is similar across demographic groups, ensuring that biases are not propagated. Therefore, we generated an interpretable machine learning model to predict readmission following open VHR while assessing fairness. METHODS: NSQIP (2018-2021) was queried for open VHR. We developed an XGBoost model to predict unplanned readmissions within 30\u00a0days of surgery with fivefold cross-validation. Performance and fairness were assessed by demographic groups: gender (female vs. male), ethnicity (Hispanic vs. non-Hispanic), and race (non-White vs. White). We identified influential features within demographic groups using SHapley Additive exPlanations (SHAP). RESULTS: 59,482 patients were included with a readmission rate of 5.5%. The model had an AUC of 0.72 and a Brier score of 0.16. Fairness metrics revealed minimal performance differences between demographic groups. SHAP revealed that influential factors were similar across demographic groups and included days from operation to discharge, morbidity probability, and operative time. CONCLUSION: Using interpretable machine learning, we identified unique predictors for unplanned readmission following open VHR. Fairness metrics revealed minimal differences in performance between demographic groups. SHAP showed similar influential factors across demographic groups. Future surgical machine learning models should similarly assess models using fairness metrics and interpretation of predictions.",
      "journal": "Surgical endoscopy",
      "year": "2025",
      "doi": "10.1007/s00464-025-11927-7",
      "authors": "Zander Tyler et al.",
      "keywords": "Algorithmic bias; Fairness; Interpretable machine learning; Risk prediction",
      "mesh_terms": "Humans; Machine Learning; Patient Readmission; Hernia, Ventral; Female; Male; Herniorrhaphy; Middle Aged; Aged; Postoperative Complications; Retrospective Studies; Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40624414/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "XGBoost/Gradient Boosting",
      "health_domain": "Surgery",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Using interpretable machine learning, we identified unique predictors for unplanned readmission following open VHR. Fairness metrics revealed minimal differences in performance between demographic groups. SHAP showed similar influential factors across demographic groups.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12345327"
    },
    {
      "pmid": "40640864",
      "title": "Benchmarking bias in embeddings of healthcare AI models: using SD-WEAT for detection and measurement across sensitive populations.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) has been shown to exhibit and perpetuate human biases; recent research efforts have focused on measuring bias within the input embeddings of AI language models, especially with non-binary classifications that are common in medicine and healthcare scenarios. For instance, ethnicity-linked terms might include categories such as Asian, Black, Hispanic, and White, complicating the definition of \u2013 traditionally binary \u2013 attribute groups. In this study, we aimed to develop a new framework to detect and measure inherent medical biases based on SD-WEAT (Standard Deviation - Word Embedding Association Test). Compared to its predecessor, WEAT, SD-WEAT was able to measure bias among multi-level attribute groups common in the field of medicine, such as age, race, and region. METHODS: We constructed a collection of medicine-based benchmarks that can be used to detect and measure biases among sex, ethnicities, and medical conditions. Then, we evaluated a collection of language models, including GloVe, BERT, LegalBERT, BioBERT, GPT-2, and BioGPT, and determined which had potential undesirable or desirable healthcare biases. RESULTS: With the presented framework, we were able to detect and measure a significant presence of bias among gender-linked (P\u2009<\u20090.01) and ethnicity-linked (P\u2009<\u20090.01) medical conditions for a biomedicine-focused language model (e.g., BioBERT) compared to general BERT models. In addition, we demonstrated that SD-WEAT was capable of simultaneously handling multiple attribute groups, detecting and measuring bias among a collection of ethnicity-linked medical conditions and multiple ethnic/racial groups. CONCLUSIONS: To conclude, we presented an AI bias measurement framework, based on SD-WEAT. This framework provided a promising approach to detect and measure biases in language models that have been applied in biomedical/healthcare text analysis. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1186/s12911-025-03102-8.",
      "journal": "BMC medical informatics and decision making",
      "year": "2025",
      "doi": "10.1186/s12911-025-03102-8",
      "authors": "Gray Magnus et al.",
      "keywords": "Artificial intelligence; Bias; Bias measurement; Healthcare; Input embeddings; Language models; Medicine; Natural Language processing",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40640864/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Representation Learning",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: To conclude, we presented an AI bias measurement framework, based on SD-WEAT. This framework provided a promising approach to detect and measure biases in language models that have been applied in biomedical/healthcare text analysis. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1186/s12911-025-03102-8.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12247235"
    },
    {
      "pmid": "40644462",
      "title": "Raising awareness of potential biases in medical machine learning: Experience from a Datathon.",
      "abstract": "OBJECTIVE: To challenge clinicians and informaticians to learn about potential sources of bias in medical machine learning models through investigation of data and predictions from an open-source severity of illness score. METHODS: Over a two-day period (total elapsed time approximately 28 hours), we conducted a datathon that challenged interdisciplinary teams to investigate potential sources of bias in the Global Open Source Severity of Illness Score. Teams were invited to develop hypotheses, to use tools of their choosing to identify potential sources of bias, and to provide a final report. RESULTS: Five teams participated, three of which included both informaticians and clinicians. Most (4/5) used Python for analyses, the remaining team used R. Common analysis themes included relationship of the GOSSIS-1 prediction score with demographics and care related variables; relationships between demographics and outcomes; calibration and factors related to the context of care; and the impact of missingness. Representativeness of the population, differences in calibration and model performance among groups, and differences in performance across hospital settings were identified as possible sources of bias. DISCUSSION: Datathons are a promising approach for challenging developers and users to explore questions relating to unrecognized biases in medical machine learning algorithms.",
      "journal": "PLOS digital health",
      "year": "2025",
      "doi": "10.1371/journal.pdig.0000932",
      "authors": "Hochheiser Harry et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40644462/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Calibration",
      "clinical_setting": "Hospital/Inpatient; Public Health/Population",
      "key_findings": "RESULTS: Five teams participated, three of which included both informaticians and clinicians. Most (4/5) used Python for analyses, the remaining team used R. Common analysis themes included relationship of the GOSSIS-1 prediction score with demographics and care related variables; relationships between demographics and outcomes; calibration and factors related to the context of care; and the impact of missingness.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12250157"
    },
    {
      "pmid": "40646553",
      "title": "The ethics of data mining in healthcare: challenges, frameworks, and future directions.",
      "abstract": "Data mining in healthcare offers transformative insights yet surfaces multilayered ethical and governance challenges that extend beyond privacy alone. Privacy and consent concerns remain paramount when handling sensitive medical data, particularly as healthcare organizations increasingly share patient information with large digital platforms. The risks of data breaches and unauthorized access are stark: 725 reportable incidents in 2023 alone exposed more than 133\u00a0million patient records, and hacking-related breaches surged by 239% since 2018. Algorithmic bias further threatens equity; models trained on historically prejudiced data can reinforce health disparities across protected groups. Therefore, transparency must span three levels-dataset documentation, model interpretability, and post-deployment audit logging-to make algorithmic reasoning and failures traceable. Security vulnerabilities in the Internet of Medical Things (IoMT) and cloud-based health platforms amplify these risks, while corporate data-sharing deals complicate questions of data ownership and patient autonomy. A comprehensive response requires (i) dataset-level artifacts such as \"datasheets,\" (ii) model-cards that disclose fairness metrics, and (iii) continuous logging of predictions and LIME/SHAP explanations for independent audits. Technical safeguards must blend differential privacy (with empirically validated noise budgets), homomorphic encryption for high-value queries, and federated learning to maintain the locality of raw data. Governance frameworks must also mandate routine bias and robust audits and harmonized penalties for non-compliance. Regular reassessments, thorough documentation, and active engagement with clinicians, patients, and regulators are critical to accountability. This paper synthesizes current evidence, from a 2019 European re-identification study demonstrating 99.98% uniqueness with 15 quasi-identifiers to recent clinical audits that trimmed false-negative rates via threshold recalibration, and proposes an integrated set of fairness, privacy, and security controls aligned with SPIRIT-AI, CONSORT-AI, and emerging PROBAST-AI guidelines. Implementing these solutions will help healthcare systems harness the benefits of data mining while safeguarding patient rights and sustaining public trust.",
      "journal": "BioData mining",
      "year": "2025",
      "doi": "10.1186/s13040-025-00461-w",
      "authors": "Ahmed Mohamed Mustaf et al.",
      "keywords": "Algorithmic bias; Data mining; Data security; Healthcare ethics; Patient consent; Privacy",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40646553/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Federated Learning; Generative AI",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Calibration; Threshold Adjustment; Fairness Metrics Evaluation; Federated Learning; Explainability/Interpretability; Bias Auditing Framework",
      "clinical_setting": "ICU",
      "key_findings": "This paper synthesizes current evidence, from a 2019 European re-identification study demonstrating 99.98% uniqueness with 15 quasi-identifiers to recent clinical audits that trimmed false-negative rates via threshold recalibration, and proposes an integrated set of fairness, privacy, and security controls aligned with SPIRIT-AI, CONSORT-AI, and emerging PROBAST-AI guidelines. Implementing these solutions will help healthcare systems harness the benefits of data mining while safeguarding patient...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12255135"
    },
    {
      "pmid": "40658470",
      "title": "Deconfounded and debiased estimation for high-dimensional linear regression under hidden confounding with application to omics data.",
      "abstract": "MOTIVATION: A critical challenge in observational studies arises from the presence of hidden confounders in high-dimensional data. This leads to biases in causal effect estimation due to both hidden confounding and high-dimensional estimation. Some classical deconfounding methods are inadequate for high-dimensional scenarios and typically require prior information on hidden confounders. We propose a two-step deconfounded and debiased estimation for high-dimensional linear regression with hidden confounding. RESULTS: First, we reduce hidden confounding via spectral transformation. Second, we correct bias from the weighted \u21131 penalty, commonly used in high-dimensional estimation, by inverting the Karush-Kuhn-Tucker conditions and solving convex optimization programs. This deconfounding technique by spectral transformation requires no prior knowledge of hidden confounders. This novel debiasing approach improves over recent work by not assuming a sparse precision matrix, making it more suitable for cases with intrinsic covariate correlations. Simulations show that the proposed method corrects both biases and provides more precise coefficient estimates than existing approaches. We also apply the proposed method to a deoxyribonucleic acid methylation dataset from the Alzheimer's disease (AD) neuroimaging initiative database to investigate the association between cerebrospinal fluid tau protein levels and AD severity. AVAILABILITY AND IMPLEMENTATION: The code for the proposed method is available on GitHub (https://github.com/Li-Zhaoy/Dec-Deb.git) and archived on Zenodo (DOI: https://10.5281/zenodo.15478745).",
      "journal": "Bioinformatics (Oxford, England)",
      "year": "2025",
      "doi": "10.1093/bioinformatics/btaf400",
      "authors": "Li Zhaoyang et al.",
      "keywords": "",
      "mesh_terms": "Humans; Linear Models; Alzheimer Disease; Algorithms; DNA Methylation; Computational Biology; Computer Simulation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40658470/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Regression",
      "health_domain": "Neurology; Infectious Disease",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Regularization",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: First, we reduce hidden confounding via spectral transformation. Second, we correct bias from the weighted \u21131 penalty, commonly used in high-dimensional estimation, by inverting the Karush-Kuhn-Tucker conditions and solving convex optimization programs. This deconfounding technique by spectral transformation requires no prior knowledge of hidden confounders.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12381636"
    },
    {
      "pmid": "40663686",
      "title": "Debiasing Medical Knowledge for Prompting Universal Model in CT Image Segmentation.",
      "abstract": "With the assistance of large language models, which offer universal medical prior knowledge via text prompts, state-of-the-art Universal Models (UM) have demonstrated considerable potential in the field of medical image segmentation. Semantically detailed text prompts, on the one hand, indicate comprehensive knowledge; on the other hand, they bring biases that may not be applicable to specific cases involving heterogeneous organs or rare cancers. To this end, we propose a Debiased Universal Model (DUM) to consider instance-level context information and remove knowledge biases in text prompts from the causal perspective. We are the first to discover and mitigate the bias introduced by universal knowledge. Specifically, we propose to extract organ-level text prompts via language models and instance-level context prompts from the visual features of each image. We aim to highlight more on factual instance-level information and mitigate organ-level's knowledge bias. This process can be derived and theoretically supported by a causal graph, and instantiated by designing a standard UM (SUM) and a biased UM. The debiased output is finally obtained by subtracting the likelihood distribution output by biased UM from that of the SUM. Experiments on three large-scale multi-center external datasets and MSD internal tumor datasets show that our method enhances the model's generalization ability in handling diverse medical scenarios and reducing the potential biases, even with an improvement of 4.16% compared with popular universal model on the AbdomenAtlas dataset, showing the strong generalizability. The code is publicly available at https://github.com/DeepMed-Lab-ECNU/DUM.",
      "journal": "IEEE transactions on medical imaging",
      "year": "2025",
      "doi": "10.1109/TMI.2025.3589399",
      "authors": "Yun Boxiang et al.",
      "keywords": "",
      "mesh_terms": "Humans; Tomography, X-Ray Computed; Image Processing, Computer-Assisted; Algorithms; Natural Language Processing",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40663686/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "NLP/LLM; Computer Vision/Imaging AI; Generative AI",
      "health_domain": "Radiology/Medical Imaging; Oncology",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Experiments on three large-scale multi-center external datasets and MSD internal tumor datasets show that our method enhances the model's generalization ability in handling diverse medical scenarios and reducing the potential biases, even with an improvement of 4.16% compared with popular universal model on the AbdomenAtlas dataset, showing the strong generalizability. The code is publicly available at https://github.com/DeepMed-Lab-ECNU/DUM.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40673267",
      "title": "Clinical Algorithms and the Legacy of Race-Based Correction: Historical Errors, Contemporary Revisions and Equity-Oriented Methodologies for Epidemiologists.",
      "abstract": "Clinical algorithms are widely used tools for predicting, diagnosing, and managing diseases. However, race correction in these algorithms has faced increasing scrutiny for potentially perpetuating health disparities and reinforcing harmful stereotypes. This narrative review synthesizes historical, clinical, and methodological literature to examine the origins and consequences of race correction in clinical algorithms. We focus primarily on developments in the United States and the United Kingdom, where many race-based algorithms originated. Drawing on interdisciplinary sources, we discuss the persistence of race-based adjustments, the implications of their removal, and emerging strategies for bias mitigation and fairness in algorithm development. The practice began in the mid-19th century with the spirometer, which measured lung capacity and was used to reinforce racial hierarchies by characterizing lower lung capacity for Black people. Despite critiques that these differences reflect environmental exposure rather than inherited traits, the belief in race-based biological differences in lung capacity and other physiological functions, including cardiac, renal, and obstetric processes, persists in contemporary clinical algorithms. Concerns about race correction compounding health inequities have led many medical organizations to re-evaluate their algorithms, with some removing race entirely. Transitioning to race-neutral equations in areas like pulmonary function testing and obstetrics has shown promise in enhancing fairness without compromising accuracy. However, the impact of these changes varies across clinical contexts, highlighting the need for careful bias identification and mitigation. Future efforts should focus on incorporating diverse data sources, capturing true social and biological health determinants, implementing bias detection and fairness strategies, ensuring transparent reporting, and engaging with diverse communities. Educating students and trainees on race as a sociopolitical construct is also important for raising awareness and achieving health equity. Moving forward, regular monitoring, evaluation, and refinement of approaches in real-world settings are needed for clinical algorithms serve all patients equitably and effectively.",
      "journal": "Clinical epidemiology",
      "year": "2025",
      "doi": "10.2147/CLEP.S527000",
      "authors": "Horsfall Laura J et al.",
      "keywords": "algorithmic bias; algorithmic fairness; bias detection; bias mitigation; clinical algorithms; health disparities; lung function; race correction; spirometer",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40673267/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Generative AI",
      "health_domain": "Cardiology; Obstetrics/Maternal Health; Nephrology; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Intersectional",
      "lifecycle_stage": "Data Collection; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Educating students and trainees on race as a sociopolitical construct is also important for raising awareness and achieving health equity. Moving forward, regular monitoring, evaluation, and refinement of approaches in real-world settings are needed for clinical algorithms serve all patients equitably and effectively.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12266067"
    },
    {
      "pmid": "40680485",
      "title": "Toward fair medical advice: Addressing and mitigating bias in large language model-based healthcare applications.",
      "abstract": "Large Language Models (LLMs) are increasingly deployed in web-based medical advice applications, offering scalable and accessible healthcare solutions. However, their outputs often reflect demographic biases, raising concerns about fairness and equity for vulnerable populations. In this work, we propose FairMed, a framework designed to mitigate biases in LLM-generated medical advice through fine-tuning and prompt engineering strategies. We evaluate FairMed using language-based and content-level metrics across demographic groups on publicly available (MedQA), synthetic (Synthea), and private (CBHS) datasets. Experimental results demonstrate consistent improvements over Llama3 - Med42, as well as over the zero-shot prompting baseline. For instance, in sentiment analysis for gender groups using MedQA, FairMed with Descriptive Prompting reduces the Statistical Parity Difference (SPD) from 0.0902 to 0.0658, improves the Disparate Impact Ratio from 1.1916 to 1.1566, and decreases the Kullback-Leibler Divergence from 0.0045 to 0.0024. Similarly, in directive language evaluation for gender groups using Synthea, SPD improves from 0.1056 to nearly zero, achieving near-perfect parity. On the CBHS dataset, FairMed with Descriptive Prompting increases Diagnostic Recommendation Divergence (DRD) for race groups from 0.9530 to 0.9848, indicating improved group-specific tailoring, while reducing the Action Disparity Index (ADI) from 0.0857 to 0.0469 and Referral Frequency Parity (RFP) from 0.0791 to 0.0511, reflecting enhanced fairness. These findings highlight FairMed's effectiveness in addressing demographic disparities and promoting equitable healthcare guidance through web technologies. This framework contributes to building trustworthy and inclusive systems for delivering medical advice by ensuring fairness in sensitive applications.",
      "journal": "Artificial intelligence in medicine",
      "year": "2025",
      "doi": "10.1016/j.artmed.2025.103216",
      "authors": "Lu Haohui et al.",
      "keywords": "AI applications; Bias mitigation; Fairness in large language models; Large language models",
      "mesh_terms": "Humans; Language; Delivery of Health Care; Internet; Bias; Male; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40680485/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Transfer Learning",
      "clinical_setting": "Public Health/Population",
      "key_findings": "These findings highlight FairMed's effectiveness in addressing demographic disparities and promoting equitable healthcare guidance through web technologies. This framework contributes to building trustworthy and inclusive systems for delivering medical advice by ensuring fairness in sensitive applications.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40683994",
      "title": "Demographic inaccuracies and biases in the depiction of patients by artificial intelligence text-to-image generators.",
      "abstract": "The wide usage of artificial intelligence (AI) text-to-image generators raises concerns about the role of AI in amplifying misconceptions in healthcare. This study therefore evaluated the demographic accuracy and potential biases in the depiction of patients by four commonly used text-to-image generators. A total of 9060 images of patients with 29 different diseases was generated using Adobe Firefly, Bing Image Generator, Meta Imagine, and Midjourney. Twelve independent raters determined the sex, age, weight, and race and ethnicity of the patients depicted. Comparison to the real-world epidemiology showed that the generated images failed to depict demographical characteristics such as sex, age, and race and ethnicity accurately. In addition, we observed an over-representation of White and normal weight individuals. Inaccuracies and biases may stem from non-representative and non-specific training data as well as insufficient or misdirected bias mitigation strategies. In consequence, new strategies to counteract such inaccuracies and biases are needed.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01817-6",
      "authors": "Wiegand Tim Luca Till et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40683994/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Inaccuracies and biases may stem from non-representative and non-specific training data as well as insufficient or misdirected bias mitigation strategies. In consequence, new strategies to counteract such inaccuracies and biases are needed.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12276360"
    },
    {
      "pmid": "40706198",
      "title": "Bridging the digital divide: artificial intelligence as a catalyst for health equity in primary care settings.",
      "abstract": "BACKGROUND: Health inequalities remain one of the most pressing challenges in contemporary healthcare, with primary care serving as both a gateway to services and a potential source of disparities. Artificial intelligence (AI) technologies offer unprecedented opportunities to address these inequities through enhanced diagnostic capabilities, improved access to care, and personalised interventions. OBJECTIVE: This comprehensive narrative review aimed to synthesise current evidence on AI applications in primary care settings, specifically targeting health inequality reduction and identifying both opportunities and barriers for equitable implementation. METHOD: Following PRISMA-ScR (Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews) guidelines, we employed a systematic approach to literature identification, selection, and synthesis across seven electronic databases covering literature from 2020 to 2024. Of 1,247 initially identified studies, 89 met inclusion criteria with 52 providing sufficient data quality for evidence synthesis. RESULTS: The review identified promising applications such as AI-powered risk stratification algorithms that improved hypertension control in low-income populations, telemedicine platforms reducing geographic barriers in rural communities, and natural language processing tools facilitating care for non-native speakers. However, significant challenges persist, including algorithmic bias that may perpetuate existing inequities, the digital divide excluding vulnerable populations, and insufficient representation in training datasets. Current evidence suggests that whilst AI holds transformative potential for advancing health equity, successful implementation requires intentional co-design with affected communities, robust bias mitigation strategies, and comprehensive digital literacy programmes. CONCLUSION: Future research must prioritise equity-centred AI development, longitudinal outcome studies in diverse populations, and policy frameworks ensuring responsible deployment. However, careful consideration of unintended consequences, including potential overdiagnosis, erosion of human clinical judgement, and inadvertent exclusion of vulnerable populations, is essential to prevent AI from exacerbating existing health disparities. The paradigm shift towards equity-first AI design represents a critical opportunity to leverage technology for social justice in healthcare.",
      "journal": "International journal of medical informatics",
      "year": "2025",
      "doi": "10.1016/j.ijmedinf.2025.106051",
      "authors": "Osonuga Ayokunle et al.",
      "keywords": "Artificial intelligence; Digital divide; Health disparities; Health equity; Primary care",
      "mesh_terms": "Artificial Intelligence; Humans; Health Equity; Primary Health Care; Digital Divide; Telemedicine",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40706198/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Primary Care",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Language; Geographic",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Primary Care/Outpatient; Public Health/Population; Telehealth/Remote",
      "key_findings": "CONCLUSION: Future research must prioritise equity-centred AI development, longitudinal outcome studies in diverse populations, and policy frameworks ensuring responsible deployment. However, careful consideration of unintended consequences, including potential overdiagnosis, erosion of human clinical judgement, and inadvertent exclusion of vulnerable populations, is essential to prevent AI from exacerbating existing health disparities. The paradigm shift towards equity-first AI design represent...",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40716036",
      "title": "Generative artificial intelligence-mediated confirmation bias in health information seeking.",
      "abstract": "Generative artificial intelligence (GenAI) applications, such as ChatGPT, are transforming how individuals access health information, offering conversational and highly personalized interactions. While these technologies can enhance health literacy and decision-making, their capacity to generate deeply tailored-hypercustomized-responses risks amplifying confirmation bias by reinforcing pre-existing beliefs, obscuring medical consensus, and perpetuating misinformation, posing significant challenges to public health. This paper examines GenAI-mediated confirmation bias in health information seeking, driven by the interplay between GenAI's hypercustomization capabilities and users' confirmatory tendencies. Drawing on parallels with traditional online information-seeking behaviors, we identify three key \"pressure points\" where biases might emerge: query phrasing, preference for belief-consistent content, and resistance to belief-inconsistent information. Using illustrative examples, we highlight the limitations of existing safeguards and argue that even minor variations in applications' configuration (e.g., Custom GPT) can exacerbate these biases along those pressure points. Given the widespread adoption and fragmentation (e.g., OpenAI's GPT Store) of GenAI applications, their influence on health-seeking behaviors demands urgent attention. Since technical safeguards alone may be insufficient, we propose a set of interventions, including enhancing digital literacy, empowering users with critical engagement strategies, and implementing robust regulatory oversight. These recommendations aim to ensure the safe integration of GenAI into daily life, supporting informed decision-making and preserving the integrity of public understanding of health information.",
      "journal": "Annals of the New York Academy of Sciences",
      "year": "2025",
      "doi": "10.1111/nyas.15413",
      "authors": "Lopez-Lopez Ezequiel et al.",
      "keywords": "confirmation bias; generative artificial intelligence; hypercustomization; information seeking; public health",
      "mesh_terms": "Humans; Artificial Intelligence; Information Seeking Behavior; Health Literacy; Decision Making; Bias; Generative Artificial Intelligence",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40716036/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "EHR/Health Informatics; Public Health",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Since technical safeguards alone may be insufficient, we propose a set of interventions, including enhancing digital literacy, empowering users with critical engagement strategies, and implementing robust regulatory oversight. These recommendations aim to ensure the safe integration of GenAI into daily life, supporting informed decision-making and preserving the integrity of public understanding of health information.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12412720"
    },
    {
      "pmid": "40718760",
      "title": "A fair machine learning model to predict flares of systemic lupus erythematosus.",
      "abstract": "OBJECTIVE: Systemic lupus erythematosus (SLE) is a chronic autoimmune disease that disproportionately affects women and racial/ethnic minority groups. Predicting disease flares is essential for improving patient outcomes, yet few studies integrate both clinical and social determinants of health (SDoH). We therefore developed FLAME (FLAre Machine learning prediction of SLE), a machine learning pipeline that uses electronic health records (EHRs) and contextual-level SDoH to predict 3-month flare risk, emphasizing explainability and fairness. MATERIALS AND METHODS: We conducted a retrospective cohort study of 28\u2009433 patients with SLE from the University of Florida Health (2011-2022), linked to 675 contextual-level SDoH variables. We used XGBoost and logistic regression models to predict 3-month flare risk, evaluating model performance using the area under the receiver operating characteristic (AUROC). We applied SHapley Additive exPlanations (SHAP) values and causal structure learning to identify key predictors. Fairness was assessed using the equality of opportunity metric, measured by the false-negative rate across racial/ethnic groups. RESULTS: The FLAME model, incorporating clinical and contextual-level SDoH, achieved an AUROC of 0.66. The clinical-only model performed slightly better (AUROC of 0.67), while the SDoH-only model had lower performance (AUROC of 0.54). SHAP analysis identified headache, organic brain syndrome, and pyuria as key predictors. Causal learning revealed interactions between clinical factors and contextual-level SDoH. Fairness assessments showed no significant biases across groups. DISCUSSION: FLAME offers a fair and interpretable approach to predicting SLE flares, providing meaningful insights that may guide future clinical interventions. CONCLUSIONS: FLAME shows promise as an EHR-based tool to support personalized, equitable, and holistic SLE care.",
      "journal": "JAMIA open",
      "year": "2025",
      "doi": "10.1093/jamiaopen/ooaf072",
      "authors": "Li Yongqiu et al.",
      "keywords": "fairness; machine learning; prediction; social determinants of health; systemic lupus erythematosus",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40718760/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Logistic Regression; XGBoost/Gradient Boosting; Generative AI; Regression",
      "health_domain": "EHR/Health Informatics; Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: FLAME shows promise as an EHR-based tool to support personalized, equitable, and holistic SLE care.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12296391"
    },
    {
      "pmid": "40726749",
      "title": "Empirical Comparison of Post-processing Debiasing Methods for Machine Learning Classifiers in Healthcare.",
      "abstract": "UNLABELLED: Machine learning classifiers in healthcare tend to reproduce or exacerbate existing health disparities due to inherent biases in training data. This relevant issue has brought the attention of researchers in both healthcare and other domains, proposing techniques that deal with it in different stages of the machine learning process. Post-processing methods adjust model predictions to ensure fairness without interfering in the learning process nor requiring access to the original training data, preserving privacy and enabling the application to any trained model. This study rigorously compares state-of-the-art debiasing methods within the family of post-processing techniques across a wide range of synthetic and real-world (healthcare) datasets, by means of different performance and fairness metrics. Our experiments reveal the strengths and weaknesses of each method, examining the trade-offs between group fairness and predictive performance, as well as among different notions of group fairness. Additionally, we analyze the impact on untreated attributes to ensure overall bias mitigation. Our comprehensive evaluation provides insights into how these debiasing methods can be optimally implemented in healthcare settings to balance accuracy and fairness. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007/s41666-025-00196-7.",
      "journal": "Journal of healthcare informatics research",
      "year": "2025",
      "doi": "10.1007/s41666-025-00196-7",
      "authors": "Dang Vien Ngoc et al.",
      "keywords": "Algorithmic bias; Fairness; Healthcare; Machine learning classifiers; Post-processing",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40726749/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability; Post-hoc Correction",
      "clinical_setting": "Not specified",
      "key_findings": "Our comprehensive evaluation provides insights into how these debiasing methods can be optimally implemented in healthcare settings to balance accuracy and fairness. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007/s41666-025-00196-7.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12290158"
    },
    {
      "pmid": "40756387",
      "title": "Ethical AI in medical text generation: balancing innovation with privacy in public health.",
      "abstract": "INTRODUCTION: The integration of artificial intelligence (AI) into medical text generation is transforming public health by enhancing clinical documentation, patient education, and decision support. However, the widespread deployment of AI in this domain introduces significant ethical challenges, including fairness, privacy protection, and accountability. Traditional AI-driven medical text generation models often inherit biases from training data, resulting in disparities in healthcare communication across different demographic groups. Moreover, ensuring patient data confidentiality while maintaining transparency in AI-generated content remains a critical concern. Existing approaches either lack robust bias mitigation mechanisms or fail to provide interpretable and privacy-preserving outputs, compromising ethical compliance and regulatory adherence. METHODS: To address these challenges, this paper proposes an innovative framework that combines privacy-preserving AI techniques with interpretable model architectures to achieve ethical compliance in medical text generation. The method employs a hybrid approach that integrates knowledge-based reasoning with deep learning, ensuring both accuracy and transparency. Privacy-enhancing technologies, such as homomorphic encryption and secure multi-party computation, are incorporated to safeguard sensitive medical data throughout the text generation process. Fairness-aware training protocols are introduced to mitigate biases in generated content and enhance trustworthiness. RESULTS AND DISCUSSION: The proposed approach effectively addresses critical challenges of bias, privacy, and interpretability in medical text generation. By combining symbolic reasoning with data-driven learning and embedding ethical principles at the system design level, the framework ensures regulatory alignment and improves public trust. This methodology lays the groundwork for broader deployment of ethically sound AI systems in healthcare communication.",
      "journal": "Frontiers in public health",
      "year": "2025",
      "doi": "10.3389/fpubh.2025.1583507",
      "authors": "Liang Mingpei",
      "keywords": "AI ethics; bias mitigation; ethical challenges; healthcare regulation; legal compliance; medical AI; privacy protection; text generation",
      "mesh_terms": "Humans; Artificial Intelligence; Public Health; Confidentiality; Privacy; Electronic Health Records",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40756387/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning",
      "health_domain": "EHR/Health Informatics; Public Health",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Fairness Constraints; Representation Learning; Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "DISCUSSION: The proposed approach effectively addresses critical challenges of bias, privacy, and interpretability in medical text generation. By combining symbolic reasoning with data-driven learning and embedding ethical principles at the system design level, the framework ensures regulatory alignment and improves public trust. This methodology lays the groundwork for broader deployment of ethically sound AI systems in healthcare communication.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12313694"
    },
    {
      "pmid": "40764552",
      "title": "Toward a fair, gender-debiased classifier for the diagnosis of attention deficit/hyperactivity disorder- a Machine-Learning based classification study.",
      "abstract": "BACKGROUND: Attention deficit/hyperactivity disorder (ADHD) is the most common neurodevelopmental disorder. Gender disparities in the diagnosis of ADHD have been reported, suggesting that females tend to be diagnosed later in life than males are. The delayed diagnosis in females has been attributed to an inequality in the diagnostic criteria, failing to focus on the gender differences regarding symptomatology, comorbidity, and societal factors contributing to this disparity. METHODS: In this study, we introduced debiased classifiers for the diagnosis of ADHD via different bias mitigation algorithms of the AI Fairness 360 toolbox on a training dataset of 400 children and adolescents with and without ADHD (98 females, 25 ADHD patients, 73 typically developing females), a subsample of the Child Mind Institute dataset. Test data were acquired in an earlier study. Two datasets were used, one including personal characteristic features, scores of the clinical questionnaire Child Behavior Checklist, and wavelet variance coefficients as quantifiers of neural dynamics (fMRI), a second dataset included personal characteristic features, scores of the clinical questionnaire Child Behavior Checklist, and radiomic features of neural structure (sMRI). RESULTS: We found that the reweighed XGBoost model achieved the best accuracy and highest fairness in both datasets. Using model explanation, we showed how reweighing influenced feature importance at the global and local levels. CONCLUSION: Based on methodological characteristics and insights from global and local model explana-tion, we discuss the reasons of these findings and conclude, that using the combination of bias mitigation and model explanation, improved classification models can be achieved.",
      "journal": "BMC medical informatics and decision making",
      "year": "2025",
      "doi": "10.1186/s12911-025-03126-0",
      "authors": "Neufang Susanne et al.",
      "keywords": "AI fairness 360; Attention deficit/hyperactivity disorder; Bias detection and mitigation; Gender bias; Machine learning; Model explanation",
      "mesh_terms": "Humans; Attention Deficit Disorder with Hyperactivity; Male; Child; Female; Machine Learning; Adolescent; Sex Factors",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40764552/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "XGBoost/Gradient Boosting",
      "health_domain": "Radiology/Medical Imaging; Pediatrics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Based on methodological characteristics and insights from global and local model explana-tion, we discuss the reasons of these findings and conclude, that using the combination of bias mitigation and model explanation, improved classification models can be achieved.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12326834"
    },
    {
      "pmid": "40771228",
      "title": "Algorithmic bias in public health AI: a silent threat to equity in low-resource settings.",
      "abstract": "",
      "journal": "Frontiers in public health",
      "year": "2025",
      "doi": "10.3389/fpubh.2025.1643180",
      "authors": "Joseph Jeena",
      "keywords": "algorithmic bias; artificial intelligence in public health; digital health disparities; health equity; inclusive AI; low-resource settings",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40771228/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Safety-Net/Underserved",
      "key_findings": "No abstract available",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12325396"
    },
    {
      "pmid": "40773445",
      "title": "Reducing bias in coronary heart disease prediction using Smote-ENN and PCA.",
      "abstract": "Coronary heart disease (CHD) is a major cardiovascular disorder that poses significant threats to global health and is increasingly affecting younger populations. Its treatment and prevention face challenges such as high costs, prolonged recovery periods, and limited efficacy of traditional methods. Additionally, the complexity of diagnostic indicators and the global shortage of medical professionals further complicate accurate diagnosis. This study employs machine learning techniques to analyze CHD-related pathogenic factors and proposes an efficient diagnostic and predictive framework. To address the data imbalance issue, SMOTE-ENN is utilized, and five machine learning algorithms-Decision Trees, KNN, SVM, XGBoost, and Random Forest-are applied for classification tasks. Principal Component Analysis (PCA) and Grid Search are used to optimize the models, with evaluation metrics including accuracy, precision, recall, F1-score, and AUC. According to the random forest model's optimization experiment, the initial unbalanced data's accuracy was 85.26%, and the F1-score was 12.58%. The accuracy increased to 92.16% and the F1-score reached 93.85% after using SMOTE-ENN for data balancing, which is an increase of 6.90% and 81.27%, respectively; the model accuracy increased to 97.91% and the F1-score increased to 97.88% after adding PCA feature dimensionality reduction processing, which is an increase of 5.75% and 4.03%, respectively, compared with the SMOTE-ENN stage. This indicates that combining data balancing and feature dimensionality reduction techniques significantly improves model accuracy and makes the random forest model the best model. This study provides an efficient diagnostic tool for CHD, alleviates the challenges posed by limited medical resources, and offers a scientific foundation for precise prevention and intervention strategies.",
      "journal": "PloS one",
      "year": "2025",
      "doi": "10.1371/journal.pone.0327569",
      "authors": "Wei Xinyi et al.",
      "keywords": "",
      "mesh_terms": "Humans; Coronary Disease; Principal Component Analysis; Machine Learning; Algorithms; Decision Trees; Male; Support Vector Machine; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40773445/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Random Forest; XGBoost/Gradient Boosting; Support Vector Machine; Decision Tree",
      "health_domain": "Cardiology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling",
      "clinical_setting": "Public Health/Population",
      "key_findings": "This indicates that combining data balancing and feature dimensionality reduction techniques significantly improves model accuracy and makes the random forest model the best model. This study provides an efficient diagnostic tool for CHD, alleviates the challenges posed by limited medical resources, and offers a scientific foundation for precise prevention and intervention strategies.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12331108"
    },
    {
      "pmid": "40775930",
      "title": "Biased AI: A Case for Positive Bias in Healthcare AI.",
      "abstract": "Bias in artificial intelligence (AI) is a pervasive challenge, often reinforcing systemic inequities in healthcare systems. This paper proposes an innovative framework to repurpose bias in AI, leveraging it as a tool for addressing structural injustices and improving outcomes for underrepresented and marginalized groups. Traditional healthcare algorithms often exhibit racial biases, such as underestimating risks for black patients or failing to detect dark-skinned individuals in diagnostic or safety-critical applications. This paper redefines AI bias as a tool for equity, proposing a framework to correct systemic healthcare disparities. By introducing purpose-driven bias, AI can enhance fairness in diagnostics, safety, and medical interventions. The approach involves bias analysis, diverse data curation, and AI fine-tuning to align with fairness objectives. This framework highlights the potential of \"biased AI\" to drive more inclusive and equitable healthcare.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI250912",
      "authors": "Shah Hurmat Ali et al.",
      "keywords": "AI; AI in healthcare; bias in AI; large language models",
      "mesh_terms": "Artificial Intelligence; Humans; Healthcare Disparities; Racism",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40775930/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Transfer Learning",
      "clinical_setting": "Not specified",
      "key_findings": "The approach involves bias analysis, diverse data curation, and AI fine-tuning to align with fairness objectives. This framework highlights the potential of \"biased AI\" to drive more inclusive and equitable healthcare.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40776043",
      "title": "Algorithmic Fairness in Machine Learning Prediction of Autism Using Electronic Health Records.",
      "abstract": "Efforts to improve early diagnosis of autism spectrum disorder (ASD) in children are beginning to use machine learning (ML) approaches applied to real-world clinical datasets, such as electronic health records (EHRs). However, sex-based disparities in ASD diagnosis highlight the need for fair prediction models that ensure equitable performance across demographic groups for ASD identification. This retrospective case-control study aimed to develop ML-based prediction models for ASD diagnosis using risk factors found in EHRs and assess their algorithmic fairness. The study cohorts included 70,803 children diagnosed with ASD and 212,409 matched controls without ASD. We built logistic regression and Xgboost models and evaluated their performance using standard metrics, including accuracy, recall, precision, F1-score, and area under the curve (AUC). To assess fairness, we examined model performance by sex and calculated fairness-specific metrics, such as equal opportunity (recall parity) and equalized odds, to identify potential biases in model predictions between boys and girls. Our results revealed significant fairness issues in ML models for ASD prediction using EHRs.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI251025",
      "authors": "Angell Amber M et al.",
      "keywords": "Autism; electronic health records; predictive modeling",
      "mesh_terms": "Humans; Electronic Health Records; Machine Learning; Male; Female; Retrospective Studies; Case-Control Studies; Child; Autism Spectrum Disorder; Algorithms; Child, Preschool",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40776043/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Logistic Regression; XGBoost/Gradient Boosting; Clinical Prediction Model",
      "health_domain": "EHR/Health Informatics; Pediatrics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "To assess fairness, we examined model performance by sex and calculated fairness-specific metrics, such as equal opportunity (recall parity) and equalized odds, to identify potential biases in model predictions between boys and girls. Our results revealed significant fairness issues in ML models for ASD prediction using EHRs.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40780075",
      "title": "Understanding the development, performance, fairness, and transparency of machine learning models used in child protection prediction: A systematic review.",
      "abstract": "OBJECTIVE: To understand the development and validation of contemporary machine learning (ML) models for child protection prediction, their performance evaluation, integration of fairness, and operationalisation of model explainability and transparency. METHODS: This systematic review followed the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guidelines. Model transparency was assessed against the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis + Artificial Intelligence (TRIPOD+AI) criteria, while study risk of bias and model applicability were evaluated using Prediction model Risk Of Bias ASsessment Tool (PROBAST) criteria. RESULTS: Eleven studies were identified, employing various ML approaches such as supervised classification models (e.g., binary classification, decision trees, support vector machines), regression models, and ensemble methods. These models utilised administrative health, child welfare, and criminal/court data. Performance was evaluated using a range of discrimination, classification, and calibration metrics, yielding variable results. Only four models incorporated group fairness, focusing on race/ethnicity as the protected attribute. Explainability and transparency were enhanced through Receiver Operating Curves, Precision-Recall Curves, feature importance plots, and SHapley Additive exPlanations (SHAP) plots. According to TRIPOD+AI criteria, only four studies reported likely reproducible models. Based on PROBAST criteria, all studies had unclear or high risk of bias. CONCLUSIONS: This is the first review to use TRIPOD+AI and PROBAST criteria to assess the risk of bias and transparency of ML models in child protection prediction. The findings reveal that the field remains methodologically immature, with many models lacking fair, transparent, and reproducible methods. Adoption of advanced fairness techniques (beyond fairness-through-unawareness), stakeholder involvement in model development and validation, and transparency through data and code sharing will be essential for the ethical and effective design of ML models, ultimately improving decision-making processes and outcomes for vulnerable children and families.",
      "journal": "Child abuse & neglect",
      "year": "2025",
      "doi": "10.1016/j.chiabu.2025.107630",
      "authors": "Bull Claudia et al.",
      "keywords": "Machine learning; PROBAST; TRIPOD+AI; algorithmic fairness; child protection; stakeholder involvement; transparency",
      "mesh_terms": "Humans; Machine Learning; Child; Child Protective Services; Child Welfare; Child Abuse",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40780075/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Support Vector Machine; Decision Tree; Ensemble Methods; Clinical Prediction Model; Regression",
      "health_domain": "Pediatrics",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Calibration; Explainability/Interpretability; Ensemble Methods",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: This is the first review to use TRIPOD+AI and PROBAST criteria to assess the risk of bias and transparency of ML models in child protection prediction. The findings reveal that the field remains methodologically immature, with many models lacking fair, transparent, and reproducible methods. Adoption of advanced fairness techniques (beyond fairness-through-unawareness), stakeholder involvement in model development and validation, and transparency through data and code sharing will be...",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40784603",
      "title": "Digital twins in increasing diversity in clinical trials: A systematic review.",
      "abstract": "The integration of digital twin (DT) technology and artificial intelligence (AI) into clinical trials holds transformative potential for addressing persistent inequities in participant representation. This systematic review evaluates the role of these technologies in improving diversity, particularly in racial, ethnic, gender, age, and socioeconomic dimensions, minimizing bias, and allowing personalized medicine in clinical research settings. Evidence from 90 studies reveals that digital twins offer dynamic simulation capabilities for trial design, while AI facilitates predictive analytics and recruitment optimization. However, implementation remains hindered by fragmented regulatory frameworks, biased datasets, and infrastructural disparities. Ethical concerns,including privacy, consent, and algorithmic opacity, further complicate the deployment. Inclusive data practices identified in the literature include the use of demographically representative training data, participatory data collection frameworks, and equity audits to detect and correct systemic bias. Fairness in AI and DT models is primarily operationalized through group fairness metrics such as demographic parity and equalized odds, along with fairness, aware model training and validation. Key gaps include the lack of global standards, underrepresentation in model training, and challenges in real-world adoption. To overcome these barriers, the review proposes actionable directions: developing inclusive data practices, harmonizing regulatory oversight, and embedding fairness into computational model design. By focusing on diversity as a design principle, AI and DT technologies can support a more equitable and generalizable future for clinical research.",
      "journal": "Journal of biomedical informatics",
      "year": "2025",
      "doi": "10.1016/j.jbi.2025.104879",
      "authors": "Tubbs Abigail et al.",
      "keywords": "Artificial intelligence (AI); Bias mitigation; Clinical trials; Data privacy; Digital twins (DT); Diversity and inclusion; Health equity; Machine learning; Personalized medicine; Predictive analytics; Recruitment optimization",
      "mesh_terms": "Humans; Clinical Trials as Topic; Artificial Intelligence; Cultural Diversity",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40784603/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Data Collection; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Representation Learning; Diverse/Representative Data; Bias Auditing Framework",
      "clinical_setting": "ICU; Clinical Trial",
      "key_findings": "To overcome these barriers, the review proposes actionable directions: developing inclusive data practices, harmonizing regulatory oversight, and embedding fairness into computational model design. By focusing on diversity as a design principle, AI and DT technologies can support a more equitable and generalizable future for clinical research.",
      "ft_include": true,
      "ft_reason": "Included: substantial approach content in abstract",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40799930",
      "title": "A community-based approach to ethical decision-making in artificial intelligence for health care.",
      "abstract": "OBJECTIVES: Artificial Intelligence (AI) is transforming healthcare by improving diagnostics, treatment recommendations, and resource allocation. However, its implementation also raises ethical concerns, particularly regarding biases in AI algorithms trained on inequitable data, which may reinforce health disparities. This article introduces the AI COmmunity-based Ethical Dialogue and DEcision-making (CODE) framework to embed ethical deliberation into AI development, focusing on Electronic Health Records (EHRs). MATERIALS AND METHODS: We propose the AI CODE framework as a structured approach to addressing ethical challenges in AI-driven healthcare and ensuring its implementation supports health equity. RESULTS: The framework outlines 5 steps to advance health equity: (1) Contextual diversity and priority: Ensuring inclusive datasets and that AI reflects the community needs; (2) Sharing ethical propositions: Structured discussions on privacy, bias, and fairness; (3) Dialogic decision-making: Collaboratively with stakeholders to develop AI solutions; (4) Integrating ethical solutions: Applying solutions into AI design to enhance fairness; and (5) Evaluating effectiveness: Continuously monitoring AI to address emerging biases. DISCUSSION: We examine the framework's role in mitigating AI biases through structured community engagement and its relevance within evolving healthcare policies. While the framework promotes ethical AI integration in healthcare, it also faces challenges in implementation. CONCLUSION: The framework provides practical guidance to ensure AI systems are ethical, community-driven, and aligned with health equity goals.",
      "journal": "JAMIA open",
      "year": "2025",
      "doi": "10.1093/jamiaopen/ooaf076",
      "authors": "Senghor Abdou S et al.",
      "keywords": "artificial intelligence; community involvement; decision making; electronic health record data; health care ethics",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40799930/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Diverse/Representative Data",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: The framework provides practical guidance to ensure AI systems are ethical, community-driven, and aligned with health equity goals.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12342142"
    },
    {
      "pmid": "40825541",
      "title": "Efficient Detection of Stigmatizing Language in Electronic Health Records via In-Context Learning: Comparative Analysis and Validation Study.",
      "abstract": "BACKGROUND: The presence of stigmatizing language within electronic health records (EHRs) poses significant risks to patient care by perpetuating biases. While numerous studies have explored the use of supervised machine learning models to detect stigmatizing language automatically, these models require large, annotated datasets, which may not always be readily available. In-context learning (ICL) has emerged as a data-efficient alternative, allowing large language models to adapt to tasks using only instructions and examples. OBJECTIVE: We aimed to investigate the efficacy of ICL in detecting stigmatizing language within EHRs under data-scarce conditions. METHODS: We analyzed 5043 sentences from the Medical Information Mart for Intensive Care-IV dataset, which contains EHRs from patients admitted to the emergency department at the Beth Israel Deaconess Medical Center. We compared ICL with zero-shot (textual entailment), few-shot (SetFit), and supervised fine-tuning approaches. The ICL approach used 4 prompting strategies: generic, chain of thought, clue and reasoning prompting, and a newly introduced stigma detection guided prompt. Model fairness was evaluated using the equal performance criterion, measuring true positive rate, false positive rate, and F1-score disparities across protected attributes, including sex, age, and race. RESULTS: In the zero-shot setting, the best-performing ICL model, GEMMA-2, achieved a mean F1-score of 0.858 (95% CI 0.854-0.862), showing an 18.7% improvement over the best textual entailment model, DEBERTA-M (mean F1-score 0.723, 95% CI 0.718-0.728; P<.001). In the few-shot setting, the top ICL model, LLAMA-3, outperformed the leading SetFit models by 21.2%, 21.4%, and 12.3% with 4, 8, and 16 annotations per class, respectively (P<.001). Using 32 labeled instances, the best ICL model achieved a mean F1-score of 0.901 (95% CI 0.895-0.907), only 3.2% lower than the best supervised fine-tuning model, ROBERTA (mean F1-score 0.931, 95% CI 0.924-0.938), which was trained on 3543 labeled instances. Under the conditions tested, fairness evaluation revealed that supervised fine-tuning models exhibited greater bias compared with ICL models in the zero-shot, 4-shot, 8-shot, and 16-shot settings, as measured by true positive rate, false positive rate, and F1-score disparities. CONCLUSIONS: ICL offers a robust and flexible solution for detecting stigmatizing language in EHRs, offering a more data-efficient and equitable alternative to conventional machine learning methods. These findings suggest that ICL could enhance bias detection in clinical documentation while reducing the reliance on extensive labeled datasets.",
      "journal": "JMIR medical informatics",
      "year": "2025",
      "doi": "10.2196/68955",
      "authors": "Chen Hongbo et al.",
      "keywords": "artificial intelligence; electronic health record; fairness; few-shot; in-context learning; large language model; machine learning; prompting strategy; stigmatizing language; text classification; zero-shot",
      "mesh_terms": "Humans; Electronic Health Records; Machine Learning; Language; Social Stigma; Stereotyping; Male; Female",
      "pub_types": "Journal Article; Comparative Study; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40825541/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Emergency Medicine; ICU/Critical Care; EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Transfer Learning",
      "clinical_setting": "Hospital/Inpatient; ICU; Emergency Department",
      "key_findings": "CONCLUSIONS: ICL offers a robust and flexible solution for detecting stigmatizing language in EHRs, offering a more data-efficient and equitable alternative to conventional machine learning methods. These findings suggest that ICL could enhance bias detection in clinical documentation while reducing the reliance on extensive labeled datasets.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12402740"
    },
    {
      "pmid": "40834630",
      "title": "BiasPruner: Mitigating bias transfer in continual learning for fair medical image analysis.",
      "abstract": "Continual Learning (CL) enables neural networks to learn new tasks while retaining previous knowledge. However, most CL methods fail to address bias transfer, where spurious correlations propagate to future tasks or influence past knowledge. This bidirectional bias transfer negatively impacts model performance and fairness, especially in medical imaging, where it can lead to misdiagnoses and unequal treatment. In this work, we show that conventional CL methods amplify these biases, posing risks for diverse patient cohorts. To address this, we propose BiasPruner, a framework that mitigates bias propagation through debiased subnetworks, while preserving sequential learning and avoiding catastrophic forgetting. BiasPruner computes a bias attribution score to identify and prune network units responsible for spurious correlations, creating task-specific subnetworks that learn unbiased representations. As new tasks are learned, the framework integrates non-biased units from previous subnetworks to preserve transferable knowledge and prevent bias transfer. During inference, a task-agnostic gating mechanism selects the optimal subnetwork for robust predictions. We evaluate BiasPruner on medical imaging benchmarks, including skin lesion and chest X-ray classification tasks, where biased data (e.g., spurious skin tone correlations) can exacerbate disparities. Our experiments show that BiasPruner outperforms state-of-the-art CL methods in both accuracy and fairness. Code is available at: BiasPruner.",
      "journal": "Medical image analysis",
      "year": "2025",
      "doi": "10.1016/j.media.2025.103764",
      "authors": "Bayasi Nourhan et al.",
      "keywords": "Bias transfer; Catastrophic forgetting; Continual learning; Debiased representations; Debiased subnetwork; Dynamic subnetwork pruning; Spurious correlations",
      "mesh_terms": "Humans; Neural Networks, Computer; Image Processing, Computer-Assisted; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40834630/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Neural Network; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Dermatology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Our experiments show that BiasPruner outperforms state-of-the-art CL methods in both accuracy and fairness. Code is available at: BiasPruner.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40863668",
      "title": "Integrating Artificial Intelligence into Perinatal Care Pathways: A Scoping Review of Reviews of Applications, Outcomes, and Equity.",
      "abstract": "Background: Artificial intelligence (AI) and machine learning (ML) have been reshaping maternal, fetal, neonatal, and reproductive healthcare by enhancing risk prediction, diagnostic accuracy, and operational efficiency across the perinatal continuum. However, no comprehensive synthesis has yet been published. Objective: To conduct a scoping review of reviews of AI/ML applications spanning reproductive, prenatal, postpartum, neonatal, and early child-development care. Methods: We searched PubMed, Embase, the Cochrane Library, Web of Science, and Scopus through April 2025. Two reviewers independently screened records, extracted data, and assessed methodological quality using AMSTAR 2 for systematic reviews, ROBIS for bias assessment, SANRA for narrative reviews, and JBI guidance for scoping reviews. Results: Thirty-nine reviews met our inclusion criteria. In preconception and fertility treatment, convolutional neural network-based platforms can identify viable embryos and key sperm parameters with over 90 percent accuracy, and machine-learning models can personalize follicle-stimulating hormone regimens to boost mature oocyte yield while reducing overall medication use. Digital sexual-health chatbots have enhanced patient education, pre-exposure prophylaxis adherence, and safer sexual behaviors, although data-privacy safeguards and bias mitigation remain priorities. During pregnancy, advanced deep-learning models can segment fetal anatomy on ultrasound images with more than 90 percent overlap compared to expert annotations and can detect anomalies with sensitivity exceeding 93 percent. Predictive biometric tools can estimate gestational age within one week with accuracy and fetal weight within approximately 190 g. In the postpartum period, AI-driven decision-support systems and conversational agents can facilitate early screening for depression and can guide follow-up care. Wearable sensors enable remote monitoring of maternal blood pressure and heart rate to support timely clinical intervention. Within neonatal care, the Heart Rate Observation (HeRO) system has reduced mortality among very low-birth-weight infants by roughly 20 percent, and additional AI models can predict neonatal sepsis, retinopathy of prematurity, and necrotizing enterocolitis with area-under-the-curve values above 0.80. From an operational standpoint, automated ultrasound workflows deliver biometric measurements at about 14 milliseconds per frame, and dynamic scheduling in IVF laboratories lowers staff workload and per-cycle costs. Home-monitoring platforms for pregnant women are associated with 7-11 percent reductions in maternal mortality and preeclampsia incidence. Despite these advances, most evidence derives from retrospective, single-center studies with limited external validation. Low-resource settings, especially in Sub-Saharan Africa, remain under-represented, and few AI solutions are fully embedded in electronic health records. Conclusions: AI holds transformative promise for perinatal care but will require prospective multicenter validation, equity-centered design, robust governance, transparent fairness audits, and seamless electronic health record integration to translate these innovations into routine practice and improve maternal and neonatal outcomes.",
      "journal": "Nursing reports (Pavia, Italy)",
      "year": "2025",
      "doi": "10.3390/nursrep15080281",
      "authors": "El Arab Rabie Adel et al.",
      "keywords": "artificial intelligence; health equity; machine learning; maternal health; neonatal care; nursing; perinatal care; reproductive health",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40863668/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Deep Learning; Neural Network; Clinical Prediction Model",
      "health_domain": "Radiology/Medical Imaging; Cardiology; Ophthalmology; Mental Health/Psychiatry; ICU/Critical Care; EHR/Health Informatics; Pediatrics; Obstetrics/Maternal Health; Wearables/Remote Monitoring",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability; Bias Auditing Framework",
      "clinical_setting": "Telehealth/Remote; Safety-Net/Underserved",
      "key_findings": "Conclusions: AI holds transformative promise for perinatal care but will require prospective multicenter validation, equity-centered design, robust governance, transparent fairness audits, and seamless electronic health record integration to translate these innovations into routine practice and improve maternal and neonatal outcomes.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12388636"
    },
    {
      "pmid": "40866555",
      "title": "Quantifying device type and handedness biases in a remote Parkinson's disease AI-powered assessment.",
      "abstract": "We investigate issues pertaining to algorithmic fairness and digital health equity within the context of using machine learning to predict Parkinson's Disease (PD) with data recorded from structured assessments of finger and hand movements. We evaluate the impact of demographic bias and bias related to device type and handedness. We collected data from 251 participants (99 with PD or suspected PD, 152 without PD or any suspicion of PD). Using a random forest model, we observe 92% accuracy, 94% AUROC, 86% sensitivity, 92% specificity, and 84% F1-score. When examining only F1-score differences across groups, no significant bias appears. However, a closer look reveals bias regarding positive prediction and error rates. While we find that sex and ethnicity have no statistically significant impact on PD predictions, biases exist regarding device type and dominant hand, as evidenced by disparate impact and equalized odds. Our findings suggest that remote digital health diagnostics may exhibit underrecognized biases related to handedness and device characteristics, the latter of which can act as a proxy for socioeconomic factors.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-01934-2",
      "authors": "Tumpa Zerin Nasrin et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40866555/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Random Forest",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "Telehealth/Remote",
      "key_findings": "While we find that sex and ethnicity have no statistically significant impact on PD predictions, biases exist regarding device type and dominant hand, as evidenced by disparate impact and equalized odds. Our findings suggest that remote digital health diagnostics may exhibit underrecognized biases related to handedness and device characteristics, the latter of which can act as a proxy for socioeconomic factors.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12391457"
    },
    {
      "pmid": "40899524",
      "title": "Towards Fairness in Synthetic Healthcare Data: A Framework for the Evaluation of Synthetization Algorithms.",
      "abstract": "INTRODUCTION: Synthetic data generation is a rapidly evolving field, with significant potential for improving data privacy. However, evaluating the performance of synthetic data generation methods, especially the tradeoff between fairness and utility of the generated data, remains a challenge. METHODOLOGY: In this work, we present our comprehensive framework, which evaluates fair synthetic data generation methods, benchmarking them against state-of-the-art synthesizers. RESULTS: The proposed framework consists of selection, evaluation, and application components that assess fairness, utility, and resemblance in real-world scenarios. The framework was applied to state-of-the-art data synthesizers, including TabFairGAN, DECAF, TVAE, and CTGAN, using a publicly available medical dataset. DISCUSSION: The results reveal the strengths and limitations of each synthesizer, including their bias mitigation strategies and trade-offs between fairness and utility, thereby showing the framework's effectiveness. The proposed framework offers valuable insights into the fairness-utility tradeoff and evaluation of synthetic data generation methods, with far-reaching implications for various applications in the medical domain and beyond. CONCLUSION: The findings demonstrate the importance of considering fairness in synthetic data generation and the need for fairness focused evaluation frameworks, highlighting the significance of continued research in this area.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI251376",
      "authors": "Warnecke Yannik et al.",
      "keywords": "Algorithms; Artificial Intelligence; Data Quality; Delivery of Health Care; Health Equity; Humans; Medical Informatics",
      "mesh_terms": "Humans; Algorithms; Confidentiality; Data Collection",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40899524/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Data Augmentation",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: The findings demonstrate the importance of considering fairness in synthetic data generation and the need for fairness focused evaluation frameworks, highlighting the significance of continued research in this area.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40905712",
      "title": "Detecting, Characterizing, and Mitigating Implicit and Explicit Racial Biases in Health Care Datasets With Subgroup Learnability: Algorithm Development and Validation Study.",
      "abstract": "BACKGROUND: The growing adoption of diagnostic and prognostic algorithms in health care has led to concerns about the perpetuation of algorithmic bias against disadvantaged groups of individuals. Deep learning methods to detect and mitigate bias have revolved around modifying models, optimization strategies, and threshold calibration with varying levels of success and tradeoffs. However, there have been limited substantive efforts to address bias at the level of the data used to generate algorithms in health care datasets. OBJECTIVE: The aim of this study is to create a simple metric (AEquity) that uses a learning curve approximation to distinguish and mitigate bias via guided dataset collection or relabeling. METHODS: We demonstrate this metric in 2 well-known examples, chest X-rays and health care cost utilization, and detect novel biases in the National Health and Nutrition Examination Survey. RESULTS: We demonstrated that using AEquity to guide data-centric collection for each diagnostic finding in the chest radiograph dataset decreased bias by between 29% and 96.5% when measured by differences in area under the curve. Next, we wanted to examine (1) whether AEquity worked on intersectional populations and (2) if AEquity is invariant to different types of fairness metrics, not just area under the curve. Subsequently, we examined the effect of AEquity on mitigating bias when measured by false negative rate, precision, and false discovery rate for Black patients on Medicaid. When we examined Black patients on Medicaid, at the intersection of race and socioeconomic status, we found that AEquity-based interventions reduced bias across a number of different fairness metrics including overall false negative rate by 33.3% (bias reduction absolute=1.88\u00d710-1, 95% CI 1.4\u00d710-1 to 2.5\u00d710-1; bias reduction of 33.3%, 95% CI 26.6%-40%; precision bias by 7.50\u00d710-2, 95% CI 7.48\u00d710-2 to 7.51\u00d710-2; bias reduction of 94.6%, 95% CI 94.5%-94.7%; false discovery rate by 94.5%; absolute bias reduction=3.50\u00d710-2, 95% CI 3.49\u00d710-2 to 3.50\u00d710-2). Similarly, AEquity-guided data collection demonstrated bias reduction of up to 80% on mortality prediction with the National Health and Nutrition Examination Survey (bias reduction absolute=0.08, 95% CI 0.07-0.09). Then, we wanted to compare AEquity to state-of-the-art data-guided debiasing measures such as balanced empirical risk minimization and calibration. Consequently, we benchmarked against balanced empirical risk minimization and calibration and showed that AEquity-guided data collection outperforms both standard approaches. Moreover, we demonstrated that AEquity works on fully connected networks; convolutional neural networks such as ResNet-50; transformer architectures such as VIT-B-16, a vision transformer with 86 million parameters; and nonparametric methods such as Light Gradient-Boosting Machine. CONCLUSIONS: In short, we demonstrated that AEquity is a robust tool by applying it to different datasets, algorithms, and intersectional analyses and measuring its effectiveness with respect to a range of traditional fairness metrics.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/71757",
      "authors": "Gulamali Faris et al.",
      "keywords": "bias; data-centric artificial intelligence; fairness; machine learning",
      "mesh_terms": "Bias; Humans; Datasets as Topic; Routinely Collected Health Data; Radiography, Thoracic; Health Care Costs; Black or African American; Medicaid; Race Factors; Social Class; Nutrition Surveys; Machine Learning; Data Curation; Mortality; United States",
      "pub_types": "Journal Article; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40905712/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Deep Learning; NLP/LLM; Neural Network",
      "health_domain": "Radiology/Medical Imaging; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Insurance Status; Intersectional",
      "lifecycle_stage": "Data Collection; Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Threshold Adjustment; Fairness Metrics Evaluation",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: In short, we demonstrated that AEquity is a robust tool by applying it to different datasets, algorithms, and intersectional analyses and measuring its effectiveness with respect to a range of traditional fairness metrics.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12410029"
    },
    {
      "pmid": "40908961",
      "title": "Human Versus Artificial Intelligence: Comparing Cochrane Authors' and ChatGPT's Risk of Bias Assessments.",
      "abstract": "INTRODUCTION: Systematic reviews and meta-analyses synthesize randomized trial data to guide clinical decisions but require significant time and resources. Artificial intelligence (AI) offers a promising solution to streamline evidence synthesis, aiding study selection, data extraction, and risk of bias assessment. This study aims to evaluate the performance of ChatGPT-4o in assessing the risk of bias in randomised controlled trials (RCTs) using the Risk of Bias 2 (RoB 2) tool, comparing its results with those conducted by human reviewers in Cochrane Reviews. METHODS: A sample of Cochrane Reviews utilizing the RoB 2 tool was identified through the Cochrane Database of Systematic Reviews (CDSR). Protocols, qualitative systematic reviews, and reviews employing alternative risk of bias assessment tools were excluded. The study utilized ChatGPT-4o to assess the risk of bias using a structured set of prompts corresponding to the RoB 2 domains. The agreement between ChatGPT-4o and consensus-based human reviewer assessments was evaluated using weighted kappa statistics. Additionally, accuracy, sensitivity, specificity, positive predictive value, and negative predictive value were calculated. All analyses were performed using R Studio (version 4.3.0). RESULTS: A total of 42 Cochrane Reviews were screened, yielding a final sample of eight eligible reviews comprising 84 RCTs. The primary outcome of each included review was selected for risk of bias assessment. ChatGPT-4o demonstrated moderate agreement with human reviewers for the overall risk of bias judgments (weighted kappa = 0.51, 95% CI: 0.36-0.66). Agreement varied across domains, ranging from fair (\u03ba\u2009=\u20090.20 for selection of the reported results) to moderate (\u03ba\u2009=\u20090.59 for measurement of outcomes). ChatGPT-4o exhibited a sensitivity of 53% for identifying high-risk studies and a specificity of 99% for classifying low-risk studies. CONCLUSION: This study shows that ChatGPT-4o can perform risk of bias assessments using RoB 2 with fair to moderate agreement with human reviewers. While AI-assisted risk of bias assessment remains imperfect, advancements in prompt engineering and model refinement may enhance performance. Future research should explore standardised prompts and investigate interrater reliability among human reviewers to provide a more robust comparison.",
      "journal": "Cochrane evidence synthesis and methods",
      "year": "2025",
      "doi": "10.1002/cesm.70044",
      "authors": "Taneri Petek Eylul",
      "keywords": "artificial intelligence; evidence synthesis; large language models; risk of bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40908961/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Clinical Trial",
      "key_findings": "CONCLUSION: This study shows that ChatGPT-4o can perform risk of bias assessments using RoB 2 with fair to moderate agreement with human reviewers. While AI-assisted risk of bias assessment remains imperfect, advancements in prompt engineering and model refinement may enhance performance. Future research should explore standardised prompts and investigate interrater reliability among human reviewers to provide a more robust comparison.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12407253"
    },
    {
      "pmid": "40927497",
      "title": "Beyond the Algorithm: A Perspective on Tackling Bias and Cultural Sensitivity in AI-Guided Aesthetic Standards for Cosmetic Surgery in the Middle East and North Africa (MENA) Region.",
      "abstract": "Artificial intelligence (AI) is increasingly reshaping cosmetic surgery by enhancing surgical planning, predicting outcomes, and enabling objective aesthetic assessment. Through narrative synthesis of existing literature and case studies, this perspective paper explores the issue of algorithmic bias in AI-powered aesthetic technologies and presents a framework for culturally sensitive application within cosmetic surgery practices in the Middle East and North Africa (MENA) region. Existing AI systems are predominantly trained on datasets that underrepresent MENA phenotypes, resulting in aesthetic recommendations that disproportionately reflect Western beauty ideals. The MENA region, however, encompasses a broad spectrum of beauty standards that merge traditional cultural aesthetics with modern global trends, posing unique challenges for AI integration. To ensure ethical and clinically relevant deployment, AI systems must undergo fundamental changes in algorithm design, including the incorporation of culturally diverse datasets with adequate MENA representation, implementation of cultural competency principles, and active collaboration with regional healthcare professionals. The framework outlines concrete criteria for evaluating cultural representativeness in AI training data and outcome assessments, supporting future empirical validation. Developing culturally aware AI tools is both a moral obligation and a clinical priority. This framework provides both a moral imperative and clinical pathway for ensuring AI serves to support, rather than homogenize, the region's diverse aesthetic traditions.",
      "journal": "Clinical, cosmetic and investigational dermatology",
      "year": "2025",
      "doi": "10.2147/CCID.S543045",
      "authors": "Makhseed Abdulrahman et al.",
      "keywords": "MENA region; aesthetic medicine; algorithmic bias; artificial intelligence; cosmetic surgery; cultural competency; facial analysis; health equity; medical ethics",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40927497/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Surgery",
      "bias_axes": "Gender/Sex; Geographic",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability; Diverse/Representative Data",
      "clinical_setting": "Not specified",
      "key_findings": "Developing culturally aware AI tools is both a moral obligation and a clinical priority. This framework provides both a moral imperative and clinical pathway for ensuring AI serves to support, rather than homogenize, the region's diverse aesthetic traditions.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12416507"
    },
    {
      "pmid": "40940655",
      "title": "FanFAIR: sensitive data sets semi-automatic fairness assessment.",
      "abstract": "BACKGROUND: Research has shown how data sets convey social bias in Artificial Intelligence systems, especially those based on machine learning. A biased data set is not representative of reality and might contribute to perpetuate societal biases within the model. To tackle this problem, it is important to understand how to avoid biases, errors, and unethical practices while creating the data sets. In order to provide guidance for the use of data sets in contexts of critical decision-making, such as health decisions, we identified six fundamental data set features (balance, numerosity, unevenness, compliance, quality, incompleteness) that could affect model fairness. These features were the foundation for the FanFAIR framework. RESULTS: We extended the FanFAIR framework for the semi-automated evaluation of fairness in data sets, by combining statistical information on data with qualitative features. In particular, we present an improved version of FanFAIR which introduces novel outlier detection capabilities working in multivariate fashion, using two state-of-the-art methods: the Empirical Cumulative-distribution Outlier Detection (ECOD) and Isolation Forest. We also introduce a novel metric for data set balance, based on an entropy measure. CONCLUSION: We addressed the issue of how much (un)fairness can be included in a data set used for machine learning research, focusing on classification issues. We developed a rule-based approach based on fuzzy logic that combines these characteristics into a single score and enables a semi-automatic evaluation of a data set in algorithmic fairness research. Our tool produces a detailed visual report about the fairness of the data set. We show the effectiveness of FanFAIR by applying the method on two open data sets.",
      "journal": "BMC medical informatics and decision making",
      "year": "2025",
      "doi": "10.1186/s12911-025-03184-4",
      "authors": "Gallese Chiara et al.",
      "keywords": "Data bias; Fairness; Fuzzy logic; Trustworthy artificial intelligence",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40940655/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: We addressed the issue of how much (un)fairness can be included in a data set used for machine learning research, focusing on classification issues. We developed a rule-based approach based on fuzzy logic that combines these characteristics into a single score and enables a semi-automatic evaluation of a data set in algorithmic fairness research. Our tool produces a detailed visual report about the fairness of the data set.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12427094"
    },
    {
      "pmid": "40940963",
      "title": "Revolutionizing Oncology Through AI: Addressing Cancer Disparities by Improving Screening, Treatment, and Survival Outcomes via Integration of Social Determinants of Health.",
      "abstract": "BACKGROUND: Social determinants of health (SDOH) are critical contributors to cancer disparities, influencing prevention, early detection, treatment access, and survival outcomes. Addressing these disparities is essential in achieving equitable oncology care. Artificial intelligence (AI) is revolutionizing oncology by leveraging advanced computational methods to address SDOH-driven disparities through predictive analytics, data integration, and precision medicine. METHODS: This review synthesizes findings from systematic reviews and original research on AI applications in cancer-focused SDOH research. Key methodologies include machine learning (ML), natural language processing (NLP), deep learning-based medical imaging, and explainable AI (XAI). Special emphasis is placed on AI's ability to analyze large-scale oncology datasets, including electronic health records (EHRs), geographic information systems (GIS), and real-world clinical trial data, to enhance cancer risk stratification, optimize screening programs, and improve resource allocation. RESULTS: AI has demonstrated significant advancements in cancer diagnostics, treatment planning, and survival prediction by integrating SDOH data. AI-driven radiomics and histopathology have enhanced early detection, particularly in underserved populations. Predictive modeling has improved personalized oncology care, enabling stratification based on socioeconomic and environmental factors. However, challenges remain, including AI bias in screening, trial underrepresentation, and treatment recommendation disparities. CONCLUSIONS: AI holds substantial potential to reduce cancer disparities by integrating SDOH into risk prediction, screening, and treatment personalization. Ethical deployment, bias mitigation, and robust regulatory frameworks are essential in ensuring fairness in AI-driven oncology. Integrating AI into precision oncology and public health strategies can bridge cancer care gaps, enhance early detection, and improve treatment outcomes for vulnerable populations.",
      "journal": "Cancers",
      "year": "2025",
      "doi": "10.3390/cancers17172866",
      "authors": "Srivastav Amit Kumar et al.",
      "keywords": "artificial intelligence (AI); cancer; explainable AI (XAI); machine learning; precision oncology; social determinants of health (SDOH)",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40940963/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Deep Learning; NLP/LLM; Computer Vision/Imaging AI; Clinical Prediction Model",
      "health_domain": "Radiology/Medical Imaging; Oncology; ICU/Critical Care; EHR/Health Informatics; Pathology; Public Health",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Language; Geographic",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population; Clinical Trial; Laboratory/Pathology; Safety-Net/Underserved",
      "key_findings": "CONCLUSIONS: AI holds substantial potential to reduce cancer disparities by integrating SDOH into risk prediction, screening, and treatment personalization. Ethical deployment, bias mitigation, and robust regulatory frameworks are essential in ensuring fairness in AI-driven oncology. Integrating AI into precision oncology and public health strategies can bridge cancer care gaps, enhance early detection, and improve treatment outcomes for vulnerable populations.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12427515"
    },
    {
      "pmid": "41005257",
      "title": "Fairness in machine learning-based hand load estimation: A case study on load carriage tasks.",
      "abstract": "Predicting external hand load from sensor data is essential for ergonomic exposure assessments, as obtaining this information typically requires direct observation or supplementary data. While machine learning can estimate hand load from posture or force data, we found systematic bias tied to biological sex, with predictive disparities worsening in imbalanced training datasets. To address this, we developed a fair predictive model using a Variational Autoencoder with feature disentanglement, which separates sex-agnostic from sex-specific motion features. This enables predictions based only on sex-agnostic patterns. Our proposed algorithm outperformed conventional machine learning models, including k-Nearest Neighbors, Support Vector Machine, and Random Forest, achieving a mean absolute error of 3.42 and improving fairness metrics like statistical parity and positive and negative residual differences, even when trained on imbalanced sex datasets. These results underscore the importance of fairness-aware algorithms in avoiding health and safety disadvantages for specific worker groups in the workplace.",
      "journal": "Applied ergonomics",
      "year": "2025",
      "doi": "10.1016/j.apergo.2025.104642",
      "authors": "Rahman Arafat et al.",
      "keywords": "Algorithmic bias; Fairness; Gait kinematics; Load carriage; Machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41005257/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Random Forest; Support Vector Machine; Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Representation Learning",
      "clinical_setting": "Not specified",
      "key_findings": "Our proposed algorithm outperformed conventional machine learning models, including k-Nearest Neighbors, Support Vector Machine, and Random Forest, achieving a mean absolute error of 3.42 and improving fairness metrics like statistical parity and positive and negative residual differences, even when trained on imbalanced sex datasets. These results underscore the importance of fairness-aware algorithms in avoiding health and safety disadvantages for specific worker groups in the workplace.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41032562",
      "title": "Enhancing Fairness and Accuracy in Diagnosing Type 2 Diabetes in Young Adult Population.",
      "abstract": "While type 2 diabetes is predominantly found in the elderly population, recent publications indicate an increasing prevalence in the young adult population. Failing to diagnose it in the minority younger age group could have significant adverse effects on their health. Several previous works acknowledge the bias of machine learning models towards different gender and race groups and propose various approaches to mitigate it. However, those works failed to propose any effective methodologies to diagnose diabetes in the young population, which is the minority group in the diabetic population. This is the first paper where we mention digital ageism towards the young adult population diagnosing diabetes. In this paper, we identify this deficiency in traditional machine learning models and propose an algorithm to mitigate the bias towards the young population when predicting diabetes. Deviating from the traditional concept of one-model-fits-all, we train customized machine-learning models for each age group. Our pipeline trains a separate machine learning model for every 5-year age band (i.e., age groups 30-34, 35-39, and 40-44). The proposed solution consistently improves recall of diabetes class by 26% to 40% in the young age group (30-44). Moreover, our technique outperforms 7 commonly used whole-group resampling techniques (i.e., random oversampling, random undersampling, SMOTE, ADASYN, Tomek-links, ENN, and Near Miss) by at least 36% in terms of diabetes recall in the young age group. Feature important analysis shows that the age attribute has a significant contribution to the decision of the original model, which was marginalized in the age-personalized model. Our method shows improved performance (e.g., balanced accuracy improved 7-12%) over multiple machine learning models and multiple sampling algorithms.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2025",
      "doi": "10.1109/JBHI.2025.3616312",
      "authors": "Pias Tanmoy Sarkar et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41032562/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Endocrinology/Diabetes",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Feature important analysis shows that the age attribute has a significant contribution to the decision of the original model, which was marginalized in the age-personalized model. Our method shows improved performance (e.g., balanced accuracy improved 7-12%) over multiple machine learning models and multiple sampling algorithms.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41036091",
      "title": "Evaluating the impact of data biases on algorithmic fairness and clinical utility of machine learning models for prolonged opioid use prediction.",
      "abstract": "OBJECTIVES: The growing use of machine learning (ML) in healthcare raises concerns about how data biases affect real-world model performance. While existing frameworks evaluate algorithmic fairness, they often overlook the impact of bias on generalizability and clinical utility, which are critical for safe deployment. Building on prior methods, this study extends bias analysis to include clinical utility, addressing a key gap between fairness evaluation and decision-making. MATERIALS AND METHODS: We applied a 3-phase evaluation to a previously developed model predicting prolonged opioid use (POU), validated on Veterans Health Administration (VHA) data. The analysis included internal and external validation, model retraining on VHA data, and subgroup evaluation across demographic, vulnerable, risk, and comorbidity groups. We assessed performance using area under the receiver operating characteristic curve (AUROC), calibration, and decision curve analysis, incorporating standardized net-benefits to evaluate clinical utility alongside fairness and generalizability. RESULTS: The internal cohort (N\u2009=\u200941\u2009929) had a 14.7% POU prevalence, compared to 34.3% in the external VHA cohort (N\u2009=\u2009397\u2009150). The model's AUROC decreased from 0.74 in the internal test cohort to 0.70 in the full external cohort. Subgroup-level performance averaged 0.69 (SD\u2009=\u20090.01), showing minimal deviation from the external cohort overall. Retraining on VHA data improved AUROCs to 0.82. Clinical utility analysis showed systematic shifts in net-benefit across threshold probabilities. DISCUSSION: While the POU model showed generalizability and fairness internally, external validation and retraining revealed performance and utility shifts across subgroups. CONCLUSION: Population-specific biases affect clinical utility-an often-overlooked dimension in fairness evaluation-a key need to ensure equitable benefits across diverse patient groups.",
      "journal": "JAMIA open",
      "year": "2025",
      "doi": "10.1093/jamiaopen/ooaf115",
      "authors": "Naderalvojoud Behzad et al.",
      "keywords": "algorithmic fairness; clinical utility; data bias; generalizability; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41036091/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Threshold Adjustment",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: Population-specific biases affect clinical utility-an often-overlooked dimension in fairness evaluation-a key need to ensure equitable benefits across diverse patient groups.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12483547"
    },
    {
      "pmid": "41041783",
      "title": "Towards an Analytical System for Supervising Fairness, Robustness, and Dataset Shifts in Health AI.",
      "abstract": "Ensuring trustworthy use of Artificial Intelligence (AI)-based Clinical Decision Support Systems (CDSSs) requires continuous evaluation of their performance and fairness, given the potential impact on patient safety and individual rights as high-risk AI systems. However, the practical implementation of health AI performance and fairness monitoring dashboards presents several challenges. Confusion-matrix-derived performance and fairness metrics are non-additive and cannot be reliably aggregated or disaggregated across time or population subgroups. Furthermore, acquiring ground-truth labels or sensitive variable information, and controlling dataset shifts-changes in data statistical distributions-may require additional interoperability with the electronic health records. We present the design of ShinAI-Agent, a modular system that enables continuous, interpretable, and privacy-aware monitoring of health AI and CDSS performance and fairness. An exploratory dashboard combines time series navigation for multiple performance and fairness metrics, model calibration and decision cutoff exploration, and dataset shift monitoring. The system adopts a two-layer database. First, a proxy database, mapping AI outcomes and essential case-level data such as the ground-truth and sensitive variables. And second, an OLAP architecture with aggregable primitives, including case-based confusion matrices and binned probability distributions for flexible computation of performance and fairness metrics across time or sensitive subgroups. The ShinAI-Agent approach supports compliance with the ethical and robustness requirements of the EU AI Act, enables advisory for model retraining and promotes the operationalisation of Trustworthy AI.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI251537",
      "authors": "S\u00e1nchez-Garc\u00eda \u00c1ngel et al.",
      "keywords": "Ethical AI; Fairness; Robust AI; Software Engineering; Trustworthy AI",
      "mesh_terms": "Artificial Intelligence; Electronic Health Records; Decision Support Systems, Clinical; Humans",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41041783/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Decision Support",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Calibration; Threshold Adjustment; Subgroup Analysis; Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "And second, an OLAP architecture with aggregable primitives, including case-based confusion matrices and binned probability distributions for flexible computation of performance and fairness metrics across time or sensitive subgroups. The ShinAI-Agent approach supports compliance with the ethical and robustness requirements of the EU AI Act, enables advisory for model retraining and promotes the operationalisation of Trustworthy AI.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41055822",
      "title": "Invisible Bias in GPT-4o-mini: Detecting Disparities in AI-Generated Patient Messaging.",
      "abstract": "Artificial intelligence (AI), specifically large language models (LLM), have gained significant popularity over the last decade with increased performance and expanding applications. AI could improve the quality of patient care in medicine but hidden biases introduced during training could be harmful. This work utilizes GPT-4o-mini to generate patient communications based on systematically generated, synthetic patient data that would be commonly available in a patient's medical record. To evaluate the AI generated communications for disparities, GPT-4o-mini was used to score the generated communications on empathy, encouragement, accuracy, clarity, professionalism, and respect. Disparities in scores associated with specific components of a patient's history were used to detect potential biases. A patient's sex and religious preference were found to have a statistically significant impact on scores. However, further work is needed to evaluate a wider collection of LLMs utilizing more specific and human validated scoring criteria. Overall, this work proposes a novel method of evaluating bias in LLMs by creating synthetic patient histories to formulate AI generated communications and score them with opportunities for further investigation.",
      "journal": "Journal of medical systems",
      "year": "2025",
      "doi": "10.1007/s10916-025-02276-y",
      "authors": "Reagen Christopher et al.",
      "keywords": "Artificial intelligence; Bias; Empathy; Large language models; Medicine; Patient communication",
      "mesh_terms": "Humans; Artificial Intelligence; Communication; Male; Female; Language; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41055822/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "However, further work is needed to evaluate a wider collection of LLMs utilizing more specific and human validated scoring criteria. Overall, this work proposes a novel method of evaluating bias in LLMs by creating synthetic patient histories to formulate AI generated communications and score them with opportunities for further investigation.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41100248",
      "title": "FairFedMed: Benchmarking Group Fairness in Federated Medical Imaging with FairLoRA.",
      "abstract": "Fairness remains a critical concern in healthcare, where unequal access to services and treatment outcomes can adversely affect patient health. While Federated Learning (FL) presents a collaborative and privacy-preserving approach to model training, ensuring fairness is challenging due to heterogeneous data across institutions, and current research primarily addresses non-medical applications. To fill this gap, we establish the first experimental benchmark for fairness in medical FL, evaluating six representative FL methods across diverse demographic attributes and imaging modalities. We introduce FairFedMed, the first medical FL dataset specifically designed to study group fairness (i.e., consistent performance across demographic groups). It comprises two parts: FairFedMed-Oph, featuring 2D fundus and 3D OCT ophthalmology samples with six demographic attributes; and FairFedMed-Chest, which simulates real cross-institutional FL using subsets of CheXpert and MIMIC-CXR. Together, they support both simulated and real-world FL across diverse medical modalities and demographic groups. Existing FL models often underperform on medical images and overlook fairness across demographic groups. To address this, we propose FairLoRA, a fairness-aware FL framework based on SVD-based low-rank approximation. It customizes singular value matrices per demographic group while sharing singular vectors, ensuring both fairness and efficiency. Experimental results on the FairFedMed dataset demonstrate that FairLoRA not only achieves state-of-the-art performance in medical image classification but also significantly improves fairness across diverse populations. Our code and dataset can be accessible via GitHub link: https://github.com/Harvard-AI-and-Robotics-Lab/FairFedMed.",
      "journal": "IEEE transactions on medical imaging",
      "year": "2025",
      "doi": "10.1109/TMI.2025.3622522",
      "authors": "Li Minghan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41100248/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Federated Learning; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Ophthalmology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Federated Learning",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Experimental results on the FairFedMed dataset demonstrate that FairLoRA not only achieves state-of-the-art performance in medical image classification but also significantly improves fairness across diverse populations. Our code and dataset can be accessible via GitHub link: https://github.com/Harvard-AI-and-Robotics-Lab/FairFedMed.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41104854",
      "title": "Role of artificial intelligence in cognitive debiasing within clinical decision-making.",
      "abstract": "",
      "journal": "Minerva medica",
      "year": "2025",
      "doi": "10.23736/S0026-4806.25.09831-3",
      "authors": "Volpe Giovanni et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41104854/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41107862",
      "title": "Navigating fairness aspects of clinical prediction models.",
      "abstract": "BACKGROUND: Algorithms are increasingly used in healthcare, yet most algorithms lack thorough evaluation and impact assessment across diverse populations. This absence of comprehensive scrutiny introduces a significant risk of inequitable clinical outcomes, particularly between different demographic and socioeconomic groups. MAIN BODY: Societal biases-rooted in structural inequalities and systemic discrimination-often shape the data used to develop these algorithms. When such biases become embedded into predictive models, algorithms frequently favor privileged populations, further deepening existing inequalities. Without proactive efforts to identify and mitigate these biases, algorithms risk disproportionately harming already marginalized groups, widening the gap between advantaged and disadvantaged patients. Various statistical metrics are available to assess algorithmic fairness, each addressing different dimensions of disparity in predictive performance across population groups. However, understanding and applying these fairness metrics in real-world healthcare settings remains limited. Transparency in both the development and communication of algorithms is essential to building a more equitable healthcare system. Openly addressing fairness concerns fosters trust and accountability, ensuring that fairness considerations become an integral part of algorithm design and implementation rather than an afterthought. Using a participatory approach involving three clinicians and three patients with lived experience of type 2 diabetes, we developed a set of guiding questions to help healthcare professionals assess algorithms critically, challenge existing practices, and stimulate discussions. CONCLUSIONS: We aim to direct healthcare professionals on navigating the complexities of bias in healthcare algorithms by encouraging critical thinking about biases present in society, data, algorithms, and healthcare systems.",
      "journal": "BMC medicine",
      "year": "2025",
      "doi": "10.1186/s12916-025-04340-3",
      "authors": "Chakradeo Kaustubh et al.",
      "keywords": "Algorithmic fairness; Algorithms; Clinical decision rules; Clinical decision-making; Delivery of healthcare; Fairness; Health inequities; Health personnel; Prediction algorithms; Risk factors",
      "mesh_terms": "Humans; Algorithms; Diabetes Mellitus, Type 2; Healthcare Disparities",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41107862/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "ICU/Critical Care; Endocrinology/Diabetes",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSIONS: We aim to direct healthcare professionals on navigating the complexities of bias in healthcare algorithms by encouraging critical thinking about biases present in society, data, algorithms, and healthcare systems.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12535043"
    },
    {
      "pmid": "41111683",
      "title": "Artificial Intelligence Applications in the Prediction and Management of Pediatric Asthma Exacerbation: A Systematic Review.",
      "abstract": "Pediatric asthma exacerbations remain a significant global health challenge due to their unpredictable nature and potential for severe morbidity. While artificial intelligence (AI) shows promise in improving prediction and management, the evidence base is fragmented. This systematic review synthesizes current literature on AI applications for pediatric asthma exacerbation prediction and management, evaluating model performance, clinical utility, and methodological quality. Following Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines, we searched PubMed, Scopus, Elsevier, Web of Science, and Excerpta Medica Database (Embase) (2020-2025) for studies applying AI/machine learning (ML) to pediatric asthma exacerbations. Eight studies met the inclusion criteria after screening 431 records. Data were extracted on study design, AI models, input features, outcomes, and performance metrics. Risk of bias was assessed using Risk Of Bias In Non-randomized Studies of Interventions (ROBINS-I) for non-randomized studies and the Cochrane Risk of Bias 2 (RoB 2) tool for randomized trials. Eight studies demonstrated AI's effectiveness in predicting pediatric asthma exacerbations, outperforming traditional methods. Performance varied, with multimodal data yielding the best results. Some models faced limitations from data biases or small samples. Most studies had a low risk of bias. AI showed potential to improve clinical workflows, but real-world impact needs more research. AI shows strong potential for pediatric asthma exacerbation prediction, particularly with multimodal data. Key challenges include algorithmic bias mitigation, prospective validation, and standardization of outcome metrics. Future research should prioritize equitable model development and clinical integration.",
      "journal": "Cureus",
      "year": "2025",
      "doi": "10.7759/cureus.92491",
      "authors": "Osman Mohmed Fatima Mahmoud et al.",
      "keywords": "artificial intelligence; asthma exacerbation; machine learning; pediatric asthma; predictive modeling; systematic review",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41111683/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Pediatrics; Pulmonology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "Key challenges include algorithmic bias mitigation, prospective validation, and standardization of outcome metrics. Future research should prioritize equitable model development and clinical integration.",
      "ft_include": true,
      "ft_reason": "Included: substantial bias/fairness methodology content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12531338"
    },
    {
      "pmid": "41127255",
      "title": "Ethical sourcing in the context of health data supply chain management: a value sensitive design approach.",
      "abstract": "OBJECTIVE: The Bridge2AI program is establishing rules of practice for creating ethically sourced health data repositories to support the effective use of ML/AI in biomedical and behavioral research. Given the initially undefined nature of ethically sourced data, this work concurrently developed definitions and guidelines alongside repository creation, grounded in a practical, operational framework. MATERIALS AND METHODS: A Value Sensitive Design (VSD) approach was used to explore ethical tensions across stages of health data repository development. The conceptual investigation drew from supply chain management (SCM) processes to (1) identify actors who would interact with or be affected by the data repository use and outcomes; (2) determine what values to consider (ie, traceability accountability, security); and (3) analyze and document value trade-offs (ie, balancing risks of harm to improvements in healthcare). This SCM framework provides operational guidance for managing complex, multi-source data flows with embedded bias mitigation strategies. RESULTS: This conceptual investigation identified the actors, values, and tensions that influence ethical sourcing when creating a health data repository. The SCM steps provide a scaffolding to support ethical sourcing across the pre-model stages of health data repository development. Ethical sourcing includes documenting data provenance, articulating expectations for experts, and practices for ensuring data privacy, equity, and public benefit. Challenges include risks of ethics washing and highlight the need for transparent, value-driven practices. DISCUSSION: Integrating VSD with SCM frameworks enables operationalization of ethical values, improving data integrity, mitigating biases, and enhancing trust. This approach highlights how foundational decisions influence repository quality and AI/ML system usability, addressing provenance, traceability, redundancy, and risk management central to ethical data sourcing. CONCLUSION: To create authentic, impactful health data repositories that serve public health goals, organizations must prioritize transparency, accountability, and operational frameworks like SCM that comprehensively address the complexities and risks inherent in data stewardship.",
      "journal": "JAMIA open",
      "year": "2025",
      "doi": "10.1093/jamiaopen/ooaf101",
      "authors": "Nebeker Camille et al.",
      "keywords": "artificial intelligence; ethically sourced; health data repository; machine learning; research ethics",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41127255/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Generative AI",
      "health_domain": "ICU/Critical Care; Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: To create authentic, impactful health data repositories that serve public health goals, organizations must prioritize transparency, accountability, and operational frameworks like SCM that comprehensively address the complexities and risks inherent in data stewardship.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12539179"
    },
    {
      "pmid": "41174617",
      "title": "Joint explainable and fair AI in healthcare.",
      "abstract": "The nature of decisions in the healthcare domain necessitates accurate, interpretable, and reliable AI solutions. Explanation Guided Learning (EGL) explores the integration of explanation annotations into learning models to align human and model explanations. In this paper, we propose Explanation Constraints Guided Learning (ECGL), a novel approach inspired by the augmented Lagrangian method that integrates domain-specific explanation constraints directly into model training. The goal is to enhance both predictive accuracy and interpretability, making machine learning models more trustworthy. Experimental results on both tabular and image datasets demonstrate that ECGL maintains high accuracy while incorporating fairness and interpretability constraints. Specifically, ECGL improves predictive accuracy on the diabetes dataset compared to the base model and enhances feature alignment, with SHAP analysis. On average, a 36.8% increase in SHAP importance demonstrates that ECGL effectively aligns model explanations with domain knowledge. Furthermore, ECGL improves the identification of clinically significant regions in pneumonia X-ray images, as validated by both improved Equalized Odds Ratio (EOR) and GradCAM visualizations. ECGL achieves a 13% improvement in the EOR fairness metric, indicating better consistency of predictive performance across different groups. These results confirm that ECGL successfully balances performance, fairness, and interpretability, positioning it as a promising approach for trustworthy healthcare AI applications.",
      "journal": "BMC medical informatics and decision making",
      "year": "2025",
      "doi": "10.1186/s12911-025-03233-y",
      "authors": "Shahbazian Reza et al.",
      "keywords": "Artificial intelligence; Augmented lagrangian; Explainable guided learning (EGL); Fairness; Healthcare",
      "mesh_terms": "Humans; Machine Learning; Medical Informatics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41174617/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Cardiology; Pulmonology; Endocrinology/Diabetes",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "ECGL achieves a 13% improvement in the EOR fairness metric, indicating better consistency of predictive performance across different groups. These results confirm that ECGL successfully balances performance, fairness, and interpretability, positioning it as a promising approach for trustworthy healthcare AI applications.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12577397"
    },
    {
      "pmid": "41216267",
      "title": "Explainable Transfer Learning with Residual Attention BiLSTM for Prognosis of Ischemic Heart Disease.",
      "abstract": "BACKGROUND: Early and accurate prediction of Ischemic Heart Disease (IHD) is critical to reducing cardiovascular mortality through timely intervention. While deep learning (DL) models have shown promise in disease prediction, many lack interpretability, generalizability, and fairness-particularly when deployed across demographically diverse populations. These shortcomings limit clinical adoption and risk reinforcing healthcare disparities. METHODS: This study proposes a novel model: X-TLRABiLSTM (Explainable Transfer Learning-based Residual Attention Bidirectional LSTM). The architecture integrates transfer learning from pre-trained cardiovascular models into a BiLSTM framework with residual attention layers to improve temporal feature extraction and convergence. To ensure transparency, the model incorporates SHAP (SHapley Additive exPlanations) to quantify the contribution of each clinical feature to the final prediction. Additionally, a demographic reweighting strategy is applied to the training process to reduce bias across subgroups defined by age, gender, and ethnicity. The model was evaluated on the UCI Heart Disease dataset using 10-fold cross-validation. RESULTS: The X-TLRABiLSTM model achieved a classification accuracy of 98.2%, with an F1-score of 98.1% and an AUC of 99.1%, outperforming standard ML classifiers and state-of-the-art DL baselines. SHAP-based interpretability analysis highlighted clinically relevant predictors such as chest pain type, ST depression, and thalassemia. A fairness-aware reweighting strategy was applied during training, and fairness evaluation revealed minimal performance disparity across demographic subgroups, with F1-score gaps \u2264 0.6% and error rate gaps \u2264 0.4%. Confusion matrix analysis demonstrated low false-positive and false-negative rates, reinforcing the model's reliability for clinical deployment. CONCLUSIONS: X-TLRABiLSTM offers a highly accurate, interpretable, and demographically fair framework for IHD prognosis. By combining transfer learning, residual attention, explainable AI, and fairness-aware optimization, this model advances trustworthy AI in healthcare. Its successful performance on benchmark clinical data supports its potential for real-world integration in ethical, AI-assisted cardiovascular diagnostics.",
      "journal": "F1000Research",
      "year": "2025",
      "doi": "10.12688/f1000research.166307.3",
      "authors": "D Cenitta et al.",
      "keywords": "Bidirectional LSTM (BiLSTM); Clinical Decision Support Systems; Deep Learning; Demographic Bias Mitigation; Explainable AI (XAI); Fairness in AI; Ischemic Heart Disease (IHD); Residual Attention Mechanism; SHAP; Transfer Learning",
      "mesh_terms": "Humans; Myocardial Ischemia; Prognosis; Deep Learning; Male; Female; Middle Aged; Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41216267/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Cardiology; Mental Health/Psychiatry; ICU/Critical Care; Pain Management",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Transfer Learning; Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSIONS: X-TLRABiLSTM offers a highly accurate, interpretable, and demographically fair framework for IHD prognosis. By combining transfer learning, residual attention, explainable AI, and fairness-aware optimization, this model advances trustworthy AI in healthcare. Its successful performance on benchmark clinical data supports its potential for real-world integration in ethical, AI-assisted cardiovascular diagnostics.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12596549"
    },
    {
      "pmid": "41223209",
      "title": "Evaluating the impact of sex bias on AI models in musculoskeletal ultrasound of joint recess distension.",
      "abstract": "With the increasing integration of artificial intelligence (AI) in healthcare, concerns about bias in AI models have emerged, particularly regarding demographic factors. In medical imaging, biases in training datasets can significantly impact diagnostic accuracy, leading to unequal healthcare outcomes. This study assessed the impact of sex bias on AI models for diagnosing knee joint recess distension using ultrasound imaging. We utilized a retrospective dataset from community clinics across Canada, comprising 5,000 de-identified MSKUS images categorized by sex and clinical findings. Two binary convolutional neural network (BCNN) classifiers were developed to detect synovial recess distension and determine patient sex. The dataset was balanced across sex and joint recess distension, with models trained using advanced data augmentation and validated through both individual and mixed demographic scenarios using a 5-fold cross-validation strategy. Our BCNN classifiers showed that AI performance varied significantly based on the training data's demographic characteristics. Models trained exclusively on female datasets achieved higher sensitivity and accuracy but exhibited decreased specificity when applied to male images, suggesting a tendency to overfit female-specific features. Conversely, classifiers trained on balanced datasets displayed enhanced generalizability. This was evident from the classification heatmaps, which varied less between sexes, aligning more closely with clinically relevant features. The study highlights the critical influence of demographic biases on the diagnostic accuracy of AI models in medical imaging. Our results demonstrate the necessity for thorough cross-demographic validation and training on diverse datasets to mitigate biases. These findings are based on a supervised CNN model; evaluating whether they extend to other architectures, such as self-supervised learning (SSL) methods, foundation models, and Vision Transformers (ViTs), remains a direction for future research.",
      "journal": "PloS one",
      "year": "2025",
      "doi": "10.1371/journal.pone.0332716",
      "authors": "Mendez M et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Male; Ultrasonography; Retrospective Studies; Middle Aged; Artificial Intelligence; Knee Joint; Adult; Neural Networks, Computer; Sexism; Aged; Canada",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41223209/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; NLP/LLM; Neural Network; Computer Vision/Imaging AI; Foundation Model",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Data Augmentation; Diverse/Representative Data",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "Our results demonstrate the necessity for thorough cross-demographic validation and training on diverse datasets to mitigate biases. These findings are based on a supervised CNN model; evaluating whether they extend to other architectures, such as self-supervised learning (SSL) methods, foundation models, and Vision Transformers (ViTs), remains a direction for future research.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12611148"
    },
    {
      "pmid": "41244777",
      "title": "Reducing misdiagnosis in AI-driven medical diagnostics: a multidimensional framework for technical, ethical, and policy solutions.",
      "abstract": "PURPOSE: This study aims to systematically identify and address key barriers to misdiagnosis in AI-driven medical diagnostics. The main research question is how technical limitations, ethical concerns, and unclear accountability hinder safe and equitable use of AI in real-world clinical practice, and what integrated solutions can minimize errors and promote trust. METHODS: We conducted a literature review and case analysis across major medical fields, evaluating failure modes such as data pathology, algorithmic bias, and human-AI interaction. Based on these findings, we propose a multidimensional framework combining technical strategies-such as dynamic data auditing and explainability engines-with ethical and policy interventions, including federated learning for bias mitigation and blockchain-based accountability. RESULTS: Our analysis shows that misdiagnosis often results from data bias, lack of model transparency, and ambiguous responsibility. When applied to published case examples and comparative evaluations from the literature, elements of our framework are associated with improvements in diagnostic accuracy, transparency, and equity. Key recommendations include bias monitoring, real-time interpretability dashboards, and legal frameworks for shared accountability. CONCLUSION: A coordinated, multidimensional approach is essential to reduce the risk of misdiagnosis in AI-supported diagnostics. By integrating robust technical controls, clear ethical guidelines, and defined accountability, our framework provides a practical roadmap for responsible, transparent, and equitable AI adoption in healthcare-improving patient safety, clinician trust, and health equity.",
      "journal": "Frontiers in medicine",
      "year": "2025",
      "doi": "10.3389/fmed.2025.1594450",
      "authors": "Li Yue et al.",
      "keywords": "AI policy and regulation; artificial intelligence (AI) diagnostics; ethical responsibility; misdiagnosis risk; patient safety and trust",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41244777/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Federated Learning",
      "health_domain": "Pathology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Federated Learning; Explainability/Interpretability; Bias Auditing Framework",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "CONCLUSION: A coordinated, multidimensional approach is essential to reduce the risk of misdiagnosis in AI-supported diagnostics. By integrating robust technical controls, clear ethical guidelines, and defined accountability, our framework provides a practical roadmap for responsible, transparent, and equitable AI adoption in healthcare-improving patient safety, clinician trust, and health equity.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12615213"
    },
    {
      "pmid": "41256428",
      "title": "A Systematic Fairness Evaluation of Racial Bias in Alzheimer's Disease Diagnosis Using Machine Learning Models.",
      "abstract": "INTRODUCTION: Alzheimer's disease (AD) is a major global health concern, expected to affect 12.7 million Americans by 2050. Machine learning (ML) algorithms have been developed for AD diagnosis and progression prediction, but the lack of racial diversity in clinical datasets raises concerns about their generalizability across demographic groups, particularly underrepresented populations. Studies show ML algorithms can inherit biases from data, leading to biased AD predictions. METHODS: This study investigates the fairness of ML models in AD diagnosis. We hypothesize that models trained on a single racial group perform well within that group but poorly in others. We employ feature selection and model training techniques to improve fairness. RESULTS: Our findings support our hypothesis that ML models trained on one group underperform on others. We also demonstrated that applying fairness techniques to ML models reduces their bias. DISCUSSION: This study highlights the need for racial diversity in datasets and fair models for AD prediction.",
      "journal": "bioRxiv : the preprint server for biology",
      "year": "2025",
      "doi": "10.1101/2025.09.30.678854",
      "authors": "Baddam Neha Goud et al.",
      "keywords": "Algorithmic Bias; Alzheimer\u2019s Disease; Fairness; Machine Learning",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41256428/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Neurology",
      "bias_axes": "Race/Ethnicity",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "RESULTS: Our findings support our hypothesis that ML models trained on one group underperform on others. We also demonstrated that applying fairness techniques to ML models reduces their bias. DISCUSSION: This study highlights the need for racial diversity in datasets and fair models for AD prediction.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12622001"
    },
    {
      "pmid": "41282840",
      "title": "Nonfasting, Telehealth-Ready LDL-C Testing With Machine Learning to Improve Cardiovascular Access and Equity.",
      "abstract": "IMPORTANCE: Current LDL-C testing requires 9-12 hour fasting and in-person visits, creating an access crisis: 40% of lipid panels occur outside fasting windows (yielding unreliable results), 60% of US counties lack cardiology services, and millions of patients with diabetes cannot safely fast. Meanwhile, telehealth infrastructure expanded 38-fold post-COVID, yet lipid workflows remain anchored to 1970s protocols. This mismatch drives ~20 million unnecessary repeat visits annually, disproportionately burdening Medicaid populations, essential workers, and rural communities. OBJECTIVE: To demonstrate that machine learning can transform lipid testing from a fasting-dependent, clinic-based bottleneck into an accurate, equitable, telehealth-ready service-eliminating three structural barriers simultaneously: fasting requirements, in-person visits, and racial algorithmic bias. DESIGN SETTING AND PARTICIPANTS: Cross-sectional analysis of All of Us Research Program (n=3,477; test n=696). Crucially, 40.1% were tested outside traditional fasting windows, reflecting real-world practice. We evaluated performance stratified by fasting status, telehealth feasibility (labs-only configuration), racial equity metrics, and economic impact. MAIN OUTCOMES AND MEASURES: Primary: MAE and calibration in non-fasting states. Secondary: Labs-only non-inferiority (\u00b10.5 mg dL-1margin); racial equity (Black-White performance gap); economic savings from eliminated repeat visits; and classification accuracy at treatment thresholds (70, 100, 130 mg dL-1). RESULTS: The ML system demonstrated paradoxical superiority in non-fasting conditions-precisely when needed most. While equations deteriorated (Friedewald MAE 29.1 vs 25.9 mg dL-1fasting, slopes 0.58-0.61), ML maintained accuracy (24.0 vs 23.2 mg dL-1, slopes 0.99-1.07), achieving 17.2% improvement over Friedewald when non-fasting vs 10.4% fasting. Labs-only configuration proved non-inferior (MAE=-0.12, p<0.001), enabling national retail-pharmacy and home-testing workflows. The system achieved racial equity without race input (Black-White gap -0.19 mg dL-1, CI includes zero) while providing greatest improvement for Black patients (19% vs 11% for White). Economically, eliminating 4,000 repeat visits per 10,000 tests helps address an estimated $2 billion annual repeat-testing cost burden and yields $815,000 total savings per 10,000 tests ($245,000 direct healthcare, $570,000 patient costs), with break-even at just 750 tests. CONCLUSIONS AND RELEVANCE: This ML approach helps address an estimated $2 billion annual problem of repeat testing while tackling three critical quality gaps in cardiovascular prevention: delayed treatment initiation, poor monitoring adherence, and specialty access barriers. By enabling accurate non-fasting, telehealth-compatible, race-free LDL-C estimation, it transforms lipid testing from an access barrier into an access enabler-particularly for the Medicaid, Medicare Advantage, and rural populations who drive both cost and outcomes in value-based care. From a technical standpoint, implementation requires only routine labs and <100 ms computation, making deployment feasible with existing infrastructure.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.10.27.25338909",
      "authors": "Doku Ronald et al.",
      "keywords": "cardiovascular quality improvement; health equity; healthcare delivery; low-density lipoprotein cholesterol; machine learning; non-fasting lipid panel; telehealth; value-based care",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41282840/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; ICU/Critical Care; Endocrinology/Diabetes",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Geographic; Insurance Status",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Threshold Adjustment",
      "clinical_setting": "ICU; Public Health/Population; Telehealth/Remote",
      "key_findings": "RESULTS: The ML system demonstrated paradoxical superiority in non-fasting conditions-precisely when needed most. While equations deteriorated (Friedewald MAE 29.1 vs 25.9 mg dL-1fasting, slopes 0.58-0.61), ML maintained accuracy (24.0 vs 23.2 mg dL-1, slopes 0.99-1.07), achieving 17.2% improvement over Friedewald when non-fasting vs 10.4% fasting. Labs-only configuration proved non-inferior (MAE=-0.12, p<0.001), enabling national retail-pharmacy and home-testing workflows.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12636691"
    },
    {
      "pmid": "41292658",
      "title": "Develop and Validate A Fair Machine Learning Model to Indentify Patients with High Care-Continuity in Electronic Health Records Data.",
      "abstract": "OBJECTIVES: Electronic health record (EHR) data often missed care outside a given health system, resulting in data discontinuity. We aimed to: (1) quantify misclassification across levels of EHR data discontinuity and identify an optimal continuity threshold. (2) develop a machine learning (ML) model to predict EHR continuity and optimize fairness across racial and ethnic groups, and (3) externally validate the EHR continuity prediction model using an independent dataset. MATERIALS AND METHODS: We used linked OneFlorida+ EHR-Medicaid claims data for model development and REACHnet EHR-Louisiana Blue Cross Blue Shield (LABlue) claims data for external validation. A novel Harmonized Encounter Proportion Score (HEPS) was applied to quantify patient-level EHR data continuity and the impact on misclassification of 42 clinical variables. ML models were trained using routinely available demographic, clinical, and healthcare utilization features derived from structured EHR data. RESULTS: Higher EHR data continuity was associated with lower rates of misclassification. A HEPS threshold of approximately 30% effectively distinguished patients with sufficient data continuity. ML models demonstrated strong performance in predicting high continuity (AUROC=0.77). Fairness assessments showed bias against Hispanic group, which was substantially improved following bias mitigation procedures. Model performance remained robust and fair in the external validation. DISCUSSION: Our study offers a practical metric for quantifying care continuity in EHR networks. The current ML model incorporating EHR-routinely collected information can accurately identify patients with high care continuity. CONCLUSIONS: We developed a generalizable care-continuity classification tool that can be easily applied across EHR systems, strengthening the rigor of EHR-based research.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.11.11.25339938",
      "authors": "Lee Yao An et al.",
      "keywords": "Data continuity; EHR; Machine learning prediction",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41292658/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: We developed a generalizable care-continuity classification tool that can be easily applied across EHR systems, strengthening the rigor of EHR-based research.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12642717"
    },
    {
      "pmid": "41296741",
      "title": "Evaluating algorithmic fairness of machine learning models in predicting underweight, overweight, and adiposity across socioeconomic and caste groups in India: evidence from the longitudinal ageing study in India.",
      "abstract": "Machine learning (ML) models are increasingly applied to predict body mass index (BMI) and related outcomes, yet their fairness across socioeconomic and caste groups remains uncertain, particularly in contexts of structural inequality. Using nationally representative data from more than 55,000 adults aged 45 years and older in the Longitudinal Ageing Study in India (LASI), we evaluated the accuracy and fairness of multiple ML algorithms-including Random Forest, XGBoost, Gradient Boosting, LightGBM, Deep Neural Networks, and Deep Cross Networks-alongside logistic regression for predicting underweight, overweight, and central adiposity. Models were trained on 80% of the data and tested on 20%, with performance assessed using AUROC, accuracy, sensitivity, specificity, and precision. Fairness was evaluated through subgroup analyses across socioeconomic and caste groups and equity-based metrics such as Equalized Odds and Demographic Parity. Feature importance was examined using SHAP values, and bias-mitigation methods were implemented at pre-processing, in-processing, and post-processing stages. Tree-based models, particularly LightGBM and Gradient Boosting, achieved the highest AUROC values (0.79-0.84). Incorporating socioeconomic and health-related variables improved prediction, but fairness gaps persisted: performance declined for scheduled tribes and lower socioeconomic groups. SHAP analyses identified grip strength, gender, and residence as key drivers of prediction differences. Among mitigation strategies, Reject Option Classification and Equalized Odds Post-processing moderately reduced subgroup disparities but sometimes decreased overall performance, whereas other approaches yielded minimal gains. ML models can effectively predict obesity and adiposity risk in India, but addressing bias is essential for equitable application. Continued refinement of fairness-aware ML methods is needed to support inclusive and effective public-health decision-making.",
      "journal": "PLOS digital health",
      "year": "2025",
      "doi": "10.1371/journal.pdig.0000951",
      "authors": "Lee John Tayu et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41296741/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Random Forest; Logistic Regression; XGBoost/Gradient Boosting; Neural Network",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Data Preprocessing; Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability; Post-hoc Correction; Diverse/Representative Data",
      "clinical_setting": "ICU",
      "key_findings": "ML models can effectively predict obesity and adiposity risk in India, but addressing bias is essential for equitable application. Continued refinement of fairness-aware ML methods is needed to support inclusive and effective public-health decision-making.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12654920"
    },
    {
      "pmid": "41334074",
      "title": "Identifying Bias at Scale in Clinical Notes Using Large Language Models.",
      "abstract": "OBJECTIVE: To evaluate whether generative pretrained transformer (GPT)-4 can detect and revise biased language in emergency department (ED) notes, against human-adjudicated gold-standard labels, and to identify modifiable factors associated with biased documentation. PATIENTS AND METHODS: We randomly sampled 50,000 ED medical and nursing notes from the Mount Sinai Health System (January 1, 2023, to December 31, 2023). We also randomly sampled 500 discharge notes from the Medical Information Mart for Intensive Care IV database. The GPT-4 flagged 4 types of bias: discrediting, stigmatizing/labeling, judgmental, and stereotyping. Two human reviewers verified model detections. We used multivariable logistic regression to examine associations between bias and health care utilization, presenting problems (eg, substance use), shift timing, and provider type. We then asked physicians to rate GPT-4's proposed language revisions on a 10-point scale. RESULTS: The GPT-4 showed 97.6% sensitivity and 85.7% specificity compared with the human review. Biased language appeared in 6.5% (3229 of 50,000) of Mount Sinai notes and 7.4% (37 of 500) of Medical Information Mart for Intensive Care IV notes. In adjusted models, frequent health care utilization (adjusted odds ratio [aOR], 2.85; 95% CI, 1.95-4.17), substance use presentations (aOR, 3.09; 95% CI, 2.51-3.80), and overnight shifts (aOR, 1.37; 95% CI, 1.23-1.52) showed elevated odds of biased documentation. Physicians were more likely to include bias than nurses (aOR, 2.26; 95% CI, 2.07-2.46); GPT-4's recommended revisions received mean physician ratings above 9 of 10. CONCLUSION: The study showed that GPT-4 accurately detects biased language in clinical notes, identifies modifiable contributors to that bias, and delivers physician-endorsed revisions. This approach may help mitigate documentation bias and reduce disparities in care.",
      "journal": "Mayo Clinic proceedings. Digital health",
      "year": "2025",
      "doi": "10.1016/j.mcpdig.2025.100296",
      "authors": "Apakama Donald U et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41334074/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "NLP/LLM; Logistic Regression",
      "health_domain": "Emergency Medicine; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Emergency Department",
      "key_findings": "CONCLUSION: The study showed that GPT-4 accurately detects biased language in clinical notes, identifies modifiable contributors to that bias, and delivers physician-endorsed revisions. This approach may help mitigate documentation bias and reduce disparities in care.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12666851"
    },
    {
      "pmid": "41334247",
      "title": "Evaluating sociodemographic bias in a deployed machine-learned patient deterioration model.",
      "abstract": "BACKGROUND: Bias evaluations of machine learning (ML) models often focus on performance in research settings, with limited assessment of downstream bias following clinical deployment. The objective of this study was to evaluate whether CHARTwatch, a real-time ML early warning system for inpatient deterioration, demonstrated algorithmic bias in model performance, or produced disparities in care processes, and outcomes across patient sociodemographic groups. METHODS: We evaluated CHARTwatch implementation on the internal medicine service at a large academic hospital. Patient outcomes during the intervention period (November 1, 2020-June 1, 2022) were compared to the control period (November 1, 2016-December 31, 2019) using propensity score overlap weighting. We evaluated differences across key sociodemographic subgroups, including age, sex, homelessness, and neighborhood-level socioeconomic and racialized composition. Outcomes included model performance (sensitivity and specificity), processes of care, and patient outcomes (non-palliative in-hospital death). RESULTS: Among 12\u2009877 patients (9079 control, 3798 intervention), 13.3% were experiencing homelessness and 36.9% lived in the quintile with the highest neighborhood racialized and newcomer populations. Model sensitivity was 70.1% overall, with no significant variation across subgroups. Model specificity varied by age, <60 years: 93% (95% Confidence Interval [CI] 91-95%), 60-80 years: 90% (95%CI 87-92%), and >80 years: 84% (95%CI 79-88%), P\u2009<\u2009.001, but not other subgroups. CHARTwatch implementation was associated with an increase in code status documentation among patients experiencing homelessness, without significant differences in other care processes or outcomes. CONCLUSION: CHARTwatch model performance and impact were generally consistent across measured sociodemographic subgroups. ML-based clinical decision support tools, and associated standardization of care, may reduce existing inequities, as was observed for code status orders among patients experiencing homelessness. This evaluation provides a framework for future bias assessments of deployed ML-CDS tools.",
      "journal": "JAMIA open",
      "year": "2025",
      "doi": "10.1093/jamiaopen/ooaf158",
      "authors": "Colacci Michael et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41334247/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Decision Support",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; Public Health/Population",
      "key_findings": "CONCLUSION: CHARTwatch model performance and impact were generally consistent across measured sociodemographic subgroups. ML-based clinical decision support tools, and associated standardization of care, may reduce existing inequities, as was observed for code status orders among patients experiencing homelessness. This evaluation provides a framework for future bias assessments of deployed ML-CDS tools.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12668680"
    },
    {
      "pmid": "41336319",
      "title": "Comparative assessment of fairness definitions and bias mitigation strategies in machine learning-based diagnosis of Alzheimer's disease from MR images.",
      "abstract": "The present study performs a comprehensive fairness analysis of machine learning (ML) models for the diagnosis of Mild Cognitive Impairment (MCI) and Alzheimer's disease (AD) from MRI-derived neuroimaging features. Biases associated with age, race, and gender in a multi-cohort dataset, as well as the influence of proxy features encoding these sensitive attributes, are investigated. The reliability of various fairness definitions and metrics in the identification of such biases is also assessed. Based on the most appropriate fairness measures, a comparative analysis of widely used pre-processing, in-processing, and post-processing bias mitigation strategies is performed. Moreover, a novel composite measure is introduced to quantify the trade-off between fairness and performance by considering the F1-score and the equalized odds ratio, making it appropriate for medical diagnostic applications. The obtained results reveal the existence of biases related to age and race, while no significant gender bias is observed. The deployed mitigation strategies yield varying improvements in terms of fairness across the different sensitive attributes and studied subproblems. For race and gender, Reject Option Classification improves equalized odds by 46% and 57%, respectively, and achieves harmonic mean scores of 0.75 and 0.80 in the MCI versus AD subproblem, whereas for age, in the same subproblem, adversarial debiasing yields the highest equalized odds improvement of 40% with a harmonic mean score of 0.69. Insights are provided into how variations in AD neuropathology and risk factors, associated with demographic characteristics, influence model fairness.",
      "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference",
      "year": "2025",
      "doi": "10.1109/EMBC58623.2025.11254128",
      "authors": "Vlontzou Maria Eleftheria et al.",
      "keywords": "",
      "mesh_terms": "Alzheimer Disease; Humans; Machine Learning; Magnetic Resonance Imaging; Male; Female; Aged; Cognitive Dysfunction; Bias; Aged, 80 and over",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41336319/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Pathology; Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Disability",
      "lifecycle_stage": "Data Preprocessing; Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Adversarial Debiasing; Fairness Metrics Evaluation; Post-hoc Correction",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "For race and gender, Reject Option Classification improves equalized odds by 46% and 57%, respectively, and achieves harmonic mean scores of 0.75 and 0.80 in the MCI versus AD subproblem, whereas for age, in the same subproblem, adversarial debiasing yields the highest equalized odds improvement of 40% with a harmonic mean score of 0.69. Insights are provided into how variations in AD neuropathology and risk factors, associated with demographic characteristics, influence model fairness.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41336889",
      "title": "Enhancing Fairness in Ultrasound Imaging: Evaluating Adversarial Debiasing Across Diverse Patient Demographics.",
      "abstract": "This paper explores the use of adversarial debiasing algorithms to mitigate bias in ultrasound imaging datasets, focusing on breast and lung images. The study evaluates fairness metrics like area under the curve (AUC), False Positive Rate (FPR), False Negative Rate (FNR), and demographic parity to assess the impact of debiasing using the recently published MEDFAIR framework. While debiasing improves fairness overall, disparities remain in certain subgroups, such as age in the breast dataset and sex in the lung dataset. The paper also compares artificial intelligence (AI) models (ResNet18, AlexNet, VGG16, MobileNetV2, DenseNet121), revealing differences in susceptibility to bias and effectiveness post-debiasing. These findings underscore the challenges of achieving full fairness in AI-driven medical imaging and highlight the need for continued refinement of debiasing methods to ensure equitable outcomes across diverse patient populations.",
      "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference",
      "year": "2025",
      "doi": "10.1109/EMBC58623.2025.11252877",
      "authors": "Kiani Parmiss et al.",
      "keywords": "",
      "mesh_terms": "Humans; Algorithms; Ultrasonography; Female; Artificial Intelligence; Male; Lung; Demography; Image Processing, Computer-Assisted; Breast",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41336889/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Pulmonology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Adversarial Debiasing; Fairness Metrics Evaluation",
      "clinical_setting": "Public Health/Population",
      "key_findings": "The paper also compares artificial intelligence (AI) models (ResNet18, AlexNet, VGG16, MobileNetV2, DenseNet121), revealing differences in susceptibility to bias and effectiveness post-debiasing. These findings underscore the challenges of achieving full fairness in AI-driven medical imaging and highlight the need for continued refinement of debiasing methods to ensure equitable outcomes across diverse patient populations.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41351059",
      "title": "A fairness-aware machine learning framework for maternal health in Ghana: integrating explainability, bias mitigation, and causal inference for ethical AI deployment.",
      "abstract": "BACKGROUND: Antenatal care (ANC) uptake in Ghana remains inequitable, with socioeconomic and geographic disparities limiting progress toward universal maternal health coverage (SDG 3). We present a novel, fairness-aware machine learning framework for predicting antenatal care uptake among women in Ghana, integrating explainability, bias mitigation, and causal inference to support ethical artificial intelligence (AI) deployment in low- and middle-income countries. METHODS: Using the 2022 Ghana Demographic and Health Survey (n\u2009=\u20093,314 eligible women with a recent live birth), we applied multiple imputation by chained equations (m\u2009=\u200910), appropriate categorical encoding, and synthetic minority oversampling (SMOTE) within training folds. Four supervised models (logistic regression, random forest, XGBoost, support vector machine) underwent stratified 5\u2011fold nested cross\u2011validation with cost\u2011sensitive threshold optimization (selected probability threshold\u2009=\u20090.45). Explainability (SHAP), fairness auditing (AIF360; metrics: statistical parity difference, disparate impact, equal opportunity difference, average odds difference, theil index), preprocessing mitigation (reweighing), counterfactual explanations (DiCE), and cautious treatment effect estimation (causal forests within a double machine learning framework) were integrated. Performance metrics included accuracy, precision, recall, F1, ROC\u2011AUC, minority class PR\u2011AUC, balanced accuracy, calibration (Brier score), and decision curve net benefit. RESULTS: The optimized random forest model achieved the highest accuracy (0.68) and recall (0.84) in identifying women with inadequate ANC contacts. Calibration was strong, with a brier score of 0.158, a calibration slope of 0.97, and an intercept of \u2212\u20090.02. Fairness auditing revealed baseline disparities in model predictions across wealth, region, ethnicity, and religion, with a statistical parity difference for wealth status of 0.182 and a Disparate Impact of 1.62. Following reweighting, disparate impact improved into the fairness range (0.92; within the recommended 0.8\u20131.25 interval), and statistical parity difference reduced to \u2212\u20090.028. Counterfactual analysis indicated that education, wealth, media exposure, and health worker contacts were the most modifiable factors for improving ANC uptake. Exploratory causal inference using double machine learning suggested that improving wealth status and education could be associated with a 16% (Average Treatment Effect [ATE]\u2009=\u20090.163) and 14% (ATE\u2009=\u20090.142) increase, respectively, in the probability of adequate ANC, with greater effects observed among urban and educated subgroups. Adjusted odds ratio (AOR) analysis showed that women in the richest quintile were nearly twice as likely to receive adequate ANC (AOR\u2009=\u20091.91, 95% CI: 1.44\u20132.53; p\u2009<\u20090.001), while those in the poorest quintile had significantly lower odds (AOR\u2009=\u20090.58, 95% CI: 0.45\u20130.75; p\u2009<\u20090.001). Additional significant predictors included health insurance coverage (AOR\u2009=\u20091.74, 95% CI: 1.19\u20132.55), health worker contacts (AOR\u2009=\u20091.33, 95% CI: 1.11\u20131.58), and pregnancy intention (AOR\u2009=\u20091.54, 95% CI: 1.30\u20131.82). CONCLUSION: This integrated, fairness-aware machine learning framework suggest robust, equitable, and actionable prediction of ANC uptake among Ghanaian women. Key modifiable determinants include wealth, education, and healthcare access barriers. The framework offers a replicable, ethical blueprint for transparent and fair AI deployment in maternal health, supporting targeted interventions to advance universal access to quality care in Ghana. Policymakers and health managers can leverage these AI tools to identify high-risk women, monitor intervention impacts, and allocate resources more equitably, advancing progress toward universal access to quality maternal care in Ghana.",
      "journal": "BioData mining",
      "year": "2025",
      "doi": "10.1186/s13040-025-00505-1",
      "authors": "Osborne Augustus et al.",
      "keywords": "Antenatal care; Causal forests; Counterfactuals; Explainable AI; Fairness; Ghana; Health equity; Machine learning; Maternal health",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41351059/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Random Forest; Logistic Regression; XGBoost/Gradient Boosting; Support Vector Machine",
      "health_domain": "Obstetrics/Maternal Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Geographic; Insurance Status",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Calibration; Threshold Adjustment; Fairness Metrics Evaluation; Counterfactual Fairness; Explainability/Interpretability; Bias Auditing Framework",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: This integrated, fairness-aware machine learning framework suggest robust, equitable, and actionable prediction of ANC uptake among Ghanaian women. Key modifiable determinants include wealth, education, and healthcare access barriers. The framework offers a replicable, ethical blueprint for transparent and fair AI deployment in maternal health, supporting targeted interventions to advance universal access to quality care in Ghana.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12781815"
    },
    {
      "pmid": "41354230",
      "title": "Development and validation of a novel machine learning-based algorithm to predict incident atrial fibrillation: A multicohort analysis.",
      "abstract": "BACKGROUND: Existing atrial fibrillation (AF) risk prediction models incorporate race as a covariate, systematically underestimating AF risk in black individuals and potentially perpetuating health care disparities. OBJECTIVE: This study aimed to develop and validate machine learning (ML)-based race-agnostic risk scores to predict AF risk and assess differences in risk stratification and bias compared with the CHARGE-AF score. METHODS: The derivation cohort included 16,719 participants free of AF at baseline (Atherosclerosis Risk in Communities visit 5, 2011-2013; Cardiovascular Health Study baseline, 1989-1990), and the validation cohort included 13,928 (Multi-Ethnic Study of Atherosclerosis and Framingham Offspring and Generation 3 studies). The primary outcome was the incidence of AF within 5 years. Model performance was assessed using concordance index, Brier score, and index of prediction accuracy. Bias was evaluated using disparate impact, equal opportunity difference, and Theil index. Population-attributable risk percentage was calculated across racial groups. RESULTS: During the 5-year follow-up, incident AF occurred in 507 participants (3.0%) in the derivation cohort and 262 (1.9%) in the validation cohort. The ML model demonstrated superior performance compared with CHARGE-AF, with better discrimination (concordance index 0.83 [95% confidence interval 0.80-0.85] vs 0.77 [95% confidence interval 0.74-0.79]; P < .001) and improved calibration (Brier score 1.82 vs 1.92; P < .001). Key predictors included age, clinical factors (electrocardiographic parameters, cardiac biomarkers, and blood pressure), and education level. Population-attributable risk analysis demonstrated marked racial differences in AF risk contribution from age (non-Hispanic black 14.3% vs white participants 34.6%). The ML model reduced algorithmic bias vs CHARGE-AF across all metrics. CONCLUSION: Race-agnostic ML models demonstrated superior predictive performance and reduced bias compared with CHARGE-AF, potentially improving clinical risk stratification while promoting health equity.",
      "journal": "Heart rhythm",
      "year": "2025",
      "doi": "10.1016/j.hrthm.2025.12.008",
      "authors": "Segar Matthew W et al.",
      "keywords": "Algorithmic bias; Atrial fibrillation; Machine learning; Race; Risk prediction",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41354230/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Cardiology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Fairness Metrics Evaluation",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: Race-agnostic ML models demonstrated superior predictive performance and reduced bias compared with CHARGE-AF, potentially improving clinical risk stratification while promoting health equity.",
      "ft_include": true,
      "ft_reason": "Included: substantial approach content in abstract",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41355748",
      "title": "Detecting Sociodemographic Biases in the Content and Quality of Large Language Model-Generated Nursing Care: Cross-Sectional Simulation Study.",
      "abstract": "BACKGROUND: Large language models (LLMs) are increasingly applied in health care. However, concerns remain that their nursing care recommendations may reflect patients' sociodemographic attributes rather than clinical needs. While this risk is acknowledged, there is a lack of empirical evidence evaluating sociodemographic bias in LLM-generated nursing care plans. OBJECTIVE: To investigate potential biases in nursing care plans generated by LLMs, we focused on whether outputs differ systematically based on patients' sociodemographic characteristics and assessed the implications for equitable nursing care. METHODS: We used a mixed methods simulation study. A standardized clinical vignette experiment was used to prompt GPT-4 to generate 9600 nursing care plans for 96 patient profiles with varying sociodemographic characteristics (eg, sex, age, income, education, and residence). We first conducted a quantitative analysis of all plans, assessing variations in thematic content. Subsequently, a panel of senior nursing experts evaluated the clinical quality (eg, safety, applicability, and completeness) of a stratified subsample of 500 plans. RESULTS: We analyzed 9600 LLM-generated nursing care plans and identified 8 consistent themes. Communication and Education (99.98%) and Emotional Support (99.97%) were nearly universal, while Nurse Training and Event Analysis were least frequent (39.3%). Multivariable analyses revealed systematic sociodemographic disparities. Care plans generated for low-income patient profiles were less likely to include the theme Environmental Adjustment (adjusted relative risk [aRR] 0.90). Profiles with lower education were associated with an increased likelihood of including Family Support (aRR 1.10). Similarly, plans generated for older patient profiles were more likely to contain recommendations for Pain Management (aRR 1.33) and Family Support (aRR 1.62) but were less likely to mention Nurse Training (aRR 0.78). Sex and regional differences were also significant. Expert review of 500 plans showed high overall quality (mean 4.47), with strong interrater reliability (\u03ba=0.76-0.81). However, urban profiles had higher completeness (\u03b2=.22) and applicability (\u03b2=.14) but lower safety scores (\u03b2=-0.09). These findings demonstrate that LLM-generated care plans exhibit systematic sociodemographic bias, raising important implications for fairness and safe deployment in nursing practice. CONCLUSIONS: This study identified that LLMs systematically reproduce sociodemographic biases in the generation of nursing care plans. These biases appear in two forms: they shape the thematic content and influence expert-rated clinical quality. These findings reveal a substantial risk that such models may reinforce existing health inequities. To our knowledge, this is the first empirical evidence documenting these nuanced biases in nursing. The study also contributes a replicable framework for evaluating LLM-generated care plans. Finally, it underscores the critical need for robust human oversight to ensure that artificial intelligence serves as a tool for advancing equity rather than perpetuating disparities.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/78132",
      "authors": "Bai Nan et al.",
      "keywords": "biases; health care equity; large language models; mixed methods study; nursing care; sociodemographic",
      "mesh_terms": "Humans; Cross-Sectional Studies; Female; Male; Language; Nursing Care; Middle Aged; Adult; Aged; Sociodemographic Factors; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41355748/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Pain Management",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Language; Geographic",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: This study identified that LLMs systematically reproduce sociodemographic biases in the generation of nursing care plans. These biases appear in two forms: they shape the thematic content and influence expert-rated clinical quality. These findings reveal a substantial risk that such models may reinforce existing health inequities.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12683325"
    },
    {
      "pmid": "41356360",
      "title": "Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data.",
      "abstract": "Achieving robust performance and fairness across diverse patient populations remains a central challenge in developing clinically deployable deep learning models for diagnostic imaging. Synthetic data generation has emerged as a promising strategy to address current limitations in dataset scale and diversity. In this study, we introduce RoentGen-v2, a state-of-the-art text-to-image diffusion model for chest radiographs that enables fine-grained control over both radiographic findings and patient demographic attributes, including sex, age, and race/ethnicity. RoentGen-v2 is the first model to generate clinically plausible chest radiographs with explicit demographic conditioning, facilitating the creation of a large, demographically balanced synthetic dataset comprising over 565,000 images. We use this large synthetic dataset to evaluate optimal training pipelines for downstream disease classification models. In contrast to prior work that combines real and synthetic data naively, we propose an improved training strategy that leverages synthetic data for supervised pretraining, followed by fine-tuning on real data. Through extensive evaluation on over 137,000 held-out chest radiographs from five institutions, we demonstrate that synthetic pretraining consistently improves model performance, generalization to out-of-distribution settings, and fairness across demographic subgroups defined across varying fairness metrics. Across datasets, synthetic pretraining led to a 6.5% accuracy increase in the performance of downstream classification models, compared to a modest 2.7% increase when naively combining real and synthetic data. We observe this performance improvement simultaneously with the reduction of the underdiagnosis fairness gap by 19.3%, with marked improvements across intersectional subgroups of sex, age, and race/ethnicity. Our proposed data-centric training approach that combines high-fidelity synthetic training data with multi-stage training pipelines is label-efficient, reducing reliance on large quantities of annotated real data. These results highlight the potential of demographically controllable synthetic imaging to advance equitable and generalizable medical deep learning under real-world data constraints. We open source our code, trained models, and synthetic dataset.",
      "journal": "Research square",
      "year": "2025",
      "doi": "10.21203/rs.3.rs-7687810/v1",
      "authors": "Moroianu Stefania L et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41356360/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Intersectional",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Data Augmentation; Fairness Metrics Evaluation; Transfer Learning",
      "clinical_setting": "Public Health/Population",
      "key_findings": "These results highlight the potential of demographically controllable synthetic imaging to advance equitable and generalizable medical deep learning under real-world data constraints. We open source our code, trained models, and synthetic dataset.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12676388"
    },
    {
      "pmid": "41359958",
      "title": "Critical Appraisal Tools for Evaluating Artificial Intelligence in Clinical Studies: Scoping Review.",
      "abstract": "BACKGROUND: Health research that uses predictive and generative artificial intelligence (AI) is rapidly growing. As in traditional clinical studies, the way in which AI studies are conducted can introduce systematic errors. The translation of this AI evidence into clinical practice and research needs critical appraisal tools for clinical decision-makers and researchers. OBJECTIVE: This study aimed to identify existing tools for the critical appraisal of clinical studies that use AI and to examine the concepts and domains these tools explore. The research question was framed using the Population-Concept-Context (PCC) framework. Population (P): AI clinical studies; Concept (C): tools for critical appraisal and associated constructs such as quality, reporting, validity, risk of bias, and applicability; and context (C): clinical practice. In addition, studies on bias classification and chatbot assessment were included. METHODS: We searched medical and engineering databases (MEDLINE, Embase, CINAHL, PsycINFO, and IEEE) from inception to April 2024. We included clinical primary research with tools for critical appraisal. Classical reviews and systematic reviews were included in the first phase of screening and excluded in the secondary phase after identifying new tools by forward snowballing. We excluded nonhuman, computer, and mathematical research, and letters, opinion papers, and editorials. We used Rayyan (Qatar Computing Research Institute) for screening. Data extraction was done by two reviewers, and discrepancies were resolved through discussion. The protocol was previously registered in Open Science Framework. We adhered to the PRISMA-ScR (Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews) and the PRISMA-S (PRISMA-Search) extension for reporting literature in systematic reviews. RESULTS: We retrieved 4393 unique records for screening. After excluding 3803 records, 119 were selected for full-text screening. From these, 59 were excluded. After inclusion of 10 studies via other methods, a total of 70 records were finally included. We found 46 tools (26 guides for reporting AI studies, 16 tools for critical appraisal, 2 for study quality, and 2 for risk of bias). Nine papers focused on bias classification or mitigation. We found 15 chatbot assessment studies or systematic reviews of chatbot studies (6 and 9, respectively), which are a very heterogeneous group. CONCLUSIONS: The results picture a landscape of evidence tools where reporting tools predominate, followed by critical appraisal, and a few tools for risk of bias. The mismatch of bias in AI and epidemiology should be considered for critical appraisal, especially regarding fairness and bias mitigation in AI. Finally, chatbot assessment studies represent a vast and evolving field in which progress in design, reporting, and critical appraisal is necessary and urgent.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/77110",
      "authors": "Cabello Juan B et al.",
      "keywords": "artificial intelligence; critical appraisal tools; reporting guides; risk of bias; scoping review",
      "mesh_terms": "Artificial Intelligence; Humans",
      "pub_types": "Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41359958/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Public Health",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: The results picture a landscape of evidence tools where reporting tools predominate, followed by critical appraisal, and a few tools for risk of bias. The mismatch of bias in AI and epidemiology should be considered for critical appraisal, especially regarding fairness and bias mitigation in AI. Finally, chatbot assessment studies represent a vast and evolving field in which progress in design, reporting, and critical appraisal is necessary and urgent.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12685289"
    },
    {
      "pmid": "41372566",
      "title": "Potential for Algorithmic Bias in Clinical Decision Instrument Development.",
      "abstract": "Clinical decision instruments (CDIs) face an equity dilemma. They reduce disparities in patient care through data-driven standardization of best practices. However, this standardization may perpetuate bias and inequality within healthcare systems. We perform a quantitative, systematic review to characterize four potential sources of bias in the development of 690 CDIs. We find evidence for potential algorithmic bias in CDI development through various analyses: self-reported participant demographics are skewed-e.g. 73% of participants are White, 55% are male; investigator teams are geographically skewed-e.g. 52% in North America, 31% in Europe; CDIs use predictor variables that may be prone to bias-e.g. 1.9% (13/690) of CDIs use Race and Ethnicity; outcome definitions may introduce bias-e.g. 26% (177/690) of CDIs involve follow-up, which may skew representation based on socioeconomic status. As CDIs become increasingly prominent in medicine, we recommend that these factors are considered during development and clearly conveyed to clinicians.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-02119-7",
      "authors": "Obra Jed Keenan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41372566/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Socioeconomic Status; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "26% (177/690) of CDIs involve follow-up, which may skew representation based on socioeconomic status. As CDIs become increasingly prominent in medicine, we recommend that these factors are considered during development and clearly conveyed to clinicians.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12708808"
    },
    {
      "pmid": "41372617",
      "title": "The future of algorithmic nondiscrimination compliance in the affordable care act.",
      "abstract": "A new Section 1557 rule bans discrimination by AI-based clinical decision tools, with compliance required by May 2025. This paper explores challenges in identifying and mitigating algorithmic bias, especially where outcome disparities exist. We emphasize the need to audit high-risk tools, address proxy discrimination, and provide standardized guidance. Political uncertainty around enforcement complicates long-term planning, making expanded regulatory support essential for health systems and developers alike.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-02224-7",
      "authors": "La Cava William G et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41372617/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Bias Auditing Framework",
      "clinical_setting": "Not specified",
      "key_findings": "We emphasize the need to audit high-risk tools, address proxy discrimination, and provide standardized guidance. Political uncertainty around enforcement complicates long-term planning, making expanded regulatory support essential for health systems and developers alike.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12804706"
    },
    {
      "pmid": "41382245",
      "title": "Automating ACMG variant classifications with BIAS-2015 v2.1.1: algorithm analysis and benchmark against the FDA-approved eRepo dataset.",
      "abstract": "BACKGROUND: In 2015, the American College of Medical Genetics and Genomics (ACMG), in collaboration with the Association of Molecular Pathologists (AMP), published guidelines for interpreting and classifying germline genomic variants. These guidelines defined five categories: benign, likely benign, uncertain significance, likely pathogenic, and pathogenic, with 28 criteria but no specific implementation algorithms. METHODS: Here we present Bitscopic Interpreting ACMG Standards 2015 (BIAS-2015 v2.1.1), an open-source software that automates the classification of variants based on 19 ACMG criteria while enabling user-defined weighting and manual adjustments for clinical contexts. BIAS-2015 supports high-throughput classification via command line, along with a web-based graphical user interface (GUI), enabling variant review, modification, and interactive curation. RESULTS: Using genomic data from the FDA-recognized ClinGen Evidence Repository (eRepo v2.2.0), we evaluated BIAS-2015\u2019s sensitivity, specificity, and F1 values with expert curation. BIAS-2015 demonstrated superior performance to InterVar, achieving a pathogenic sensitivity of 73.99% (vs. 64.31%), benign sensitivity of 80.23% (vs. 53.91%), and a 11x speed improvement, classifying 1,327 variants per second. CONCLUSIONS: BIAS-2015 provides an accurate, scalable, and transparent ACMG classification framework. By standardizing ACMG interpretation and providing transparent rule-based logic, BIAS-2015 enables reproducible and comparable variant classification across research and clinical laboratories. All code and the interactive variant curation platform are available on GitHub. https://github.com/bitscopic/BIAS-2015 SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1186/s13073-025-01581-y.",
      "journal": "Genome medicine",
      "year": "2025",
      "doi": "10.1186/s13073-025-01581-y",
      "authors": "Eisenhart Chris et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41382245/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: BIAS-2015 provides an accurate, scalable, and transparent ACMG classification framework. By standardizing ACMG interpretation and providing transparent rule-based logic, BIAS-2015 enables reproducible and comparable variant classification across research and clinical laboratories. All code and the interactive variant curation platform are available on GitHub.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12706976"
    },
    {
      "pmid": "41385778",
      "title": "Hype vs Reality in the Integration of Artificial Intelligence in Clinical Workflows.",
      "abstract": "Artificial intelligence (AI) has the capacity to transform health care by improving clinical decision-making, optimizing workflows, and enhancing patient outcomes. However, this potential remains limited by a complex set of technological, human, and ethical barriers that constrain its safe and equitable implementation. This paper argues for a holistic, systems-based approach to AI integration that addresses these challenges as interconnected rather than isolated. It identifies key technological barriers, including limited explainability, algorithmic bias, integration and interoperability issues, lack of generalizability, and difficulties in validation. Human factors such as resistance to change, insufficient stakeholder engagement, and education and resource constraints further impede adoption, whereas ethical and legal challenges related to liability, privacy, informed consent, and inequity compound these obstacles. Addressing these issues requires transparent model design, diverse datasets, participatory development, and adaptive governance. Recommendations emerging from this synthesis are as follows: (1) establish standardized international regulatory and governance frameworks; (2) promote multidisciplinary co-design involving clinicians, developers, and patients; (3) invest in clinician education, AI literacy, and continuous training; (4) ensure equitable resource allocation through dedicated funding and public-private partnerships; (5) prioritize multimodal, explainable, and ethically aligned AI development; and (6) focus on long-term evaluation of AI in real-world settings to ensure adaptive, transparent, and inclusive deployment. Adopting these measures can align innovation with accountability, enabling health care systems to harness AI's transformative potential responsibly and sustainably to advance patient care and health equity.",
      "journal": "JMIR formative research",
      "year": "2025",
      "doi": "10.2196/70921",
      "authors": "Abd-Alrazaq Alaa et al.",
      "keywords": "AI; artificial intelligence; challenges; clinical workflow; ethics; health care; human factors; regulation; solutions; technology",
      "mesh_terms": "Artificial Intelligence; Humans; Workflow",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41385778/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Intersectional",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability; Diverse/Representative Data",
      "clinical_setting": "ICU",
      "key_findings": "Recommendations emerging from this synthesis are as follows: (1) establish standardized international regulatory and governance frameworks; (2) promote multidisciplinary co-design involving clinicians, developers, and patients; (3) invest in clinician education, AI literacy, and continuous training; (4) ensure equitable resource allocation through dedicated funding and public-private partnerships; (5) prioritize multimodal, explainable, and ethically aligned AI development; and (6) focus on long...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12700513"
    },
    {
      "pmid": "41393018",
      "title": "Implicit bias in digital health: systematic biases in large language models' representation of global public health attitudes and challenges to health equity.",
      "abstract": "INTRODUCTION: As emerging instruments in digital health, large language models (LLMs) assimilate values and attitudes from human-generated data, thereby possessing the latent capacity to reflect public health perspectives. This study investigates into the representational biases of LLMs through the lens of health equity. We propose and empirically validate a three-dimensional explanatory framework encompassing Data Resources, Opinion Distribution, and Prompt Language, positing that prompts are not just communicative media but critical conduits that embed cultural context. METHODS: Utilizing a selection of prominent LLMs from the United States and China-namely Gemini 2.5 Pro, GPT-5, DeepSeek-V3, and Qwen 3. We conduct a systematic empirical analysis of their performance in representing health attitudes across diverse nations and demographic strata. RESULTS: Our findings demonstrate that: first, the accessibility of data resources is a primary determinant of an LLM's representational fidelity for internet users and nations with high internet penetration. Second, a greater consensus in public health opinion correlates with an increased propensity for the models to replicate the dominant viewpoint. Third, a significant \"native language association\" is observed, wherein Gemini 2.5 Pro and DeepSeek-V3 exhibit superior performance when prompted in their respective native languages. Conversely, models with enhanced multilingual proficiencies, such as GPT-5.0 and Qwen 3, display greater cross-lingual consistency. DISCUSSION: This paper not only quantifies the degree to which these leading LLMs reflect public health attitudes but also furnishes a robust analytical pathway for dissecting the underlying mechanisms of their representational biases. These findings bear profound implications for the advancement of health equity in the artificial intelligence era.",
      "journal": "Frontiers in public health",
      "year": "2025",
      "doi": "10.3389/fpubh.2025.1705082",
      "authors": "Gao Yuan et al.",
      "keywords": "algorithmic audit; digital health; health equity; large language models; representational bias",
      "mesh_terms": "Humans; Health Equity; Public Health; Language; China; Global Health; United States; Large Language Models; Digital Health",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41393018/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Public Health",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "RESULTS: Our findings demonstrate that: first, the accessibility of data resources is a primary determinant of an LLM's representational fidelity for internet users and nations with high internet penetration. Second, a greater consensus in public health opinion correlates with an increased propensity for the models to replicate the dominant viewpoint. Third, a significant \"native language association\" is observed, wherein Gemini 2.5 Pro and DeepSeek-V3 exhibit superior performance when prompted ...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12698530"
    },
    {
      "pmid": "41402410",
      "title": "SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models.",
      "abstract": "Automatic sleep staging with deep learning has advanced considerably, yet clinical adoption remains hindered by limited generalization, model bias, and inconsistent evaluation practices. We present SLEEPYLAND, an open-source framework comprising ~ 220,000\u2009h of in-domain and ~ 84,000\u2009h of out-of-domain polysomnographic recordings, spanning diverse ages, disorders, and hardware configurations. We release pre-trained state-of-the-art models, evaluating them across single- and multi-channel EEG/EOG setups. We introduce SOMNUS, an ensemble that integrates models via soft-voting, achieving robust performance across 24 datasets (macro-F1, 68.7-87.2%), outperforming individual models in 94.9% of cases and exceeding prior state-of-the-art. Exploiting the Bern-Sleep-Wake-Registry (N\u2009=\u20096633), we show that while SOMNUS improves generalization, no model architecture consistently minimizes model demographic/clinical bias. On multi-annotated datasets, SOMNUS surpasses the best human scorer (macro-F1, 85.2% vs 80.8% on DOD-H, and 80.2% vs 75.9% on DOD-O), more closely reproducing consensus. Finally, ensemble disagreement metrics predict scorer ambiguity (ROC-AUC 82.8%), providing reliable proxies for human uncertainty.",
      "journal": "NPJ digital medicine",
      "year": "2025",
      "doi": "10.1038/s41746-025-02237-2",
      "authors": "Rossi Alvise Dei et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41402410/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Deep Learning; Ensemble Methods",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Ensemble Methods",
      "clinical_setting": "Not specified",
      "key_findings": "On multi-annotated datasets, SOMNUS surpasses the best human scorer (macro-F1, 85.2% vs 80.8% on DOD-H, and 80.2% vs 75.9% on DOD-O), more closely reproducing consensus. Finally, ensemble disagreement metrics predict scorer ambiguity (ROC-AUC 82.8%), providing reliable proxies for human uncertainty.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12816009"
    },
    {
      "pmid": "41406939",
      "title": "Contrastive learning enhances fairness in pathology artificial intelligence systems.",
      "abstract": "AI-enhanced pathology evaluation systems hold significant potential to improve cancer diagnosis but frequently exhibit biases against underrepresented populations due to limited diversity in training data. Here, we present the Fairness-aware Artificial Intelligence Review for Pathology (FAIR-Path), a framework that leverages contrastive learning and weakly supervised machine learning to mitigate bias in AI-based pathology evaluation. In a pan-cancer AI fairness analysis spanning 20 cancer types, we identify significant performance disparities in 29.3% of diagnostic tasks across demographic groups defined by self-reported race, gender, and age. FAIR-Path effectively mitigates 88.5% of these disparities, with external validation showing a 91.1% reduction in performance gaps across 15 independent cohorts. We find that variations in somatic mutation prevalence among populations contribute to these performance disparities. FAIR-Path represents a promising step toward addressing fairness challenges in AI-powered pathology diagnoses and provides a robust framework for mitigating bias in medical AI applications.",
      "journal": "Cell reports. Medicine",
      "year": "2025",
      "doi": "10.1016/j.xcrm.2025.102527",
      "authors": "Lin Shih-Yen et al.",
      "keywords": "AI; algorithmic bias; artificial intelligence; bias mitigation; cancer diagnosis; contrastive learning; deep learning; fairness; pathology; weakly supervised learning",
      "mesh_terms": "Humans; Artificial Intelligence; Neoplasms; Male; Female; Pathology; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41406939/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; Pathology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Laboratory/Pathology",
      "key_findings": "We find that variations in somatic mutation prevalence among populations contribute to these performance disparities. FAIR-Path represents a promising step toward addressing fairness challenges in AI-powered pathology diagnoses and provides a robust framework for mitigating bias in medical AI applications.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12765949"
    },
    {
      "pmid": "41419484",
      "title": "Connecting algorithmic fairness and fair outcomes in a sociotechnical simulation case study of AI-assisted healthcare.",
      "abstract": "Artificial intelligence (AI) has vast potential for improving healthcare delivery, but concerns regarding biases in these systems have raised important questions regarding fairness when deployed clinically. Most prior studies on fairness in clinical AI focus solely on performance disparities between subpopulations, which often fall short of connecting the technical outputs of AI systems with sociotechnical outcomes. In this work, we present a simulation-based approach to explore how statistical definitions of algorithmic fairness translate to fairness in long-term outcomes, using AI-assisted breast cancer screening as a case example. We evaluate four fairness criteria and their impact on mortality rates and socioeconomic disparities, while also considering how clinical decision makers' reliance on AI and patients' access to healthcare affect outcomes. Our results highlight how algorithmic fairness does not directly translate into fair and equitable outcomes, underscoring the importance of integrating sociotechnical perspectives to gain a holistic understanding of fairness in healthcare AI.",
      "journal": "Nature communications",
      "year": "2025",
      "doi": "10.1038/s41467-025-67470-5",
      "authors": "Stanley Emma A M et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Breast Neoplasms; Algorithms; Female; Delivery of Health Care; Computer Simulation; Early Detection of Cancer",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41419484/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology",
      "bias_axes": "Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Subgroup Analysis",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We evaluate four fairness criteria and their impact on mortality rates and socioeconomic disparities, while also considering how clinical decision makers' reliance on AI and patients' access to healthcare affect outcomes. Our results highlight how algorithmic fairness does not directly translate into fair and equitable outcomes, underscoring the importance of integrating sociotechnical perspectives to gain a holistic understanding of fairness in healthcare AI.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12824401"
    },
    {
      "pmid": "41437965",
      "title": "Real-world evidence and the future of personalized medicine: a global perspective on data, ethics, and equity.",
      "abstract": "Real-World Evidence (RWE) is a critical enabler of personalized medicine (PM), offering granular insights into how interventions perform across diverse, real-life populations. This manuscript, grounded in over 30 years of health data science and regulatory experience, explores the evolving role of RWE in transforming healthcare delivery-from regulatory frameworks and policy alignment to artificial intelligence (AI)-enabled patient stratification. Through real-world case examples in oncology, ophthalmology, and dermatology, the article illustrates how digital tools and data integration can enhance patient-centred care. Each vignette concludes with an \"adoption path\" outlining data requirements, minimal IT changes, training, and payer-relevant endpoints. The discussion critically examines risks-such as bias, opacity in algorithms, and lack of harmonization-and translates them into a pre-deployment audit checklist and an equity checklist for subgroup performance and representativeness audits. To guide global regulatory practice, a \"regulatory pragmatics\" checklist is proposed, covering data quality, traceability, validation, transparency, and patient voice, including patient-generated health data (PGHD). Building on the Healthcare 5.0 vision, the manuscript aligns RWE with human-centric, sustainable, and resilient pillars, highlighting IoT wearables, environmental sensors, and continuous lifestyle data streams. Policy and implementation recommendations, together with a global convergence roadmap, position RWE as a strategic tool for regulators, payers, and clinicians. The paper concludes with a call for systemic accountability: industry must innovate responsibly, regulators must approve with foresight, payers must assess tools beyond medications, and health systems must bridge infrastructure gaps. Over the next 12-24 months, measurable commitments are required across all stakeholders to ensure that PM becomes everyday care. PM must serve patients-not just science, policy, or business-and that demands leadership grounded in scientific integrity and human empathy.",
      "journal": "Frontiers in health services",
      "year": "2025",
      "doi": "10.3389/frhs.2025.1682159",
      "authors": "Sagkriotis Alexandros",
      "keywords": "artificial intelligence; dermatology; digital health; health policy; oncology; ophthalmology; personalized medicine; real-world evidence",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41437965/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Dermatology; Oncology; Ophthalmology; Wearables/Remote Monitoring",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Insurance Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Bias Auditing Framework",
      "clinical_setting": "Public Health/Population; Telehealth/Remote",
      "key_findings": "Over the next 12-24 months, measurable commitments are required across all stakeholders to ensure that PM becomes everyday care. PM must serve patients-not just science, policy, or business-and that demands leadership grounded in scientific integrity and human empathy.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12719448"
    },
    {
      "pmid": "41459572",
      "title": "Machine learning-based mortality prediction in critically ill patients with hypertension: comparative analysis, fairness, and interpretability.",
      "abstract": "BACKGROUND: Hypertension is a leading global health concern, significantly contributing to cardiovascular, cerebrovascular, and renal diseases. In critically ill patients, hypertension poses increased risks of complications and mortality. Early and accurate mortality prediction in this population is essential for timely intervention and improved outcomes. Machine learning (ML) and deep learning (DL) approaches offer promising solutions by leveraging high-dimensional electronic health record (EHR) data. OBJECTIVE: To develop and evaluate ML and DL models for predicting in-hospital mortality in hypertensive patients using the MIMIC-IV critical care dataset, and to assess the fairness and interpretability of the models. METHODS: We developed four ML models-gradient boosting machine (GBM), logistic regression, support vector machine (SVM), and random forest-and two DL models-multilayer perceptron (MLP) and long short-term memory (LSTM). A comprehensive set of features, including demographics, lab values, vital signs, comorbidities, and ICU-specific variables, were extracted or engineered. Models were trained using 5-fold cross-validation and evaluated on a separate test set. Feature importance was analyzed using SHapley Additive exPlanations (SHAP) values, and fairness was assessed using demographic parity difference (DPD) and equalized odds difference (EOD), with and without the application of debiasing techniques. RESULTS: The GBM model outperformed all other models, with an AUC-ROC score of 96.3%, accuracy of 89.4%, sensitivity of 87.8%, specificity of 90.7%, and F1 score of 89.2%. Key features contributing to mortality prediction included Glasgow Coma Scale (GCS) scores, Braden Scale scores, blood urea nitrogen, age, red cell distribution width (RDW), bicarbonate, and lactate levels. Fairness analysis revealed that models trained on the top 30 most important features demonstrated lower DPD and EOD, suggesting reduced bias. Debiasing methods improved fairness in models trained with all features but had limited effects on models using the top 30 features. CONCLUSION: ML models show strong potential for mortality prediction in critically ill hypertensive patients. Feature selection not only enhances interpretability and reduces computational complexity but may also contribute to improved model fairness. These findings support the integration of interpretable and equitable AI tools in critical care settings to assist with clinical decision-making.",
      "journal": "Frontiers in artificial intelligence",
      "year": "2025",
      "doi": "10.3389/frai.2025.1686378",
      "authors": "Zhang Shenghan et al.",
      "keywords": "SHAP; deep learning; fairness in AI; hypertension; intensive care unit; machine learning; mortality prediction",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41459572/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning; Random Forest; Logistic Regression; XGBoost/Gradient Boosting; Support Vector Machine; Neural Network",
      "health_domain": "Cardiology; ICU/Critical Care; EHR/Health Informatics; Nephrology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "Hospital/Inpatient; ICU; Public Health/Population",
      "key_findings": "CONCLUSION: ML models show strong potential for mortality prediction in critically ill hypertensive patients. Feature selection not only enhances interpretability and reduces computational complexity but may also contribute to improved model fairness. These findings support the integration of interpretable and equitable AI tools in critical care settings to assist with clinical decision-making.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12738824"
    },
    {
      "pmid": "41460829",
      "title": "When noise mitigates bias in human-algorithm decision-making: An agent-based model.",
      "abstract": "Algorithmic systems increasingly inform human decision-making in domains such as criminal justice, healthcare, and finance. Although algorithms can exhibit bias, they are much less prone to undesirable variability in judgments (noise) than human decision-makers. While presented as an advantageous feature of algorithmic advice, we actually know little about how (biased) algorithmic advice interacts with noisy human judgment. Does undesirable variability in human judgment decrease under noiseless algorithmic advice? Is bias in human judgment exacerbated or mitigated by noise in advice? To answer these questions, we built an agent-based model that simulates the judgment of decision-makers receiving guidance from a (more or less) biased algorithm or a (more or less) biased and noisy human advisor. The model simulations show that, contrary to expectations, noise can be desirable: human noise can mitigate the harms of algorithmic bias by dampening the influence of algorithmic advice. Noise in human advice leads decision-makers to rely more heavily on their prior beliefs, an emergent behavior with implications for belief updating. When decision-makers' prior beliefs are polarized, an asymmetry occurs: decision-makers respond only to interventionist advice and not to non-interventionist cues. Finally, the model simulations show that population-level variability in decision-making stems from occasion noise in the environment and not from noise in human advice. This result challenges the common wisdom that population-level noise can be straightforwardly decomposed into individual-level sources and questions the feasibility of noise audits in organizations. Together, these findings demonstrate that the absence of noise as a feature of algorithmic advice is not generally desirable, suggesting critical implications for how human-algorithm systems are designed, regulated, and evaluated.",
      "journal": "PloS one",
      "year": "2025",
      "doi": "10.1371/journal.pone.0339273",
      "authors": "Poodiack Parsons Spencer et al.",
      "keywords": "",
      "mesh_terms": "Humans; Algorithms; Decision Making; Bias; Judgment; Computer Simulation; Models, Theoretical",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41460829/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Bias Auditing Framework",
      "clinical_setting": "Public Health/Population",
      "key_findings": "This result challenges the common wisdom that population-level noise can be straightforwardly decomposed into individual-level sources and questions the feasibility of noise audits in organizations. Together, these findings demonstrate that the absence of noise as a feature of algorithmic advice is not generally desirable, suggesting critical implications for how human-algorithm systems are designed, regulated, and evaluated.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12747352"
    },
    {
      "pmid": "41487722",
      "title": "Artificial Intelligence-Driven Risk Stratification in Chronic Kidney Disease Progression: Minimizing Bias via Race-Specific Algorithms.",
      "abstract": "Background Chronic kidney disease (CKD) is a prevalent condition that affects a substantial portion of the adult population and progresses unevenly across different demographic groups. Recent updates to estimated glomerular filtration rate (eGFR) estimation have removed race adjustments to promote greater equity. Yet, the impact of such changes on model performance and fairness across populations remains uncertain. Objective To ascertain whether, in comparison to a traditional pooled (or \"race-blind\") model, a race-specific, modular deep-learning architecture can enhance clinical utility and fairness in a five-year CKD-progression prediction. Methods We retrospectively pooled ~30,000 patients with stage 1-4 CKD from databases such as the National Health and Nutrition Examination Survey (NHANES), UK Biobank, and Chronic Renal Insufficiency Cohort\u00a0Study (CRIC), and two U.S. health-system electronic health records (EHRs). The endpoint was \u226540% sustained eGFR decline, \u22655 ml/min/1.73 m\u00b2/year drop, or kidney-failure event within five years. Two fully connected neural-network strategies were trained: (i) a pooled model on all races without race as an input; (ii) a modular model comprising separate subnetworks for Black and White patients, sharing architecture but trained on race-specific data. Performance was evaluated by discrimination (area under the curve or AUC), calibration, decision-curve net benefit, and fairness metrics (predictive parity, equalized odds, statistical parity). Results Overall AUCs were comparable (pooled 0.79, modular 0.80). The pooled model systematically underestimated risk in Black patients (calibration-in-the-large -3.8 percentage points (pp)) and yielded unequal positive predictive value (PPV 67.5% Black vs 58.6% White patients). The modular model virtually eliminated calibration bias (intercept \u22640.5 pp) and aligned PPV across races (~64% each) while preserving discrimination. Decision-curve analysis showed a small but consistent net-benefit gain for the modular approach at clinically relevant thresholds (10-35% risk). Trade-offs remained in equalized-odds: the modular model showed higher sensitivity for Black patients (510/840, 60.7%) than for White patients (294/900, 32.7%), though at the cost of a larger false-positive-rate disparity (365/2,160, 16.9% vs 144/3,600, 4.0%). Overall, CKD progression occurred in 1,820/7,500 (24%) patients - 840/3,000 (28%) Black and 900/4,500 (20%) White patients. Conclusions Ongoing monitoring and stakeholder-guided threshold setting are crucial to balance competing fairness criteria. Race-specific modular artificial intelligence (AI) models offer a practical route toward fairer, precision risk stratification by correcting miscalibration and PPV inequities inherent in pooled, race-blind CKD risk tools without sacrificing accuracy.",
      "journal": "Cureus",
      "year": "2025",
      "doi": "10.7759/cureus.98319",
      "authors": "Behmard Nima et al.",
      "keywords": "algorithmic fairness; artificial intelligence; calibration; chronic kidney disease; decision curve analysis; health equity; predictive parity; race-specific modeling",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41487722/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "EHR/Health Informatics; Nephrology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Threshold Adjustment; Fairness Metrics Evaluation",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Conclusions Ongoing monitoring and stakeholder-guided threshold setting are crucial to balance competing fairness criteria. Race-specific modular artificial intelligence (AI) models offer a practical route toward fairer, precision risk stratification by correcting miscalibration and PPV inequities inherent in pooled, race-blind CKD risk tools without sacrificing accuracy.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12757500"
    },
    {
      "pmid": "41497309",
      "title": "Mitigating Bias in Opportunistic Screening for MACE with Causal Reasoning.",
      "abstract": "Mitigating population drift is vital for developing robust AI models for clinical use. While current methodologies focus on reducing demographic bias in disease predictions, they overlook the significant impact of chronic comorbidities. Addressing these complexities is essential to enhance predictive accuracy and reliability across diverse patient demographics, ultimately improving healthcare outcomes. We propose a causal reasoning framework to address selection bias in opportunistic screening for 1-year composite MACE risk using chest X-ray images. Training in high-risk primarily Caucasian patients (43% MACE event), the model was evaluated in a lower-risk emergency department setting (12.8% MACE event) and a relatively lower-risk external Asian patient population (23.81% MACE event) to assess selection bias effects. We benchmarked our approach against a high-performance disease classification model, a propensity score matching strategy, and a debiasing model for unknown biases. The causal+confounder framework achieved an AUC of 0.75 and 0.7 on Shift data and Shift external, outperforming baselines, and a comparable AUC of 0.7 on internal data despite penalties for confounders. It minimized disparities in confounding factors and surpassed traditional and state-of-the-art debiasing methods. Experimental data show that integrating causal reasoning and confounder adjustments in AI models enhances their effectiveness. This approach shows promise for creating fair and robust clinical decision support systems that account for population shifts, ultimately improving the reliability and ethical integrity of AI-driven clinical decision-making.",
      "journal": "IEEE transactions on artificial intelligence",
      "year": "2025",
      "doi": "10.1109/tai.2025.3567961",
      "authors": "Pi Jialu et al.",
      "keywords": "Major adverse cardiovascular events; medical imaging; population shift; predictive models",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41497309/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Decision Support",
      "health_domain": "Radiology/Medical Imaging; Emergency Medicine",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Emergency Department; Public Health/Population",
      "key_findings": "Experimental data show that integrating causal reasoning and confounder adjustments in AI models enhances their effectiveness. This approach shows promise for creating fair and robust clinical decision support systems that account for population shifts, ultimately improving the reliability and ethical integrity of AI-driven clinical decision-making.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12768338"
    },
    {
      "pmid": "41502566",
      "title": "Considerations for evaluating the practical utility of machine learning in suicide risk estimation: the role of cost and equity.",
      "abstract": "A key vulnerability in modeling suicide death is a lack of precision and therefore estimates are thought as ultimately unhelpful to clinicians, even with more advanced or nuanced machine learning (ML) techniques. We sought to fill several conceptual gaps by assessing performance, focusing on the precision-recall tradeoff, across multiple techniques, and with ad hoc contextualization for sensitivity, cost-balance, and fairness. To identify robust, differential performances of a cross section of ML techniques on a suicide risk task, emphasizing overall AUPRC maximization and downstream effects on hypothetical decision support. A retrospective cohort was selected for patients receiving care or having died per the Office of the Medical Examiner (OCME), between 2017 and 2020 using the Maryland Suicide Datawarehouse (MSDW). AUPRC-optimized settings yielded cross-validated AUPRC significantly improved over logistic regressions, especially for XGBoost in both hospital discharge (AUPRC: 0.667; PPV: 0.941) and commercial claims records (AUPRC: 0.558; PPV: 0.857). F-Beta statistics revealed that when precision is preferred (e.g., 99.9 percentile), XGBoost are among the most efficient tools, while random forest and MLP are better when sensitivity is preferred (90 percentile or lower). No algorithmic bias was identified by age, sex or race, but significant changes in performance are noted with certain clinical characteristics. To our knowledge, this is the first use of an AUPRC-maxima optimization for ML tools with predicting suicide death. The utility of suicide risk models in clinical decision support is discussed as being tied to innate class imbalance challenges in model training, with recommendations being provided on how to better evaluate performance.",
      "journal": "Research square",
      "year": "2025",
      "doi": "10.21203/rs.3.rs-8216032/v1",
      "authors": "Kitchen Christopher et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41502566/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Random Forest; Logistic Regression; XGBoost/Gradient Boosting; Neural Network; Clinical Decision Support",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "To our knowledge, this is the first use of an AUPRC-maxima optimization for ML tools with predicting suicide death. The utility of suicide risk models in clinical decision support is discussed as being tied to innate class imbalance challenges in model training, with recommendations being provided on how to better evaluate performance.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12772703"
    },
    {
      "pmid": "41561161",
      "title": "Large language model bias auditing for periodontal diagnosis using an ambiguity-probe methodology: a pilot study.",
      "abstract": "BACKGROUND: Large Language Models (LLMs) in healthcare holds immense promise yet carries the risk of perpetuating social biases. While artificial intelligence (AI) fairness is a growing concern, a gap exists in understanding how these models perform under conditions of clinical ambiguity, a common feature in real-world practice. METHODS: We conducted a study using an ambiguity-probe methodology with a set of 42 sociodemographic personas and 15 clinical vignettes based on the 2018 classification of periodontal diseases. Ten were clear-cut scenarios with established ground truths, while five were intentionally ambiguous. OpenAI's GPT-4o and Google's Gemini 2.5 Pro were prompted to provide periodontal stage and grade assessments using 630 vignette-persona combinations per model. RESULTS: In clear-cut scenarios, GPT-4o demonstrated significantly higher combined (stage and grade) accuracy (70.5%) than Gemini Pro (33.3%). However, a robust fairness analysis using cumulative link models with false discovery rate correction revealed no statistically significant sociodemographic bias in either model. This finding held true across both clear-cut and ambiguous clinical scenarios. CONCLUSION: To our knowledge, this is among the first study to use simulated clinical ambiguity to reveal the distinct ethical fingerprints of LLMs in a dental context. While LLM performance gaps exist, our analysis decouples accuracy from fairness, demonstrating that both models maintain sociodemographic neutrality. We identify that the observed errors are not bias, but rather diagnostic boundary instability. This highlights a critical need for future research to differentiate between these two distinct types of model failure to build genuinely reliable AI.",
      "journal": "Frontiers in digital health",
      "year": "2025",
      "doi": "10.3389/fdgth.2025.1687820",
      "authors": "Nantakeeratipat Teerachate",
      "keywords": "AI bias; GPT-4o; Gemini Pro; clinical ambiguity; dental informatics; ethical auditing; health inequities; large language models",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41561161/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Language",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: To our knowledge, this is among the first study to use simulated clinical ambiguity to reveal the distinct ethical fingerprints of LLMs in a dental context. While LLM performance gaps exist, our analysis decouples accuracy from fairness, demonstrating that both models maintain sociodemographic neutrality. We identify that the observed errors are not bias, but rather diagnostic boundary instability.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12812596"
    },
    {
      "pmid": "41584908",
      "title": "Bias in AI systems: integrating formal and socio-technical approaches.",
      "abstract": "Artificial Intelligence (AI) systems are increasingly embedded in high-stakes decision-making across domains such as healthcare, finance, criminal justice, and employment. Evidence has been accumulated showing that these systems can reproduce and amplify structural inequities, leading to ethical, social, and technical concerns. In this review, formal mathematical definitions of bias are integrated with socio-technical perspectives to examine its origins, manifestations, and impacts. Bias is categorized into four interrelated families: historical/representational, selection/measurement, algorithmic/optimization, and feedback/emergent, and its operation is illustrated through case studies in facial recognition, large language models, credit scoring, healthcare, employment, and criminal justice. Current mitigation strategies are critically evaluated, including dataset diversification, fairness-aware modeling, post-deployment auditing, regulatory frameworks, and participatory design. An integrated framework is proposed in which statistical diagnostics are coupled with governance mechanisms to enable bias mitigation across the entire AI lifecycle. By bridging technical precision with sociological insight, guidance is offered for the development of AI systems that are equitable, accountable, and responsive to the needs of diverse populations.",
      "journal": "Frontiers in big data",
      "year": "2025",
      "doi": "10.3389/fdata.2025.1686452",
      "authors": "Ahmad Amar et al.",
      "keywords": "algorithmic bias; bias mitigation; ethical AI; fairness in machine learning; responsible AI; socio-technical systems",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41584908/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Language",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Bias Auditing Framework",
      "clinical_setting": "Public Health/Population",
      "key_findings": "An integrated framework is proposed in which statistical diagnostics are coupled with governance mechanisms to enable bias mitigation across the entire AI lifecycle. By bridging technical precision with sociological insight, guidance is offered for the development of AI systems that are equitable, accountable, and responsive to the needs of diverse populations.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12823528"
    },
    {
      "pmid": "41607892",
      "title": "Can AI developers avoid bias in public health applications?",
      "abstract": "Developments in the field of engineering biology and artificial intelligence have made it increasingly possible to deliver personalised treatments which are tailored to the individual and can help prevent illnesses before they occur. While such advancements have important implications for public health, the use of AI-enabled personalised treatments comes with potential downsides, not least of which is the potential for bias which may cause harm to certain subpopulations. As one of the key actors in the AI development pipeline, developers are ideally placed to ensure that treatments are designed in an equitable manner. However, existing bias mitigation strategies often fail to consider the practical challenges faced by developers which can significantly impact their abilities to detect and remove bias from any treatments which they help to design. In this paper, we highlight some of the practical challenges that developers face in mitigating bias. We also consider the implications of acknowledging such limitations for attributing responsibility related to bias mitigation.",
      "journal": "Frontiers in public health",
      "year": "2025",
      "doi": "10.3389/fpubh.2025.1752729",
      "authors": "Harms Rebekah J et al.",
      "keywords": "artificial intelligence; bias; engineering biology; public health; responsibility",
      "mesh_terms": "Humans; Artificial Intelligence; Public Health; Bias; Precision Medicine",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41607892/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Subgroup Analysis",
      "clinical_setting": "Public Health/Population",
      "key_findings": "In this paper, we highlight some of the practical challenges that developers face in mitigating bias. We also consider the implications of acknowledging such limitations for attributing responsibility related to bias mitigation.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12835397"
    },
    {
      "pmid": "41613098",
      "title": "Opportunities and challenges of artificial intelligence in public health: a systematic review on technological efficacy, ethical dilemmas, and governance pathways.",
      "abstract": "INTRODUCTION: Artificial intelligence (AI) holds profound potential to reshape public health through enhanced disease prediction, diagnosis, and health management. However, this technological advancement is accompanied by significant ethical, social, and governance challenges. This systematic review aims to comprehensively examine the opportunities and challenges of AI in public health, focusing on its applications, associated dilemmas, and governance pathways. METHODS: This review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. A systematic search was performed across multiple databases (e.g., PubMed, Web of Science, Scopus, IEEE Xplore, CNKI, Wanfang) from January 2019 to January 2025. The PICOS framework guided the inclusion of studies addressing AI applications in public health functions, their outcomes, and ethical or governance aspects. From an initial 901 records, 136 studies were included in the qualitative synthesis after screening and quality assessment using tools such as the Newcastle-Ottawa Scale and CASP checklist. RESULTS: The analysis reveals a dual effect of AI in public health. It significantly enhances efficiency in epidemic surveillance, emergency response, health communication, and clinical decision-support. However, these benefits are coupled with risks including algorithmic bias, data privacy concerns, the exacerbation of health inequities, and erosion of public trust. Public acceptance is context-dependent and influenced by factors like transparency, the digital divide, and task criticality. The evidence base exhibits a geographical imbalance, with a majority of studies from high-income countries, highlighting challenges in translating findings to low- and middle-income contexts. Effective governance requires a multi-layered, adaptive ecosystem that integrates technical standards, ethical oversight, community engagement, and global collaboration. DISCUSSION: The integration of AI into public health represents a major socio-technical transformation beyond mere technical upgrade. Navigating its dual nature requires a balanced approach that embeds ethical foresight into design, promotes equitable and participatory governance, and addresses global evidence disparities. Future efforts should prioritize explainable AI, robust data governance models, transdisciplinary research, and forward-looking policy frameworks to steer AI development towards equitable and trustworthy public health outcomes.",
      "journal": "Frontiers in public health",
      "year": "2025",
      "doi": "10.3389/fpubh.2025.1748797",
      "authors": "Gao Qin et al.",
      "keywords": "artificial intelligence; ethical governance; health equity; public health; public trust; systematic review",
      "mesh_terms": "Artificial Intelligence; Humans; Public Health; Ethical Dilemmas",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41613098/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "RESULTS: The analysis reveals a dual effect of AI in public health. It significantly enhances efficiency in epidemic surveillance, emergency response, health communication, and clinical decision-support. However, these benefits are coupled with risks including algorithmic bias, data privacy concerns, the exacerbation of health inequities, and erosion of public trust.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12847321"
    },
    {
      "pmid": "41623885",
      "title": "Federated multimodal AI for precision-equitable diabetes care.",
      "abstract": "Type 2 diabetes mellitus (T2DM) constitutes a rapidly expanding global epidemic whose societal burden is amplified by deep-rooted health inequities. Socio-economic disadvantage, minority ethnicity, low health literacy, and limited access to nutritious food or timely care disproportionately expose under-insured populations to earlier onset, poorer glycaemic control, and higher rates of cardiovascular, renal, and neurocognitive complications. Artificial intelligence (AI) is emerging as a transformative counterforce, capable of mitigating these disparities across the entire care continuum. Early detection and risk prediction have progressed from static clinical scores to dynamic machine-learning (ML) models that integrate multimodal data-electronic health records, genomics, socio-environmental variables, and wearable-derived behavioural signatures-to yield earlier and more accurate identification of high-risk individuals. Complication surveillance is being revolutionised by AI systems that screen for diabetic retinopathy with near-specialist accuracy, forecast renal function decline, and detect pre-ulcerative foot lesions through image-based deep learning, enabling timely, targeted interventions. Convergence with continuous glucose monitoring (CGM) and wearable technologies supports real-time, AI-driven glycaemic forecasting and decision support, while telemedicine platforms extend these benefits to remote or resource-constrained settings. Nevertheless, widespread implementation faces challenges of data heterogeneity, algorithmic bias against minority groups, privacy risks, and the digital divide that could paradoxically widen inequities if left unaddressed. Future directions centre on multimodal large language models, digital-twin simulations for personalised policy testing, and human-in-the-loop governance frameworks that embed ethical oversight, trauma-informed care, and community co-design. Realising AI's societal promise demands coordinated action across patients, clinicians, technologists, and policymakers to ensure solutions are not only clinically effective but also equitable, culturally attuned, and economically sustainable.",
      "journal": "Frontiers in digital health",
      "year": "2025",
      "doi": "10.3389/fdgth.2025.1678047",
      "authors": "Bai Bing et al.",
      "keywords": "artificial intelligence; digital-twin; federated learning; health-equity; precision medicine",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41623885/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Deep Learning; NLP/LLM; Clinical Prediction Model",
      "health_domain": "Cardiology; Ophthalmology; Emergency Medicine; EHR/Health Informatics; Nephrology; Genomics/Genetics; Public Health; Endocrinology/Diabetes; Wearables/Remote Monitoring",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Telehealth/Remote",
      "key_findings": "Future directions centre on multimodal large language models, digital-twin simulations for personalised policy testing, and human-in-the-loop governance frameworks that embed ethical oversight, trauma-informed care, and community co-design. Realising AI's societal promise demands coordinated action across patients, clinicians, technologists, and policymakers to ensure solutions are not only clinically effective but also equitable, culturally attuned, and economically sustainable.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12856318"
    },
    {
      "pmid": "41626932",
      "title": "Exploring the potential of Claude 2 for risk of bias assessment: Using a large language model to assess randomized controlled trials with RoB 2.",
      "abstract": "Systematic reviews are essential for evidence-based health care, but conducting them is time- and resource-consuming. To date, efforts have been made to accelerate and (semi-)automate various steps of systematic reviews through the use of artificial intelligence (AI) and the emergence of large language models (LLMs) promises further opportunities. One crucial but complex task within systematic review conduct is assessing the risk of bias (RoB) of included studies. Therefore, the aim of this study was to test the LLM Claude 2 for RoB assessment of 100 randomized controlled trials, published in English language from 2013 onwards, using the revised Cochrane risk of bias tool ('RoB 2'; involving judgements for five specific domains and an overall judgement). We assessed the agreement of RoB judgements by Claude with human judgements published in Cochrane reviews. The observed agreement between Claude and Cochrane authors ranged from 41% for the overall judgement to 71% for domain 4 ('outcome measurement'). Cohen's \u03ba was lowest for domain 5 ('selective reporting'; 0.10 (95% confidence interval (CI): -0.10-0.31)) and highest for domain 3 ('missing data'; 0.31 (95% CI: 0.10-0.52)), indicating slight to fair agreement. Fair agreement was found for the overall judgement (Cohen's \u03ba: 0.22 (95% CI: 0.06-0.38)). Sensitivity analyses using alternative prompting techniques or the more recent version Claude 3 did not result in substantial changes. Currently, Claude's RoB 2 judgements cannot replace human RoB assessment. However, the potential of LLMs to support RoB assessment should be further explored.",
      "journal": "Research synthesis methods",
      "year": "2025",
      "doi": "10.1017/rsm.2025.12",
      "authors": "Eisele-Metzger Angelika et al.",
      "keywords": "GPT; artificial intelligence; automation; large language models; risk of bias; systematic review as topic",
      "mesh_terms": "Humans; Randomized Controlled Trials as Topic; Bias; Language; Artificial Intelligence; Risk Assessment; Systematic Reviews as Topic; Evidence-Based Medicine; Research Design; Judgment; Large Language Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41626932/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Clinical Trial",
      "key_findings": "Currently, Claude's RoB 2 judgements cannot replace human RoB assessment. However, the potential of LLMs to support RoB assessment should be further explored.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12527486"
    },
    {
      "pmid": "33501466",
      "title": "Identifying bias in models that detect vocal fold paralysis from audio recordings using explainable machine learning and clinician ratings.",
      "abstract": "INTRODUCTION: Detecting voice disorders from voice recordings could allow for frequent, remote, and low-cost screening before costly clinical visits and a more invasive laryngoscopy examination. Our goals were to detect unilateral vocal fold paralysis (UVFP) from voice recordings using machine learning, to identify which acoustic variables were important for prediction to increase trust, and to determine model performance relative to clinician performance. METHODS: Patients with confirmed UVFP through endoscopic examination (N=77) and controls with normal voices matched for age and sex (N=77) were included. Voice samples were elicited by reading the Rainbow Passage and sustaining phonation of the vowel \"a\". Four machine learning models of differing complexity were used. SHapley Additive explanations (SHAP) was used to identify important features. RESULTS: The highest median bootstrapped ROC AUC score was 0.87 and beat clinician's performance (range: 0.74 - 0.81) based on the recordings. Recording durations were different between UVFP recordings and controls due to how that data was originally processed when storing, which we can show can classify both groups. And counterintuitively, many UVFP recordings had higher intensity than controls, when UVFP patients tend to have weaker voices, revealing a dataset-specific bias which we mitigate in an additional analysis. CONCLUSION: We demonstrate that recording biases in audio duration and intensity created dataset-specific differences between patients and controls, which models used to improve classification. Furthermore, clinician's ratings provide further evidence that patients were over-projecting their voices and being recorded at a higher amplitude signal than controls. Interestingly, after matching audio duration and removing variables associated with intensity in order to mitigate the biases, the models were able to achieve a similar high performance. We provide a set of recommendations to avoid bias when building and evaluating machine learning models for screening in laryngology.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2024",
      "doi": "10.1101/2020.11.23.20235945",
      "authors": "Low Daniel M et al.",
      "keywords": "acoustic analysis; bias; explainability; interpretability; machine learning; speech; vocal fold paralysis; voice",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33501466/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Telehealth/Remote",
      "key_findings": "CONCLUSION: We demonstrate that recording biases in audio duration and intensity created dataset-specific differences between patients and controls, which models used to improve classification. Furthermore, clinician's ratings provide further evidence that patients were over-projecting their voices and being recorded at a higher amplitude signal than controls. Interestingly, after matching audio duration and removing variables associated with intensity in order to mitigate the biases, the models...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7836138"
    },
    {
      "pmid": "36190605",
      "title": "Scheduling mobile dental clinics: A heuristic approach considering fairness among school districts.",
      "abstract": "Mobile dental clinics (MDCs) are suitable solutions for servicing people living in rural and urban areas that require dental healthcare. MDCs can provide dental care to the most vulnerable high-school students. However, scheduling MDCs to visit patients is critical to developing efficient dental programs. Here, we study a mobile dental clinic scheduling problem that arises from the real-life logistics management challenge faced by a school-based mobile dental care program in Southern Chile. This problem involves scheduling MDCs to treat high-school students at public schools while considering a fairness constraint among districts. Schools are circumscribed into districts, and by program regulations, at least 50% of the students in each district must receive dental care during the first semester. Fairness prevents some districts from waiting more time to receive dental care than others. We model the problem as a parallel machine scheduling problem with sequence-dependent setup costs and batch due dates and propose a mathematical model and a genetic algorithm-based solution to solve the problem. Our computational results demonstrate the effectiveness of our approaches in obtaining near-optimal solutions. Finally, dental program managers can use the methodologies presented in this work to schedule mobile dental clinics and improve their operations.",
      "journal": "Health care management science",
      "year": "2024",
      "doi": "10.1007/s10729-022-09612-5",
      "authors": "Sep\u00falveda Ignacio A et al.",
      "keywords": "Dental care; Health care management; Mobile dental clinic; Operations research; Scheduling",
      "mesh_terms": "Humans; Dental Clinics; Heuristics; Delivery of Health Care; Students; Costs and Cost Analysis",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36190605/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Fairness Constraints",
      "clinical_setting": "Not specified",
      "key_findings": "Our computational results demonstrate the effectiveness of our approaches in obtaining near-optimal solutions. Finally, dental program managers can use the methodologies presented in this work to schedule mobile dental clinics and improve their operations.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37660301",
      "title": "An intersectional framework for counterfactual fairness in risk prediction.",
      "abstract": "Along with the increasing availability of health data has come the rise of data-driven models to inform decision making and policy. These models have the potential to benefit both patients and health care providers but can also exacerbate health inequities. Existing \"algorithmic fairness\" methods for measuring and correcting model bias fall short of what is needed for health policy in two key ways. First, methods typically focus on a single grouping along which discrimination may occur rather than considering multiple, intersecting groups. Second, in clinical applications, risk prediction is typically used to guide treatment, creating distinct statistical issues that invalidate most existing techniques. We present novel unfairness metrics that address both challenges. We also develop a complete framework of estimation and inference tools for our metrics, including the unfairness value (\"u-value\"), used to determine the relative extremity of unfairness, and standard errors and confidence intervals employing an alternative to the standard bootstrap. We demonstrate application of our framework to a COVID-19 risk prediction model deployed in a major Midwestern health system.",
      "journal": "Biostatistics (Oxford, England)",
      "year": "2024",
      "doi": "10.1093/biostatistics/kxad021",
      "authors": "Wastvedt Solvejg et al.",
      "keywords": "Algorithmic fairness; COVID-19; Causal inference; Intersectionality; Risk prediction",
      "mesh_terms": "Humans; COVID-19; Risk Assessment; Models, Statistical; SARS-CoV-2",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37660301/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Pulmonology",
      "bias_axes": "Gender/Sex; Intersectional",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "We also develop a complete framework of estimation and inference tools for our metrics, including the unfairness value (\"u-value\"), used to determine the relative extremity of unfairness, and standard errors and confidence intervals employing an alternative to the standard bootstrap. We demonstrate application of our framework to a COVID-19 risk prediction model deployed in a major Midwestern health system.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37703989",
      "title": "Evaluating the fairness and accuracy of machine learning-based predictions of clinical outcomes after anatomic and reverse total shoulder arthroplasty.",
      "abstract": "BACKGROUND: Machine learning (ML)-based clinical decision support tools (CDSTs) make personalized predictions for different treatments; by comparing predictions of multiple treatments, these tools can be used to optimize decision making for a particular patient. However, CDST prediction accuracy varies for different patients and also for different treatment options. If these differences are sufficiently large and consistent for a particular subcohort of patients, then that bias may result in those patients not receiving a particular treatment. Such level of bias would deem the CDST \"unfair.\" The purpose of this study is to evaluate the \"fairness\" of ML CDST-based clinical outcomes predictions after anatomic (aTSA) and reverse total shoulder arthroplasty (rTSA) for patients of different demographic attributes. METHODS: Clinical data from 8280 shoulder arthroplasty patients with 19,249 postoperative visits was used to evaluate the prediction fairness and accuracy associated with the following patient demographic attributes: ethnicity, sex, and age at the time of surgery. Performance of clinical outcome and range of motion regression predictions were quantified by the mean absolute error (MAE) and performance of minimal clinically important difference (MCID) and substantial clinical benefit classification predictions were quantified by accuracy, sensitivity, and the F1 score. Fairness of classification predictions leveraged the \"four-fifths\" legal guideline from the US Equal Employment Opportunity Commission and fairness of regression predictions leveraged established MCID thresholds associated with each outcome measure. RESULTS: For both aTSA and rTSA clinical outcome predictions, only minor differences in MAE were observed between patients of different ethnicity, sex, and age. Evaluation of prediction fairness demonstrated that 0 of 486 MCID (0%) and only 3 of 486 substantial clinical benefit (0.6%) classification predictions were outside the 20% fairness boundary and only 14 of 972 (1.4%) regression predictions were outside of the MCID fairness boundary. Hispanic and Black patients were more likely to have ML predictions out of fairness tolerance for aTSA and rTSA. Additionally, patients <60 years old were more likely to have ML predictions out of fairness tolerance for rTSA. No disparate predictions were identified for sex and no disparate regression predictions were observed for forward elevation, internal rotation score, American Shoulder and Elbow Surgeons Standardized Shoulder Assessment Form score, or global shoulder function. CONCLUSION: The ML algorithms analyzed in this study accurately predict clinical outcomes after aTSA and rTSA for patients of different ethnicity, sex, and age, where only 1.4% of regression predictions and only 0.3% of classification predictions were out of fairness tolerance using the proposed fairness evaluation method and acceptance criteria. Future work is required to externally validate these ML algorithms to ensure they are equally accurate for all legally protected patient groups.",
      "journal": "Journal of shoulder and elbow surgery",
      "year": "2024",
      "doi": "10.1016/j.jse.2023.08.005",
      "authors": "Allen Christine et al.",
      "keywords": "Machine learning; accuracy evaluation; anatomic total shoulder arthroplasty; clinical outcomes; fairness and equity; reverse total shoulder arthroplasty",
      "mesh_terms": "Humans; Middle Aged; Arthroplasty, Replacement, Shoulder; Shoulder Joint; Treatment Outcome; Retrospective Studies; Range of Motion, Articular",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37703989/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Decision Support",
      "health_domain": "ICU/Critical Care; Surgery",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: The ML algorithms analyzed in this study accurately predict clinical outcomes after aTSA and rTSA for patients of different ethnicity, sex, and age, where only 1.4% of regression predictions and only 0.3% of classification predictions were out of fairness tolerance using the proposed fairness evaluation method and acceptance criteria. Future work is required to externally validate these ML algorithms to ensure they are equally accurate for all legally protected patient groups.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37936347",
      "title": "Algorithmic fairness in precision psychiatry: analysis of prediction models in individuals at clinical high risk for psychosis.",
      "abstract": "BACKGROUND: Computational models offer promising potential for personalised treatment of psychiatric diseases. For their clinical deployment, fairness must be evaluated alongside accuracy. Fairness requires predictive models to not unfairly disadvantage specific demographic groups. Failure to assess model fairness prior to use risks perpetuating healthcare inequalities. Despite its importance, empirical investigation of fairness in predictive models for psychiatry remains scarce. AIMS: To evaluate fairness in prediction models for development of psychosis and functional outcome. METHOD: Using data from the PRONIA study, we examined fairness in 13 published models for prediction of transition to psychosis (n = 11) and functional outcome (n = 2) in people at clinical high risk for psychosis or with recent-onset depression. Using accuracy equality, predictive parity, false-positive error rate balance and false-negative error rate balance, we evaluated relevant fairness aspects for the demographic attributes 'gender' and 'educational attainment' and compared them with the fairness of clinicians' judgements. RESULTS: Our findings indicate systematic bias towards assigning less favourable outcomes to individuals with lower educational attainment in both prediction models and clinicians' judgements, resulting in higher false-positive rates in 7 of 11 models for transition to psychosis. Interestingly, the bias patterns observed in algorithmic predictions were not significantly more pronounced than those in clinicians' predictions. CONCLUSIONS: Educational bias was present in algorithmic and clinicians' predictions, assuming more favourable outcomes for individuals with higher educational level (years of education). This bias might lead to increased stigma and psychosocial burden in patients with lower educational attainment and suboptimal psychosis prevention in those with higher educational attainment.",
      "journal": "The British journal of psychiatry : the journal of mental science",
      "year": "2024",
      "doi": "10.1192/bjp.2023.141",
      "authors": "\u015eahin Derya et al.",
      "keywords": "Ethics; psychotic disorders/schizophrenia; risk assessment; schizophrenia; stigma and discrimination",
      "mesh_terms": "Humans; Psychotic Disorders; Psychiatry",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37936347/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Educational bias was present in algorithmic and clinicians' predictions, assuming more favourable outcomes for individuals with higher educational level (years of education). This bias might lead to increased stigma and psychosocial burden in patients with lower educational attainment and suboptimal psychosis prevention in those with higher educational attainment.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38018360",
      "title": "Measurement and Mitigation of Bias in Artificial Intelligence: A Narrative Literature Review for Regulatory Science.",
      "abstract": "Artificial intelligence (AI) is increasingly being used in decision making across various industries, including the public health arena. Bias in any decision-making process can significantly skew outcomes, and AI systems have been shown to exhibit biases at times. The potential for AI systems to perpetuate and even amplify biases is a growing concern. Bias, as used in this paper, refers to the tendency toward a particular characteristic or behavior, and thus, a biased AI system is one that shows biased associations entities. In this literature review, we examine the current state of research on AI bias, including its sources, as well as the methods for measuring, benchmarking, and mitigating it. We also examine the biases and methods of mitigation specifically relevant to the healthcare field and offer a perspective on bias measurement and mitigation in regulatory science decision making.",
      "journal": "Clinical pharmacology and therapeutics",
      "year": "2024",
      "doi": "10.1002/cpt.3117",
      "authors": "Gray Magnus et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Benchmarking; Bias; Public Health",
      "pub_types": "Journal Article; Review; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38018360/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Public Health",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "In this literature review, we examine the current state of research on AI bias, including its sources, as well as the methods for measuring, benchmarking, and mitigating it. We also examine the biases and methods of mitigation specifically relevant to the healthcare field and offer a perspective on bias measurement and mitigation in regulatory science decision making.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38123252",
      "title": "Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a model evaluation study.",
      "abstract": "BACKGROUND: Large language models (LLMs) such as GPT-4 hold great promise as transformative tools in health care, ranging from automating administrative tasks to augmenting clinical decision making. However, these models also pose a danger of perpetuating biases and delivering incorrect medical diagnoses, which can have a direct, harmful impact on medical care. We aimed to assess whether GPT-4 encodes racial and gender biases that impact its use in health care. METHODS: Using the Azure OpenAI application interface, this model evaluation study tested whether GPT-4 encodes racial and gender biases and examined the impact of such biases on four potential applications of LLMs in the clinical domain-namely, medical education, diagnostic reasoning, clinical plan generation, and subjective patient assessment. We conducted experiments with prompts designed to resemble typical use of GPT-4 within clinical and medical education applications. We used clinical vignettes from NEJM Healer and from published research on implicit bias in health care. GPT-4 estimates of the demographic distribution of medical conditions were compared with true US prevalence estimates. Differential diagnosis and treatment planning were evaluated across demographic groups using standard statistical tests for significance between groups. FINDINGS: We found that GPT-4 did not appropriately model the demographic diversity of medical conditions, consistently producing clinical vignettes that stereotype demographic presentations. The differential diagnoses created by GPT-4 for standardised clinical vignettes were more likely to include diagnoses that stereotype certain races, ethnicities, and genders. Assessment and plans created by the model showed significant association between demographic attributes and recommendations for more expensive procedures as well as differences in patient perception. INTERPRETATION: Our findings highlight the urgent need for comprehensive and transparent bias assessments of LLM tools such as GPT-4 for intended use cases before they are integrated into clinical care. We discuss the potential sources of these biases and potential mitigation strategies before clinical implementation. FUNDING: Priscilla Chan and Mark Zuckerberg.",
      "journal": "The Lancet. Digital health",
      "year": "2024",
      "doi": "10.1016/S2589-7500(23)00225-X",
      "authors": "Zack Travis et al.",
      "keywords": "",
      "mesh_terms": "Female; Humans; Male; Health Facilities; Clinical Decision-Making; Diagnosis, Differential; Education, Medical; Delivery of Health Care",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38123252/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "FINDINGS: We found that GPT-4 did not appropriately model the demographic diversity of medical conditions, consistently producing clinical vignettes that stereotype demographic presentations. The differential diagnoses created by GPT-4 for standardised clinical vignettes were more likely to include diagnoses that stereotype certain races, ethnicities, and genders. Assessment and plans created by the model showed significant association between demographic attributes and recommendations for more ...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38199323",
      "title": "Measuring Implicit Bias in ICU Notes Using Word-Embedding Neural Network Models.",
      "abstract": "BACKGROUND: Language in nonmedical data sets is known to transmit human-like biases when used in natural language processing (NLP) algorithms that can reinforce disparities. It is unclear if NLP algorithms of medical notes could lead to similar transmissions of biases. RESEARCH QUESTION: Can we identify implicit bias in clinical notes, and are biases stable across time and geography? STUDY DESIGN AND METHODS: To determine whether different racial and ethnic descriptors are similar contextually to stigmatizing language in ICU notes and whether these relationships are stable across time and geography, we identified notes on critically ill adults admitted to the University of California, San Francisco (UCSF), from 2012 through 2022 and to Beth Israel Deaconess Hospital (BIDMC) from 2001 through 2012. Because word meaning is derived largely from context, we trained unsupervised word-embedding algorithms to measure the similarity (cosine similarity) quantitatively of the context between a racial or ethnic descriptor (eg, African-American) and a stigmatizing target word (eg, nonco-operative) or group of words (violence, passivity, noncompliance, nonadherence). RESULTS: In UCSF notes, Black descriptors were less likely to be similar contextually to violent words compared with White descriptors. Contrastingly, in BIDMC notes, Black descriptors were more likely to be similar contextually to violent words compared with White descriptors. The UCSF data set\u00a0also showed that Black descriptors were more similar contextually to passivity and noncompliance words compared with Latinx descriptors. INTERPRETATION: Implicit bias is identifiable in ICU notes. Racial and ethnic group descriptors carry different contextual relationships to stigmatizing words, depending on when and where notes were written. Because NLP models seem able to transmit implicit bias from training data, use of NLP algorithms in clinical prediction could reinforce disparities. Active debiasing strategies may be necessary to achieve algorithmic fairness when using language models in clinical research.",
      "journal": "Chest",
      "year": "2024",
      "doi": "10.1016/j.chest.2023.12.031",
      "authors": "Cobert Julien et al.",
      "keywords": "critical care; inequity; linguistics; machine learning; natural language processing",
      "mesh_terms": "Humans; Natural Language Processing; Intensive Care Units; Neural Networks, Computer; Algorithms; Critical Illness; Bias; Electronic Health Records; Male; Female",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38199323/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM; Neural Network; Clinical Prediction Model; Clustering",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics; Surgery",
      "bias_axes": "Race/Ethnicity; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Representation Learning",
      "clinical_setting": "Hospital/Inpatient; ICU",
      "key_findings": "RESULTS: In UCSF notes, Black descriptors were less likely to be similar contextually to violent words compared with White descriptors. Contrastingly, in BIDMC notes, Black descriptors were more likely to be similar contextually to violent words compared with White descriptors. The UCSF data set\u00a0also showed that Black descriptors were more similar contextually to passivity and noncompliance words compared with Latinx descriptors.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11317817"
    },
    {
      "pmid": "38260285",
      "title": "Evaluating and Improving the Performance and Racial Fairness of Algorithms for GFR Estimation.",
      "abstract": "Data-driven clinical prediction algorithms are used widely by clinicians. Understanding what factors can impact the performance and fairness of data-driven algorithms is an important step towards achieving equitable healthcare. To investigate the impact of modeling choices on the algorithmic performance and fairness, we make use of a case study to build a prediction algorithm for estimating glomerular filtration rate (GFR) based on the patient's electronic health record (EHR). We compare three distinct approaches for estimating GFR: CKD-EPI equations, epidemiological models, and EHR-based models. For epidemiological models and EHR-based models, four machine learning models of varying computational complexity (i.e., linear regression, support vector machine, random forest regression, and neural network) were compared. Performance metrics included root mean squared error (RMSE), median difference, and the proportion of GFR estimates within 30% of the measured GFR value (P30). Differential performance between non-African American and African American group was used to assess algorithmic fairness with respect to race. Our study showed that the variable race had a negligible effect on error, accuracy, and differential performance. Furthermore, including more relevant clinical features (e.g., common comorbidities of chronic kidney disease) and using more complex machine learning models, namely random forest regression, significantly lowered the estimation error of GFR. However, the difference in performance between African American and non-African American patients did not decrease, where the estimation error for African American patients remained consistently higher than non-African American patients, indicating that more objective patient characteristics should be discovered and included to improve algorithm performance.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2024",
      "doi": "10.1101/2024.01.07.24300943",
      "authors": "Zhang Linying et al.",
      "keywords": "algorithmic fairness; electronic health record; glomerular filtration rate; machine learning; predictive modeling",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38260285/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Random Forest; Support Vector Machine; Neural Network; Clinical Prediction Model; Regression",
      "health_domain": "EHR/Health Informatics; Nephrology",
      "bias_axes": "Race/Ethnicity",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Furthermore, including more relevant clinical features (e.g., common comorbidities of chronic kidney disease) and using more complex machine learning models, namely random forest regression, significantly lowered the estimation error of GFR. However, the difference in performance between African American and non-African American patients did not decrease, where the estimation error for African American patients remained consistently higher than non-African American patients, indicating that more...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10802656"
    },
    {
      "pmid": "38264718",
      "title": "Unified fair federated learning for digital healthcare.",
      "abstract": "Federated learning (FL) is a promising approach for healthcare institutions to train high-quality medical models collaboratively while protecting sensitive data privacy. However, FL models encounter fairness issues at diverse levels, leading to performance disparities across different subpopulations. To address this, we propose Federated Learning with Unified Fairness Objective (FedUFO), a unified framework consolidating diverse fairness levels within FL. By leveraging distributionally robust optimization and a unified uncertainty set, it ensures consistent performance across all subpopulations and enhances the overall efficacy of FL in healthcare and other domains while maintaining accuracy levels comparable with those of existing methods. Our model was validated by applying it to four digital healthcare tasks using real-world datasets in federated settings. Our collaborative machine learning paradigm not only promotes artificial intelligence in digital healthcare but also fosters social equity by embodying fairness.",
      "journal": "Patterns (New York, N.Y.)",
      "year": "2024",
      "doi": "10.1016/j.patter.2023.100907",
      "authors": "Zhang Fengda et al.",
      "keywords": "algorithmic fairness; digital healthcare; federated learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38264718/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Federated Learning",
      "health_domain": "General Healthcare",
      "bias_axes": "Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Subgroup Analysis; Federated Learning",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Our model was validated by applying it to four digital healthcare tasks using real-world datasets in federated settings. Our collaborative machine learning paradigm not only promotes artificial intelligence in digital healthcare but also fosters social equity by embodying fairness.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10801255"
    },
    {
      "pmid": "38288010",
      "title": "Identifying and handling data bias within primary healthcare data using synthetic data generators.",
      "abstract": "Advanced synthetic data generators can simulate data samples that closely resemble sensitive personal datasets while significantly reducing the risk of individual identification. The use of these advanced generators holds enormous potential in the medical field, as it allows for the simulation and sharing of sensitive patient data. This enables the development and rigorous validation of novel AI technologies for accurate diagnosis and efficient disease management. Despite the availability of massive ground truth datasets (such as UK-NHS databases that contain millions of patient records), the risk of biases being carried over to data generators still exists. These biases may arise from the under-representation of specific patient cohorts due to cultural sensitivities within certain communities or standardised data collection procedures. Machine learning models can exhibit bias in various forms, including the under-representation of certain groups in the data. This can lead to missing data and inaccurate correlations and distributions, which may also be reflected in synthetic data. Our paper aims to improve synthetic data generators by introducing probabilistic approaches to first detect difficult-to-predict data samples in ground truth data and then boost them when applying the generator. In addition, we explore strategies to generate synthetic data that can reduce bias and, at the same time, improve the performance of predictive models.",
      "journal": "Heliyon",
      "year": "2024",
      "doi": "10.1016/j.heliyon.2024.e24164",
      "authors": "Draghi Barbara et al.",
      "keywords": "Bayesian networks; Data bias; Machine learning; Over-sampling; Synthetic data generators",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38288010/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Data Augmentation",
      "clinical_setting": "ICU",
      "key_findings": "Our paper aims to improve synthetic data generators by introducing probabilistic approaches to first detect difficult-to-predict data samples in ground truth data and then boost them when applying the generator. In addition, we explore strategies to generate synthetic data that can reduce bias and, at the same time, improve the performance of predictive models.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10823075"
    },
    {
      "pmid": "38290289",
      "title": "Understanding skin color bias in deep learning-based skin lesion segmentation.",
      "abstract": "BACKGROUND: The field of dermatological image analysis using deep neural networks includes the semantic segmentation of skin lesions, pivotal for lesion analysis, pathology inference, and diagnoses. While biases in neural network-based dermatoscopic image classification against darker skin tones due to dataset imbalance and contrast disparities are acknowledged, a comprehensive exploration of skin color bias in lesion segmentation models is lacking. It is imperative to address and understand the biases in these models. METHODS: Our study comprehensively evaluates skin tone bias within prevalent neural networks for skin lesion segmentation. Since no information about skin color exists in widely used datasets, to quantify the bias we use three distinct skin color estimation methods: Fitzpatrick skin type estimation, Individual Typology Angle estimation as well as manual grouping of images by skin color. We assess bias across common models by training a variety of U-Net-based models on three widely-used datasets with 1758 different dermoscopic and clinical images. We also evaluate commonly suggested methods to mitigate bias. RESULTS: Our findings expose a significant and large correlation between segmentation performance and skin color, revealing consistent challenges in segmenting lesions for darker skin tones across diverse datasets. Using various methods of skin color quantification, we have found significant bias in skin lesion segmentation against darker-skinned individuals when evaluated both in and out-of-sample. We also find that commonly used methods for bias mitigation do not result in any significant reduction in bias. CONCLUSIONS: Our findings suggest a pervasive bias in most published lesion segmentation methods, given our use of commonly employed neural network architectures and publicly available datasets. In light of our findings, we propose recommendations for unbiased dataset collection, labeling, and model development. This presents the first comprehensive evaluation of fairness in skin lesion segmentation.",
      "journal": "Computer methods and programs in biomedicine",
      "year": "2024",
      "doi": "10.1016/j.cmpb.2024.108044",
      "authors": "Ben\u010devi\u0107 Marin et al.",
      "keywords": "AI fairness; Deep neural networks; Dermatological image analysis; Skin lesion segmentation",
      "mesh_terms": "Humans; Skin Pigmentation; Deep Learning; Dermoscopy; Skin Diseases; Skin; Image Processing, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38290289/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Deep Learning; Neural Network; Computer Vision/Imaging AI",
      "health_domain": "Dermatology; Pathology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Diverse/Representative Data",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "CONCLUSIONS: Our findings suggest a pervasive bias in most published lesion segmentation methods, given our use of commonly employed neural network architectures and publicly available datasets. In light of our findings, we propose recommendations for unbiased dataset collection, labeling, and model development. This presents the first comprehensive evaluation of fairness in skin lesion segmentation.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38315667",
      "title": "Assessment of differentially private synthetic data for utility and fairness in end-to-end machine learning pipelines for tabular data.",
      "abstract": "Differentially private (DP) synthetic datasets are a solution for sharing data while preserving the privacy of individual data providers. Understanding the effects of utilizing DP synthetic data in end-to-end machine learning pipelines impacts areas such as health care and humanitarian action, where data is scarce and regulated by restrictive privacy laws. In this work, we investigate the extent to which synthetic data can replace real, tabular data in machine learning pipelines and identify the most effective synthetic data generation techniques for training and evaluating machine learning models. We systematically investigate the impacts of differentially private synthetic data on downstream classification tasks from the point of view of utility as well as fairness. Our analysis is comprehensive and includes representatives of the two main types of synthetic data generation algorithms: marginal-based and GAN-based. To the best of our knowledge, our work is the first that: (i) proposes a training and evaluation framework that does not assume that real data is available for testing the utility and fairness of machine learning models trained on synthetic data; (ii) presents the most extensive analysis of synthetic dataset generation algorithms in terms of utility and fairness when used for training machine learning models; and (iii) encompasses several different definitions of fairness. Our findings demonstrate that marginal-based synthetic data generators surpass GAN-based ones regarding model training utility for tabular data. Indeed, we show that models trained using data generated by marginal-based algorithms can exhibit similar utility to models trained using real data. Our analysis also reveals that the marginal-based synthetic data generated using AIM and MWEM PGM algorithms can train models that simultaneously achieve utility and fairness characteristics close to those obtained by models trained with real data.",
      "journal": "PloS one",
      "year": "2024",
      "doi": "10.1371/journal.pone.0297271",
      "authors": "Pereira Mayana et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Health Facilities; Interior Design and Furnishings; Knowledge; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38315667/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Data Augmentation",
      "clinical_setting": "Not specified",
      "key_findings": "Indeed, we show that models trained using data generated by marginal-based algorithms can exhibit similar utility to models trained using real data. Our analysis also reveals that the marginal-based synthetic data generated using AIM and MWEM PGM algorithms can train models that simultaneously achieve utility and fairness characteristics close to those obtained by models trained with real data.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10843030"
    },
    {
      "pmid": "38354499",
      "title": "Artificial intelligence image-based prediction models in IBD exhibit high risk of bias: A systematic review.",
      "abstract": "BACKGROUND: There has been an increase in the development of both machine learning (ML) and deep learning (DL) prediction models in Inflammatory Bowel Disease. We aim in this systematic review to assess the methodological quality and risk of bias of ML and DL IBD image-based prediction studies. METHODS: We searched three databases, PubMed, Scopus and Embase, to identify ML and DL diagnostic or prognostic predictive models using imaging data in IBD, to Dec 31, 2022. We restricted our search to include studies that primarily used conventional imaging data, were undertaken in human participants, and published in English. Two reviewers independently reviewed the abstracts. The methodological quality of the studies was determined, and risk of bias evaluated using the prediction risk of bias assessment tool (PROBAST). RESULTS: Forty studies were included, thirty-nine developed diagnostic models. Seven studies utilized ML approaches, six were retrospective and none used multicenter data for model development. Thirty-three studies utilized DL approaches, ten were prospective, and twelve multicenter studies. Overall, all studies demonstrated high risk of bias. ML studies were evaluated in 4 domains all rated as high risk of bias: participants (6/7), predictors (1/7), outcome (3/7), and analysis (7/7), and DL studies evaluated in 3 domains: participants (24/33), outcome (10/33), and analysis (18/33). The majority of image-based studies used colonoscopy images. CONCLUSION: The risk of bias was high in AI IBD image-based prediction models, owing to insufficient sample size, unreported missingness and lack of an external validation cohort. Models with a high risk of bias are unlikely to be generalizable and suitable for clinical implementation.",
      "journal": "Computers in biology and medicine",
      "year": "2024",
      "doi": "10.1016/j.compbiomed.2024.108093",
      "authors": "Liu Xiaoxuan et al.",
      "keywords": "Artificial intelligence; Computer vision; Deep learning; Imaging; Inflammatory bowel disease; Machine learning",
      "mesh_terms": "Humans; Inflammatory Bowel Diseases; Deep Learning; Image Interpretation, Computer-Assisted; Artificial Intelligence; Bias",
      "pub_types": "Systematic Review; Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38354499/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Deep Learning; Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: The risk of bias was high in AI IBD image-based prediction models, owing to insufficient sample size, unreported missingness and lack of an external validation cohort. Models with a high risk of bias are unlikely to be generalizable and suitable for clinical implementation.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38360543",
      "title": "Calibration and XGBoost reweighting to reduce coverage and non-response biases in overlapping panel surveys: application to the Healthcare and Social Survey.",
      "abstract": "BACKGROUND: Surveys have been used worldwide to provide information on the COVID-19 pandemic impact so as to prepare and deliver an effective Public Health response. Overlapping panel surveys allow longitudinal estimates and more accurate cross-sectional estimates to be obtained thanks to the larger sample size. However, the problem of non-response is particularly aggravated in the case of panel surveys due to population fatigue with repeated surveys. OBJECTIVE: To develop a new reweighting method for overlapping panel surveys affected by non-response. METHODS: We chose the Healthcare and Social Survey which has an overlapping panel survey design with measurements throughout 2020 and 2021, and random samplings stratified by province and degree of urbanization. Each measurement comprises two samples: a longitudinal sample taken from previous measurements and a new sample taken at each measurement. RESULTS: Our reweighting methodological approach is the result of a two-step process: the original sampling design weights are corrected by modelling non-response with respect to the longitudinal sample obtained in a previous measurement using machine learning techniques, followed by calibration using the auxiliary information available at the population level. It is applied to the estimation of totals, proportions, ratios, and differences between measurements, and to gender gaps in the variable of self-perceived general health. CONCLUSION: The proposed method produces suitable estimators for both cross-sectional and longitudinal samples. For addressing future health crises such as COVID-19, it is therefore necessary to reduce potential coverage and non-response biases in surveys by means of utilizing reweighting techniques as proposed in this study.",
      "journal": "BMC medical research methodology",
      "year": "2024",
      "doi": "10.1186/s12874-024-02171-z",
      "authors": "Castro Luis et al.",
      "keywords": "COVID-19; Machine learning; Non-response bias; Panel surveys; Public health; Sampling",
      "mesh_terms": "Humans; Cross-Sectional Studies; Calibration; Pandemics; Surveys and Questionnaires; COVID-19; Bias; Delivery of Health Care",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38360543/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "XGBoost/Gradient Boosting",
      "health_domain": "ICU/Critical Care; Pulmonology; Public Health",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Calibration",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: The proposed method produces suitable estimators for both cross-sectional and longitudinal samples. For addressing future health crises such as COVID-19, it is therefore necessary to reduce potential coverage and non-response biases in surveys by means of utilizing reweighting techniques as proposed in this study.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10868104"
    },
    {
      "pmid": "38387381",
      "title": "Bias reduction using combined stain normalization and augmentation for AI-based classification of histological images.",
      "abstract": "Artificial intelligence (AI)-assisted diagnosis is an ongoing revolution in pathology. However, a frequent drawback of AI models is their propension to make decisions based rather on bias in training dataset than on concrete biological features, thus weakening pathologists' trust in these tools. Technically, it is well known that microscopic images are altered by tissue processing and staining procedures, being one of the main sources of bias in machine learning for digital pathology. So as to deal with it, many teams have written about color normalization and augmentation methods. However, only a few of them have monitored their effects on bias reduction and model generalizability. In our study, two methods for stain augmentation (AugmentHE) and fast normalization (HEnorm) have been created and their effect on bias reduction has been monitored. Actually, they have also been compared to previously described strategies. To that end, a multicenter dataset created for breast cancer histological grading has been used. Thanks to it, classification models have been trained in a single center before assessing its performance in other centers images. This setting led to extensively monitor bias reduction while providing accurate insight of both augmentation and normalization methods. AugmentHE provided an 81% increase in color dispersion compared to geometric augmentations only. In addition, every classification model that involved AugmentHE presented a significant increase in the area under receiving operator characteristic curve (AUC) over the widely used RGB shift. More precisely, AugmentHE-based models showed at least 0.14 AUC increase over RGB shift-based models. Regarding normalization, HEnorm appeared to be up to 78x faster than conventional methods. It also provided satisfying results in terms of bias reduction. Altogether, our pipeline composed of AugmentHE and HEnorm improved AUC on biased data by up to 21.7% compared to usual augmentations. Conventional normalization methods coupled with AugmentHE yielded similar results while being much slower. In conclusion, we have validated an open-source tool that can be used in any deep learning-based digital pathology project on H&E whole slide images (WSI) that efficiently reduces stain-induced bias and later on might help increase pathologists' confidence when using AI-based products.",
      "journal": "Computers in biology and medicine",
      "year": "2024",
      "doi": "10.1016/j.compbiomed.2024.108130",
      "authors": "Franchet Camille et al.",
      "keywords": "Bias mitigation; Color-induced bias; Data augmentation; Deep learning; Histopathology; Normalization",
      "mesh_terms": "Female; Humans; Artificial Intelligence; Breast Neoplasms; Coloring Agents; Machine Learning; Staining and Labeling; Multicenter Studies as Topic",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38387381/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Oncology; Pathology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "Conventional normalization methods coupled with AugmentHE yielded similar results while being much slower. In conclusion, we have validated an open-source tool that can be used in any deep learning-based digital pathology project on H&E whole slide images (WSI) that efficiently reduces stain-induced bias and later on might help increase pathologists' confidence when using AI-based products.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38402414",
      "title": "Evaluating and improving health equity and fairness of polygenic scores.",
      "abstract": "Polygenic scores (PGSs) are quantitative metrics for predicting phenotypic values, such as human height or disease status. Some PGS methods require only summary statistics of a relevant genome-wide association study (GWAS) for their score. One such method is Lassosum, which inherits the model selection advantages of Lasso to select a meaningful subset of the GWAS single-nucleotide polymorphisms as predictors from their association statistics. However, even efficient scores like Lassosum, when derived from European-based GWASs, are poor predictors of phenotype for subjects of non-European ancestry; that is, they have limited portability to other ancestries. To increase the portability of Lassosum, when GWAS information and estimates of linkage disequilibrium are available for both ancestries, we propose Joint-Lassosum (JLS). In the simulation settings we explore, JLS provides more accurate PGSs compared to other methods, especially when measured in terms of fairness. In analyses of UK Biobank data, JLS was computationally more efficient but slightly less accurate than a Bayesian comparator, SDPRX. Like all PGS methods, JLS requires selection of predictors, which are determined by data-driven tuning parameters. We describe a new approach to selecting tuning parameters and note its relevance for model selection for any PGS. We also draw connections to the literature on algorithmic fairness and discuss how JLS can help mitigate fairness-related harms that might result from the use of PGSs in clinical settings. While no PGS method is likely to be universally portable, due to the diversity of human populations and unequal information content of GWASs for different ancestries, JLS is an effective approach for enhancing portability and reducing predictive bias.",
      "journal": "HGG advances",
      "year": "2024",
      "doi": "10.1016/j.xhgg.2024.100280",
      "authors": "Zhang Tianyu et al.",
      "keywords": "",
      "mesh_terms": "Humans; Bayes Theorem; Genome-Wide Association Study; Health Equity; Benchmarking; Computer Simulation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38402414/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We also draw connections to the literature on algorithmic fairness and discuss how JLS can help mitigate fairness-related harms that might result from the use of PGSs in clinical settings. While no PGS method is likely to be universally portable, due to the diversity of human populations and unequal information content of GWASs for different ancestries, JLS is an effective approach for enhancing portability and reducing predictive bias.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10937319"
    },
    {
      "pmid": "38422347",
      "title": "Evaluating Algorithmic Bias in 30-Day Hospital Readmission Models: Retrospective Analysis.",
      "abstract": "BACKGROUND: The adoption of predictive algorithms in health care comes with the potential for algorithmic bias, which could exacerbate existing disparities. Fairness metrics have been proposed to measure algorithmic bias, but their application to real-world tasks is limited. OBJECTIVE: This study aims to evaluate the algorithmic bias associated with the application of common 30-day hospital readmission models and assess the usefulness and interpretability of selected fairness metrics. METHODS: We used 10.6 million adult inpatient discharges from Maryland and Florida from 2016 to 2019 in this retrospective study. Models predicting 30-day hospital readmissions were evaluated: LACE Index, modified HOSPITAL score, and modified Centers for Medicare & Medicaid Services (CMS) readmission measure, which were applied as-is (using existing coefficients) and retrained (recalibrated with 50% of the data). Predictive performances and bias measures were evaluated for all, between Black and White populations, and between low- and other-income groups. Bias measures included the parity of false negative rate (FNR), false positive rate (FPR), 0-1 loss, and generalized entropy index. Racial bias represented by FNR and FPR differences was stratified to explore shifts in algorithmic bias in different populations. RESULTS: The retrained CMS model demonstrated the best predictive performance (area under the curve: 0.74 in Maryland and 0.68-0.70 in Florida), and the modified HOSPITAL score demonstrated the best calibration (Brier score: 0.16-0.19 in Maryland and 0.19-0.21 in Florida). Calibration was better in White (compared to Black) populations and other-income (compared to low-income) groups, and the area under the curve was higher or similar in the Black (compared to White) populations. The retrained CMS and modified HOSPITAL score had the lowest racial and income bias in Maryland. In Florida, both of these models overall had the lowest income bias and the modified HOSPITAL score showed the lowest racial bias. In both states, the White and higher-income populations showed a higher FNR, while the Black and low-income populations resulted in a higher FPR and a higher 0-1 loss. When stratified by hospital and population composition, these models demonstrated heterogeneous algorithmic bias in different contexts and populations. CONCLUSIONS: Caution must be taken when interpreting fairness measures' face value. A higher FNR or FPR could potentially reflect missed opportunities or wasted resources, but these measures could also reflect health care use patterns and gaps in care. Simply relying on the statistical notions of bias could obscure or underplay the causes of health disparity. The imperfect health data, analytic frameworks, and the underlying health systems must be carefully considered. Fairness measures can serve as a useful routine assessment to detect disparate model performances but are insufficient to inform mechanisms or policy changes. However, such an assessment is an important first step toward data-driven improvement to address existing health disparities.",
      "journal": "Journal of medical Internet research",
      "year": "2024",
      "doi": "10.2196/47125",
      "authors": "Wang H Echo et al.",
      "keywords": "algorithmic bias; health disparity; hospital readmission; model bias; model fairness; predictive models; retrospective analysis",
      "mesh_terms": "Aged; Adult; Humans; United States; Patient Readmission; Retrospective Studies; Medicare; Hospitals; Florida",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, P.H.S.; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38422347/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "Hospital/Inpatient; Public Health/Population",
      "key_findings": "CONCLUSIONS: Caution must be taken when interpreting fairness measures' face value. A higher FNR or FPR could potentially reflect missed opportunities or wasted resources, but these measures could also reflect health care use patterns and gaps in care. Simply relying on the statistical notions of bias could obscure or underplay the causes of health disparity.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11066744"
    },
    {
      "pmid": "38438371",
      "title": "Enhancing the fairness of AI prediction models by Quasi-Pareto improvement among heterogeneous thyroid nodule population.",
      "abstract": "Artificial Intelligence (AI) models for medical diagnosis often face challenges of generalizability and fairness. We highlighted the algorithmic unfairness in a large thyroid ultrasound dataset with significant diagnostic performance disparities across subgroups linked causally to sample size imbalances. To address this, we introduced the Quasi-Pareto Improvement (QPI) approach and a deep learning implementation (QP-Net) combining multi-task learning and domain adaptation to improve model performance among disadvantaged subgroups without compromising overall population performance. On the thyroid ultrasound dataset, our method significantly mitigated the area under curve (AUC) disparity for three less-prevalent subgroups by 0.213, 0.112, and 0.173 while maintaining the AUC for dominant subgroups; we also further confirmed the generalizability of our approach on two public datasets: the ISIC2019 skin disease dataset and the CheXpert chest radiograph dataset. Here we show the QPI approach to be widely applicable in promoting AI for equitable healthcare outcomes.",
      "journal": "Nature communications",
      "year": "2024",
      "doi": "10.1038/s41467-024-44906-y",
      "authors": "Yao Siqiong et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Thyroid Nodule; Area Under Curve; Health Facilities; Sample Size",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38438371/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning; Clinical Prediction Model",
      "health_domain": "Radiology/Medical Imaging; Dermatology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Transfer Learning; Multi-task Learning",
      "clinical_setting": "Public Health/Population",
      "key_findings": "On the thyroid ultrasound dataset, our method significantly mitigated the area under curve (AUC) disparity for three less-prevalent subgroups by 0.213, 0.112, and 0.173 while maintaining the AUC for dominant subgroups; we also further confirmed the generalizability of our approach on two public datasets: the ISIC2019 skin disease dataset and the CheXpert chest radiograph dataset. Here we show the QPI approach to be widely applicable in promoting AI for equitable healthcare outcomes.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10912763"
    },
    {
      "pmid": "38452862",
      "title": "A scoping review of fair machine learning techniques when using real-world data.",
      "abstract": "OBJECTIVE: The integration of artificial intelligence (AI) and machine learning (ML) in health care to aid clinical decisions is widespread. However, as AI and ML take important roles in health care, there are concerns about AI and ML associated fairness and bias. That is, an AI tool may have a disparate impact, with its benefits and drawbacks unevenly distributed across societal strata and subpopulations, potentially exacerbating existing health inequities. Thus, the objectives of this scoping review were to summarize existing literature and identify gaps in the topic of tackling algorithmic bias and optimizing fairness in AI/ML models using real-world data (RWD) in health care domains. METHODS: We conducted a thorough review of techniques for assessing and optimizing AI/ML model fairness in health care when using RWD in health care domains. The focus lies on appraising different quantification metrics for accessing fairness, publicly accessible datasets for ML fairness research, and bias mitigation approaches. RESULTS: We identified 11 papers that are focused on optimizing model fairness in health care applications. The current research on mitigating bias issues in RWD is limited, both in terms of disease variety and health care applications, as well as the accessibility of public datasets for ML fairness research. Existing studies often indicate positive outcomes when using pre-processing techniques to address algorithmic bias. There remain unresolved questions within the field that require further research, which includes pinpointing the root causes of bias in ML models, broadening fairness research in AI/ML with the use of RWD and exploring its implications in healthcare settings, and evaluating and addressing bias in multi-modal data. CONCLUSION: This paper provides useful reference material and insights to researchers regarding AI/ML fairness in real-world health care data and reveals the gaps in the field. Fair AI/ML in health care is a burgeoning field that requires a heightened research focus to cover diverse applications and different types of RWD.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104622",
      "authors": "Huang Yu et al.",
      "keywords": "Bias mitigation; Fair machine learning; Real-world data",
      "mesh_terms": "Humans; Artificial Intelligence; Machine Learning; Benchmarking; Research Personnel",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38452862/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Data Preprocessing; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Subgroup Analysis; Fairness Metrics Evaluation",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: This paper provides useful reference material and insights to researchers regarding AI/ML fairness in real-world health care data and reveals the gaps in the field. Fair AI/ML in health care is a burgeoning field that requires a heightened research focus to cover diverse applications and different types of RWD.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11146346"
    },
    {
      "pmid": "38467001",
      "title": "The Impact of Health Care Algorithms on Racial and Ethnic Disparities : A Systematic Review.",
      "abstract": "BACKGROUND: There is increasing concern for the potential impact of health care algorithms on racial and ethnic disparities. PURPOSE: To examine the evidence on how health care algorithms and associated mitigation strategies affect racial and ethnic disparities. DATA SOURCES: Several databases were searched for relevant studies published from 1 January 2011 to 30 September 2023. STUDY SELECTION: Using predefined criteria and dual review, studies were screened and selected to determine: 1) the effect of algorithms on racial and ethnic disparities in health and health care outcomes and 2) the effect of strategies or approaches to mitigate racial and ethnic bias in the development, validation, dissemination, and implementation of algorithms. DATA EXTRACTION: Outcomes of interest (that is, access to health care, quality of care, and health outcomes) were extracted with risk-of-bias assessment using the ROBINS-I (Risk Of Bias In Non-randomised Studies - of Interventions) tool and adapted CARE-CPM (Critical Appraisal for Racial and Ethnic Equity in Clinical Prediction Models) equity extension. DATA SYNTHESIS: Sixty-three studies (51 modeling, 4 retrospective, 2 prospective, 5 prepost studies, and 1 randomized controlled trial) were included. Heterogenous evidence on algorithms was found to: a) reduce disparities (for example, the revised kidney allocation system), b) perpetuate or exacerbate disparities (for example, severity-of-illness scores applied to critical care resource allocation), and/or c) have no statistically significant effect on select outcomes (for example, the HEART Pathway [history, electrocardiogram, age, risk factors, and troponin]). To mitigate disparities, 7 strategies were identified: removing an input variable, replacing a variable, adding race, adding a non-race-based variable, changing the racial and ethnic composition of the population used in model development, creating separate thresholds for subpopulations, and modifying algorithmic analytic techniques. LIMITATION: Results are mostly based on modeling studies and may be highly context-specific. CONCLUSION: Algorithms can mitigate, perpetuate, and exacerbate racial and ethnic disparities, regardless of the explicit use of race and ethnicity, but evidence is heterogeneous. Intentionality and implementation of the algorithm can impact the effect on disparities, and there may be tradeoffs in outcomes. PRIMARY FUNDING SOURCE: Agency for Healthcare Quality and Research.",
      "journal": "Annals of internal medicine",
      "year": "2024",
      "doi": "10.7326/M23-2960",
      "authors": "Siddique Shazia Mehmood et al.",
      "keywords": "",
      "mesh_terms": "Humans; Algorithms; Healthcare Disparities; Health Services Accessibility; Quality of Health Care; Ethnicity",
      "pub_types": "Systematic Review; Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38467001/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Cardiology; ICU/Critical Care; Nephrology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment; Subgroup Analysis",
      "clinical_setting": "ICU; Public Health/Population; Clinical Trial",
      "key_findings": "CONCLUSION: Algorithms can mitigate, perpetuate, and exacerbate racial and ethnic disparities, regardless of the explicit use of race and ethnicity, but evidence is heterogeneous. Intentionality and implementation of the algorithm can impact the effect on disparities, and there may be tradeoffs in outcomes. PRIMARY FUNDING SOURCE: Agency for Healthcare Quality and Research.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38471396",
      "title": "Drop the shortcuts: image augmentation improves fairness and decreases AI detection of race and other demographics from medical images.",
      "abstract": "BACKGROUND: It has been shown that AI models can learn race on medical images, leading to algorithmic bias. Our aim in this study was to enhance the fairness of medical image models by eliminating bias related to race, age, and sex. We hypothesise models may be learning demographics via shortcut learning and combat this using image augmentation. METHODS: This study included 44,953 patients who identified as Asian, Black, or White (mean age, 60.68 years\u00a0\u00b118.21; 23,499 women) for a total of 194,359 chest X-rays (CXRs) from MIMIC-CXR database. The included CheXpert images comprised 45,095 patients (mean age 63.10 years\u00a0\u00b118.14; 20,437 women) for a total of 134,300 CXRs were used for external validation. We also collected 1195 3D brain magnetic resonance imaging (MRI) data from the ADNI database, which included 273 participants with an average age of 76.97 years\u00a0\u00b114.22, and 142 females. DL models were trained on either non-augmented or augmented images and assessed using disparity metrics. The features learned by the models were analysed using task transfer experiments and model visualisation techniques. FINDINGS: In the detection of radiological findings, training a model using augmented CXR images was shown to reduce disparities in error rate among racial groups (-5.45%), age groups (-13.94%), and sex (-22.22%). For AD detection, the model trained with augmented MRI images was shown 53.11% and 31.01% reduction of disparities in error rate among age and sex groups, respectively. Image augmentation led to a reduction in the model's ability to identify demographic attributes and resulted in the model trained for clinical purposes incorporating fewer demographic features. INTERPRETATION: The model trained using the augmented images was less likely to be influenced by demographic information in detecting image labels. These results demonstrate that the proposed augmentation scheme could enhance the fairness of interpretations by DL models when dealing with data from patients with different demographic backgrounds. FUNDING: National Science and Technology Council (Taiwan), National Institutes of Health.",
      "journal": "EBioMedicine",
      "year": "2024",
      "doi": "10.1016/j.ebiom.2024.105047",
      "authors": "Wang Ryan et al.",
      "keywords": "Augmentation; Bias mitigation; Deep learning; Fairness; Shortcuts",
      "mesh_terms": "Aged; Female; Humans; Middle Aged; Benchmarking; Black People; Brain; Demography; Learning; United States; Asian People; White People; Male; Black or African American",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38471396/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "FINDINGS: In the detection of radiological findings, training a model using augmented CXR images was shown to reduce disparities in error rate among racial groups (-5.45%), age groups (-13.94%), and sex (-22.22%). For AD detection, the model trained with augmented MRI images was shown 53.11% and 31.01% reduction of disparities in error rate among age and sex groups, respectively. Image augmentation led to a reduction in the model's ability to identify demographic attributes and resulted in the m...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10945176"
    },
    {
      "pmid": "38487797",
      "title": "FRAMM: Fair ranking with missing modalities for clinical trial site selection.",
      "abstract": "The underrepresentation of gender, racial, and ethnic minorities in clinical trials is a problem undermining the efficacy of treatments on minorities and preventing precise estimates of the effects within these subgroups. We propose FRAMM, a deep reinforcement learning framework for fair trial site selection to help address this problem. We focus on two real-world challenges: the data modalities used to guide selection are often incomplete for many potential trial sites, and the site selection needs to simultaneously optimize for both enrollment and diversity. To address the missing data challenge, FRAMM has a modality encoder with a masked cross-attention mechanism for bypassing missing data. To make efficient trade-offs, FRAMM uses deep reinforcement learning with a reward function designed to simultaneously optimize for both enrollment and fairness. We evaluate FRAMM using real-world historical clinical trials and show that it outperforms the leading baseline in enrollment-only settings while also greatly improving diversity.",
      "journal": "Patterns (New York, N.Y.)",
      "year": "2024",
      "doi": "10.1016/j.patter.2024.100944",
      "authors": "Theodorou Brandon et al.",
      "keywords": "deep learning; fairness in healthcare; fairness in machine learning; learning to ranking; machine learning for healthcare; missing data; reinforcement learning; trial site selection",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38487797/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "NLP/LLM; Reinforcement Learning",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Clinical Trial",
      "key_findings": "To make efficient trade-offs, FRAMM uses deep reinforcement learning with a reward function designed to simultaneously optimize for both enrollment and fairness. We evaluate FRAMM using real-world historical clinical trials and show that it outperforms the leading baseline in enrollment-only settings while also greatly improving diversity.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10935501"
    },
    {
      "pmid": "38520723",
      "title": "Unmasking bias in artificial intelligence: a systematic review of bias detection and mitigation strategies in electronic health record-based models.",
      "abstract": "OBJECTIVES: Leveraging artificial intelligence (AI) in conjunction with electronic health records (EHRs) holds transformative potential to improve healthcare. However, addressing bias in AI, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to handle various biases in AI models developed using EHR data. MATERIALS AND METHODS: We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-analyses guidelines, analyzing articles from PubMed, Web of Science, and IEEE published between January 01, 2010 and December 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the AI model development, and analyzed metrics for bias assessment. RESULTS: Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Fifteen studies proposed strategies for mitigating biases, especially targeting implicit and selection biases. These strategies, evaluated through both performance and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling and reweighting. DISCUSSION: This review highlights evolving strategies to mitigate bias in EHR-based AI models, emphasizing the urgent need for both standardized and detailed reporting of the methodologies and systematic real-world testing and evaluation. Such measures are essential for gauging models' practical impact and fostering ethical AI that ensures fairness and equity in healthcare.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2024",
      "doi": "10.1093/jamia/ocae060",
      "authors": "Chen Feng et al.",
      "keywords": "artificial intelligence; bias; deep learning; electronic health record; scoping review",
      "mesh_terms": "Electronic Health Records; Artificial Intelligence; Bias; Humans; Algorithms; Models, Theoretical",
      "pub_types": "Systematic Review; Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38520723/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11031231"
    },
    {
      "pmid": "38529077",
      "title": "A survey of recent methods for addressing AI fairness and bias in biomedicine.",
      "abstract": "OBJECTIVES: Artificial intelligence (AI) systems have the potential to revolutionize clinical practices, including improving diagnostic accuracy and surgical decision-making, while also reducing costs and manpower. However, it is important to recognize that these systems may perpetuate social inequities or demonstrate biases, such as those based on race or gender. Such biases can occur before, during, or after the development of AI models, making it critical to understand and address potential biases to enable the accurate and reliable application of AI models in clinical settings. To mitigate bias concerns during model development, we surveyed recent publications on different debiasing methods in the fields of biomedical natural language processing (NLP) or computer vision (CV). Then we discussed the methods, such as data perturbation and adversarial learning, that have been applied in the biomedical domain to address bias. METHODS: We performed our literature search on PubMed, ACM digital library, and IEEE Xplore of relevant articles published between January 2018 and December 2023 using multiple combinations of keywords. We then filtered the result of 10,041 articles automatically with loose constraints, and manually inspected the abstracts of the remaining 890 articles to identify the 55 articles included in this review. Additional articles in the references are also included in this review. We discuss each method and compare its strengths and weaknesses. Finally, we review other potential methods from the general domain that could be applied to biomedicine to address bias and improve fairness. RESULTS: The bias of AIs in biomedicine can originate from multiple sources such as insufficient data, sampling bias and the use of health-irrelevant features or race-adjusted algorithms. Existing debiasing methods that focus on algorithms can be categorized into distributional or algorithmic. Distributional methods include data augmentation, data perturbation, data reweighting methods, and federated learning. Algorithmic approaches include unsupervised representation learning, adversarial learning, disentangled representation learning, loss-based methods and causality-based methods.",
      "journal": "ArXiv",
      "year": "2024",
      "doi": "10.48550/arXiv.2110.08527",
      "authors": "Yang Yifan et al.",
      "keywords": "AI; bias; biomedicine; fairness",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38529077/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "NLP/LLM; Federated Learning; Computer Vision/Imaging AI; Clustering",
      "health_domain": "Surgery",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Development/Training; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Adversarial Debiasing; Data Augmentation; Representation Learning; Federated Learning",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: The bias of AIs in biomedicine can originate from multiple sources such as insufficient data, sampling bias and the use of health-irrelevant features or race-adjusted algorithms. Existing debiasing methods that focus on algorithms can be categorized into distributional or algorithmic. Distributional methods include data augmentation, data perturbation, data reweighting methods, and federated learning.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10962742"
    },
    {
      "pmid": "38531676",
      "title": "Preparing for the bedside-optimizing a postpartum depression risk prediction model for clinical implementation in a health system.",
      "abstract": "OBJECTIVE: We developed and externally validated a machine-learning model to predict postpartum depression (PPD) using data from electronic health records (EHRs). Effort is under way to implement the PPD prediction model within the EHR system for clinical decision support. We describe the pre-implementation evaluation process that considered model performance, fairness, and clinical appropriateness. MATERIALS AND METHODS: We used EHR data from an academic medical center (AMC) and a clinical research network database from 2014 to 2020 to evaluate the predictive performance and net benefit of the PPD risk model. We used area under the curve and sensitivity as predictive performance and conducted a decision curve analysis. In assessing model fairness, we employed metrics such as disparate impact, equal opportunity, and predictive parity with the White race being the privileged value. The model was also reviewed by multidisciplinary experts for clinical appropriateness. Lastly, we debiased the model by comparing 5 different debiasing approaches of fairness through blindness and reweighing. RESULTS: We determined the classification threshold through a performance evaluation that prioritized sensitivity and decision curve analysis. The baseline PPD model exhibited some unfairness in the AMC data but had a fair performance in the clinical research network data. We revised the model by fairness through blindness, a debiasing approach that yielded the best overall performance and fairness, while considering clinical appropriateness suggested by the expert reviewers. DISCUSSION AND CONCLUSION: The findings emphasize the need for a thorough evaluation of intervention-specific models, considering predictive performance, fairness, and appropriateness before clinical implementation.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2024",
      "doi": "10.1093/jamia/ocae056",
      "authors": "Liu Yifan et al.",
      "keywords": "electronic health record; health equity; machine learning implementation; postpartum depression",
      "mesh_terms": "Humans; Depression, Postpartum; Female; Electronic Health Records; Machine Learning; Risk Assessment; Decision Support Systems, Clinical",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38531676/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clinical Prediction Model; Clinical Decision Support",
      "health_domain": "Mental Health/Psychiatry; EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Threshold Adjustment; Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: The findings emphasize the need for a thorough evaluation of intervention-specific models, considering predictive performance, fairness, and appropriateness before clinical implementation.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11105144"
    },
    {
      "pmid": "38570587",
      "title": "Fairness and bias correction in machine learning for depression prediction across four study populations.",
      "abstract": "A significant level of stigma and inequality exists in mental healthcare, especially in under-served populations. Inequalities are reflected in the data collected for scientific purposes. When not properly accounted for, machine learning (ML) models learned from data can reinforce these structural inequalities or biases. Here, we present a systematic study of bias in ML models designed to predict depression in four different case studies covering different countries and populations. We find that standard ML approaches regularly present biased behaviors. We also show that mitigation techniques, both standard and our own post-hoc method, can be effective in reducing the level of unfair bias. There is no one best ML model for depression prediction that provides equality of outcomes. This emphasizes the importance of analyzing fairness during model selection and transparent reporting about the impact of debiasing interventions. Finally, we also identify positive habits and open challenges that practitioners could follow to enhance fairness in their models.",
      "journal": "Scientific reports",
      "year": "2024",
      "doi": "10.1038/s41598-024-58427-7",
      "authors": "Dang Vien Ngoc et al.",
      "keywords": "Algorithmic fairness; Bias mitigation; Machine learning for depression prediction; Novel post-hoc method; Psychiatric healthcare equity",
      "mesh_terms": "Humans; Depression; Bias; Habits; Health Facilities; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38570587/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Post-hoc Correction",
      "clinical_setting": "Public Health/Population",
      "key_findings": "This emphasizes the importance of analyzing fairness during model selection and transparent reporting about the impact of debiasing interventions. Finally, we also identify positive habits and open challenges that practitioners could follow to enhance fairness in their models.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10991528"
    },
    {
      "pmid": "38585746",
      "title": "Algorithmic Individual Fairness and Healthcare: A Scoping Review.",
      "abstract": "OBJECTIVE: Statistical and artificial intelligence algorithms are increasingly being developed for use in healthcare. These algorithms may reflect biases that magnify disparities in clinical care, and there is a growing need for understanding how algorithmic biases can be mitigated in pursuit of algorithmic fairness. Individual fairness in algorithms constrains algorithms to the notion that \"similar individuals should be treated similarly.\" We conducted a scoping review on algorithmic individual fairness to understand the current state of research in the metrics and methods developed to achieve individual fairness and its applications in healthcare. METHODS: We searched three databases, PubMed, ACM Digital Library, and IEEE Xplore, for algorithmic individual fairness metrics, algorithmic bias mitigation, and healthcare applications. Our search was restricted to articles published between January 2013 and September 2023. We identified 1,886 articles through database searches and manually identified one article from which we included 30 articles in the review. Data from the selected articles were extracted, and the findings were synthesized. RESULTS: Based on the 30 articles in the review, we identified several themes, including philosophical underpinnings of fairness, individual fairness metrics, mitigation methods for achieving individual fairness, implications of achieving individual fairness on group fairness and vice versa, fairness metrics that combined individual fairness and group fairness, software for measuring and optimizing individual fairness, and applications of individual fairness in healthcare. CONCLUSION: While there has been significant work on algorithmic individual fairness in recent years, the definition, use, and study of individual fairness remain in their infancy, especially in healthcare. Future research is needed to apply and evaluate individual fairness in healthcare comprehensively.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2024",
      "doi": "10.1101/2024.03.25.24304853",
      "authors": "Anderson Joshua W et al.",
      "keywords": "algorithmic fairness; health disparities; healthcare; individual fairness",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38585746/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: While there has been significant work on algorithmic individual fairness in recent years, the definition, use, and study of individual fairness remain in their infancy, especially in healthcare. Future research is needed to apply and evaluate individual fairness in healthcare comprehensively.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10996729"
    },
    {
      "pmid": "38638465",
      "title": "Optimal site selection strategies for urban parks green spaces under the joint perspective of spatial equity and social equity.",
      "abstract": "Urban park green spaces (UPGS) are a crucial element of social public resources closely related to the health and well-being of urban residents, and issues of equity have always been a focal point of concern. This study takes the downtown area of Nanchang as an example and uses more accurate point of interest (POI) and area of interest (AOI) data as analysis sources. The improved Gaussian two-step floating catchment area (G2SFCA) and spatial autocorrelation models are then used to assess the spatial and social equity in the study area, and the results of the two assessments were coupled to determine the optimization objective using the community as the smallest unit. Finally, the assessment results are combined with the k-means algorithm and particle swarm algorithm (PSO) to propose practical optimization strategies with the objectives of minimum walking distance and maximum fairness. The results indicate (1) There are significant differences in UPGS accessibility among residents with different walking distances, with the more densely populated Old Town and Honggu Tan areas having lower average accessibility and being the main areas of hidden blindness, while the fringe areas in the northern and south-western parts of the city are the main areas of visible blindness. (2) Overall, the UPGS accessibility in Nanchang City exhibits a spatial pattern of decreasing from the east, south, and west to the center. Nanchang City is in transition towards improving spatial and social equity while achieving basic regional equity. (3) There is a spatial positive correlation between socioeconomic level and UPGS accessibility, reflecting certain social inequity. (4) Based on the above research results, the UPGS layout optimization scheme was proposed, 29 new UPGS locations and regions were identified, and the overall accessibility was improved by 2.76. The research methodology and framework can be used as a tool to identify the underserved areas of UPGS and optimize the spatial and social equity of UPGS, which is in line with the current trend of urban development in the world and provides a scientific basis for urban infrastructure planning and spatial resource allocation.",
      "journal": "Frontiers in public health",
      "year": "2024",
      "doi": "10.3389/fpubh.2024.1310340",
      "authors": "Zhao Youqiang et al.",
      "keywords": "Gaussian two-step floating catchment area; accessibility; park quality; social equity; spatial equity; urban park green spaces",
      "mesh_terms": "Humans; Parks, Recreational; Cities; Spatial Analysis; Social Class; Blindness",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38638465/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Clustering",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Safety-Net/Underserved",
      "key_findings": "(4) Based on the above research results, the UPGS layout optimization scheme was proposed, 29 new UPGS locations and regions were identified, and the overall accessibility was improved by 2.76. The research methodology and framework can be used as a tool to identify the underserved areas of UPGS and optimize the spatial and social equity of UPGS, which is in line with the current trend of urban development in the world and provides a scientific basis for urban infrastructure planning and spatial...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11024374"
    },
    {
      "pmid": "38641744",
      "title": "Demographic bias in misdiagnosis by computational pathology models.",
      "abstract": "Despite increasing numbers of regulatory approvals, deep learning-based computational pathology systems often overlook the impact of demographic factors on performance, potentially leading to biases. This concern is all the more important as computational pathology has leveraged large public datasets that underrepresent certain demographic groups. Using publicly available data from The Cancer Genome Atlas and the EBRAINS brain tumor atlas, as well as internal patient data, we show that whole-slide image classification models display marked performance disparities across different demographic groups when used to subtype breast and lung carcinomas and to predict IDH1 mutations in gliomas. For example, when using common modeling approaches, we observed performance gaps (in area under the receiver operating characteristic curve) between white and Black patients of 3.0% for breast cancer subtyping, 10.9% for lung cancer subtyping and 16.0% for IDH1 mutation prediction in gliomas. We found that richer feature representations obtained from self-supervised vision foundation models reduce performance variations between groups. These representations provide improvements upon weaker models even when those weaker models are combined with state-of-the-art bias mitigation strategies and modeling choices. Nevertheless, self-supervised vision foundation models do not fully eliminate these discrepancies, highlighting the continuing need for bias mitigation efforts in computational pathology. Finally, we demonstrate that our results extend to other demographic factors beyond patient race. Given these findings, we encourage regulatory and policy agencies to integrate demographic-stratified evaluation into their assessment guidelines.",
      "journal": "Nature medicine",
      "year": "2024",
      "doi": "10.1038/s41591-024-02885-z",
      "authors": "Vaidya Anurag et al.",
      "keywords": "",
      "mesh_terms": "Humans; Bias; Black or African American; Black People; Demography; Diagnostic Errors; Glioma; Lung Neoplasms; White; Isocitrate Dehydrogenase",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38641744/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Deep Learning; Computer Vision/Imaging AI; Foundation Model",
      "health_domain": "Oncology; Pathology; Pulmonology; Neurology; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "Finally, we demonstrate that our results extend to other demographic factors beyond patient race. Given these findings, we encourage regulatory and policy agencies to integrate demographic-stratified evaluation into their assessment guidelines.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38649446",
      "title": "Measuring algorithmic bias to analyze the reliability of AI tools that predict depression risk using smartphone sensed-behavioral data.",
      "abstract": "AI tools intend to transform mental healthcare by providing remote estimates of depression risk using behavioral data collected by sensors embedded in smartphones. While these tools accurately predict elevated depression symptoms in small, homogenous populations, recent studies show that these tools are less accurate in larger, more diverse populations. In this work, we show that accuracy is reduced because sensed-behaviors are unreliable predictors of depression across individuals: sensed-behaviors that predict depression risk are inconsistent across demographic and socioeconomic subgroups. We first identified subgroups where a developed AI tool underperformed by measuring algorithmic bias, where subgroups with depression were incorrectly predicted to be at lower risk than healthier subgroups. We then found inconsistencies between sensed-behaviors predictive of depression across these subgroups. Our findings suggest that researchers developing AI tools predicting mental health from sensed-behaviors should think critically about the generalizability of these tools, and consider tailored solutions for targeted populations.",
      "journal": "Npj mental health research",
      "year": "2024",
      "doi": "10.1038/s44184-024-00057-y",
      "authors": "Adler Daniel A et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38649446/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Telehealth/Remote",
      "key_findings": "We then found inconsistencies between sensed-behaviors predictive of depression across these subgroups. Our findings suggest that researchers developing AI tools predicting mental health from sensed-behaviors should think critically about the generalizability of these tools, and consider tailored solutions for targeted populations.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11035598"
    },
    {
      "pmid": "38677633",
      "title": "A survey of recent methods for addressing AI fairness and bias in biomedicine.",
      "abstract": "OBJECTIVES: Artificial intelligence (AI) systems have the potential to revolutionize clinical practices, including improving diagnostic accuracy and surgical decision-making, while also reducing costs and manpower. However, it is important to recognize that these systems may perpetuate social inequities or demonstrate biases, such as those based on race or gender. Such biases can occur before, during, or after the development of AI models, making it critical to understand and address potential biases to enable the accurate and reliable application of AI models in clinical settings. To mitigate bias concerns during model development, we surveyed recent publications on different debiasing methods in the fields of biomedical natural language processing (NLP) or computer vision (CV). Then we discussed the methods, such as data perturbation and adversarial learning, that have been applied in the biomedical domain to address bias. METHODS: We performed our literature search on PubMed, ACM digital library, and IEEE Xplore of relevant articles published between January 2018 and December 2023 using multiple combinations of keywords. We then filtered the result of 10,041 articles automatically with loose constraints, and manually inspected the abstracts of the remaining 890 articles to identify the 55 articles included in this review. Additional articles in the references are also included in this review. We discuss each method and compare its strengths and weaknesses. Finally, we review other potential methods from the general domain that could be applied to biomedicine to address bias and improve fairness. RESULTS: The bias of AIs in biomedicine can originate from multiple sources such as insufficient data, sampling bias and the use of health-irrelevant features or race-adjusted algorithms. Existing debiasing methods that focus on algorithms can be categorized into distributional or algorithmic. Distributional methods include data augmentation, data perturbation, data reweighting methods, and federated learning. Algorithmic approaches include unsupervised representation learning, adversarial learning, disentangled representation learning, loss-based methods and causality-based methods.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104646",
      "authors": "Yang Yifan et al.",
      "keywords": "AI; Bias; Biomedicine; Fairness",
      "mesh_terms": "Artificial Intelligence; Humans; Natural Language Processing; Bias; Surveys and Questionnaires; Machine Learning; Algorithms",
      "pub_types": "Journal Article; Review; Research Support, N.I.H., Intramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38677633/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "NLP/LLM; Federated Learning; Computer Vision/Imaging AI; Clustering",
      "health_domain": "Surgery",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Development/Training; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Adversarial Debiasing; Data Augmentation; Representation Learning; Federated Learning",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: The bias of AIs in biomedicine can originate from multiple sources such as insufficient data, sampling bias and the use of health-irrelevant features or race-adjusted algorithms. Existing debiasing methods that focus on algorithms can be categorized into distributional or algorithmic. Distributional methods include data augmentation, data perturbation, data reweighting methods, and federated learning.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11129918"
    },
    {
      "pmid": "38680842",
      "title": "Inherent Bias in Electronic Health Records: A Scoping Review of Sources of Bias.",
      "abstract": "OBJECTIVES: 1.1Biases inherent in electronic health records (EHRs), and therefore in medical artificial intelligence (AI) models may significantly exacerbate health inequities and challenge the adoption of ethical and responsible AI in healthcare. Biases arise from multiple sources, some of which are not as documented in the literature. Biases are encoded in how the data has been collected and labeled, by implicit and unconscious biases of clinicians, or by the tools used for data processing. These biases and their encoding in healthcare records undermine the reliability of such data and bias clinical judgments and medical outcomes. Moreover, when healthcare records are used to build data-driven solutions, the biases are further exacerbated, resulting in systems that perpetuate biases and induce healthcare disparities. This literature scoping review aims to categorize the main sources of biases inherent in EHRs. METHODS: 1.2We queried PubMed and Web of Science on January 19th, 2023, for peer-reviewed sources in English, published between 2016 and 2023, using the PRISMA approach to stepwise scoping of the literature. To select the papers that empirically analyze bias in EHR, from the initial yield of 430 papers, 27 duplicates were removed, and 403 studies were screened for eligibility. 196 articles were removed after the title and abstract screening, and 96 articles were excluded after the full-text review resulting in a final selection of 116 articles. RESULTS: 1.3Systematic categorizations of diverse sources of bias are scarce in the literature, while the effects of separate studies are often convoluted and methodologically contestable. Our categorization of published empirical evidence identified the six main sources of bias: a) bias arising from past clinical trials; b) data-related biases arising from missing, incomplete information or poor labeling of data; human-related bias induced by c) implicit clinician bias, d) referral and admission bias; e) diagnosis or risk disparities bias and finally, (f) biases in machinery and algorithms. CONCLUSIONS: 1.4Machine learning and data-driven solutions can potentially transform healthcare delivery, but not without limitations. The core inputs in the systems (data and human factors) currently contain several sources of bias that are poorly documented and analyzed for remedies. The current evidence heavily focuses on data-related biases, while other sources are less often analyzed or anecdotal. However, these different sources of biases add to one another exponentially. Therefore, to understand the issues holistically we need to explore these diverse sources of bias. While racial biases in EHR have been often documented, other sources of biases have been less frequently investigated and documented (e.g. gender-related biases, sexual orientation discrimination, socially induced biases, and implicit, often unconscious, human-related cognitive biases). Moreover, some existing studies lack causal evidence, illustrating the different prevalences of disease across groups, which does not per se prove the causality. Our review shows that data-, human- and machine biases are prevalent in healthcare and they significantly impact healthcare outcomes and judgments and exacerbate disparities and differential treatment. Understanding how diverse biases affect AI systems and recommendations is critical. We suggest that researchers and medical personnel should develop safeguards and adopt data-driven solutions with a \"bias-in-mind\" approach. More empirical evidence is needed to tease out the effects of different sources of bias on health outcomes.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2024",
      "doi": "10.1101/2024.04.09.24305594",
      "authors": "Perets Oriel et al.",
      "keywords": "AI Bias; Clinical Trial; Electronic Health Records (EHRs); Human Bias; Implicit bias; Machine Learning (ML); Medical Machinery",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38680842/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Clinical Trial",
      "key_findings": "CONCLUSIONS: 1.4Machine learning and data-driven solutions can potentially transform healthcare delivery, but not without limitations. The core inputs in the systems (data and human factors) currently contain several sources of bias that are poorly documented and analyzed for remedies. The current evidence heavily focuses on data-related biases, while other sources are less often analyzed or anecdotal.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11046491"
    },
    {
      "pmid": "38681756",
      "title": "Improving Equity in Deep Learning Medical Applications with the Gerchberg-Saxton Algorithm.",
      "abstract": "Deep learning (DL) has gained prominence in healthcare for its ability to facilitate early diagnosis, treatment identification with associated prognosis, and varying patient outcome predictions. However, because of highly variable medical practices and unsystematic data collection approaches, DL can unfortunately exacerbate biases and distort estimates. For example, the presence of sampling bias poses a significant challenge to the efficacy and generalizability of any statistical model. Even with DL approaches, selection bias can lead to inconsistent, suboptimal, or inaccurate model results, especially for underrepresented populations. Therefore, without addressing bias, wider implementation of DL approaches can potentially cause unintended harm. In this paper, we studied a novel method for bias reduction that leverages the frequency domain transformation via the Gerchberg-Saxton and corresponding impact on the outcome from a racio-ethnic bias perspective.",
      "journal": "Journal of healthcare informatics research",
      "year": "2024",
      "doi": "10.1007/s41666-024-00163-8",
      "authors": "Ay Seha et al.",
      "keywords": "Deep learning; MIMIC-III; Medical decision-making; Mortality rate prediction; Racial bias mitigation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38681756/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Deep Learning",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Therefore, without addressing bias, wider implementation of DL approaches can potentially cause unintended harm. In this paper, we studied a novel method for bias reduction that leverages the frequency domain transformation via the Gerchberg-Saxton and corresponding impact on the outcome from a racio-ethnic bias perspective.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11052977"
    },
    {
      "pmid": "38685924",
      "title": "Health equity assessment of machine learning performance (HEAL): a framework and dermatology AI model case study.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) has repeatedly been shown to encode historical inequities in healthcare. We aimed to develop a framework to quantitatively assess the performance equity of health AI technologies and to illustrate its utility via a case study. METHODS: Here, we propose a methodology to assess whether health AI technologies prioritise performance for patient populations experiencing worse outcomes, that is complementary to existing fairness metrics. We developed the Health Equity Assessment of machine Learning performance (HEAL) framework designed to quantitatively assess the performance equity of health AI technologies via a four-step interdisciplinary process to understand and quantify domain-specific criteria, and the resulting HEAL metric. As an illustrative case study (analysis conducted between October 2022 and January 2023), we applied the HEAL framework to a dermatology AI model. A set of 5420 teledermatology cases (store-and-forward cases from patients of 20 years or older, submitted from primary care providers in the USA and skin cancer clinics in Australia), enriched for diversity in age, sex and race/ethnicity, was used to retrospectively evaluate the AI model's HEAL metric, defined as the likelihood that the AI model performs better for subpopulations with worse average health outcomes as compared to others. The likelihood that AI performance was anticorrelated to pre-existing health outcomes was estimated using bootstrap methods as the probability that the negated Spearman's rank correlation coefficient (i.e., \"R\") was greater than zero. Positive values of R suggest that subpopulations with poorer health outcomes have better AI model performance. Thus, the HEAL metric, defined as p (R\u00a0>0), measures how likely the AI technology is to prioritise performance for subpopulations with worse average health outcomes as compared to others (presented as a percentage below). Health outcomes were quantified as disability-adjusted life years (DALYs) when grouping by sex and age, and years of life lost (YLLs) when grouping by race/ethnicity. AI performance was measured as top-3 agreement with the reference diagnosis from a panel of 3 dermatologists per case. FINDINGS: Across all dermatologic conditions, the HEAL metric was 80.5% for prioritizing AI performance of racial/ethnic subpopulations based on YLLs, and 92.1% and 0.0% respectively for prioritizing AI performance of sex and age subpopulations based on DALYs. Certain dermatologic conditions were significantly associated with greater AI model performance compared to a reference category of less common conditions. For skin cancer conditions, the HEAL metric was 73.8% for prioritizing AI performance of age subpopulations based on DALYs. INTERPRETATION: Analysis using the proposed HEAL framework showed that the dermatology AI model prioritised performance for race/ethnicity, sex (all conditions) and age (cancer conditions) subpopulations with respect to pre-existing health disparities. More work is needed to investigate ways of promoting equitable AI performance across age for non-cancer conditions and to better understand how AI models can contribute towards improving equity in health outcomes. FUNDING: Google LLC.",
      "journal": "EClinicalMedicine",
      "year": "2024",
      "doi": "10.1016/j.eclinm.2024.102479",
      "authors": "Schaekermann Mike et al.",
      "keywords": "Artificial intelligence; Dermatology; Health equity; Machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38685924/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Dermatology; Oncology; Primary Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Disability",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Subgroup Analysis; Fairness Metrics Evaluation",
      "clinical_setting": "Primary Care/Outpatient; Public Health/Population",
      "key_findings": "FINDINGS: Across all dermatologic conditions, the HEAL metric was 80.5% for prioritizing AI performance of racial/ethnic subpopulations based on YLLs, and 92.1% and 0.0% respectively for prioritizing AI performance of sex and age subpopulations based on DALYs. Certain dermatologic conditions were significantly associated with greater AI model performance compared to a reference category of less common conditions. For skin cancer conditions, the HEAL metric was 73.8% for prioritizing AI performan...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11056401"
    },
    {
      "pmid": "38689643",
      "title": "Reinvestigating the performance of artificial intelligence classification algorithms on COVID-19 X-Ray and CT images.",
      "abstract": "There are concerns that artificial intelligence (AI) algorithms may create underdiagnosis bias by mislabeling patient individuals with certain attributes (e.g., female and young) as healthy. Addressing this bias is crucial given the urgent need for AI diagnostics facing rapidly spreading infectious diseases like COVID-19. We find the prevalent AI diagnostic models show an underdiagnosis rate among specific patient populations, and the underdiagnosis rate is higher in some intersectional specific patient populations (for example, females aged 20-40 years). Additionally, we find training AI models on heterogeneous datasets (positive and negative samples from different datasets) may lead to poor model generalization. The model's classification performance varies significantly across test sets, with the accuracy of the better performance being over 40% higher than that of the poor performance. In conclusion, we developed an AI bias analysis pipeline to help researchers recognize and address biases that impact medical equality and ethics.",
      "journal": "iScience",
      "year": "2024",
      "doi": "10.1016/j.isci.2024.109712",
      "authors": "Cao Rui et al.",
      "keywords": "Artificial intelligence applications; Health informatics; Microbiology",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38689643/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Pulmonology; Infectious Disease",
      "bias_axes": "Gender/Sex; Age; Intersectional",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "The model's classification performance varies significantly across test sets, with the accuracy of the better performance being over 40% higher than that of the poor performance. In conclusion, we developed an AI bias analysis pipeline to help researchers recognize and address biases that impact medical equality and ethics.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11059117"
    },
    {
      "pmid": "38696359",
      "title": "Achieving health equity through conversational AI: A roadmap for design and implementation of inclusive chatbots in healthcare.",
      "abstract": "BACKGROUND: The rapid evolution of conversational and generative artificial intelligence (AI) has led to the increased deployment of AI tools in healthcare settings. While these conversational AI tools promise efficiency and expanded access to healthcare services, there are growing concerns ethically, practically and in terms of inclusivity. This study aimed to identify activities which reduce bias in conversational AI and make their designs and implementation more equitable. METHODS: A qualitative research approach was employed to develop an analytical framework based on the content analysis of 17 guidelines about AI use in clinical settings. A stakeholder consultation was subsequently conducted with a total of 33 ethnically diverse community members, AI designers, industry experts and relevant health professionals to further develop a roadmap for equitable design and implementation of conversational AI in healthcare. Framework analysis was conducted on the interview data. RESULTS: A 10-stage roadmap was developed to outline activities relevant to equitable conversational AI design and implementation phases: 1) Conception and planning, 2) Diversity and collaboration, 3) Preliminary research, 4) Co-production, 5) Safety measures, 6) Preliminary testing, 7) Healthcare integration, 8) Service evaluation and auditing, 9) Maintenance, and 10) Termination. DISCUSSION: We have made specific recommendations to increase conversational AI's equity as part of healthcare services. These emphasise the importance of a collaborative approach and the involvement of patient groups in navigating the rapid evolution of conversational AI technologies. Further research must assess the impact of recommended activities on chatbots' fairness and their ability to reduce health inequalities.",
      "journal": "PLOS digital health",
      "year": "2024",
      "doi": "10.1371/journal.pdig.0000492",
      "authors": "Nadarzynski Tom et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38696359/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Bias Auditing Framework",
      "clinical_setting": "Public Health/Population",
      "key_findings": "RESULTS: A 10-stage roadmap was developed to outline activities relevant to equitable conversational AI design and implementation phases: 1) Conception and planning, 2) Diversity and collaboration, 3) Preliminary research, 4) Co-production, 5) Safety measures, 6) Preliminary testing, 7) Healthcare integration, 8) Service evaluation and auditing, 9) Maintenance, and 10) Termination. DISCUSSION: We have made specific recommendations to increase conversational AI's equity as part of healthcare serv...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11065243"
    },
    {
      "pmid": "38701410",
      "title": "mbDecoda: a debiased approach to compositional data analysis for microbiome surveys.",
      "abstract": "Potentially pathogenic or probiotic microbes can be identified by comparing their abundance levels between healthy and diseased populations, or more broadly, by linking microbiome composition with clinical phenotypes or environmental factors. However, in microbiome studies, feature tables provide relative rather than absolute abundance of each feature in each sample, as the microbial loads of the samples and the ratios of sequencing depth to microbial load are both unknown and subject to considerable variation. Moreover, microbiome abundance data are count-valued, often over-dispersed and contain a substantial proportion of zeros. To carry out differential abundance analysis while addressing these challenges, we introduce mbDecoda, a model-based approach for debiased analysis of sparse compositions of microbiomes. mbDecoda employs a zero-inflated negative binomial model, linking mean abundance to the variable of interest through a log link function, and it accommodates the adjustment for confounding factors. To efficiently obtain maximum likelihood estimates of model parameters, an Expectation Maximization algorithm is developed. A minimum coverage interval approach is then proposed to rectify compositional bias, enabling accurate and reliable absolute abundance analysis. Through extensive simulation studies and analysis of real-world microbiome datasets, we demonstrate that mbDecoda compares favorably with state-of-the-art methods in terms of effectiveness, robustness and reproducibility.",
      "journal": "Briefings in bioinformatics",
      "year": "2024",
      "doi": "10.1093/bib/bbae205",
      "authors": "Zong Yuxuan et al.",
      "keywords": "ANCOM; Bias correction; Differential abundance testing; Zero-inflated models",
      "mesh_terms": "Microbiota; Humans; Algorithms; Data Analysis",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38701410/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "A minimum coverage interval approach is then proposed to rectify compositional bias, enabling accurate and reliable absolute abundance analysis. Through extensive simulation studies and analysis of real-world microbiome datasets, we demonstrate that mbDecoda compares favorably with state-of-the-art methods in terms of effectiveness, robustness and reproducibility.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11066923"
    },
    {
      "pmid": "38718715",
      "title": "Achieve fairness without demographics for dermatological disease diagnosis.",
      "abstract": "In medical image diagnosis, fairness has become increasingly crucial. Without bias mitigation, deploying unfair AI would harm the interests of the underprivileged population and potentially tear society apart. Recent research addresses prediction biases in deep learning models concerning demographic groups (e.g., gender, age, and race) by utilizing demographic (sensitive attribute) information during training. However, many sensitive attributes naturally exist in dermatological disease images. If the trained model only targets fairness for a specific attribute, it remains unfair for other attributes. Moreover, training a model that can accommodate multiple sensitive attributes is impractical due to privacy concerns. To overcome this, we propose a method enabling fair predictions for sensitive attributes during the testing phase without using such information during training. Inspired by prior work highlighting the impact of feature entanglement on fairness, we enhance the model features by capturing the features related to the sensitive and target attributes and regularizing the feature entanglement between corresponding classes. This ensures that the model can only classify based on the features related to the target attribute without relying on features associated with sensitive attributes, thereby improving fairness and accuracy. Additionally, we use disease masks from the Segment Anything Model (SAM) to enhance the quality of the learned feature. Experimental results demonstrate that the proposed method can improve fairness in classification compared to state-of-the-art methods in two dermatological disease datasets.",
      "journal": "Medical image analysis",
      "year": "2024",
      "doi": "10.1016/j.media.2024.103188",
      "authors": "Chiu Ching-Hao et al.",
      "keywords": "AI fairness; Dermatological disease diagnosis; Fairness through unawareness",
      "mesh_terms": "Humans; Skin Diseases; Deep Learning; Image Interpretation, Computer-Assisted; Demography",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38718715/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Dermatology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Intersectional",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Regularization",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Additionally, we use disease masks from the Segment Anything Model (SAM) to enhance the quality of the learned feature. Experimental results demonstrate that the proposed method can improve fairness in classification compared to state-of-the-art methods in two dermatological disease datasets.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38723025",
      "title": "Development and preliminary testing of Health Equity Across the AI Lifecycle (HEAAL): A framework for healthcare delivery organizations to mitigate the risk of AI solutions worsening health inequities.",
      "abstract": "The use of data-driven technologies such as Artificial Intelligence (AI) and Machine Learning (ML) is growing in healthcare. However, the proliferation of healthcare AI tools has outpaced regulatory frameworks, accountability measures, and governance standards to ensure safe, effective, and equitable use. To address these gaps and tackle a common challenge faced by healthcare delivery organizations, a case-based workshop was organized, and a framework was developed to evaluate the potential impact of implementing an AI solution on health equity. The Health Equity Across the AI Lifecycle (HEAAL) is co-designed with extensive engagement of clinical, operational, technical, and regulatory leaders across healthcare delivery organizations and ecosystem partners in the US. It assesses 5 equity assessment domains-accountability, fairness, fitness for purpose, reliability and validity, and transparency-across the span of eight key decision points in the AI adoption lifecycle. It is a process-oriented framework containing 37 step-by-step procedures for evaluating an existing AI solution and 34 procedures for evaluating a new AI solution in total. Within each procedure, it identifies relevant key stakeholders and data sources used to conduct the procedure. HEAAL guides how healthcare delivery organizations may mitigate the potential risk of AI solutions worsening health inequities. It also informs how much resources and support are required to assess the potential impact of AI solutions on health inequities.",
      "journal": "PLOS digital health",
      "year": "2024",
      "doi": "10.1371/journal.pdig.0000390",
      "authors": "Kim Jee Young et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38723025/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "HEAAL guides how healthcare delivery organizations may mitigate the potential risk of AI solutions worsening health inequities. It also informs how much resources and support are required to assess the potential impact of AI solutions on health inequities.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11081364"
    },
    {
      "pmid": "38725085",
      "title": "Preoperative prediction model for risk of readmission after total joint replacement surgery: a random forest approach leveraging NLP and unfairness mitigation for improved patient care and cost-effectiveness.",
      "abstract": "BACKGROUND: The Center for Medicare and Medicaid Services (CMS) imposes payment penalties for readmissions following total joint replacement surgeries. This study focuses on total hip, knee, and shoulder arthroplasty procedures as they account for most joint replacement surgeries. Apart from being a burden to healthcare systems, readmissions are also troublesome for patients. There are several studies which only utilized structured data from Electronic Health Records (EHR) without considering any gender and payor bias adjustments. METHODS: For this study, dataset of 38,581 total knee, hip, and shoulder replacement surgeries performed from 2015 to 2021 at Novant Health was gathered. This data was used to train a random forest machine learning model to predict the combined endpoint of emergency department (ED) visit or unplanned readmissions within 30 days of discharge or discharge to Skilled Nursing Facility (SNF) following the surgery. 98 features of laboratory results, diagnoses, vitals, medications, and utilization history were extracted. A natural language processing (NLP) model finetuned from Clinical BERT was used to generate an NLP risk score feature for each patient based on their clinical notes. To address societal biases, a feature bias analysis was performed in conjunction with propensity score matching. A threshold optimization algorithm from the Fairlearn toolkit was used to mitigate gender and payor biases to promote fairness in predictions. RESULTS: The model achieved an Area Under the Receiver Operating characteristic Curve (AUROC) of 0.738 (95% confidence interval, 0.724 to 0.754) and an Area Under the Precision-Recall Curve (AUPRC) of 0.406 (95% confidence interval, 0.384 to 0.433). Considering an outcome prevalence of 16%, these metrics indicate the model's ability to accurately discriminate between readmission and non-readmission cases within the context of total arthroplasty surgeries while adjusting patient scores in the model to mitigate bias based on patient gender and payor. CONCLUSION: This work culminated in a model that identifies the most predictive and protective features associated with the combined endpoint. This model serves as a tool to empower healthcare providers to proactively intervene based on these influential factors without introducing bias towards protected patient classes, effectively mitigating the risk of negative outcomes and ultimately improving quality of care regardless of socioeconomic factors.",
      "journal": "Journal of orthopaedic surgery and research",
      "year": "2024",
      "doi": "10.1186/s13018-024-04774-0",
      "authors": "Digumarthi Varun et al.",
      "keywords": "Classification; Fairlearn; Natural language processing; Orthopedic; Predictive model",
      "mesh_terms": "Humans; Patient Readmission; Female; Male; Machine Learning; Aged; Cost-Benefit Analysis; Natural Language Processing; Middle Aged; Arthroplasty, Replacement, Knee; Arthroplasty, Replacement, Hip; Arthroplasty, Replacement; Risk Assessment; Preoperative Period; Aged, 80 and over; Quality Improvement; Random Forest",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38725085/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "NLP/LLM; Random Forest; Clinical Prediction Model",
      "health_domain": "Emergency Medicine; EHR/Health Informatics; Surgery",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Language; Insurance Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "Emergency Department; Laboratory/Pathology",
      "key_findings": "CONCLUSION: This work culminated in a model that identifies the most predictive and protective features associated with the combined endpoint. This model serves as a tool to empower healthcare providers to proactively intervene based on these influential factors without introducing bias towards protected patient classes, effectively mitigating the risk of negative outcomes and ultimately improving quality of care regardless of socioeconomic factors.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11084055"
    },
    {
      "pmid": "38782170",
      "title": "Causal fairness assessment of treatment allocation with electronic health records.",
      "abstract": "OBJECTIVE: Healthcare continues to grapple with the persistent issue of treatment disparities, sparking concerns regarding the equitable allocation of treatments in clinical practice. While various fairness metrics have emerged to assess fairness in decision-making processes, a growing focus has been on causality-based fairness concepts due to their capacity to mitigate confounding effects and reason about bias. However, the application of causal fairness notions in evaluating the fairness of clinical decision-making with electronic health record (EHR) data remains an understudied domain. This study aims to address the methodological gap in assessing causal fairness of treatment allocation with electronic health records data. In addition, we investigate the impact of social determinants of health on the assessment of causal fairness of treatment allocation. METHODS: We propose a causal fairness algorithm to assess fairness in clinical decision-making. Our algorithm accounts for the heterogeneity of patient populations and identifies potential unfairness in treatment allocation by conditioning on patients who have the same likelihood to benefit from the treatment. We apply this framework to a patient cohort with coronary artery disease derived from an EHR database to evaluate the fairness of treatment decisions. RESULTS: Our analysis reveals notable disparities in coronary artery bypass grafting (CABG) allocation among different patient groups. Women were found to be 4.4%-7.7% less likely to receive CABG than men in two out of four treatment response strata. Similarly, Black or African American patients were 5.4%-8.7% less likely to receive CABG than others in three out of four response strata. These results were similar when social determinants of health (insurance and area deprivation index) were dropped from the algorithm. These findings highlight the presence of disparities in treatment allocation among similar patients, suggesting potential unfairness in the clinical decision-making process. CONCLUSION: This study introduces a novel approach for assessing the fairness of treatment allocation in healthcare. By incorporating responses to treatment into fairness framework, our method explores the potential of quantifying fairness from a causal perspective using EHR data. Our research advances the methodological development of fairness assessment in healthcare and highlight the importance of causality in determining treatment fairness.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104656",
      "authors": "Zhang Linying et al.",
      "keywords": "Causal fairness; Electronic health record; Health equity; Machine learning; Principal fairness",
      "mesh_terms": "Humans; Electronic Health Records; Algorithms; Male; Female; Clinical Decision-Making; Coronary Artery Disease; Healthcare Disparities; Middle Aged; Social Determinants of Health; Causality",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38782170/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Counterfactual Fairness",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: This study introduces a novel approach for assessing the fairness of treatment allocation in healthcare. By incorporating responses to treatment into fairness framework, our method explores the potential of quantifying fairness from a causal perspective using EHR data. Our research advances the methodological development of fairness assessment in healthcare and highlight the importance of causality in determining treatment fairness.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11180553"
    },
    {
      "pmid": "38814939",
      "title": "Identifying bias in models that detect vocal fold paralysis from audio recordings using explainable machine learning and clinician ratings.",
      "abstract": "Detecting voice disorders from voice recordings could allow for frequent, remote, and low-cost screening before costly clinical visits and a more invasive laryngoscopy examination. Our goals were to detect unilateral vocal fold paralysis (UVFP) from voice recordings using machine learning, to identify which acoustic variables were important for prediction to increase trust, and to determine model performance relative to clinician performance. Patients with confirmed UVFP through endoscopic examination (N = 77) and controls with normal voices matched for age and sex (N = 77) were included. Voice samples were elicited by reading the Rainbow Passage and sustaining phonation of the vowel \"a\". Four machine learning models of differing complexity were used. SHapley Additive exPlanations (SHAP) was used to identify important features. The highest median bootstrapped ROC AUC score was 0.87 and beat clinician's performance (range: 0.74-0.81) based on the recordings. Recording durations were different between UVFP recordings and controls due to how that data was originally processed when storing, which we can show can classify both groups. And counterintuitively, many UVFP recordings had higher intensity than controls, when UVFP patients tend to have weaker voices, revealing a dataset-specific bias which we mitigate in an additional analysis. We demonstrate that recording biases in audio duration and intensity created dataset-specific differences between patients and controls, which models used to improve classification. Furthermore, clinician's ratings provide further evidence that patients were over-projecting their voices and being recorded at a higher amplitude signal than controls. Interestingly, after matching audio duration and removing variables associated with intensity in order to mitigate the biases, the models were able to achieve a similar high performance. We provide a set of recommendations to avoid bias when building and evaluating machine learning models for screening in laryngology.",
      "journal": "PLOS digital health",
      "year": "2024",
      "doi": "10.1371/journal.pdig.0000516",
      "authors": "Low Daniel M et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38814939/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Telehealth/Remote",
      "key_findings": "Interestingly, after matching audio duration and removing variables associated with intensity in order to mitigate the biases, the models were able to achieve a similar high performance. We provide a set of recommendations to avoid bias when building and evaluating machine learning models for screening in laryngology.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11139298"
    },
    {
      "pmid": "38827050",
      "title": "FERI: A Multitask-based Fairness Achieving Algorithm with Applications to Fair Organ Transplantation.",
      "abstract": "Liver transplantation often faces fairness challenges across subgroups defined by sensitive attributes such as age group, gender, and race/ethnicity. Machine learning models for outcome prediction can introduce additional biases. Therefore, we introduce Fairness through the Equitable Rate of Improvement in Multitask Learning (FERI) algorithm for fair predictions of graft failure risk in liver transplant patients. FERI constrains subgroup loss by balancing learning rates and preventing subgroup dominance in the training process. Our results show that FERI maintained high predictive accuracy with AUROC and AUPRC comparable to baseline models. More importantly, FERI demonstrated an ability to improve fairness without sacrificing accuracy. Specifically, for the gender, FERI reduced the demographic parity disparity by 71.74%, and for the age group, it decreased the equalized odds disparity by 40.46%. Therefore, the FERI algorithm advanced fairness-aware predictive modeling in healthcare and provides an invaluable tool for equitable healthcare systems.",
      "journal": "AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science",
      "year": "2024",
      "doi": "10.1101/2023.08.04.551906",
      "authors": "Li Can et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38827050/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clinical Prediction Model; Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Fairness Metrics Evaluation; Multi-task Learning",
      "clinical_setting": "Not specified",
      "key_findings": "Specifically, for the gender, FERI reduced the demographic parity disparity by 71.74%, and for the age group, it decreased the equalized odds disparity by 40.46%. Therefore, the FERI algorithm advanced fairness-aware predictive modeling in healthcare and provides an invaluable tool for equitable healthcare systems.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11141863"
    },
    {
      "pmid": "38834557",
      "title": "Fairer AI in ophthalmology via implicit fairness learning for mitigating sexism and ageism.",
      "abstract": "The transformative role of artificial intelligence (AI) in various fields highlights the need for it to be both accurate and fair. Biased medical AI systems pose significant potential risks to achieving fair and equitable healthcare. Here, we show an implicit fairness learning approach to build a fairer ophthalmology AI (called FairerOPTH) that mitigates sex (biological attribute) and age biases in AI diagnosis of eye diseases. Specifically, FairerOPTH incorporates the causal relationship between fundus features and eye diseases, which is relatively independent of sensitive attributes such as race, sex, and age. We demonstrate on a large and diverse collected dataset that FairerOPTH significantly outperforms several state-of-the-art approaches in terms of diagnostic accuracy and fairness for 38 eye diseases in ultra-widefield imaging and 16 eye diseases in narrow-angle imaging. This work demonstrates the significant potential of implicit fairness learning in promoting equitable treatment for patients regardless of their sex or age.",
      "journal": "Nature communications",
      "year": "2024",
      "doi": "10.1038/s41467-024-48972-0",
      "authors": "Tan Weimin et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Female; Male; Ophthalmology; Sexism; Ageism; Eye Diseases; Middle Aged; Adult; Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38834557/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Ophthalmology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We demonstrate on a large and diverse collected dataset that FairerOPTH significantly outperforms several state-of-the-art approaches in terms of diagnostic accuracy and fairness for 38 eye diseases in ultra-widefield imaging and 16 eye diseases in narrow-angle imaging. This work demonstrates the significant potential of implicit fairness learning in promoting equitable treatment for patients regardless of their sex or age.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11150422"
    },
    {
      "pmid": "38844546",
      "title": "Assessing calibration and bias of a deployed machine learning malnutrition prediction model within a large healthcare system.",
      "abstract": "Malnutrition is a frequently underdiagnosed condition leading to increased morbidity, mortality, and healthcare costs. The Mount Sinai Health System (MSHS) deployed a machine learning model (MUST-Plus) to detect malnutrition upon hospital admission. However, in diverse patient groups, a poorly calibrated model may lead to misdiagnosis, exacerbating health care disparities. We explored the model's calibration across different variables and methods to improve calibration. Data from adult patients admitted to five MSHS hospitals from January 1, 2021 - December 31, 2022, were analyzed. We compared MUST-Plus prediction to the registered dietitian's formal assessment. Hierarchical calibration was assessed and compared between the recalibration sample (N\u2009=\u200949,562) of patients admitted between January 1, 2021 - December 31, 2022, and the hold-out sample (N\u2009=\u200917,278) of patients admitted between January 1, 2023 - September 30, 2023. Statistical differences in calibration metrics were tested using bootstrapping with replacement. Before recalibration, the overall model calibration intercept was -1.17 (95% CI: -1.20, -1.14), slope was 1.37 (95% CI: 1.34, 1.40), and Brier score was 0.26 (95% CI: 0.25, 0.26). Both weak and moderate measures of calibration were significantly different between White and Black patients and between male and female patients. Logistic recalibration significantly improved calibration of the model across race and gender in the hold-out sample. The original MUST-Plus model showed significant differences in calibration between White vs. Black patients. It also overestimated malnutrition in females compared to males. Logistic recalibration effectively reduced miscalibration across all patient subgroups. Continual monitoring and timely recalibration can improve model accuracy.",
      "journal": "NPJ digital medicine",
      "year": "2024",
      "doi": "10.1038/s41746-024-01141-5",
      "authors": "Liou Lathan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38844546/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Fairness Metrics Evaluation",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "Logistic recalibration effectively reduced miscalibration across all patient subgroups. Continual monitoring and timely recalibration can improve model accuracy.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11156633"
    },
    {
      "pmid": "38851413",
      "title": "Evaluating accuracy and fairness of clinical decision support algorithms when health care resources are limited.",
      "abstract": "OBJECTIVE: Guidance on how to evaluate accuracy and algorithmic fairness across subgroups is missing for clinical models that flag patients for an intervention but when health care resources to administer that intervention are limited. We aimed to propose a framework of metrics that would fit this specific use case. METHODS: We evaluated the following metrics and applied them to a Veterans Health Administration clinical model that flags patients for intervention who are at risk of overdose or a suicidal event among outpatients who were prescribed opioids (N\u00a0=\u00a0405,817): Receiver - Operating Characteristic and area under the curve, precision - recall curve, calibration - reliability curve, false positive rate, false negative rate, and false omission rate. In addition, we developed a new approach to visualize false positives and false negatives that we named 'per true positive bars.' We demonstrate the utility of these metrics to our use case for three cohorts of patients at the highest risk (top 0.5\u00a0%, 1.0\u00a0%, and 5.0\u00a0%) by evaluating algorithmic fairness across the following age groups: <=30, 31-50, 51-65, and\u00a0>65\u00a0years old. RESULTS: Metrics that allowed us to assess group differences more clearly were the false positive rate, false negative rate, false omission rate, and the new 'per true positive bars'. Metrics with limited utility to our use case were the Receiver - Operating Characteristic and area under the curve, the calibration - reliability curve, and the precision - recall curve. CONCLUSION: There is no \"one size fits all\" approach to model performance monitoring and bias analysis. Our work informs future researchers and clinicians who seek to evaluate accuracy and fairness of predictive models that identify patients to intervene on in the context of limited health care resources. In terms of ease of interpretation and utility for our use case, the new 'per true positive bars' may be the most intuitive to a range of stakeholders and facilitates choosing a threshold that allows weighing false positives against false negatives, which is especially important when predicting severe adverse events.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104664",
      "authors": "Meerwijk Esther L et al.",
      "keywords": "Algorithmic fairness; Bias; Prediction models; Suicide; Veterans",
      "mesh_terms": "Humans; Algorithms; Decision Support Systems, Clinical; Middle Aged; Adult; Aged; Reproducibility of Results; ROC Curve; Female; Male; United States; United States Department of Veterans Affairs",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38851413/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Prediction Model; Clinical Decision Support",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Age",
      "lifecycle_stage": "Model Development/Training; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Calibration; Threshold Adjustment",
      "clinical_setting": "Primary Care/Outpatient",
      "key_findings": "CONCLUSION: There is no \"one size fits all\" approach to model performance monitoring and bias analysis. Our work informs future researchers and clinicians who seek to evaluate accuracy and fairness of predictive models that identify patients to intervene on in the context of limited health care resources. In terms of ease of interpretation and utility for our use case, the new 'per true positive bars' may be the most intuitive to a range of stakeholders and facilitates choosing a threshold that ...",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38858466",
      "title": "Mitigating machine learning bias between high income and low-middle income countries for enhanced model fairness and generalizability.",
      "abstract": "Collaborative efforts in artificial intelligence (AI) are increasingly common between high-income countries (HICs) and low- to middle-income countries (LMICs). Given the resource limitations often encountered by LMICs, collaboration becomes crucial for pooling resources, expertise, and knowledge. Despite the apparent advantages, ensuring the fairness and equity of these collaborative models is essential, especially considering the distinct differences between LMIC and HIC hospitals. In this study, we show that collaborative AI approaches can lead to divergent performance outcomes across HIC and LMIC settings, particularly in the presence of data imbalances. Through a real-world COVID-19 screening case study, we demonstrate that implementing algorithmic-level bias mitigation methods significantly improves outcome fairness between HIC and LMIC sites while maintaining high diagnostic sensitivity. We compare our results against previous benchmarks, utilizing datasets from four independent United Kingdom Hospitals and one Vietnamese hospital, representing HIC and LMIC settings, respectively.",
      "journal": "Scientific reports",
      "year": "2024",
      "doi": "10.1038/s41598-024-64210-5",
      "authors": "Yang Jenny et al.",
      "keywords": "",
      "mesh_terms": "Humans; COVID-19; Machine Learning; Developing Countries; Developed Countries; SARS-CoV-2; United Kingdom; Bias; Vietnam; Income; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38858466/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Pulmonology",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; ICU",
      "key_findings": "Through a real-world COVID-19 screening case study, we demonstrate that implementing algorithmic-level bias mitigation methods significantly improves outcome fairness between HIC and LMIC sites while maintaining high diagnostic sensitivity. We compare our results against previous benchmarks, utilizing datasets from four independent United Kingdom Hospitals and one Vietnamese hospital, representing HIC and LMIC settings, respectively.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11164855"
    },
    {
      "pmid": "38876452",
      "title": "Identify and mitigate bias in electronic phenotyping: A comprehensive study from computational perspective.",
      "abstract": "Electronic phenotyping is a fundamental task that identifies the special group of patients, which plays an important role in precision medicine in the era of digital health. Phenotyping provides real-world evidence for other related biomedical research and clinical tasks, e.g., disease diagnosis, drug development, and clinical trials, etc. With the development of electronic health records, the performance of electronic phenotyping has been significantly boosted by advanced machine learning techniques. In the healthcare domain, precision and fairness are both essential aspects that should be taken into consideration. However, most related efforts are put into designing phenotyping models with higher accuracy. Few attention is put on the fairness perspective of phenotyping. The neglection of bias in phenotyping leads to subgroups of patients being underrepresented which will further affect the following healthcare activities such as patient recruitment in clinical trials. In this work, we are motivated to bridge this gap through a comprehensive experimental study to identify the bias existing in electronic phenotyping models and evaluate the widely-used debiasing methods' performance on these models. We choose pneumonia and sepsis as our phenotyping target diseases. We benchmark 9 kinds of electronic phenotyping methods spanning from rule-based to data-driven methods. Meanwhile, we evaluate the performance of the 5 bias mitigation strategies covering pre-processing, in-processing, and post-processing. Through the extensive experiments, we summarize several insightful findings from the bias identified in the phenotyping and key points of the bias mitigation strategies in phenotyping.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104671",
      "authors": "Ding Sirui et al.",
      "keywords": "Algorithm fairness; Bias mitigation; Electronic phenotyping; Fairness in healthcare",
      "mesh_terms": "Bias; Benchmarking; Data Mining; Electronic Health Records; Algorithms; Pneumonia; Sepsis; Cohort Studies; Machine Learning; Phenotype; Humans; Male; Female; Racial Groups; Sex Factors; Datasets as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38876452/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics; Pulmonology; Drug Discovery/Pharmacology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Preprocessing; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability; Post-hoc Correction",
      "clinical_setting": "Clinical Trial",
      "key_findings": "Meanwhile, we evaluate the performance of the 5 bias mitigation strategies covering pre-processing, in-processing, and post-processing. Through the extensive experiments, we summarize several insightful findings from the bias identified in the phenotyping and key points of the bias mitigation strategies in phenotyping.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38876453",
      "title": "Assessing fairness in machine learning models: A study of racial bias using matched counterparts in mortality prediction for patients with chronic diseases.",
      "abstract": "OBJECTIVE: Existing approaches to fairness evaluation often overlook systematic differences in the social determinants of health, like demographics and socioeconomics, among comparison groups, potentially leading to inaccurate or even contradictory conclusions. This study aims to evaluate racial disparities in predicting mortality among patients with chronic diseases using a fairness detection method that considers systematic differences. METHODS: We created five datasets from Mass General Brigham's electronic health records (EHR), each focusing on a different chronic condition: congestive heart failure (CHF), chronic kidney disease (CKD), chronic obstructive pulmonary disease (COPD), chronic liver disease (CLD), and dementia. For each dataset, we developed separate machine learning models to predict 1-year mortality and examined racial disparities by comparing prediction performances between Black and White individuals. We compared racial fairness evaluation between the overall Black and White individuals versus their counterparts who were Black and matched White individuals identified by propensity score matching, where the systematic differences were mitigated. RESULTS: We identified significant differences between Black and White individuals in age, gender, marital status, education level, smoking status, health insurance type, body mass index, and Charlson comorbidity index (p-value\u00a0<\u00a00.001). When examining matched Black and White subpopulations identified through propensity score matching, significant differences between particular covariates existed. We observed weaker significance levels in the CHF cohort for insurance type (p\u00a0=\u00a00.043), in the CKD cohort for insurance type (p\u00a0=\u00a00.005) and education level (p\u00a0=\u00a00.016), and in the dementia cohort for body mass index (p\u00a0=\u00a00.041); with no significant differences for other covariates. When examining mortality prediction models across the five study cohorts, we conducted a comparison of fairness evaluations before and after mitigating systematic differences. We revealed significant differences in the CHF cohort with p-values of 0.021 and 0.001 in terms of F1 measure and Sensitivity for the AdaBoost model, and p-values of 0.014 and 0.003 in terms of F1 measure and Sensitivity for the MLP model, respectively. DISCUSSION AND CONCLUSION: This study contributes to research on fairness assessment by focusing on the examination of systematic disparities and underscores the potential for revealing racial bias in machine learning models used in clinical settings.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104677",
      "authors": "Wang Yifei et al.",
      "keywords": "Chronic Disease; Electronic Health Records; Fairness Analysis; Machine Learning; Mortality Prediction; Racism",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Black or African American; Chronic Disease; Electronic Health Records; Heart Failure; Machine Learning; Pulmonary Disease, Chronic Obstructive; Racism; White; Health Status Disparities",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38876453/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Neural Network; Clinical Prediction Model",
      "health_domain": "Cardiology; ICU/Critical Care; EHR/Health Informatics; Nephrology; Pulmonology; Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Subgroup Analysis",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: This study contributes to research on fairness assessment by focusing on the examination of systematic disparities and underscores the potential for revealing racial bias in machine learning models used in clinical settings.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11272432"
    },
    {
      "pmid": "38898411",
      "title": "Predictive models of Alzheimer's disease dementia risk in older adults with mild cognitive impairment: a systematic review and critical appraisal.",
      "abstract": "BACKGROUND: Mild cognitive impairment has received widespread attention as a high-risk population for Alzheimer's disease, and many studies have developed or validated predictive models to assess it. However, the performance of the model development remains unknown. OBJECTIVE: The objective of this review was to provide an overview of prediction models for the risk of Alzheimer's disease dementia in older adults with mild cognitive impairment. METHOD: PubMed, EMBASE, Web of Science, and MEDLINE were systematically searched up to October 19, 2023. We included cohort studies in which risk prediction models for Alzheimer's disease dementia in older adults with mild cognitive impairment were developed or validated. The Predictive Model Risk of Bias Assessment Tool (PROBAST) was employed to assess model bias and applicability. Random-effects models combined model AUCs and calculated (approximate) 95% prediction intervals for estimations. Heterogeneity across studies was evaluated using the I2 statistic, and subgroup analyses were conducted to investigate sources of heterogeneity. Additionally, funnel plot analysis was utilized to identify publication bias. RESULTS: The analysis included 16 studies involving 9290 participants. Frequency analysis of predictors showed that 14 appeared at least twice and more, with age, functional activities questionnaire, and Mini-mental State Examination scores of cognitive functioning being the most common predictors. From the studies, only two models were externally validated. Eleven studies ultimately used machine learning, and four used traditional modelling methods. However, we found that in many of the studies, there were problems with insufficient sample sizes, missing important methodological information, lack of model presentation, and all of the models were rated as having a high or unclear risk of bias. The average AUC of the 15 best-developed predictive models was 0.87 (95% CI: 0.83, 0.90). DISCUSSION: Most published predictive modelling studies are deficient in rigour, resulting in a high risk of bias. Upcoming research should concentrate on enhancing methodological rigour and conducting external validation of models predicting Alzheimer's disease dementia. We also emphasize the importance of following the scientific method and transparent reporting to improve the accuracy, generalizability and reproducibility of study results. REGISTRATION: This systematic review was registered in PROSPERO (Registration ID: CRD42023468780).",
      "journal": "BMC geriatrics",
      "year": "2024",
      "doi": "10.1186/s12877-024-05044-8",
      "authors": "Wang Xiaotong et al.",
      "keywords": "Alzheimer\u2019s disease; Dementia; Elderly; Mild cognitive impairment; Predictive model; Systematic review",
      "mesh_terms": "Humans; Cognitive Dysfunction; Alzheimer Disease; Aged; Risk Assessment",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38898411/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Neurology",
      "bias_axes": "Gender/Sex; Age; Disability",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "RESULTS: The analysis included 16 studies involving 9290 participants. Frequency analysis of predictors showed that 14 appeared at least twice and more, with age, functional activities questionnaire, and Mini-mental State Examination scores of cognitive functioning being the most common predictors. From the studies, only two models were externally validated.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11188292"
    },
    {
      "pmid": "38914859",
      "title": "Deep learning-based prediction of one-year mortality in Finland is an accurate but unfair aging marker.",
      "abstract": "Short-term mortality risk, which is indicative of individual frailty, serves as a marker for aging. Previous age clocks focused on predicting either chronological age or longer-term mortality. Aging clocks predicting short-term mortality are lacking and their algorithmic fairness remains unexamined. We developed a deep learning model to predict 1-year mortality using nationwide longitudinal data from the Finnish population (FinRegistry; n\u2009=\u20095.4 million), incorporating more than 8,000 features spanning up to 50 years. We achieved an area under the curve (AUC) of 0.944, outperforming a baseline model that included only age and sex (AUC\u2009=\u20090.897). The model generalized well to different causes of death (AUC\u2009>\u20090.800 for 45 of 50 causes), including coronavirus disease 2019, which was absent in the training data. Performance varied among demographics, with young females exhibiting the best and older males the worst results. Extensive prediction fairness analyses highlighted disparities among disadvantaged groups, posing challenges to equitable integration into public health interventions. Our model accurately identified short-term mortality risk, potentially serving as a population-wide aging marker.",
      "journal": "Nature aging",
      "year": "2024",
      "doi": "10.1038/s43587-024-00657-5",
      "authors": "Vabalas Andrius et al.",
      "keywords": "",
      "mesh_terms": "Humans; Deep Learning; Finland; Male; Female; Middle Aged; Aged; Mortality; Aging; Adult; Aged, 80 and over; COVID-19; Young Adult; Frailty; Adolescent",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38914859/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Pulmonology; Public Health",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Extensive prediction fairness analyses highlighted disparities among disadvantaged groups, posing challenges to equitable integration into public health interventions. Our model accurately identified short-term mortality risk, potentially serving as a population-wide aging marker.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11257968"
    },
    {
      "pmid": "38918281",
      "title": "Background removal for debiasing computer-aided cytological diagnosis.",
      "abstract": "To address the background-bias problem in computer-aided cytology caused by microscopic slide deterioration, this article proposes a deep learning approach for cell segmentation and background removal without requiring cell annotation. A U-Net-based model was trained to separate cells from the background in an unsupervised manner by leveraging the redundancy of the background and the sparsity of cells in liquid-based cytology (LBC) images. The experimental results demonstrate that the U-Net-based model trained on a small set of cytology images can exclude background features and accurately segment cells. This capability is beneficial for debiasing in the detection and classification of the cells of interest in oral LBC. Slide deterioration can significantly affect deep learning-based cell classification. Our proposed method effectively removes background features at no cost of cell annotation, thereby enabling accurate cytological diagnosis through the deep learning of microscopic slide images.",
      "journal": "International journal of computer assisted radiology and surgery",
      "year": "2024",
      "doi": "10.1007/s11548-024-03169-0",
      "authors": "Takeda Keita et al.",
      "keywords": "Data cleaning; Deep learning; Oral cytology; Robust principal component analysis; U-Net",
      "mesh_terms": "Humans; Deep Learning; Diagnosis, Computer-Assisted; Cytodiagnosis; Microscopy; Image Interpretation, Computer-Assisted",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38918281/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Clustering",
      "health_domain": "Pathology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Slide deterioration can significantly affect deep learning-based cell classification. Our proposed method effectively removes background features at no cost of cell annotation, thereby enabling accurate cytological diagnosis through the deep learning of microscopic slide images.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11541310"
    },
    {
      "pmid": "38925281",
      "title": "Assessing racial bias in healthcare predictive models: Practical lessons from an empirical evaluation of 30-day hospital readmission models.",
      "abstract": "OBJECTIVE: Despite increased availability of methodologies to identify algorithmic bias, the operationalization of bias evaluation for healthcare predictive models is still limited. Therefore, this study proposes a process for bias evaluation through an empirical assessment of common hospital readmission models. The process includes selecting bias measures, interpretation, determining disparity impact and potential mitigations. METHODS: This retrospective analysis evaluated racial bias of four common models predicting 30-day unplanned readmission (i.e., LACE Index, HOSPITAL Score, and the CMS readmission measure applied as is and retrained). The models were assessed using 2.4 million adult inpatient discharges in Maryland from 2016 to 2019. Fairness metrics that are model-agnostic, easy to compute, and interpretable were implemented and apprised to select the most appropriate bias measures. The impact of changing model's risk thresholds on these measures was further assessed to guide the selection of optimal thresholds to control and mitigate bias. RESULTS: Four bias measures were selected for the predictive task: zero-one-loss difference, false negative rate (FNR) parity, false positive rate (FPR) parity, and generalized entropy index. Based on these measures, the HOSPITAL score and the retrained CMS measure demonstrated the lowest racial bias. White patients showed a higher FNR while Black patients resulted in a higher FPR and zero-one-loss. As the models' risk threshold changed, trade-offs between models' fairness and overall performance were observed, and the assessment showed all models' default thresholds were reasonable for balancing accuracy and bias. CONCLUSIONS: This study proposes an Applied Framework to Assess Fairness of Predictive Models (AFAFPM) and demonstrates the process using 30-day hospital readmission model as the example. It suggests the feasibility of applying algorithmic bias assessment to determine optimized risk thresholds so that predictive models can be used more equitably and accurately. It is evident that a combination of qualitative and quantitative methods and a multidisciplinary team are necessary to identify, understand and respond to algorithm bias in real-world healthcare settings. Users should also apply multiple bias measures to ensure a more comprehensive, tailored, and balanced view. The results of bias measures, however, must be interpreted with caution and consider the larger operational, clinical, and policy context.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104683",
      "authors": "Wang H Echo et al.",
      "keywords": "Algorithmic Bias; Algorithmic Fairness; Health Disparity; Hospital Readmission; Population Health Management; Predictive Models",
      "mesh_terms": "Humans; Patient Readmission; Racism; Retrospective Studies; Male; Female; Middle Aged; Adult; Aged; Maryland; Algorithms; Healthcare Disparities",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38925281/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment; Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "CONCLUSIONS: This study proposes an Applied Framework to Assess Fairness of Predictive Models (AFAFPM) and demonstrates the process using 30-day hospital readmission model as the example. It suggests the feasibility of applying algorithmic bias assessment to determine optimized risk thresholds so that predictive models can be used more equitably and accurately. It is evident that a combination of qualitative and quantitative methods and a multidisciplinary team are necessary to identify, underst...",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38929638",
      "title": "The Sociodemographic Biases in Machine Learning Algorithms: A Biomedical Informatics Perspective.",
      "abstract": "Artificial intelligence models represented in machine learning algorithms are promising tools for risk assessment used to guide clinical and other health care decisions. Machine learning algorithms, however, may house biases that propagate stereotypes, inequities, and discrimination that contribute to socioeconomic health care disparities. The biases include those related to some sociodemographic characteristics such as race, ethnicity, gender, age, insurance, and socioeconomic status from the use of erroneous electronic health record data. Additionally, there is concern that training data and algorithmic biases in large language models pose potential drawbacks. These biases affect the lives and livelihoods of a significant percentage of the population in the United States and globally. The social and economic consequences of the associated backlash cannot be underestimated. Here, we outline some of the sociodemographic, training data, and algorithmic biases that undermine sound health care risk assessment and medical decision-making that should be addressed in the health care system. We present a perspective and overview of these biases by gender, race, ethnicity, age, historically marginalized communities, algorithmic bias, biased evaluations, implicit bias, selection/sampling bias, socioeconomic status biases, biased data distributions, cultural biases and insurance status bias, conformation bias, information bias and anchoring biases and make recommendations to improve large language model training data, including de-biasing techniques such as counterfactual role-reversed sentences during knowledge distillation, fine-tuning, prefix attachment at training time, the use of toxicity classifiers, retrieval augmented generation and algorithmic modification to mitigate the biases moving forward.",
      "journal": "Life (Basel, Switzerland)",
      "year": "2024",
      "doi": "10.3390/life14060652",
      "authors": "Franklin Gillian et al.",
      "keywords": "algorithms; artificial intelligence; bias; biomedical informatics; electronic health records; health care; machine learning; models; sociodemographic",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38929638/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Language; Insurance Status",
      "lifecycle_stage": "Data Collection; Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Counterfactual Fairness; Transfer Learning",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Here, we outline some of the sociodemographic, training data, and algorithmic biases that undermine sound health care risk assessment and medical decision-making that should be addressed in the health care system. We present a perspective and overview of these biases by gender, race, ethnicity, age, historically marginalized communities, algorithmic bias, biased evaluations, implicit bias, selection/sampling bias, socioeconomic status biases, biased data distributions, cultural biases and insura...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11204917"
    },
    {
      "pmid": "38942737",
      "title": "Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging.",
      "abstract": "OBJECTIVE: Artificial intelligence (AI) models trained using medical images for clinical tasks often exhibit bias in the form of subgroup performance disparities. However, since not all sources of bias in real-world medical imaging data are easily identifiable, it is challenging to comprehensively assess their impacts. In this article, we introduce an analysis framework for systematically and objectively investigating the impact of biases in medical images on AI models. MATERIALS AND METHODS: Our framework utilizes synthetic neuroimages with known disease effects and sources of bias. We evaluated the impact of bias effects and the efficacy of 3 bias mitigation strategies in counterfactual data scenarios on a convolutional neural network (CNN) classifier. RESULTS: The analysis revealed that training a CNN model on the datasets containing bias effects resulted in expected subgroup performance disparities. Moreover, reweighing was the most successful bias mitigation strategy for this setup. Finally, we demonstrated that explainable AI methods can aid in investigating the manifestation of bias in the model using this framework. DISCUSSION: The value of this framework is showcased in our findings on the impact of bias scenarios and efficacy of bias mitigation in a deep learning model pipeline. This systematic analysis can be easily expanded to conduct further controlled in silico trials in other investigations of bias in medical imaging AI. CONCLUSION: Our novel methodology for objectively studying bias in medical imaging AI can help support the development of clinical decision-support tools that are robust and responsible.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2024",
      "doi": "10.1093/jamia/ocae165",
      "authors": "Stanley Emma A M et al.",
      "keywords": "algorithmic bias; artificial intelligence; bias mitigation; synthetic data",
      "mesh_terms": "Humans; Artificial Intelligence; Bias; Neural Networks, Computer; Diagnostic Imaging; Neuroimaging",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38942737/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Neural Network; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Counterfactual Fairness; Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Our novel methodology for objectively studying bias in medical imaging AI can help support the development of clinical decision-support tools that are robust and responsible.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11491635"
    },
    {
      "pmid": "38947177",
      "title": "Unbiasing Fairness Evaluation of Radiology AI Model.",
      "abstract": "Fairness of artificial intelligence and machine learning models, often caused by imbalanced datasets, has long been a concern. While many efforts aim to minimize model bias, this study suggests that traditional fairness evaluation methods may be biased, highlighting the need for a proper evaluation scheme with multiple evaluation metrics due to varying results under different criteria. Moreover, the limited data size of minority groups introduces significant data uncertainty, which can undermine the judgement of fairness. This paper introduces an innovative evaluation approach that estimates data uncertainty in minority groups through bootstrapping from majority groups for a more objective statistical assessment. Extensive experiments reveal that traditional evaluation methods might have drawn inaccurate conclusions about model fairness. The proposed method delivers an unbiased fairness assessment by adeptly addressing the inherent complications of model evaluation on imbalanced datasets. The results show that such comprehensive evaluation can provide more confidence when adopting those models.",
      "journal": "Meta-radiology",
      "year": "2024",
      "doi": "10.1016/j.metrad.2024.100084",
      "authors": "Liang Yuxuan et al.",
      "keywords": "Data uncertainty; Deep learning; Evaluation metrics; Fairness; Medical imaging",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38947177/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Diverse/Representative Data",
      "clinical_setting": "Not specified",
      "key_findings": "The proposed method delivers an unbiased fairness assessment by adeptly addressing the inherent complications of model evaluation on imbalanced datasets. The results show that such comprehensive evaluation can provide more confidence when adopting those models.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11210324"
    },
    {
      "pmid": "38962581",
      "title": "Challenges in Reducing Bias Using Post-Processing Fairness for Breast Cancer Stage Classification with Deep Learning.",
      "abstract": "Breast cancer is the most common cancer affecting women globally. Despite the significant impact of deep learning models on breast cancer diagnosis and treatment, achieving fairness or equitable outcomes across diverse populations remains a challenge when some demographic groups are underrepresented in the training data. We quantified the bias of models trained to predict breast cancer stage from a dataset consisting of 1000 biopsies from 842 patients provided by AIM-Ahead (Artificial Intelligence/Machine Learning Consortium to Advance Health Equity and Researcher Diversity). Notably, the majority of data (over 70%) were from White patients. We found that prior to post-processing adjustments, all deep learning models we trained consistently performed better for White patients than for non-White patients. After model calibration, we observed mixed results, with only some models demonstrating improved performance. This work provides a case study of bias in breast cancer medical imaging models and highlights the challenges in using post-processing to attempt to achieve fairness.",
      "journal": "Algorithms",
      "year": "2024",
      "doi": "10.3390/a17040141",
      "authors": "Soltan Armin et al.",
      "keywords": "algorithmic fairness; breast cancer; deep learning; equalized odds; equalized opportunity; post-processing method",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38962581/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Oncology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Post-hoc Correction",
      "clinical_setting": "Public Health/Population",
      "key_findings": "After model calibration, we observed mixed results, with only some models demonstrating improved performance. This work provides a case study of bias in breast cancer medical imaging models and highlights the challenges in using post-processing to attempt to achieve fairness.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11221567"
    },
    {
      "pmid": "38985468",
      "title": "Fairness in Predicting Cancer Mortality Across Racial Subgroups.",
      "abstract": "IMPORTANCE: Machine learning has potential to transform cancer care by helping clinicians prioritize patients for serious illness conversations. However, models need to be evaluated for unequal performance across racial groups (ie, racial bias) so that existing racial disparities are not exacerbated. OBJECTIVE: To evaluate whether racial bias exists in a predictive machine learning model that identifies 180-day cancer mortality risk among patients with solid malignant tumors. DESIGN, SETTING, AND PARTICIPANTS: In this cohort study, a machine learning model to predict cancer mortality for patients aged 21 years or older diagnosed with cancer between January 2016 and December 2021 was developed with a random forest algorithm using retrospective data from the Mount Sinai Health System cancer registry, Social Security Death Index, and electronic health records up to the date when databases were accessed for cohort extraction (February 2022). EXPOSURE: Race category. MAIN OUTCOMES AND MEASURES: The primary outcomes were model discriminatory performance (area under the receiver operating characteristic curve [AUROC], F1 score) among each race category (Asian, Black, Native American, White, and other or unknown) and fairness metrics (equal opportunity, equalized odds, and disparate impact) among each pairwise comparison of race categories. True-positive rate ratios represented equal opportunity; both true-positive and false-positive rate ratios, equalized odds; and the percentage of predictive positive rate ratios, disparate impact. All metrics were estimated as a proportion or ratio, with variability captured through 95% CIs. The prespecified criterion for the model's clinical use was a threshold of at least 80% for fairness metrics across different racial groups to ensure the model's prediction would not be biased against any specific race. RESULTS: The test validation dataset included 43\u202f274 patients with balanced demographics. Mean (SD) age was 64.09 (14.26) years, with 49.6% older than 65 years. A total of 53.3% were female; 9.5%, Asian; 18.9%, Black; 0.1%, Native American; 52.2%, White; and 19.2%, other or unknown race; 0.1% had missing race data. A total of 88.9% of patients were alive, and 11.1% were dead. The AUROCs, F1 scores, and fairness metrics maintained reasonable concordance among the racial subgroups: the AUROCs ranged from 0.75 (95% CI, 0.72-0.78) for Asian patients and 0.75 (95% CI, 0.73-0.77) for Black patients to 0.77 (95% CI, 0.75-0.79) for patients with other or unknown race; F1 scores, from 0.32 (95% CI, 0.32-0.33) for White patients to 0.40 (95% CI, 0.39-0.42) for Black patients; equal opportunity ratios, from 0.96 (95% CI, 0.95-0.98) for Black patients compared with White patients to 1.02 (95% CI, 1.00-1.04) for Black patients compared with patients with other or unknown race; equalized odds ratios, from 0.87 (95% CI, 0.85-0.92) for Black patients compared with White patients to 1.16 (1.10-1.21) for Black patients compared with patients with other or unknown race; and disparate impact ratios, from 0.86 (95% CI, 0.82-0.89) for Black patients compared with White patients to 1.17 (95% CI, 1.12-1.22) for Black patients compared with patients with other or unknown race. CONCLUSIONS AND RELEVANCE: In this cohort study, the lack of significant variation in performance or fairness metrics indicated an absence of racial bias, suggesting that the model fairly identified cancer mortality risk across racial groups. It remains essential to consistently review the model's application in clinical settings to ensure equitable patient care.",
      "journal": "JAMA network open",
      "year": "2024",
      "doi": "10.1001/jamanetworkopen.2024.21290",
      "authors": "Ganta Teja et al.",
      "keywords": "",
      "mesh_terms": "Humans; Neoplasms; Female; Male; Middle Aged; Aged; Machine Learning; Retrospective Studies; Adult; Racial Groups; Cohort Studies; Racism",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38985468/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Random Forest",
      "health_domain": "Oncology; EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Threshold Adjustment; Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: The test validation dataset included 43\u202f274 patients with balanced demographics. Mean (SD) age was 64.09 (14.26) years, with 49.6% older than 65 years. A total of 53.3% were female; 9.5%, Asian; 18.9%, Black; 0.1%, Native American; 52.2%, White; and 19.2%, other or unknown race; 0.1% had missing race data.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11238025"
    },
    {
      "pmid": "39009174",
      "title": "Evaluating gender bias in ML-based clinical risk prediction models: A study on multiple use cases at different hospitals.",
      "abstract": "BACKGROUND: An inherent difference exists between male and female bodies, the historical under-representation of females in clinical trials widened this gap in existing healthcare data. The fairness of clinical decision-support tools is at risk when developed based on biased data. This paper aims to quantitatively assess the gender bias in risk prediction models. We aim to generalize our findings by performing this investigation on multiple use cases at different hospitals. METHODS: First, we conduct a thorough analysis of the source data to find gender-based disparities. Secondly, we assess the model performance on different gender groups at different hospitals and on different use cases. Performance evaluation is quantified using the area under the receiver-operating characteristic curve (AUROC). Lastly, we investigate the clinical implications of these biases by analyzing the underdiagnosis and overdiagnosis rate, and the decision curve analysis (DCA). We also investigate the influence of model calibration on mitigating gender-related disparities in decision-making processes. RESULTS: Our data analysis reveals notable variations in incidence rates, AUROC, and over-diagnosis rates across different genders, hospitals and clinical use cases. However, it is also observed the underdiagnosis rate is consistently higher in the female population. In general, the female population exhibits lower incidence rates and the models perform worse when applied to this group. Furthermore, the decision curve analysis demonstrates there is no statistically significant difference between the model's clinical utility across gender groups within the interested range of thresholds. CONCLUSION: The presence of gender bias within risk prediction models varies across different clinical use cases and healthcare institutions. Although inherent difference is observed between male and female populations at the data source level, this variance does not affect the parity of clinical utility. In conclusion, the evaluations conducted in this study highlight the significance of continuous monitoring of gender-based disparities in various perspectives for clinical risk prediction models.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104692",
      "authors": "Cabanillas Silva Patricia et al.",
      "keywords": "Clinical risk prediction; Gender bias; Machine learning; Model evaluation; Model fairness; Prediction model",
      "mesh_terms": "Humans; Female; Male; Sexism; ROC Curve; Risk Assessment; Hospitals; Area Under Curve; Decision Support Systems, Clinical",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39009174/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Collection; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Threshold Adjustment",
      "clinical_setting": "Hospital/Inpatient; Public Health/Population; Clinical Trial",
      "key_findings": "CONCLUSION: The presence of gender bias within risk prediction models varies across different clinical use cases and healthcare institutions. Although inherent difference is observed between male and female populations at the data source level, this variance does not affect the parity of clinical utility. In conclusion, the evaluations conducted in this study highlight the significance of continuous monitoring of gender-based disparities in various perspectives for clinical risk prediction model...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39010875",
      "title": "Unmasking bias in artificial intelligence: a systematic review of bias detection and mitigation strategies in electronic health record-based models.",
      "abstract": "OBJECTIVES: Leveraging artificial intelligence (AI) in conjunction with electronic health records (EHRs) holds transformative potential to improve healthcare. However, addressing bias in AI, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to handle various biases in AI models developed using EHR data. MATERIALS AND METHODS: We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-analyses guidelines, analyzing articles from PubMed, Web of Science, and IEEE published between January 01, 2010 and December 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the AI model development, and analyzed metrics for bias assessment. RESULTS: Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Fifteen studies proposed strategies for mitigating biases, especially targeting implicit and selection biases. These strategies, evaluated through both performance and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling and reweighting. DISCUSSION: This review highlights evolving strategies to mitigate bias in EHR-based AI models, emphasizing the urgent need for both standardized and detailed reporting of the methodologies and systematic real-world testing and evaluation. Such measures are essential for gauging models' practical impact and fostering ethical AI that ensures fairness and equity in healthcare.",
      "journal": "ArXiv",
      "year": "2024",
      "doi": "",
      "authors": "Chen Feng et al.",
      "keywords": "artificial intelligence; bias; deep learning; electronic health record; scoping review",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39010875/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11247915"
    },
    {
      "pmid": "39046989",
      "title": "Evaluating and mitigating unfairness in multimodal remote mental health assessments.",
      "abstract": "Research on automated mental health assessment tools has been growing in recent years, often aiming to address the subjectivity and bias that existed in the current clinical practice of the psychiatric evaluation process. Despite the substantial health and economic ramifications, the potential unfairness of those automated tools was understudied and required more attention. In this work, we systematically evaluated the fairness level in a multimodal remote mental health dataset and an assessment system, where we compared the fairness level in race, gender, education level, and age. Demographic parity ratio (DPR) and equalized odds ratio (EOR) of classifiers using different modalities were compared, along with the F1 scores in different demographic groups. Post-training classifier threshold optimization was employed to mitigate the unfairness. No statistically significant unfairness was found in the composition of the dataset. Varying degrees of unfairness were identified among modalities, with no single modality consistently demonstrating better fairness across all demographic variables. Post-training mitigation effectively improved both DPR and EOR metrics at the expense of a decrease in F1 scores. Addressing and mitigating unfairness in these automated tools are essential steps in fostering trust among clinicians, gaining deeper insights into their use cases, and facilitating their appropriate utilization.",
      "journal": "PLOS digital health",
      "year": "2024",
      "doi": "10.1371/journal.pdig.0000413",
      "authors": "Jiang Zifan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39046989/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment; Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "Telehealth/Remote",
      "key_findings": "Post-training mitigation effectively improved both DPR and EOR metrics at the expense of a decrease in F1 scores. Addressing and mitigating unfairness in these automated tools are essential steps in fostering trust among clinicians, gaining deeper insights into their use cases, and facilitating their appropriate utilization.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11268595"
    },
    {
      "pmid": "39075127",
      "title": "Developing a fair and interpretable representation of the clock drawing test for mitigating low education and racial bias.",
      "abstract": "The clock drawing test (CDT) is a neuropsychological assessment tool to screen an individual's cognitive ability. In this study, we developed a Fair and Interpretable Representation of Clock drawing test (FaIRClocks) to evaluate and mitigate classification bias against people with less than 8\u00a0years of education, while screening their cognitive function using an array of neuropsychological measures. In this study, we represented clock drawings by a priorly published 10-dimensional deep learning feature set trained on publicly available data from the National Health and Aging Trends Study (NHATS). These embeddings were further fine-tuned with clocks from a preoperative cognitive screening program at the University of Florida to predict three cognitive scores: the Mini-Mental State Examination (MMSE) total score, an attention composite z-score (ATT-C), and a memory composite z-score (MEM-C). ATT-C and MEM-C scores were developed by averaging z-scores based on normative references. The cognitive screening classifiers were initially tested to see their relative performance in patients with low years of education (<\u2009=\u20098\u00a0years) versus patients with higher education (>\u20098\u00a0years) and race. Results indicated that the initial unweighted classifiers confounded lower education with cognitive compromise resulting in a 100% type I error rate for this group. Thereby, the samples were re-weighted using multiple fairness metrics to achieve sensitivity/specificity and positive/negative predictive value (PPV/NPV) balance across groups. In summary, we report the FaIRClocks model, with promise to help identify and mitigate bias against people with less than 8\u00a0years of education during preoperative cognitive screening.",
      "journal": "Scientific reports",
      "year": "2024",
      "doi": "10.1038/s41598-024-68481-w",
      "authors": "Zhang Jiaqing et al.",
      "keywords": "AI Fairness; Attention; Memory; Mini-mental state examination; Relevance factor variational autoencoder; Semi-supervised deep learning",
      "mesh_terms": "Humans; Male; Female; Aged; Educational Status; Racism; Neuropsychological Tests; Cognition; Cognitive Dysfunction; Aged, 80 and over; Mental Status and Dementia Tests; Middle Aged; Deep Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39075127/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Surgery; Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Representation Learning; Transfer Learning; Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Thereby, the samples were re-weighted using multiple fairness metrics to achieve sensitivity/specificity and positive/negative predictive value (PPV/NPV) balance across groups. In summary, we report the FaIRClocks model, with promise to help identify and mitigate bias against people with less than 8\u00a0years of education during preoperative cognitive screening.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11286895"
    },
    {
      "pmid": "39085344",
      "title": "A novel approach for assessing fairness in deployed machine learning algorithms.",
      "abstract": "Fairness in machine learning (ML) emerges as a critical concern as AI systems increasingly influence diverse aspects of society, from healthcare decisions to legal judgments. Many studies show evidence of unfair ML outcomes. However, the current body of literature lacks a statistically validated approach that can evaluate the fairness of a deployed ML algorithm against a dataset. A novel evaluation approach is introduced in this research based on k-fold cross-validation and statistical t-tests to assess the fairness of ML algorithms. This approach was exercised across five benchmark datasets using six classical ML algorithms. Considering four fair ML definitions guided by the current literature, our analysis showed that the same dataset generates a fair outcome for one ML algorithm but an unfair result for another. Such an observation reveals complex, context-dependent fairness issues in ML, complicated further by the varied operational mechanisms of the underlying ML models. Our proposed approach enables researchers to check whether deploying any ML algorithms against a protected attribute within datasets is fair. We also discuss the broader implications of the proposed approach, highlighting a notable variability in its fairness outcomes. Our discussion underscores the need for adaptable fairness definitions and the exploration of methods to enhance the fairness of ensemble approaches, aiming to advance fair ML practices and ensure equitable AI deployment across societal sectors.",
      "journal": "Scientific reports",
      "year": "2024",
      "doi": "10.1038/s41598-024-68651-w",
      "authors": "Uddin Shahadat et al.",
      "keywords": "Fair machine learning; Fairness; Machine learning",
      "mesh_terms": "Machine Learning; Algorithms; Humans",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39085344/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Ensemble Methods",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Ensemble Methods",
      "clinical_setting": "Not specified",
      "key_findings": "We also discuss the broader implications of the proposed approach, highlighting a notable variability in its fairness outcomes. Our discussion underscores the need for adaptable fairness definitions and the exploration of methods to enhance the fairness of ensemble approaches, aiming to advance fair ML practices and ensure equitable AI deployment across societal sectors.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11291763"
    },
    {
      "pmid": "39106773",
      "title": "Fairness gaps in Machine learning models for hospitalization and emergency department visit risk prediction in home healthcare patients with heart failure.",
      "abstract": "OBJECTIVES: This study aims to evaluate the fairness performance metrics of Machine Learning (ML) models to predict hospitalization and emergency department (ED) visits in heart failure patients receiving home healthcare. We analyze biases, assess performance disparities, and propose solutions to improve model performance in diverse subpopulations. METHODS: The study used a dataset of 12,189 episodes of home healthcare collected between 2015 and 2017, including structured (e.g., standard assessment tool) and unstructured data (i.e., clinical notes). ML risk prediction models, including Light Gradient-boosting model (LightGBM) and AutoGluon, were developed using demographic information, vital signs, comorbidities, service utilization data, and the area deprivation index (ADI) associated with the patient's home address. Fairness metrics, such as Equal Opportunity, Predictive Equality, Predictive Parity, and Statistical Parity, were calculated to evaluate model performance across subpopulations. RESULTS: Our study revealed significant disparities in model performance across diverse demographic subgroups. For example, the Hispanic, Male, High-ADI subgroup excelled in terms of Equal Opportunity with a metric value of 0.825, which was 28% higher than the lowest-performing Other, Female, Low-ADI subgroup, which scored 0.644. In Predictive Parity, the gap between the highest and lowest-performing groups was 29%, and in Statistical Parity, the gap reached 69%. In Predictive Equality, the difference was 45%. DISCUSSION AND CONCLUSION: The findings highlight substantial differences in fairness metrics across diverse patient subpopulations in ML risk prediction models for heart failure patients receiving home healthcare services. Ongoing monitoring and improvement of fairness metrics are essential to mitigate biases.",
      "journal": "International journal of medical informatics",
      "year": "2024",
      "doi": "10.1016/j.ijmedinf.2024.105534",
      "authors": "Davoudi Anahita et al.",
      "keywords": "Bias; Healthcare Disparities; Heart Failure; Home Care Services; Machine Learning; Socioeconomic Factors",
      "mesh_terms": "Humans; Heart Failure; Machine Learning; Emergency Service, Hospital; Male; Female; Hospitalization; Home Care Services; Aged; Risk Assessment; Middle Aged; Aged, 80 and over; Emergency Room Visits",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39106773/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "XGBoost/Gradient Boosting; Clinical Prediction Model",
      "health_domain": "Cardiology; Emergency Medicine",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Subgroup Analysis; Fairness Metrics Evaluation",
      "clinical_setting": "Hospital/Inpatient; Emergency Department; Public Health/Population",
      "key_findings": "CONCLUSION: The findings highlight substantial differences in fairness metrics across diverse patient subpopulations in ML risk prediction models for heart failure patients receiving home healthcare services. Ongoing monitoring and improvement of fairness metrics are essential to mitigate biases.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39116187",
      "title": "Targeting Machine Learning and Artificial Intelligence Algorithms in Health Care to Reduce Bias and Improve Population Health.",
      "abstract": "Policy Points Artificial intelligence (AI) is disruptively innovating health care and surpassing our ability to define its boundaries and roles in health care and regulate its application in legal and ethical ways. Significant progress has been made in governance in the United States and the European Union. It is incumbent on developers, end users, the public, providers, health care systems, and policymakers to collaboratively ensure that we adopt a national AI health strategy that realizes the Quintuple Aim; minimizes race-based medicine; prioritizes transparency, equity, and algorithmic vigilance; and integrates the patient and community voices throughout all aspects of AI development and deployment.",
      "journal": "The Milbank quarterly",
      "year": "2024",
      "doi": "10.1111/1468-0009.12712",
      "authors": "Hurd Thelma C et al.",
      "keywords": "algorithmic bias; artificial intelligence; ethics; machine learning; minority health",
      "mesh_terms": "Humans; Artificial Intelligence; Machine Learning; Population Health; United States; Delivery of Health Care; Algorithms; Bias",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39116187/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Significant progress has been made in governance in the United States and the European Union. It is incumbent on developers, end users, the public, providers, health care systems, and policymakers to collaboratively ensure that we adopt a national AI health strategy that realizes the Quintuple Aim; minimizes race-based medicine; prioritizes transparency, equity, and algorithmic vigilance; and integrates the patient and community voices throughout all aspects of AI development and deployment.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11576591"
    },
    {
      "pmid": "39119589",
      "title": "Deconstructing demographic bias in speech-based machine learning models for digital health.",
      "abstract": "INTRODUCTION: Machine learning (ML) algorithms have been heralded as promising solutions to the realization of assistive systems in digital healthcare, due to their ability to detect fine-grain patterns that are not easily perceived by humans. Yet, ML algorithms have also been critiqued for treating individuals differently based on their demography, thus propagating existing disparities. This paper explores gender and race bias in speech-based ML algorithms that detect behavioral and mental health outcomes. METHODS: This paper examines potential sources of bias in the data used to train the ML, encompassing acoustic features extracted from speech signals and associated labels, as well as in the ML decisions. The paper further examines approaches to reduce existing bias via using the features that are the least informative of one's demographic information as the ML input, and transforming the feature space in an adversarial manner to diminish the evidence of the demographic information while retaining information about the focal behavioral and mental health state. RESULTS: Results are presented in two domains, the first pertaining to gender and race bias when estimating levels of anxiety, and the second pertaining to gender bias in detecting depression. Findings indicate the presence of statistically significant differences in both acoustic features and labels among demographic groups, as well as differential ML performance among groups. The statistically significant differences present in the label space are partially preserved in the ML decisions. Although variations in ML performance across demographic groups were noted, results are mixed regarding the models' ability to accurately estimate healthcare outcomes for the sensitive groups. DISCUSSION: These findings underscore the necessity for careful and thoughtful design in developing ML models that are capable of maintaining crucial aspects of the data and perform effectively across all populations in digital healthcare applications.",
      "journal": "Frontiers in digital health",
      "year": "2024",
      "doi": "10.3389/fdgth.2024.1351637",
      "authors": "Yang Michael et al.",
      "keywords": "anxiety; demographic bias; depression; fairness; machine learning; speech",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39119589/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "RESULTS: Results are presented in two domains, the first pertaining to gender and race bias when estimating levels of anxiety, and the second pertaining to gender bias in detecting depression. Findings indicate the presence of statistically significant differences in both acoustic features and labels among demographic groups, as well as differential ML performance among groups. The statistically significant differences present in the label space are partially preserved in the ML decisions.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11306200"
    },
    {
      "pmid": "39163597",
      "title": "Mitigating Sociodemographic Bias in Opioid Use Disorder Prediction: Fairness-Aware Machine Learning Framework.",
      "abstract": "BACKGROUND: Opioid use disorder (OUD) is a critical public health crisis in the United States, affecting >5.5 million Americans in 2021. Machine learning has been used to predict patient risk of incident OUD. However, little is known about the fairness and bias of these predictive models. OBJECTIVE: The aims of this study are two-fold: (1) to develop a machine learning bias mitigation algorithm for sociodemographic features and (2) to develop a fairness-aware weighted majority voting (WMV) classifier for OUD prediction. METHODS: We used the 2020 National Survey on Drug and Health data to develop a neural network (NN) model using stochastic gradient descent (SGD; NN-SGD) and an NN model using Adam (NN-Adam) optimizers and evaluated sociodemographic bias by comparing the area under the curve values. A bias mitigation algorithm, based on equality of odds, was implemented to minimize disparities in specificity and recall. Finally, a WMV classifier was developed for fairness-aware prediction of OUD. To further analyze bias detection and mitigation, we did a 1-N matching of OUD to non-OUD cases, controlling for socioeconomic variables, and evaluated the performance of the proposed bias mitigation algorithm and WMV classifier. RESULTS: Our bias mitigation algorithm substantially reduced bias with NN-SGD, by 21.66% for sex, 1.48% for race, and 21.04% for income, and with NN-Adam by 16.96% for sex, 8.87% for marital status, 8.45% for working condition, and 41.62% for race. The fairness-aware WMV classifier achieved a recall of 85.37% and 92.68% and an accuracy of 58.85% and 90.21% using NN-SGD and NN-Adam, respectively. The results after matching also indicated remarkable bias reduction with NN-SGD and NN-Adam, respectively, as follows: sex (0.14% vs 0.97%), marital status (12.95% vs 10.33%), working condition (14.79% vs 15.33%), race (60.13% vs 41.71%), and income (0.35% vs 2.21%). Moreover, the fairness-aware WMV classifier achieved high performance with a recall of 100% and 85.37% and an accuracy of 73.20% and 89.38% using NN-SGD and NN-Adam, respectively. CONCLUSIONS: The application of the proposed bias mitigation algorithm shows promise in reducing sociodemographic bias, with the WMV classifier confirming bias reduction and high performance in OUD prediction.",
      "journal": "JMIR AI",
      "year": "2024",
      "doi": "10.2196/55820",
      "authors": "Yaseliani Mohammad et al.",
      "keywords": "bias mitigation; fairness and bias; machine learning; majority voting; opioid use disorder",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39163597/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Neural Network; Clinical Prediction Model",
      "health_domain": "Mental Health/Psychiatry; Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: The application of the proposed bias mitigation algorithm shows promise in reducing sociodemographic bias, with the WMV classifier confirming bias reduction and high performance in OUD prediction.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11372321"
    },
    {
      "pmid": "39198519",
      "title": "Acquisition parameters influence AI recognition of race in chest x-rays and mitigating these factors reduces underdiagnosis bias.",
      "abstract": "A core motivation for the use of artificial intelligence (AI) in medicine is to reduce existing healthcare disparities. Yet, recent studies have demonstrated two distinct findings: (1) AI models can show performance biases in underserved populations, and (2) these same models can be directly trained to recognize patient demographics, such as predicting self-reported race from medical images alone. Here, we investigate how these findings may be related, with an end goal of reducing a previously identified underdiagnosis bias. Using two popular chest x-ray datasets, we first demonstrate that technical parameters related to image acquisition and processing influence AI models trained to predict patient race, where these results partly reflect underlying biases in the original clinical datasets. We then find that mitigating the observed differences through a demographics-independent calibration strategy reduces the previously identified bias. While many factors likely contribute to AI bias and demographics prediction, these results highlight the importance of carefully considering data acquisition and processing parameters in AI development and healthcare equity more broadly.",
      "journal": "Nature communications",
      "year": "2024",
      "doi": "10.1038/s41467-024-52003-3",
      "authors": "Lotter William",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Bias; Radiography, Thoracic; Male; Female; Racial Groups; Healthcare Disparities; Middle Aged; Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39198519/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration",
      "clinical_setting": "Public Health/Population; Safety-Net/Underserved",
      "key_findings": "findings: (1) AI models can show performance biases in underserved populations, and (2) these same models can be directly trained to recognize patient demographics, such as predicting self-reported race from medical images alone. Here, we investigate how these findings may be related, with an end goal of reducing a previously identified underdiagnosis bias. Using two popular chest x-ray datasets, we first demonstrate that technical parameters related to image acquisition and processing influence...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11358468"
    },
    {
      "pmid": "39232870",
      "title": "Predicting serious postoperative complications and evaluating racial fairness in machine learning algorithms for metabolic and bariatric surgery.",
      "abstract": "BACKGROUND: Predicting the risk of complications is critical in metabolic and bariatric surgery (MBS). OBJECTIVES: To develop machine learning (ML) models to predict serious postoperative complications of MBS and evaluate racial fairness of the models. SETTING: Metabolic and Bariatric Surgery Accreditation and Quality Improvement Program (MBSAQIP) national database, United States. METHODS: We developed logistic regression, random forest (RF), gradient-boosted tree (GBT), and XGBoost model using the MBSAQIP Participant Use Data File from 2016 to 2020. To address the class imbalance, we randomly undersampled the complication-negative class to match the complication-positive class. Model performance was evaluated using the area under the receiver operating characteristic curve (AUROC), precision, recall, and F1 score. Fairness across White and non-White patient groups was assessed using equal opportunity difference and disparate impact metrics. RESULTS: A total of 40,858 patients were included after undersampling the complication-negative class. The XGBoost model was the best-performing model in terms of AUROC; however, the difference was not statistically significant. While the F1 score and precision did not vary significantly across models, the RF exhibited better recall compared to the logistic regression. Surgery type was the most important feature to predict complications, followed by operative time. The logistic regression model had the best fairness metrics for race. CONCLUSIONS: The XGBoost model achieved the highest AUROC, albeit without a statistically significant difference. The RF may be useful when recall is the primary concern. Undersampling of the privileged group may improve the fairness of boosted tree models.",
      "journal": "Surgery for obesity and related diseases : official journal of the American Society for Bariatric Surgery",
      "year": "2024",
      "doi": "10.1016/j.soard.2024.08.008",
      "authors": "Kang Dong-Won et al.",
      "keywords": "Complication; Machine learning; Metabolic and bariatric surgery; Roux-en-Y gastric bypass; Sleeve gastrectomy",
      "mesh_terms": "Humans; Bariatric Surgery; Machine Learning; Postoperative Complications; Female; Male; Middle Aged; Adult; United States; Algorithms; Obesity, Morbid; Risk Assessment",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39232870/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Random Forest; Logistic Regression; XGBoost/Gradient Boosting; Regression",
      "health_domain": "Surgery",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: The XGBoost model achieved the highest AUROC, albeit without a statistically significant difference. The RF may be useful when recall is the primary concern. Undersampling of the privileged group may improve the fairness of boosted tree models.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39242810",
      "title": "MyThisYourThat for interpretable identification of systematic bias in federated learning for biomedical images.",
      "abstract": "Distributed collaborative learning is a promising approach for building predictive models for privacy-sensitive biomedical images. Here, several data owners (clients) train a joint model without sharing their original data. However, concealed systematic biases can compromise model performance and fairness. This study presents MyThisYourThat (MyTH) approach, which adapts an interpretable prototypical part learning network to a distributed setting, enabling each client to visualize feature differences learned by others on their own image: comparing one client's 'This' with others' 'That'. Our setting demonstrates four clients collaboratively training two diagnostic classifiers on a benchmark X-ray dataset. Without data bias, the global model reaches 74.14% balanced accuracy for cardiomegaly and 74.08% for pleural effusion. We show that with systematic visual bias in one client, the performance of global models drops to near-random. We demonstrate how differences between local and global prototypes reveal biases and allow their visualization on each client's data without compromising privacy.",
      "journal": "NPJ digital medicine",
      "year": "2024",
      "doi": "10.1038/s41746-024-01226-1",
      "authors": "Naumova Klavdiia et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39242810/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Federated Learning; Clinical Prediction Model",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "We show that with systematic visual bias in one client, the performance of global models drops to near-random. We demonstrate how differences between local and global prototypes reveal biases and allow their visualization on each client's data without compromising privacy.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11379706"
    },
    {
      "pmid": "39309255",
      "title": "Beyond ideals: why the (medical) AI industry needs to motivate behavioural change in line with fairness and transparency values, and how it can do it.",
      "abstract": "Artificial intelligence (AI) is increasingly relied upon by clinicians for making diagnostic and treatment decisions, playing an important role in imaging, diagnosis, risk analysis, lifestyle monitoring, and health information management. While research has identified biases in healthcare AI systems and proposed technical solutions to address these, we argue that effective solutions require human engagement. Furthermore, there is a lack of research on how to motivate the adoption of these solutions and promote investment in designing AI systems that align with values such as transparency and fairness from the outset. Drawing on insights from psychological theories, we assert the need to understand the values that underlie decisions made by individuals involved in creating and deploying AI systems. We describe how this understanding can be leveraged to increase engagement with de-biasing and fairness-enhancing practices within the AI healthcare industry, ultimately leading to sustained behavioral change via autonomy-supportive communication strategies rooted in motivational and social psychology theories. In developing these pathways to engagement, we consider the norms and needs that govern the AI healthcare domain, and we evaluate incentives for maintaining the status quo against economic, legal, and social incentives for behavior change in line with transparency and fairness values.",
      "journal": "AI & society",
      "year": "2024",
      "doi": "10.1007/s00146-023-01684-3",
      "authors": "Liefgreen Alice et al.",
      "keywords": "Artificial intelligence; Behaviour change; Bias; Fairness; Healthcare; Medicine; Motivation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39309255/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We describe how this understanding can be leveraged to increase engagement with de-biasing and fairness-enhancing practices within the AI healthcare industry, ultimately leading to sustained behavioral change via autonomy-supportive communication strategies rooted in motivational and social psychology theories. In developing these pathways to engagement, we consider the norms and needs that govern the AI healthcare domain, and we evaluate incentives for maintaining the status quo against economi...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11415467"
    },
    {
      "pmid": "39313595",
      "title": "A toolbox for surfacing health equity harms and biases in large language models.",
      "abstract": "Large language models (LLMs) hold promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities. Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity. We present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and conduct a large-scale empirical case study with the Med-PaLM 2 LLM. Our contributions include a multifactorial framework for human assessment of LLM-generated answers for biases and EquityMedQA, a collection of seven datasets enriched for adversarial queries. Both our human assessment framework and our dataset design process are grounded in an iterative participatory approach and review of Med-PaLM 2 answers. Through our empirical study, we find that our approach surfaces biases that may be missed by narrower evaluation approaches. Our experience underscores the importance of using diverse assessment methodologies and involving raters of varying backgrounds and expertise. While our approach is not sufficient to holistically assess whether the deployment of an artificial intelligence (AI) system promotes equitable health outcomes, we hope that it can be leveraged and built upon toward a shared goal of LLMs that promote accessible and equitable healthcare.",
      "journal": "Nature medicine",
      "year": "2024",
      "doi": "10.1038/s41591-024-03258-2",
      "authors": "Pfohl Stephen R et al.",
      "keywords": "",
      "mesh_terms": "Health Equity; Humans; Language; Bias; Artificial Intelligence; Healthcare Disparities",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39313595/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Our experience underscores the importance of using diverse assessment methodologies and involving raters of varying backgrounds and expertise. While our approach is not sufficient to holistically assess whether the deployment of an artificial intelligence (AI) system promotes equitable health outcomes, we hope that it can be leveraged and built upon toward a shared goal of LLMs that promote accessible and equitable healthcare.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11645264"
    },
    {
      "pmid": "39357045",
      "title": "Leveraging Temporal Trends for Training Contextual Word Embeddings to Address Bias in Biomedical Applications: Development Study.",
      "abstract": "BACKGROUND: Women have been underrepresented in clinical trials for many years. Machine-learning models trained on clinical trial abstracts may capture and amplify biases in the data. Specifically, word embeddings are models that enable representing words as vectors and are the building block of most natural language processing systems. If word embeddings are trained on clinical trial abstracts, predictive models that use the embeddings will exhibit gender performance gaps. OBJECTIVE: We aim to capture temporal trends in clinical trials through temporal distribution matching on contextual word embeddings (specifically, BERT) and explore its effect on the bias manifested in downstream tasks. METHODS: We present TeDi-BERT, a method to harness the temporal trend of increasing women's inclusion in clinical trials to train contextual word embeddings. We implement temporal distribution matching through an adversarial classifier, trying to distinguish old from new clinical trial abstracts based on their embeddings. The temporal distribution matching acts as a form of domain adaptation from older to more recent clinical trials. We evaluate our model on 2 clinical tasks: prediction of unplanned readmission to the intensive care unit and hospital length of stay prediction. We also conduct an algorithmic analysis of the proposed method. RESULTS: In readmission prediction, TeDi-BERT achieved area under the receiver operating characteristic curve of 0.64 for female patients versus the baseline of 0.62 (P<.001), and 0.66 for male patients versus the baseline of 0.64 (P<.001). In the length of stay regression, TeDi-BERT achieved a mean absolute error of 4.56 (95% CI 4.44-4.68) for female patients versus 4.62 (95% CI 4.50-4.74, P<.001) and 4.54 (95% CI 4.44-4.65) for male patients versus 4.6 (95% CI 4.50-4.71, P<.001). CONCLUSIONS: In both clinical tasks, TeDi-BERT improved performance for female patients, as expected; but it also improved performance for male patients. Our results show that accuracy for one gender does not need to be exchanged for bias reduction, but rather that good science improves clinical results for all. Contextual word embedding models trained to capture temporal trends can help mitigate the effects of bias that changes over time in the training data.",
      "journal": "JMIR AI",
      "year": "2024",
      "doi": "10.2196/49546",
      "authors": "Agmon Shunit et al.",
      "keywords": "BERT; NLP; algorithms; bias; gender; natural language processing; statistical models; word embeddings",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39357045/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM; Clinical Prediction Model",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Representation Learning; Transfer Learning",
      "clinical_setting": "Hospital/Inpatient; ICU; Clinical Trial",
      "key_findings": "CONCLUSIONS: In both clinical tasks, TeDi-BERT improved performance for female patients, as expected; but it also improved performance for male patients. Our results show that accuracy for one gender does not need to be exchanged for bias reduction, but rather that good science improves clinical results for all. Contextual word embedding models trained to capture temporal trends can help mitigate the effects of bias that changes over time in the training data.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11483253"
    },
    {
      "pmid": "39369018",
      "title": "A fair individualized polysocial risk score for identifying increased social risk in type 2 diabetes.",
      "abstract": "Racial and ethnic minorities bear a disproportionate burden of type 2 diabetes (T2D) and its complications, with social determinants of health (SDoH) recognized as key drivers of these disparities. Implementing efficient and effective social needs management strategies is crucial. We propose a machine learning analytic pipeline to calculate the individualized polysocial risk score (iPsRS), which can identify T2D patients at high social risk for hospitalization, incorporating explainable AI techniques and algorithmic fairness optimization. We use electronic health records (EHR) data from T2D patients in the University of Florida Health Integrated Data Repository, incorporating both contextual SDoH (e.g., neighborhood deprivation) and person-level SDoH (e.g., housing instability). After fairness optimization across racial and ethnic groups, the iPsRS achieved a C statistic of 0.71 in predicting 1-year hospitalization. Our iPsRS can fairly and accurately screen patients with T2D who are at increased social risk for hospitalization.",
      "journal": "Nature communications",
      "year": "2024",
      "doi": "10.1038/s41467-024-52960-9",
      "authors": "Huang Yu et al.",
      "keywords": "",
      "mesh_terms": "Adult; Aged; Female; Humans; Male; Middle Aged; Diabetes Mellitus, Type 2; Electronic Health Records; Ethnicity; Florida; Hospitalization; Machine Learning; Risk Assessment; Risk Factors; Social Determinants of Health; Racial Groups",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39369018/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "EHR/Health Informatics; Endocrinology/Diabetes",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "After fairness optimization across racial and ethnic groups, the iPsRS achieved a C statistic of 0.71 in predicting 1-year hospitalization. Our iPsRS can fairly and accurately screen patients with T2D who are at increased social risk for hospitalization.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11455957"
    },
    {
      "pmid": "39371141",
      "title": "Evaluating and Reducing Subgroup Disparity in AI Models: An Analysis of Pediatric COVID-19 Test Outcomes.",
      "abstract": "Artificial Intelligence (AI) fairness in healthcare settings has attracted significant attention due to the concerns to propagate existing health disparities. Despite ongoing research, the frequency and extent of subgroup fairness have not been sufficiently studied. In this study, we extracted a nationally representative pediatric dataset (ages 0-17, n=9,935) from the US National Health Interview Survey (NHIS) concerning COVID-19 test outcomes. For subgroup disparity assessment, we trained 50 models using five machine learning algorithms. We assessed the models' area under the curve (AUC) on 12 small (<15% of the total n) subgroups defined using social economic factors versus the on the overall population. Our results show that subgroup disparities were prevalent (50.7%) in the models. Subgroup AUCs were generally lower, with a mean difference of 0.01, ranging from -0.29 to +0.41. Notably, the disparities were not always statistically significant, with four out of 12 subgroups having statistically significant disparities across models. Additionally, we explored the efficacy of synthetic data in mitigating identified disparities. The introduction of synthetic data enhanced subgroup disparity in 57.7% of the models. The mean AUC disparities for models with synthetic data decreased on average by 0.03 via resampling and 0.04 via generative adverbial network methods.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2024",
      "doi": "10.1101/2024.09.18.24313889",
      "authors": "Libin Alexander et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39371141/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "Pediatrics; Pulmonology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Data Augmentation; Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "The introduction of synthetic data enhanced subgroup disparity in 57.7% of the models. The mean AUC disparities for models with synthetic data decreased on average by 0.03 via resampling and 0.04 via generative adverbial network methods.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11451670"
    },
    {
      "pmid": "39384748",
      "title": "Enhancing fairness in AI-enabled medical systems with the attribute neutral framework.",
      "abstract": "Questions of unfairness and inequity pose critical challenges to the successful deployment of artificial intelligence (AI) in healthcare settings. In AI models, unequal performance across protected groups may be partially attributable to the learning of spurious or otherwise undesirable correlations between sensitive attributes and disease-related information. Here, we introduce the Attribute Neutral Framework, designed to disentangle biased attributes from disease-relevant information and subsequently neutralize them to improve representation across diverse subgroups. Within the framework, we develop the Attribute Neutralizer (AttrNzr) to generate neutralized data, for which protected attributes can no longer be easily predicted by humans or by machine learning classifiers. We then utilize these data to train the disease diagnosis model (DDM). Comparative analysis with other unfairness mitigation algorithms demonstrates that AttrNzr outperforms in reducing the unfairness of the DDM while maintaining DDM's overall disease diagnosis performance. Furthermore, AttrNzr supports the simultaneous neutralization of multiple attributes and demonstrates utility even when applied solely during the training phase, without being used in the test phase. Moreover, instead of introducing additional constraints to the DDM, the AttrNzr directly addresses a root cause of unfairness, providing a model-independent solution. Our results with AttrNzr highlight the potential of data-centered and model-independent solutions for fairness challenges in AI-enabled medical systems.",
      "journal": "Nature communications",
      "year": "2024",
      "doi": "10.1038/s41467-024-52930-1",
      "authors": "Hu Lianting et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Algorithms; Machine Learning; Delivery of Health Care",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39384748/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Representation Learning",
      "clinical_setting": "Not specified",
      "key_findings": "Moreover, instead of introducing additional constraints to the DDM, the AttrNzr directly addresses a root cause of unfairness, providing a model-independent solution. Our results with AttrNzr highlight the potential of data-centered and model-independent solutions for fairness challenges in AI-enabled medical systems.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11464531"
    },
    {
      "pmid": "39399154",
      "title": "Predicting Prenatal Depression and Assessing Model Bias Using Machine Learning Models.",
      "abstract": "BACKGROUND: Perinatal depression is one of the most common medical complications during pregnancy and postpartum period, affecting 10% to 20% of pregnant individuals, with higher rates among Black and Latina women who are also less likely to be diagnosed and treated. Machine learning (ML) models based on electronic medical records (EMRs) have effectively predicted postpartum depression in middle-class White women but have rarely included sufficient proportions of racial/ethnic minorities, which has contributed to biases in ML models. Our goal is to determine whether ML models could predict depression in early pregnancy in racial/ethnic minority women by leveraging EMR data. METHODS: We extracted EMRs from a large U.S. urban hospital serving mostly low-income Black and Hispanic women (n\u00a0= 5875). Depressive symptom severity was assessed using the Patient Health Questionnaire-9 self-report questionnaire. We investigated multiple ML classifiers using Shapley additive explanations for model interpretation and determined prediction bias with 4 metrics: disparate impact, equal opportunity difference, and equalized odds (standard deviations of true positives and false positives). RESULTS: Although the best-performing ML model's (elastic net) performance was low (area under the receiver operating characteristic curve\u00a0= 0.61), we identified known perinatal depression risk factors such as unplanned pregnancy and being single and underexplored factors such as self-reported pain, lower prenatal vitamin intake, asthma, carrying a male fetus, and lower platelet levels. Despite the sample comprising mostly low-income minority women (54% Black, 27% Latina), the model performed worse for these communities (area under the receiver operating characteristic curve: 57% Black, 59% Latina women vs. 64% White women). CONCLUSIONS: EMR-based ML models could moderately predict early pregnancy depression but exhibited biased performance against low-income minority women. Perinatal depression affects 10% to 20% of pregnant individuals, with higher rates among racial/ethnic minorities who are underdiagnosed and undertreated. This study used machine learning models on electronic medical record data from a hospital serving mostly low-income Black and Hispanic women to predict early pregnancy depression. While the best model performed moderately well, it exhibited bias, predicting depression less accurately for Black and Latina women compared with White women. The study identified some known risk factors such as unplanned pregnancy and underexplored factors such as self-reported pain, lower prenatal vitamin intake, and carrying a male fetus that may contribute to perinatal depression.",
      "journal": "Biological psychiatry global open science",
      "year": "2024",
      "doi": "10.1016/j.bpsgos.2024.100376",
      "authors": "Huang Yongchao et al.",
      "keywords": "Electronic medical records; Health disparities; Machine learning; Model performance bias; Perinatal depression",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39399154/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry; EHR/Health Informatics; Obstetrics/Maternal Health; Pulmonology; Pain Management",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "CONCLUSIONS: EMR-based ML models could moderately predict early pregnancy depression but exhibited biased performance against low-income minority women. Perinatal depression affects 10% to 20% of pregnant individuals, with higher rates among racial/ethnic minorities who are underdiagnosed and undertreated. This study used machine learning models on electronic medical record data from a hospital serving mostly low-income Black and Hispanic women to predict early pregnancy depression.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11470166"
    },
    {
      "pmid": "39420378",
      "title": "Inconsistent values and algorithmic fairness: a review of organ allocation priority systems in the United States.",
      "abstract": "BACKGROUND: The Organ Procurement and Transplant Network (OPTN) Final Rule guides national organ transplantation policies, mandating equitable organ allocation and organ-specific priority stratification systems. Current allocation scores rely on mortality predictions. METHODS: We examined the alignment between the ethical priorities across organ prioritization systems and the statistical design of the risk models in question. We searched PubMed for literature on organ allocation history, policy, and ethics in the United States. RESULTS: We identified 127 relevant articles, covering kidney (19), liver (60), lung (24), and heart transplants (23), and transplant accessibility (1). Current risk scores emphasize model performance and overlook ethical concerns in variable selection. The inclusion of race, sex, and geographical limits as categorical variables lacks biological basis; therefore, blurring the line between evidence-based models and discrimination. Comprehensive ethical and equity evaluation of risk scores is lacking, with only limited discussion of the algorithmic fairness of the Model for End-Stage Liver Disease (MELD) and the Kidney Donor Risk Index (KDRI) in some literature. We uncovered the inconsistent ethical standards underlying organ allocation scores in the United States. Specifically, we highlighted the exception points in MELD, the inclusion of race in KDRI, the geographical limit in the Lung Allocation Score, and the inadequacy of risk stratification in the Heart Tier system, creating obstacles for medically underserved populations. CONCLUSIONS: We encourage efforts to address statistical and ethical concerns in organ allocation models and urge standardization and transparency in policy development to ensure fairness, equitability, and evidence-based risk predictions.",
      "journal": "BMC medical ethics",
      "year": "2024",
      "doi": "10.1186/s12910-024-01116-x",
      "authors": "Dale Reid et al.",
      "keywords": "Estimated Post Transplant Survival (EPTS); Heart Tier System; Kidney Donor Profile Index (KDPI); Kidney Donor Risk Index (KDRI); Model for End-Stage Liver Disease (MELD); Organ allocation; The Lung Allocation Score (LAS); Transplantation",
      "mesh_terms": "Humans; Algorithms; Health Care Rationing; Organ Transplantation; Resource Allocation; Risk Assessment; Tissue and Organ Procurement; Tissue Donors; United States",
      "pub_types": "Journal Article; Systematic Review; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39420378/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Clinical Prediction Model; Generative AI",
      "health_domain": "Cardiology; Nephrology; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Safety-Net/Underserved",
      "key_findings": "CONCLUSIONS: We encourage efforts to address statistical and ethical concerns in organ allocation models and urge standardization and transparency in policy development to ensure fairness, equitability, and evidence-based risk predictions.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11483980"
    },
    {
      "pmid": "39427028",
      "title": "Guidance for unbiased predictive information for healthcare decision-making and equity (GUIDE): considerations when race may be a prognostic factor.",
      "abstract": "Clinical prediction models (CPMs) are tools that compute the risk of an outcome given a set of patient characteristics and are routinely used to inform patients, guide treatment decision-making, and resource allocation. Although much hope has been placed on CPMs to mitigate human biases, CPMs may potentially contribute to racial disparities in decision-making and resource allocation. While some policymakers, professional organizations, and scholars have called for eliminating race as a variable from CPMs, others raise concerns that excluding race may exacerbate healthcare disparities and this controversy remains unresolved. The Guidance for Unbiased predictive Information for healthcare Decision-making and Equity (GUIDE) provides expert guidelines for model developers and health system administrators on the transparent use of race in CPMs and mitigation of algorithmic bias across contexts developed through a 5-round, modified Delphi process from a diverse 14-person technical expert panel (TEP). Deliberations affirmed that race is a social construct and that the goals of prediction are distinct from those of causal inference, and emphasized: the importance of decisional context (e.g., shared decision-making versus healthcare rationing); the conflicting nature of different anti-discrimination principles (e.g., anticlassification versus antisubordination principles); and the importance of identifying and balancing trade-offs in achieving equity-related goals with race-aware versus race-unaware CPMs for conditions where racial identity is prognostically informative. The GUIDE, comprising 31 key items in the development and use of CPMs in healthcare, outlines foundational principles, distinguishes between bias and fairness, and offers guidance for examining subgroup invalidity and using race as a variable in CPMs. This GUIDE presents a living document that supports appraisal and reporting of bias in CPMs to support best practice in CPM development and use.",
      "journal": "NPJ digital medicine",
      "year": "2024",
      "doi": "10.1038/s41746-024-01245-y",
      "authors": "Ladin Keren et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39427028/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Prediction Model; Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Counterfactual Fairness",
      "clinical_setting": "Not specified",
      "key_findings": "The GUIDE, comprising 31 key items in the development and use of CPMs in healthcare, outlines foundational principles, distinguishes between bias and fairness, and offers guidance for examining subgroup invalidity and using race as a variable in CPMs. This GUIDE presents a living document that supports appraisal and reporting of bias in CPMs to support best practice in CPM development and use.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11490638"
    },
    {
      "pmid": "39433945",
      "title": "Evaluation and mitigation of cognitive biases in medical language models.",
      "abstract": "Increasing interest in applying large language models (LLMs) to medicine is due in part to their impressive performance on medical exam questions. However, these exams do not capture the complexity of real patient-doctor interactions because of factors like patient compliance, experience, and cognitive bias. We hypothesized that LLMs would produce less accurate responses when faced with clinically biased questions as compared to unbiased ones. To test this, we developed the BiasMedQA dataset, which consists of 1273 USMLE questions modified to replicate common clinically relevant cognitive biases. We assessed six LLMs on BiasMedQA and found that GPT-4 stood out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which showed large drops in performance. Additionally, we introduced three bias mitigation strategies, which improved but did not fully restore accuracy. Our findings highlight the need to improve LLMs' robustness to cognitive biases, in order to achieve more reliable applications of LLMs in healthcare.",
      "journal": "NPJ digital medicine",
      "year": "2024",
      "doi": "10.1038/s41746-024-01283-6",
      "authors": "Schmidgall Samuel et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39433945/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Additionally, we introduced three bias mitigation strategies, which improved but did not fully restore accuracy. Our findings highlight the need to improve LLMs' robustness to cognitive biases, in order to achieve more reliable applications of LLMs in healthcare.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11494053"
    },
    {
      "pmid": "39441784",
      "title": "Conceptualizing bias in EHR data: A case study in performance disparities by demographic subgroups for a pediatric obesity incidence classifier.",
      "abstract": "Electronic Health Records (EHRs) are increasingly used to develop machine learning models in predictive medicine. There has been limited research on utilizing machine learning methods to predict childhood obesity and related disparities in classifier performance among vulnerable patient subpopulations. In this work, classification models are developed to recognize pediatric obesity using temporal condition patterns obtained from patient EHR data in a U.S. study population. We trained four machine learning algorithms (Logistic Regression, Random Forest, Gradient Boosted Trees, and Neural Networks) to classify cases and controls as obesity positive or negative, and optimized hyperparameter settings through a bootstrapping methodology. To assess the classifiers for bias, we studied model performance by population subgroups then used permutation analysis to identify the most predictive features for each model and the demographic characteristics of patients with these features. Mean AUC-ROC values were consistent across classifiers, ranging from 0.72-0.80. Some evidence of bias was identified, although this was through the models performing better for minority subgroups (African Americans and patients enrolled in Medicaid). Permutation analysis revealed that patients from vulnerable population subgroups were over-represented among patients with the most predictive diagnostic patterns. We hypothesize that our models performed better on under-represented groups because the features more strongly associated with obesity were more commonly observed among minority patients. These findings highlight the complex ways that bias may arise in machine learning models and can be incorporated into future research to develop a thorough analytical approach to identify and mitigate bias that may arise from features and within EHR datasets when developing more equitable models.",
      "journal": "PLOS digital health",
      "year": "2024",
      "doi": "10.1371/journal.pdig.0000642",
      "authors": "Campbell Elizabeth A et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39441784/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Random Forest; Logistic Regression; Neural Network",
      "health_domain": "EHR/Health Informatics; Pediatrics",
      "bias_axes": "Race/Ethnicity; Age; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Subgroup Analysis",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We hypothesize that our models performed better on under-represented groups because the features more strongly associated with obesity were more commonly observed among minority patients. These findings highlight the complex ways that bias may arise in machine learning models and can be incorporated into future research to develop a thorough analytical approach to identify and mitigate bias that may arise from features and within EHR datasets when developing more equitable models.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11498669"
    },
    {
      "pmid": "39495385",
      "title": "Evaluating machine learning model bias and racial disparities in non-small cell lung cancer using SEER registry data.",
      "abstract": "BACKGROUND: Despite decades of pursuing health equity, racial and ethnic disparities persist in healthcare in America. For cancer specifically, one of the leading observed disparities is worse mortality among non-Hispanic Black patients compared to non-Hispanic White patients across the cancer care continuum. These real-world disparities are reflected in the data used to inform the decisions made to alleviate such inequities. Failing to account for inherently biased data underlying these observations could intensify racial cancer disparities and lead to misguided efforts that fail to appropriately address the real causes of health inequity. OBJECTIVE: Estimate the racial/ethnic bias of machine learning models in predicting two-year survival and surgery treatment recommendation for non-small cell lung cancer (NSCLC) patients. METHODS: A Cox survival model, and a LOGIT model as well as three other machine learning models for predicting surgery recommendation were trained using SEER data from NSCLC patients diagnosed from 2000-2018. Models were trained with a 70/30 train/test split (both including and excluding race/ethnicity) and evaluated using performance and fairness metrics. The effects of oversampling the training data were also evaluated. RESULTS: The survival models show disparate impact towards non-Hispanic Black patients regardless of whether race/ethnicity is used as a predictor. The models including race/ethnicity amplified the disparities observed in the data. The exclusion of race/ethnicity as a predictor in the survival and surgery recommendation models improved fairness metrics without degrading model performance. Stratified oversampling strategies reduced disparate impact while reducing the accuracy of the model. CONCLUSION: NSCLC disparities are complex and multifaceted. Yet, even when accounting for age and stage at diagnosis, non-Hispanic Black patients with NSCLC are less often recommended to have surgery than non-Hispanic White patients. Machine learning models amplified the racial/ethnic disparities across the cancer care continuum (which are reflected in the data used to make model decisions). Excluding race/ethnicity lowered the bias of the models but did not affect disparate impact. Developing analytical strategies to improve fairness would in turn improve the utility of machine learning approaches analyzing population-based cancer data.",
      "journal": "Health care management science",
      "year": "2024",
      "doi": "10.1007/s10729-024-09691-6",
      "authors": "Trentz Cameron et al.",
      "keywords": "Fairness in AI; Health disparities; Non-small cell lung cancer survival; Racial disparities",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Bias; Black or African American; Carcinoma, Non-Small-Cell Lung; Ethnicity; Healthcare Disparities; Lung Neoplasms; Machine Learning; Proportional Hazards Models; Racial Groups; SEER Program; United States; White",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39495385/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; Surgery; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Fairness Metrics Evaluation",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: NSCLC disparities are complex and multifaceted. Yet, even when accounting for age and stage at diagnosis, non-Hispanic Black patients with NSCLC are less often recommended to have surgery than non-Hispanic White patients. Machine learning models amplified the racial/ethnic disparities across the cancer care continuum (which are reflected in the data used to make model decisions).",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39502657",
      "title": "Raising awareness of potential biases in medical machine learning: Experience from a Datathon.",
      "abstract": "OBJECTIVE: To challenge clinicians and informaticians to learn about potential sources of bias in medical machine learning models through investigation of data and predictions from an open-source severity of illness score. METHODS: Over a two-day period (total elapsed time approximately 28 hours), we conducted a datathon that challenged interdisciplinary teams to investigate potential sources of bias in the Global Open Source Severity of Illness Score. Teams were invited to develop hypotheses, to use tools of their choosing to identify potential sources of bias, and to provide a final report. RESULTS: Five teams participated, three of which included both informaticians and clinicians. Most (4/5) used Python for analyses, the remaining team used R. Common analysis themes included relationship of the GOSSIS-1 prediction score with demographics and care related variables; relationships between demographics and outcomes; calibration and factors related to the context of care; and the impact of missingness. Representativeness of the population, differences in calibration and model performance among groups, and differences in performance across hospital settings were identified as possible sources of bias. DISCUSSION: Datathons are a promising approach for challenging developers and users to explore questions relating to unrecognized biases in medical machine learning algorithms.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2024",
      "doi": "10.1101/2024.10.21.24315543",
      "authors": "Hochheiser Harry et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39502657/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Calibration",
      "clinical_setting": "Hospital/Inpatient; Public Health/Population",
      "key_findings": "RESULTS: Five teams participated, three of which included both informaticians and clinicians. Most (4/5) used Python for analyses, the remaining team used R. Common analysis themes included relationship of the GOSSIS-1 prediction score with demographics and care related variables; relationships between demographics and outcomes; calibration and factors related to the context of care; and the impact of missingness.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11537317"
    },
    {
      "pmid": "39509461",
      "title": "Bias in medical AI: Implications for clinical decision-making.",
      "abstract": "Biases in medical artificial intelligence (AI) arise and compound throughout the AI lifecycle. These biases can have significant clinical consequences, especially in applications that involve clinical decision-making. Left unaddressed, biased medical AI can lead to substandard clinical decisions and the perpetuation and exacerbation of longstanding healthcare disparities. We discuss potential biases that can arise at different stages in the AI development pipeline and how they can affect AI algorithms and clinical decision-making. Bias can occur in data features and labels, model development and evaluation, deployment, and publication. Insufficient sample sizes for certain patient groups can result in suboptimal performance, algorithm underestimation, and clinically unmeaningful predictions. Missing patient findings can also produce biased model behavior, including capturable but nonrandomly missing data, such as diagnosis codes, and data that is not usually or not easily captured, such as social determinants of health. Expertly annotated labels used to train supervised learning models may reflect implicit cognitive biases or substandard care practices. Overreliance on performance metrics during model development may obscure bias and diminish a model's clinical utility. When applied to data outside the training cohort, model performance can deteriorate from previous validation and can do so differentially across subgroups. How end users interact with deployed solutions can introduce bias. Finally, where models are developed and published, and by whom, impacts the trajectories and priorities of future medical AI development. Solutions to mitigate bias must be implemented with care, which include the collection of large and diverse data sets, statistical debiasing methods, thorough model evaluation, emphasis on model interpretability, and standardized bias reporting and transparency requirements. Prior to real-world implementation in clinical settings, rigorous validation through clinical trials is critical to demonstrate unbiased application. Addressing biases across model development stages is crucial for ensuring all patients benefit equitably from the future of medical AI.",
      "journal": "PLOS digital health",
      "year": "2024",
      "doi": "10.1371/journal.pdig.0000651",
      "authors": "Cross James L et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39509461/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Intersectional",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Clinical Trial",
      "key_findings": "Prior to real-world implementation in clinical settings, rigorous validation through clinical trials is critical to demonstrate unbiased application. Addressing biases across model development stages is crucial for ensuring all patients benefit equitably from the future of medical AI.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11542778"
    },
    {
      "pmid": "39543648",
      "title": "Prognostic risk prediction model for patients with acute exacerbation of chronic obstructive pulmonary disease (AECOPD): a systematic review and meta-analysis.",
      "abstract": "BACKGROUND: Chronic obstructive pulmonary disease (COPD) is a prevalent respiratory condition and a leading cause of mortality, with acute exacerbations (AECOPD) significantly complicating its management and prognosis. Despite the development of various prognostic prediction models for patients with AECOPD, their performance and clinical applicability remain unclear, necessitating a systematic review to evaluate these models and provide guidance for their future improvement and clinical use. METHOD: PubMed, Web of Science, CINAHL, Scopus, EMBASE, and Medline were searched for studies published from their inception until February 5, 2024. Data extraction and evaluation were conducted using the Checklist for Critical Appraisal and Data Extraction for Systematic Reviews of Prediction Modelling Studies (CHARMS). The Prediction model Risk Of Bias Assessment Tool (PROBAST) was employed to assess the risk of bias and applicability of the models. RESULTS: After deduplication and screening 5942 retrieved articles, 46 studies comprising 53 models were included. Of these, 17 (37.0%) studies developed from studies conducted in China. All models were based on cohort studies. Mortality was the predicted outcome in 27 (50.9%) models. Logistic regression was used in 41 (77.4%) models, while machine learning methods were employed in 9 (17.0%) models. The median (minimum, maximum) sample size for model development was 672 (106, 150,035). The median (minimum, maximum) number of predictors per model was 5 (2, 42). Frequently used predictors included age (n\u2009=\u200928), dyspnea severity scores (n\u2009=\u200912), and PaCO2 (n\u2009=\u200911). The pooled AUC was 0.80 for mortality prediction models and 0.84 for hospitalization-related outcomes. 52 models have a high overall risk of bias, and all models were judged to have low concern regarding applicability. Major sources of bias included insufficient sample sizes (83.0%), reliance on univariate analysis for predictor selection (73.6%), inappropriate internal and external validation methods (54.7%), inappropriate inclusion and exclusion criteria for study subjects (50.9%) and so on. The only model with low bias was the PEARL score. CONCLUSION: Current prognostic risk prediction models for patients with AECOPD generally exhibit high bias. Future efforts should standardize model development and validation methods, and develop widely usable clinical models.",
      "journal": "Respiratory research",
      "year": "2024",
      "doi": "10.1186/s12931-024-03033-4",
      "authors": "Xu Zihan et al.",
      "keywords": "Acute exacerbation; Chronic obstructive pulmonary disease; Prediction model; Prognostic risk; Systematic review",
      "mesh_terms": "Humans; Pulmonary Disease, Chronic Obstructive; Prognosis; Risk Assessment; Disease Progression; Predictive Value of Tests; Risk Factors",
      "pub_types": "Systematic Review; Journal Article; Meta-Analysis",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39543648/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Deep Learning; Logistic Regression; Clinical Prediction Model",
      "health_domain": "Pulmonology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "CONCLUSION: Current prognostic risk prediction models for patients with AECOPD generally exhibit high bias. Future efforts should standardize model development and validation methods, and develop widely usable clinical models.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11566839"
    },
    {
      "pmid": "39569213",
      "title": "FAIM: Fairness-aware interpretable modeling for trustworthy machine learning in healthcare.",
      "abstract": "The escalating integration of machine learning in high-stakes fields such as healthcare raises substantial concerns about model fairness. We propose an interpretable framework, fairness-aware interpretable modeling (FAIM), to improve model fairness without compromising performance, featuring an interactive interface to identify a \"fairer\" model from a set of high-performing models and promoting the integration of data-driven evidence and clinical expertise to enhance contextualized fairness. We demonstrate FAIM's value in reducing intersectional biases arising from race and sex by predicting hospital admission with two real-world databases, the Medical Information Mart for Intensive Care IV Emergency Department (MIMIC-IV-ED) and the database collected from Singapore General Hospital Emergency Department (SGH-ED). For both datasets, FAIM models not only exhibit satisfactory discriminatory performance but also significantly mitigate biases as measured by well-established fairness metrics, outperforming commonly used bias mitigation methods. Our approach demonstrates the feasibility of improving fairness without sacrificing performance and provides a modeling mode that invites domain experts to engage, fostering a multidisciplinary effort toward tailored AI fairness.",
      "journal": "Patterns (New York, N.Y.)",
      "year": "2024",
      "doi": "10.1016/j.patter.2024.101059",
      "authors": "Liu Mingxuan et al.",
      "keywords": "AI fairness; fairness in healthcare; interpretable machine learning; trustworthy machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39569213/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Emergency Medicine; ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Intersectional",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "Hospital/Inpatient; ICU; Emergency Department",
      "key_findings": "For both datasets, FAIM models not only exhibit satisfactory discriminatory performance but also significantly mitigate biases as measured by well-established fairness metrics, outperforming commonly used bias mitigation methods. Our approach demonstrates the feasibility of improving fairness without sacrificing performance and provides a modeling mode that invites domain experts to engage, fostering a multidisciplinary effort toward tailored AI fairness.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11573921"
    },
    {
      "pmid": "39602221",
      "title": "Ensuring Appropriate Representation in Artificial Intelligence-Generated Medical Imagery: Protocol for a Methodological Approach to Address Skin Tone Bias.",
      "abstract": "BACKGROUND: In medical education, particularly in anatomy and dermatology, generative artificial intelligence (AI) can be used to create customized illustrations. However, the underrepresentation of darker skin tones in medical textbooks and elsewhere, which serve as training data for AI, poses a significant challenge in ensuring diverse and inclusive educational materials. OBJECTIVE: This study aims to evaluate the extent of skin tone diversity in AI-generated medical images and to test whether the representation of skin tones can be improved by modifying AI prompts to better reflect the demographic makeup of the US population. METHODS: In total, 2 standard AI models (Dall-E [OpenAI] and Midjourney [Midjourney Inc]) each generated 100 images of people with psoriasis. In addition, a custom model was developed that incorporated a prompt injection aimed at \"forcing\" the AI (Dall-E 3) to reflect the skin tone distribution of the US population according to the 2012 American National Election Survey. This custom model generated another set of 100 images. The skin tones in these images were assessed by 3 researchers using the New Immigrant Survey skin tone scale, with the median value representing each image. A chi-square goodness of fit analysis compared the skin tone distributions from each set of images to that of the US population. RESULTS: The standard AI models (Dalle-3 and Midjourney) demonstrated a significant difference between the expected skin tones of the US population and the observed tones in the generated images (P<.001). Both standard AI models overrepresented lighter skin. Conversely, the custom model with the modified prompt yielded a distribution of skin tones that closely matched the expected demographic representation, showing no significant difference (P=.04). CONCLUSIONS: This study reveals a notable bias in AI-generated medical images, predominantly underrepresenting darker skin tones. This bias can be effectively addressed by modifying AI prompts to incorporate real-life demographic distributions. The findings emphasize the need for conscious efforts in AI development to ensure diverse and representative outputs, particularly in educational and medical contexts. Users of generative AI tools should be aware that these biases exist, and that similar tendencies may also exist in other types of generative AI (eg, large language models) and in other characteristics (eg, sex, gender, culture, and ethnicity). Injecting demographic data into AI prompts may effectively counteract these biases, ensuring a more accurate representation of the general population.",
      "journal": "JMIR AI",
      "year": "2024",
      "doi": "10.2196/58275",
      "authors": "O'Malley Andrew et al.",
      "keywords": "AI images; United States; anatomy; artificial intelligence; dermatology; digital imagery; educational material; generative AI; medical education; medical imaging; psoriasis; skin; skin tone",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39602221/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "NLP/LLM; Generative AI",
      "health_domain": "Dermatology; ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSIONS: This study reveals a notable bias in AI-generated medical images, predominantly underrepresenting darker skin tones. This bias can be effectively addressed by modifying AI prompts to incorporate real-life demographic distributions. The findings emphasize the need for conscious efforts in AI development to ensure diverse and representative outputs, particularly in educational and medical contexts.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11635324"
    },
    {
      "pmid": "39625723",
      "title": "Evaluating Bias-Mitigated Predictive Models of Perinatal Mood and Anxiety Disorders.",
      "abstract": "IMPORTANCE: Machine learning for augmented screening of perinatal mood and anxiety disorders (PMADs) requires thorough consideration of clinical biases embedded in electronic health records (EHRs) and rigorous evaluations of model performance. OBJECTIVE: To mitigate bias in predictive models of PMADs trained on commonly available EHRs. DESIGN, SETTING, AND PARTICIPANTS: This diagnostic study collected data as part of a quality improvement initiative from 2020 to 2023 at Cedars-Sinai Medical Center in Los Angeles, California. The study inclusion criteria were birthing patients aged 14 to 59 years with live birth records and admission to the postpartum unit or the maternal-fetal care unit after delivery. EXPOSURE: Patient-reported race and ethnicity (7 levels) obtained through EHRs. MAIN OUTCOMES AND MEASURES: Logistic regression, random forest, and extreme gradient boosting models were trained to predict 2 binary outcomes: moderate to high-risk (positive) screen assessed using the 9-item Patient Health Questionnaire (PHQ-9), and the Edinburgh Postnatal Depression Scale (EPDS). Each model was fitted with or without reweighing data during preprocessing and evaluated through repeated K-fold cross validation. In every iteration, each model was evaluated on its area under the receiver operating curve (AUROC) and on 2 fairness metrics: demographic parity (DP), and difference in false negatives between races and ethnicities (relative to non-Hispanic White patients). RESULTS: Among 19\u202f430 patients in this study, 1402 (7%) identified as African American or Black, 2371 (12%) as Asian American and Pacific Islander; 1842 (10%) as Hispanic White, 10\u202f942 (56.3%) as non-Hispanic White, 606 (3%) as multiple races, 2146 (11%) as other (not further specified), and 121 (<1%) did not provide this information. The mean (SD) age was 34.1 (4.9) years, and all patients identified as female. Racial and ethnic minority patients were significantly more likely than non-Hispanic White patients to screen positive on both the PHQ-9 (odds ratio, 1.47 [95% CI, 1.23-1.77]) and the EPDS (odds ratio, 1.38 [95% CI, 1.20-1.57]). Mean AUROCs ranged from 0.610 to 0.635 without reweighing (baseline), and from 0.602 to 0.622 with reweighing. Baseline models predicted significantly greater prevalence of postpartum depression for patients who were not non-Hispanic White relative to those who were (mean DP, 0.238 [95% CI, 0.231-0.244]; P\u2009<\u2009.001) and displayed significantly lower false-negative rates (mean difference, -0.184 [95% CI, -0.195 to -0.174]; P\u2009<\u2009.001). Reweighing significantly reduced differences in DP (mean DP with reweighing, 0.022 [95% CI, 0.017-0.026]; P\u2009<\u2009.001) and false-negative rates (mean difference with reweighing, 0.018 [95% CI, 0.008-0.028]; P\u2009<\u2009.001) between racial and ethnic groups. CONCLUSIONS AND RELEVANCE: In this diagnostic study of predictive models of postpartum depression, clinical prediction models trained to predict psychometric screening results from commonly available EHRs achieved modest performance and were less likely to widen existing health disparities in PMAD diagnosis and potentially treatment. These findings suggest that is critical for researchers and physicians to consider their model design (eg, desired target and predictor variables) and evaluate model bias to minimize health disparities.",
      "journal": "JAMA network open",
      "year": "2024",
      "doi": "10.1001/jamanetworkopen.2024.38152",
      "authors": "Wong Emily F et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Adult; Pregnancy; Anxiety Disorders; Mood Disorders; Adolescent; Young Adult; Electronic Health Records; Machine Learning; Bias; Pregnancy Complications; Middle Aged; Logistic Models",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39625723/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Random Forest; Logistic Regression; XGBoost/Gradient Boosting; Clinical Prediction Model",
      "health_domain": "Mental Health/Psychiatry; EHR/Health Informatics; Obstetrics/Maternal Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: Among 19\u202f430 patients in this study, 1402 (7%) identified as African American or Black, 2371 (12%) as Asian American and Pacific Islander; 1842 (10%) as Hispanic White, 10\u202f942 (56.3%) as non-Hispanic White, 606 (3%) as multiple races, 2146 (11%) as other (not further specified), and 121 (<1%) did not provide this information. The mean (SD) age was 34.1 (4.9) years, and all patients identified as female. Racial and ethnic minority patients were significantly more likely than non-Hispanic...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11615713"
    },
    {
      "pmid": "39671594",
      "title": "Survival After Radical Cystectomy for Bladder Cancer: Development of a Fair Machine Learning Model.",
      "abstract": "BACKGROUND: Prediction models based on machine learning (ML) methods are being increasingly developed and adopted in health care. However, these models may be prone to bias and considered unfair if they demonstrate variable performance in population subgroups. An unfair model is of particular concern in bladder cancer, where disparities have been identified in sex and racial subgroups. OBJECTIVE: This study aims (1) to develop a ML model to predict survival after radical cystectomy for bladder cancer and evaluate for potential model bias in sex and racial subgroups; and (2) to compare algorithm unfairness mitigation techniques to improve model fairness. METHODS: We trained and compared various ML classification algorithms to predict 5-year survival after radical cystectomy using the National Cancer Database. The primary model performance metric was the F1-score. The primary metric for model fairness was the equalized odds ratio (eOR). We compared 3 algorithm unfairness mitigation techniques to improve eOR. RESULTS: We identified 16,481 patients; 23.1% (n=3800) were female, and 91.5% (n=15,080) were \"White,\" 5% (n=832) were \"Black,\" 2.3% (n=373) were \"Hispanic,\" and 1.2% (n=196) were \"Asian.\" The 5-year mortality rate was 75% (n=12,290). The best naive model was extreme gradient boosting (XGBoost), which had an F1-score of 0.860 and eOR of 0.619. All unfairness mitigation techniques increased the eOR, with correlation remover showing the highest increase and resulting in a final eOR of 0.750. This mitigated model had F1-scores of 0.86, 0.904, and 0.824 in the full, Black male, and Asian female test sets, respectively. CONCLUSIONS: The ML model predicting survival after radical cystectomy exhibited bias across sex and racial subgroups. By using algorithm unfairness mitigation techniques, we improved algorithmic fairness as measured by the eOR. Our study highlights the role of not only evaluating for model bias but also actively mitigating such disparities to ensure equitable health care delivery. We also deployed the first web-based fair ML model for predicting survival after radical cystectomy.",
      "journal": "JMIR medical informatics",
      "year": "2024",
      "doi": "10.2196/63289",
      "authors": "Carbunaru Samuel et al.",
      "keywords": "algorithmic fairness; bias; bladder cancer; fairness; health equity; healthcare disparities; machine learning; model; mortality rate; prediction; radical cystectomy; survival",
      "mesh_terms": "Humans; Urinary Bladder Neoplasms; Cystectomy; Machine Learning; Female; Male; Aged; Middle Aged; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39671594/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "XGBoost/Gradient Boosting; Clinical Prediction Model",
      "health_domain": "Oncology; ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSIONS: The ML model predicting survival after radical cystectomy exhibited bias across sex and racial subgroups. By using algorithm unfairness mitigation techniques, we improved algorithmic fairness as measured by the eOR. Our study highlights the role of not only evaluating for model bias but also actively mitigating such disparities to ensure equitable health care delivery.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11694706"
    },
    {
      "pmid": "39677419",
      "title": "Not the Models You Are Looking For: Traditional ML Outperforms LLMs in Clinical Prediction Tasks.",
      "abstract": "OBJECTIVES: To determine the extent to which current Large Language Models (LLMs) can serve as substitutes for traditional machine learning (ML) as clinical predictors using data from electronic health records (EHRs), we investigated various factors that can impact their adoption, including overall performance, calibration, fairness, and resilience to privacy protections that reduce data fidelity. MATERIALS AND METHODS: We evaluated GPT-3.5, GPT-4, and ML (as gradient-boosting trees) on clinical prediction tasks in EHR data from Vanderbilt University Medical Center and MIMIC IV. We measured predictive performance with AUROC and model calibration using Brier Score. To evaluate the impact of data privacy protections, we assessed AUROC when demographic variables are generalized. We evaluated algorithmic fairness using equalized odds and statistical parity across race, sex, and age of patients. We also considered the impact of using in-context learning by incorporating labeled examples within the prompt. RESULTS: Traditional ML (AUROC: 0.847, 0.894 (VUMC, MIMIC)) substantially outperformed GPT-3.5 (AUROC: 0.537, 0.517) and GPT-4 (AUROC: 0.629, 0.602) (with and without in-context learning) in predictive performance and output probability calibration (Brier Score (ML vs GPT-3.5 vs GPT-4): 0.134 versus 0.384 versus 0.251, 0.042 versus 0.06 versus 0.219). Traditional ML is more robust than GPT-3.5 and GPT-4 to generalizing demographic information to protect privacy. GPT-4 is the fairest model according to our selected metrics but at the cost of poor model performance. CONCLUSION: These findings suggest that LLMs are much less effective and robust than locally-trained ML for clinical prediction tasks, but they are getting better over time.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2024",
      "doi": "10.1101/2024.12.03.24318400",
      "authors": "Brown Katherine E et al.",
      "keywords": "Large language models; clinical prediction models; fairness; privacy",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39677419/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM; Clinical Prediction Model",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: These findings suggest that LLMs are much less effective and robust than locally-trained ML for clinical prediction tasks, but they are getting better over time.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11643212"
    },
    {
      "pmid": "39682584",
      "title": "Mitigating Algorithmic Bias in AI-Driven Cardiovascular Imaging for Fairer Diagnostics.",
      "abstract": "Background/Objectives: The research addresses algorithmic bias in deep learning models for cardiovascular risk prediction, focusing on fairness across demographic and socioeconomic groups to mitigate health disparities. It integrates fairness-aware algorithms, susceptible carrier-infected-recovered (SCIR) models, and interpretability frameworks to combine fairness with actionable AI insights supported by robust segmentation and classification metrics. Methods: The research utilised quantitative 3D/4D heart magnetic resonance imaging and tabular datasets from the Cardiac Atlas Project's (CAP) open challenges to explore AI-driven methodologies for mitigating algorithmic bias in cardiac imaging. The SCIR model, known for its robustness, was adapted with the Capuchin algorithm, adversarial debiasing, Fairlearn, and post-processing with equalised odds. The robustness of the SCIR model was further demonstrated in the fairness evaluation metrics, which included demographic parity, equal opportunity difference (0.037), equalised odds difference (0.026), disparate impact (1.081), and Theil Index (0.249). For interpretability, YOLOv5, Mask R-CNN, and ResNet18 were implemented with LIME and SHAP. Bias mitigation improved disparate impact (0.80 to 0.95), reduced equal opportunity difference (0.20 to 0.05), and decreased false favourable rates for males (0.0059 to 0.0033) and females (0.0096 to 0.0064) through balanced probability adjustment. Results: The SCIR model outperformed the SIR model (recovery rate: 1.38 vs 0.83) with a -10% transmission bias impact. Parameters (\u03b2=0.5, \u03b4=0.2, \u03b3=0.15) reduced susceptible counts to 2.53\u00d710-12 and increased recovered counts to 9.98 by t=50. YOLOv5 achieved high Intersection over Union (IoU) scores (94.8%, 93.7%, 80.6% for normal, severe, and abnormal cases). Mask R-CNN showed 82.5% peak confidence, while ResNet demonstrated a 10.4% accuracy drop under noise. Performance metrics (IoU: 0.91-0.96, Dice: 0.941-0.980, Kappa: 0.95) highlighted strong predictive accuracy and reliability. Conclusions: The findings validate the effectiveness of fairness-aware algorithms in addressing cardiovascular predictive model biases. The integration of fairness and explainable AI not only promotes equitable diagnostic precision but also significantly reduces diagnostic disparities across vulnerable populations. This reduction in disparities is a key outcome of the research, enhancing clinical trust in AI-driven systems. The promising results of this study pave the way for future work that will explore scalability in real-world clinical settings and address limitations such as computational complexity in large-scale data processing.",
      "journal": "Diagnostics (Basel, Switzerland)",
      "year": "2024",
      "doi": "10.3390/diagnostics14232675",
      "authors": "Sufian Md Abu et al.",
      "keywords": "LIME; Mask R-CNN; ResNet-18; SCIR model; SHAP; YOLOv5; adversarial debiasing; algorithmic bias; cardiovascular risk prediction; demographic fairness; fairness-aware AI; predictive analytics",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39682584/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Clinical Prediction Model",
      "health_domain": "Cardiology",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Adversarial Debiasing; Fairness Metrics Evaluation; Explainability/Interpretability; Post-hoc Correction",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Conclusions: The findings validate the effectiveness of fairness-aware algorithms in addressing cardiovascular predictive model biases. The integration of fairness and explainable AI not only promotes equitable diagnostic precision but also significantly reduces diagnostic disparities across vulnerable populations. This reduction in disparities is a key outcome of the research, enhancing clinical trust in AI-driven systems.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11640708"
    },
    {
      "pmid": "39688772",
      "title": "A Bias Network Approach (BNA) to Encourage Ethical Reflection Among AI Developers.",
      "abstract": "We introduce the Bias Network Approach (BNA) as a sociotechnical method for AI developers to identify, map, and relate biases across the AI development process. This approach addresses the limitations of what we call the \"isolationist approach to AI bias,\" a trend in AI literature where biases are seen as separate occurrences linked to specific stages in an AI pipeline. Dealing with these multiple biases can trigger a sense of excessive overload in managing each potential bias individually or promote the adoption of an uncritical approach to understanding the influence of biases in developers' decision-making. The BNA fosters dialogue and a critical stance among developers, guided by external experts, using graphical representations to depict biased connections. To test the BNA, we conducted a pilot case study on the \"waiting list\" project, involving a small AI developer team creating a healthcare waiting list NPL model in Chile. The analysis showed promising findings: (i) the BNA aids in visualizing interconnected biases and their impacts, facilitating ethical reflection in a more accessible way; (ii) it promotes transparency in decision-making throughout AI development; and (iii) more focus is necessary on professional biases and material limitations as sources of bias in AI development.",
      "journal": "Science and engineering ethics",
      "year": "2024",
      "doi": "10.1007/s11948-024-00526-9",
      "authors": "Arriagada-Bruneau Gabriela et al.",
      "keywords": "AI bias; AI ethics; Decision-making; Professional bias; Sociotechnical",
      "mesh_terms": "Humans; Decision Making; Bias; Artificial Intelligence; Pilot Projects; Chile",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39688772/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "findings: (i) the BNA aids in visualizing interconnected biases and their impacts, facilitating ethical reflection in a more accessible way; (ii) it promotes transparency in decision-making throughout AI development; and (iii) more focus is necessary on professional biases and material limitations as sources of bias in AI development.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11652403"
    },
    {
      "pmid": "39716098",
      "title": "A comprehensive and bias-free machine learning approach for risk prediction of preeclampsia with severe features in a nulliparous study cohort.",
      "abstract": "Preeclampsia is one of the leading causes of maternal morbidity, with consequences during and after pregnancy. Because of its diverse clinical presentation, preeclampsia is an adverse pregnancy outcome that is uniquely challenging to predict and manage. In this paper, we developed racial bias-free machine learning models that predict the onset of preeclampsia with severe features or eclampsia at discrete time points in a nulliparous pregnant study cohort. To focus on those most at risk, we selected probands with severe PE (sPE). Those with mild preeclampsia, superimposed preeclampsia, and new onset hypertension were excluded.The prospective study cohort to which we applied machine learning is the Nulliparous Pregnancy Outcomes Study: Monitoring Mothers-to-be (nuMoM2b) study, which contains information from eight clinical sites across the US. Maternal serum samples were collected for 1,857 individuals between the first and second trimesters. These patients with serum samples collected are selected as the final cohort.Our prediction models achieved an AUROC of 0.72 (95% CI, 0.69-0.76), 0.75 (95% CI, 0.71-0.79), and 0.77 (95% CI, 0.74-0.80), respectively, for the three visits. Our initial models were biased toward non-Hispanic black participants with a high predictive equality ratio of 1.31. We corrected this bias and reduced this ratio to 1.14. This lowers the rate of false positives in our predictive model for the non-Hispanic black participants. The exact cause of the bias is still under investigation, but previous studies have recognized PLGF as a potential bias-inducing factor. However, since our model includes various factors that exhibit a positive correlation with PLGF, such as blood pressure measurements and BMI, we have employed an algorithmic approach to disentangle this bias from the model.The top features of our built model stress the importance of using several tests, particularly for biomarkers (BMI and blood pressure measurements) and ultrasound measurements. Placental analytes (PLGF and Endoglin) were strong predictors for screening for the early onset of preeclampsia with severe features in the first two trimesters.",
      "journal": "BMC pregnancy and childbirth",
      "year": "2024",
      "doi": "10.1186/s12884-024-06988-w",
      "authors": "Lin Yun C et al.",
      "keywords": "Ensemble\u00a0model; Fairness in machine learning; Machine learning; PlGF; Preeclampsia; Preeclampsia with severe features",
      "mesh_terms": "Humans; Pregnancy; Female; Pre-Eclampsia; Machine Learning; Adult; Prospective Studies; Parity; Risk Assessment; Placenta Growth Factor; Risk Factors; Biomarkers; Severity of Illness Index; Cohort Studies; Pregnancy Trimester, First; Pregnancy Trimester, Second",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39716098/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care; Obstetrics/Maternal Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Representation Learning",
      "clinical_setting": "ICU",
      "key_findings": "However, since our model includes various factors that exhibit a positive correlation with PLGF, such as blood pressure measurements and BMI, we have employed an algorithmic approach to disentangle this bias from the model.The top features of our built model stress the importance of using several tests, particularly for biomarkers (BMI and blood pressure measurements) and ultrasound measurements. Placental analytes (PLGF and Endoglin) were strong predictors for screening for the early onset of p...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11667971"
    },
    {
      "pmid": "39731446",
      "title": "De-biasing the bias: methods for improving disparity assessments with noisy group measurements.",
      "abstract": "Health care decisions are increasingly informed by clinical decision support algorithms, but these algorithms may perpetuate or increase racial and ethnic disparities in access to and quality of health care. Further complicating the problem, clinical data often have missing or poor quality racial and ethnic information, which can lead to misleading assessments of algorithmic bias. We present novel statistical methods that allow for the use of probabilities of racial/ethnic group membership in assessments of algorithm performance and quantify the statistical bias that results from error in these imputed group probabilities. We propose a sensitivity analysis approach to estimating the statistical bias that allows practitioners to assess disparities in algorithm performance under a range of assumed levels of group probability error. We also prove theoretical bounds on the statistical bias for a set of commonly used fairness metrics and describe real-world scenarios where our theoretical results are likely to apply. We present a case study using imputed race and ethnicity from the modified Bayesian Improved First and Surname Geocoding algorithm for estimation of disparities in a clinical decision support algorithm used to inform osteoporosis treatment. Our novel methods allow policymakers to understand the range of potential disparities under a given algorithm even when race and ethnicity information is missing and to make informed decisions regarding the implementation of machine learning for clinical decision support.",
      "journal": "Biometrics",
      "year": "2024",
      "doi": "10.1093/biomtc/ujae155",
      "authors": "Wastvedt Solvejg et al.",
      "keywords": "Bayesian improved surname geocoding; algorithmic fairness; race imputation; sensitivity analysis",
      "mesh_terms": "Humans; Algorithms; Bias; Bayes Theorem; Healthcare Disparities; Ethnicity; Osteoporosis; Racial Groups; Decision Support Systems, Clinical; Biometry; Models, Statistical",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39731446/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Decision Support",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "We present a case study using imputed race and ethnicity from the modified Bayesian Improved First and Surname Geocoding algorithm for estimation of disparities in a clinical decision support algorithm used to inform osteoporosis treatment. Our novel methods allow policymakers to understand the range of potential disparities under a given algorithm even when race and ethnicity information is missing and to make informed decisions regarding the implementation of machine learning for clinical deci...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39732655",
      "title": "Bias in machine learning applications to address non-communicable diseases at a population-level: a scoping review.",
      "abstract": "BACKGROUND: Machine learning (ML) is increasingly used in population and public health to support epidemiological studies, surveillance, and evaluation. Our objective was to conduct a scoping review to identify studies that use ML in population health, with a focus on its use in non-communicable diseases (NCDs). We also examine potential algorithmic biases in model design, training, and implementation, as well as efforts to mitigate these biases. METHODS: We searched the peer-reviewed, indexed literature using Medline, Embase, Cochrane Central Register of Controlled Trials and Cochrane Database of Systematic Reviews, CINAHL, Scopus, ACM Digital Library, Inspec, Web of Science's Science Citation Index, Social Sciences Citation Index, and the Emerging Sources Citation Index, up to March 2022. RESULTS: The search identified 27 310 studies and 65 were included. Study aims were separated into algorithm comparison (n\u2009=\u200913, 20%) or disease modelling for population-health-related outputs (n\u2009=\u200952, 80%). We extracted data on NCD type, data sources, technical approach, possible algorithmic bias, and jurisdiction. Type 2 diabetes was the most studied NCD. The most common use of ML was for risk modeling. Mitigating bias was not extensively addressed, with most methods focused on mitigating sex-related bias. CONCLUSION: This review examines current applications of ML in NCDs, highlighting potential biases and strategies for mitigation. Future research should focus on communicable diseases and the transferability of ML models in low and middle-income settings. Our findings can guide the development of guidelines for the equitable use of ML to improve population health outcomes.",
      "journal": "BMC public health",
      "year": "2024",
      "doi": "10.1186/s12889-024-21081-9",
      "authors": "Birdi Sharon et al.",
      "keywords": "Artificial intelligence; Machine learning; Non-communicable disease; Population health",
      "mesh_terms": "Humans; Noncommunicable Diseases; Machine Learning; Bias; Population Health; Algorithms",
      "pub_types": "Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39732655/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health; Endocrinology/Diabetes",
      "bias_axes": "Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: This review examines current applications of ML in NCDs, highlighting potential biases and strategies for mitigation. Future research should focus on communicable diseases and the transferability of ML models in low and middle-income settings. Our findings can guide the development of guidelines for the equitable use of ML to improve population health outcomes.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11682638"
    },
    {
      "pmid": "39738228",
      "title": "Attention-guided convolutional network for bias-mitigated and interpretable oral lesion classification.",
      "abstract": "Accurate diagnosis of oral lesions, early indicators of oral cancer, is a complex clinical challenge. Recent advances in deep learning have demonstrated potential in supporting clinical decisions. This paper introduces a deep learning model for classifying oral lesions, focusing on accuracy, interpretability, and reducing dataset bias. The model integrates three components: (i) a Classification Stream, utilizing a CNN to categorize images into 16 lesion types (baseline model), (ii) a Guidance Stream, which aligns class activation maps with clinically relevant areas using ground truth segmentation masks (GAIN model), and (iii) an Anatomical Site Prediction Stream, improving interpretability by predicting lesion location (GAIN+ASP model). The development dataset comprised 2765 intra-oral digital images of 16 lesion types from 1079 patients seen at an oral pathology clinic between 1999 and 2021. The GAIN model demonstrated a 7.2% relative improvement in accuracy over the baseline for 16-class classification, with superior class-specific balanced accuracy and AUC scores. Additionally, the GAIN model enhanced lesion localization and improved the alignment between attention maps and ground truth. The proposed models also exhibited greater robustness against dataset bias, as shown in ablation studies.",
      "journal": "Scientific reports",
      "year": "2024",
      "doi": "10.1038/s41598-024-81724-0",
      "authors": "Patel Adeetya et al.",
      "keywords": "Bias mitigation; CNN; Guided attention inference network; Interpretability; Oral lesion diagnosis",
      "mesh_terms": "Humans; Mouth Neoplasms; Deep Learning; Neural Networks, Computer; Bias; Image Processing, Computer-Assisted",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39738228/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Oncology; Pathology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "Additionally, the GAIN model enhanced lesion localization and improved the alignment between attention maps and ground truth. The proposed models also exhibited greater robustness against dataset bias, as shown in ablation studies.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11685657"
    },
    {
      "pmid": "39738436",
      "title": "Assessing the documentation of publicly available medical image and signal datasets and their impact on bias using the BEAMRAD tool.",
      "abstract": "Medical datasets are vital for advancing Artificial Intelligence (AI) in healthcare. Yet biases in these datasets on which deep-learning models are trained can compromise reliability. This study investigates biases stemming from dataset-creation practices. Drawing on existing guidelines, we first developed a BEAMRAD tool to assess the documentation of public Magnetic Resonance Imaging (MRI); Color Fundus Photography (CFP), and Electrocardiogram (ECG) datasets. In doing so, we provide an overview of the biases that may emerge due to inadequate dataset documentation. Second, we examine the current state of documentation for public medical images and signal data. Our research reveals that there is substantial variance in the documentation of image and signal datasets, even though guidelines have been developed in medical imaging. This indicates that dataset documentation is subject to individual discretionary decisions. Furthermore, we find that aspects such as hardware and data acquisition details are commonly documented, while information regarding data annotation practices, annotation error quantification, or data limitations are not consistently reported. This risks having considerable implications for the abilities of data users to detect potential sources of bias through these respective aspects and develop reliable and robust models that can be adapted for clinical practice.",
      "journal": "Scientific reports",
      "year": "2024",
      "doi": "10.1038/s41598-024-83218-5",
      "authors": "Galanty Maria et al.",
      "keywords": "",
      "mesh_terms": "Humans; Magnetic Resonance Imaging; Documentation; Bias; Electrocardiography; Deep Learning; Artificial Intelligence; Diagnostic Imaging; Datasets as Topic; Reproducibility of Results",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39738436/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Cardiology; Ophthalmology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Furthermore, we find that aspects such as hardware and data acquisition details are commonly documented, while information regarding data annotation practices, annotation error quantification, or data limitations are not consistently reported. This risks having considerable implications for the abilities of data users to detect potential sources of bias through these respective aspects and develop reliable and robust models that can be adapted for clinical practice.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11686007"
    },
    {
      "pmid": "39803613",
      "title": "Validation, bias assessment, and optimization of the UNAFIED 2-year risk prediction model for undiagnosed atrial fibrillation using national electronic health data.",
      "abstract": "BACKGROUND: Prediction models for atrial fibrillation (AF) may enable earlier detection and guideline-directed treatment decisions. However, model bias may lead to inaccurate predictions and unintended consequences. OBJECTIVE: The purpose of this study was to validate, assess bias, and improve generalizability of \"UNAFIED-10,\" a 2-year, 10-variable predictive model of undiagnosed AF in a national data set (originally developed using the Indiana Network for Patient Care regional data). METHODS: UNAFIED-10 was validated and optimized using Optum de-identified electronic health record data set. AF diagnoses were recorded in the January 2018-December 2019 period (outcome period), with January 2016-December 2017 as the baseline period. Validation cohorts (patients with AF and non-AF controls, aged \u226540 years) comprised the full imbalanced and randomly sampled balanced data sets. Model performance and bias in patient subpopulations based on sex, insurance, race, and region were evaluated. RESULTS: Of the 6,058,657 eligible patients (mean age 60 \u00b1 12 years), 4.1% (n = 246,975) had their first AF diagnosis within the outcome period. The validated UNAFIED-10 model achieved a higher C-statistic (0.85 [95% confidence interval 0.85-0.86] vs 0.81 [0.80-0.81]) and sensitivity (86% vs 74%) but lower specificity (66% vs 74%) than the original UNAFIED-10 model. During retraining and optimization, the variables insurance, shock, and albumin were excluded to address bias and improve generalizability. This generated an 8-variable model (UNAFIED-8) with consistent performance. CONCLUSION: UNAFIED-10, developed using regional patient data, displayed consistent performance in a large national data set. UNAFIED-8 is more parsimonious and generalizable for using advanced analytics for AF detection. Future directions include validation on additional data sets.",
      "journal": "Heart rhythm O2",
      "year": "2024",
      "doi": "10.1016/j.hroo.2024.09.010",
      "authors": "Ateya Mohammad et al.",
      "keywords": "Atrial fibrillation; Electronic health record; Machine learning; Predictive model; Screening",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39803613/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Cardiology; EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Geographic; Insurance Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Subgroup Analysis",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: UNAFIED-10, developed using regional patient data, displayed consistent performance in a large national data set. UNAFIED-8 is more parsimonious and generalizable for using advanced analytics for AF detection. Future directions include validation on additional data sets.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11721729"
    },
    {
      "pmid": "39886186",
      "title": "From prediction to practice: mitigating bias and data shift in machine-learning models for chemotherapy-induced organ dysfunction across unseen cancers.",
      "abstract": "OBJECTIVES: Routine monitoring of renal and hepatic function during chemotherapy ensures that treatment-related organ damage has not occurred and clearance of subsequent treatment is not hindered; however, frequency and timing are not optimal. Model bias and data heterogeneity concerns have hampered the ability of machine learning (ML) to be deployed into clinical practice. This study aims to develop models that could support individualised decisions on the timing of renal and hepatic monitoring while exploring the effect of data shift on model performance. METHODS AND ANALYSIS: We used retrospective data from three UK hospitals to develop and validate ML models predicting unacceptable rises in creatinine/bilirubin post cycle 3 for patients undergoing treatment for the following cancers: breast, colorectal, lung, ovarian and diffuse large B-cell lymphoma. RESULTS: We extracted 3614 patients with no missing blood test data across cycles 1-6 of chemotherapy treatment. We improved on previous work by including predictions post cycle 3. Optimised for sensitivity, we achieve F2 scores of 0.7773 (bilirubin) and 0.6893 (creatinine) on unseen data. Performance is consistent on tumour types unseen during training (F2 bilirubin: 0.7423, F2 creatinine: 0.6820). CONCLUSION: Our technique highlights the effectiveness of ML in clinical settings, demonstrating the potential to improve the delivery of care. Notably, our ML models can generalise to unseen tumour types. We propose gold-standard bias mitigation steps for ML models: evaluation on multisite data, thorough patient population analysis, and both formalised bias measures and model performance comparisons on patient subgroups. We demonstrate that data aggregation techniques have unintended consequences on model bias.",
      "journal": "BMJ oncology",
      "year": "2024",
      "doi": "10.1136/bmjonc-2024-000430",
      "authors": "Watson Matthew et al.",
      "keywords": "Adverse effects; Chemotherapy",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39886186/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Generative AI",
      "health_domain": "Oncology; Nephrology; Pulmonology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; Public Health/Population",
      "key_findings": "CONCLUSION: Our technique highlights the effectiveness of ML in clinical settings, demonstrating the potential to improve the delivery of care. Notably, our ML models can generalise to unseen tumour types. We propose gold-standard bias mitigation steps for ML models: evaluation on multisite data, thorough patient population analysis, and both formalised bias measures and model performance comparisons on patient subgroups.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11557724"
    },
    {
      "pmid": "41397397",
      "title": "Counterfactual fairness for small subgroups.",
      "abstract": "While methods for measuring and correcting differential performance in risk prediction models have proliferated in recent years, most existing techniques can only be used to assess fairness across relatively large subgroups. The purpose of algorithmic fairness efforts is often to redress discrimination against groups that are both marginalized and small, so this sample size limitation can prevent existing techniques from accomplishing their main aim. In clinical applications, this challenge combines with statistical issues that arise when models are used to guide treatment. We take a 3-step approach to addressing both of these challenges, building on the \"counterfactual fairness\" framework that accounts for confounding by treatment. First, we propose new estimands that leverage information across groups. Second, we estimate these quantities using a larger volume of data than existing techniques. Finally, we propose a novel data borrowing approach to incorporate \"external data\" that lacks outcomes and predictions but contains covariate and group membership information. We demonstrate application of our estimators to a risk prediction model used by a major Midwestern health system during the coronavirus disease 2019 (COVID-19) pandemic.",
      "journal": "Biostatistics (Oxford, England)",
      "year": "2024",
      "doi": "10.1093/biostatistics/kxaf046",
      "authors": "Wastvedt Solvejg et al.",
      "keywords": "algorithmic fairness; causal inference; risk prediction; small subgroups",
      "mesh_terms": "Humans; COVID-19; Models, Statistical; Risk Assessment; Sample Size; SARS-CoV-2",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41397397/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Pulmonology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Counterfactual Fairness",
      "clinical_setting": "Not specified",
      "key_findings": "Finally, we propose a novel data borrowing approach to incorporate \"external data\" that lacks outcomes and predictions but contains covariate and group membership information. We demonstrate application of our estimators to a risk prediction model used by a major Midwestern health system during the coronavirus disease 2019 (COVID-19) pandemic.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "35802380",
      "title": "A new model for categorizing cognitive biases and debiasing strategies in dermatology.",
      "abstract": "Cognitive biases are a significant cause of medical error. They arise from \"system 1\" thinking, which depends on heuristics to make quick decisions in complex situations. Heuristics make us \"predictably irrational,\" distorting our ability to accurately assess probabilities in clinical scenarios. It is well reported in the literature that metacognition, the art of reflecting on one's thought processes, is the optimal way to deal with cognitive biases. However, it is unclear how this can be consistently implemented in dermatological practice. Our debiasing attempts thus far have been sporadic at best. This article categorizes important cognitive biases according to each stage of the doctor-patient interaction (history taking, clinical examination, investigations, diagnosis, and management). We hope that providing this clinically relevant framework can foster metacognition and a platform for algorithmic debiasing. This will enable us to engage \"system 2\" (analytical thinking) in a targeted way, thereby avoiding excessive cognitive load. Organization-level interventions should also be implemented to free up the cognitive capacity of an individual and to enable them to employ system 2 thinking more regularly.",
      "journal": "International journal of dermatology",
      "year": "2023",
      "doi": "10.1111/ijd.16348",
      "authors": "Yesudian Rohan I et al.",
      "keywords": "",
      "mesh_terms": "Humans; Dermatology; Diagnostic Errors; Bias; Cognition",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35802380/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Generative AI",
      "health_domain": "Dermatology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "This will enable us to engage \"system 2\" (analytical thinking) in a targeted way, thereby avoiding excessive cognitive load. Organization-level interventions should also be implemented to free up the cognitive capacity of an individual and to enable them to employ system 2 thinking more regularly.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36155458",
      "title": "D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias.",
      "abstract": "With the rise of AI, algorithms have become better at learning underlying patterns from the training data including ingrained social biases based on gender, race, etc. Deployment of such algorithms to domains such as hiring, healthcare, law enforcement, etc. has raised serious concerns about fairness, accountability, trust and interpretability in machine learning algorithms. To alleviate this problem, we propose D-BIAS, a visual interactive tool that embodies human-in-the-loop AI approach for auditing and mitigating social biases from tabular datasets. It uses a graphical causal model to represent causal relationships among different features in the dataset and as a medium to inject domain knowledge. A user can detect the presence of bias against a group, say females, or a subgroup, say black females, by identifying unfair causal relationships in the causal network and using an array of fairness metrics. Thereafter, the user can mitigate bias by refining the causal model and acting on the unfair causal edges. For each interaction, say weakening/deleting a biased causal edge, the system uses a novel method to simulate a new (debiased) dataset based on the current causal model while ensuring a minimal change from the original dataset. Users can visually assess the impact of their interactions on different fairness metrics, utility metrics, data distortion, and the underlying data distribution. Once satisfied, they can download the debiased dataset and use it for any downstream application for fairer predictions. We evaluate D-BIAS by conducting experiments on 3 datasets and also a formal user study. We found that D-BIAS helps reduce bias significantly compared to the baseline debiasing approach across different fairness metrics while incurring little data distortion and a small loss in utility. Moreover, our human-in-the-loop based approach significantly outperforms an automated approach on trust, interpretability and accountability.",
      "journal": "IEEE transactions on visualization and computer graphics",
      "year": "2023",
      "doi": "10.1109/TVCG.2022.3209484",
      "authors": "Ghai Bhavya et al.",
      "keywords": "",
      "mesh_terms": "Female; Humans; Computer Graphics; Causality; Algorithms; Machine Learning; Bias",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36155458/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability; Bias Auditing Framework",
      "clinical_setting": "Not specified",
      "key_findings": "We found that D-BIAS helps reduce bias significantly compared to the baseline debiasing approach across different fairness metrics while incurring little data distortion and a small loss in utility. Moreover, our human-in-the-loop based approach significantly outperforms an automated approach on trust, interpretability and accountability.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36397723",
      "title": "Reporting and risk of bias of prediction models based on machine learning methods in preterm birth: A systematic review.",
      "abstract": "INTRODUCTION: There was limited evidence on the quality of reporting and methodological quality of prediction models using machine learning methods in preterm birth. This systematic review aimed to assess the reporting quality and risk of bias of a machine learning-based prediction model in preterm birth. MATERIAL AND METHODS: We conducted a systematic review, searching the PubMed, Embase, the Cochrane Library, China National Knowledge Infrastructure, China Biology Medicine disk, VIP Database, and WanFang Data from inception to September 27, 2021. Studies that developed (validated) a prediction model using machine learning methods in preterm birth were included. We used the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) statement and Prediction model Risk of Bias Assessment Tool (PROBAST) to evaluate the reporting quality and the risk of bias of included studies, respectively. Findings were summarized using descriptive statistics and visual plots. The protocol was registered in PROSPERO (no. CRD 42022301623). RESULTS: Twenty-nine studies met the inclusion criteria, with 24 development-only studies and 5 development-with-validation studies. Overall, TRIPOD adherence per study ranged from 17% to 79%, with a median adherence of 49%. The reporting of title, abstract, blinding of predictors, sample size justification, explanation of model, and model performance were mostly poor, with TRIPOD adherence ranging from 4% to 17%. For all included studies, 79% had a high overall risk of bias, and 21% had an unclear overall risk of bias. The analysis domain was most commonly rated as high risk of bias in included studies, mainly as a result of small effective sample size, selection of predictors based on univariable analysis, and lack of calibration evaluation. CONCLUSIONS: Reporting and methodological quality of machine learning-based prediction models in preterm birth were poor. It is urgent to improve the design, conduct, and reporting of such studies to boost the application of machine learning-based prediction models in preterm birth in clinical practice.",
      "journal": "Acta obstetricia et gynecologica Scandinavica",
      "year": "2023",
      "doi": "10.1111/aogs.14475",
      "authors": "Yang Qiuyu et al.",
      "keywords": "machine learning; prediction model; preterm birth; quality of reporting; risk of bias; systematic review",
      "mesh_terms": "Infant, Newborn; Female; Humans; Premature Birth; Prognosis; Research Design; Machine Learning; China; Bias",
      "pub_types": "Systematic Review; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36397723/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Deep Learning; Clinical Prediction Model",
      "health_domain": "Pediatrics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Calibration",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Reporting and methodological quality of machine learning-based prediction models in preterm birth were poor. It is urgent to improve the design, conduct, and reporting of such studies to boost the application of machine learning-based prediction models in preterm birth in clinical practice.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9780725"
    },
    {
      "pmid": "36610413",
      "title": "Optimizing Equity: Working towards Fair Machine Learning Algorithms in Laboratory Medicine.",
      "abstract": "BACKGROUND: Methods of machine learning provide opportunities to use real-world data to solve complex problems. Applications of these methods in laboratory medicine promise to increase diagnostic accuracy and streamline laboratory operations leading to improvement in the quality and efficiency of healthcare delivery. However, machine learning models are vulnerable to learning from undesirable patterns in the data that reflect societal biases. As a result, irresponsible application of machine learning may lead to the perpetuation, or even amplification, of existing disparities in healthcare outcomes. CONTENT: In this work, we review what it means for a model to be unfair, discuss the various ways that machine learning models become unfair, and present engineering principles emerging from the field of algorithmic fairness. These materials are presented with a focus on the development of machine learning models in laboratory medicine. SUMMARY: We hope that this work will serve to increase awareness, and stimulate further discussion, of this important issue among laboratorians as the field moves forward with the incorporation of machine learning models into laboratory practice.",
      "journal": "The journal of applied laboratory medicine",
      "year": "2023",
      "doi": "10.1093/jalm/jfac085",
      "authors": "Azimi Vahid et al.",
      "keywords": "",
      "mesh_terms": "Humans; Machine Learning; Delivery of Health Care; Algorithms; Laboratories; Bias",
      "pub_types": "Review; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36610413/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "These materials are presented with a focus on the development of machine learning models in laboratory medicine. SUMMARY: We hope that this work will serve to increase awareness, and stimulate further discussion, of this important issue among laboratorians as the field moves forward with the incorporation of machine learning models into laboratory practice.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36633483",
      "title": "Difficult diagnosis in the ICU: making the right call but beware uncertainty and bias.",
      "abstract": "Dealing with an uncertain or missed diagnosis is commonplace in the intensive care unit setting. Affected patients are subject to a potential decrease in quality of care and a greater risk of a poor outcome. The diagnostic process is a complex task that starts with information gathering, followed by integration and interpretation of data, hypothesis generation and, finally, confirmation of a (hopefully correct) diagnosis. This may be particularly challenging in the patient who is critically ill where a good history may not be forthcoming and/or clinical, laboratory and imaging features are non-specific. The aim of this narrative review is to analyse and describe common causes of diagnostic error in the intensive care unit, highlighting the multiple types of cognitive bias, and to suggest a diagnostic framework. To inform this review, we performed a literature search to identify relevant articles, particularly those pertinent to unclear diagnoses in patients who are critically ill. Clinicians should be cognisant as to how they formulate diagnoses and utilise debiasing strategies. Multidisciplinary teamwork and more time spent with the patient, supported by effective and efficient use of electronic healthcare records and decision support resources, is likely to improve the quality of the diagnostic process, patient care and outcomes.",
      "journal": "Anaesthesia",
      "year": "2023",
      "doi": "10.1111/anae.15897",
      "authors": "Pisciotta W et al.",
      "keywords": "cognitive bias; intensive care unit; misdiagnosis; multidisciplinary team; unclear diagnosis",
      "mesh_terms": "Humans; Critical Illness; Uncertainty; Intensive Care Units",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36633483/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Laboratory/Pathology",
      "key_findings": "Clinicians should be cognisant as to how they formulate diagnoses and utilise debiasing strategies. Multidisciplinary teamwork and more time spent with the patient, supported by effective and efficient use of electronic healthcare records and decision support resources, is likely to improve the quality of the diagnostic process, patient care and outcomes.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36633513",
      "title": "SeroTracker-RoB: A decision rule-based algorithm for reproducible risk of bias assessment of seroprevalence studies.",
      "abstract": "Risk of bias (RoB) assessments are a core element of evidence synthesis but can be time consuming and subjective. We aimed to develop a decision rule-based algorithm for RoB assessment of seroprevalence studies. We developed the SeroTracker-RoB algorithm. The algorithm derives seven objective and two subjective critical appraisal items from the Joanna Briggs Institute Critical Appraisal Checklist for Prevalence studies and implements decision rules that determine study risk of bias based on the items. Decision rules were validated using the SeroTracker seroprevalence study database, which included non-algorithmic RoB judgments from two reviewers. We quantified efficiency as the mean difference in time for the algorithmic and non-algorithmic assessments of 80 randomly selected articles, coverage as the proportion of studies where the decision rules yielded an assessment, and reliability using intraclass correlations comparing algorithmic and non-algorithmic assessments for 2070 articles. A set of decision rules with 61 branches was developed using responses to the nine critical appraisal items. The algorithmic approach was faster than non-algorithmic assessment (mean reduction 2.32\u2009min [SD 1.09] per article), classified 100% (n\u00a0=\u20092070) of studies, and had good reliability compared to non-algorithmic assessment (ICC 0.77, 95% CI 0.74-0.80). We built the SeroTracker-RoB Excel Tool, which embeds this algorithm for use by other researchers. The SeroTracker-RoB decision-rule based algorithm was faster than non-algorithmic assessment with complete coverage and good reliability. This algorithm enabled rapid, transparent, and reproducible RoB evaluations of seroprevalence studies and may support evidence synthesis efforts during future disease outbreaks. This decision rule-based approach could be applied to other types of prevalence studies.",
      "journal": "Research synthesis methods",
      "year": "2023",
      "doi": "10.1002/jrsm.1620",
      "authors": "Bobrovitz Niklas et al.",
      "keywords": "algorithm; critical appraisal; decision rule; evidence synthesis; infectious disease; prevalence; risk of bias",
      "mesh_terms": "Reproducibility of Results; Seroepidemiologic Studies; Research Design; Bias; Risk Assessment",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36633513/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "This algorithm enabled rapid, transparent, and reproducible RoB evaluations of seroprevalence studies and may support evidence synthesis efforts during future disease outbreaks. This decision rule-based approach could be applied to other types of prevalence studies.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36647369",
      "title": "Gender and sex bias in COVID-19 epidemiological data through the lens of causality.",
      "abstract": "The COVID-19 pandemic has spurred a large amount of experimental and observational studies reporting clear correlation between the risk of developing severe COVID-19 (or dying from it) and whether the individual is male or female. This paper is an attempt to explain the supposed male vulnerability to COVID-19 using a causal approach. We proceed by identifying a set of confounding and mediating factors, based on the review of epidemiological literature and analysis of sex-dis-aggregated data. Those factors are then taken into consideration to produce explainable and fair prediction and decision models from observational data. The paper outlines how non-causal models can motivate discriminatory policies such as biased allocation of the limited resources in intensive care units (ICUs). The objective is to anticipate and avoid disparate impact and discrimination, by considering causal knowledge and causal-based techniques to compliment the collection and analysis of observational big-data. The hope is to contribute to more careful use of health related information access systems for developing fair and robust predictive models.",
      "journal": "Information processing & management",
      "year": "2023",
      "doi": "10.1016/j.ipm.2023.103276",
      "authors": "D\u00edaz-Rodr\u00edguez Natalia et al.",
      "keywords": "Artificial intelligence; COVID-19; Causal fairness; Causality; Equality; Explainability; Gender; Healthcare; Sex",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36647369/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "ICU/Critical Care; Pulmonology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "The objective is to anticipate and avoid disparate impact and discrimination, by considering causal knowledge and causal-based techniques to compliment the collection and analysis of observational big-data. The hope is to contribute to more careful use of health related information access systems for developing fair and robust predictive models.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9834203"
    },
    {
      "pmid": "36706849",
      "title": "Evaluating and mitigating bias in machine learning models for cardiovascular disease prediction.",
      "abstract": "OBJECTIVE: The study aims to investigate whether machine learning-based predictive models for cardiovascular disease (CVD) risk assessment show equivalent performance across demographic groups (such as race and gender) and if bias mitigation methods can reduce any bias present in the models. This is important as systematic bias may be introduced when collecting and preprocessing health data, which could affect the performance of the models on certain demographic sub-cohorts. The study is to investigate this using electronic health records data and various machine learning models. METHODS: The study used large de-identified Electronic Health Records data from Vanderbilt University Medical Center. Machine learning (ML) algorithms including logistic regression, random forest, gradient-boosting trees, and long short-term memory were applied to build multiple predictive models. Model bias and fairness were evaluated using equal opportunity difference (EOD, 0 indicates fairness) and disparate impact (DI, 1 indicates fairness). In our study, we also evaluated the fairness of a non-ML baseline model, the American Heart Association (AHA) Pooled Cohort Risk Equations (PCEs). Moreover, we compared the performance of three different de-biasing methods: removing protected attributes (e.g., race and gender), resampling the imbalanced training dataset by sample size, and resampling by the proportion of people with CVD outcomes. RESULTS: The study cohort included 109,490 individuals (mean [SD] age 47.4 [14.7] years; 64.5% female; 86.3% White; 13.7% Black). The experimental results suggested that most ML models had smaller EOD and DI than PCEs. For ML models, the mean EOD ranged from -0.001 to 0.018 and the mean DI ranged from 1.037 to 1.094 across race groups. There was a larger EOD and DI across gender groups, with EOD ranging from 0.131 to 0.136 and DI ranging from 1.535 to 1.587. For debiasing methods, removing protected attributes didn't significantly reduced the bias for most ML models. Resampling by sample size also didn't consistently decrease bias. Resampling by case proportion reduced the EOD and DI for gender groups but slightly reduced accuracy in many cases. CONCLUSIONS: Among the VUMC cohort, both PCEs and ML models were biased against women, suggesting the need to investigate and correct gender disparities in CVD risk prediction. Resampling by proportion reduced the bias for gender groups but not for race groups.",
      "journal": "Journal of biomedical informatics",
      "year": "2023",
      "doi": "10.1016/j.jbi.2023.104294",
      "authors": "Li Fuchen et al.",
      "keywords": "Bias mitigation; Cardiovascular diseases; Clinical predictive models; Electronic health records; Fairness; Machine learning",
      "mesh_terms": "Humans; Female; Middle Aged; Male; Cardiovascular Diseases; Machine Learning; Algorithms; Random Forest; Logistic Models",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36706849/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Random Forest; Logistic Regression; Clinical Prediction Model",
      "health_domain": "Cardiology; EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Among the VUMC cohort, both PCEs and ML models were biased against women, suggesting the need to investigate and correct gender disparities in CVD risk prediction. Resampling by proportion reduced the bias for gender groups but not for race groups.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11104322"
    },
    {
      "pmid": "36716365",
      "title": "Bias in machine learning models can be significantly mitigated by careful training: Evidence from neuroimaging studies.",
      "abstract": "Despite the great promise that machine learning has offered in many fields of medicine, it has also raised concerns about potential biases and poor generalization across genders, age distributions, races and ethnicities, hospitals, and data acquisition equipment and protocols. In the current study, and in the context of three brain diseases, we provide evidence which suggests that when properly trained, machine learning models can generalize well across diverse conditions and do not necessarily suffer from bias. Specifically, by using multistudy magnetic resonance imaging consortia for diagnosing Alzheimer's disease, schizophrenia, and autism spectrum disorder, we find that well-trained models have a high area-under-the-curve (AUC) on subjects across different subgroups pertaining to attributes such as gender, age, racial groups and different clinical studies and are unbiased under multiple fairness metrics such as demographic parity difference, equalized odds difference, equal opportunity difference, etc. We find that models that incorporate multisource data from demographic, clinical, genetic factors, and cognitive scores are also unbiased. These models have a better predictive AUC across subgroups than those trained only with imaging features, but there are also situations when these additional features do not help.",
      "journal": "Proceedings of the National Academy of Sciences of the United States of America",
      "year": "2023",
      "doi": "10.1073/pnas.2211613120",
      "authors": "Wang Rongguang et al.",
      "keywords": "MRI; algorithmic bias; heterogeneity; machine learning; neuroscience",
      "mesh_terms": "Humans; Male; Female; Autism Spectrum Disorder; Neuroimaging; Machine Learning; Magnetic Resonance Imaging; Alzheimer Disease; Bias",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36716365/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Neurology; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "We find that models that incorporate multisource data from demographic, clinical, genetic factors, and cognitive scores are also unbiased. These models have a better predictive AUC across subgroups than those trained only with imaging features, but there are also situations when these additional features do not help.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9962919"
    },
    {
      "pmid": "36877519",
      "title": "Evaluation of Risk of Bias in Neuroimaging-Based Artificial Intelligence Models for Psychiatric Diagnosis: A Systematic Review.",
      "abstract": "IMPORTANCE: Neuroimaging-based artificial intelligence (AI) diagnostic models have proliferated in psychiatry. However, their clinical applicability and reporting quality (ie, feasibility) for clinical practice have not been systematically evaluated. OBJECTIVE: To systematically assess the risk of bias (ROB) and reporting quality of neuroimaging-based AI models for psychiatric diagnosis. EVIDENCE REVIEW: PubMed was searched for peer-reviewed, full-length articles published between January 1, 1990, and March 16, 2022. Studies aimed at developing or validating neuroimaging-based AI models for clinical diagnosis of psychiatric disorders were included. Reference lists were further searched for suitable original studies. Data extraction followed the CHARMS (Checklist for Critical Appraisal and Data Extraction for Systematic Reviews of Prediction Modeling Studies) and PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-analyses) guidelines. A closed-loop cross-sequential design was used for quality control. The PROBAST (Prediction Model Risk of Bias Assessment Tool) and modified CLEAR (Checklist for Evaluation of Image-Based Artificial Intelligence Reports) benchmarks were used to systematically evaluate ROB and reporting quality. FINDINGS: A total of 517 studies presenting 555 AI models were included and evaluated. Of these models, 461 (83.1%; 95% CI, 80.0%-86.2%) were rated as having a high overall ROB based on the PROBAST. The ROB was particular high in the analysis domain, including inadequate sample size (398 of 555 models [71.7%; 95% CI, 68.0%-75.6%]), poor model performance examination (with 100% of models lacking calibration examination), and lack of handling data complexity (550 of 555 models [99.1%; 95% CI, 98.3%-99.9%]). None of the AI models was perceived to be applicable to clinical practices. Overall reporting completeness (ie, number of reported items/number of total items) for the AI models was 61.2% (95% CI, 60.6%-61.8%), and the completeness was poorest for the technical assessment domain with 39.9% (95% CI, 38.8%-41.1%). CONCLUSIONS AND RELEVANCE: This systematic review found that the clinical applicability and feasibility of neuroimaging-based AI models for psychiatric diagnosis were challenged by a high ROB and poor reporting quality. Particularly in the analysis domain, ROB in AI diagnostic models should be addressed before clinical application.",
      "journal": "JAMA network open",
      "year": "2023",
      "doi": "10.1001/jamanetworkopen.2023.1671",
      "authors": "Chen Zhiyi et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Benchmarking; Bias; Calibration; Neuroimaging",
      "pub_types": "Systematic Review; Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36877519/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Mental Health/Psychiatry; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration",
      "clinical_setting": "ICU",
      "key_findings": "FINDINGS: A total of 517 studies presenting 555 AI models were included and evaluated. Of these models, 461 (83.1%; 95% CI, 80.0%-86.2%) were rated as having a high overall ROB based on the PROBAST. The ROB was particular high in the analysis domain, including inadequate sample size (398 of 555 models [71.7%; 95% CI, 68.0%-75.6%]), poor model performance examination (with 100% of models lacking calibration examination), and lack of handling data complexity (550 of 555 models [99.1%; 95% CI, 98.3...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9989906"
    },
    {
      "pmid": "36942183",
      "title": "Racial Equity in Healthcare Machine Learning: Illustrating Bias in Models With Minimal Bias Mitigation.",
      "abstract": "Background and objective While the potential of\u00a0machine learning (ML) in healthcare to positively impact human health\u00a0continues to grow, the potential for inequity in these methods must be assessed. In this study, we aimed to evaluate the presence of racial bias when five of the most common ML algorithms are used to create models with minimal processing to reduce racial bias. Methods By utilizing a CDC public database, we constructed models for the prediction of healthcare access (binary variable). Using area under the curve (AUC) as our performance metric, we calculated race-specific performance comparisons for each ML algorithm. We bootstrapped our entire analysis 20 times to produce confidence intervals\u00a0for our AUC performance metrics. Results With the exception of only a few cases, we found that the performance for the White group was, in general, significantly higher than that of the other racial groups across all ML algorithms. Additionally, we found that the most accurate algorithm in our modeling was Extreme Gradient Boosting (XGBoost) followed by random forest, naive Bayes, support vector machine (SVM), and k-nearest neighbors (KNN). Conclusion Our study illustrates the predictive perils of incorporating minimal racial bias mitigation in ML models, resulting in predictive disparities by race. This is particularly concerning in the setting of evidence for limited bias mitigation in healthcare-related ML. There needs to be more conversation, research, and guidelines surrounding methods for racial bias assessment and mitigation in healthcare-related ML models, both those currently used and those in development.",
      "journal": "Cureus",
      "year": "2023",
      "doi": "10.7759/cureus.35037",
      "authors": "Barton Michael et al.",
      "keywords": "data science; health equity; healthcare technology; machine learning; racial bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36942183/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Random Forest; XGBoost/Gradient Boosting; Support Vector Machine",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "This is particularly concerning in the setting of evidence for limited bias mitigation in healthcare-related ML. There needs to be more conversation, research, and guidelines surrounding methods for racial bias assessment and mitigation in healthcare-related ML models, both those currently used and those in development.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10023594"
    },
    {
      "pmid": "36991077",
      "title": "An adversarial training framework for mitigating algorithmic biases in clinical machine learning.",
      "abstract": "Machine learning is becoming increasingly prominent in healthcare. Although its benefits are clear, growing attention is being given to how these tools may exacerbate existing biases and disparities. In this study, we introduce an adversarial training framework that is capable of mitigating biases that may have been acquired through data collection. We demonstrate this proposed framework on the real-world task of rapidly predicting COVID-19, and focus on mitigating site-specific (hospital) and demographic (ethnicity) biases. Using the statistical definition of equalized odds, we show that adversarial training improves outcome fairness, while still achieving clinically-effective screening performances (negative predictive values >0.98). We compare our method to previous benchmarks, and perform prospective and external validation across four independent hospital cohorts. Our method can be generalized to any outcomes, models, and definitions of fairness.",
      "journal": "NPJ digital medicine",
      "year": "2023",
      "doi": "10.1038/s41746-023-00805-y",
      "authors": "Yang Jenny et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36991077/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Pulmonology",
      "bias_axes": "Race/Ethnicity",
      "lifecycle_stage": "Data Collection; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Adversarial Debiasing; Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "We compare our method to previous benchmarks, and perform prospective and external validation across four independent hospital cohorts. Our method can be generalized to any outcomes, models, and definitions of fairness.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10050816"
    },
    {
      "pmid": "36997642",
      "title": "Human visual explanations mitigate bias in AI-based assessment of surgeon skills.",
      "abstract": "Artificial intelligence (AI) systems can now reliably assess surgeon skills through videos of intraoperative surgical activity. With such systems informing future high-stakes decisions such as whether to credential surgeons and grant them the privilege to operate on patients, it is critical that they treat all surgeons fairly. However, it remains an open question whether surgical AI systems exhibit bias against surgeon sub-cohorts, and, if so, whether such bias can be mitigated. Here, we examine and mitigate the bias exhibited by a family of surgical AI systems-SAIS-deployed on videos of robotic surgeries from three geographically-diverse hospitals (USA and EU). We show that SAIS exhibits an underskilling bias, erroneously downgrading surgical performance, and an overskilling bias, erroneously upgrading surgical performance, at different rates across surgeon sub-cohorts. To mitigate such bias, we leverage a strategy -TWIX-which teaches an AI system to provide a visual explanation for its skill assessment that otherwise would have been provided by human experts. We show that whereas baseline strategies inconsistently mitigate algorithmic bias, TWIX can effectively mitigate the underskilling and overskilling bias while simultaneously improving the performance of these AI systems across hospitals. We discovered that these findings carry over to the training environment where we assess medical students' skills today. Our study is a critical prerequisite to the eventual implementation of AI-augmented global surgeon credentialing programs, ensuring that all surgeons are treated fairly.",
      "journal": "NPJ digital medicine",
      "year": "2023",
      "doi": "10.1038/s41746-023-00766-2",
      "authors": "Kiyasseh Dani et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36997642/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Surgery",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "We discovered that these findings carry over to the training environment where we assess medical students' skills today. Our study is a critical prerequisite to the eventual implementation of AI-augmented global surgeon credentialing programs, ensuring that all surgeons are treated fairly.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10063676"
    },
    {
      "pmid": "37046593",
      "title": "From Head and Neck Tumour and Lymph Node Segmentation to Survival Prediction on PET/CT: An End-to-End Framework Featuring Uncertainty, Fairness, and Multi-Region Multi-Modal Radiomics.",
      "abstract": "Automatic delineation and detection of the primary tumour (GTVp) and lymph nodes (GTVn) using PET and CT in head and neck cancer and recurrence-free survival prediction can be useful for diagnosis and patient risk stratification. We used data from nine different centres, with 524 and 359 cases used for training and testing, respectively. We utilised posterior sampling of the weight space in the proposed segmentation model to estimate the uncertainty for false positive reduction. We explored the prognostic potential of radiomics features extracted from the predicted GTVp and GTVn in PET and CT for recurrence-free survival prediction and used SHAP analysis for explainability. We evaluated the bias of models with respect to age, gender, chemotherapy, HPV status, and lesion size. We achieved an aggregate Dice score of 0.774 and 0.760 on the test set for GTVp and GTVn, respectively. We observed a per image false positive reduction of 19.5% and 7.14% using the uncertainty threshold for GTVp and GTVn, respectively. Radiomics features extracted from GTVn in PET and from both GTVp and GTVn in CT are the most prognostic, and our model achieves a C-index of 0.672 on the test set. Our framework incorporates uncertainty estimation, fairness, and explainability, demonstrating the potential for accurate detection and risk stratification.",
      "journal": "Cancers",
      "year": "2023",
      "doi": "10.3390/cancers15071932",
      "authors": "Salahuddin Zohaib et al.",
      "keywords": "CT radiomics; PET radiomics; explainability; fair artificial intelligence; head and neck cancer; segmentation; uncertainty estimation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37046593/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment; Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Radiomics features extracted from GTVn in PET and from both GTVp and GTVn in CT are the most prognostic, and our model achieves a C-index of 0.672 on the test set. Our framework incorporates uncertainty estimation, fairness, and explainability, demonstrating the potential for accurate detection and risk stratification.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10093277"
    },
    {
      "pmid": "37090627",
      "title": "A Comprehensive and Bias-Free Machine Learning Approach for Risk Prediction of Preeclampsia with Severe Features in a Nulliparous Study Cohort.",
      "abstract": "OBJECTIVE: Preeclampsia is one of the leading causes of maternal morbidity, with consequences during and after pregnancy. Because of its diverse clinical presentation, preeclampsia is an adverse pregnancy outcome that is uniquely challenging to predict and manage. In this paper, we developed machine learning models that predict the onset of preeclampsia with severe features or eclampsia at discrete time points in a nulliparous pregnant study cohort. MATERIALS AND METHODS: The prospective study cohort to which we applied machine learning is the Nulliparous Pregnancy Outcomes Study: Monitoring Mothers-to-be (nuMoM2b) study, which contains information from eight clinical sites across the US. Maternal serum samples were collected for 1,857 individuals between the first and second trimesters. These patients with serum samples collected are selected as the final cohort. RESULTS: Our prediction models achieved an AUROC of 0.72 (95% CI, 0.69-0.76), 0.75 (95% CI, 0.71-0.79), and 0.77 (95% CI, 0.74-0.80), respectively, for the three visits. Our initial models were biased toward non-Hispanic black participants with a high predictive equality ratio of 1.31. We corrected this bias and reduced this ratio to 1.14. The top features stress the importance of using several tests, particularly for biomarkers and ultrasound measurements. Placental analytes were strong predictors for screening for the early onset of preeclampsia with severe features in the first two trimesters. CONCLUSION: Experiments suggest that it is possible to create racial bias-free early screening models to predict the patients at risk of developing preeclampsia with severe features or eclampsia nulliparous pregnant study cohort.",
      "journal": "Research square",
      "year": "2023",
      "doi": "10.21203/rs.3.rs-2635419/v1",
      "authors": "Lin Yun et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37090627/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care; Obstetrics/Maternal Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: Experiments suggest that it is possible to create racial bias-free early screening models to predict the patients at risk of developing preeclampsia with severe features or eclampsia nulliparous pregnant study cohort.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10120773"
    },
    {
      "pmid": "37097792",
      "title": "Post-Processing Fairness Evaluation of Federated Models: An Unsupervised Approach in Healthcare.",
      "abstract": "Modern Healthcare cyberphysical systems have begun to rely more and more on distributed AI leveraging the power of Federated Learning (FL). Its ability to train Machine Learning (ML) and Deep Learning (DL) models for the wide variety of medical fields, while at the same time fortifying the privacy of the sensitive information that are present in the medical sector, makes the FL technology a necessary tool in modern health and medical systems. Unfortunately, due to the polymorphy of distributed data and the shortcomings of distributed learning, the local training of Federated models sometimes proves inadequate and thus negatively imposes the federated learning optimization process and in extend in the subsequent performance of the rest Federated models. Badly trained models can cause dire implications in the healthcare field due to their critical nature. This work strives to solve this problem by applying a post-processing pipeline to models used by FL. In particular, the proposed work ranks the model by finding how fair they are by discovering and inspecting micro-Manifolds that cluster each neural model's latent knowledge. The produced work applies a completely unsupervised both model and data agnostic methodology that can be leveraged for general model fairness discovery. The proposed methodology is tested against a variety of benchmark DL architectures and in the FL environment, showing an average 8.75% increase in Federated model accuracy in comparison with similar work.",
      "journal": "IEEE/ACM transactions on computational biology and bioinformatics",
      "year": "2023",
      "doi": "10.1109/TCBB.2023.3269767",
      "authors": "Siniosoglou Ilias et al.",
      "keywords": "",
      "mesh_terms": "Benchmarking; Machine Learning; Delivery of Health Care",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37097792/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Federated Learning; Clustering",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Federated Learning; Post-hoc Correction",
      "clinical_setting": "ICU",
      "key_findings": "The produced work applies a completely unsupervised both model and data agnostic methodology that can be leveraged for general model fairness discovery. The proposed methodology is tested against a variety of benchmark DL architectures and in the FL environment, showing an average 8.75% increase in Federated model accuracy in comparison with similar work.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37125409",
      "title": "Toward fairness in artificial intelligence for medical image analysis: identification and mitigation of potential biases in the roadmap from data collection to model deployment.",
      "abstract": "PURPOSE: To recognize and address various sources of bias essential for algorithmic fairness and trustworthiness and to contribute to a just and equitable deployment of AI in medical imaging, there is an increasing interest in developing medical imaging-based machine learning methods, also known as medical imaging artificial intelligence (AI), for the detection, diagnosis, prognosis, and risk assessment of disease with the goal of clinical implementation. These tools are intended to help improve traditional human decision-making in medical imaging. However, biases introduced in the steps toward clinical deployment may impede their intended function, potentially exacerbating inequities. Specifically, medical imaging AI can propagate or amplify biases introduced in the many steps from model inception to deployment, resulting in a systematic difference in the treatment of different groups. APPROACH: Our multi-institutional team included medical physicists, medical imaging artificial intelligence/machine learning (AI/ML) researchers, experts in AI/ML bias, statisticians, physicians, and scientists from regulatory bodies. We identified sources of bias in AI/ML, mitigation strategies for these biases, and developed recommendations for best practices in medical imaging AI/ML development. RESULTS: Five main steps along the roadmap of medical imaging AI/ML were identified: (1)\u00a0data collection, (2)\u00a0data preparation and annotation, (3)\u00a0model development, (4)\u00a0model evaluation, and (5)\u00a0model deployment. Within these steps, or bias categories, we identified 29 sources of potential bias, many of which can impact multiple steps, as well as mitigation strategies. CONCLUSIONS: Our findings provide a valuable resource to researchers, clinicians, and the public at large.",
      "journal": "Journal of medical imaging (Bellingham, Wash.)",
      "year": "2023",
      "doi": "10.1117/1.JMI.10.6.061104",
      "authors": "Drukker Karen et al.",
      "keywords": "artificial intelligence; bias; fairness; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37125409/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Deep Learning; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Our findings provide a valuable resource to researchers, clinicians, and the public at large.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10129875"
    },
    {
      "pmid": "37185650",
      "title": "Predictive care: a protocol for a computational ethnographic approach to building fair models of inpatient violence in emergency psychiatry.",
      "abstract": "INTRODUCTION: Managing violence or aggression is an ongoing challenge in emergency psychiatry. Many patients identified as being at risk do not go on to become violent or aggressive. Efforts to automate the assessment of risk involve training machine learning (ML) models on data from electronic health records (EHRs) to predict these behaviours. However, no studies to date have examined which patient groups may be over-represented in false positive predictions, despite evidence of social and clinical biases that may lead to higher perceptions of risk in patients defined by intersecting features (eg, race, gender). Because risk assessment can impact psychiatric care (eg, via coercive measures, such as restraints), it is unclear which patients might be underserved or harmed by the application of ML. METHODS AND ANALYSIS: We pilot a computational ethnography to study how the integration of ML into risk assessment might impact acute psychiatric care, with a focus on how EHR data is compiled and used to predict a risk of violence or aggression. Our objectives include: (1) evaluating an ML model trained on psychiatric EHRs to predict violent or aggressive incidents for intersectional bias; and (2) completing participant observation and qualitative interviews in an emergency psychiatric setting to explore how social, clinical and structural biases are encoded in the training data. Our overall aim is to study the impact of ML applications in acute psychiatry on marginalised and underserved patient groups. ETHICS AND DISSEMINATION: The project was approved by the research ethics board at The Centre for Addiction and Mental Health (053/2021). Study findings will be presented in peer-reviewed journals, conferences and shared with service users and providers.",
      "journal": "BMJ open",
      "year": "2023",
      "doi": "10.1136/bmjopen-2022-069255",
      "authors": "Sikstrom Laura et al.",
      "keywords": "ethnography; health equity; machine learning; psychiatry; risk assessment",
      "mesh_terms": "Humans; Inpatients; Violence; Aggression; Anthropology, Cultural; Psychiatry",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37185650/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry; EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Intersectional",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; Safety-Net/Underserved",
      "key_findings": "ETHICS AND DISSEMINATION: The project was approved by the research ethics board at The Centre for Addiction and Mental Health (053/2021). Study findings will be presented in peer-reviewed journals, conferences and shared with service users and providers.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10151964"
    },
    {
      "pmid": "37252970",
      "title": "Cohort bias in predictive risk assessments of future criminal justice system involvement.",
      "abstract": "Risk assessment instruments (RAIs) are widely used to aid high-stakes decision-making in criminal justice settings and other areas such as health care and child welfare. These tools, whether using machine learning or simpler algorithms, typically assume a time-invariant relationship between predictors and outcome. Because societies are themselves changing and not just individuals, this assumption may be violated in many behavioral settings, generating what we call cohort bias. Analyzing criminal histories in a cohort-sequential longitudinal study of children, we demonstrate that regardless of model type or predictor sets, a tool trained to predict the likelihood of arrest between the ages of 17 and 24 y on older birth cohorts systematically overpredicts the likelihood of arrest for younger birth cohorts over the period 1995 to 2020. Cohort bias is found for both relative and absolute risks, and it persists for all racial groups and within groups at highest risk for arrest. The results imply that cohort bias is an underappreciated mechanism generating inequality in contacts with the criminal legal system that is distinct from racial bias. Cohort bias is a challenge not only for predictive instruments with respect to crime and justice, but also for RAIs more broadly.",
      "journal": "Proceedings of the National Academy of Sciences of the United States of America",
      "year": "2023",
      "doi": "10.1073/pnas.2301990120",
      "authors": "Montana Erika et al.",
      "keywords": "bias; cohort; criminal justice; risk assessment; social change",
      "mesh_terms": "Child; Humans; Adolescent; Young Adult; Adult; Longitudinal Studies; Criminal Law; Crime; Cohort Studies; Risk Assessment",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37252970/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Pediatrics; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The results imply that cohort bias is an underappreciated mechanism generating inequality in contacts with the criminal legal system that is distinct from racial bias. Cohort bias is a challenge not only for predictive instruments with respect to crime and justice, but also for RAIs more broadly.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10265989"
    },
    {
      "pmid": "37289496",
      "title": "Gender Bias When Using Artificial Intelligence to Assess Anorexia Nervosa on Social Media: Data-Driven Study.",
      "abstract": "BACKGROUND: Social media sites are becoming an increasingly important source of information about mental health disorders. Among them, eating disorders are complex psychological problems that involve unhealthy eating habits. In particular, there is evidence showing that signs and symptoms of anorexia nervosa can be traced in social media platforms. Knowing that input data biases tend to be amplified by artificial intelligence algorithms and, in particular, machine learning, these methods should be revised to mitigate biased discrimination in such important domains. OBJECTIVE: The main goal of this study was to detect and analyze the performance disparities across genders in algorithms trained for the detection of anorexia nervosa on social media posts. We used a collection of automated predictors trained on a data set in Spanish containing cases of 177 users that showed signs of anorexia (471,262 tweets) and 326 control cases (910,967 tweets). METHODS: We first inspected the predictive performance differences between the algorithms for male and female users. Once biases were detected, we applied a feature-level bias characterization to evaluate the source of such biases and performed a comparative analysis of such features and those that are relevant for clinicians. Finally, we showcased different bias mitigation strategies to develop fairer automated classifiers, particularly for risk assessment in sensitive domains. RESULTS: Our results revealed concerning predictive performance differences, with substantially higher false negative rates (FNRs) for female samples (FNR=0.082) compared with male samples (FNR=0.005). The findings show that biological processes and suicide risk factors were relevant for classifying positive male cases, whereas age, emotions, and personal concerns were more relevant for female cases. We also proposed techniques for bias mitigation, and we could see that, even though disparities can be mitigated, they cannot be eliminated. CONCLUSIONS: We concluded that more attention should be paid to the assessment of biases in automated methods dedicated to the detection of mental health issues. This is particularly relevant before the deployment of systems that are thought to assist clinicians, especially considering that the outputs of such systems can have an impact on the diagnosis of people at risk.",
      "journal": "Journal of medical Internet research",
      "year": "2023",
      "doi": "10.2196/45184",
      "authors": "Solans Noguero David et al.",
      "keywords": "anorexia nervosa; artificial intelligence; gender bias; social media",
      "mesh_terms": "Female; Humans; Male; Anorexia Nervosa; Artificial Intelligence; Sexism; Social Media; Feeding and Eating Disorders",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37289496/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry; ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSIONS: We concluded that more attention should be paid to the assessment of biases in automated methods dedicated to the detection of mental health issues. This is particularly relevant before the deployment of systems that are thought to assist clinicians, especially considering that the outputs of such systems can have an impact on the diagnosis of people at risk.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10288345"
    },
    {
      "pmid": "37311802",
      "title": "Bias in AI-based models for medical applications: challenges and mitigation strategies.",
      "abstract": "Artificial intelligence systems are increasingly being applied to healthcare. In surgery, AI applications hold promise as tools to predict surgical outcomes, assess technical skills, or guide surgeons intraoperatively via computer vision. On the other hand, AI systems can also suffer from bias, compounding existing inequities in socioeconomic status, race, ethnicity, religion, gender, disability, or sexual orientation. Bias particularly impacts disadvantaged populations, which can be subject to algorithmic predictions that are less accurate or underestimate the need for care. Thus, strategies for detecting and mitigating bias are pivotal for creating AI technology that is generalizable and fair. Here, we discuss a recent study that developed a new strategy to mitigate bias in surgical AI systems.",
      "journal": "NPJ digital medicine",
      "year": "2023",
      "doi": "10.1038/s41746-023-00858-z",
      "authors": "Mittermaier Mirja et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37311802/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "ICU/Critical Care; Surgery",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Disability; Intersectional",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "Thus, strategies for detecting and mitigating bias are pivotal for creating AI technology that is generalizable and fair. Here, we discuss a recent study that developed a new strategy to mitigate bias in surgical AI systems.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10264403"
    },
    {
      "pmid": "37318804",
      "title": "Racial and Ethnic Bias in Risk Prediction Models for Colorectal Cancer Recurrence When Race and Ethnicity Are Omitted as Predictors.",
      "abstract": "IMPORTANCE: Including race and ethnicity as a predictor in clinical risk prediction algorithms has received increased scrutiny, but there continues to be a lack of empirical studies addressing whether simply omitting race and ethnicity from the algorithms will ultimately affect decision-making for patients of minoritized racial and ethnic groups. OBJECTIVE: To examine whether including race and ethnicity as a predictor in a colorectal cancer recurrence risk algorithm is associated with racial bias, defined as racial and ethnic differences in model accuracy that could potentially lead to unequal treatment. DESIGN, SETTING, AND PARTICIPANTS: This retrospective prognostic study was conducted using data from a large integrated health care system in Southern California for patients with colorectal cancer who received primary treatment between 2008 and 2013 and follow-up until December 31, 2018. Data were analyzed from January 2021 to June 2022. MAIN OUTCOMES AND MEASURES: Four Cox proportional hazards regression prediction models were fitted to predict time from surveillance start to cancer recurrence: (1) a race-neutral model that explicitly excluded race and ethnicity as a predictor, (2) a race-sensitive model that included race and ethnicity, (3) a model with 2-way interactions between clinical predictors and race and ethnicity, and (4) separate models by race and ethnicity. Algorithmic fairness was assessed using model calibration, discriminative ability, false-positive and false-negative rates, positive predictive value (PPV), and negative predictive value (NPV). RESULTS: The study cohort included 4230 patients (mean [SD] age, 65.3 [12.5] years; 2034 [48.1%] female; 490 [11.6%] Asian, Hawaiian, or Pacific Islander; 554 [13.1%] Black or African American; 937 [22.1%] Hispanic; and 2249 [53.1%] non-Hispanic White). The race-neutral model had worse calibration, NPV, and false-negative rates among racial and ethnic minority subgroups than non-Hispanic White individuals (eg, false-negative rate for Hispanic patients: 12.0% [95% CI, 6.0%-18.6%]; for non-Hispanic White patients: 3.1% [95% CI, 0.8%-6.2%]). Adding race and ethnicity as a predictor improved algorithmic fairness in calibration slope, discriminative ability, PPV, and false-negative rates (eg, false-negative rate for Hispanic patients: 9.2% [95% CI, 3.9%-14.9%]; for non-Hispanic White patients: 7.9% [95% CI, 4.3%-11.9%]). Inclusion of race interaction terms or using race-stratified models did not improve model fairness, likely due to small sample sizes in subgroups. CONCLUSIONS AND RELEVANCE: In this prognostic study of the racial bias in a cancer recurrence risk algorithm, removing race and ethnicity as a predictor worsened algorithmic fairness in multiple measures, which could lead to inappropriate care recommendations for patients who belong to minoritized racial and ethnic groups. Clinical algorithm development should include evaluation of fairness criteria to understand the potential consequences of removing race and ethnicity for health inequities.",
      "journal": "JAMA network open",
      "year": "2023",
      "doi": "10.1001/jamanetworkopen.2023.18495",
      "authors": "Khor Sara et al.",
      "keywords": "",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Black or African American; Colorectal Neoplasms; Ethnicity; Hispanic or Latino; Minority Groups; Retrospective Studies; White; Asian American Native Hawaiian and Pacific Islander",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37318804/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Oncology; Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration",
      "clinical_setting": "Public Health/Population",
      "key_findings": "RESULTS: The study cohort included 4230 patients (mean [SD] age, 65.3 [12.5] years; 2034 [48.1%] female; 490 [11.6%] Asian, Hawaiian, or Pacific Islander; 554 [13.1%] Black or African American; 937 [22.1%] Hispanic; and 2249 [53.1%] non-Hispanic White). The race-neutral model had worse calibration, NPV, and false-negative rates among racial and ethnic minority subgroups than non-Hispanic White individuals (eg, false-negative rate for Hispanic patients: 12.0% [95% CI, 6.0%-18.6%]; for non-Hispani...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10273018"
    },
    {
      "pmid": "37358896",
      "title": "Risk of Bias Mitigation for Vulnerable and Diverse Groups in Community-Based Primary Health Care Artificial Intelligence Models: Protocol for a Rapid Review.",
      "abstract": "BACKGROUND: The current literature identifies several potential benefits of artificial intelligence models for populations' health and health care systems' efficiency. However, there is a lack of understanding on how the risk of bias is considered in the development of primary health care and community health service artificial intelligence algorithms and to what extent they perpetuate or introduce potential biases toward groups that could be considered vulnerable in terms of their characteristics. To the best of our knowledge, no reviews are currently available to identify relevant methods to assess the risk of bias in these algorithms. The primary research question of this review is which strategies can assess the risk of bias in primary health care algorithms toward vulnerable or diverse groups? OBJECTIVE: This review aims to identify relevant methods to assess the risk of bias toward vulnerable or diverse groups in the development or deployment of algorithms in community-based primary health care and mitigation interventions deployed to promote and increase equity, diversity, and inclusion. This review looks at what attempts to mitigate bias have been documented and which vulnerable or diverse groups have been considered. METHODS: A rapid systematic review of the scientific literature will be conducted. In November 2022, an information specialist developed a specific search strategy based on the main concepts of our primary review question in 4 relevant databases in the last 5 years. We completed the search strategy in December 2022, and 1022 sources were identified. Since February 2023, two reviewers independently screened the titles and abstracts on the Covidence systematic review software. Conflicts are solved through consensus and discussion with a senior researcher. We include all studies on methods developed or tested to assess the risk of bias in algorithms that are relevant in community-based primary health care. RESULTS: In early May 2023, almost 47% (479/1022) of the titles and abstracts have been screened. We completed this first stage in May 2023. In June and July 2023, two reviewers will independently apply the same criteria to full texts, and all exclusion motives will be recorded. Data from selected studies will be extracted using a validated grid in August and analyzed in September 2023. Results will be presented using structured qualitative narrative summaries and submitted for publication by the end of 2023. CONCLUSIONS: The approach to identifying methods and target populations of this review is primarily qualitative. However, we will consider a meta-analysis if quantitative data and results are sufficient. This review will develop structured qualitative summaries of strategies to mitigate bias toward vulnerable populations and diverse groups in artificial intelligence models. This could be useful to researchers and other stakeholders to identify potential sources of bias in algorithms and try to reduce or eliminate them. TRIAL REGISTRATION: OSF Registries qbph8; https://osf.io/qbph8. INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): DERR1-10.2196/46684.",
      "journal": "JMIR research protocols",
      "year": "2023",
      "doi": "10.2196/46684",
      "authors": "Sasseville Maxime et al.",
      "keywords": "algorithms; artificial intelligence; health disparity; health equity; primary care; rapid review; systematic review",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37358896/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: The approach to identifying methods and target populations of this review is primarily qualitative. However, we will consider a meta-analysis if quantitative data and results are sufficient. This review will develop structured qualitative summaries of strategies to mitigate bias toward vulnerable populations and diverse groups in artificial intelligence models.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10337340"
    },
    {
      "pmid": "37377633",
      "title": "Bias Analysis in Healthcare Time Series (BAHT) Decision Support Systems from Meta Data.",
      "abstract": "One of the hindrances in the widespread acceptance of deep learning-based decision support systems in healthcare is bias. Bias in its many forms occurs in the datasets used to train and test deep learning models and is amplified when deployed in the real world, leading to challenges such as model drift. Recent advancements in the field of deep learning have led to the deployment of deployable automated healthcare diagnosis decision support systems at hospitals as well as tele-medicine through IoT devices. Research has been focused primarily on the development and improvement of these systems leaving a gap in the analysis of the fairness. The domain of FAccT ML (fairness, accountability, and transparency) accounts for the analysis of these deployable machine learning systems. In this work, we present a framework for bias analysis in healthcare time series (BAHT) signals such as electrocardiogram (ECG) and electroencephalogram (EEG). BAHT provides a graphical interpretive analysis of bias in the training, testing datasets in terms of protected variables, and analysis of bias amplification by the trained supervised learning model for time series healthcare decision support systems. We thoroughly investigate three prominent time series ECG and EEG healthcare datasets used for model training and research. We show the extensive presence of bias in the datasets leads to potentially biased or unfair machine-learning models. Our experiments also demonstrate the amplification of identified bias with an observed maximum of 66.66%. We investigate the effect of model drift due to unanalyzed bias in datasets and algorithms. Bias mitigation though prudent is a nascent area of research. We present experiments and analyze the most prevalently accepted bias mitigation strategies of under-sampling, oversampling, and the use of synthetic data for balancing the dataset through augmentation. It is important that healthcare models, datasets, and bias mitigation strategies should be properly analyzed for a fair unbiased delivery of service.",
      "journal": "Journal of healthcare informatics research",
      "year": "2023",
      "doi": "10.1007/s41666-023-00133-6",
      "authors": "Dakshit Sagnik et al.",
      "keywords": "Bias analysis; Bias mitigation; Decision support systems; Fairness; Synthetic data",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37377633/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Clinical Decision Support",
      "health_domain": "Cardiology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Data Augmentation",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "We present experiments and analyze the most prevalently accepted bias mitigation strategies of under-sampling, oversampling, and the use of synthetic data for balancing the dataset through augmentation. It is important that healthcare models, datasets, and bias mitigation strategies should be properly analyzed for a fair unbiased delivery of service.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10290973"
    },
    {
      "pmid": "37463884",
      "title": "Detecting shortcut learning for fair medical AI using shortcut testing.",
      "abstract": "Machine learning (ML) holds great promise for improving healthcare, but it is critical to ensure that its use will not propagate or amplify health disparities. An important step is to characterize the (un)fairness of ML models-their tendency to perform differently across subgroups of the population-and to understand its underlying mechanisms. One potential driver of algorithmic unfairness, shortcut learning, arises when ML models base predictions on improper correlations in the training data. Diagnosing this phenomenon is difficult as sensitive attributes may be causally linked with disease. Using multitask learning, we propose a method to directly test for the presence of shortcut learning in clinical ML systems and demonstrate its application to clinical tasks in radiology and dermatology. Finally, our approach reveals instances when shortcutting is not responsible for unfairness, highlighting the need for a holistic approach to fairness mitigation in medical AI.",
      "journal": "Nature communications",
      "year": "2023",
      "doi": "10.1038/s41467-023-39902-7",
      "authors": "Brown Alexander et al.",
      "keywords": "",
      "mesh_terms": "Health Facilities; Machine Learning",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37463884/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Dermatology; ICU/Critical Care",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Multi-task Learning",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "Using multitask learning, we propose a method to directly test for the presence of shortcut learning in clinical ML systems and demonstrate its application to clinical tasks in radiology and dermatology. Finally, our approach reveals instances when shortcutting is not responsible for unfairness, highlighting the need for a holistic approach to fairness mitigation in medical AI.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10354021"
    },
    {
      "pmid": "37503225",
      "title": "Predicting prenatal depression and assessing model bias using machine learning models.",
      "abstract": "Perinatal depression (PND) is one of the most common medical complications during pregnancy and postpartum period, affecting 10-20% of pregnant individuals. Black and Latina women have higher rates of PND, yet they are less likely to be diagnosed and receive treatment. Machine learning (ML) models based on Electronic Medical Records (EMRs) have been effective in predicting postpartum depression in middle-class White women but have rarely included sufficient proportions of racial and ethnic minorities, which contributed to biases in ML models for minority women. Our goal is to determine whether ML models could serve to predict depression in early pregnancy in racial/ethnic minority women by leveraging EMR data. We extracted EMRs from a hospital in a large urban city that mostly served low-income Black and Hispanic women (N=5,875) in the U.S. Depressive symptom severity was assessed from a self-reported questionnaire, PHQ-9. We investigated multiple ML classifiers, used Shapley Additive Explanations (SHAP) for model interpretation, and determined model prediction bias with two metrics, Disparate Impact, and Equal Opportunity Difference. While ML model (Elastic Net) performance was low (ROCAUC=0.67), we identified well-known factors associated with PND, such as unplanned pregnancy and being single, as well as underexplored factors, such as self-report pain levels, lower levels of prenatal vitamin supplement intake, asthma, carrying a male fetus, and lower platelet levels blood. Our findings showed that despite being based on a sample mostly composed of 75% low-income minority women (54% Black and 27% Latina), the model performance was lower for these communities. In conclusion, ML models based on EMRs could moderately predict depression in early pregnancy, but their performance is biased against low-income minority women.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2023",
      "doi": "10.1101/2023.07.17.23292587",
      "authors": "Huang Yongchao et al.",
      "keywords": "Perinatal depression; electronic medical records; machine learning; model performance bias",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37503225/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry; EHR/Health Informatics; Obstetrics/Maternal Health; Pulmonology; Pain Management",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "Our findings showed that despite being based on a sample mostly composed of 75% low-income minority women (54% Black and 27% Latina), the model performance was lower for these communities. In conclusion, ML models based on EMRs could moderately predict depression in early pregnancy, but their performance is biased against low-income minority women.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10371186"
    },
    {
      "pmid": "37514877",
      "title": "Development of Debiasing Technique for Lung Nodule Chest X-ray Datasets to Generalize Deep Learning Models.",
      "abstract": "Screening programs for early lung cancer diagnosis are uncommon, primarily due to the challenge of reaching at-risk patients located in rural areas far from medical facilities. To overcome this obstacle, a comprehensive approach is needed that combines mobility, low cost, speed, accuracy, and privacy. One potential solution lies in combining the chest X-ray imaging mode with federated deep learning, ensuring that no single data source can bias the model adversely. This study presents a pre-processing pipeline designed to debias chest X-ray images, thereby enhancing internal classification and external generalization. The pipeline employs a pruning mechanism to train a deep learning model for nodule detection, utilizing the most informative images from a publicly available lung nodule X-ray dataset. Histogram equalization is used to remove systematic differences in image brightness and contrast. Model training is then performed using combinations of lung field segmentation, close cropping, and rib/bone suppression. The resulting deep learning models, generated through this pre-processing pipeline, demonstrate successful generalization on an independent lung nodule dataset. By eliminating confounding variables in chest X-ray images and suppressing signal noise from the bone structures, the proposed deep learning lung nodule detection algorithm achieves an external generalization accuracy of 89%. This approach paves the way for the development of a low-cost and accessible deep learning-based clinical system for lung cancer screening.",
      "journal": "Sensors (Basel, Switzerland)",
      "year": "2023",
      "doi": "10.3390/s23146585",
      "authors": "Horry Michael J et al.",
      "keywords": "chest X-ray; confounding bias; deep learning; federated learning; lung cancer; model generalization",
      "mesh_terms": "Humans; Deep Learning; Neural Networks, Computer; X-Rays; Early Detection of Cancer; Lung Neoplasms; Lung",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37514877/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Radiology/Medical Imaging; Oncology; Pulmonology",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "By eliminating confounding variables in chest X-ray images and suppressing signal noise from the bone structures, the proposed deep learning lung nodule detection algorithm achieves an external generalization accuracy of 89%. This approach paves the way for the development of a low-cost and accessible deep learning-based clinical system for lung cancer screening.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10385599"
    },
    {
      "pmid": "37541180",
      "title": "Investigating the impact of cognitive biases in radiologists' image interpretation: A scoping review.",
      "abstract": "RATIONALE AND OBJECTIVE: Image interpretation is a fundamental aspect of radiology. The treatment and management of patients relies on accurate and timely imaging diagnosis. However, errors in radiological reports can negatively impact on patient health outcomes. These misdiagnoses can be caused by several different errors, but cognitive biases account for 74\u00a0% of all image interpretation errors. There are many biases that can impact on a radiologist's perception and cognitive processes. Several recent narrative reviews have discussed these cognitive biases and have offered possible strategies to mitigate their effects. However, these strategies remain untested. Therefore, the purpose of this scoping review is to evaluate the current knowledge on the extent that cognitive biases impact on medical image interpretation. MATERIAL AND METHODS: Scopus and Medline Databases were searched using relevant keywords to identify papers published between 2012 and 2022. A subsequent hand search of the narrative reviews was also performed. All studies collected were screened and assessed against the inclusion and exclusion criteria. RESULTS: Twenty-four publications were included and categorised into five main themes: satisfaction of search, availability bias, hindsight bias, framing bias and other biases. From these studies, there were mixed results regarding the impact of cognitive biases, highlighting the need for further investigation in this area. Moreover, the limited and untested debiasing methods offered by a minority of the publications and narrative reviews also suggests the need for further research. The potential of role of artificial intelligence is also highlighted to further assist radiologists in identifying and mitigating these cognitive biases. CONCLUSION: Cognitive biases can impact radiologists' image interpretation, however the effectiveness of debiasing strategies remain largely untested.",
      "journal": "European journal of radiology",
      "year": "2023",
      "doi": "10.1016/j.ejrad.2023.111013",
      "authors": "Chen Jacky et al.",
      "keywords": "Cognitive bias; Error; Heuristics; Interpretation; Radiology",
      "mesh_terms": "Humans; Cognition; Artificial Intelligence; Bias; Diagnostic Imaging; Radiologists",
      "pub_types": "Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37541180/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Cognitive biases can impact radiologists' image interpretation, however the effectiveness of debiasing strategies remain largely untested.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37599147",
      "title": "Illness severity assessment of older adults in critical illness using machine learning (ELDER-ICU): an international multicentre study with subgroup bias evaluation.",
      "abstract": "BACKGROUND: Comorbidity, frailty, and decreased cognitive function lead to a higher risk of death in elderly patients (more than 65 years of age) during acute medical events. Early and accurate illness severity assessment can support appropriate decision making for clinicians caring for these patients. We aimed to develop ELDER-ICU, a machine learning model to assess the illness severity of older adults admitted to the intensive care unit (ICU) with cohort-specific calibration and evaluation for potential model bias. METHODS: In this retrospective, international multicentre study, the ELDER-ICU model was developed using data from 14 US hospitals, and validated in 171 hospitals from the USA and Netherlands. Data were extracted from the Medical Information Mart for Intensive Care database, electronic ICU Collaborative Research Database, and Amsterdam University Medical Centers Database. We used six categories of data as predictors, including demographics and comorbidities, physical frailty, laboratory tests, vital signs, treatments, and urine output. Patient data from the first day of ICU stay were used to predict in-hospital mortality. We used the eXtreme Gradient Boosting algorithm (XGBoost) to develop models and the SHapley Additive exPlanations method to explain model prediction. The trained model was calibrated before internal, external, and temporal validation. The final XGBoost model was compared against three other machine learning algorithms and five clinical scores. We performed subgroup analysis based on age, sex, and race. We assessed the discrimination and calibration of models using the area under receiver operating characteristic (AUROC) and standardised mortality ratio (SMR) with 95% CIs. FINDINGS: Using the development dataset (n=50\u2008366) and predictive model building process, the XGBoost algorithm performed the best in all types of validations compared with other machine learning algorithms and clinical scores (internal validation with 5037 patients from 14 US hospitals, AUROC=0\u00b7866 [95% CI 0\u00b7851-0\u00b7880]; external validation in the US population with 20\u2008541 patients from 169 hospitals, AUROC=0\u00b7838 [0\u00b7829-0\u00b7847]; external validation in European population with 2411 patients from one hospital, AUROC=0\u00b7833 [0\u00b7812-0\u00b7853]; temporal validation with 4311 patients from one hospital, AUROC=0\u00b7884 [0\u00b7869-0\u00b7897]). In the external validation set (US population), the median AUROCs of bias evaluations covering eight subgroups were above 0\u00b781, and the overall SMR was 0\u00b799 (0\u00b796-1\u00b703). The top ten risk predictors were the minimum Glasgow Coma Scale score, total urine output, average respiratory rate, mechanical ventilation use, best state of activity, Charlson Comorbidity Index score, geriatric nutritional risk index, code status, age, and maximum blood urea nitrogen. A simplified model containing only the top 20 features (ELDER-ICU-20) had similar predictive performance to the full model. INTERPRETATION: The ELDER-ICU model reliably predicts the risk of in-hospital mortality using routinely collected clinical features. The predictions could inform clinicians about patients who are at elevated risk of deterioration. Prospective validation of this model in clinical practice and a process for continuous performance monitoring and model recalibration are needed. FUNDING: National Institutes of Health, National Natural Science Foundation of China, National Special Health Science Program, Health Science and Technology Plan of Zhejiang Province, Fundamental Research Funds for the Central Universities, Drug Clinical Evaluate Research of Chinese Pharmaceutical Association, and National Key R&D Program of China.",
      "journal": "The Lancet. Digital health",
      "year": "2023",
      "doi": "10.1016/S2589-7500(23)00128-0",
      "authors": "Liu Xiaoli et al.",
      "keywords": "",
      "mesh_terms": "United States; Aged; Humans; Critical Illness; Frailty; Retrospective Studies; Intensive Care Units; Machine Learning",
      "pub_types": "Multicenter Study; Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37599147/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "XGBoost/Gradient Boosting; Clinical Prediction Model",
      "health_domain": "ICU/Critical Care; Pulmonology; Drug Discovery/Pharmacology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Calibration; Subgroup Analysis; Explainability/Interpretability",
      "clinical_setting": "Hospital/Inpatient; ICU; Public Health/Population; Laboratory/Pathology",
      "key_findings": "FINDINGS: Using the development dataset (n=50\u2008366) and predictive model building process, the XGBoost algorithm performed the best in all types of validations compared with other machine learning algorithms and clinical scores (internal validation with 5037 patients from 14 US hospitals, AUROC=0\u00b7866 [95% CI 0\u00b7851-0\u00b7880]; external validation in the US population with 20\u2008541 patients from 169 hospitals, AUROC=0\u00b7838 [0\u00b7829-0\u00b7847]; external validation in European population with 2411 patients from o...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12557411"
    },
    {
      "pmid": "37609241",
      "title": "Enhancing Fairness in Disease Prediction by Optimizing Multiple Domain Adversarial Networks.",
      "abstract": "Predictive models in biomedicine need to ensure equitable and reliable outcomes for the populations they are applied to. Unfortunately, biases in medical predictions can lead to unfair treatment and widening disparities, underscoring the need for effective techniques to address these issues. To enhance fairness, we introduce a framework based on a Multiple Domain Adversarial Neural Network (MDANN), which incorporates multiple adversarial components. In an MDANN, an adversarial module is applied to learn a fair pattern by negative gradients back-propagating across multiple sensitive features (i.e., characteristics of individuals that should not be used to discriminate unfairly between individuals when making predictions or decisions.) We leverage loss functions based on the Area Under the Receiver Operating Characteristic Curve (AUC) to address the class imbalance, promoting equitable classification performance for minority groups (e.g., a subset of the population that is underrepresented or disadvantaged.) Moreover, we utilize pre-trained convolutional autoencoders (CAEs) to extract deep representations of data, aiming to enhance prediction accuracy and fairness. Combining these mechanisms, we alleviate biases and disparities to provide reliable and equitable disease prediction. We empirically demonstrate that the MDANN approach leads to better accuracy and fairness in predicting disease progression using brain imaging data for Alzheimer's Disease and Autism populations than state-of-the-art techniques.",
      "journal": "bioRxiv : the preprint server for biology",
      "year": "2023",
      "doi": "10.1101/2023.08.04.551906",
      "authors": "Li Bin et al.",
      "keywords": "Fairness; adversarial training; biases; health disparities; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37609241/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Neural Network; Clinical Prediction Model",
      "health_domain": "Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Intersectional",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Combining these mechanisms, we alleviate biases and disparities to provide reliable and equitable disease prediction. We empirically demonstrate that the MDANN approach leads to better accuracy and fairness in predicting disease progression using brain imaging data for Alzheimer's Disease and Autism populations than state-of-the-art techniques.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10441334"
    },
    {
      "pmid": "37615031",
      "title": "Algorithmic fairness and bias mitigation for clinical machine learning with deep reinforcement learning.",
      "abstract": "As models based on machine learning continue to be developed for healthcare applications, greater effort is needed to ensure that these technologies do not reflect or exacerbate any unwanted or discriminatory biases that may be present in the data. Here we introduce a reinforcement learning framework capable of mitigating biases that may have been acquired during data collection. In particular, we evaluated our model for the task of rapidly predicting COVID-19 for patients presenting to hospital emergency departments and aimed to mitigate any site (hospital)-specific and ethnicity-based biases present in the data. Using a specialized reward function and training procedure, we show that our method achieves clinically effective screening performances, while significantly improving outcome fairness compared with current benchmarks and state-of-the-art machine learning methods. We performed external validation across three independent hospitals, and additionally tested our method on a patient intensive care unit discharge status task, demonstrating model generalizability.",
      "journal": "Nature machine intelligence",
      "year": "2023",
      "doi": "10.1038/s42256-023-00697-3",
      "authors": "Yang Jenny et al.",
      "keywords": "Diagnosis; Medical ethics; Translational research",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37615031/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Reinforcement Learning",
      "health_domain": "Emergency Medicine; ICU/Critical Care; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; ICU; Emergency Department",
      "key_findings": "Using a specialized reward function and training procedure, we show that our method achieves clinically effective screening performances, while significantly improving outcome fairness compared with current benchmarks and state-of-the-art machine learning methods. We performed external validation across three independent hospitals, and additionally tested our method on a patient intensive care unit discharge status task, demonstrating model generalizability.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10442224"
    },
    {
      "pmid": "37693388",
      "title": "Assessing Racial and Ethnic Bias in Text Generation for Healthcare-Related Tasks by ChatGPT1.",
      "abstract": "Large Language Models (LLM) are AI tools that can respond human-like to voice or free-text commands without training on specific tasks. However, concerns have been raised about their potential racial bias in healthcare tasks. In this study, ChatGPT was used to generate healthcare-related text for patients with HIV, analyzing data from 100 deidentified electronic health record encounters. Each patient's data were fed four times with all information remaining the same except for race/ethnicity (African American, Asian, Hispanic White, Non-Hispanic White). The text output was analyzed for sentiment, subjectivity, reading ease, and most used words by race/ethnicity and insurance type. Results showed that instructions for African American, Asian, Hispanic White, and Non-Hispanic White patients had an average polarity of 0.14, 0.14, 0.15, and 0.14, respectively, with an average subjectivity of 0.46 for all races/ethnicities. The differences in polarity and subjectivity across races/ethnicities were not statistically significant. However, there was a statistically significant difference in word frequency across races/ethnicities and a statistically significant difference in subjectivity across insurance types with commercial insurance eliciting the most subjective responses and Medicare and other payer types the lowest. The study suggests that ChatGPT is relatively invariant to race/ethnicity and insurance type in terms of linguistic and readability measures. Further studies are needed to validate these results and assess their implications.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2023",
      "doi": "10.1101/2023.08.28.23294730",
      "authors": "Hanna John J et al.",
      "keywords": "Large Language Model; artificial intelligence; bias; racism; reading ease; sentiment analysis; word frequency",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37693388/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "EHR/Health Informatics; Genomics/Genetics; Infectious Disease",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Language; Insurance Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The study suggests that ChatGPT is relatively invariant to race/ethnicity and insurance type in terms of linguistic and readability measures. Further studies are needed to validate these results and assess their implications.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10491360"
    },
    {
      "pmid": "37698583",
      "title": "AI pitfalls and what not to do: mitigating bias in AI.",
      "abstract": "Various forms of artificial intelligence (AI) applications are being deployed and used in many healthcare systems. As the use of these applications increases, we are learning the failures of these models and how they can perpetuate bias. With these new lessons, we need to prioritize bias evaluation and mitigation for radiology applications; all the while not ignoring the impact of changes in the larger enterprise AI deployment which may have downstream impact on performance of AI models. In this paper, we provide an updated review of known pitfalls causing AI bias and discuss strategies for mitigating these biases within the context of AI deployment in the larger healthcare enterprise. We describe these pitfalls by framing them in the larger AI lifecycle from problem definition, data set selection and curation, model training and deployment emphasizing that bias exists across a spectrum and is a sequela of a combination of both human and machine factors.",
      "journal": "The British journal of radiology",
      "year": "2023",
      "doi": "10.1259/bjr.20230023",
      "authors": "Gichoya Judy Wawira et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Bias; Disease Progression; Learning; Radiology",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37698583/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "In this paper, we provide an updated review of known pitfalls causing AI bias and discuss strategies for mitigating these biases within the context of AI deployment in the larger healthcare enterprise. We describe these pitfalls by framing them in the larger AI lifecycle from problem definition, data set selection and curation, model training and deployment emphasizing that bias exists across a spectrum and is a sequela of a combination of both human and machine factors.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10546443"
    },
    {
      "pmid": "37700029",
      "title": "Considerations for addressing bias in artificial intelligence for health equity.",
      "abstract": "Health equity is a primary goal of healthcare stakeholders: patients and their advocacy groups, clinicians, other providers and their professional societies, bioethicists, payors and value based care organizations, regulatory agencies, legislators, and creators of artificial intelligence/machine learning (AI/ML)-enabled medical devices. Lack of equitable access to diagnosis and treatment may be improved through new digital health technologies, especially AI/ML, but these may also exacerbate disparities, depending on how bias is addressed. We propose an expanded Total Product Lifecycle (TPLC) framework for healthcare AI/ML, describing the sources and impacts of undesirable bias in AI/ML systems in each phase, how these can be analyzed using appropriate metrics, and how they can be potentially mitigated. The goal of these \"Considerations\" is to educate stakeholders on how potential AI/ML bias may impact healthcare outcomes and how to identify and mitigate inequities; to initiate a discussion between stakeholders on these issues, in order to ensure health equity along the expanded AI/ML TPLC framework, and ultimately, better health outcomes for all.",
      "journal": "NPJ digital medicine",
      "year": "2023",
      "doi": "10.1038/s41746-023-00913-9",
      "authors": "Abr\u00e0moff Michael D et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37700029/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We propose an expanded Total Product Lifecycle (TPLC) framework for healthcare AI/ML, describing the sources and impacts of undesirable bias in AI/ML systems in each phase, how these can be analyzed using appropriate metrics, and how they can be potentially mitigated. The goal of these \"Considerations\" is to educate stakeholders on how potential AI/ML bias may impact healthcare outcomes and how to identify and mitigate inequities; to initiate a discussion between stakeholders on these issues, in...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10497548"
    },
    {
      "pmid": "37709945",
      "title": "A translational perspective towards clinical AI fairness.",
      "abstract": "Artificial intelligence (AI) has demonstrated the ability to extract insights from data, but the fairness of such data-driven insights remains a concern in high-stakes fields. Despite extensive developments, issues of AI fairness in clinical contexts have not been adequately addressed. A fair model is normally expected to perform equally across subgroups defined by sensitive variables (e.g., age, gender/sex, race/ethnicity, socio-economic status, etc.). Various fairness measurements have been developed to detect differences between subgroups as evidence of bias, and bias mitigation methods are designed to reduce the differences detected. This perspective of fairness, however, is misaligned with some key considerations in clinical contexts. The set of sensitive variables used in healthcare applications must be carefully examined for relevance and justified by clear clinical motivations. In addition, clinical AI fairness should closely investigate the ethical implications of fairness measurements (e.g., potential conflicts between group- and individual-level fairness) to select suitable and objective metrics. Generally defining AI fairness as \"equality\" is not necessarily reasonable in clinical settings, as differences may have clinical justifications and do not indicate biases. Instead, \"equity\" would be an appropriate objective of clinical AI fairness. Moreover, clinical feedback is essential to developing fair and well-performing AI models, and efforts should be made to actively involve clinicians in the process. The adaptation of AI fairness towards healthcare is not self-evident due to misalignments between technical developments and clinical considerations. Multidisciplinary collaboration between AI researchers, clinicians, and ethicists is necessary to bridge the gap and translate AI fairness into real-life benefits.",
      "journal": "NPJ digital medicine",
      "year": "2023",
      "doi": "10.1038/s41746-023-00918-4",
      "authors": "Liu Mingxuan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37709945/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The adaptation of AI fairness towards healthcare is not self-evident due to misalignments between technical developments and clinical considerations. Multidisciplinary collaboration between AI researchers, clinicians, and ethicists is necessary to bridge the gap and translate AI fairness into real-life benefits.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10502051"
    },
    {
      "pmid": "37745001",
      "title": "Detecting biased validation of predictive models in the positive-unlabeled setting: disease gene prioritization case study.",
      "abstract": "MOTIVATION: Positive-unlabeled data consists of points with either positive or unknown labels. It is widespread in medical, genetic, and biological settings, creating a high demand for predictive positive-unlabeled models. The performance of such models is usually estimated using validation sets, assumed to be selected completely at random (SCAR) from known positive examples. For certain metrics, this assumption enables unbiased performance estimation when treating positive-unlabeled data as positive/negative. However, the SCAR assumption is often adopted without proper justifications, simply for the sake of convenience. RESULTS: We provide an algorithm that under the weak assumptions of a lower bound on the number of positive examples can test for the violation of the SCAR assumption. Applying it to the problem of gene prioritization for complex genetic traits, we illustrate that the SCAR assumption is often violated there, causing the inflation of performance estimates, which we refer to as validation bias. We estimate the potential impact of validation bias on performance estimation. Our analysis reveals that validation bias is widespread in gene prioritization data and can significantly overestimate the performance of models. This finding elucidates the discrepancy between the reported good performance of models and their limited practical applications. AVAILABILITY AND IMPLEMENTATION: Python code with examples of application of the validation bias detection algorithm is available at github.com/ArtomovLab/ValidationBias.",
      "journal": "Bioinformatics advances",
      "year": "2023",
      "doi": "10.1093/bioadv/vbad128",
      "authors": "Molotkov Ivan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37745001/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: We provide an algorithm that under the weak assumptions of a lower bound on the number of positive examples can test for the violation of the SCAR assumption. Applying it to the problem of gene prioritization for complex genetic traits, we illustrate that the SCAR assumption is often violated there, causing the inflation of performance estimates, which we refer to as validation bias. We estimate the potential impact of validation bias on performance estimation.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10517638"
    },
    {
      "pmid": "37782868",
      "title": "Mitigating Racial And Ethnic Bias And Advancing Health Equity In Clinical Algorithms: A Scoping Review.",
      "abstract": "In August 2022 the Department of Health and Human Services (HHS) issued a notice of proposed rulemaking prohibiting covered entities, which include health care providers and health plans, from discriminating against individuals when using clinical algorithms in decision making. However, HHS did not provide specific guidelines on how covered entities should prevent discrimination. We conducted a scoping review of literature published during the period 2011-22 to identify health care applications, frameworks, reviews and perspectives, and assessment tools that identify and mitigate bias in clinical algorithms, with a specific focus on racial and ethnic bias. Our scoping review encompassed 109 articles comprising 45 empirical health care applications that included tools tested in health care settings, 16 frameworks, and 48 reviews and perspectives. We identified a wide range of technical, operational, and systemwide bias mitigation strategies for clinical algorithms, but there was no consensus in the literature on a single best practice that covered entities could employ to meet the HHS requirements. Future research should identify optimal bias mitigation methods for various scenarios, depending on factors such as patient population, clinical setting, algorithm design, and types of bias to be addressed.",
      "journal": "Health affairs (Project Hope)",
      "year": "2023",
      "doi": "10.1377/hlthaff.2023.00553",
      "authors": "Cary Michael P et al.",
      "keywords": "",
      "mesh_terms": "Humans; Health Equity; Racial Groups; Delivery of Health Care; Health Personnel; Algorithms",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37782868/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We identified a wide range of technical, operational, and systemwide bias mitigation strategies for clinical algorithms, but there was no consensus in the literature on a single best practice that covered entities could employ to meet the HHS requirements. Future research should identify optimal bias mitigation methods for various scenarios, depending on factors such as patient population, clinical setting, algorithm design, and types of bias to be addressed.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10668606"
    },
    {
      "pmid": "37803009",
      "title": "Improving model fairness in image-based computer-aided diagnosis.",
      "abstract": "Deep learning has become a popular tool for computer-aided diagnosis using medical images, sometimes matching or exceeding the performance of clinicians. However, these models can also reflect and amplify human bias, potentially resulting inaccurate missed diagnoses. Despite this concern, the problem of improving model fairness in medical image classification by deep learning has yet to be fully studied. To address this issue, we propose an algorithm that leverages the marginal pairwise equal opportunity to reduce bias in medical image classification. Our evaluations across four tasks using four independent large-scale cohorts demonstrate that our proposed algorithm not only improves fairness in individual and intersectional subgroups but also maintains overall performance. Specifically, the relative change in pairwise fairness difference between our proposed model and the baseline model was reduced by over 35%, while the relative change in AUC value was typically within 1%. By reducing the bias generated by deep learning models, our proposed approach can potentially alleviate concerns about the fairness and reliability of image-based computer-aided diagnosis.",
      "journal": "Nature communications",
      "year": "2023",
      "doi": "10.1038/s41467-023-41974-4",
      "authors": "Lin Mingquan et al.",
      "keywords": "",
      "mesh_terms": "Humans; Reproducibility of Results; Diagnosis, Computer-Assisted; Algorithms; Computers",
      "pub_types": "Journal Article; Research Support, N.I.H., Intramural; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37803009/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning; Computer Vision/Imaging AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Age; Intersectional",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "Specifically, the relative change in pairwise fairness difference between our proposed model and the baseline model was reduced by over 35%, while the relative change in AUC value was typically within 1%. By reducing the bias generated by deep learning models, our proposed approach can potentially alleviate concerns about the fairness and reliability of image-based computer-aided diagnosis.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10558498"
    },
    {
      "pmid": "37819812",
      "title": "Bipartite Ranking Fairness Through a Model Agnostic Ordering Adjustment.",
      "abstract": "Recently, with the applications of algorithms in various risky scenarios, algorithmic fairness has been a serious concern and received lots of interest in machine learning community. In this article, we focus on the bipartite ranking scenario, where the instances come from either the positive or negative class and the goal is to learn a ranking function that ranks positive instances higher than negative ones. We are interested in whether the learned ranking function can cause systematic disparity across different protected groups defined by sensitive attributes. While there could be a trade-off between fairness and performance, we propose a model agnostic post-processing framework xOrder for achieving fairness in bipartite ranking and maintaining the algorithm classification performance. In particular, we optimize a weighted sum of the utility as identifying an optimal warping path across different protected groups and solve it through a dynamic programming process. xOrder is compatible with various classification models and ranking fairness metrics, including supervised and unsupervised fairness metrics. In addition to binary groups, xOrder can be applied to multiple protected groups. We evaluate our proposed algorithm on four benchmark data sets and two real-world patient electronic health record repositories. xOrder consistently achieves a better balance between the algorithm utility and ranking fairness on a variety of datasets with different metrics. From the visualization of the calibrated ranking scores, xOrder mitigates the score distribution shifts of different groups compared with baselines. Moreover, additional analytical results verify that xOrder achieves a robust performance when faced with fewer samples and a bigger difference between training and testing ranking score distributions.",
      "journal": "IEEE transactions on pattern analysis and machine intelligence",
      "year": "2023",
      "doi": "10.1109/TPAMI.2023.3290949",
      "authors": "Cui Sen et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37819812/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clustering",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Post-hoc Correction",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "From the visualization of the calibrated ranking scores, xOrder mitigates the score distribution shifts of different groups compared with baselines. Moreover, additional analytical results verify that xOrder achieves a robust performance when faced with fewer samples and a bigger difference between training and testing ranking score distributions.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37824494",
      "title": "Going beyond the means: Exploring the role of bias from digital determinants of health in technologies.",
      "abstract": "BACKGROUND: In light of recent retrospective studies revealing evidence of disparities in access to medical technology and of bias in measurements, this narrative review assesses digital determinants of health (DDoH) in both technologies and medical formulae that demonstrate either evidence of bias or suboptimal performance, identifies potential mechanisms behind such bias, and proposes potential methods or avenues that can guide future efforts to address these disparities. APPROACH: Mechanisms are broadly grouped into physical and biological biases (e.g., pulse oximetry, non-contact infrared thermometry [NCIT]), interaction of human factors and cultural practices (e.g., electroencephalography [EEG]), and interpretation bias (e.g, pulmonary function tests [PFT], optical coherence tomography [OCT], and Humphrey visual field [HVF] testing). This review scope specifically excludes technologies incorporating artificial intelligence and machine learning. For each technology, we identify both clinical and research recommendations. CONCLUSIONS: Many of the DDoH mechanisms encountered in medical technologies and formulae result in lower accuracy or lower validity when applied to patients outside the initial scope of development or validation. Our clinical recommendations caution clinical users in completely trusting result validity and suggest correlating with other measurement modalities robust to the DDoH mechanism (e.g., arterial blood gas for pulse oximetry, core temperatures for NCIT). Our research recommendations suggest not only increasing diversity in development and validation, but also awareness in the modalities of diversity required (e.g., skin pigmentation for pulse oximetry but skin pigmentation and sex/hormonal variation for NCIT). By increasing diversity that better reflects patients in all scenarios of use, we can mitigate DDoH mechanisms and increase trust and validity in clinical practice and research.",
      "journal": "PLOS digital health",
      "year": "2023",
      "doi": "10.1371/journal.pdig.0000244",
      "authors": "Charpignon Marie-Laure et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37824494/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Ophthalmology; Pulmonology; Wearables/Remote Monitoring",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Many of the DDoH mechanisms encountered in medical technologies and formulae result in lower accuracy or lower validity when applied to patients outside the initial scope of development or validation. Our clinical recommendations caution clinical users in completely trusting result validity and suggest correlating with other measurement modalities robust to the DDoH mechanism (e.g., arterial blood gas for pulse oximetry, core temperatures for NCIT). Our research recommendations sugg...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10569586"
    },
    {
      "pmid": "37884627",
      "title": "The value of standards for health datasets in artificial intelligence-based applications.",
      "abstract": "Artificial intelligence as a medical device is increasingly being applied to healthcare for diagnosis, risk stratification and resource allocation. However, a growing body of evidence has highlighted the risk of algorithmic bias, which may perpetuate existing health inequity. This problem arises in part because of systemic inequalities in dataset curation, unequal opportunity to participate in research and inequalities of access. This study aims to explore existing standards, frameworks and best practices for ensuring adequate data diversity in health datasets. Exploring the body of existing literature and expert views is an important step towards the development of consensus-based guidelines. The study comprises two parts: a systematic review of existing standards, frameworks and best practices for healthcare datasets; and a survey and thematic analysis of stakeholder views of bias, health equity and best practices for artificial intelligence as a medical device. We found that the need for dataset diversity was well described in literature, and experts generally favored the development of a robust set of guidelines, but there were mixed views about how these could be implemented practically. The outputs of this study will be used to inform the development of standards for transparency of data diversity in health datasets (the STANDING Together initiative).",
      "journal": "Nature medicine",
      "year": "2023",
      "doi": "10.1038/s41591-023-02608-w",
      "authors": "Arora Anmol et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Consensus; Delivery of Health Care; Systematic Reviews as Topic",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37884627/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Fairness Metrics Evaluation; Diverse/Representative Data",
      "clinical_setting": "Not specified",
      "key_findings": "We found that the need for dataset diversity was well described in literature, and experts generally favored the development of a robust set of guidelines, but there were mixed views about how these could be implemented practically. The outputs of this study will be used to inform the development of standards for transparency of data diversity in health datasets (the STANDING Together initiative).",
      "ft_include": true,
      "ft_reason": "Included: substantial bias/fairness methodology content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10667100"
    },
    {
      "pmid": "37886534",
      "title": "FaIRClocks: Fair and Interpretable Representation of the Clock Drawing Test for mitigating classifier bias against lower educational groups.",
      "abstract": "The clock drawing test (CDT) is a neuropsychological assessment tool to evaluate a patient's cognitive ability. In this study, we developed a Fair and Interpretable Representation of Clock drawing tests (FaIRClocks) to evaluate and mitigate bias against people with lower education while predicting their cognitive status. We represented clock drawings with a 10-dimensional latent embedding using Relevance Factor Variational Autoencoder (RF-VAE) network pretrained on publicly available clock drawings from the National Health and Aging Trends Study (NHATS) dataset. These embeddings were later fine-tuned for predicting three cognitive scores: the Mini-Mental State Examination (MMSE) total score, attention composite z-score (ATT-C), and memory composite z-score (MEM-C). The classifiers were initially tested to see their relative performance in patients with low education (<= 8 years) versus patients with higher education (> 8 years). Results indicated that the initial unweighted classifiers confounded lower education with cognitive impairment, resulting in a 100% type I error rate for this group. Thereby, the samples were re-weighted using multiple fairness metrics to achieve balanced performance. In summary, we report the FaIRClocks model, which a) can identify attention and memory deficits using clock drawings and b) exhibits identical performance between people with higher and lower education levels.",
      "journal": "Research square",
      "year": "2023",
      "doi": "10.21203/rs.3.rs-3398970/v1",
      "authors": "Zhang Jiaqing et al.",
      "keywords": "AI Fairness; Mini-mental state examination; Relevance Factor Variational Autoencoder; attention; memory; semi-supervised deep learning",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37886534/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Disability",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Representation Learning; Transfer Learning; Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Thereby, the samples were re-weighted using multiple fairness metrics to achieve balanced performance. In summary, we report the FaIRClocks model, which a) can identify attention and memory deficits using clock drawings and b) exhibits identical performance between people with higher and lower education levels.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10602062"
    },
    {
      "pmid": "37902833",
      "title": "Architectural Design of a Blockchain-Enabled, Federated Learning Platform for Algorithmic Fairness in Predictive Health Care: Design Science Study.",
      "abstract": "BACKGROUND: Developing effective and generalizable predictive models is critical for disease prediction and clinical decision-making, often requiring diverse samples to mitigate population bias and address algorithmic fairness. However, a major challenge is to retrieve learning models across multiple institutions without bringing in local biases and inequity, while preserving individual patients' privacy at each site. OBJECTIVE: This study aims to understand the issues of bias and fairness in the machine learning process used in the predictive health care domain. We proposed a software architecture that integrates federated learning and blockchain to improve fairness, while maintaining acceptable prediction accuracy and minimizing overhead costs. METHODS: We improved existing federated learning platforms by integrating blockchain through an iterative design approach. We used the design science research method, which involves 2 design cycles (federated learning for bias mitigation and decentralized architecture). The design involves a bias-mitigation process within the blockchain-empowered federated learning framework based on a novel architecture. Under this architecture, multiple medical institutions can jointly train predictive models using their privacy-protected data effectively and efficiently and ultimately achieve fairness in decision-making in the health care domain. RESULTS: We designed and implemented our solution using the Aplos smart contract, microservices, Rahasak blockchain, and Apache Cassandra-based distributed storage. By conducting 20,000 local model training iterations and 1000 federated model training iterations across 5 simulated medical centers as peers in the Rahasak blockchain network, we demonstrated how our solution with an improved fairness mechanism can enhance the accuracy of predictive diagnosis. CONCLUSIONS: Our study identified the technical challenges of prediction biases faced by existing predictive models in the health care domain. To overcome these challenges, we presented an innovative design solution using federated learning and blockchain, along with the adoption of a unique distributed architecture for a fairness-aware system. We have illustrated how this design can address privacy, security, prediction accuracy, and scalability challenges, ultimately improving fairness and equity in the predictive health care domain.",
      "journal": "Journal of medical Internet research",
      "year": "2023",
      "doi": "10.2196/46547",
      "authors": "Liang Xueping et al.",
      "keywords": "bias; blockchain; fairness; federated learning; health care; implementation; privacy; proof of concept; software",
      "mesh_terms": "Humans; Blockchain; Hospitals; Awareness; Clinical Decision-Making; Machine Learning",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37902833/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Federated Learning; Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Federated Learning",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: Our study identified the technical challenges of prediction biases faced by existing predictive models in the health care domain. To overcome these challenges, we presented an innovative design solution using federated learning and blockchain, along with the adoption of a unique distributed architecture for a fairness-aware system. We have illustrated how this design can address privacy, security, prediction accuracy, and scalability challenges, ultimately improving fairness and equ...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10644196"
    },
    {
      "pmid": "37921762",
      "title": "Fairness of Machine Learning Algorithms for Predicting Foregone Preventive Dental Care for Adults.",
      "abstract": "IMPORTANCE: Access to routine dental care prevents advanced dental disease and improves oral and overall health. Identifying individuals at risk of foregoing preventive dental care can direct prevention efforts toward high-risk populations. OBJECTIVE: To predict foregone preventive dental care among adults overall and in sociodemographic subgroups and to assess the algorithmic fairness. DESIGN, SETTING, AND PARTICIPANTS: This prognostic study was a secondary analyses of longitudinal data from the US Medical Expenditure Panel Survey (MEPS) from 2016 to 2019, each with 2 years of follow-up. Participants included adults aged 18 years and older. Data analysis was performed from December 2022 to June 2023. EXPOSURE: A total of 50 predictors, including demographic and socioeconomic characteristics, health conditions, behaviors, and health services use, were assessed. MAIN OUTCOMES AND MEASURES: The outcome of interest was foregoing preventive dental care, defined as either cleaning, general examination, or an appointment with the dental hygienist, in the past year. RESULTS: Among 32\u202f234 participants, the mean (SD) age was 48.5 (18.2) years and 17\u202f386 participants (53.9%) were female; 1935 participants (6.0%) were Asian, 5138 participants (15.9%) were Black, 7681 participants (23.8%) were Hispanic, 16\u202f503 participants (51.2%) were White, and 977 participants (3.0%) identified as other (eg, American Indian and Alaska Native) or multiple racial or ethnic groups. There were 21\u202f083 (65.4%) individuals who missed preventive dental care in the past year. The algorithms demonstrated high performance, achieving an area under the receiver operating characteristic curve (AUC) of 0.84 (95% CI, 0.84-0.85) in the overall population. While the full sample model performed similarly when applied to White individuals and older adults (AUC, 0.88; 95% CI, 0.87-0.90), there was a loss of performance for other subgroups. Removing the subgroup-sensitive predictors (ie, race and ethnicity, age, and income) did not impact model performance. Models stratified by race and ethnicity performed similarly or worse than the full model for all groups, with the lowest performance for individuals who identified as other or multiple racial groups (AUC, 0.76; 95% CI, 0.70-0.81). Previous pattern of dental visits, health care utilization, dental benefits, and sociodemographic characteristics were the highest contributing predictors to the models' performance. CONCLUSIONS AND RELEVANCE: Findings of this prognostic study using cohort data suggest that tree-based ensemble machine learning models could accurately predict adults at risk of foregoing preventive dental care and demonstrated bias against underrepresented sociodemographic groups. These results highlight the importance of evaluating model fairness during development and testing to avoid exacerbating existing biases.",
      "journal": "JAMA network open",
      "year": "2023",
      "doi": "10.1001/jamanetworkopen.2023.41625",
      "authors": "Schuch Helena Silveira et al.",
      "keywords": "",
      "mesh_terms": "Humans; Aged; Ethnicity; Racial Groups; Algorithms; Machine Learning; Dental Care",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37921762/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Ensemble Methods",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Ensemble Methods",
      "clinical_setting": "Public Health/Population",
      "key_findings": "RESULTS: Among 32\u202f234 participants, the mean (SD) age was 48.5 (18.2) years and 17\u202f386 participants (53.9%) were female; 1935 participants (6.0%) were Asian, 5138 participants (15.9%) were Black, 7681 participants (23.8%) were Hispanic, 16\u202f503 participants (51.2%) were White, and 977 participants (3.0%) identified as other (eg, American Indian and Alaska Native) or multiple racial or ethnic groups. There were 21\u202f083 (65.4%) individuals who missed preventive dental care in the past year. The algo...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10625037"
    },
    {
      "pmid": "37990734",
      "title": "Improving Fairness in AI Models on Electronic Health Records: The Case for Federated Learning Methods.",
      "abstract": "Developing AI tools that preserve fairness is of critical importance, specifically in high-stakes applications such as those in healthcare. However, health AI models' overall prediction performance is often prioritized over the possible biases such models could have. In this study, we show one possible approach to mitigate bias concerns by having healthcare institutions collaborate through a federated learning paradigm (FL; which is a popular choice in healthcare settings). While FL methods with an emphasis on fairness have been previously proposed, their underlying model and local implementation techniques, as well as their possible applications to the healthcare domain remain widely underinvestigated. Therefore, we propose a comprehensive FL approach with adversarial debiasing and a fair aggregation method, suitable to various fairness metrics, in the healthcare domain where electronic health records are used. Not only our approach explicitly mitigates bias as part of the optimization process, but an FL-based paradigm would also implicitly help with addressing data imbalance and increasing the data size, offering a practical solution for healthcare applications. We empirically demonstrate our method's superior performance on multiple experiments simulating large-scale real-world scenarios and compare it to several baselines. Our method has achieved promising fairness performance with the lowest impact on overall discrimination performance (accuracy). Our code is available at https://github.com/healthylaife/FairFedAvg.",
      "journal": "FAccT '23 : Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. Association for Computing Machinery",
      "year": "2023",
      "doi": "10.1145/3593013.3594102",
      "authors": "Poulain Raphael et al.",
      "keywords": "Adversarial Fairness; Algorithmic Fairness; Federated Learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37990734/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Federated Learning",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Adversarial Debiasing; Fairness Metrics Evaluation; Federated Learning",
      "clinical_setting": "Not specified",
      "key_findings": "Our method has achieved promising fairness performance with the lowest impact on overall discrimination performance (accuracy). Our code is available at https://github.com/healthylaife/FairFedAvg.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10661580"
    },
    {
      "pmid": "38014176",
      "title": "DeepN4: Learning N4ITK Bias Field Correction for T1-weighted Images.",
      "abstract": "T1-weighted (T1w) MRI has low frequency intensity artifacts due to magnetic field inhomogeneities. Removal of these biases in T1w MRI images is a critical preprocessing step to ensure spatially consistent image interpretation. N4ITK bias field correction, the current state-of-the-art, is implemented in such a way that makes it difficult to port between different pipelines and workflows, thus making it hard to reimplement and reproduce results across local, cloud, and edge platforms. Moreover, N4ITK is opaque to optimization before and after its application, meaning that methodological development must work around the inhomogeneity correction step. Given the importance of bias fields correction in structural preprocessing and flexible implementation, we pursue a deep learning approximation / reinterpretation of the N4ITK bias fields correction to create a method which is portable, flexible, and fully differentiable. In this paper, we trained a deep learning network \"DeepN4\" on eight independent cohorts from 72 different scanners and age ranges with N4ITK-corrected T1w MRI and bias field for supervision in log space. We found that we can closely approximate N4ITK bias fields correction with na\u00efve networks. We evaluate the peak signal to noise ratio (PSNR) in test dataset against the N4ITK corrected images. The median PSNR of corrected images between N4ITK and DeepN4 was 47.96 dB. In addition, we assess the DeepN4 model on eight additional external datasets and show the generalizability of the approach. This study establishes that incompatible N4ITK preprocessing steps can be closely approximated by na\u00efve deep neural networks, facilitating more flexibility. All code and models are released at https://github.com/MASILab/DeepN4.",
      "journal": "Research square",
      "year": "2023",
      "doi": "10.21203/rs.3.rs-3585882/v1",
      "authors": "Kanakaraj Praitayini et al.",
      "keywords": "3D U-Net; N4ITK; T1-weighted images; bias field correction; inhomogeneity",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38014176/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "This study establishes that incompatible N4ITK preprocessing steps can be closely approximated by na\u00efve deep neural networks, facilitating more flexibility. All code and models are released at https://github.com/MASILab/DeepN4.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10680935"
    },
    {
      "pmid": "38074789",
      "title": "Risk of Bias in Chest Radiography Deep Learning Foundation Models.",
      "abstract": "PURPOSE: To analyze a recently published chest radiography foundation model for the presence of biases that could lead to subgroup performance disparities across biologic sex and race. MATERIALS AND METHODS: This Health Insurance Portability and Accountability Act-compliant retrospective study used 127\u2009118 chest radiographs from 42\u2009884 patients (mean age, 63 years \u00b1 17 [SD]; 23\u2009623 male, 19\u2009261 female) from the CheXpert dataset that were collected between October 2002 and July 2017. To determine the presence of bias in features generated by a chest radiography foundation model and baseline deep learning model, dimensionality reduction methods together with two-sample Kolmogorov-Smirnov tests were used to detect distribution shifts across sex and race. A comprehensive disease detection performance analysis was then performed to associate any biases in the features to specific disparities in classification performance across patient subgroups. RESULTS: Ten of 12 pairwise comparisons across biologic sex and race showed statistically significant differences in the studied foundation model, compared with four significant tests in the baseline model. Significant differences were found between male and female (P < .001) and Asian and Black (P < .001) patients in the feature projections that primarily capture disease. Compared with average model performance across all subgroups, classification performance on the \"no finding\" label decreased between 6.8% and 7.8% for female patients, and performance in detecting \"pleural effusion\" decreased between 10.7% and 11.6% for Black patients. CONCLUSION: The studied chest radiography foundation model demonstrated racial and sex-related bias, which led to disparate performance across patient subgroups; thus, this model may be unsafe for clinical applications.Keywords: Conventional Radiography, Computer Application-Detection/Diagnosis, Chest Radiography, Bias, Foundation Models Supplemental material is available for this article. Published under a CC BY 4.0 license.See also commentary by Czum and Parr in this issue.",
      "journal": "Radiology. Artificial intelligence",
      "year": "2023",
      "doi": "10.1148/ryai.230060",
      "authors": "Glocker Ben et al.",
      "keywords": "Bias; Chest Radiography; Computer Application-Detection/Diagnosis; Conventional Radiography; Foundation Models",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38074789/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Deep Learning; Foundation Model",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: The studied chest radiography foundation model demonstrated racial and sex-related bias, which led to disparate performance across patient subgroups; thus, this model may be unsafe for clinical applications.Keywords: Conventional Radiography, Computer Application-Detection/Diagnosis, Chest Radiography, Bias, Foundation Models Supplemental material is available for this article. Published under a CC BY 4.0 license.See also commentary by Czum and Parr in this issue.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10698597"
    },
    {
      "pmid": "38075950",
      "title": "The AI cycle of health inequity and digital ageism: mitigating biases through the EU regulatory framework on medical devices.",
      "abstract": "The use of Artificial Intelligence (AI) medical devices is rapidly growing. Although AI may benefit the quality and safety of healthcare for older adults, it simultaneously introduces new ethical and legal issues. Many AI medical devices exhibit age-related biases. The first part of this paper explains how 'digital ageism' is produced throughout the entire lifecycle of medical AI and may lead to health inequity for older people: systemic, avoidable differences in the health status of different population groups. This paper takes digital ageism as a use case to show the potential inequitable effects of AI, conceptualized as the 'AI cycle of health inequity'. The second part of this paper explores how the European Union (EU) regulatory framework addresses the issue of digital ageism. It argues that the negative effects of age-related bias in AI medical devices are insufficiently recognized within the regulatory framework of the EU Medical Devices Regulation and the new AI Act. It concludes that while the EU framework does address some of the key issues related to technical biases in AI medical devices by stipulating rules for performance and data quality, it does not account for contextual biases, therefore neglecting part of the AI cycle of health inequity.",
      "journal": "Journal of law and the biosciences",
      "year": "2023",
      "doi": "10.1093/jlb/lsad031",
      "authors": "van Kolfschooten Hannah",
      "keywords": "EU regulation; ageism; artificial intelligence; bias; discrimination; medical devices",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38075950/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "It argues that the negative effects of age-related bias in AI medical devices are insufficiently recognized within the regulatory framework of the EU Medical Devices Regulation and the new AI Act. It concludes that while the EU framework does address some of the key issues related to technical biases in AI medical devices by stipulating rules for performance and data quality, it does not account for contextual biases, therefore neglecting part of the AI cycle of health inequity.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10709664"
    },
    {
      "pmid": "38076212",
      "title": "Critical Appraisal for Racial and Ethnic Equity in Clinical Prediction Models Extension: Development of a Critical Appraisal Tool Extension to Assess Racial and Ethnic Equity-Related Risk of Bias for Clinical Prediction Models.",
      "abstract": "INTRODUCTION: Despite mounting evidence that the inclusion of race and ethnicity in clinical prediction models may contribute to health disparities, existing critical appraisal tools do not directly address such equity considerations. OBJECTIVE: This study developed a critical appraisal tool extension to assess algorithmic bias in clinical prediction models. METHODS: A modified e-Delphi approach was utilized to develop and obtain expert consensus on a set of racial and ethnic equity-based signaling questions for appraisal of risk of bias in clinical prediction models. Through a series of virtual meetings, initial pilot application, and an online survey, individuals with expertise in clinical prediction model development, systematic review methodology, and health equity developed and refined this tool. RESULTS: Consensus was reached for ten equity-based signaling questions, which led to the development of the Critical Appraisal for Racial and Ethnic Equity in Clinical Prediction Models (CARE-CPM) extension. This extension is intended for use along with existing critical appraisal tools for clinical prediction models. CONCLUSION: CARE-CPM provides a valuable risk-of-bias assessment tool extension for clinical prediction models to identify potential algorithmic bias and health equity concerns. Further research is needed to test usability, interrater reliability, and application to decision-makers.",
      "journal": "Health equity",
      "year": "2023",
      "doi": "10.1089/heq.2023.0035",
      "authors": "Siddique Shazia M et al.",
      "keywords": "critical appraisal; guideline development; health equity; risk of bias; systematic review",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38076212/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: CARE-CPM provides a valuable risk-of-bias assessment tool extension for clinical prediction models to identify potential algorithmic bias and health equity concerns. Further research is needed to test usability, interrater reliability, and application to decision-makers.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10698767"
    },
    {
      "pmid": "38076213",
      "title": "Eliminating Algorithmic Racial Bias in Clinical Decision Support Algorithms: Use Cases from the Veterans Health Administration.",
      "abstract": "The Veterans Health Administration uses equity- and evidence-based principles to examine, correct, and eliminate use of potentially biased clinical equations and predictive models. We discuss the processes, successes, challenges, and next steps in four examples. We detail elimination of the race modifier for estimated kidney function and discuss steps to achieve more equitable pulmonary function testing measurement. We detail the use of equity lenses in two predictive clinical modeling tools: Stratification Tool for Opioid Risk Mitigation (STORM) and Care Assessment Need (CAN) predictive models. We conclude with consideration of ways to advance racial health equity in clinical decision support algorithms.",
      "journal": "Health equity",
      "year": "2023",
      "doi": "10.1089/heq.2023.0037",
      "authors": "List Justin M et al.",
      "keywords": "Veterans Health Administration; clinical equations; health equity; predictive models; racial bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38076213/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model; Clinical Decision Support",
      "health_domain": "Mental Health/Psychiatry; Nephrology; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We detail the use of equity lenses in two predictive clinical modeling tools: Stratification Tool for Opioid Risk Mitigation (STORM) and Care Assessment Need (CAN) predictive models. We conclude with consideration of ways to advance racial health equity in clinical decision support algorithms.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10698768"
    },
    {
      "pmid": "38100101",
      "title": "Guiding Principles to Address the Impact of Algorithm Bias on Racial and Ethnic Disparities in Health and Health Care.",
      "abstract": "IMPORTANCE: Health care algorithms are used for diagnosis, treatment, prognosis, risk stratification, and allocation of resources. Bias in the development and use of algorithms can lead to worse outcomes for racial and ethnic minoritized groups and other historically marginalized populations such as individuals with lower income. OBJECTIVE: To provide a conceptual framework and guiding principles for mitigating and preventing bias in health care algorithms to promote health and health care equity. EVIDENCE REVIEW: The Agency for Healthcare Research and Quality and the National Institute for Minority Health and Health Disparities convened a diverse panel of experts to review evidence, hear from stakeholders, and receive community feedback. FINDINGS: The panel developed a conceptual framework to apply guiding principles across an algorithm's life cycle, centering health and health care equity for patients and communities as the goal, within the wider context of structural racism and discrimination. Multiple stakeholders can mitigate and prevent bias at each phase of the algorithm life cycle, including problem formulation (phase 1); data selection, assessment, and management (phase 2); algorithm development, training, and validation (phase 3); deployment and integration of algorithms in intended settings (phase 4); and algorithm monitoring, maintenance, updating, or deimplementation (phase 5). Five principles should guide these efforts: (1) promote health and health care equity during all phases of the health care algorithm life cycle; (2) ensure health care algorithms and their use are transparent and explainable; (3) authentically engage patients and communities during all phases of the health care algorithm life cycle and earn trustworthiness; (4) explicitly identify health care algorithmic fairness issues and trade-offs; and (5) establish accountability for equity and fairness in outcomes from health care algorithms. CONCLUSIONS AND RELEVANCE: Multiple stakeholders must partner to create systems, processes, regulations, incentives, standards, and policies to mitigate and prevent algorithmic bias. Reforms should implement guiding principles that support promotion of health and health care equity in all phases of the algorithm life cycle as well as transparency and explainability, authentic community engagement and ethical partnerships, explicit identification of fairness issues and trade-offs, and accountability for equity and fairness.",
      "journal": "JAMA network open",
      "year": "2023",
      "doi": "10.1001/jamanetworkopen.2023.45050",
      "authors": "Chin Marshall H et al.",
      "keywords": "",
      "mesh_terms": "United States; Humans; Health Promotion; Racial Groups; Academies and Institutes; Algorithms; Health Equity",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38100101/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "FINDINGS: The panel developed a conceptual framework to apply guiding principles across an algorithm's life cycle, centering health and health care equity for patients and communities as the goal, within the wider context of structural racism and discrimination. Multiple stakeholders can mitigate and prevent bias at each phase of the algorithm life cycle, including problem formulation (phase 1); data selection, assessment, and management (phase 2); algorithm development, training, and validation...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11181958"
    },
    {
      "pmid": "38132653",
      "title": "Deep Learning Paradigm and Its Bias for Coronary Artery Wall Segmentation in Intravascular Ultrasound Scans: A Closer Look.",
      "abstract": "BACKGROUND AND MOTIVATION: Coronary artery disease (CAD) has the highest mortality rate; therefore, its diagnosis is vital. Intravascular ultrasound (IVUS) is a high-resolution imaging solution that can image coronary arteries, but the diagnosis software via wall segmentation and quantification has been evolving. In this study, a deep learning (DL) paradigm was explored along with its bias. METHODS: Using a PRISMA model, 145 best UNet-based and non-UNet-based methods for wall segmentation were selected and analyzed for their characteristics and scientific and clinical validation. This study computed the coronary wall thickness by estimating the inner and outer borders of the coronary artery IVUS cross-sectional scans. Further, the review explored the bias in the DL system for the first time when it comes to wall segmentation in IVUS scans. Three bias methods, namely (i) ranking, (ii) radial, and (iii) regional area, were applied and compared using a Venn diagram. Finally, the study presented explainable AI (XAI) paradigms in the DL framework. FINDINGS AND CONCLUSIONS: UNet provides a powerful paradigm for the segmentation of coronary walls in IVUS scans due to its ability to extract automated features at different scales in encoders, reconstruct the segmented image using decoders, and embed the variants in skip connections. Most of the research was hampered by a lack of motivation for XAI and pruned AI (PAI) models. None of the UNet models met the criteria for bias-free design. For clinical assessment and settings, it is necessary to move from a paper-to-practice approach.",
      "journal": "Journal of cardiovascular development and disease",
      "year": "2023",
      "doi": "10.3390/jcdd10120485",
      "authors": "Kumari Vandana et al.",
      "keywords": "AI bias; UNet; coronary artery disease; deep learning; intravascular ultrasound; wall segmentation",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38132653/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Radiology/Medical Imaging; Cardiology; Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: UNet provides a powerful paradigm for the segmentation of coronary walls in IVUS scans due to its ability to extract automated features at different scales in encoders, reconstruct the segmented image using decoders, and embed the variants in skip connections. Most of the research was hampered by a lack of motivation for XAI and pruned AI (PAI) models. None of the UNet models met the criteria for bias-free design.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10743870"
    },
    {
      "pmid": "38875587",
      "title": "Strategies to Improve the Impact of Artificial Intelligence on Health Equity: Scoping Review.",
      "abstract": "BACKGROUND: Emerging artificial intelligence (AI) applications have the potential to improve health, but they may also perpetuate or exacerbate inequities. OBJECTIVE: This review aims to provide a comprehensive overview of the health equity issues related to the use of AI applications and identify strategies proposed to address them. METHODS: We searched PubMed, Web of Science, the IEEE (Institute of Electrical and Electronics Engineers) Xplore Digital Library, ProQuest U.S. Newsstream, Academic Search Complete, the Food and Drug Administration (FDA) website, and ClinicalTrials.gov to identify academic and gray literature related to AI and health equity that were published between 2014 and 2021 and additional literature related to AI and health equity during the COVID-19 pandemic from 2020 and 2021. Literature was eligible for inclusion in our review if it identified at least one equity issue and a corresponding strategy to address it. To organize and synthesize equity issues, we adopted a 4-step AI application framework: Background Context, Data Characteristics, Model Design, and Deployment. We then created a many-to-many mapping of the links between issues and strategies. RESULTS: In 660 documents, we identified 18 equity issues and 15 strategies to address them. Equity issues related to Data Characteristics and Model Design were the most common. The most common strategies recommended to improve equity were improving the quantity and quality of data, evaluating the disparities introduced by an application, increasing model reporting and transparency, involving the broader community in AI application development, and improving governance. CONCLUSIONS: Stakeholders should review our many-to-many mapping of equity issues and strategies when planning, developing, and implementing AI applications in health care so that they can make appropriate plans to ensure equity for populations affected by their products. AI application developers should consider adopting equity-focused checklists, and regulators such as the FDA should consider requiring them. Given that our review was limited to documents published online, developers may have unpublished knowledge of additional issues and strategies that we were unable to identify.",
      "journal": "JMIR AI",
      "year": "2023",
      "doi": "10.2196/42936",
      "authors": "Berdahl Carl Thomas et al.",
      "keywords": "algorithmic bias; algorithms; artificial intelligence; decision making; equity; gray literature; health care disparities; health data; health equity; machine learning; social determinants of health",
      "mesh_terms": "",
      "pub_types": "Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38875587/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Generative AI",
      "health_domain": "Pulmonology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: Stakeholders should review our many-to-many mapping of equity issues and strategies when planning, developing, and implementing AI applications in health care so that they can make appropriate plans to ensure equity for populations affected by their products. AI application developers should consider adopting equity-focused checklists, and regulators such as the FDA should consider requiring them. Given that our review was limited to documents published online, developers may have u...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11041459"
    },
    {
      "pmid": "33559881",
      "title": "EMBRACE: An EM-based bias reduction approach through Copas-model estimation for quantifying the evidence of selective publishing in network meta-analysis.",
      "abstract": "Systematic reviews and meta-analyses synthesize results from well-conducted studies to optimize healthcare decision-making. Network meta-analysis (NMA) is particularly useful for improving precision, drawing new comparisons, and ranking multiple interventions. However, recommendations can be misled if published results are a selective sample of what has been collected by trialists, particularly when publication status is related to the significance of the findings. Unfortunately, the missing-not-at-random nature of this problem and the numerous parameters involved in modeling NMAs pose unique computational challenges to quantifying and correcting for publication bias, such that sensitivity analysis is used in practice. Motivated by this important methodological gap, we developed a novel and stable expectation-maximization (EM) algorithm to correct for publication bias in the network setting. We validate the method through simulation studies and show that it achieves substantial bias reduction in small to moderately sized NMAs. We also calibrate the method against a Bayesian analysis of a published NMA on antiplatlet therapies for maintaining vascular\u00a0patency.",
      "journal": "Biometrics",
      "year": "2022",
      "doi": "10.1111/biom.13441",
      "authors": "Marks-Anglin Arielle et al.",
      "keywords": "Copas model; EM algorithm; missing data; multiarm trial; network meta-analysis; publication bias",
      "mesh_terms": "Bayes Theorem; Bias; Publication Bias; Research Design",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Network Meta-Analysis",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33559881/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "We validate the method through simulation studies and show that it achieves substantial bias reduction in small to moderately sized NMAs. We also calibrate the method against a Bayesian analysis of a published NMA on antiplatlet therapies for maintaining vascular\u00a0patency.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "34772497",
      "title": "Artificial intelligence for mechanical ventilation: systematic review of design, reporting standards, and bias.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) has the potential to personalise mechanical ventilation strategies for patients with respiratory failure. However, current methodological deficiencies could limit clinical impact. We identified common limitations and propose potential solutions to facilitate translation of AI to mechanical ventilation of patients. METHODS: A systematic review was conducted in MEDLINE, Embase, and PubMed Central to February 2021. Studies investigating the application of AI to patients undergoing mechanical ventilation were included. Algorithm design and adherence to reporting standards were assessed with a rubric combining published guidelines, satisfying the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis [TRIPOD] statement. Risk of bias was assessed by using the Prediction model Risk Of Bias ASsessment Tool (PROBAST), and correspondence with authors to assess data and code availability. RESULTS: Our search identified 1,342 studies, of which 95 were included: 84 had single-centre, retrospective study design, with only one randomised controlled trial. Access to data sets and code was severely limited (unavailable in 85% and 87% of studies, respectively). On request, data and code were made available from 12 and 10 authors, respectively, from a list of 54 studies published in the last 5 yr. Ethnicity was frequently under-reported 18/95 (19%), as was model calibration 17/95 (18%). The risk of bias was high in 89% (85/95) of the studies, especially because of analysis bias. CONCLUSIONS: Development of algorithms should involve prospective and external validation, with greater code and data availability to improve confidence in and translation of this promising approach. TRIAL REGISTRATION NUMBER: PROSPERO - CRD42021225918.",
      "journal": "British journal of anaesthesia",
      "year": "2022",
      "doi": "10.1016/j.bja.2021.09.025",
      "authors": "Gallifant Jack et al.",
      "keywords": "artificial intelligence; bias; critical care; decision support; mechanical ventilation respiratory failure",
      "mesh_terms": "Algorithms; Artificial Intelligence; Bias; Humans; Models, Theoretical; Randomized Controlled Trials as Topic; Research Design; Research Report; Respiration, Artificial; Respiratory Insufficiency",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34772497/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "ICU/Critical Care; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Calibration",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Development of algorithms should involve prospective and external validation, with greater code and data availability to improve confidence in and translation of this promising approach. TRIAL REGISTRATION NUMBER: PROSPERO - CRD42021225918.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8792831"
    },
    {
      "pmid": "35012941",
      "title": "Conceptualising fairness: three pillars for medical algorithms and health equity.",
      "abstract": "OBJECTIVES: Fairness is a core concept meant to grapple with different forms of discrimination and bias that emerge with advances in Artificial Intelligence (eg, machine learning, ML). Yet, claims to fairness in ML discourses are often vague and contradictory. The response to these issues within the scientific community has been technocratic. Studies either measure (mathematically) competing definitions of fairness, and/or recommend a range of governance tools (eg, fairness checklists or guiding principles). To advance efforts to operationalise fairness in medicine, we synthesised a broad range of literature. METHODS: We conducted an environmental scan of English language literature on fairness from 1960-July 31, 2021. Electronic databases Medline, PubMed and Google Scholar were searched, supplemented by additional hand searches. Data from 213 selected publications were analysed using rapid framework analysis. Search and analysis were completed in two rounds: to explore previously identified issues (a priori), as well as those emerging from the analysis (de novo). RESULTS: Our synthesis identified 'Three Pillars for Fairness': transparency, impartiality and inclusion. We draw on these insights to propose a multidimensional conceptual framework to guide empirical research on the operationalisation of fairness in healthcare. DISCUSSION: We apply the conceptual framework generated by our synthesis to risk assessment in psychiatry as a case study. We argue that any claim to fairness must reflect critical assessment and ongoing social and political deliberation around these three pillars with a range of stakeholders, including patients. CONCLUSION: We conclude by outlining areas for further research that would bolster ongoing commitments to fairness and health equity in healthcare.",
      "journal": "BMJ health & care informatics",
      "year": "2022",
      "doi": "10.1136/bmjhci-2021-100459",
      "authors": "Sikstrom Laura et al.",
      "keywords": "artificial intelligence; health equity; health services research; machine learning; patient-centered care",
      "mesh_terms": "Artificial Intelligence; Delivery of Health Care; Health Equity; Humans; Machine Learning; Risk Assessment",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35012941/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: We conclude by outlining areas for further research that would bolster ongoing commitments to fairness and health equity in healthcare.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8753410"
    },
    {
      "pmid": "35075160",
      "title": "Peeking into a black box, the fairness and generalizability of a MIMIC-III benchmarking model.",
      "abstract": "As artificial intelligence (AI) makes continuous progress to improve quality of care for some patients by leveraging ever increasing amounts of digital health data, others are left behind. Empirical evaluation studies are required to keep biased AI models from reinforcing systemic health disparities faced by minority populations through dangerous feedback loops. The aim of this study is to raise broad awareness of the pervasive challenges around bias and fairness in risk prediction models. We performed a case study on a MIMIC-trained benchmarking model using a broadly applicable fairness and generalizability assessment framework. While open-science benchmarks are crucial to overcome many study limitations today, this case study revealed a strong class imbalance problem as well as fairness concerns for Black and publicly insured ICU patients. Therefore, we advocate for the widespread use of comprehensive fairness and performance assessment frameworks to effectively monitor and validate benchmark pipelines built on open data resources.",
      "journal": "Scientific data",
      "year": "2022",
      "doi": "10.1038/s41597-021-01110-7",
      "authors": "R\u00f6\u00f6sli Eliane et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35075160/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "While open-science benchmarks are crucial to overcome many study limitations today, this case study revealed a strong class imbalance problem as well as fairness concerns for Black and publicly insured ICU patients. Therefore, we advocate for the widespread use of comprehensive fairness and performance assessment frameworks to effectively monitor and validate benchmark pipelines built on open data resources.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8786878"
    },
    {
      "pmid": "35259009",
      "title": "Eliminating unintended bias in personalized policies using bias-eliminating adapted trees (BEAT).",
      "abstract": "SignificanceDecision makers now use algorithmic personalization for resource allocation decisions in many domains (e.g., medical treatments, hiring decisions, product recommendations, or dynamic pricing). An inherent risk of personalization is disproportionate targeting of individuals from certain protected groups. Existing solutions that firms use to avoid this bias often do not eliminate the bias and may even exacerbate it. We propose BEAT (bias-eliminating adapted trees) to ensure balanced allocation of resources across individuals-guaranteeing both group and individual fairness-while still leveraging the value of personalization. We validate our method using simulations as well as an online experiment with N = 3,146 participants. BEAT is easy to implement in practice, has desirable scalability properties, and is applicable to many personalization problems.",
      "journal": "Proceedings of the National Academy of Sciences of the United States of America",
      "year": "2022",
      "doi": "10.1073/pnas.2115293119",
      "authors": "Ascarza Eva et al.",
      "keywords": "algorithmic bias; discrimination; fairness; personalization; targeting",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35259009/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We validate our method using simulations as well as an online experiment with N = 3,146 participants. BEAT is easy to implement in practice, has desirable scalability properties, and is applicable to many personalization problems.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8931211"
    },
    {
      "pmid": "35396247",
      "title": "Evaluating algorithmic fairness in the presence of clinical guidelines: the case of atherosclerotic cardiovascular disease risk estimation.",
      "abstract": "OBJECTIVES: The American College of Cardiology and the American Heart Association guidelines on primary prevention of atherosclerotic cardiovascular disease (ASCVD) recommend using 10-year ASCVD risk estimation models to initiate statin treatment. For guideline-concordant decision-making, risk estimates need to be calibrated. However, existing models are often miscalibrated for race, ethnicity and sex based subgroups. This study evaluates two algorithmic fairness approaches to adjust the risk estimators (group recalibration and equalised odds) for their compatibility with the assumptions underpinning the guidelines' decision rules.MethodsUsing an updated pooled cohorts data set, we derive unconstrained, group-recalibrated and equalised odds-constrained versions of the 10-year ASCVD risk estimators, and compare their calibration at guideline-concordant decision thresholds. RESULTS: We find that, compared with the unconstrained model, group-recalibration improves calibration at one of the relevant thresholds for each group, but exacerbates differences in false positive and false negative rates between groups. An equalised odds constraint, meant to equalise error rates across groups, does so by miscalibrating the model overall and at relevant decision thresholds. DISCUSSION: Hence, because of induced miscalibration, decisions guided by risk estimators learned with an equalised odds fairness constraint are not concordant with existing guidelines. Conversely, recalibrating the model separately for each group can increase guideline compatibility, while increasing intergroup differences in error rates. As such, comparisons of error rates across groups can be misleading when guidelines recommend treating at fixed decision thresholds. CONCLUSION: The illustrated tradeoffs between satisfying a fairness criterion and retaining guideline compatibility underscore the need to evaluate models in the context of downstream interventions.",
      "journal": "BMJ health & care informatics",
      "year": "2022",
      "doi": "10.1136/bmjhci-2021-100460",
      "authors": "Foryciarz Agata et al.",
      "keywords": "BMJ Health Informatics; clinical; decision support systems; health equity; machine learning; medical informatics",
      "mesh_terms": "American Heart Association; Atherosclerosis; Cardiology; Cardiovascular Diseases; Humans; Hydroxymethylglutaryl-CoA Reductase Inhibitors; United States",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35396247/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Constraints; Calibration; Threshold Adjustment",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: The illustrated tradeoffs between satisfying a fairness criterion and retaining guideline compatibility underscore the need to evaluate models in the context of downstream interventions.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8996004"
    },
    {
      "pmid": "35463778",
      "title": "Fairness in Cardiac Magnetic Resonance Imaging: Assessing Sex and Racial Bias in Deep Learning-Based Segmentation.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) techniques have been proposed for automation of cine CMR segmentation for functional quantification. However, in other applications AI models have been shown to have potential for sex and/or racial bias. The objective of this paper is to perform the first analysis of sex/racial bias in AI-based cine CMR segmentation using a large-scale database. METHODS: A state-of-the-art deep learning (DL) model was used for automatic segmentation of both ventricles and the myocardium from cine short-axis CMR. The dataset consisted of end-diastole and end-systole short-axis cine CMR images of 5,903 subjects from the UK Biobank database (61.5 \u00b1 7.1 years, 52% male, 81% white). To assess sex and racial bias, we compared Dice scores and errors in measurements of biventricular volumes and function between patients grouped by race and sex. To investigate whether segmentation bias could be explained by potential confounders, a multivariate linear regression and ANCOVA were performed. RESULTS: Results on the overall population showed an excellent agreement between the manual and automatic segmentations. We found statistically significant differences in Dice scores between races (white \u223c94% vs. minority ethnic groups 86-89%) as well as in absolute/relative errors in volumetric and functional measures, showing that the AI model was biased against minority racial groups, even after correction for possible confounders. The results of a multivariate linear regression analysis showed that no covariate could explain the Dice score bias between racial groups. However, for the Mixed and Black race groups, sex showed a weak positive association with the Dice score. The results of an ANCOVA analysis showed that race was the main factor that can explain the overall difference in Dice scores between racial groups. CONCLUSION: We have shown that racial bias can exist in DL-based cine CMR segmentation models when training with a database that is sex-balanced but not race-balanced such as the UK Biobank.",
      "journal": "Frontiers in cardiovascular medicine",
      "year": "2022",
      "doi": "10.3389/fcvm.2022.859310",
      "authors": "Puyol-Ant\u00f3n Esther et al.",
      "keywords": "cardiac magnetic resonance; deep learning; fair AI; inequality fairness in deep learning-based CMR segmentation; segmentation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35463778/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Regression",
      "health_domain": "Cardiology; ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: We have shown that racial bias can exist in DL-based cine CMR segmentation models when training with a database that is sex-balanced but not race-balanced such as the UK Biobank.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9021445"
    },
    {
      "pmid": "35504931",
      "title": "Interpretability and fairness evaluation of deep learning models on MIMIC-IV dataset.",
      "abstract": "The recent release of large-scale healthcare datasets has greatly propelled the research of data-driven deep learning models for healthcare applications. However, due to the nature of such deep black-boxed models, concerns about interpretability, fairness, and biases in healthcare scenarios where human lives are at stake call for a careful and thorough examination of both datasets and models. In this work, we focus on MIMIC-IV (Medical Information Mart for Intensive Care, version IV), the largest publicly available healthcare dataset, and conduct comprehensive analyses of interpretability as well as dataset representation bias and prediction fairness of deep learning models for in-hospital mortality prediction. First, we analyze the interpretability of deep learning mortality prediction models and observe that (1) the best-performing interpretability method successfully identifies critical features for mortality prediction on various prediction models as well as recognizing new important features that domain knowledge does not consider; (2) prediction models rely on demographic features, raising concerns in fairness. Therefore, we then evaluate the fairness of models and do observe the unfairness: (1) there exists disparate treatment in prescribing mechanical ventilation among patient groups across ethnicity, gender and age; (2) models often rely on racial attributes unequally across subgroups to generate their predictions. We further draw concrete connections between interpretability methods and fairness metrics by showing how feature importance from interpretability methods can be beneficial in quantifying potential disparities in mortality predictors. Our analysis demonstrates that the prediction performance is not the only factor to consider when evaluating models for healthcare applications, since high prediction performance might be the result of unfair utilization of demographic features. Our findings suggest that future research in AI models for healthcare applications can benefit from utilizing the analysis workflow of interpretability and fairness as well as verifying if models achieve superior performance at the cost of introducing bias.",
      "journal": "Scientific reports",
      "year": "2022",
      "doi": "10.1038/s41598-022-11012-2",
      "authors": "Meng Chuizheng et al.",
      "keywords": "",
      "mesh_terms": "Benchmarking; Critical Care; Deep Learning; Forecasting; Hospital Mortality; Humans",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35504931/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Clinical Prediction Model",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "Hospital/Inpatient; ICU",
      "key_findings": "Our analysis demonstrates that the prediction performance is not the only factor to consider when evaluating models for healthcare applications, since high prediction performance might be the result of unfair utilization of demographic features. Our findings suggest that future research in AI models for healthcare applications can benefit from utilizing the analysis workflow of interpretability and fairness as well as verifying if models achieve superior performance at the cost of introducing bi...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9065125"
    },
    {
      "pmid": "35511151",
      "title": "An objective framework for evaluating unrecognized bias in medical AI models predicting COVID-19 outcomes.",
      "abstract": "OBJECTIVE: The increasing translation of artificial intelligence (AI)/machine learning (ML) models into clinical practice brings an increased risk of direct harm from modeling bias; however, bias remains incompletely measured in many medical AI applications. This article aims to provide a framework for objective evaluation of medical AI from multiple aspects, focusing on binary classification models. MATERIALS AND METHODS: Using data from over 56\u00a0000 Mass General Brigham (MGB) patients with confirmed severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), we evaluate unrecognized bias in 4 AI models developed during the early months of the pandemic in Boston, Massachusetts that predict risks of hospital admission, ICU admission, mechanical ventilation, and death after a SARS-CoV-2 infection purely based on their pre-infection longitudinal medical records. Models were evaluated both retrospectively and prospectively using model-level metrics of discrimination, accuracy, and reliability, and a novel individual-level metric for error. RESULTS: We found inconsistent instances of model-level bias in the prediction models. From an individual-level aspect, however, we found most all models performing with slightly higher error rates for older patients. DISCUSSION: While a model can be biased against certain protected groups (ie, perform worse) in certain tasks, it can be at the same time biased towards another protected group (ie, perform better). As such, current bias evaluation studies may lack a full depiction of the variable effects of a model on its subpopulations. CONCLUSION: Only a holistic evaluation, a diligent search for unrecognized bias, can provide enough information for an unbiased judgment of AI bias that can invigorate follow-up investigations on identifying the underlying roots of bias and ultimately make a change.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2022",
      "doi": "10.1093/jamia/ocac070",
      "authors": "Estiri Hossein et al.",
      "keywords": "COVID-19; bias; electronic health records; medical AI; predictive model",
      "mesh_terms": "Artificial Intelligence; COVID-19; Humans; Reproducibility of Results; Retrospective Studies; SARS-CoV-2",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35511151/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "ICU/Critical Care; Pulmonology; Infectious Disease",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Subgroup Analysis",
      "clinical_setting": "Hospital/Inpatient; ICU; Public Health/Population",
      "key_findings": "CONCLUSION: Only a holistic evaluation, a diligent search for unrecognized bias, can provide enough information for an unbiased judgment of AI bias that can invigorate follow-up investigations on identifying the underlying roots of bias and ultimately make a change.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9277645"
    },
    {
      "pmid": "35538215",
      "title": "Algorithmic fairness in pandemic forecasting: lessons from COVID-19.",
      "abstract": "Racial and ethnic minorities have borne a particularly acute burden of the COVID-19 pandemic in the United States. There is a growing awareness from both researchers and public health leaders of the critical need to ensure fairness in forecast results. Without careful and deliberate bias mitigation, inequities embedded in data can be transferred to model predictions, perpetuating disparities, and exacerbating the disproportionate harms of the COVID-19 pandemic. These biases in data and forecasts can be viewed through both statistical and sociological lenses, and the challenges of both building hierarchical models with limited data availability and drawing on data that reflects structural inequities must be confronted. We present an outline of key modeling domains in which unfairness may be introduced and draw on our experience building and testing the Google-Harvard COVID-19 Public Forecasting model to illustrate these challenges and offer strategies to address them. While targeted toward pandemic forecasting, these domains of potentially biased modeling and concurrent approaches to pursuing fairness present important considerations for equitable machine-learning innovation.",
      "journal": "NPJ digital medicine",
      "year": "2022",
      "doi": "10.1038/s41746-022-00602-z",
      "authors": "Tsai Thomas C et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35538215/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Pulmonology; Public Health",
      "bias_axes": "Race/Ethnicity",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "We present an outline of key modeling domains in which unfairness may be introduced and draw on our experience building and testing the Google-Harvard COVID-19 Public Forecasting model to illustrate these challenges and offer strategies to address them. While targeted toward pandemic forecasting, these domains of potentially biased modeling and concurrent approaches to pursuing fairness present important considerations for equitable machine-learning innovation.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9090910"
    },
    {
      "pmid": "35579522",
      "title": "Radiomic Analysis: Study Design, Statistical Analysis, and Other Bias Mitigation Strategies.",
      "abstract": "Rapid advances in automated methods for extracting large numbers of quantitative features from medical images have led to tremendous growth of publications reporting on radiomic analyses. Translation of these research studies into clinical practice can be hindered by biases introduced during the design, analysis, or reporting of the studies. Herein, the authors review biases, sources of variability, and pitfalls that frequently arise in radiomic research, with an emphasis on study design and statistical analysis considerations. Drawing on existing work in the statistical, radiologic, and machine learning literature, approaches for avoiding these pitfalls are described.",
      "journal": "Radiology",
      "year": "2022",
      "doi": "10.1148/radiol.211597",
      "authors": "Moskowitz Chaya S et al.",
      "keywords": "",
      "mesh_terms": "Bias; Humans; Machine Learning; Radiology; Research Design",
      "pub_types": "Journal Article; Review; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35579522/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Herein, the authors review biases, sources of variability, and pitfalls that frequently arise in radiomic research, with an emphasis on study design and statistical analysis considerations. Drawing on existing work in the statistical, radiologic, and machine learning literature, approaches for avoiding these pitfalls are described.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9340236"
    },
    {
      "pmid": "35579815",
      "title": "Identifying and Mitigating Potential Biases in Predicting Drug Approvals.",
      "abstract": "INTRODUCTION: Machine learning models are increasingly applied to predict the drug development outcomes based on intermediary clinical trial results. A key challenge to this task is to address various forms of bias in the historical drug approval data. OBJECTIVE: We aimed to identify and mitigate the bias in drug approval predictions and quantify the impacts of debiasing in terms of financial value and drug safety. METHODS: We instantiated the Debiasing Variational Autoencoder, the state-of-the-art model for automated debiasing. We trained and evaluated the model on the Citeline dataset provided by Informa Pharma Intelligence\u00a0to predict the final drug development outcome from phase II trial results. RESULTS: The debiased Debiasing Variational Autoencoder model achieved better performance (measured by the [Formula: see text] score 0.48) in predicting the drug development outcomes than its un-debiased baseline ([Formula: see text] score 0.25). It had a much higher true-positive rate than baseline (60% vs 15%), while its true-negative rate was slightly lower (88% vs 99%). The Debiasing Variational Autoencoder distinguished between drugs developed by large pharmaceutical firms and those by small biotech companies. The model prediction is strongly influenced by multiple factors such as prior approval of the drug for another indication, whether the trial meets the positive/negative endpoints, and the year when the trial is completed. We estimate that the debiased model generates financial value for the drug developer in six major therapeutic areas, with a range of US$763-1,365 million. CONCLUSIONS: Our analysis shows that debiasing improves the financial efficiency of late-stage drug development. From the pharmacovigilance perspective, the debiased model is more likely to identify drugs that are both safe and effective. Meanwhile, it may predict a higher probability of success for drugs with potential adverse effects (because of its lower true-negative rate), thus it must be used with caution to predict the development outcomes of drug candidates currently in the pipeline.",
      "journal": "Drug safety",
      "year": "2022",
      "doi": "10.1007/s40264-022-01160-9",
      "authors": "Xu Qingyang et al.",
      "keywords": "",
      "mesh_terms": "Bias; Drug Approval; Drug-Related Side Effects and Adverse Reactions; Humans; Machine Learning; Pharmacovigilance",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35579815/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Drug Discovery/Pharmacology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Clinical Trial",
      "key_findings": "CONCLUSIONS: Our analysis shows that debiasing improves the financial efficiency of late-stage drug development. From the pharmacovigilance perspective, the debiased model is more likely to identify drugs that are both safe and effective. Meanwhile, it may predict a higher probability of success for drugs with potential adverse effects (because of its lower true-negative rate), thus it must be used with caution to predict the development outcomes of drug candidates currently in the pipeline.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "35609495",
      "title": "Artificial intelligence for prediction of treatment outcomes in breast cancer: Systematic review of design, reporting standards, and bias.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) has the potential to personalize treatment strategies for patients with cancer. However, current methodological weaknesses could limit clinical impact. We identified common limitations and suggested potential solutions to facilitate translation of AI to breast cancer management. METHODS: A systematic review was conducted in MEDLINE, Embase, SCOPUS, Google Scholar and PubMed Central in July 2021. Studies investigating the performance of AI to predict outcomes among patients undergoing treatment for breast cancer were included. Algorithm design and adherence to reporting standards were assessed following the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) statement. Risk of bias was assessed by using the Prediction model Risk Of Bias Assessment Tool (PROBAST), and correspondence with authors to assess data and code availability. RESULTS: Our search identified 1,124 studies, of which 64 were included: 58 had a retrospective study design, with 6 studies with a prospective design. Access to datasets and code was severely limited (unavailable in 77% and 88% of studies, respectively). On request, data and code were made available in 28% and 18% of cases, respectively. Ethnicity was often under-reported (not reported in 52 of 64, 81%), as was model calibration (63/64, 99%). The risk of bias was high in 72% (46/64) of the studies, especially because of analysis bias. CONCLUSION: Development of AI algorithms should involve external and prospective validation, with improved code and data availability to enhance reliability and translation of this promising approach. Protocol registration number: PROSPERO - CRD42022292495.",
      "journal": "Cancer treatment reviews",
      "year": "2022",
      "doi": "10.1016/j.ctrv.2022.102410",
      "authors": "Corti Chiara et al.",
      "keywords": "Artificial intelligence; Bias; Breast cancer; Decision support; Outcome prediction",
      "mesh_terms": "Artificial Intelligence; Bias; Breast Neoplasms; Female; Humans; Prognosis; Reproducibility of Results; Retrospective Studies; Treatment Outcome",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35609495/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Oncology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Calibration",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Development of AI algorithms should involve external and prospective validation, with improved code and data availability to enhance reliability and translation of this promising approach. Protocol registration number: PROSPERO - CRD42022292495.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "35639450",
      "title": "Evaluation and Mitigation of Racial Bias in Clinical Machine Learning Models: Scoping Review.",
      "abstract": "BACKGROUND: Racial bias is a key concern regarding the development, validation, and implementation of machine learning (ML) models in clinical settings. Despite the potential of bias to propagate health disparities, racial bias in clinical ML has yet to be thoroughly examined and best practices for bias mitigation remain unclear. OBJECTIVE: Our objective was to perform a scoping review to characterize the methods by which the racial bias of ML has been assessed and describe strategies that may be used to enhance algorithmic fairness in clinical ML. METHODS: A scoping review was conducted in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) Extension for Scoping Reviews. A literature search using PubMed, Scopus, and Embase databases, as well as Google Scholar, identified 635 records, of which 12 studies were included. RESULTS: Applications of ML were varied and involved diagnosis, outcome prediction, and clinical score prediction performed on data sets including images, diagnostic studies, clinical text, and clinical variables. Of the 12 studies, 1 (8%) described a model in routine clinical use, 2 (17%) examined prospectively validated clinical models, and the remaining 9 (75%) described internally validated models. In addition, 8 (67%) studies concluded that racial bias was present, 2 (17%) concluded that it was not, and 2 (17%) assessed the implementation of bias mitigation strategies without comparison to a baseline model. Fairness metrics used to assess algorithmic racial bias were inconsistent. The most commonly observed metrics were equal opportunity difference (5/12, 42%), accuracy (4/12, 25%), and disparate impact (2/12, 17%). All 8 (67%) studies that implemented methods for mitigation of racial bias successfully increased fairness, as measured by the authors' chosen metrics. Preprocessing methods of bias mitigation were most commonly used across all studies that implemented them. CONCLUSIONS: The broad scope of medical ML applications and potential patient harms demand an increased emphasis on evaluation and mitigation of racial bias in clinical ML. However, the adoption of algorithmic fairness principles in medicine remains inconsistent and is limited by poor data availability and ML model reporting. We recommend that researchers and journal editors emphasize standardized reporting and data availability in medical ML studies to improve transparency and facilitate evaluation for racial bias.",
      "journal": "JMIR medical informatics",
      "year": "2022",
      "doi": "10.2196/36388",
      "authors": "Huang Jonathan et al.",
      "keywords": "algorithm; algorithmic fairness; artificial intelligence; assessment; bias; clinical machine learning; diagnosis; fairness; machine learning; medical machine learning; mitigation; model; outcome prediction; prediction; race; racial bias; scoping review; score prediction",
      "mesh_terms": "",
      "pub_types": "Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35639450/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: The broad scope of medical ML applications and potential patient harms demand an increased emphasis on evaluation and mitigation of racial bias in clinical ML. However, the adoption of algorithmic fairness principles in medicine remains inconsistent and is limited by poor data availability and ML model reporting. We recommend that researchers and journal editors emphasize standardized reporting and data availability in medical ML studies to improve transparency and facilitate evalua...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9198828"
    },
    {
      "pmid": "35652218",
      "title": "Artificial Intelligence and Machine Learning Technologies in Cancer Care: Addressing Disparities, Bias, and Data Diversity.",
      "abstract": "Artificial intelligence (AI) and machine learning (ML) technologies have not only tremendous potential to augment clinical decision-making and enhance quality care and precision medicine efforts, but also the potential to worsen existing health disparities without a thoughtful, transparent, and inclusive approach that includes addressing bias in their design and implementation along the cancer discovery and care continuum. We discuss applications of AI/ML tools in cancer and provide recommendations for addressing and mitigating potential bias with AI and ML technologies while promoting cancer health equity.",
      "journal": "Cancer discovery",
      "year": "2022",
      "doi": "10.1158/2159-8290.CD-22-0373",
      "authors": "Dankwa-Mullan Irene et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Humans; Machine Learning; Neoplasms; Precision Medicine",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35652218/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Artificial intelligence (AI) and machine learning (ML) technologies have not only tremendous potential to augment clinical decision-making and enhance quality care and precision medicine efforts, but also the potential to worsen existing health disparities without a thoughtful, transparent, and inclusive approach that includes addressing bias in their design and implementation along the cancer discovery and care continuum. We discuss applications of AI/ML tools in cancer and provide recommendati...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9662931"
    },
    {
      "pmid": "35699997",
      "title": "Fairness in Mobile Phone-Based Mental Health Assessment Algorithms: Exploratory Study.",
      "abstract": "BACKGROUND: Approximately 1 in 5 American adults experience mental illness every year. Thus, mobile phone-based mental health prediction apps that use phone data and artificial intelligence techniques for mental health assessment have become increasingly important and are being rapidly developed. At the same time, multiple artificial intelligence-related technologies (eg, face recognition and search results) have recently been reported to be biased regarding age, gender, and race. This study moves this discussion to a new domain: phone-based mental health assessment algorithms. It is important to ensure that such algorithms do not contribute to gender disparities through biased predictions across gender groups. OBJECTIVE: This research aimed to analyze the susceptibility of multiple commonly used machine learning approaches for gender bias in mobile mental health assessment and explore the use of an algorithmic disparate impact remover (DIR) approach to reduce bias levels while maintaining high accuracy. METHODS: First, we performed preprocessing and model training using the data set (N=55) obtained from a previous study. Accuracy levels and differences in accuracy across genders were computed using 5 different machine learning models. We selected the random forest model, which yielded the highest accuracy, for a more detailed audit and computed multiple metrics that are commonly used for fairness in the machine learning literature. Finally, we applied the DIR approach to reduce bias in the mental health assessment algorithm. RESULTS: The highest observed accuracy for the mental health assessment was 78.57%. Although this accuracy level raises optimism, the audit based on gender revealed that the performance of the algorithm was statistically significantly different between the male and female groups (eg, difference in accuracy across genders was 15.85%; P<.001). Similar trends were obtained for other fairness metrics. This disparity in performance was found to reduce significantly after the application of the DIR approach by adapting the data used for modeling (eg, the difference in accuracy across genders was 1.66%, and the reduction is statistically significant with P<.001). CONCLUSIONS: This study grounds the need for algorithmic auditing in phone-based mental health assessment algorithms and the use of gender as a protected attribute to study fairness in such settings. Such audits and remedial steps are the building blocks for the widespread adoption of fair and accurate mental health assessment algorithms in the future.",
      "journal": "JMIR formative research",
      "year": "2022",
      "doi": "10.2196/34366",
      "authors": "Park Jinkyung et al.",
      "keywords": "algorithmic bias; gender bias; health equity; health information systems; medical informatics; mental health; mobile phone",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35699997/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Random Forest",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing; Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Bias Auditing Framework",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: This study grounds the need for algorithmic auditing in phone-based mental health assessment algorithms and the use of gender as a protected attribute to study fairness in such settings. Such audits and remedial steps are the building blocks for the widespread adoption of fair and accurate mental health assessment algorithms in the future.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9240929"
    },
    {
      "pmid": "35857214",
      "title": "Bias in algorithms of AI systems developed for COVID-19: A scoping review.",
      "abstract": "To analyze which ethically relevant biases have been identified by academic literature in artificial intelligence (AI) algorithms developed either for patient risk prediction and triage, or for contact tracing to deal with the COVID-19 pandemic. Additionally, to specifically investigate whether the role of social determinants of health (SDOH) have been considered in these AI developments or not. We conducted a scoping review of the literature, which covered publications from March 2020 to April 2021. \u200bStudies mentioning biases on AI algorithms developed for contact tracing and medical triage or risk prediction regarding COVID-19 were included. From 1054 identified articles, 20 studies were finally included. We propose a typology of biases identified in the literature based on bias, limitations and other ethical issues in both areas of analysis. Results on health disparities and SDOH were classified into five categories: racial disparities, biased data, socio-economic disparities, unequal accessibility and workforce, and information communication. SDOH needs to be considered in the clinical context, where they still seem underestimated. Epidemiological conditions depend on geographic location, so the use of local data in studies to develop international solutions may increase some biases. Gender bias was not specifically addressed in the articles included. The main biases are related to data collection and management. Ethical problems related to privacy, consent, and lack of regulation have been identified in contact tracing while some bias-related health inequalities have been highlighted. There is a need for further research focusing on SDOH and these specific AI apps.",
      "journal": "Journal of bioethical inquiry",
      "year": "2022",
      "doi": "10.1007/s11673-022-10200-z",
      "authors": "Delgado Janet et al.",
      "keywords": "COVID-19; artificial intelligence; bias; digital contact tracing; patient risk prediction",
      "mesh_terms": "Artificial Intelligence; Bias; COVID-19; Contact Tracing; Humans; Pandemics",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35857214/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Emergency Medicine; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Ethical problems related to privacy, consent, and lack of regulation have been identified in contact tracing while some bias-related health inequalities have been highlighted. There is a need for further research focusing on SDOH and these specific AI apps.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9463236"
    },
    {
      "pmid": "36059892",
      "title": "Subpopulation-specific machine learning prognosis for underrepresented patients with double prioritized bias correction.",
      "abstract": "BACKGROUND: Many clinical datasets are intrinsically imbalanced, dominated by overwhelming majority groups. Off-the-shelf machine learning models that optimize the prognosis of majority patient types (e.g., healthy class) may cause substantial errors on the minority prediction class (e.g., disease class) and demographic subgroups (e.g., Black or young patients). In the typical one-machine-learning-model-fits-all paradigm, racial and age disparities are likely to exist, but unreported. In addition, some widely used whole-population metrics give misleading results. METHODS: We design a double prioritized (DP) bias correction technique to mitigate representational biases in machine learning-based prognosis. Our method trains customized machine learning models for specific ethnicity or age groups, a substantial departure from the one-model-predicts-all convention. We compare with other sampling and reweighting techniques in mortality and cancer survivability prediction tasks. RESULTS: We first provide empirical evidence showing various prediction deficiencies in a typical machine learning setting without bias correction. For example, missed death cases are 3.14 times higher than missed survival cases for mortality prediction. Then, we show DP consistently boosts the minority class recall for underrepresented groups, by up to 38.0%. DP also reduces relative disparities across race and age groups, e.g., up to 88.0% better than the 8 existing sampling solutions in terms of the relative disparity of minority class recall. Cross-race and cross-age-group evaluation also suggests the need for subpopulation-specific machine learning models. CONCLUSIONS: Biases exist in the widely accepted one-machine-learning-model-fits-all-population approach. We invent a bias correction method that produces specialized machine learning prognostication models for underrepresented racial and age groups. This technique may reduce potentially life-threatening prediction mistakes for minority populations.",
      "journal": "Communications medicine",
      "year": "2022",
      "doi": "10.1038/s43856-022-00165-w",
      "authors": "Afrose Sharmin et al.",
      "keywords": "Cancer; Prognosis",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36059892/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology",
      "bias_axes": "Race/Ethnicity; Age",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Subgroup Analysis",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: Biases exist in the widely accepted one-machine-learning-model-fits-all-population approach. We invent a bias correction method that produces specialized machine learning prognostication models for underrepresented racial and age groups. This technique may reduce potentially life-threatening prediction mistakes for minority populations.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9436942"
    },
    {
      "pmid": "36084616",
      "title": "Algorithmic fairness in computational medicine.",
      "abstract": "Machine learning models are increasingly adopted for facilitating clinical decision-making. However, recent research has shown that machine learning techniques may result in potential biases when making decisions for people in different subgroups, which can lead to detrimental effects on the health and well-being of specific demographic groups such as vulnerable ethnic minorities. This problem, termed algorithmic bias, has been extensively studied in theoretical machine learning recently. However, the impact of algorithmic bias on medicine and methods to mitigate this bias remain topics of active discussion. This paper presents a comprehensive review of algorithmic fairness in the context of computational medicine, which aims at improving medicine with computational approaches. Specifically, we overview the different types of algorithmic bias, fairness quantification metrics, and bias mitigation methods, and summarize popular software libraries and tools for bias evaluation and mitigation, with the goal of providing reference and insights to researchers and practitioners in computational medicine.",
      "journal": "EBioMedicine",
      "year": "2022",
      "doi": "10.1016/j.ebiom.2022.104250",
      "authors": "Xu Jie et al.",
      "keywords": "Algorithmic fairness; Computational medicine",
      "mesh_terms": "Bias; Clinical Decision-Making; Decision Making; Humans; Machine Learning",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36084616/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "This paper presents a comprehensive review of algorithmic fairness in the context of computational medicine, which aims at improving medicine with computational approaches. Specifically, we overview the different types of algorithmic bias, fairness quantification metrics, and bias mitigation methods, and summarize popular software libraries and tools for bias evaluation and mitigation, with the goal of providing reference and insights to researchers and practitioners in computational medicine.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9463525"
    },
    {
      "pmid": "36111262",
      "title": "Foundations for fairness in digital health apps.",
      "abstract": "Digital mental health applications promise scalable and cost-effective solutions to mitigate the gap between the demand and supply of mental healthcare services. However, very little attention is paid on differential impact and potential discrimination in digital mental health services with respect to different sensitive user groups (e.g., race, age, gender, ethnicity, socio-economic status) as the extant literature as well as the market lack the corresponding evidence. In this paper, we outline a 7-step model to assess algorithmic discrimination in digital mental health services, focusing on algorithmic bias assessment and differential impact. We conduct a pilot analysis with 610 users of the model applied on a digital wellbeing service called Foundations that incorporates a rich set of 150 proposed activities designed to increase wellbeing and reduce stress. We further apply the 7-step model on the evaluation of two algorithms that could extend the current service: monitoring step-up model, and a popularity-based activities recommender system. This study applies an algorithmic fairness analysis framework for digital mental health and explores differences in the outcome metrics for the interventions, monitoring model, and recommender engine for the users of different age, gender, type of work, country of residence, employment status and monthly income. Systematic Review Registration: The study with main hypotheses is registered at: https://osf.io/hvtf8.",
      "journal": "Frontiers in digital health",
      "year": "2022",
      "doi": "10.3389/fdgth.2022.943514",
      "authors": "Buda Teodora Sandra et al.",
      "keywords": "algorithmic discrimination; algorithmic fairness; case study; digital mental health; undesired bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36111262/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "This study applies an algorithmic fairness analysis framework for digital mental health and explores differences in the outcome metrics for the interventions, monitoring model, and recommender engine for the users of different age, gender, type of work, country of residence, employment status and monthly income. Systematic Review Registration: The study with main hypotheses is registered at: https://osf.io/hvtf8.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9468215"
    },
    {
      "pmid": "36185063",
      "title": "The paradox of the artificial intelligence system development process: the use case of corporate wellness programs using smart wearables.",
      "abstract": "Artificial intelligence (AI) systems have been widely applied to various contexts, including high-stake decision processes in healthcare, banking, and judicial systems. Some developed AI models fail to offer a fair output for specific minority groups, sparking comprehensive discussions about AI fairness. We argue that the development of AI systems is marked by a central paradox: the less participation one stakeholder has within the AI system's life cycle, the more influence they have over the way the system will function. This means that the impact on the fairness of the system is in the hands of those who are less impacted by it. However, most of the existing works ignore how different aspects of AI fairness are dynamically and adaptively affected by different stages of AI system development. To this end, we present a use case to discuss fairness in the development of corporate wellness programs using smart wearables and AI algorithms to analyze data. The four key stakeholders throughout this type of AI system development process are presented. These stakeholders\u00a0are called service designer, algorithm designer, system deployer, and end-user. We identify three core aspects of AI fairness, namely, contextual fairness, model fairness, and device fairness. We propose a relative contribution of the four stakeholders to the three aspects of fairness. Furthermore, we propose the boundaries and interactions between the four roles, from which we make our conclusion about the possible unfairness in such an AI developing process.",
      "journal": "AI & society",
      "year": "2022",
      "doi": "10.1007/s00146-022-01562-4",
      "authors": "Angelucci Alessandra et al.",
      "keywords": "Artificial intelligence; Classification model; Corporate wellness program; Fairness; Smartwatches",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36185063/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Wearables/Remote Monitoring",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Telehealth/Remote",
      "key_findings": "We propose a relative contribution of the four stakeholders to the three aspects of fairness. Furthermore, we propose the boundaries and interactions between the four roles, from which we make our conclusion about the possible unfairness in such an AI developing process.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9511446"
    },
    {
      "pmid": "36258241",
      "title": "Algorithmic fairness audits in intensive care medicine: artificial intelligence for all?",
      "abstract": "",
      "journal": "Critical care (London, England)",
      "year": "2022",
      "doi": "10.1186/s13054-022-04197-5",
      "authors": "van de Sande Davy et al.",
      "keywords": "Artificial intelligence; Bias; Equity; Intensive care",
      "mesh_terms": "Humans; Artificial Intelligence; Critical Care; Algorithms",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36258241/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "No abstract available",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9578232"
    },
    {
      "pmid": "36294820",
      "title": "Machine Learning in Predicting Tooth Loss: A Systematic Review and Risk of Bias Assessment.",
      "abstract": "Predicting tooth loss is a persistent clinical challenge in the 21st century. While an emerging field in dentistry, computational solutions that employ machine learning are promising for enhancing clinical outcomes, including the chairside prognostication of tooth loss. We aimed to evaluate the risk of bias in prognostic prediction models of tooth loss that use machine learning. To do this, literature was searched in two electronic databases (MEDLINE via PubMed; Google Scholar) for studies that reported the accuracy or area under the curve (AUC) of prediction models. AUC measures the entire two-dimensional area underneath the entire receiver operating characteristic (ROC) curves. AUC provides an aggregate measure of performance across all possible classification thresholds. Although both development and validation were included in this review, studies that did not assess the accuracy or validation of boosting models (AdaBoosting, Gradient-boosting decision tree, XGBoost, LightGBM, CatBoost) were excluded. Five studies met criteria for inclusion and revealed high accuracy; however, models displayed a high risk of bias. Importantly, patient-level assessments combined with socioeconomic predictors performed better than clinical predictors alone. While there are current limitations, machine-learning-assisted models for tooth loss may enhance prognostication accuracy in combination with clinical and patient metadata in the future.",
      "journal": "Journal of personalized medicine",
      "year": "2022",
      "doi": "10.3390/jpm12101682",
      "authors": "Hasuike Akira et al.",
      "keywords": "boosting; deep learning; machine learning; periodontitis; prognosis; tooth loss",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36294820/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "XGBoost/Gradient Boosting; Decision Tree; Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "Not specified",
      "key_findings": "Importantly, patient-level assessments combined with socioeconomic predictors performed better than clinical predictors alone. While there are current limitations, machine-learning-assisted models for tooth loss may enhance prognostication accuracy in combination with clinical and patient metadata in the future.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9605501"
    },
    {
      "pmid": "36463869",
      "title": "Clinical Informatics Approaches to Understand and Address Cancer Disparities.",
      "abstract": "OBJECTIVES: Disparities in cancer incidence and outcomes across race, ethnicity, gender, socioeconomic status, and geography are well-documented, but their etiologies are often poorly understood and multifactorial. Clinical informatics can provide tools to better understand and address these disparities by enabling high-throughput analysis of multiple types of data. Here, we review recent efforts in clinical informatics to study and measure disparities in cancer. METHODS: We carried out a narrative review of clinical informatics studies related to cancer disparities and bias published from 2018-2021, with a focus on domains such as real-world data (RWD) analysis, natural language processing (NLP), radiomics, genomics, proteomics, metabolomics, and metagenomics. RESULTS: Clinical informatics studies that investigated cancer disparities across race, ethnicity, gender, and age were identified. Most cancer disparities work within clinical informatics used RWD analysis, NLP, radiomics, and genomics. Emerging applications of clinical informatics to understand cancer disparities, including proteomics, metabolomics, and metagenomics, were less well represented in the literature but are promising future research avenues. Algorithmic bias was identified as an important consideration when developing and implementing cancer clinical informatics techniques, and efforts to address this bias were reviewed. CONCLUSIONS: In recent years, clinical informatics has been used to probe a range of data sources to understand cancer disparities across different populations. As informatics tools become integrated into clinical decision-making, attention will need to be paid to ensure that algorithmic bias does not amplify existing disparities. In our increasingly interconnected medical systems, clinical informatics is poised to untap the full potential of multi-platform health data to address cancer disparities.",
      "journal": "Yearbook of medical informatics",
      "year": "2022",
      "doi": "10.1055/s-0042-1742511",
      "authors": "Chaunzwa Tafadzwa L et al.",
      "keywords": "",
      "mesh_terms": "Humans; Medical Informatics; Neoplasms; Genomics; Natural Language Processing; Proteomics",
      "pub_types": "Review; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36463869/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Oncology; EHR/Health Informatics; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Language",
      "lifecycle_stage": "Data Collection; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: In recent years, clinical informatics has been used to probe a range of data sources to understand cancer disparities across different populations. As informatics tools become integrated into clinical decision-making, attention will need to be paid to ensure that algorithmic bias does not amplify existing disparities. In our increasingly interconnected medical systems, clinical informatics is poised to untap the full potential of multi-platform health data to address cancer disparit...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9719762"
    },
    {
      "pmid": "36714611",
      "title": "Fairness in the prediction of acute postoperative pain using machine learning models.",
      "abstract": "INTRODUCTION: Overall performance of machine learning-based prediction models is promising; however, their generalizability and fairness must be vigorously investigated to ensure they perform sufficiently well for all patients. OBJECTIVE: This study aimed to evaluate prediction bias in machine learning models used for predicting acute postoperative pain. METHOD: We conducted a retrospective review of electronic health records for patients undergoing orthopedic surgery from June 1, 2011, to June 30, 2019, at the University of Florida Health system/Shands Hospital. CatBoost machine learning models were trained for predicting the binary outcome of low (\u22644) and high pain (>4). Model biases were assessed against seven protected attributes of age, sex, race, area deprivation index (ADI), speaking language, health literacy, and insurance type. Reweighing of protected attributes was investigated for reducing model bias compared with base models. Fairness metrics of equal opportunity, predictive parity, predictive equality, statistical parity, and overall accuracy equality were examined. RESULTS: The final dataset included 14,263 patients [age: 60.72 (16.03) years, 53.87% female, 39.13% low acute postoperative pain]. The machine learning model (area under the curve, 0.71) was biased in terms of age, race, ADI, and insurance type, but not in terms of sex, language, and health literacy. Despite promising overall performance in predicting acute postoperative pain, machine learning-based prediction models may be biased with respect to protected attributes. CONCLUSION: These findings show the need to evaluate fairness in machine learning models involved in perioperative pain before they are implemented as clinical decision support tools.",
      "journal": "Frontiers in digital health",
      "year": "2022",
      "doi": "10.3389/fdgth.2022.970281",
      "authors": "Davoudi Anis et al.",
      "keywords": "algorithmic bias; clinical decision support systems; machine learing; orthopedic procedures; postoperative pain",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36714611/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "XGBoost/Gradient Boosting; Clinical Prediction Model; Clinical Decision Support",
      "health_domain": "EHR/Health Informatics; Surgery; Pain Management",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Language; Insurance Status",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Fairness Metrics Evaluation",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "CONCLUSION: These findings show the need to evaluate fairness in machine learning models involved in perioperative pain before they are implemented as clinical decision support tools.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9874861"
    },
    {
      "pmid": "36865610",
      "title": "Identification of Social and Racial Disparities in Risk of HIV Infection in Florida using Causal AI Methods.",
      "abstract": "Florida -the 3rd most populous state in the USA-has the highest rates of Human Immunodeficiency Virus (HIV) infections and of unfavorable HIV outcomes, with marked social and racial disparities. In this work, we leveraged large-scale, real-world data, i.e., statewide surveillance records and publicly available data resources encoding social determinants of health (SDoH), to identify social and racial disparities contributing to individuals' risk of HIV infection. We used the Florida Department of Health's Syndromic Tracking and Reporting System (STARS) database (including 100,000+ individuals screened for HIV infection and their partners), and a novel algorithmic fairness assessment method -the Fairness-Aware Causal paThs decompoSition (FACTS)- merging causal inference and artificial intelligence. FACTS deconstructs disparities based on SDoH and individuals' characteristics, and can discover novel mechanisms of inequity, quantifying to what extent they could be reduced by interventions. We paired the deidentified demographic information (age, gender, drug use) of 44,350 individuals in STARS -with non-missing data on interview year, county of residence, and infection status- to eight SDoH, including access to healthcare facilities, % uninsured, median household income, and violent crime rate. Using an expert-reviewed causal graph, we found that the risk of HIV infection for African Americans was higher than for non- African Americans (both in terms of direct and total effect), although a null effect could not be ruled out. FACTS identified several paths leading to racial disparity in HIV risk, including multiple SDoH: education, income, violent crime, drinking, smoking, and rurality.",
      "journal": "Proceedings. IEEE International Conference on Bioinformatics and Biomedicine",
      "year": "2022",
      "doi": "10.1109/bibm55620.2022.9995662",
      "authors": "Prosperi Mattia et al.",
      "keywords": "artificial intelligence; causal inference; disparity; epidemiology; human immunodeficiency virus; machine learning; real-world data; social determinants of health; surveillance",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36865610/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health; Infectious Disease",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Geographic; Insurance Status",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Counterfactual Fairness",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Using an expert-reviewed causal graph, we found that the risk of HIV infection for African Americans was higher than for non- African Americans (both in terms of direct and total effect), although a null effect could not be ruled out. FACTS identified several paths leading to racial disparity in HIV risk, including multiple SDoH: education, income, violent crime, drinking, smoking, and rurality.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9977319"
    },
    {
      "pmid": "33220494",
      "title": "An empirical characterization of fair machine learning for clinical risk prediction.",
      "abstract": "The use of machine learning to guide clinical decision making has the potential to worsen existing health disparities. Several recent works frame the problem as that of algorithmic fairness, a framework that has attracted considerable attention and criticism. However, the appropriateness of this framework is unclear due to both ethical as well as technical considerations, the latter of which include trade-offs between measures of fairness and model performance that are not well-understood for predictive models of clinical outcomes. To inform the ongoing debate, we conduct an empirical study to characterize the impact of penalizing group fairness violations on an array of measures of model performance and group fairness. We repeat the analysis across multiple observational healthcare databases, clinical outcomes, and sensitive attributes. We find that procedures that penalize differences between the distributions of predictions across groups induce nearly-universal degradation of multiple performance metrics within groups. On examining the secondary impact of these procedures, we observe heterogeneity of the effect of these procedures on measures of fairness in calibration and ranking across experimental conditions. Beyond the reported trade-offs, we emphasize that analyses of algorithmic fairness in healthcare lack the contextual grounding and causal awareness necessary to reason about the mechanisms that lead to health disparities, as well as about the potential of algorithmic fairness methods to counteract those mechanisms. In light of these limitations, we encourage researchers building predictive models for clinical use to step outside the algorithmic fairness frame and engage critically with the broader sociotechnical context surrounding the use of machine learning in healthcare.",
      "journal": "Journal of biomedical informatics",
      "year": "2021",
      "doi": "10.1016/j.jbi.2020.103621",
      "authors": "Pfohl Stephen R et al.",
      "keywords": "Algorithmic fairness; Bias; Clinical risk prediction",
      "mesh_terms": "Delivery of Health Care; Empirical Research; Machine Learning",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33220494/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Beyond the reported trade-offs, we emphasize that analyses of algorithmic fairness in healthcare lack the contextual grounding and causal awareness necessary to reason about the mechanisms that lead to health disparities, as well as about the potential of algorithmic fairness methods to counteract those mechanisms. In light of these limitations, we encourage researchers building predictive models for clinical use to step outside the algorithmic fairness frame and engage critically with the broad...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7871979"
    },
    {
      "pmid": "33315263",
      "title": "The Emerging Hazard of AI-Related Health Care Discrimination.",
      "abstract": "Artificial intelligence holds great promise for improved health-care outcomes. But it also poses substantial new hazards, including algorithmic discrimination. For example, an algorithm used to identify candidates for beneficial \"high risk care management\" programs routinely failed to select racial minorities. Furthermore, some algorithms deliberately adjust for race in ways that divert resources away from minority patients. To illustrate, algorithms have underestimated African Americans' risks of kidney stones and death from heart failure. Algorithmic discrimination can violate Title VI of the Civil Rights Act and Section 1557 of the Affordable Care Act when it unjustifiably disadvantages underserved populations. This article urges that both legal and technical tools be deployed to promote AI fairness. Plaintiffs should be able to assert disparate impact claims in health-care litigation, and Congress should enact an Algorithmic Accountability Act. In addition, fairness should be a key element in designing, implementing, validating, and employing AI.",
      "journal": "The Hastings Center report",
      "year": "2021",
      "doi": "10.1002/hast.1203",
      "authors": "Hoffman Sharona",
      "keywords": "algorithmic fairness; artificial intelligence; civil rights; discrimination; disparate impact",
      "mesh_terms": "Artificial Intelligence; Civil Rights; Delivery of Health Care; Humans; Minority Groups; Patient Protection and Affordable Care Act; United States",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33315263/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; Nephrology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "Public Health/Population; Safety-Net/Underserved",
      "key_findings": "Plaintiffs should be able to assert disparate impact claims in health-care litigation, and Congress should enact an Algorithmic Accountability Act. In addition, fairness should be a key element in designing, implementing, validating, and employing AI.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "33459685",
      "title": "COVID-19 diagnosis from chest X-ray images using transfer learning: Enhanced performance by debiasing dataloader.",
      "abstract": "BACKGROUND: Chest X-ray imaging has been proved as a powerful diagnostic method to detect and diagnose COVID-19 cases due to its easy accessibility, lower cost and rapid imaging time. OBJECTIVE: This study aims to improve efficacy of screening COVID-19 infected patients using chest X-ray images with the help of a developed deep convolutional neural network model (CNN) entitled nCoV-NET. METHODS: To train and to evaluate the performance of the developed model, three datasets were collected from resources of \"ChestX-ray14\", \"COVID-19 image data collection\", and \"Chest X-ray collection from Indiana University,\" respectively. Overall, 299 COVID-19 pneumonia cases and 1,522 non-COVID 19 cases are involved in this study. To overcome the probable bias due to the unbalanced cases in two classes of the datasets, ResNet, DenseNet, and VGG architectures were re-trained in the fine-tuning stage of the process to distinguish COVID-19 classes using a transfer learning method. Lastly, the optimized final nCoV-NET model was applied to the testing dataset to verify the performance of the proposed model. RESULTS: Although the performance parameters of all re-trained architectures were determined close to each other, the final nCOV-NET model optimized by using DenseNet-161 architecture in the transfer learning stage exhibits the highest performance for classification of COVID-19 cases with the accuracy of 97.1 %. The Activation Mapping method was used to create activation maps that highlights the crucial areas of the radiograph to improve causality and intelligibility. CONCLUSION: This study demonstrated that the proposed CNN model called nCoV-NET can be utilized for reliably detecting COVID-19 cases using chest X-ray images to accelerate the triaging and save critical time for disease control as well as assisting the radiologist to validate their initial diagnosis.",
      "journal": "Journal of X-ray science and technology",
      "year": "2021",
      "doi": "10.3233/XST-200757",
      "authors": "Polat \u00c7a\u011f\u00edn et al.",
      "keywords": "Coronavirus (COVID-19) infection; chest X-ray images; convolutional neural network (CNN); deep learning; transfer learning",
      "mesh_terms": "Algorithms; COVID-19; Deep Learning; Early Diagnosis; Humans; Neural Networks, Computer; Pneumonia; Radiography, Thoracic; Reproducibility of Results; SARS-CoV-2; Tomography, X-Ray Computed",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33459685/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Radiology/Medical Imaging; Pulmonology",
      "bias_axes": "Age",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Transfer Learning",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: This study demonstrated that the proposed CNN model called nCoV-NET can be utilized for reliably detecting COVID-19 cases using chest X-ray images to accelerate the triaging and save critical time for disease control as well as assisting the radiologist to validate their initial diagnosis.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7990426"
    },
    {
      "pmid": "33568741",
      "title": "Systematic auditing is essential to debiasing machine learning in biology.",
      "abstract": "Biases in data used to train machine learning (ML) models can inflate their prediction performance and confound our understanding of how and what they learn. Although biases are common in biological data, systematic auditing of ML models to identify and eliminate these biases is not a common practice when applying ML in the life sciences. Here we devise a systematic, principled, and general approach to audit ML models in the life sciences. We use this auditing framework to examine biases in three ML applications of therapeutic interest and identify unrecognized biases that hinder the ML process and result in substantially reduced model performance on new datasets. Ultimately, we show that ML models tend to learn primarily from data biases when there is insufficient signal in the data to learn from. We provide detailed protocols, guidelines, and examples of code to enable tailoring of the auditing framework to other biomedical applications.",
      "journal": "Communications biology",
      "year": "2021",
      "doi": "10.1038/s42003-021-01674-5",
      "authors": "Eid Fatma-Elzahraa et al.",
      "keywords": "",
      "mesh_terms": "Animals; Bias; Data Mining; Databases, Protein; Histocompatibility Antigens; Humans; Machine Learning; Pharmaceutical Preparations; Protein Binding; Protein Interaction Maps; Proteins; Proteome; Proteomics; Reproducibility of Results",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33568741/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Drug Discovery/Pharmacology",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Bias Auditing Framework",
      "clinical_setting": "Not specified",
      "key_findings": "Ultimately, we show that ML models tend to learn primarily from data biases when there is insufficient signal in the data to learn from. We provide detailed protocols, guidelines, and examples of code to enable tailoring of the auditing framework to other biomedical applications.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7876113"
    },
    {
      "pmid": "33856478",
      "title": "Comparison of Methods to Reduce Bias From Clinical Prediction Models of Postpartum Depression.",
      "abstract": "IMPORTANCE: The lack of standards in methods to reduce bias for clinical algorithms presents various challenges in providing reliable predictions and in addressing health disparities. OBJECTIVE: To evaluate approaches for reducing bias in machine learning models using a real-world clinical scenario. DESIGN, SETTING, AND PARTICIPANTS: Health data for this cohort study were obtained from the IBM MarketScan Medicaid Database. Eligibility criteria were as follows: (1) Female individuals aged 12 to 55 years with a live birth record identified by delivery-related codes from January 1, 2014, through December 31, 2018; (2) greater than 80% enrollment through pregnancy to 60 days post partum; and (3) evidence of coverage for depression screening and mental health services. Statistical analysis was performed in 2020. EXPOSURES: Binarized race (Black individuals and White individuals). MAIN OUTCOMES AND MEASURES: Machine learning models (logistic regression [LR], random forest, and extreme gradient boosting) were trained for 2 binary outcomes: postpartum depression (PPD) and postpartum mental health service utilization. Risk-adjusted generalized linear models were used for each outcome to assess potential disparity in the cohort associated with binarized race (Black or White). Methods for reducing bias, including reweighing, Prejudice Remover, and removing race from the models, were examined by analyzing changes in fairness metrics compared with the base models. Baseline characteristics of female individuals at the top-predicted risk decile were compared for systematic differences. Fairness metrics of disparate impact (DI, 1 indicates fairness) and equal opportunity difference (EOD, 0 indicates fairness). RESULTS: Among 573\u202f634 female individuals initially examined for this study, 314\u202f903 were White (54.9%), 217\u202f899 were Black (38.0%), and the mean (SD) age was 26.1 (5.5) years. The risk-adjusted odds ratio comparing White participants with Black participants was 2.06 (95% CI, 2.02-2.10) for clinically recognized PPD and 1.37 (95% CI, 1.33-1.40) for postpartum mental health service utilization. Taking the LR model for PPD prediction as an example, reweighing reduced bias as measured by improved DI and EOD metrics from 0.31 and -0.19 to 0.79 and 0.02, respectively. Removing race from the models had inferior performance for reducing bias compared with the other methods (PPD: DI\u2009=\u20090.61; EOD\u2009=\u2009-0.05; mental health service utilization: DI\u2009=\u20090.63; EOD\u2009=\u2009-0.04). CONCLUSIONS AND RELEVANCE: Clinical prediction models trained on potentially biased data may produce unfair outcomes on the basis of the chosen metrics. This study's results suggest that the performance varied depending on the model, outcome label, and method for reducing bias. This approach toward evaluating algorithmic bias can be used as an example for the growing number of researchers who wish to examine and address bias in their data and models.",
      "journal": "JAMA network open",
      "year": "2021",
      "doi": "10.1001/jamanetworkopen.2021.3909",
      "authors": "Park Yoonyoung et al.",
      "keywords": "",
      "mesh_terms": "Adolescent; Adult; Algorithms; Cohort Studies; Depression, Postpartum; Female; Humans; Middle Aged; Models, Statistical; Odds Ratio; Patient-Specific Modeling; Postpartum Period; Pregnancy; Prognosis; Retrospective Studies; Risk Assessment; Risk Factors; United States; Young Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33856478/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM; Random Forest; Logistic Regression; XGBoost/Gradient Boosting; Clinical Prediction Model",
      "health_domain": "Mental Health/Psychiatry; Obstetrics/Maternal Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling; Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: Among 573\u202f634 female individuals initially examined for this study, 314\u202f903 were White (54.9%), 217\u202f899 were Black (38.0%), and the mean (SD) age was 26.1 (5.5) years. The risk-adjusted odds ratio comparing White participants with Black participants was 2.06 (95% CI, 2.02-2.10) for clinically recognized PPD and 1.37 (95% CI, 1.33-1.40) for postpartum mental health service utilization. Taking the LR model for PPD prediction as an example, reweighing reduced bias as measured by improved D...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8050742"
    },
    {
      "pmid": "34012993",
      "title": "A Joint Fairness Model with Applications to Risk Predictions for Under-represented Populations.",
      "abstract": "In data collection for predictive modeling, under-representation of certain groups, based on gender, race/ethnicity, or age, may yield less-accurate predictions for these groups. Recently, this issue of fairness in predictions has attracted significant attention, as data-driven models are increasingly utilized to perform crucial decision-making tasks. Existing methods to achieve fairness in the machine learning literature typically build a single prediction model in a manner that encourages fair prediction performance for all groups. These approaches have two major limitations: i) fairness is often achieved by compromising accuracy for some groups; ii) the underlying relationship between dependent and independent variables may not be the same across groups. We propose a Joint Fairness Model (JFM) approach for logistic regression models for binary outcomes that estimates group-specific classifiers using a joint modeling objective function that incorporates fairness criteria for prediction. We introduce an Accelerated Smoothing Proximal Gradient Algorithm to solve the convex objective function, and present the key asymptotic properties of the JFM estimates. Through simulations, we demonstrate the efficacy of the JFM in achieving good prediction performance and across-group parity, in comparison with the single fairness model, group-separate model, and group-ignorant model, especially when the minority group's sample size is small. Finally, we demonstrate the utility of the JFM method in a real-world example to obtain fair risk predictions for under-represented older patients diagnosed with coronavirus disease 2019 (COVID-19).",
      "journal": "ArXiv",
      "year": "2021",
      "doi": "",
      "authors": "Do Hyungrok et al.",
      "keywords": "algorithmic bias; algorithmic fairness; joint estimation; under-represented population",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34012993/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Logistic Regression; Clinical Prediction Model; Regression",
      "health_domain": "Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Deployment",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Through simulations, we demonstrate the efficacy of the JFM in achieving good prediction performance and across-group parity, in comparison with the single fairness model, group-separate model, and group-ignorant model, especially when the minority group's sample size is small. Finally, we demonstrate the utility of the JFM method in a real-world example to obtain fair risk predictions for under-represented older patients diagnosed with coronavirus disease 2019 (COVID-19).",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8132236"
    },
    {
      "pmid": "34222857",
      "title": "Gender Bias in the News: A Scalable Topic Modelling and Visualization Framework.",
      "abstract": "We present a topic modelling and data visualization methodology to examine gender-based disparities in news articles by topic. Existing research in topic modelling is largely focused on the text mining of closed corpora, i.e., those that include a fixed collection of composite texts. We showcase a methodology to discover topics via Latent Dirichlet Allocation, which can reliably produce human-interpretable topics over an open news corpus that continually grows with time. Our system generates topics, or distributions of keywords, for news articles on a monthly basis, to consistently detect key events and trends aligned with events in the real world. Findings from 2 years worth of news articles in mainstream English-language Canadian media indicate that certain topics feature either women or men more prominently and exhibit different types of language. Perhaps unsurprisingly, topics such as lifestyle, entertainment, and healthcare tend to be prominent in articles that quote more women than men. Topics such as sports, politics, and business are characteristic of articles that quote more men than women. The data shows a self-reinforcing gendered division of duties and representation in society. Quoting female sources more frequently in a caregiving role and quoting male sources more frequently in political and business roles enshrines women's status as caregivers and men's status as leaders and breadwinners. Our results can help journalists and policy makers better understand the unequal gender representation of those quoted in the news and facilitate news organizations' efforts to achieve gender parity in their sources. The proposed methodology is robust, reproducible, and scalable to very large corpora, and can be used for similar studies involving unsupervised topic modelling and language analyses.",
      "journal": "Frontiers in artificial intelligence",
      "year": "2021",
      "doi": "10.3389/frai.2021.664737",
      "authors": "Rao Prashanth et al.",
      "keywords": "corpus linguistics; gender bias; machine learning; natural language processing; news media; topic modelling",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34222857/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM; Generative AI; Clustering",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Our results can help journalists and policy makers better understand the unequal gender representation of those quoted in the news and facilitate news organizations' efforts to achieve gender parity in their sources. The proposed methodology is robust, reproducible, and scalable to very large corpora, and can be used for similar studies involving unsupervised topic modelling and language analyses.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8242240"
    },
    {
      "pmid": "34383925",
      "title": "Bias and fairness assessment of a natural language processing opioid misuse classifier: detection and mitigation of electronic health record data disadvantages across racial subgroups.",
      "abstract": "OBJECTIVES: To assess fairness and bias of a previously validated machine learning opioid misuse classifier. MATERIALS & METHODS: Two experiments were conducted with the classifier's original (n\u2009=\u20091000) and external validation (n\u2009=\u200953 974) datasets from 2 health systems. Bias was assessed via testing for differences in type II error rates across racial/ethnic subgroups (Black, Hispanic/Latinx, White, Other) using bootstrapped 95% confidence intervals. A local surrogate model was estimated to interpret the classifier's predictions by race and averaged globally from the datasets. Subgroup analyses and post-hoc recalibrations were conducted to attempt to mitigate biased metrics. RESULTS: We identified bias in the false negative rate (FNR = 0.32) of the Black subgroup compared to the FNR (0.17) of the White subgroup. Top features included \"heroin\" and \"substance abuse\" across subgroups. Post-hoc recalibrations eliminated bias in FNR with minimal changes in other subgroup error metrics. The Black FNR subgroup had higher risk scores for readmission and mortality than the White FNR subgroup, and a higher mortality risk score than the Black true positive subgroup (P\u2009<\u2009.05). DISCUSSION: The Black FNR subgroup had the greatest severity of disease and risk for poor outcomes. Similar features were present between subgroups for predicting opioid misuse, but inequities were present. Post-hoc mitigation techniques mitigated bias in type II error rate without creating substantial type I error rates. From model design through deployment, bias and data disadvantages should be systematically addressed. CONCLUSION: Standardized, transparent bias assessments are needed to improve trustworthiness in clinical machine learning models.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2021",
      "doi": "10.1093/jamia/ocab148",
      "authors": "Thompson Hale M et al.",
      "keywords": "bias and fairness; interpretability; machine learning; natural language processing; opioid use disorder; structural racism",
      "mesh_terms": "Electronic Health Records; Hispanic or Latino; Humans; Machine Learning; Natural Language Processing; Opioid-Related Disorders",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34383925/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM; Clinical Prediction Model",
      "health_domain": "Mental Health/Psychiatry; EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Post-hoc Correction",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Standardized, transparent bias assessments are needed to improve trustworthiness in clinical machine learning models.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8510285"
    },
    {
      "pmid": "34441187",
      "title": "Toward Learning Trustworthily from Data Combining Privacy, Fairness, and Explainability: An Application to Face Recognition.",
      "abstract": "In many decision-making scenarios, ranging from recreational activities to healthcare and policing, the use of artificial intelligence coupled with the ability to learn from historical data is becoming ubiquitous. This widespread adoption of automated systems is accompanied by the increasing concerns regarding their ethical implications. Fundamental rights, such as the ones that require the preservation of privacy, do not discriminate based on sensible attributes (e.g., gender, ethnicity, political/sexual orientation), or require one to provide an explanation for a decision, are daily undermined by the use of increasingly complex and less understandable yet more accurate learning algorithms. For this purpose, in this work, we work toward the development of systems able to ensure trustworthiness by delivering privacy, fairness, and explainability by design. In particular, we show that it is possible to simultaneously learn from data while preserving the privacy of the individuals thanks to the use of Homomorphic Encryption, ensuring fairness by learning a fair representation from the data, and ensuring explainable decisions with local and global explanations without compromising the accuracy of the final models. We test our approach on a widespread but still controversial application, namely face recognition, using the recent FairFace dataset to prove the validity of our approach.",
      "journal": "Entropy (Basel, Switzerland)",
      "year": "2021",
      "doi": "10.3390/e23081047",
      "authors": "Franco Danilo et al.",
      "keywords": "Algorithmic Fairness; Homomorphic Encryption; attention maps; deep neural networks; dimensionality reduction; explainable artificial intelligence; learning fair representation; privacy-preserving machine learning; trustworthy artificial intelligence",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34441187/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Representation Learning; Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "In particular, we show that it is possible to simultaneously learn from data while preserving the privacy of the individuals thanks to the use of Homomorphic Encryption, ensuring fairness by learning a fair representation from the data, and ensuring explainable decisions with local and global explanations without compromising the accuracy of the final models. We test our approach on a widespread but still controversial application, namely face recognition, using the recent FairFace dataset to pr...",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8393832"
    },
    {
      "pmid": "34515682",
      "title": "Construction of a Wireless-Enabled Endoscopically Implantable Sensor for pH Monitoring with Zero-Bias Schottky Diode-based Receiver.",
      "abstract": "Ambulatory pH monitoring of pathological reflux is an opportunity to observe the relationship between symptoms and exposure of the esophagus to acidic or non-acidic refluxate. This paper describes a method for the development, manufacturing, and implantation of a miniature wireless-enabled pH sensor. The sensor is designed to be implanted endoscopically with a single hemostatic clip. A fully passive rectenna-based receiver based on a zero-bias Schottky diode is also constructed and tested. To construct the device, a two-layer printed circuit board and off-the-shelf components were used. A miniature microcontroller with integrated analog peripherals is used as an analog front end for the ion-sensitive field-effect transistor (ISFET) sensor and to generate a digital signal which is transmitted with an amplitude shift keying transmitter chip. The device is powered by two primary alkaline cells. The implantable device has a total volume of 0.6 cm3 and a weight of 1.2 grams, and its performance was verified in an ex vivo model (porcine esophagus and stomach). Next, a small footprint passive rectenna-based receiver which can be easily integrated either into an external receiver or the implantable neurostimulator, was constructed and proven to receive the RF signal from the implant when in proximity (20 cm) to it. The small size of the sensor provides continuous pH monitoring with minimal obstruction of the esophagus. The sensor could be used in routine clinical practice for 24/96 h esophageal pH monitoring without the need to insert a nasal catheter. The \"zero-power\" nature of the receiver also enables the use of the sensor for automatic in-vivo calibration of miniature lower esophageal sphincter neurostimulation devices. An active sensor-based control enables the development of advanced algorithms to minimize the used energy to achieve a desirable clinical outcome. One of the examples of such an algorithm would be a closed-loop system for on-demand neurostimulation therapy of gastroesophageal reflux disease (GERD).",
      "journal": "Journal of visualized experiments : JoVE",
      "year": "2021",
      "doi": "10.3791/62864",
      "authors": "Nov\u00e1k Marek et al.",
      "keywords": "",
      "mesh_terms": "Animals; Esophageal pH Monitoring; Gastroesophageal Reflux; Hydrogen-Ion Concentration; Prostheses and Implants; Swine",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Video-Audio Media",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34515682/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Primary Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Deployment",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Calibration",
      "clinical_setting": "Primary Care/Outpatient",
      "key_findings": "An active sensor-based control enables the development of advanced algorithms to minimize the used energy to achieve a desirable clinical outcome. One of the examples of such an algorithm would be a closed-loop system for on-demand neurostimulation therapy of gastroesophageal reflux disease (GERD).",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "34533458",
      "title": "Health Equity in Artificial Intelligence and Primary Care Research: Protocol for a Scoping Review.",
      "abstract": "BACKGROUND: Though artificial intelligence (AI) has the potential to augment the patient-physician relationship in primary care, bias in intelligent health care systems has the potential to differentially impact vulnerable patient populations. OBJECTIVE: The purpose of this scoping review is to summarize the extent to which AI systems in primary care examine the inherent bias toward or against vulnerable populations and appraise how these systems have mitigated the impact of such biases during their development. METHODS: We will conduct a search update from an existing scoping review to identify studies on AI and primary care in the following databases: Medline-OVID, Embase, CINAHL, Cochrane Library, Web of Science, Scopus, IEEE Xplore, ACM Digital Library, MathSciNet, AAAI, and arXiv. Two screeners will independently review all abstracts, titles, and full-text articles. The team will extract data using a structured data extraction form and synthesize the results in accordance with PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) guidelines. RESULTS: This review will provide an assessment of the current state of health care equity within AI for primary care. Specifically, we will identify the degree to which vulnerable patients have been included, assess how bias is interpreted and documented, and understand the extent to which harmful biases are addressed. As of October 2020, the scoping review is in the title- and abstract-screening stage. The results are expected to be submitted for publication in fall 2021. CONCLUSIONS: AI applications in primary care are becoming an increasingly common tool in health care delivery and in preventative care efforts for underserved populations. This scoping review would potentially show the extent to which studies on AI in primary care employ a health equity lens and take steps to mitigate bias. INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): PRR1-10.2196/27799.",
      "journal": "JMIR research protocols",
      "year": "2021",
      "doi": "10.2196/27799",
      "authors": "Wang Jonathan Xin et al.",
      "keywords": "artificial intelligence; big data; data mining; decision support; diagnosis; electronic health records; family medicine; health disparity; health equity; health informatics; health information technology; primary care; scoping review; treatment",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34533458/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Primary Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Primary Care/Outpatient; Public Health/Population; Safety-Net/Underserved",
      "key_findings": "CONCLUSIONS: AI applications in primary care are becoming an increasingly common tool in health care delivery and in preventative care efforts for underserved populations. This scoping review would potentially show the extent to which studies on AI in primary care employ a health equity lens and take steps to mitigate bias. INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): PRR1-10.2196/27799.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8486995"
    },
    {
      "pmid": "34573790",
      "title": "The Problem of Fairness in Synthetic Healthcare Data.",
      "abstract": "Access to healthcare data such as electronic health records (EHR) is often restricted by laws established to protect patient privacy. These restrictions hinder the reproducibility of existing results based on private healthcare data and also limit new research. Synthetically-generated healthcare data solve this problem by preserving privacy and enabling researchers and policymakers to drive decisions and methods based on realistic data. Healthcare data can include information about multiple in- and out- patient visits of patients, making it a time-series dataset which is often influenced by protected attributes like age, gender, race etc. The COVID-19 pandemic has exacerbated health inequities, with certain subgroups experiencing poorer outcomes and less access to healthcare. To combat these inequities, synthetic data must \"fairly\" represent diverse minority subgroups such that the conclusions drawn on synthetic data are correct and the results can be generalized to real data. In this article, we develop two fairness metrics for synthetic data, and analyze all subgroups defined by protected attributes to analyze the bias in three published synthetic research datasets. These covariate-level disparity metrics revealed that synthetic data may not be representative at the univariate and multivariate subgroup-levels and thus, fairness should be addressed when developing data generation methods. We discuss the need for measuring fairness in synthetic healthcare data to enable the development of robust machine learning models to create more equitable synthetic healthcare datasets.",
      "journal": "Entropy (Basel, Switzerland)",
      "year": "2021",
      "doi": "10.3390/e23091165",
      "authors": "Bhanot Karan et al.",
      "keywords": "covariate; disparate impact; fairness; health inequities; healthcare; synthetic data; temporal; time-series",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34573790/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "EHR/Health Informatics; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Data Augmentation; Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "These covariate-level disparity metrics revealed that synthetic data may not be representative at the univariate and multivariate subgroup-levels and thus, fairness should be addressed when developing data generation methods. We discuss the need for measuring fairness in synthetic healthcare data to enable the development of robust machine learning models to create more equitable synthetic healthcare datasets.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8468495"
    },
    {
      "pmid": "34670780",
      "title": "Risk of bias in studies on prediction models developed using supervised machine learning techniques: systematic review.",
      "abstract": "OBJECTIVE: To assess the methodological quality of studies on prediction models developed using machine learning techniques across all medical specialties. DESIGN: Systematic review. DATA SOURCES: PubMed from 1 January 2018 to 31 December 2019. ELIGIBILITY CRITERIA: Articles reporting on the development, with or without external validation, of a multivariable prediction model (diagnostic or prognostic) developed using supervised machine learning for individualised predictions. No restrictions applied for study design, data source, or predicted patient related health outcomes. REVIEW METHODS: Methodological quality of the studies was determined and risk of bias evaluated using the prediction risk of bias assessment tool (PROBAST). This tool contains 21 signalling questions tailored to identify potential biases in four domains. Risk of bias was measured for each domain (participants, predictors, outcome, and analysis) and each study (overall). RESULTS: 152 studies were included: 58 (38%) included a diagnostic prediction model and 94 (62%) a prognostic prediction model. PROBAST was applied to 152 developed models and 19 external validations. Of these 171 analyses, 148 (87%, 95% confidence interval 81% to 91%) were rated at high risk of bias. The analysis domain was most frequently rated at high risk of bias. Of the 152 models, 85 (56%, 48% to 64%) were developed with an inadequate number of events per candidate predictor, 62 handled missing data inadequately (41%, 33% to 49%), and 59 assessed overfitting improperly (39%, 31% to 47%). Most models used appropriate data sources to develop (73%, 66% to 79%) and externally validate the machine learning based prediction models (74%, 51% to 88%). Information about blinding of outcome and blinding of predictors was, however, absent in 60 (40%, 32% to 47%) and 79 (52%, 44% to 60%) of the developed models, respectively. CONCLUSION: Most studies on machine learning based prediction models show poor methodological quality and are at high risk of bias. Factors contributing to risk of bias include small study size, poor handling of missing data, and failure to deal with overfitting. Efforts to improve the design, conduct, reporting, and validation of such studies are necessary to boost the application of machine learning based prediction models in clinical practice. SYSTEMATIC REVIEW REGISTRATION: PROSPERO CRD42019161764.",
      "journal": "BMJ (Clinical research ed.)",
      "year": "2021",
      "doi": "10.1136/bmj.n2281",
      "authors": "Andaur Navarro Constanza L et al.",
      "keywords": "",
      "mesh_terms": "Bias; Clinical Decision Rules; Data Interpretation, Statistical; Humans; Machine Learning; Models, Statistical; Multivariate Analysis; Risk",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34670780/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Collection; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Most studies on machine learning based prediction models show poor methodological quality and are at high risk of bias. Factors contributing to risk of bias include small study size, poor handling of missing data, and failure to deal with overfitting. Efforts to improve the design, conduct, reporting, and validation of such studies are necessary to boost the application of machine learning based prediction models in clinical practice.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8527348"
    },
    {
      "pmid": "34811466",
      "title": "Advancing health equity with artificial intelligence.",
      "abstract": "Population and public health are in the midst of an artificial intelligence revolution capable of radically altering existing models of care delivery and practice. Just as AI seeks to mirror human cognition through its data-driven analytics, it can also reflect the biases present in our collective conscience. In this Viewpoint, we use past and counterfactual examples to illustrate the sequelae of unmitigated bias in healthcare artificial intelligence. Past examples indicate that if the benefits of emerging AI technologies are to be realized, consensus around the regulation of algorithmic bias at the policy level is needed to ensure their ethical integration into the health system. This paper puts forth regulatory strategies for uprooting bias in healthcare AI that can inform ongoing efforts to establish a framework for federal oversight. We highlight three overarching oversight principles in bias mitigation that maps to each phase of the algorithm life cycle.",
      "journal": "Journal of public health policy",
      "year": "2021",
      "doi": "10.1057/s41271-021-00319-5",
      "authors": "Thomasian Nicole M et al.",
      "keywords": "Algorithmic bias; Artificial intelligence; Health equity; Health policy; Machine learning",
      "mesh_terms": "Artificial Intelligence; Delivery of Health Care; Health Equity; Humans; Policy",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34811466/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Counterfactual Fairness",
      "clinical_setting": "Public Health/Population",
      "key_findings": "This paper puts forth regulatory strategies for uprooting bias in healthcare AI that can inform ongoing efforts to establish a framework for federal oversight. We highlight three overarching oversight principles in bias mitigation that maps to each phase of the algorithm life cycle.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8607970"
    },
    {
      "pmid": "34812384",
      "title": "Bias Analysis on Public X-Ray Image Datasets of Pneumonia and COVID-19 Patients.",
      "abstract": "Chest X-ray images are useful for early COVID-19 diagnosis with the advantage that X-ray devices are already available in health centers and images are obtained immediately. Some datasets containing X-ray images with cases (pneumonia or COVID-19) and controls have been made available to develop machine-learning-based methods to aid in diagnosing the disease. However, these datasets are mainly composed of different sources coming from pre-COVID-19 datasets and COVID-19 datasets. Particularly, we have detected a significant bias in some of the released datasets used to train and test diagnostic systems, which might imply that the results published are optimistic and may overestimate the actual predictive capacity of the techniques proposed. In this article, we analyze the existing bias in some commonly used datasets and propose a series of preliminary steps to carry out before the classic machine learning pipeline in order to detect possible biases, to avoid them if possible and to report results that are more representative of the actual predictive power of the methods under analysis.",
      "journal": "IEEE access : practical innovations, open solutions",
      "year": "2021",
      "doi": "10.1109/ACCESS.2021.3065456",
      "authors": "Catala Omar Del Tejo et al.",
      "keywords": "COVID-19; Deep learning; bias; chest X-ray; convolutional neural networks; saliency map; segmentation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34812384/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care; Pulmonology",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "Particularly, we have detected a significant bias in some of the released datasets used to train and test diagnostic systems, which might imply that the results published are optimistic and may overestimate the actual predictive capacity of the techniques proposed. In this article, we analyze the existing bias in some commonly used datasets and propose a series of preliminary steps to carry out before the classic machine learning pipeline in order to detect possible biases, to avoid them if poss...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8545228"
    },
    {
      "pmid": "31313407",
      "title": "Diffusion gradient nonlinearity bias correction reduces bias of breast cancer bone metastasis ADC values.",
      "abstract": "CONTRACT GRANT SPONSOR: Health Research Fund of Central Denmark Region. BACKGROUND: Diffusion gradient nonlinearity (DGNL) bias causes apparent diffusion coefficient (ADC) values to drop with increasing superior-inferior (SI) isocenter offset. This is a concern when performing quantitative diffusion-weighted imaging (DWI). PURPOSE/HYPOTHESIS: To investigate if DGNL ADC bias can be corrected in breast cancer bone metastases using a clinical DWI protocol and an online correction algorithm. STUDY TYPE: Prospective. SUBJECTS/PHANTOM: A diffusion phantom (Model 128, High Precision Devices, Boulder, CO) was used for in vitro validation. Twenty-three women with bone-metastasizing breast cancer were enrolled to assess DGNL correction in vivo. FIELD STRENGTH/SEQUENCE: DWI was performed on a 1.5T MRI system as single-shot, spin-echo, echo-planar imaging with short-tau inversion recovery (STIR) fat-saturation. ADC maps with and without DGNL correction were created from the b50 and b800 images. ASSESSMENT: Uncorrected and DGNL-corrected ADC values were measured in phantom and bone metastases by placing regions of interest on b800 images and copying them to the ADC map. The SI offset was recorded. STATISTICAL TESTS: In all, 79 bone metastases were assessed. ADC values with and without DGNL correction were compared at 14 cm SI offset using a two-tailed t-test. RESULTS: In the diffusion phantom, DGNL correction increased SI offset, where ADC bias was lower than 5%, from 7.3-13.8 cm. Of the 23 patients examined, six had no metastases in the covered regions. In the remaining patients, bias of uncorrected bone metastasis ADC values was 19.1% (95% confidence interval [CI]: 15.4-22.9%) at 14 cm SI offset. After DGNL correction, ADC bias was significantly reduced to 3.5% (95% CI: 0.7-6.3%, P\u2009<\u20090.001), thus reducing bias due to DGNL by 82%. DATA CONCLUSION: Online DGNL correction corrects DGNL ADC value bias and allows increased station lengths in the SI direction. LEVEL OF EVIDENCE: 2 Technical Efficacy: Stage 2 J. Magn. Reson. Imaging 2020;51:904-911.",
      "journal": "Journal of magnetic resonance imaging : JMRI",
      "year": "2020",
      "doi": "10.1002/jmri.26873",
      "authors": "Buus Thomas W et al.",
      "keywords": "bone marrow diseases; breast neoplasms; diffusion magnetic resonance imaging; software validation",
      "mesh_terms": "Breast Neoplasms; Diffusion Magnetic Resonance Imaging; Female; Humans; Image Interpretation, Computer-Assisted; Prospective Studies; Reproducibility of Results",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31313407/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Oncology",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Online DGNL correction corrects DGNL ADC value bias and allows increased station lengths in the SI direction. LEVEL OF EVIDENCE: 2 Technical Efficacy: Stage 2 J. Magn.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "32529043",
      "title": "Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare.",
      "abstract": "Precision Medicine implies a deep understanding of inter-individual differences in health and disease that are due to genetic and environmental factors. To acquire such understanding there is a need for the implementation of different types of technologies based on artificial intelligence (AI) that enable the identification of biomedically relevant patterns, facilitating progress towards individually tailored preventative and therapeutic interventions. Despite the significant scientific advances achieved so far, most of the currently used biomedical AI technologies do not account for bias detection. Furthermore, the design of the majority of algorithms ignore the sex and gender dimension and its contribution to health and disease differences among individuals. Failure in accounting for these differences will generate sub-optimal results and produce mistakes as well as discriminatory outcomes. In this review we examine the current sex and gender gaps in a subset of biomedical technologies used in relation to Precision Medicine. In addition, we provide recommendations to optimize their utilization to improve the global health and disease landscape and decrease inequalities.",
      "journal": "NPJ digital medicine",
      "year": "2020",
      "doi": "10.1038/s41746-020-0288-5",
      "authors": "Cirillo Davide et al.",
      "keywords": "Biomarkers; Computational models; Medical ethics; Risk factors",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32529043/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "In this review we examine the current sex and gender gaps in a subset of biomedical technologies used in relation to Precision Medicine. In addition, we provide recommendations to optimize their utilization to improve the global health and disease landscape and decrease inequalities.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7264169"
    },
    {
      "pmid": "32821854",
      "title": "Predictably unequal: understanding and addressing concerns that algorithmic clinical prediction may increase health disparities.",
      "abstract": "The machine learning community has become alert to the ways that predictive algorithms can inadvertently introduce unfairness in decision-making. Herein, we discuss how concepts of algorithmic fairness might apply in healthcare, where predictive algorithms are being increasingly used to support decision-making. Central to our discussion is the distinction between algorithmic fairness and algorithmic bias. Fairness concerns apply specifically when algorithms are used to support polar decisions (i.e., where one pole of prediction leads to decisions that are generally more desired than the other), such as when predictions are used to allocate scarce health care resources to a group of patients that could all benefit. We review different fairness criteria and demonstrate their mutual incompatibility. Even when models are used to balance benefits-harms to make optimal decisions for individuals (i.e., for non-polar decisions)-and fairness concerns are not germane-model, data or sampling issues can lead to biased predictions that support decisions that are differentially harmful/beneficial across groups. We review these potential sources of bias, and also discuss ways to diagnose and remedy algorithmic bias. We note that remedies for algorithmic fairness may be more problematic, since we lack agreed upon definitions of fairness. Finally, we propose a provisional framework for the evaluation of clinical prediction models offered for further elaboration and refinement. Given the proliferation of prediction models used to guide clinical decisions, developing consensus for how these concerns can be addressed should be prioritized.",
      "journal": "NPJ digital medicine",
      "year": "2020",
      "doi": "10.1038/s41746-020-0304-9",
      "authors": "Paulus Jessica K et al.",
      "keywords": "Health care; Medical research",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32821854/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Finally, we propose a provisional framework for the evaluation of clinical prediction models offered for further elaboration and refinement. Given the proliferation of prediction models used to guide clinical decisions, developing consensus for how these concerns can be addressed should be prioritized.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7393367"
    },
    {
      "pmid": "32838979",
      "title": "Bias and ethical considerations in machine learning and the automation of perioperative risk assessment.",
      "abstract": "",
      "journal": "British journal of anaesthesia",
      "year": "2020",
      "doi": "10.1016/j.bja.2020.07.040",
      "authors": "O'Reilly-Shah Vikas N et al.",
      "keywords": "artificial intelligence; gender bias; healthcare inequality; machine learning; perioperative medicine; racial bias; risk prediction",
      "mesh_terms": "Anesthesiology; Automation; Bias; Humans; Machine Learning; Perioperative Care; Risk Assessment",
      "pub_types": "Editorial; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32838979/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Surgery",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7442146"
    },
    {
      "pmid": "33090117",
      "title": "A Racially Unbiased, Machine Learning Approach to Prediction of Mortality: Algorithm Development Study.",
      "abstract": "BACKGROUND: Racial disparities in health care are well documented in the United States. As machine learning methods become more common in health care settings, it is important to ensure that these methods do not contribute to racial disparities through biased predictions or differential accuracy across racial groups. OBJECTIVE: The goal of the research was to assess a machine learning algorithm intentionally developed to minimize bias in in-hospital mortality predictions between white and nonwhite patient groups. METHODS: Bias was minimized through preprocessing of algorithm training data. We performed a retrospective analysis of electronic health record data from patients admitted to the intensive care unit (ICU) at a large academic health center between 2001 and 2012, drawing data from the Medical Information Mart for Intensive Care-III database. Patients were included if they had at least 10 hours of available measurements after ICU admission, had at least one of every measurement used for model prediction, and had recorded race/ethnicity data. Bias was assessed through the equal opportunity difference. Model performance in terms of bias and accuracy was compared with the Modified Early Warning Score (MEWS), the Simplified Acute Physiology Score II (SAPS II), and the Acute Physiologic Assessment and Chronic Health Evaluation (APACHE). RESULTS: The machine learning algorithm was found to be more accurate than all comparators, with a higher sensitivity, specificity, and area under the receiver operating characteristic. The machine learning algorithm was found to be unbiased (equal opportunity difference 0.016, P=.20). APACHE was also found to be unbiased (equal opportunity difference 0.019, P=.11), while SAPS II and MEWS were found to have significant bias (equal opportunity difference 0.038, P=.006 and equal opportunity difference 0.074, P<.001, respectively). CONCLUSIONS: This study indicates there may be significant racial bias in commonly used severity scoring systems and that machine learning algorithms may reduce bias while improving on the accuracy of these methods.",
      "journal": "JMIR public health and surveillance",
      "year": "2020",
      "doi": "10.2196/22400",
      "authors": "Allen Angier et al.",
      "keywords": "health disparities; machine learning; mortality; prediction; racial disparities",
      "mesh_terms": "APACHE; Adult; Aged; Algorithms; Cohort Studies; Early Warning Score; Electronic Health Records; Female; Forecasting; Hospital Mortality; Humans; Machine Learning; Male; Middle Aged; Retrospective Studies; Simplified Acute Physiology Score",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33090117/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "Hospital/Inpatient; ICU",
      "key_findings": "CONCLUSIONS: This study indicates there may be significant racial bias in commonly used severity scoring systems and that machine learning algorithms may reduce bias while improving on the accuracy of these methods.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7644374"
    },
    {
      "pmid": "33332380",
      "title": "Artificial Intelligence in mental health and the biases of language based models.",
      "abstract": "BACKGROUND: The rapid integration of Artificial Intelligence (AI) into the healthcare field has occurred with little communication between computer scientists and doctors. The impact of AI on health outcomes and inequalities calls for health professionals and data scientists to make a collaborative effort to ensure historic health disparities are not encoded into the future. We present a study that evaluates bias in existing Natural Language Processing (NLP) models used in psychiatry and discuss how these biases may widen health inequalities. Our approach systematically evaluates each stage of model development to explore how biases arise from a clinical, data science and linguistic perspective. DESIGN/METHODS: A literature review of the uses of NLP in mental health was carried out across multiple disciplinary databases with defined Mesh terms and keywords. Our primary analysis evaluated biases within 'GloVe' and 'Word2Vec' word embeddings. Euclidean distances were measured to assess relationships between psychiatric terms and demographic labels, and vector similarity functions were used to solve analogy questions relating to mental health. RESULTS: Our primary analysis of mental health terminology in GloVe and Word2Vec embeddings demonstrated significant biases with respect to religion, race, gender, nationality, sexuality and age. Our literature review returned 52 papers, of which none addressed all the areas of possible bias that we identify in model development. In addition, only one article existed on more than one research database, demonstrating the isolation of research within disciplinary silos and inhibiting cross-disciplinary collaboration or communication. CONCLUSION: Our findings are relevant to professionals who wish to minimize the health inequalities that may arise as a result of AI and data-driven algorithms. We offer primary research identifying biases within these technologies and provide recommendations for avoiding these harms in the future.",
      "journal": "PloS one",
      "year": "2020",
      "doi": "10.1371/journal.pone.0240376",
      "authors": "Straw Isabel et al.",
      "keywords": "",
      "mesh_terms": "Bias; Data Science; Health Status Disparities; Humans; Intersectoral Collaboration; Linguistics; Mental Health; Natural Language Processing; Psychiatry",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33332380/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Representation Learning",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Our findings are relevant to professionals who wish to minimize the health inequalities that may arise as a result of AI and data-driven algorithms. We offer primary research identifying biases within these technologies and provide recommendations for avoiding these harms in the future.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7745984"
    },
    {
      "pmid": "33981989",
      "title": "Addressing Fairness, Bias, and Appropriate Use of Artificial Intelligence and Machine Learning in Global Health.",
      "abstract": "In Low- and Middle- Income Countries (LMICs), machine learning (ML) and artificial intelligence (AI) offer attractive solutions to address the shortage of health care resources and improve the capacity of the local health care infrastructure. However, AI and ML should also be used cautiously, due to potential issues of fairness and algorithmic bias that may arise if not applied properly. Furthermore, populations in LMICs can be particularly vulnerable to bias and fairness in AI algorithms, due to a lack of technical capacity, existing social bias against minority groups, and a lack of legal protections. In order to address the need for better guidance within the context of global health, we describe three basic criteria (Appropriateness, Fairness, and Bias) that can be used to help evaluate the use of machine learning and AI systems: 1) APPROPRIATENESS is the process of deciding how the algorithm should be used in the local context, and properly matching the machine learning model to the target population; 2) BIAS is a systematic tendency in a model to favor one demographic group vs another, which can be mitigated but can lead to unfairness; and 3) FAIRNESS involves examining the impact on various demographic groups and choosing one of several mathematical definitions of group fairness that will adequately satisfy the desired set of legal, cultural, and ethical requirements. Finally, we illustrate how these principles can be applied using a case study of machine learning applied to the diagnosis and screening of pulmonary disease in Pune, India. We hope that these methods and principles can help guide researchers and organizations working in global health who are considering the use of machine learning and artificial intelligence.",
      "journal": "Frontiers in artificial intelligence",
      "year": "2020",
      "doi": "10.3389/frai.2020.561802",
      "authors": "Fletcher Richard Rib\u00f3n et al.",
      "keywords": "appropriate use; artificial intelligence; bias; ethics; fairness; global health; machine learning; medicine",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33981989/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Generative AI",
      "health_domain": "ICU/Critical Care; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "Finally, we illustrate how these principles can be applied using a case study of machine learning applied to the diagnosis and screening of pulmonary disease in Pune, India. We hope that these methods and principles can help guide researchers and organizations working in global health who are considering the use of machine learning and artificial intelligence.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8107824"
    },
    {
      "pmid": "30794127",
      "title": "Can AI Help Reduce Disparities in General Medical and Mental Health Care?",
      "abstract": "BACKGROUND: As machine learning becomes increasingly common in health care applications, concerns have been raised about bias in these systems' data, algorithms, and recommendations. Simply put, as health care improves for some, it might not improve for all. METHODS: Two case studies are examined using a machine learning algorithm on unstructured clinical and psychiatric notes to predict intensive care unit (ICU) mortality and 30-day psychiatric readmission with respect to race, gender, and insurance payer type as a proxy for socioeconomic status. RESULTS: Clinical note topics and psychiatric note topics were heterogenous with respect to race, gender, and insurance payer type, which reflects known clinical findings. Differences in prediction accuracy and therefore machine bias are shown with respect to gender and insurance type for ICU mortality and with respect to insurance policy for psychiatric 30-day readmission. CONCLUSIONS: This analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care.",
      "journal": "AMA journal of ethics",
      "year": "2019",
      "doi": "10.1001/amajethics.2019.167",
      "authors": "Chen Irene Y et al.",
      "keywords": "",
      "mesh_terms": "Adult; Aged; Aged, 80 and over; Artificial Intelligence; Delivery of Health Care; Female; Healthcare Disparities; Humans; Intensive Care Units; Male; Mental Health Services; Middle Aged; Mortality; Patient Readmission; Sex Factors",
      "pub_types": "Comparative Study; Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30794127/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry; ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSIONS: This analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "31795063",
      "title": "Bias-adjustment in neuroimaging-based brain age frameworks: A robust scheme.",
      "abstract": "The level of prediction error in the brain age estimation frameworks is associated with the authenticity of statistical inference on the basis of regression models. In this paper, we present an efficacious and plain bias-adjustment scheme using chronological age as a covariate through the training set for downgrading the prediction bias in a Brain-age estimation framework. We applied proposed bias-adjustment scheme coupled by a machine learning-based brain age framework on a large set of metabolic brain features acquired from 675 cognitively unimpaired adults through fluorodeoxyglucose positron emission tomography data as the training set to build a robust Brain-age estimation framework. Then, we tested the reliability of proposed bias-adjustment scheme on 75 cognitively unimpaired adults, 561 mild cognitive impairment patients as well as 362 Alzheimer's disease patients as independent test sets. Using the proposed method, we gained a strong R2 of 0.81 between the chronological age and brain estimated age, as well as an excellent mean absolute error of 2.66 years on 75 cognitively unimpaired adults as an independent set; whereas an R2 of 0.24 and a mean absolute error of 4.71 years was achieved without bias-adjustment. The simulation results demonstrated that the proposed bias-adjustment scheme has a strong capability to diminish prediction error in brain age estimation frameworks for clinical settings.",
      "journal": "NeuroImage. Clinical",
      "year": "2019",
      "doi": "10.1016/j.nicl.2019.102063",
      "authors": "Beheshti Iman et al.",
      "keywords": "Bias-adjustment; Brain age; Brain metabolism; Estimation; Pet",
      "mesh_terms": "Aged; Aged, 80 and over; Brain; Female; Humans; Image Processing, Computer-Assisted; Machine Learning; Male; Middle Aged; Neuroimaging; Positron-Emission Tomography",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31795063/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Regression",
      "health_domain": "Neurology; Endocrinology/Diabetes",
      "bias_axes": "Gender/Sex; Age; Disability",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Using the proposed method, we gained a strong R2 of 0.81 between the chronological age and brain estimated age, as well as an excellent mean absolute error of 2.66 years on 75 cognitively unimpaired adults as an independent set; whereas an R2 of 0.24 and a mean absolute error of 4.71 years was achieved without bias-adjustment. The simulation results demonstrated that the proposed bias-adjustment scheme has a strong capability to diminish prediction error in brain age estimation frameworks for cl...",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC6861562"
    },
    {
      "pmid": "29253575",
      "title": "Brain extraction in partial volumes T2*@7T by using a quasi-anatomic segmentation with bias field correction.",
      "abstract": "BACKGROUND: Poor brain extraction in Magnetic Resonance Imaging (MRI) has negative consequences in several types of brain post-extraction such as tissue segmentation and related statistical measures or pattern recognition algorithms. Current state of the art algorithms for brain extraction work on weighted T1 and T2, being not adequate for non-whole brain images such as the case of T2*FLASH@7T partial volumes. NEW METHOD: This paper proposes two new methods that work directly in T2*FLASH@7T partial volumes. The first is an improvement of the semi-automatic threshold-with-morphology approach adapted to incomplete volumes. The second method uses an improved version of a current implementation of the fuzzy c-means algorithm with bias correction for brain segmentation. RESULTS: Under high inhomogeneity conditions the performance of the first method degrades, requiring user intervention which is unacceptable. The second method performed well for all volumes, being entirely automatic. COMPARISON WITH EXISTING METHODS: State of the art algorithms for brain extraction are mainly semi-automatic, requiring a correct initialization by the user and knowledge of the software. These methods can't deal with partial volumes and/or need information from atlas which is not available in T2*FLASH@7T. Also, combined volumes suffer from manipulations such as re-sampling which deteriorates significantly voxel intensity structures making segmentation tasks difficult. The proposed method can overcome all these difficulties, reaching good results for brain extraction using only T2*FLASH@7T volumes. CONCLUSIONS: The development of this work will lead to an improvement of automatic brain lesions segmentation in T2*FLASH@7T volumes, becoming more important when lesions such as cortical Multiple-Sclerosis need to be detected.",
      "journal": "Journal of neuroscience methods",
      "year": "2018",
      "doi": "10.1016/j.jneumeth.2017.12.006",
      "authors": "Valente Jo\u00e3o et al.",
      "keywords": "Brain extraction; High resolution MRI; Multiple sclerosis; Partial brain scanning",
      "mesh_terms": "Algorithms; Brain; Fuzzy Logic; Head; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Movement; Pattern Recognition, Automated; Software",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29253575/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSIONS: The development of this work will lead to an improvement of automatic brain lesions segmentation in T2*FLASH@7T volumes, becoming more important when lesions such as cortical Multiple-Sclerosis need to be detected.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "30064041",
      "title": "Potential of near infrared spectroscopy and pattern recognition for rapid discrimination and quantification of Gleditsia sinensis thorn powder with adulterants.",
      "abstract": "The Gleditsia sinensis Lam thorn (GST) is a classical traditional Chinese medical herb, which is of high medical and economic value. GST could be easily adulterated with branch of Rosa multiflora thunb (BRM) and Rosa rugosa thumb (BRR), because of their similar appearances and much lower cost for these adulterants. In this study Fourier transform near-infrared spectroscopy (FT-NIR) combined with chemical pattern recognition techniques was explored for the first time to discriminate and quantify of cheaper materials (BRM and BRR) in GST. The Savitzkye-Golay (SG) smoothing, vector normalization (VN), min max normalization (MMN), first derivative (1\u202fst D) and second derivative (2nd D) methods were used to pre-process the raw FT-NIR spectra. Successive projections algorithm was adopted to select the characteristic variables and linear discriminate analysis (LDA), support vector machine (SVM), as while as back propagation neural network (BPNN) algorithms were applied to construct the identification models. Results showed that BPNN models performance best compared with LDA and SVM models for it could reach 100% accuracy for identifying authentic GST, and GST adulterated with BRM and BRR based on the spectral region of 6500-5500 cm-1 combined with 1\u202fst D pre-processing. In addition, the BRM and BRR content in adulterated GST were determined by partial least squares (PLS) regression. The correlation coefficient of prediction (rp), root mean square error of prediction (RMSEP) and bias for the prediction by PLS regression model were 0.9972, 1.969% and 0.3198 for BRM, 0.9972, 1.879% and 0.05408 for BRR, respectively. These results suggest that the combination of NIR spectroscopy and chemometric methods offers a simple, fast and reliable method for classification and quantification in the quality control of the tradition Chinese medicine herb of GST.",
      "journal": "Journal of pharmaceutical and biomedical analysis",
      "year": "2018",
      "doi": "10.1016/j.jpba.2018.07.036",
      "authors": "Wang Lijun et al.",
      "keywords": "BPNN; Gleditsia sinensis thorn; LDA; Near-infrared spectroscopy; PLS regression; SVM",
      "mesh_terms": "Drug Contamination; Gleditsia; Models, Chemical; Neural Networks, Computer; Powders; Quality Control; Reproducibility of Results; Spectroscopy, Near-Infrared; Support Vector Machine; Technology, Pharmaceutical",
      "pub_types": "Journal Article; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30064041/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Support Vector Machine; Neural Network; Regression",
      "health_domain": "Drug Discovery/Pharmacology",
      "bias_axes": "Geographic",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The correlation coefficient of prediction (rp), root mean square error of prediction (RMSEP) and bias for the prediction by PLS regression model were 0.9972, 1.969% and 0.3198 for BRM, 0.9972, 1.879% and 0.05408 for BRR, respectively. These results suggest that the combination of NIR spectroscopy and chemometric methods offers a simple, fast and reliable method for classification and quantification in the quality control of the tradition Chinese medicine herb of GST.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "27271102",
      "title": "Can Bias Evaluation Provide Protection Against False-Negative Results in QT Studies Without a Positive Control Using Exposure-Response Analysis?",
      "abstract": "The revised ICH E14 document allows the use of exposure-response analysis to exclude a small QT effect of a drug. If plasma concentrations exceeding clinically relevant levels is achieved, a positive control is not required. In cases when this cannot be achieved, there may be a need for metrics to protect against false-negative results. The objectives of this study were to create bias in electrocardiogram laboratory QT-interval measurements and define a metric that can be used to detect bias severe enough to cause false-negative results using exposure-response analysis. Data from the IQ-CSRC study, which evaluated the QT effect of 5 QT-prolonging drugs, were used. Negative bias using 3 deterministic and 2 random methods was introduced into the reported QTc values and compared with fully automated data from the underlying electrocardiogram algorithm (COMPAS). The slope estimate of the Bland-Altman plot was used as a bias metric. With the deterministic bias methods, negative bias, measured between electrocardiogram laboratory values and COMPAS, had to be larger than approximately -20 milliseconds over a QTcF range of 100 milliseconds to cause failures to predict the QT effect of ondansetron, quinine, dolasetron, moxifloxacin, and dofetilide. With the random methods, the rate of false-negatives was \u22645% with bias severity < -10 milliseconds for all 5 drugs when plasma levels exceeded those of interest. Severe and therefore detectable bias has to be introduced into reported QTc values to cause false-negative predictions with exposure-response analysis.",
      "journal": "Journal of clinical pharmacology",
      "year": "2017",
      "doi": "10.1002/jcph.779",
      "authors": "Ferber Georg et al.",
      "keywords": "QT; bias; early phase; exposure response analysis; first-in-human; positive control",
      "mesh_terms": "Cardiotoxins; Dose-Response Relationship, Drug; Electrocardiography; Heart Conduction System; Heart Rate; Humans; Long QT Syndrome",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/27271102/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "With the random methods, the rate of false-negatives was \u22645% with bias severity < -10 milliseconds for all 5 drugs when plasma levels exceeded those of interest. Severe and therefore detectable bias has to be introduced into reported QTc values to cause false-negative predictions with exposure-response analysis.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "28114009",
      "title": "A New Variational Method for Bias Correction and Its Applications to Rodent Brain Extraction.",
      "abstract": "Brain extraction is an important preprocessing step for further analysis of brain MR images. Significant intensity inhomogeneity can be observed in rodent brain images due to the high-field MRI technique. Unlike most existing brain extraction methods that require bias corrected MRI, we present a high-order and L0 regularized variational model for bias correction and brain extraction. The model is composed of a data fitting term, a piecewise constant regularization and a smooth regularization, which is constructed on a 3-D formulation for medical images with anisotropic voxel sizes. We propose an efficient multi-resolution algorithm for fast computation. At each resolution layer, we solve an alternating direction scheme, all subproblems of which have the closed-form solutions. The method is tested on three T2 weighted acquisition configurations comprising a total of 50 rodent brain volumes, which are with the acquisition field strengths of 4.7 Tesla, 9.4 Tesla and 17.6 Tesla, respectively. On one hand, we compare the results of bias correction with N3 and N4 in terms of the coefficient of variations on 20 different tissues of rodent brain. On the other hand, the results of brain extraction are compared against manually segmented gold standards, BET, BSE and 3-D PCNN based on a number of metrics. With the high accuracy and efficiency, our proposed method can facilitate automatic processing of large-scale brain studies.",
      "journal": "IEEE transactions on medical imaging",
      "year": "2017",
      "doi": "10.1109/TMI.2016.2636026",
      "authors": "Chang Huibin et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Animals; Brain; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Mice; Mice, Inbred C57BL; Phantoms, Imaging",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/28114009/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Radiology/Medical Imaging; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing; Model Development/Training",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Regularization",
      "clinical_setting": "Not specified",
      "key_findings": "On the other hand, the results of brain extraction are compared against manually segmented gold standards, BET, BSE and 3-D PCNN based on a number of metrics. With the high accuracy and efficiency, our proposed method can facilitate automatic processing of large-scale brain studies.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "28615228",
      "title": "Stronger Together: Aggregated Z-values of Traditional Quality Control Measurements and Patient Medians Improve Detection of Biases.",
      "abstract": "BACKGROUND: In clinical chemistry, quality control (QC) often relies on measurements of control samples, but limitations, such as a lack of commutability, compromise the ability of such measurements to detect out-of-control situations. Medians of patient results have also been used for QC purposes, but it may be difficult to distinguish changes observed in the patient population from analytical errors. This study aims to combine traditional control measurements and patient medians for facilitating detection of biases. METHODS: The software package \"rSimLab\" was developed to simulate measurements of 5 analytes. Internal QC measurements and patient medians were assessed for detecting impermissible biases. Various control rules combined these parameters. A Westgard-like algorithm was evaluated and new rules that aggregate Z-values of QC parameters were proposed. RESULTS: Mathematical approximations estimated the required sample size for calculating meaningful patient medians. The appropriate number was highly dependent on the ratio of the spread of sample values to their center. Instead of applying a threshold to each QC parameter separately like the Westgard algorithm, the proposed aggregation of Z-values averaged these parameters. This behavior was found beneficial, as a bias could affect QC parameters unequally, resulting in differences between their Z-transformed values. In our simulations, control rules tended to outperform the simple QC parameters they combined. The inclusion of patient medians substantially improved bias detection for some analytes. CONCLUSIONS: Patient result medians can supplement traditional QC, and aggregations of Z-values are novel and beneficial tools for QC strategies to detect biases.",
      "journal": "Clinical chemistry",
      "year": "2017",
      "doi": "10.1373/clinchem.2016.269845",
      "authors": "Bietenbeck Andreas et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Bias; Chemistry, Clinical; Humans; Laboratories; Quality Control; Software",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/28615228/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSIONS: Patient result medians can supplement traditional QC, and aggregations of Z-values are novel and beneficial tools for QC strategies to detect biases.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "29312464",
      "title": "Correcting Classifiers for Sample Selection Bias in Two-Phase Case-Control Studies.",
      "abstract": "Epidemiological studies often utilize stratified data in which rare outcomes or exposures are artificially enriched. This design can increase precision in association tests but distorts predictions when applying classifiers on nonstratified data. Several methods correct for this so-called sample selection bias, but their performance remains unclear especially for machine learning classifiers. With an emphasis on two-phase case-control studies, we aim to assess which corrections to perform in which setting and to obtain methods suitable for machine learning techniques, especially the random forest. We propose two new resampling-based methods to resemble the original data and covariance structure: stochastic inverse-probability oversampling and parametric inverse-probability bagging. We compare all techniques for the random forest and other classifiers, both theoretically and on simulated and real data. Empirical results show that the random forest profits from only the parametric inverse-probability bagging proposed by us. For other classifiers, correction is mostly advantageous, and methods perform uniformly. We discuss consequences of inappropriate distribution assumptions and reason for different behaviors between the random forest and other classifiers. In conclusion, we provide guidance for choosing correction methods when training classifiers on biased samples. For random forests, our method outperforms state-of-the-art procedures if distribution assumptions are roughly fulfilled. We provide our implementation in the R package sambia.",
      "journal": "Computational and mathematical methods in medicine",
      "year": "2017",
      "doi": "10.1155/2017/7847531",
      "authors": "Krautenbacher Norbert et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Case-Control Studies; Computer Simulation; Hepatitis; Humans; Machine Learning; Patient Selection; Research Design; Selection Bias; Software",
      "pub_types": "Journal Article; Randomized Controlled Trial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29312464/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Random Forest; Ensemble Methods",
      "health_domain": "Infectious Disease",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Reweighting/Resampling",
      "clinical_setting": "Not specified",
      "key_findings": "For random forests, our method outperforms state-of-the-art procedures if distribution assumptions are roughly fulfilled. We provide our implementation in the R package sambia.",
      "ft_include": true,
      "ft_reason": "Included: discusses approaches for bias assessment/mitigation in health AI",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC5632994"
    },
    {
      "pmid": "30288421",
      "title": "Efficient and Fair Heart Allocation Policies for Transplantation.",
      "abstract": "Background: The optimal allocation of limited donated hearts to patients on the waiting list is one of the top priorities in heart transplantation management. We developed a simulation model of the US waiting list for heart transplantation to investigate the potential impacts of allocation policies on several outcomes such as pre- and posttransplant mortality. Methods: We used data from the United Network for Organ Sharing (UNOS) and the Scientific Registry of Transplant Recipient (SRTR) to simulate the heart allocation system. The model is validated by comparing the outcomes of the simulation with historical data. We also adapted fairness schemes studied in welfare economics to provide a framework to assess the fairness of allocation policies for transplantation. We considered three allocation policies, each a modification to the current UNOS allocation policy, and analyzed their performance via simulation. The first policy broadens the geographical allocation zones, the second modifies the health status order for receiving hearts, and the third prioritizes patients according to their waiting time. Results: Our results showed that the allocation policy similar to the current UNOS practice except that it aggregates the three immediate geographical allocation zones, improves the health outcomes, and is \"closer\" to an optimal fair policy compared to all other policies considered in this study. Specifically, this policy could have saved 319 total deaths (out of 3738 deaths) during the 2006 to 2014 time horizon, in average. This policy slightly differs from the current UNOS allocation policy and allows for easy implementation. Conclusion: We developed a model to compare the outcomes of heart allocation policies. Combining the three immediate geographical zones in the current allocation algorithm could potentially reduce mortality rate and is closer to an optimal fair policy.",
      "journal": "MDM policy & practice",
      "year": "2017",
      "doi": "10.1177/2381468317709475",
      "authors": "Hasankhani Farhad et al.",
      "keywords": "allocation policy; fairness; heart failure; simulation; survival; transplantation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30288421/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Generative AI",
      "health_domain": "Cardiology",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Conclusion: We developed a model to compare the outcomes of heart allocation policies. Combining the three immediate geographical zones in the current allocation algorithm could potentially reduce mortality rate and is closer to an optimal fair policy.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC6125046"
    },
    {
      "pmid": "24525488",
      "title": "Evaluating treatment effectiveness under model misspecification: A comparison of targeted maximum likelihood estimation with bias-corrected matching.",
      "abstract": "Statistical approaches for estimating treatment effectiveness commonly model the endpoint, or the propensity score, using parametric regressions such as generalised linear models. Misspecification of these models can lead to biased parameter estimates. We compare two approaches that combine the propensity score and the endpoint regression, and can make weaker modelling assumptions, by using machine learning approaches to estimate the regression function and the propensity score. Targeted maximum likelihood estimation is a double-robust method designed to reduce bias in the estimate of the parameter of interest. Bias-corrected matching reduces bias due to covariate imbalance between matched pairs by using regression predictions. We illustrate the methods in an evaluation of different types of hip prosthesis on the health-related quality of life of patients with osteoarthritis. We undertake a simulation study, grounded in the case study, to compare the relative bias, efficiency and confidence interval coverage of the methods. We consider data generating processes with non-linear functional form relationships, normal and non-normal endpoints. We find that across the circumstances considered, bias-corrected matching generally reported less bias, but higher variance than targeted maximum likelihood estimation. When either targeted maximum likelihood estimation or bias-corrected matching incorporated machine learning, bias was much reduced, compared to using misspecified parametric models.",
      "journal": "Statistical methods in medical research",
      "year": "2016",
      "doi": "10.1177/0962280214521341",
      "authors": "Kreif No\u00e9mi et al.",
      "keywords": "bias-corrected matching; double robustness; machine learning; model misspecification; targeted maximum likelihood estimation; treatment effectiveness",
      "mesh_terms": "Aged; Bias; Computer Simulation; Confidence Intervals; Data Interpretation, Statistical; Hip Prosthesis; Humans; Likelihood Functions; Machine Learning; Male; Models, Statistical; Osteoarthritis; Quality of Life; Treatment Outcome",
      "pub_types": "Comparative Study; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/24525488/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We find that across the circumstances considered, bias-corrected matching generally reported less bias, but higher variance than targeted maximum likelihood estimation. When either targeted maximum likelihood estimation or bias-corrected matching incorporated machine learning, bias was much reduced, compared to using misspecified parametric models.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC5051604"
    },
    {
      "pmid": "25966488",
      "title": "Automating risk of bias assessment for clinical trials.",
      "abstract": "Systematic reviews, which summarize the entirety of the evidence pertaining to a specific clinical question, have become critical for evidence-based decision making in healthcare. But such reviews have become increasingly onerous to produce due to the exponentially expanding biomedical literature base. This study proposes a step toward mitigating this problem by automating risk of bias assessment in systematic reviews, in which reviewers determine whether study results may be affected by biases (e.g., poor randomization or blinding). Conducting risk of bias assessment is an important but onerous task. We thus describe a machine learning approach to automate this assessment, using the standard Cochrane Risk of Bias Tool which assesses seven common types of bias. Training such a system would typically require a large labeled corpus, which would be prohibitively expensive to collect here. Instead, we use distant supervision, using data from the Cochrane Database of Systematic Reviews (a large repository of systematic reviews), to pseudoannotate a corpus of 2200 clinical trial reports in PDF format. We then develop a joint model which, using the full text of a clinical trial report as input, predicts the risks of bias while simultaneously extracting the text fragments supporting these assessments. This study represents a step toward automating or semiautomating extraction of data necessary for the synthesis of clinical trials.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2015",
      "doi": "10.1109/JBHI.2015.2431314",
      "authors": "Marshall Iain J et al.",
      "keywords": "",
      "mesh_terms": "Bias; Clinical Trials as Topic; Humans; Medical Informatics; Natural Language Processing; Risk Assessment",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/25966488/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Clinical Trial",
      "key_findings": "We then develop a joint model which, using the full text of a clinical trial report as input, predicts the risks of bias while simultaneously extracting the text fragments supporting these assessments. This study represents a step toward automating or semiautomating extraction of data necessary for the synthesis of clinical trials.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "20879389",
      "title": "Standing on the shoulders of giants: improving medical image segmentation via bias correction.",
      "abstract": "We propose a simple strategy to improve automatic medical image segmentation. The key idea is that without deep understanding of a segmentation method, we can still improve its performance by directly calibrating its results with respect to manual segmentation. We formulate the calibration process as a bias correction problem, which is addressed by machine learning using training data. We apply this methodology on three segmentation problems/methods and show significant improvements for all of them.",
      "journal": "Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "year": "2010",
      "doi": "10.1007/978-3-642-15711-0_14",
      "authors": "Wang Hongzhi et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artifacts; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/20879389/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Calibration",
      "clinical_setting": "Not specified",
      "key_findings": "We formulate the calibration process as a bias correction problem, which is addressed by machine learning using training data. We apply this methodology on three segmentation problems/methods and show significant improvements for all of them.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic with approach content",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC3095022"
    },
    {
      "pmid": "18065243",
      "title": "The impact of skull-stripping and radio-frequency bias correction on grey-matter segmentation for voxel-based morphometry.",
      "abstract": "This study evaluates the application of (i) skull-stripping methods (hybrid watershed algorithm (HWA), brain surface extractor (BSE) and brain-extraction tool (BET2)) and (ii) bias correction algorithms (nonparametric nonuniform intensity normalisation (N3), bias field corrector (BFC) and FMRIB's automated segmentation tool (FAST)) as pre-processing pipelines for the technique of voxel-based morphometry (VBM) using statistical parametric mapping v.5 (SPM5). The pipelines were evaluated using a BrainWeb phantom, and those that performed consistently were further assessed using artificial-lesion masks applied to 10 healthy controls compared to the original unlesioned scans, and finally, 20 Alzheimer's disease (AD) patients versus 23 controls. In each case, pipelines were compared to each other and to those from default SPM5 methodology. The BET2+N3 pipeline was found to produce the least miswarping to template induced by real abnormalities, and performed consistently better than the other methods for the above experiments. Occasionally, the clusters of significant differences located close to the boundary were dragged out of the glass-brain projections -- this could be corrected by adding background noise to low-probability voxels in the grey matter segments. This method was confirmed in a one-dimensional simulation and was preferable to threshold and explicit (simple) masking which excluded true abnormalities.",
      "journal": "NeuroImage",
      "year": "2008",
      "doi": "10.1016/j.neuroimage.2007.10.051",
      "authors": "Acosta-Cabronero Julio et al.",
      "keywords": "",
      "mesh_terms": "Adult; Aged; Algorithms; Alzheimer Disease; Brain; Brain Mapping; Echo-Planar Imaging; Female; Humans; Image Processing, Computer-Assisted; Male; Middle Aged; Models, Statistical; Skull",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/18065243/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Neurology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "Not specified",
      "key_findings": "Occasionally, the clusters of significant differences located close to the boundary were dragged out of the glass-brain projections -- this could be corrected by adding background noise to low-probability voxels in the grey matter segments. This method was confirmed in a one-dimensional simulation and was preferable to threshold and explicit (simple) masking which excluded true abnormalities.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "19110834",
      "title": "Human factors of the confirmation bias in intelligence analysis: decision support from graphical evidence landscapes.",
      "abstract": "OBJECTIVE: This study addresses the human factors challenge of designing and validating decision support to promote less biased intelligence analysis. BACKGROUND: The confirmation bias can compromise objectivity in ambiguous medical and military decision making through neglect of conflicting evidence and judgments not reflective of the entire evidence spectrum. Previous debiasing approaches have had mixed success and have tended to place additional demands on users' decision making. METHOD: Two new debiasing interventions that help analysts picture the full spectrum of evidence, the relation of evidence to a hypothesis, and other analysts' evidence assessments were manipulated in a repeated-measures design: (a) an integrated graphical evidence layout, compared with a text baseline; and (b) evidence tagged with other analysts' assessments, compared with participants' own assessments. Twenty-seven naval trainee analysts and reservists assessed, selected, and prioritized evidence in analysis vignettes carefully constructed to have balanced supporting and conflicting evidence sets. Bias was measured for all three evidence analysis steps. RESULTS: A bias to select a skewed distribution of confirming evidence occurred across conditions. However, graphical evidence layout, but not other analysts' assessments, significantly reduced this selection bias, resulting in more balanced evidence selection. Participants systematically prioritized the most supportive evidence as most important. CONCLUSION: Domain experts exhibited confirmation bias in a realistic intelligence analysis task and apparently conflated evidence supportiveness with importance. Graphical evidence layout promoted more balanced and less biased evidence selection. APPLICATION: Results have application to real-world decision making, implications for basic decision theory, and lessons for how shrewd visualization can help reduce bias.",
      "journal": "Human factors",
      "year": "2008",
      "doi": "10.1518/001872008X354183",
      "authors": "Cook Maia B et al.",
      "keywords": "",
      "mesh_terms": "Decision Making; Decision Support Techniques; Female; Humans; Intelligence; Male; Military Science; Observer Variation",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/19110834/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Domain experts exhibited confirmation bias in a realistic intelligence analysis task and apparently conflated evidence supportiveness with importance. Graphical evidence layout promoted more balanced and less biased evidence selection. APPLICATION: Results have application to real-world decision making, implications for basic decision theory, and lessons for how shrewd visualization can help reduce bias.",
      "ft_include": true,
      "ft_reason": "Included: bias central + approach content (abstract only)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "9291452",
      "title": "Optimal procedures for detecting analytic bias using patient samples.",
      "abstract": "We recently described the performance characteristics of the exponentially adjusted moving mean (EAMM), a patient-data, moving block mean procedure, which is a generalized algorithm that unifies Bull's algorithm and the classic average of normals (AON) procedure. Herein we describe the trend EAMM (TEAMM), a continuous signal analog of the EAMM procedure related to classic trend analysis. Using computer simulation, we have compared EAMM and TEAMM over a range of biases for various sample sizes (N or equivalent smoothing factor alpha) and exponential parameters (P) under conditions of equivalent false rejection (fixed on a per patient sample basis). We found optimal pairs of N and P for each level of bias by determination of minimum mean patient samples to rejection. Overall optimal algorithms were determined through calculation of undetected lost medical utility (ULMU), a novel function that quantifies the medical damage due to analytic bias. The ULMU function was calculated based on lost test specificity in a normal population. We found that optimized TEAMM was superior to optimized EAMM for all levels of analytic bias. If these observations hold true for non-Gaussian populations, TEAMM procedures are the method of choice for detecting bias using patient samples or as an event gauge to trigger use of known-value control materials.",
      "journal": "American journal of clinical pathology",
      "year": "1997",
      "doi": "10.1093/ajcp/108.3.254",
      "authors": "Smith F A et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Clinical Laboratory Information Systems; Computer Simulation; Humans; Laboratories, Hospital; Models, Statistical; Pathology, Clinical; Quality Control; Reproducibility of Results; Selection Bias",
      "pub_types": "Comparative Study; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/9291452/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Pathology",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We found that optimized TEAMM was superior to optimized EAMM for all levels of analytic bias. If these observations hold true for non-Gaussian populations, TEAMM procedures are the method of choice for detecting bias using patient samples or as an event gauge to trigger use of known-value control materials.",
      "ft_include": true,
      "ft_reason": "Included: bias is central topic (abstract only, needs verification)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    }
  ],
  "ft_excluded": [
    {
      "pmid": "40748022",
      "title": "Fusion of Artificial Intelligence and Mind-Body Medicine for Holistic Health: A Systematic Review.",
      "abstract": "BACKGROUND: Mind-Body Medicine (MBM) is an integrative healthcare approach that employs practices such as meditation, yoga, biofeedback, and Tai Chi to promote well-being. Artificial Intelligence (AI) is increasingly used in MBM to offer data-driven, real-time, and adaptive interventions. While the impact of AI on diagnostics and treatment in healthcare is well documented, its efficacy and challenges in MBM are less studied. OBJECTIVE: This study critically evaluates the effectiveness of AI-based MBM interventions in improving mental and physiological health outcomes, compares AI-based MBM with traditional MBM interventions, compares the performance of different AI models in MBM interventions, and identifies data privacy challenges, AI explainability, and ethical concerns in AI-based MBM interventions. METHODS: The literature search was conducted across five major academic databases: PubMed, Scopus, Web of Science, IEEE Xplore, and Google Scholar. Peer-reviewed journal articles on AI-based MBM interventions with quantitative health outcomes in stress levels, heart rate variability (HRV), electroencephalography (EEG) activity, anxiety, or depression were included. Meta-analysis was performed using Cohen's d to determine effect sizes, and statistical heterogeneity was estimated using I\u00b2 statistics. RESULTS: 15 studies were selected for final analysis based on inclusion and exclusion criteria. AI-based MBM interventions were found to significantly improve both mental and physiological health outcomes, such as cortisol levels, anxiety and depression scores, HRV, and EEG alpha wave activity. Although AI-assisted MBM was found to outperform the traditional approach in personalization, engagement, and adherence rate, concerns exist about data privacy, algorithmic bias, AI explainability, and user trust issues. Furthermore, AI-driven interventions showed variability in effectiveness depending on the training datasets, with a potential risk of bias in AI-generated recommendations. CONCLUSION: AI-driven MBM interventions have immense scope in upgrading classic mind-body techniques by delivering truly personalized, adaptive, and timely interventions. However, further research and consideration are warranted to overcome ethical concerns and biases. KEYWORDS: Artificial Intelligence, Mind-Body Medicine, Holistic Health, Biofeedback, Meditation, Yoga, Tai Chi.",
      "journal": "Advances in mind-body medicine",
      "year": "2026",
      "doi": "",
      "authors": "Nadaf Husain et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Mind-Body Therapies; Holistic Health",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40748022/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: AI-driven MBM interventions have immense scope in upgrading classic mind-body techniques by delivering truly personalized, adaptive, and timely interventions. However, further research and consideration are warranted to overcome ethical concerns and biases. KEYWORDS: Artificial Intelligence, Mind-Body Medicine, Holistic Health, Biofeedback, Meditation, Yoga, Tai Chi.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40840292",
      "title": "Enhancing feature discrimination with pseudo-labels for foundation model in segmentation of 3D medical images.",
      "abstract": "Development of medical image segmentation foundation models relies on large-scale samples. However, it is more time-consuming to annotate 3D medical images than 2D natural images, making it challenging to collect sufficient annotated samples. While pseudo-labeling offers a potential solution to expand the annotated dataset, it may introduce noisy labels that can create systematic biases, particularly affecting the segmentation performance of smaller anatomical structures. To this end, we propose a pseudo-label enriched segmentation framework (PESF), which integrates confidence filtering and perturbation-based curriculum learning. To begin with, our pseudo-labeling approach applies a well-pretrained foundation model to generate pseudo-labels for previously unannotated organ categories, effectively expanding the number of classes in the original dataset. Subsequently, we develop a confidence-based filtering mechanism, leveraging a feature extraction module combined with a confidence prediction module to quantitatively assess and filter out low-quality pseudo-labels, thereby minimizing the detrimental effects of noisy pseudo-labels on the model's optimization. Furthermore, a progressive sampling strategy that integrates curriculum learning with Gaussian random perturbations is proposed, systematically introducing training samples from simpler to more complex cases, thereby enhancing the model's generalization capability across organs of varying shapes and sizes. Additionally, our theoretical analysis reveals that incorporating these extra pseudo-labeled classes strengthens feature discrimination by increasing the angular margins between class decision boundaries in the embedding space. Experimental results demonstrate that PESF achieves a 6.8% improvement in the overall average Dice Similarity Coefficient (DSC) compared to the baseline SAM-Med3D on (Amos, FLARE22, WORD, BTCV), with particularly gains in challenging anatomical structures such as the pancreas and esophagus. The code is available at https://github.com/lonezhizi/PESF.",
      "journal": "Neural networks : the official journal of the International Neural Network Society",
      "year": "2026",
      "doi": "10.1016/j.neunet.2025.107979",
      "authors": "Jin Ge et al.",
      "keywords": "Curriculum learning; Foundation model; Medical image segmentation; Pseudo-label; Segment anything model",
      "mesh_terms": "Humans; Imaging, Three-Dimensional; Neural Networks, Computer; Image Processing, Computer-Assisted; Machine Learning; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40840292/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Computer Vision/Imaging AI; Foundation Model; Generative AI",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Representation Learning; Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "Experimental results demonstrate that PESF achieves a 6.8% improvement in the overall average Dice Similarity Coefficient (DSC) compared to the baseline SAM-Med3D on (Amos, FLARE22, WORD, BTCV), with particularly gains in challenging anatomical structures such as the pancreas and esophagus. The code is available at https://github.com/lonezhizi/PESF.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41032683",
      "title": "Regulating AI in Nursing and Healthcare: Ensuring Safety, Equity, and Accessibility in the Era of Federal Innovation Policy.",
      "abstract": "The rapid integration of artificial intelligence in healthcare, accelerated by the Trump administration's 2025 AI Action Plan and private sector innovations from companies like Nvidia and Hippocratic AI, poses urgent challenges for nursing and health policy. This policy analysis examines the intersection of federal AI initiatives, emerging healthcare technologies, and nursing workforce implications through document analysis of regulatory frameworks, the federal AI Action Plan's 90+ initiatives, and insights from the American Academy of Nursing's November 2024 policy dialogue on AI transformation. The analysis reveals that while AI demonstrates measurable improvements in discrete clinical tasks-including 16% better medication assessment accuracy and 43% greater precision in identifying drug interactions at $9 per hour compared to nurses' median $41.38 hourly wage-current federal policy lacks critical healthcare-specific safeguards. The AI Action Plan's emphasis on rapid deployment and deregulation fails to address safety-net infrastructure needs, implementation pathways for vulnerable populations, or mechanisms ensuring health equity. Evidence from the Academy dialogue indicates that AI's \"technosocial reality\" fundamentally alters care delivery while potentially exacerbating disparities in underserved communities, as demonstrated by algorithmic bias in systems like Optum's care allocation algorithm. The findings suggest that achieving equitable AI integration requires comprehensive regulatory frameworks coordinating FDA, CMS, OCR, and HRSA oversight; community-centered governance approaches redistributing decision-making power to affected populations; and nursing leadership in AI development to preserve patient-centered care values. Without proactive nursing engagement in AI governance, healthcare risks adopting technologies that prioritize efficiency over the holistic, compassionate care fundamental to nursing practice.",
      "journal": "Policy, politics & nursing practice",
      "year": "2026",
      "doi": "10.1177/15271544251381228",
      "authors": "Yang Y Tony et al.",
      "keywords": "algorithms; artificial intelligence; health care delivery; health equity; health policy; nursing",
      "mesh_terms": "Artificial Intelligence; Humans; United States; Health Policy; Health Equity; Health Services Accessibility; Delivery of Health Care; Patient Safety",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41032683/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Safety-Net/Underserved",
      "key_findings": "The findings suggest that achieving equitable AI integration requires comprehensive regulatory frameworks coordinating FDA, CMS, OCR, and HRSA oversight; community-centered governance approaches redistributing decision-making power to affected populations; and nursing leadership in AI development to preserve patient-centered care values. Without proactive nursing engagement in AI governance, healthcare risks adopting technologies that prioritize efficiency over the holistic, compassionate care f...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41056294",
      "title": "Artificial Intelligence Applications in Haemophilia Care: A Narrative Review of the Literature.",
      "abstract": "INTRODUCTION: Haemophilia is a rare X-linked bleeding disorder caused by deficiencies in coagulation factors, leading to recurrent bleeding episodes, particularly in joints and muscles. Haemophilia A accounts for 80%-85% of cases, while Haemophilia B represents 15%-20%. Despite advances in treatment, challenges such as inhibitor development, treatment variability, data scarcity, algorithmic bias, and disparities in technology access persist. Artificial intelligence (AI) has the potential to improve diagnostic accuracy, prognostication, and management, advancing personalised treatment strategies. AIM: This review examines AI applications in haemophilia care, assessing their impact on diagnosis, predictive modelling, digital health solutions, and treatment optimisation while addressing limitations and ethical concerns. METHODS: A narrative review of 40 articles was conducted, focusing on AI-driven diagnostic tools, predictive modelling, digital health technologies, and treatment optimisation. Additionally, barriers to AI integration, including algorithmic bias, cost, and accessibility, were evaluated. RESULTS: AI enhances diagnostic accuracy, predicts disease severity, assesses inhibitor risks, and optimises recombinant therapies. Machine learning improves precision in robot-assisted surgeries, while AI-powered digital tools, including chatbots and wearables, support self-management and real-time monitoring. Generative AI facilitates patient education and predictive modelling, aiding clinical decision-making. AI-driven individualised prophylaxis strategies using factor mimetics and rebalancing agents are emerging. CONCLUSION: AI represents a paradigm shift toward precision medicine in haemophilia care. However, ethical concerns, data scarcity, and financial barriers limit its full potential. Future research should focus on mitigating biases, improving data availability, and refining AI-driven personalised treatment strategies to optimise patient outcomes.",
      "journal": "Haemophilia : the official journal of the World Federation of Hemophilia",
      "year": "2026",
      "doi": "10.1111/hae.70135",
      "authors": "Aramouni Karl et al.",
      "keywords": "artificial intelligence; diagnosis; haemophilia; machine learning; prognosis; treatment",
      "mesh_terms": "Humans; Artificial Intelligence; Hemophilia A; Machine Learning",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41056294/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Clinical Prediction Model; Generative AI",
      "health_domain": "ICU/Critical Care; Wearables/Remote Monitoring",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Telehealth/Remote",
      "key_findings": "CONCLUSION: AI represents a paradigm shift toward precision medicine in haemophilia care. However, ethical concerns, data scarcity, and financial barriers limit its full potential. Future research should focus on mitigating biases, improving data availability, and refining AI-driven personalised treatment strategies to optimise patient outcomes.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41126537",
      "title": "Balancing Bias and Variance in Deep Learning-Based Tumor Microstructural Parameter Mapping.",
      "abstract": "PURPOSE: Time-dependent diffusion MRI enables quantification of tumor microstructural parameters useful for diagnosis and prognosis. Nevertheless, current model fitting approaches exhibit suboptimal bias-variance trade-offs; specifically, nonlinear least squares fitting (NLLS) demonstrated low bias but high variance, whereas supervised deep learning methods trained with mean squared error loss (MSE-Net) yielded low variance but elevated bias. This study investigates these bias-variance characteristics and proposes a method to control fitting bias and variance. METHODS: Random walk with barrier model was used as a representative biophysical model. NLLS and MSE-Net were reformulated within the Bayesian framework to elucidate their bias-variance behaviors. We introduced B2V-Net, a supervised learning approach using a loss function with adjustable bias-variance weighting, to control bias-variance trade-off. B2V-Net was evaluated and compared against NLLS and MSE-Net numerically across a wide range of parameters and noise levels, as well as in vivo in patients with head and neck cancer. RESULTS: Flat posterior distributions that were not centered at ground truth parameters explained the bias-variance behaviors of NLLS and MSE-Net. B2V-Net controlled the bias-variance trade-off, achieving a 56% reduction in standard deviation relative to NLLS and an 18% reduction in bias compared to MSE-Net. In vivo parameter maps from B2V-Net demonstrated a balance between smoothness and accuracy. CONCLUSION: We demonstrated and explained the low bias-high variance of NLLS and the low variance-high bias of MSE-Net. The proposed B2V-Net can balance bias and variance. Our work provided insights and methods to guide the design of customized loss functions tailored to specific clinical imaging needs.",
      "journal": "Magnetic resonance in medicine",
      "year": "2026",
      "doi": "10.1002/mrm.70154",
      "authors": "Zou Jiaren et al.",
      "keywords": "deep learning; diffusion MRI; head and neck cancers; model fitting; tissue microstructure",
      "mesh_terms": "Humans; Deep Learning; Head and Neck Neoplasms; Algorithms; Bayes Theorem; Diffusion Magnetic Resonance Imaging; Image Processing, Computer-Assisted; Image Interpretation, Computer-Assisted; Least-Squares Analysis; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41126537/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Radiology/Medical Imaging; Oncology",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: We demonstrated and explained the low bias-high variance of NLLS and the low variance-high bias of MSE-Net. The proposed B2V-Net can balance bias and variance. Our work provided insights and methods to guide the design of customized loss functions tailored to specific clinical imaging needs.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41172593",
      "title": "CIMB-MVQA: Causal intervention on modality-specific biases for medical visual question answering.",
      "abstract": "Medical Visual Question Answering (Med-VQA) systems frequently rely on spurious visual and language cues produced by dataset biases and structural con-founders, which undermines robustness and real-world generalization. To alleviate spurious cue reliance attributable to particular confounders, we propose CIMB-MVQA, a framework for Causal Intervention on Modality-specific Biases, which suppresses cross-modal bias by explicitly modeling and adjusting for confounding factors. For unobservable visual confounders, we introduce a front-door adjustment pipeline combining contrastive representation learning, feature disentanglement, and dual semantic masking to eliminate co-occurring but non-causal visual patterns. For observable linguistic confounders, we apply a back-door adjustment strategy using a global language bias dictionary to detect spurious signals. A vision-guided pseudo-token injection mechanism is further designed to embed critical visual cues into the language stream, reducing language dominance and aligning causal semantics across modalities. This is followed by a causal graph reasoning module that explicitly intervenes in bias-inducing paths. Experiments on multiple Med-VQA benchmarks demonstrate that CIMB-MVQA significantly improves answer accuracy and causal interpretability. Additionally, on the curated imbalanced VQA-RAD* and a suite of controlled-shift datasets, confounder-level experiments consistently show robust causal generalization under realistic bias conditions. The source code is publicly available at https://github.com/cloneiq/CIMB-MVQA.",
      "journal": "Medical image analysis",
      "year": "2026",
      "doi": "10.1016/j.media.2025.103850",
      "authors": "Liu Bing et al.",
      "keywords": "Causal inference; Causal intervention; Medical visual question answering; Multimodal bias mitigation",
      "mesh_terms": "Algorithms; Cues; Language; Semantics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41172593/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Representation Learning; Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "Additionally, on the curated imbalanced VQA-RAD* and a suite of controlled-shift datasets, confounder-level experiments consistently show robust causal generalization under realistic bias conditions. The source code is publicly available at https://github.com/cloneiq/CIMB-MVQA.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41199593",
      "title": "Artificial Intelligence Use in Acne Diagnosis and Management-A Scoping Review.",
      "abstract": "Artificial intelligence (AI) techniques can allow for early diagnosis and treatment of acne. Bias in AI model training remains, leading to various challenges in achieving health equity in clinical practice. We aim to assess and provide an updated overview of (1) the types of AI-based tools developed for acne, (2) the various applications of AI in acne diagnosis and management, (3) the performance of these tools, and (4) the current data reported on skin diversity in AI model training. [Correction added on 27 December 2025, after first online publication: The preceding sentence has been corrected.] We queried PubMed, Cochrane and Scopus databases using the terms: \"acne\", \"artificial intelligence\", \"machine learning\", \"deep learning\", \"large language model\", and \"ChatGPT\". 105 articles were included for analysis. Of the 105 research articles, 96.2% (N\u2009=\u2009101) were focused on acne diagnosis only, 9.5% (N\u2009=\u200910) on acne management only, and 5.7% (N\u2009=\u20096) on both. Most manuscripts used image-based models, including deep learning (76.2%, N\u2009=\u200980), classical machine learning (9.5%, N\u2009=\u200910), and ensemble models (11.4%, N\u2009=\u200912). The ensemble models hold the highest mean accuracy (89.7%), followed by deep learning (88.5%), large language models (87.5%), and machine learning models (86.9%). Only 13% (N\u2009=\u200914) of studies reported data on patient skin color, while 4 of the 14 studies included a full spectrum of diverse skin tones. [Correction added on 27 December 2025, after first online publication: The preceding sentence has been corrected.] The application of AI algorithms in healthcare is rapidly emerging, providing significant support to providers. With ensemble models demonstrating superior performance, AI algorithm use in acne may offer a convenient method to consistently diagnose and manage patients remotely. Designing systematic guidelines that require a diverse representation of all skin colors may improve social justice in healthcare.",
      "journal": "International journal of dermatology",
      "year": "2026",
      "doi": "10.1111/ijd.70110",
      "authors": "Frederickson Katie L et al.",
      "keywords": "AI health equity; acne; artificial intelligence; dermatology; diagnosis; ethnic disparities; management; skin color",
      "mesh_terms": "Humans; Acne Vulgaris; Artificial Intelligence; Machine Learning; Deep Learning; Skin",
      "pub_types": "Journal Article; Scoping Review; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41199593/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Deep Learning; NLP/LLM; Ensemble Methods",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Development/Training; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Ensemble Methods",
      "clinical_setting": "Telehealth/Remote",
      "key_findings": "With ensemble models demonstrating superior performance, AI algorithm use in acne may offer a convenient method to consistently diagnose and manage patients remotely. Designing systematic guidelines that require a diverse representation of all skin colors may improve social justice in healthcare.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41236165",
      "title": "Public Justification and Normatively Meaningful Bias: Against Imposing Egalitarian Accounts of Algorithmic Bias.",
      "abstract": "In a number of policy, institutional, activist and advocacy contexts, attributing bias to an algorithm does not just describe the algorithm but also imposes a particular, normatively laden conception of bias on others. Given the normative content of such bias attributions, this would involve making moral demands on others to rectify the algorithm, compensate the victims of such bias and/or not unselectively deploy the algorithm. It is also the case that moral demands, especially in the above-mentioned contexts, are subject to a public justification requirement. As it turns out, the dominant accounts of bias in the literature presuppose some version of egalitarianism about justice and that any action that causally contributes to an unjust situation is itself wrong. Since these presuppositions are subject to reasonable disagreement, bias attributions in such situations are wrong because they violate the public justification requirement. In response, we develop a publicly justifiable conception of algorithmic bias.",
      "journal": "Bioethics",
      "year": "2026",
      "doi": "10.1111/bioe.70047",
      "authors": "Muralidharan Anantharaman et al.",
      "keywords": "algorithmic bias; egalitarianism; normatively meaningful bias; public justification; responsibility; structural injustice",
      "mesh_terms": "Humans; Algorithms; Social Justice; Bias; Morals; Ethical Theory",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41236165/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "Since these presuppositions are subject to reasonable disagreement, bias attributions in such situations are wrong because they violate the public justification requirement. In response, we develop a publicly justifiable conception of algorithmic bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7618478"
    },
    {
      "pmid": "41297683",
      "title": "Chloroplast structure, codon usage bias, and machine learning-based molecular identification using DNA barcoding of Sophorae Tonkinensis Radix et Rhizoma (Shan Dou Gen) and its analogues.",
      "abstract": "To investigate the complete chloroplast genome sequences and codon usage bias of \"Shan Dou Gen\" (Sophorae Tonkinensis Radix et Rhizoma) and its six easily confused species, analyze their evolutionary patterns, and evaluate the identification efficiency of four DNA barcodes combined with two machine learning methods for these seven plant species. Chloroplast gene structures of the seven species were aligned to construct phylogenetic trees. Codon usage bias was analyzed using CodonW and CUSP. Genetic distances were calculated based on the Kimura-2-Parameter model to assess the barcoding gap and reconstruct phylogenetic trees.Species discrimination capabilities of four DNA barcodes (ITS2, matK, psbA-trnH, and rbcL) were compared. Species identification was performed using BLOG and WEKA machine learning algorithms. Single-nucleotide SSRs predominated in chloroplast genomes, primarily composed of A/T bases. Complete species differentiation was achieved using cpDNA. Natural selection was the primary factor influencing codon usage bias, followed by mutation pressure. Among synonymous codons, A\u00a0>\u00a0T and G\u00a0>\u00a0C in base composition, with optimal codons ending in A/U at the third position across all seven species. All four DNA barcodes successfully discriminated Shandougen from its confusable species. The BLOG algorithm achieved >85\u00a0% identification accuracy, outperforming WEKA. This research provides a theoretical foundation for ensuring clinical medication safety, elucidating plant phylogeny, facilitating species identification, and guiding resource conservation and utilization of Shandougen and its analogues.",
      "journal": "Fitoterapia",
      "year": "2026",
      "doi": "10.1016/j.fitote.2025.107005",
      "authors": "Zheng Mengdi et al.",
      "keywords": "Chloroplast genome; Codon bias; Machine learning; Molecular identification; Shan Dou Gen",
      "mesh_terms": "DNA Barcoding, Taxonomic; Codon Usage; Machine Learning; Phylogeny; Genome, Chloroplast; Chloroplasts",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41297683/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The BLOG algorithm achieved >85\u00a0% identification accuracy, outperforming WEKA. This research provides a theoretical foundation for ensuring clinical medication safety, elucidating plant phylogeny, facilitating species identification, and guiding resource conservation and utilization of Shandougen and its analogues.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41354138",
      "title": "From Clinical Trials to Real-World Impact: Introducing a Computational Framework to Detect Endpoint Bias in Opioid Use Disorder Research.",
      "abstract": "INTRODUCTION: Clinical trial endpoints are a 'finite sequence of instructions to perform a task' (measure treatment effectiveness), making them algorithms. Consequently, they may exhibit algorithmic bias: internal and external performance can vary across demographic groups, impacting fairness, validity and clinical decision-making. METHODS: We developed the open-source Detecting Algorithmic Bias (DAB) Pipeline in Python to identify endpoint 'performance variance'-a specific algorithmic bias-as the proportion of minority participants changes. This pipeline assesses internal performance (on demographically matched test data) and external performance (on demographically diverse validation data) using metrics including F1 scores and area under the receiver operating characteristic curve (AUROC). We applied it to representative opioid use disorder (OUD) trial endpoints. RESULTS: F1 scores remained stable across minority representation levels, suggesting consistency in precision-recall balance (F1) despite demographic shifts. Conversely, AUROC measures were more sensitive, revealing significant performance variance. Training on demographically homogeneous populations boosted internal performance (accuracy within similar cohorts) but critically compromised external generalisability (accuracy within diverse cohorts). This pattern reveals an 'endpoint bias trade-off': optimising performance for homogeneous populations vs. having generalisable performance for the real world. DISCUSSION AND CONCLUSIONS: Consistently performing endpoints for one demographic profile may lose generalisability during population shifts, potentially introducing endpoint bias. Increasing minority representation in the training data consistently improved generalisability. The endpoint bias trade-off reinforces the importance of diverse recruitment in OUD trials. The DAB Pipeline helps researchers systematically pinpoint when an endpoint may suffer 'performance variance' (i.e., bias). As an open-source tool, it promotes transparent endpoint evaluation and supports selecting demographically invariant OUD endpoints.",
      "journal": "Drug and alcohol review",
      "year": "2026",
      "doi": "10.1111/dar.70085",
      "authors": "Odom Gabriel J et al.",
      "keywords": "algorithmic bias; demographic parity; open\u2010source software; opioid use disorder; performance variance",
      "mesh_terms": "Humans; Opioid-Related Disorders; Clinical Trials as Topic; Algorithms; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41354138/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Clinical Trial",
      "key_findings": "CONCLUSIONS: Consistently performing endpoints for one demographic profile may lose generalisability during population shifts, potentially introducing endpoint bias. Increasing minority representation in the training data consistently improved generalisability. The endpoint bias trade-off reinforces the importance of diverse recruitment in OUD trials.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41360694",
      "title": "Seeing Is Believing? Exploring Gender Bias in Artificial Intelligence Imagery of Specialty Doctors.",
      "abstract": "BACKGROUND: In medicine and medical education, women are disproportionately affected by gender bias. Artificial intelligence (AI) is increasingly being employed in medical education. As gender bias exists within AI, there is a risk of reinforcing gender stereotypes if AI is used to generate images of medical professionals. We examined whether the gender distribution of doctors seen in AI-generated images was representative of UK specialty trainee doctors. METHODS: Free-to-use AI text-to-image generators were used to create 1200 images across 30 specialties. NHS England recruitment data provided figures on gender. Specialties accounting for <\u20090.25% of overall recruitment were excluded as small numbers precluded meaningful analysis. Each image was independently reviewed by both authors and classified (male/female/not-classifiable). Any disagreement was resolved by discussion. 'Not-classifiable' images were removed from analysis. Gender distribution between the AI images and recruitment data was compared (chi-squared test, significance p\u2009<\u20090.05). FINDINGS: There was a significantly higher proportion of males in the AI-generated images compared to NHS specialty data (82% vs. 47%; p\u2009<\u20090.0001). Notably, both AI tools created no images of female general practitioners, orthopaedic surgeons or urologists. Conversely females were overrepresented as dermatologists, obstetricians and gynaecologists and plastic surgeons. CONCLUSION: The finding of representational and presentational gender bias in AI-generated images of doctors is consequential because 'visual culture' within medical school, and beyond, matters. We contend that healthcare educators ought to employ caution when using AI and consider developing guidance on responsible use of AI imagery; otherwise, they risk perpetuating, rather than challenging, harmful gender stereotypes about medical career pathways.",
      "journal": "The clinical teacher",
      "year": "2026",
      "doi": "10.1111/tct.70297",
      "authors": "Hartley Alice et al.",
      "keywords": "artificial intelligence; careers; gender bias; medicine; stereotypes; visual culture",
      "mesh_terms": "Humans; Artificial Intelligence; Female; Sexism; Male; United Kingdom; Physicians",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41360694/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Obstetrics/Maternal Health",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: The finding of representational and presentational gender bias in AI-generated images of doctors is consequential because 'visual culture' within medical school, and beyond, matters. We contend that healthcare educators ought to employ caution when using AI and consider developing guidance on responsible use of AI imagery; otherwise, they risk perpetuating, rather than challenging, harmful gender stereotypes about medical career pathways.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12685619"
    },
    {
      "pmid": "41399713",
      "title": "Design and validation of a responsible artificial intelligence-based system for the referral of diabetic retinopathy patients.",
      "abstract": "Diabetic Retinopathy (DR) is a leading cause of vision loss among working-age individuals. Early detection can reduce the risk of vision loss by up to 95%, yet a shortage of retinologists and logistical challenges often delay the DR detection. Artificial Intelligence (AI) systems using Retinal Fundus Photographs (RFPs) present a promising solution. However, their clinical adoption is often hindered by issues such as low-quality data, model biases, learning of spurious features or lack of external validation. To address these challenges, we developed RAIS-DR, a Responsible AI System for DR screening that incorporates ethical principles across the AI lifecycle. RAIS-DR integrates efficient convolutional models for preprocessing, quality assessment, and three specialized DR classification models. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local dataset of 1046 patients, unseen by both systems. Results are reported for two clinically relevant referral criteria: Referable DR (RDR) and All-Cause Referable (ACR), the latter including low-quality or ungradable images. Evaluations were conducted both per patient and per image. RAIS-DR demonstrated performance improvements in patient-level referral: for RDR, F1-score, accuracy, and specificity increased by 12, 19, and 20%, respectively; for ACR, the corresponding increases were 5, 6, and 10%. RAIS-DR demonstrated equitable performance across demographic subgroups, with Disparate Impact (DI) values between 0.984 and 1.031 and Equal Opportunity Difference (EOD) values near zero. Model explainability helped identify a clinical limitation: false positives were linked to patients with a history of LASER treatment. These findings position RAIS-DR as a robust, reproducible, responsible, and clinically viable solution for DR screening.",
      "journal": "Health information science and systems",
      "year": "2026",
      "doi": "10.1007/s13755-025-00405-y",
      "authors": "Moya-S\u00e1nchez E Ulises et al.",
      "keywords": "Diabetic retinopathy; Fairness; Responsible AI system",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41399713/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Ophthalmology; Endocrinology/Diabetes",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Model explainability helped identify a clinical limitation: false positives were linked to patients with a history of LASER treatment. These findings position RAIS-DR as a robust, reproducible, responsible, and clinically viable solution for DR screening.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 2 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12701887"
    },
    {
      "pmid": "41460164",
      "title": "Addressing algorithmic bias in lung cancer screening eligibility.",
      "abstract": "BACKGROUND: The US Preventive Services Task Force (USPSTF) lung cancer screening eligibility guidelines and proposed risk models have been developed using data predominantly from White populations. Studies show that these eligibility strategies perform inconsistently across racially diverse populations, suggesting evidence of algorithmic bias. We assessed several lung cancer screening eligibility strategies and explored how algorithmic bias can be resolved to improve equity in eligibility. METHODS: Using the Southern Community Cohort Study, a large US study of predominantly Black/African American individuals, we evaluated the performance of 8 existing lung cancer screening eligibility strategies (USPSTF 2021; American Cancer Society 2023 recommendations; USPSTFSmokeDuration; Prostate, Lung, Colorectal and Ovarian 2012 risk prediction model [PLCOm2012]; PLCOm2012NoRace; PLCOm2012Update; Lung Cancer Risk Assessment Tool; and Lung Cancer Death Risk Assessment tool) and 2 new race-aware strategies proposed by our team (USPSTFRaceSpecific and PLCOm2012RaceSpecific). RESULTS: Among 52\u200a667 adults (65% Black/African American, 31% White, 4% Multiracial/Other) with a smoking history, 1689 developed lung cancer over 15\u2009years. Most screening strategies identified fewer Black/African American participants who developed lung cancer as eligible for screening vs their White counterparts (sensitivity for Black/African American individuals\u2009=\u20090.46-0.73 vs 0.72-0.80 for their White counterparts). Racial eligibility disparities were not resolved by removing race, removing the \"years since quit\" criterion, or using uniform risk thresholds. Replacing pack-years with smoking duration improved equity but overinflated the false-positive rate (0.71 for Black/African American persons vs 0.61 for White persons). Instead, race-aware approaches that tailored eligibility thresholds by race yielded the best sensitivity-specificity trade-off and minimized inequities (sensitivity\u2009=\u20090.71-0.73 for Black/African American persons vs 0.72-0.74 for White persons; false-positive rate\u2009=\u20090.49-0.50 for Black/African American persons vs 0.50-0.53 for White persons). CONCLUSION: Our findings suggest that race-aware approaches are necessary to address algorithmic bias and ensure equitable opportunities for lung cancer screening.",
      "journal": "Journal of the National Cancer Institute",
      "year": "2026",
      "doi": "10.1093/jnci/djaf298",
      "authors": "Manful Adoma et al.",
      "keywords": "",
      "mesh_terms": "Humans; Lung Neoplasms; Female; Early Detection of Cancer; Male; Middle Aged; Algorithms; Aged; Black or African American; United States; Eligibility Determination; Risk Assessment; White People; Bias; Cohort Studies; Adult; Mass Screening; White",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41460164/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Oncology; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: Our findings suggest that race-aware approaches are necessary to address algorithmic bias and ensure equitable opportunities for lung cancer screening.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41475525",
      "title": "Digital twin paradigm in diabetes prediction and management.",
      "abstract": "Traditional diabetes management employs reactive strategies with therapeutic adjustments after adverse glycaemic events rather than proactive prevention, resulting in suboptimal control and increased complications. Digital twin (DT) technology creates virtual replicas through computational modelling and real time data integration as a transformative approach. However, questions remain regarding clinical validation, implementation feasibility, and generalisability. This review examines current applications, challenges, and future potential of digital twin technology in diabetes prediction and management. PubMed, Scopus, Web of Science, and IEEE Xplore databases were searched for peer reviewed articles (2015-2024) on DT applications in diabetes care, predictive modelling, and therapeutic optimisation. Critical synthesis compared methodological approaches, performance metrics, and implementation challenges. DT demonstrate variable but promising potential through glucose prediction, personalised insulin dosing, dietary optimisation, and complication risk assessment, integrating continuous glucose monitoring, wearable sensors, and machine learning algorithms. Evidence quality varies substantially, with most studies representing proof-of-concept or pilot implementations. Implementation faces data privacy concerns, validation requirements, and integration complexities. Critical gaps exist in long-term effectiveness, algorithmic bias mitigation, and generalisability to underserved populations. DT technology represents an evolving paradigm towards precision diabetes care. However, rigorous clinical validation, addressing equity concerns, and establishing sustainable implementation frameworks remain essential for widespread adoption.",
      "journal": "Diabetes research and clinical practice",
      "year": "2026",
      "doi": "10.1016/j.diabres.2025.113075",
      "authors": "Olawade David B et al.",
      "keywords": "Computational modelling; Diabetes mellitus; Digital twin technology; Glucose prediction; Personalised medicine",
      "mesh_terms": "Humans; Diabetes Mellitus; Blood Glucose Self-Monitoring; Blood Glucose; Machine Learning",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41475525/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Endocrinology/Diabetes; Wearables/Remote Monitoring",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Telehealth/Remote; Safety-Net/Underserved",
      "key_findings": "DT technology represents an evolving paradigm towards precision diabetes care. However, rigorous clinical validation, addressing equity concerns, and establishing sustainable implementation frameworks remain essential for widespread adoption.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (1 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41493860",
      "title": "Conceptual Model for the Integration of Marketing Strategies and Biomedical Innovation in Patient-Centered Care: Mixed Methods Study.",
      "abstract": "BACKGROUND: The increasing integration of biomedical technology and digital marketing is quickly transforming how patients engage with health care. The patient as an organization (PAO) model is explored in this study. The PAO model encourages patients to be active participants in health care decisions by leveraging wearables, mobile health (mHealth) apps, artificial intelligence (AI) platforms, and health care marketing strategies. OBJECTIVE: This study aims to examine how the PAO model is evolving in practice and gain insight into both the opportunities and challenges created by the intersection of biomedical innovation and marketing practices in patient care. METHODS: The scoping review was conducted across Scopus, Web of Science, PubMed, and Google Scholar. Selection criteria included articles published from 2014 to 2024. Studies were included if they examined connections among biomedical technologies, marketing strategies, and models of behavior and organizations. Studies lacking interdisciplinary focus or methodological rigor were excluded. Since this work was exploratory, it did not require a strict bias assessment. Additionally, findings derived from qualitative analysis of 18 semistructured interviews with patients, health care professionals, and technologists regarding their experiences with digital technologies and perceptions of trust, autonomy, and engagement were analyzed. Thematic analysis was applied to these interviews using open, axial, and selective coding. RESULTS: From an initial pool of 22,740 records, 45 studies met the inclusion criteria and were analyzed. The review revealed that the integration of AI-based personalization, biosensors, and remote monitoring with marketing strategies, such as segmentation, customer relationship management systems, and behavioral nudging, offers potential to enhance patient autonomy and engagement. However, most studies were descriptive or exploratory, with limited empirical evaluation, particularly regarding ethical risks and digital inequality. Qualitative findings further illustrated how patients are adopting organizational behaviors, such as self-monitoring, real-time decision-making, and strategic management of health data. The following 5 key themes emerged: (1) patients as autonomous digital actors, (2) digital health as a behavioral ecosystem, (3) inequities in digital empowerment, (4) negotiating trust and ethical transparency, and (5) blended care as the preferred future. Although many participants embraced digital tools, concerns about data transparency, algorithmic bias, and loss of human connection highlighted important barriers to equitable adoption. CONCLUSIONS: The PAO model shows strong potential for personalizing care and engaging patients in health care. However, it is important to note that, so far, conceptual models have dominated the PAO literature, with little empirical evidence to support them. Therefore, as health care practices increasingly integrate digital technologies, it is crucial to develop appropriate safeguards for PAO models.",
      "journal": "JMIR biomedical engineering",
      "year": "2026",
      "doi": "10.2196/77115",
      "authors": "Das Gupta Atantra et al.",
      "keywords": "AI in health care; CRM; biomedical technology; customer relationship management; digital health; health marketing; patient engagement; wearables",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41493860/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Generative AI",
      "health_domain": "ICU/Critical Care; Wearables/Remote Monitoring",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Telehealth/Remote",
      "key_findings": "CONCLUSIONS: The PAO model shows strong potential for personalizing care and engaging patients in health care. However, it is important to note that, so far, conceptual models have dominated the PAO literature, with little empirical evidence to support them. Therefore, as health care practices increasingly integrate digital technologies, it is crucial to develop appropriate safeguards for PAO models.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12772582"
    },
    {
      "pmid": "41511794",
      "title": "Ten Core Concepts for Ensuring Data Equity in Public Health.",
      "abstract": "IMPORTANCE: Public health decisions increasingly rely on large-scale data and emerging technologies such as artificial intelligence and mobile health. However, many populations-including those in rural areas, with disabilities, experiencing homelessness, or living in low- and middle-income regions of the world-remain underrepresented in health datasets, leading to biased findings and suboptimal health outcomes for certain subgroups. Addressing data inequities is critical to ensuring that technological and digital advances improve health outcomes for all. OBSERVATIONS: This article proposes 10 core concepts to improve data equity throughout the operational arc of data science research and practice in public health. The framework integrates computer science principles such as fairness, transparency, and privacy protection, with best practices in public health data science that focus on mitigating information and selection biases, learning causality, and ensuring generalizability. These concepts are applied together throughout the data life cycle, from study design to data collection, analysis, and interpretation to policy translation, offering a structured approach for evaluating whether data practices adequately represent and serve all populations. CONCLUSIONS AND RELEVANCE: Data equity is a foundational requirement for producing trustworthy inference and actionable evidence. When data equity is built into public health research from the start, technological and digital advances are more likely to improve health outcomes for everyone rather than widening existing health gaps. These 10 core concepts can be used to operationalize data equity in public health. Although data equity is an essential first step, it does not automatically guarantee information, learning, or decision equity. Advancing data equity must be accompanied by parallel efforts in information theory and structural changes that promote informed decision-making.",
      "journal": "JAMA health forum",
      "year": "2026",
      "doi": "10.1001/jamahealthforum.2025.6031",
      "authors": "Wang Yiran et al.",
      "keywords": "",
      "mesh_terms": "Humans; Public Health; Data Science",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41511794/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health; Wearables/Remote Monitoring",
      "bias_axes": "Gender/Sex; Socioeconomic Status; Geographic",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Telehealth/Remote",
      "key_findings": "Although data equity is an essential first step, it does not automatically guarantee information, learning, or decision equity. Advancing data equity must be accompanied by parallel efforts in information theory and structural changes that promote informed decision-making.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41513661",
      "title": "Shipped and shifted: modeling collection-induced bias in microbiome multi-omics using a tractable fermentation system.",
      "abstract": "Large-scale, decentralized microbiome sampling surveys and citizen science initiatives often require periods of storage at ambient temperature, potentially altering sample composition during collection and transport. We developed a generalizable framework to quantify and model these biases using sourdough as a tractable fermentation system, with samples subjected to controlled storage conditions (4\u2009\u00b0C, 17\u2009\u00b0C, 30\u2009\u00b0C, regularly sampled up to 28 days). Machine-learning models paired with multi-omics profiling-including microbiome, targeted and untargeted metabolome profiling, and cultivation-revealed temperature-dependent shifts in bacterial community structure and metabolic profiles, while fungal communities remained stable. Storage induced ecological restructuring, marked by reduced network modularity and increased centrality of dominant taxa at higher temperatures. Notably, storage duration and temperature were strongly encoded in the multi-omics data, with temperature exerting a more pronounced influence than time. 24 of the top 25 predictors of storage condition were metabolites, underscoring functional layers as both sensitive to and informative of environmental exposure. These findings demonstrate that even short-term ambient storage (<2 days) can substantially reshape microbiome, metabolome, and biochemical profiles, posing risks to data comparability in decentralized studies and emphasizing the need to recognize and address such biases. Critically, the high predictability of storage history offers a path toward bias detection and correction- particularly when standardized collection protocols are infeasible, as is common in decentralized sampling contexts. Our approach enables robust quantification and modeling of such storage effects across multi-omics datasets, unlocking more accurate interpretation of large-scale microbiome surveys.",
      "journal": "NPJ biofilms and microbiomes",
      "year": "2026",
      "doi": "10.1038/s41522-025-00909-1",
      "authors": "Meyer Annina R et al.",
      "keywords": "",
      "mesh_terms": "Fermentation; Microbiota; Bacteria; Temperature; Fungi; Metabolome; Metabolomics; Machine Learning; Specimen Handling; Bread; Multiomics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41513661/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "Critically, the high predictability of storage history offers a path toward bias detection and correction- particularly when standardized collection protocols are infeasible, as is common in decentralized sampling contexts. Our approach enables robust quantification and modeling of such storage effects across multi-omics datasets, unlocking more accurate interpretation of large-scale microbiome surveys.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12901149"
    },
    {
      "pmid": "41519721",
      "title": "Assessing nonresponse bias in a 30-year study of gulf war and gulf era veterans.",
      "abstract": "BACKGROUND: Cohort studies of veterans are critical for understanding the long-term health effects of deployment and toxic exposures. However, longitudinal research is susceptible to attrition and potential nonresponse bias. The Gulf War Era Cohort Study (GWECS) is the largest and longest-running longitudinal cohort study of 1990\u20131991 Gulf War veterans. In this paper, we identify demographic and military service characteristics associated with patterns of response over time and examine the extent to which accounting for nonresponse bias in Wave 4, conducted more than 30 years after the Gulf War, might impact the estimates of health conditions. METHOD: Multivariate multinomial logistic regression analysis was used to identify demographic and military service characteristics associated with response patterns over time (always responder, current responder, past responder, never responder). To adjust for nonresponse at Wave 4, a search algorithm was used to identify predictors of response and form weighting class cells. The effectiveness of nonresponse adjustments was evaluated by (1) comparing estimates of demographic and military characteristics before and after weighting, (2) examining the correlation between the weighting classes and health outcomes, and (3) comparing early and late responders on health outcomes. RESULTS: Wave 4 obtained a response rate of 47%, close to the 50% response rate obtained in Wave 3 over a decade earlier. Veterans most likely to respond to the survey over time and in Wave 4 were older, White, deployed, officers, and married in 1991. Weighting adjustments accounted for these differences and reduced bias in demographic and military characteristics, as well as in survey variables related to those characteristics. However, differences observed between early and late responders on alcohol and drug dependence suggest that these conditions may be underestimated. CONCLUSION: GWECS achieved a relatively high response rate in the most recent follow-up. Although differential response occurred, nonresponse adjustments effectively reduced bias in the key variables examined. This cohort continues to provide insight into the long-term health effects of Gulf War deployment, including cancer and chronic conditions, as the cohort ages. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1186/s12874-025-02761-5.",
      "journal": "BMC medical research methodology",
      "year": "2026",
      "doi": "10.1186/s12874-025-02761-5",
      "authors": "Gasper Joseph et al.",
      "keywords": "Cohort study; Longitudinal study; Nonresponse bias; Veterans; Weighting",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41519721/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Logistic Regression",
      "health_domain": "Oncology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: GWECS achieved a relatively high response rate in the most recent follow-up. Although differential response occurred, nonresponse adjustments effectively reduced bias in the key variables examined. This cohort continues to provide insight into the long-term health effects of Gulf War deployment, including cancer and chronic conditions, as the cohort ages.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12882365"
    },
    {
      "pmid": "41528321",
      "title": "Auditor models to suppress poor artificial intelligence predictions can improve human-artificial intelligence collaborative performance.",
      "abstract": "OBJECTIVE: Healthcare decisions are increasingly made with the assistance of machine learning (ML). ML has been known to have unfairness-inconsistent outcomes across subpopulations. Clinicians interacting with these systems can perpetuate such unfairness by overreliance. Recent work exploring ML suppression-silencing predictions based on auditing the ML-shows promise in mitigating performance issues originating from overreliance. This study aims to evaluate the impact of suppression on collaboration fairness and evaluate ML uncertainty as desiderata to audit the ML. MATERIALS AND METHODS: We used data from the Vanderbilt University Medical Center electronic health record (n\u2009=\u200958\u2009817) and the MIMIC-IV-ED dataset (n\u2009=\u2009363\u2009145) to predict likelihood of death or intensive care unit transfer and likelihood of 30-day readmission using gradient-boosted trees and an artificially high-performing oracle model. We derived clinician decisions directly from the dataset and simulated clinician acceptance of ML predictions based on previous empirical work on acceptance of clinical decision support alerts. We measured performance as area under the receiver operating characteristic curve and algorithmic fairness using absolute averaged odds difference. RESULTS: When the ML outperforms humans, suppression outperforms the human alone (P\u2009<\u20098.2\u2009\u00d7\u200910-6) and at least does not degrade fairness. When the human outperforms the ML, the human is either fairer than suppression (P\u2009<\u20098.2\u2009\u00d7\u200910-4) or there is no statistically significant difference in fairness. Incorporating uncertainty quantification into suppression approaches can improve performance. CONCLUSION: Suppression of poor-quality ML predictions through an auditor model shows promise in improving collaborative human-AI performance and fairness.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2026",
      "doi": "10.1093/jamia/ocaf235",
      "authors": "Brown Katherine E et al.",
      "keywords": "artificial intelligence; human-AI collaboration; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41528321/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Decision Support",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics",
      "bias_axes": "Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Subgroup Analysis; Bias Auditing Framework",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: Suppression of poor-quality ML predictions through an auditor model shows promise in improving collaborative human-AI performance and fairness.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41528598",
      "title": "From glycolytic signatures to patients: A translational roadmap for reproducible, equitable deployment of multi-omics and AI in colorectal cancer.",
      "abstract": "Recent advances in Medical Oncology highlight the integration of bulk and single-cell transcriptomics to reveal glycolytic heterogeneity in colorectal cancer. Translating these discoveries into reliable clinical tools requires rigorous methods, transparent validation, and equity-minded implementation. This communication proposes a standards-first roadmap for reproducible and globally relevant biomarker development. It identifies major technical pitfalls such as batch-effect over-correction and normalization bias, and recommends the application of internationally recognized frameworks-TRIPOD\u2009+\u2009AI, PROBAST\u2009+\u2009AI, and DECIDE-AI-to ensure transparency, calibration, and staged clinical evaluation. Orthogonal validation using metabolic imaging and spectroscopy is emphasized to confirm biological realism beyond transcriptomic data. The roadmap concludes with strategies for global equity, including LMIC-inclusive trial design, FAIR data standards, and cost-aware clinical surrogates. This structured approach bridges discovery science with practical implementation, aligning precision oncology with reproducibility, accountability, and global accessibility.",
      "journal": "Medical oncology (Northwood, London, England)",
      "year": "2026",
      "doi": "10.1007/s12032-026-03236-3",
      "authors": "Vijayasimha M",
      "keywords": "Artificial intelligence; Colorectal cancer; DECIDE-AI; Equity; Glycolysis; Multi-omics; PROBAST\u2009+\u2009AI; Reproducibility; TRIPOD\u2009+\u2009AI; Translational oncology",
      "mesh_terms": "Humans; Artificial Intelligence; Biomarkers, Tumor; Colorectal Neoplasms; Glycolysis; Multiomics; Precision Medicine; Reproducibility of Results; Translational Research, Biomedical",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41528598/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration",
      "clinical_setting": "Not specified",
      "key_findings": "The roadmap concludes with strategies for global equity, including LMIC-inclusive trial design, FAIR data standards, and cost-aware clinical surrogates. This structured approach bridges discovery science with practical implementation, aligning precision oncology with reproducibility, accountability, and global accessibility.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41533193",
      "title": "Addressing Health Disparities through Community Engagement in Artificial Intelligence-Driven Prevention Science.",
      "abstract": "Artificial intelligence and machine learning (AI/ML) in prevention science may improve or perpetuate health inequities. Community engagement is one proposed strategy thought to empirically mitigate bias in AI/ML tools. We outline how to incorporate community engagement at every stage of the model development and implementation. Borrowing from a framework for phases of prevention research, we describe the value and application of engaging communities to help shape more rigorous and relevant applications of AI/ML for prevention science. We provide concrete examples from real-world applications, including efforts in suicide prevention with Indigenous communities, on chronic disease prevention for Hispanic and Latino populations, and a community-driven effort to leverage AI/ML to improve allocation of resources focused on social determinants of health for Native Hawaiians. This work aims to provide applied examples of how community-engagement has been incorporated into AI/ML development and implementation, with the goal of encouraging those in the prevention science field to consider the voices of the community as the use of such tools grows. Engaging with the community around AI/ML is critical to ensure these tools reach populations in need and advance health equity for all.",
      "journal": "Prevention science : the official journal of the Society for Prevention Research",
      "year": "2026",
      "doi": "10.1007/s11121-025-01863-2",
      "authors": "Haroz Emily E et al.",
      "keywords": "Artificial intelligence; Community engagement; Machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41533193/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "This work aims to provide applied examples of how community-engagement has been incorporated into AI/ML development and implementation, with the goal of encouraging those in the prevention science field to consider the voices of the community as the use of such tools grows. Engaging with the community around AI/ML is critical to ensure these tools reach populations in need and advance health equity for all.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12865688"
    },
    {
      "pmid": "41552743",
      "title": "Overcoming Normalcy Bias in Acute Myocardial Infarction: A Case Report of Generative AI as a Behavioral Catalyst for Emergency Care Seeking.",
      "abstract": "Pre-hospital delay remains a major determinant of outcomes in acute myocardial infarction (AMI), with normalcy bias playing a central role in patients' failure to interpret symptoms as signals of serious illness. This case report examines the role of generative artificial intelligence (AI) not as a diagnostic instrument\u00a0but as a behavioral catalyst that prompted timely emergency care seeking. A man in his early sixties presented with chest discomfort, neck radiation, bilateral lower molar pain, diaphoresis, and cold extremities. Although these symptoms are medically typical of AMI, myocardial infarction was not part of the patient's immediate cognitive framework, and they were initially interpreted as dental discomfort or nonspecific physical fatigue. After consulting a publicly available generative AI system that issued a clear imperative to contact emergency medical services, the patient activated emergency care. He was subsequently diagnosed with inferior ST-elevation myocardial infarction due to right coronary artery occlusion and underwent successful emergency percutaneous coronary intervention. This case suggests that AI-generated language can mitigate normalcy bias and accelerate patient decision-making in acute medical settings without functioning as a diagnostic tool.",
      "journal": "Cureus",
      "year": "2026",
      "doi": "10.7759/cureus.101199",
      "authors": "Ikeda Osamu et al.",
      "keywords": "acute myocardial infarction; artificial intelligence; behavioral catalyst; generative ai; large language models; normalcy bias; patient decision making; percutaneous coronary intervention; pre-hospital delay; stemi",
      "mesh_terms": "",
      "pub_types": "Case Reports; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41552743/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Generative AI",
      "health_domain": "Cardiology; Pain Management",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; Clinical Trial",
      "key_findings": "He was subsequently diagnosed with inferior ST-elevation myocardial infarction due to right coronary artery occlusion and underwent successful emergency percutaneous coronary intervention. This case suggests that AI-generated language can mitigate normalcy bias and accelerate patient decision-making in acute medical settings without functioning as a diagnostic tool.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12811831"
    },
    {
      "pmid": "41635417",
      "title": "Airway quantifications of bronchitis patients with photon-counting and energy-integrating computed tomography.",
      "abstract": "PURPOSE: Accurate airway measurement is critical for bronchitis quantification with computed tomography (CT), yet optimal protocols and the added value of photon-counting CT (PCCT) over energy-integrating CT (EICT) for reducing bias remain unclear. We quantified biomarker accuracy across modalities and protocols and assessed strategies to reduce bias. APPROACH: A virtual imaging trial with 20 bronchitis anthropomorphic models was scanned using a validated simulator for two systems (EICT: SOMATOM Flash; PCCT: NAEOTOM Alpha) at 6.3 and 12.6\u00a0mGy. Reconstructions varied algorithm, kernel sharpness, slice thickness, and pixel size. Pi10 (square-root wall thickness at 10-mm perimeter) and WA% (wall-area percentage) were compared against ground-truth airway dimensions obtained from the 0.1-mm-precision anatomical models prior to CT simulation. External validation used clinical PCCT ( n = 22  ) and EICT ( n = 80  ). RESULTS: Simulated airway dimensions agreed with pathological references ( R = 0.89 - 0.93  ). PCCT had lower errors than EICT across segmented generations ( p < 0.05  ). Under optimal parameters, PCCT improved Pi10 and WA% accuracy by 26.3% and 64.9%. Across the tested PCCT and EICT imaging protocols, improvements were associated with sharper kernels (25.8% Pi10, 33.0% WA%), thinner slices (23.9% Pi10, 49.8% WA%), smaller pixels (17.0% Pi10, 23.1% WA%), and higher dose ( \u2264 3.9 %  ). Clinically, PCCT achieved higher maximum airway generation ( 8.8 \u00b1 0.5  versus 6.0 \u00b1 1.1  ) and lower variability, mirroring trends in virtual results. CONCLUSIONS: PCCT improves the accuracy and consistency of airway biomarker quantification relative to EICT, particularly with optimized protocols. The validated virtual platform enables modality-bias assessment and protocol optimization for accurate, reproducible bronchitis measurements.",
      "journal": "Journal of medical imaging (Bellingham, Wash.)",
      "year": "2026",
      "doi": "10.1117/1.JMI.13.1.013501",
      "authors": "Ho Fong Chi et al.",
      "keywords": "DukeSim; chronic obstructive pulmonary disease; computational phantoms; extended cardiac-torso; photon-counting computed tomography; virtual clinical trial",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41635417/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSIONS: PCCT improves the accuracy and consistency of airway biomarker quantification relative to EICT, particularly with optimized protocols. The validated virtual platform enables modality-bias assessment and protocol optimization for accurate, reproducible bronchitis measurements.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12863983"
    },
    {
      "pmid": "41646534",
      "title": "Patient perceptions of artificial intelligence integration in dermatology: a cross-sectional study of trust, comfort and equity across multiple care modalities.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) and telemedicine are rapidly changing the way dermatological care is delivered. As these tools are increasingly used in tandem, understanding how patients perceive the integration of AI across different care settings is important for responsible implementation. OBJECTIVES: To assess patient perceptions of AI in dermatology across five care modalities and examine how demographic factors influence acceptance. METHODS: A cross-sectional survey was conducted among 130 adults at a US academic dermatology clinic between December 2024 and April 2025. Participants rated trust, comfort, perceived quality, privacy and confidence in equitable performance across three AI-involved modalities: standalone AI apps, AI-assisted in-person visits and AI-assisted telemedicine visits. Differences in perception outcomes across the three care modalities were analysed using repeated measures Anova. Logistic and linear regressions analysed predictors of acceptance, including age, race, skin tone, socioeconomic status, rurality and technology experience. RESULTS: Patients strongly preferred dermatologist-involved care over standalone AI, with 73.8% trusting dermatologist-guided AI and only 1.5% trusting AI apps alone. Comfort and perceptions of equal performance across skin tones were significantly higher for telemedicine and AI-assisted visits compared with AI apps (P < 0.001). Darker skin tone and Black race predicted lower acceptance of AI-assisted care (P = 0.01 and P = 0.003, respectively), while greater technology familiarity predicted higher acceptance (P = 0.05). Comfort varied by clinical scenario, with in-person visits showing dramatically higher odds of patient comfort compared with AI apps alone [odds ratio (OR) 232.8 for new concerns, OR 137.3 for serious concerns, OR 18.4 for sensitive concerns]. AI-assisted in-person visits also showed significantly higher odds of comfort over AI apps (OR 18.4 for serious concerns, OR 3.6 for ongoing concerns). CONCLUSIONS: Patients strongly prefer AI as clinical support systems rather than autonomous decision-makers, especially for high-stakes and sensitive concerns. Differences in acceptance by race and skin tone point to the need for better representation in datasets and clearer communication about how these tools perform. Moving forward, development and implementation should emphasize clinician and patient involvement, fairness and patient choice to ensure AI is integrated into dermatology in a way that earns patient trust.",
      "journal": "Skin health and disease",
      "year": "2026",
      "doi": "10.1093/skinhd/vzaf086",
      "authors": "McRae Charlotte et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41646534/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Regression",
      "health_domain": "Dermatology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Emergency Department; Telehealth/Remote",
      "key_findings": "CONCLUSIONS: Patients strongly prefer AI as clinical support systems rather than autonomous decision-makers, especially for high-stakes and sensitive concerns. Differences in acceptance by race and skin tone point to the need for better representation in datasets and clearer communication about how these tools perform. Moving forward, development and implementation should emphasize clinician and patient involvement, fairness and patient choice to ensure AI is integrated into dermatology in a way...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12867938"
    },
    {
      "pmid": "41652293",
      "title": "Artificial intelligence for fracture detection on computed tomography: a comprehensive systematic review and meta-analysis of diagnostic test accuracy in non-commercial and commercial solutions.",
      "abstract": "Rising patient volumes, the increasing use of computed tomography (CT) imaging in emergency departments and the resulting prolonged waiting times highlight the urgent need for efficient and accurate diagnostic tools, especially given that the number of experienced healthcare professionals is not increasing at the same pace. Artificial intelligence (AI) has emerged as a promising tool to support fracture detection on CT scans, with the potential to streamline diagnostic workflows in emergency care. However, concerns exist regarding dataset bias, limited external testing, and methodological variability. This systematic review and diagnostic test accuracy (DTA) meta-analysis aimed to comprehensively assess the diagnostic accuracy of AI-driven fracture detection solutions, with a particular focus on the effect of the testing strategy, cohort composition and commercial availability on diagnostic accuracy. The Cochrane Handbook for Systematic Reviews of DTA and reported according to PRISMA-DTA guidelines were followed. We systematically searched Embase, MEDLINE, Cochrane Library, Web of Science, and Google Scholar for studies published from January 2010 onward, complemented by citation chasing and manual searches for commercial AI fracture detection solutions (CAAI-FDS). Two reviewers independently conducted study selection, data extraction, and risk of bias assessment using a modified QUADAS-2 tool. Statistical analysis was conducted using STATA 18.1 and the -metadta- command. Primary analyses evaluated diagnostic accuracy (sensitivity and specificity) of stand-alone AI based on (1) cohort type (selected vs. unselected), (2) test dataset origin (internal vs. external), and (3) level of analysis (patient-wise, vertebra-wise, rib-wise). Secondary analyses explored accuracy differences according to (1) CAAI-FDS, (2) anatomical region and (3) reader type (stand-alone AI, human unaided, human aided by AI). Forest plots visualized results, and heterogeneity was measured using generalized I2 statistics. Out of 7683 identified articles, 44 studies were included for meta-analysis. 14 CAAI-FDS were identified. Primary analyses of stand-alone AI showed moderate sensitivity (0.85, 95% CI: 0.77, 0.90) and good specificity (0.92, 95% CI: 0.87, 0.95) in unselected patient cohorts, whereas selected cohorts achieved slightly higher sensitivity (0.89, 95% CI: 0.80, 0.94). Diagnostic accuracy was higher when studies used internal test datasets (sensitivity 0.94, 95% CI: 0.88, 0.97; specificity 0.91, 95% CI: 0.86, 0.94) compared to external test datasets (sensitivity 0.85, 95% CI: 0,77, 0.91; specificity 0.92, 95% CI: 0.89, 0.95). Vertebra- and rib-wise analyses achieved higher specificity (0.98) compared to patient-wise analysis (0.92, 95% CI: 0.89, 0.95), although sensitivity remained moderate across all levels (0.85-0.89). Secondary analyses showed variability among CAAI-FDS (sensitivities 0.68-0.80; specificities 0.87-0.97) and by anatomical region, with the highest sensitivity for skull (0.90, 95% CI: 0.85, 0.93), rib (0.92, 95% CI: 0.83, 0.96) and pelvis fractures (1.00), and lowest for spine fractures (0.82, 95% CI: 0.73, 0.88). Stand-alone AI showed moderate to good diagnostic accuracy, slightly outperforming unaided human readers, with minimal further improvement when humans were aided by AI. While AI demonstrates promising diagnostic accuracy in fracture detection, study biases, stringent patient selection, and lack of external testing raise concerns about real-world applicability. Commercially available solutions tend to underperform compared to pooled study results, highlighting the gap between research settings and clinical practice. Future efforts should focus on reducing bias, improving generalizability and robustness, as well as conducting prospective trials to assess AI's true impact on clinical outcomes.",
      "journal": "Emergency radiology",
      "year": "2026",
      "doi": "10.1007/s10140-026-02437-7",
      "authors": "Husarek Julius et al.",
      "keywords": "Artificial intelligence; Computed tomography; Computer-aided diagnosis; Diagnostic test accuracy; Systematic review and meta-analysis",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41652293/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Emergency Medicine; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Emergency Department",
      "key_findings": "Commercially available solutions tend to underperform compared to pooled study results, highlighting the gap between research settings and clinical practice. Future efforts should focus on reducing bias, improving generalizability and robustness, as well as conducting prospective trials to assess AI's true impact on clinical outcomes.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (1 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41659839",
      "title": "Big data in healthcare and medicine revisited design and managerial challenges in the age of artificial intelligence.",
      "abstract": "A decade ago, we characterized big data in healthcare as a nascent field anchored in distributed computing paradigms. The intervening years have witnessed a transformation so profound that revisiting our original framework is essential. This paper critically examines the evolution of big data in healthcare and medicine, assessing the shift from Hadoop-centric architectures to cloud computing platforms and GPU-accelerated artificial intelligence, including large language models and the emerging paradigm of agentic AI. The landscape has been reshaped by landmark biobank initiatives, breakthrough applications such as AlphaFold's Nobel Prize-winning solution to protein structure prediction, and the rapid growth of FDA-cleared AI medical devices from fewer than ten in 2015 to over 1200 by mid-2025. AI has enabled advances across precision oncology, drug discovery, and public health surveillance. Yet new challenges have emerged: algorithmic bias perpetuating health disparities, opacity undermining clinical trust, environmental sustainability concerns, and unresolved questions of privacy, security, data ownership, and interoperability. We propose extending the original \"4Vs\" framework to accommodate veracity through explainability, validity through fairness, and viability through sustainability. The paper concludes with prescriptive implications for healthcare organizations, technology developers, policymakers, and researchers.",
      "journal": "Health information science and systems",
      "year": "2026",
      "doi": "10.1007/s13755-026-00433-2",
      "authors": "Raghupathi Wullianallur et al.",
      "keywords": "Agentic AI; Algorithmic bias; Artificial intelligence; Big data; Clinical trials; Cloud computing; Drug discovery; Electronic health records; Explainable AI,; Governance; Healthcare; Interoperability; Large language models; Medicine; Precision medicine; Privacy; Public health surveillance; Security",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41659839/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM; Generative AI",
      "health_domain": "Oncology; Public Health; Drug Discovery/Pharmacology",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We propose extending the original \"4Vs\" framework to accommodate veracity through explainability, validity through fairness, and viability through sustainability. The paper concludes with prescriptive implications for healthcare organizations, technology developers, policymakers, and researchers.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12881233"
    },
    {
      "pmid": "41663205",
      "title": "Evaluating Sociodemographic Biases in Artificial Intelligence-Based Glioblastoma Response Assessment Algorithms.",
      "abstract": "BACKGROUND AND PURPOSE: Recent studies have demonstrated bias in various medical imaging artificial intelligence (AI) models, yet the factors underpinning these biases remain relatively unclear. This study evaluated potential sociodemographic biases in AI-based glioblastoma MRI segmentation models trained on datasets varying in size and demographic composition. We evaluated four nnUNet models with different training datasets: (1) the Federated Tumor Segmentation postoperative (FeTS2) model trained on a large (>10k exams) multi-national, multi-institution dataset, (2) the Brain Tumor Segmentation (BraTS) 2024 postoperative glioma model trained on a moderate size (>2k exams) multi-institution, North American dataset, (3) a model trained on a small (>200 exams), private, demographically homogenous, single-institution dataset, and (4) a model trained on an equally small (>200 exams), but demographically heterogenous dataset. MATERIALS AND METHODS: Models were evaluated for bias using an independent, manually corrected dataset of 480 patients (mean age 52 \u00b1 14) that was prospectively collected from a single high-volume academic brain tumor center. Automated FLAIR and enhancing tumor segmentations from the AI models were evaluated using Dice scores. Sociodemographic factors were collected and analyzed using beta regression to assess their influence on model performance. RESULTS: The model trained exclusively on White, non-Hispanic males had the lowest overall Dice scores (0.943 for FLAIR, 0.909 for Enhancement) and exhibited biases in age and smoking status. The BraTS model demonstrated the highest Dice scores (0.996 for FLAIR, 0.999 for Enhancement) and had the least bias overall. CONCLUSIONS: Demographic bias was relatively low in glioblastoma MRI segmentation models. The model trained on the smallest and most homogenous dataset exhibited the most bias. Greater demographic heterogeneity even without increasing training dataset size was associated with reduced bias. The BraTS model, trained on a moderate-sized cohort that included more diverse tumor types, performed better and demonstrated less bias than the FeTS2 model, despite the FeTS2 being trained on the largest dataset.",
      "journal": "AJNR. American journal of neuroradiology",
      "year": "2026",
      "doi": "10.3174/ajnr.A9217",
      "authors": "Lee Rachel S et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41663205/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Oncology; Surgery; Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Demographic bias was relatively low in glioblastoma MRI segmentation models. The model trained on the smallest and most homogenous dataset exhibited the most bias. Greater demographic heterogeneity even without increasing training dataset size was associated with reduced bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41666048",
      "title": "Causal modeling of chronic kidney disease in a participatory framework for informing the inclusion of social drivers in health algorithms.",
      "abstract": "OBJECTIVES: Incomplete or incorrect causal theories are a key source of bias in machine learning (ML) algorithms. Community-engaged methodologies provide an avenue for mitigating this bias through incorporating causal insights from community stakeholders into ML development. In health applications, community-engaged approaches can enable the study of social drivers of health (SDOH), which are known to shape health inequities. However, it remains challenging for SDOH to inform ML algorithms, partially because SDOH variables are known to be interrelated, yet it is difficult to elucidate the causal relationships between them. Community-based system dynamics is a community-engaged methodology that can be used to cocreate formal causal graphs, called causal loop diagrams, with patients. MATERIALS AND METHODS: We used community-based system dynamics to create a causal graph representing the impacts of SDOH on the progression of chronic kidney disease, a chronic condition with SDOH-driven health disparities. We conducted focus groups with 42 participants and a day-long model building workshop with 11 participants. RESULTS: Our model building workshop resulted in a final graph comprising 16 variables, 42 causal links, and 5 subsystems of semantically related SDOH variables. CONCLUSION: This final graph, representing the causal relationships between social variables relevant to chronic kidney disease, can inform the development of clinical ML algorithms and other technological interventions.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2026",
      "doi": "10.1093/jamia/ocag019",
      "authors": "Foryciarz Agata et al.",
      "keywords": "chronic; community-based participatory research; health inequities; renal insufficiency; social determinants of health; systems theory",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41666048/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Nephrology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: This final graph, representing the causal relationships between social variables relevant to chronic kidney disease, can inform the development of clinical ML algorithms and other technological interventions.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41673000",
      "title": "Biomedical Data Manifest: A lightweight data documentation mapping to increase transparency for AI/ML.",
      "abstract": "Biomedical machine learning (ML) models raise critical concerns about embedded assumptions influencing clinical decision-making, necessitating robust documentation frameworks for datasets that are shared via external repositories. Fairness-aware algorithm effectiveness hinges on users' prior awareness of specific issues in the data - information such as data collection methodology, provenance and quality. Current ML-focused documentation approaches impose impractical burdens on data generators and conflate data/model accountability. This is problematic for resource datasets not explicitly created for ML applications. This study addresses these gaps through a two-step process: First, we derived consensus documentation fields by mapping elements across four key templates. Second, we surveyed biomedical stakeholders across four roles (clinicians, bench scientists, data manager and computationalists) to assess field importance and relevance. This revealed important role-dependent prioritization differences, motivating the development of the Biomedical Data Manifest - a modular template employing persona-specific field presentation reducing generator burden while ensuring end-users receive role-relevant information. The Biomedical Data Manifest improves transparency for datasets deposited in public or controlled-access repositories and bias mitigation across ML applications.",
      "journal": "Scientific data",
      "year": "2026",
      "doi": "10.1038/s41597-026-06670-0",
      "authors": "Bottomly Daniel et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41673000/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "This revealed important role-dependent prioritization differences, motivating the development of the Biomedical Data Manifest - a modular template employing persona-specific field presentation reducing generator burden while ensuring end-users receive role-relevant information. The Biomedical Data Manifest improves transparency for datasets deposited in public or controlled-access repositories and bias mitigation across ML applications.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (2 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41681030",
      "title": "Demographic-aware deep learning for multi-organ segmentation: Mitigating gender and age biases in CT images.",
      "abstract": "BACKGROUND: Deep learning algorithms have shown promising results for automated organ-at-risk (OAR) segmentation in medical imaging. However, their performance is frequently compromised by demographic bias. This limitation becomes pronounced when conventional models fail to account for Complex 3D anatomical variations across diverse groups, as they often overlook critical factors such as age and gender. Consequently, this oversight can lead to inaccurate segmentations, thereby posing significant risks to clinical safety in\u00a0radiotherapy. PURPOSE: To address this challenge, in this work, we develop a demographic-aware deep learning framework for multi-organ segmentation in computed tomography (CT) images. Our approach is designed to explicitly mitigate age- and gender-specific biases by incorporating demographic prompts and adaptive attention mechanisms, enabling the capture of multi-view anatomical features across diverse\u00a0groups. METHODS: We propose the Demographic-Aware Network (DA-Net), a novel framework trained on a unified dataset of 489 adult (AMOS2022) and 370 pediatric (Pediatric CT-SEG) CT scans, covering 30 organs and including 355 female scans. To robustly learn group-specific anatomical characteristics, DA-Net integrates the Demographic-Aware Hyper-Convolution (DA-HyperConv) module that dynamically adapts convolutional kernels based on demographic prompts. Additionally, an Adaptive Triplet Attention Block (ATAB) is embedded to further leverage multi-view features and enhance segmentation accuracy. We validate the generalizability and effectiveness of our framework on an external dataset (WORD, 150 adults, 62 females). The framework is evaluated quantitatively using the Dice Similarity Coefficient (DICE) and Normalized Surface Dice (NSD). RESULTS: DA-Net surpasses state-of-the-art (SOTA) methods across both the general group and specific demographic subgroups. In the AMOS2022 dataset (mean age 52.8 \u00b1 $\\pm$  16.1 years), DA-Net achieves the highest average DICE of 88.6% and NSD of 76.3% for adults. On the Pediatric CT-SEG (mean age 6.9 \u00b1 $\\pm$  4.5 years), it achieves top performance with an average DICE of 75.3% \u00b1 $\\pm$  20.4% and NSD of 54.8% \u00b1 $\\pm$  20.9%. Notably, our proposed framework achieves substantial DICE improvements of 11% to 30% for gender-specific organs, significantly reducing performance disparities. Robustness and generalizability are further supported by consistent results on external validation using the WORD dataset. Compared with the SOTA methods, the performance improvement of our approach is of substantial importance in both the WORD dataset and the Pediatric\u00a0CT-SEG. CONCLUSIONS: In this work, we propose DA-Net, a segmentation network that explicitly incorporates age and gender attributes to mitigate performance disparities between pediatric and adult groups while combining multiple views of anatomic features to improve performance. By leveraging demographic information, DA-Net enhances segmentation accuracy, especially for gender-specific organs. The proposed framework highlights the necessity of developing fair and personalized models tailored to clinical applications, providing a foundation for building more equitable artificial intelligence systems in medical\u00a0imaging.",
      "journal": "Medical physics",
      "year": "2026",
      "doi": "10.1002/mp.70322",
      "authors": "Ma Junqiang et al.",
      "keywords": "age bias; computed tomography; deep learning; gender bias; multi\u2010organs segmentation",
      "mesh_terms": "Deep Learning; Humans; Tomography, X-Ray Computed; Female; Male; Adult; Image Processing, Computer-Assisted; Child; Adolescent; Middle Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41681030/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Computer Vision/Imaging AI; Generative AI",
      "health_domain": "Radiology/Medical Imaging; Pediatrics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: In this work, we propose DA-Net, a segmentation network that explicitly incorporates age and gender attributes to mitigate performance disparities between pediatric and adult groups while combining multiple views of anatomic features to improve performance. By leveraging demographic information, DA-Net enhances segmentation accuracy, especially for gender-specific organs. The proposed framework highlights the necessity of developing fair and personalized models tailored to clinical ...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41692962",
      "title": "Addressing Algorithmic Bias in Artificial Intelligence-Driven Medical Education Assessment.",
      "abstract": "",
      "journal": "Academic medicine : journal of the Association of American Medical Colleges",
      "year": "2026",
      "doi": "10.1093/acamed/wvag039",
      "authors": "Du Zhicheng",
      "keywords": "artificial intelligence; assessment; large language models; moral injury",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41692962/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38851934",
      "title": "Generative Artificial Intelligence Biases, Limitations and Risks in Nuclear Medicine: An Argument for Appropriate Use Framework and Recommendations.",
      "abstract": "Generative artificial intelligence (AI) algorithms for both text-to-text and text-to-image applications have seen rapid and widespread adoption in the general and medical communities. While limitations of generative AI have been widely reported, there remain valuable applications in patient and professional communities. Here, the limitations and biases of both text-to-text and text-to-image generative AI are explored using purported applications in medical imaging as case examples. A direct comparison of the capabilities of four common text-to-image generative AI algorithms is reported and recommendations for the most appropriate use, DALL-E 3, justified. The risks use and biases are outlined, and appropriate use guidelines framed for use of generative AI in nuclear medicine. Generative AI text-to-text and text-to-image generation includes inherent biases, particularly gender and ethnicity, that could misrepresent nuclear medicine. The assimilation of generative AI tools into medical education, image interpretation, patient education, health promotion and marketing in nuclear medicine risks propagating errors and amplification of biases. Mitigation strategies should reside inside appropriate use criteria and minimum standards for quality and professionalism for the application of generative AI in nuclear medicine.",
      "journal": "Seminars in nuclear medicine",
      "year": "2025",
      "doi": "10.1053/j.semnuclmed.2024.05.005",
      "authors": "Currie Geoffrey M et al.",
      "keywords": "",
      "mesh_terms": "Nuclear Medicine; Artificial Intelligence; Humans; Bias; Risk; Generative Artificial Intelligence",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38851934/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Computer Vision/Imaging AI; Generative AI",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "The assimilation of generative AI tools into medical education, image interpretation, patient education, health promotion and marketing in nuclear medicine risks propagating errors and amplification of biases. Mitigation strategies should reside inside appropriate use criteria and minimum standards for quality and professionalism for the application of generative AI in nuclear medicine.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39115640",
      "title": "Identifying sources of bias when testing three available algorithms for quantifying white matter lesions: BIANCA, LPA and LGA.",
      "abstract": "Brain magnetic resonance imaging frequently reveals white matter lesions (WMLs) in older adults. They are often associated with cognitive impairment and risk of dementia. Given the continuous search for the optimal segmentation algorithm, we broke down this question by exploring whether the output of algorithms frequently used might be biased by the presence of different influencing factors. We studied the impact of age, sex, blood glucose levels, diabetes, systolic blood pressure and hypertension on automatic WML segmentation algorithms. We evaluated three widely used algorithms (BIANCA, LPA and LGA) using the population-based 1000BRAINS cohort (N\u2009=\u20091166, aged 18-87, 523 females, 643 males). We analysed two main aspects. Firstly, we examined whether training data (TD) characteristics influenced WML estimations, assessing the impact of relevant factors in the TD. Secondly, algorithm's output and performance within selected subgroups defined by these factors were assessed. Results revealed that BIANCA's WML estimations are influenced by the characteristics present in the TD. LPA and LGA consistently provided lower WML estimations compared to BIANCA's output when tested on participants under 67 years of age without risk cardiovascular factors. Notably, LPA and LGA showed reduced accuracy for these participants. However, LPA and LGA showed better performance for older participants presenting cardiovascular risk factors. Results suggest that incorporating comprehensive cohort factors like diverse age, sex and participants with and without hypertension in the TD could enhance WML-based analyses and mitigate potential sources of bias. LPA and LGA are a fast and valid option for older participants with cardiovascular risk factors.",
      "journal": "GeroScience",
      "year": "2025",
      "doi": "10.1007/s11357-024-01306-w",
      "authors": "Miller Tatiana et al.",
      "keywords": "BIANCA; LGA; LPA; Training data characteristics; White matter lesion",
      "mesh_terms": "Humans; Female; Male; Aged; Middle Aged; Algorithms; Adult; Aged, 80 and over; Magnetic Resonance Imaging; White Matter; Adolescent; Young Adult; Bias; Risk Factors",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39115640/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; Neurology; Endocrinology/Diabetes",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Disability",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Results suggest that incorporating comprehensive cohort factors like diverse age, sex and participants with and without hypertension in the TD could enhance WML-based analyses and mitigate potential sources of bias. LPA and LGA are a fast and valid option for older participants with cardiovascular risk factors.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11872996"
    },
    {
      "pmid": "39446671",
      "title": "Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support.",
      "abstract": "The integration of large language models (LLMs) into healthcare highlights the need to ensure their efficacy while mitigating potential harms, such as the perpetuation of biases. Current evidence on the existence of bias within LLMs remains inconclusive. In this study, we present an approach to investigate the presence of bias within an LLM designed for mental health support. We simulated physician-patient conversations by using a communication loop between an LLM-based conversational agent and digital standardized patients (DSPs) that engaged the agent in dialogue while remaining agnostic to sociodemographic characteristics. In contrast, the conversational agent was made aware of each DSP's characteristics, including age, sex, race/ethnicity, and annual income. The agent's responses were analyzed to discern potential systematic biases using the Linguistic Inquiry and Word Count tool. Multivariate regression analysis, trend analysis, and group-based trajectory models were used to quantify potential biases. Among 449 conversations, there was no evidence of bias in both descriptive assessments and multivariable linear regression analyses. Moreover, when evaluating changes in mean tone scores throughout a dialogue, the conversational agent exhibited a capacity to show understanding of the DSPs' chief complaints and to elevate the tone scores of the DSPs throughout conversations. This finding did not vary by any sociodemographic characteristics of the DSP. Using an objective methodology, our study did not uncover significant evidence of bias within an LLM-enabled mental health conversational agent. These findings offer a complementary approach to examining bias in LLM-based conversational agents for mental health support.",
      "journal": "Cyberpsychology, behavior and social networking",
      "year": "2025",
      "doi": "10.1089/cyber.2024.0199",
      "authors": "Yeo Yee Hui et al.",
      "keywords": "artificial intelligence; bias; disparity; large language model; linguistic inquiry and word count; mental health",
      "mesh_terms": "Humans; Male; Female; Communication; Adult; Middle Aged; Mental Health; Physician-Patient Relations; Language; Mental Health Services; Sociodemographic Factors",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39446671/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM; Regression",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Using an objective methodology, our study did not uncover significant evidence of bias within an LLM-enabled mental health conversational agent. These findings offer a complementary approach to examining bias in LLM-based conversational agents for mental health support.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11807910"
    },
    {
      "pmid": "39488857",
      "title": "The bias algorithm: how AI in healthcare exacerbates ethnic and racial disparities - a scoping review.",
      "abstract": "This scoping review examined racial and ethnic bias in artificial intelligence health algorithms (AIHA), the role of stakeholders in oversight, and the consequences of AIHA for health equity. Using the PRISMA-ScR guidelines, databases were searched between 2020 and 2024 using the terms racial and ethnic bias in health algorithms resulting in a final sample of 23 sources. Suggestions for how to mitigate algorithmic bias were compiled and evaluated, roles played by stakeholders were identified, and governance and stewardship plans for AIHA were examined. While AIHA represent a significant breakthrough in predictive analytics and treatment optimization, regularly outperforming humans in diagnostic precision and accuracy, they also present serious challenges to patient privacy, data security, institutional transparency, and health equity. Evidence from extant sources including those in this review showed that AIHA carry the potential to perpetuate health inequities. While the current study considered AIHA in the US, the use of AIHA carries implications for global health equity.",
      "journal": "Ethnicity & health",
      "year": "2025",
      "doi": "10.1080/13557858.2024.2422848",
      "authors": "Hussain Syed Ali et al.",
      "keywords": "Health algorithms; Reduced inequalities; algorithm oversight; algorithmic bias; bias mitigation; digital health equity; good health and well-being",
      "mesh_terms": "Humans; Algorithms; Artificial Intelligence; Ethnicity; Health Equity; Healthcare Disparities; Racism; United States",
      "pub_types": "Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39488857/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Evidence from extant sources including those in this review showed that AIHA carry the potential to perpetuate health inequities. While the current study considered AIHA in the US, the use of AIHA carries implications for global health equity.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39495690",
      "title": "UnBias: Unveiling Bias Implications in Deep Learning Models for Healthcare Applications.",
      "abstract": "The rapid integration of deep learning-powered artificial intelligence systems in diverse applications such as healthcare, credit assessment, employment, and criminal justice has raised concerns about their fairness, particularly in how they handle various demographic groups. This study delves into the existing biases and their ethical implications in deep learning models. It introduces an UnBias approach for assessing bias in different deep neural network architectures and detects instances where bias seeps into the learning process, shifting the model's focus away from the main features. This contributes to the advancement of equitable and trustworthy AI applications in diverse social settings, especially in healthcare. A case study on COVID-19 detection is carried out, involving chest X-ray scan datasets from various publicly accessible repositories and five well-represented and underrepresented gender-based models across four deep-learning architectures: ResNet50V2, DenseNet121, InceptionV3, and Xception.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2025",
      "doi": "10.1109/JBHI.2024.3484951",
      "authors": "AbdulQawy Asmaa et al.",
      "keywords": "",
      "mesh_terms": "Deep Learning; Humans; COVID-19; SARS-CoV-2; Male; Female; Bias; Neural Networks, Computer",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39495690/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care; Pulmonology",
      "bias_axes": "Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "This contributes to the advancement of equitable and trustworthy AI applications in diverse social settings, especially in healthcare. A case study on COVID-19 detection is carried out, involving chest X-ray scan datasets from various publicly accessible repositories and five well-represented and underrepresented gender-based models across four deep-learning architectures: ResNet50V2, DenseNet121, InceptionV3, and Xception.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39511117",
      "title": "Bias Sensitivity in Diagnostic Decision-Making: Comparing ChatGPT with Residents.",
      "abstract": "BACKGROUND: Diagnostic errors, often due to biases in clinical reasoning, significantly affect patient care. While artificial intelligence chatbots like ChatGPT could help mitigate such biases, their potential susceptibility to biases is unknown. METHODS: This study evaluated diagnostic accuracy of ChatGPT against the performance of 265 medical residents in five previously published experiments aimed at inducing bias. The residents worked in several major teaching hospitals in the Netherlands. The biases studied were case-intrinsic (presence of salient distracting findings in the patient history, effects of disruptive patient behaviors) and situational (prior availability of a look-alike patient). ChatGPT's accuracy in identifying the most-likely diagnosis was measured. RESULTS: Diagnostic accuracy of residents and ChatGPT was equivalent. For clinical cases involving case-intrinsic bias, both ChatGPT and the residents exhibited a decline in diagnostic accuracy. Residents' accuracy decreased on average 12%, while the accuracy of ChatGPT 4.0 decreased 21%. Accuracy of ChatGPT 3.5 decreased 9%. These findings suggest that, like human diagnosticians, ChatGPT is sensitive to bias when the biasing information is part of the patient history. When the biasing information was extrinsic to the case in the form of the prior availability of a look-alike case, residents' accuracy decreased by 15%. By contrast, ChatGPT's performance was not affected by the biasing information. Chi-square goodness-of-fit tests corroborated these outcomes. CONCLUSIONS: It seems that, while ChatGPT is not sensitive to bias when biasing information is situational, it is sensitive to bias when the biasing information is part of the patient's disease history. Its utility in diagnostic support has potential, but caution is advised. Future research should enhance AI's bias detection and mitigation to make it truly useful for diagnostic support.",
      "journal": "Journal of general internal medicine",
      "year": "2025",
      "doi": "10.1007/s11606-024-09177-9",
      "authors": "Schmidt Henk G et al.",
      "keywords": "",
      "mesh_terms": "Humans; Internship and Residency; Diagnostic Errors; Clinical Decision-Making; Bias; Male; Female; Clinical Competence; Artificial Intelligence; Netherlands; Adult; Generative Artificial Intelligence",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39511117/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "CONCLUSIONS: It seems that, while ChatGPT is not sensitive to bias when biasing information is situational, it is sensitive to bias when the biasing information is part of the patient's disease history. Its utility in diagnostic support has potential, but caution is advised. Future research should enhance AI's bias detection and mitigation to make it truly useful for diagnostic support.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11914423"
    },
    {
      "pmid": "39569464",
      "title": "Using human factors methods to mitigate bias in artificial intelligence-based clinical decision support.",
      "abstract": "OBJECTIVES: To highlight the often overlooked role of user interface (UI) design in mitigating bias in artificial intelligence (AI)-based clinical decision support (CDS). MATERIALS AND METHODS: This perspective paper discusses the interdependency between AI-based algorithm development and UI design and proposes strategies for increasing the safety and efficacy of CDS. RESULTS: The role of design in biasing user behavior is well documented in behavioral economics and other disciplines. We offer an example of how UI designs play a role in how bias manifests in our machine learning-based CDS development. DISCUSSION: Much discussion on bias in AI revolves around data quality and algorithm design; less attention is given to how UI design can exacerbate or mitigate limitations of AI-based applications. CONCLUSION: This work highlights important considerations including the role of UI design in reinforcing/mitigating bias, human factors methods for identifying issues before an application is released, and risk communication strategies.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2025",
      "doi": "10.1093/jamia/ocae291",
      "authors": "Militello Laura G et al.",
      "keywords": "artificial intelligence; bias mitigation; clinical decision support; human factors methods",
      "mesh_terms": "Decision Support Systems, Clinical; Humans; Artificial Intelligence; Algorithms; User-Computer Interface; Bias; Ergonomics; Machine Learning",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39569464/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Clinical Decision Support",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: This work highlights important considerations including the role of UI design in reinforcing/mitigating bias, human factors methods for identifying issues before an application is released, and risk communication strategies.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11756570"
    },
    {
      "pmid": "39612748",
      "title": "The role of artificial intelligence in enhancing healthcare for people with disabilities.",
      "abstract": "The integration of artificial intelligence (AI) in healthcare delivery represents a transformative opportunity to enhance the lives of people living with disabilities. AI-driven technologies, such as assistive devices, conversational agents, and rehabilitation tools, can mitigate health disparities, improve diagnostic accuracy, and facilitate effective communication with healthcare providers, fostering more equitable healthcare environments. This commentary explores these applications while addressing the ethical challenges and limitations associated with AI deployment. Specific challenges, such as algorithmic bias, privacy risks with patient data, and the complexity of designing inclusive technologies, are discussed to provide a balanced perspective. For example, biased diagnostic tools may lead to inequitable care, and privacy breaches can compromise sensitive data. Key areas of focus include personalised care through AI-powered systems, the design of inclusive AI technologies incorporating continuous feedback loops and partnerships with advocacy groups, and the development of AI-enabled robotics for physical assistance. This commentary paper emphasises the importance of addressing these limitations alongside advancing ethical AI practices and ensuring continuous user involvement to meet the diverse needs of people living with disabilities, ultimately promoting greater independence and participation in society. Consequently, while AI holds transformative potential in advancing equitable and inclusive healthcare for people with disabilities, addressing ethical challenges, overcoming limitations, and fostering user-centred design are essential to fully realise its benefits and ensure these innovations promote autonomy, accessibility, and well-being.",
      "journal": "Social science & medicine (1982)",
      "year": "2025",
      "doi": "10.1016/j.socscimed.2024.117560",
      "authors": "Olawade David Bamidele et al.",
      "keywords": "Artificial intelligence; Disability care; Ethical challenges; Healthcare accessibility; Inclusive design",
      "mesh_terms": "Humans; Artificial Intelligence; Persons with Disabilities; Delivery of Health Care; Self-Help Devices",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39612748/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "This commentary paper emphasises the importance of addressing these limitations alongside advancing ethical AI practices and ensuring continuous user involvement to meet the diverse needs of people living with disabilities, ultimately promoting greater independence and participation in society. Consequently, while AI holds transformative potential in advancing equitable and inclusive healthcare for people with disabilities, addressing ethical challenges, overcoming limitations, and fostering use...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39626231",
      "title": "Racial Bias in Clinical and Population Health Algorithms: A Critical Review of Current Debates.",
      "abstract": "Among health care researchers, there is increasing debate over how best to assess and ensure the fairness of algorithms used for clinical decision support and population health, particularly concerning potential racial bias. Here we first distill concerns over the fairness of health care algorithms into four broad categories: (a) the explicit inclusion (or, conversely, the exclusion) of race and ethnicity in algorithms, (b) unequal algorithm decision rates across groups, (c) unequal error rates across groups, and (d) potential bias in the target variable used in prediction. With this taxonomy, we critically examine seven prominent and controversial health care algorithms. We show that popular approaches that aim to improve the fairness of health care algorithms can in fact worsen outcomes for individuals across all racial and ethnic groups. We conclude by offering an alternative, consequentialist framework for algorithm design that mitigates these harms by instead foregrounding outcomes and clarifying trade-offs in the pursuit of equitable decision-making.",
      "journal": "Annual review of public health",
      "year": "2025",
      "doi": "10.1146/annurev-publhealth-071823-112058",
      "authors": "Coots Madison et al.",
      "keywords": "algorithms; bias; equity; fairness; health care; race",
      "mesh_terms": "Humans; Algorithms; Racism; Population Health",
      "pub_types": "Journal Article; Review; Research Support, U.S. Gov't, Non-P.H.S.; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39626231/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Decision Support",
      "health_domain": "ICU/Critical Care; Public Health",
      "bias_axes": "Race/Ethnicity",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "We show that popular approaches that aim to improve the fairness of health care algorithms can in fact worsen outcomes for individuals across all racial and ethnic groups. We conclude by offering an alternative, consequentialist framework for algorithm design that mitigates these harms by instead foregrounding outcomes and clarifying trade-offs in the pursuit of equitable decision-making.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39627045",
      "title": "Gender bias in text-to-image generative artificial intelligence depiction of Australian paramedics and first responders.",
      "abstract": "INTRODUCTION: In Australia, almost 50\u00a0% of paramedics are female yet they remain under-represented in stereotypical depictions of the profession. The potentially transformative value of generative artificial intelligence (AI) may be limited by stereotypical errors, misrepresentations and bias. Increasing use of text-to-image generative AI, like DALL-E 3, could reinforce gender and ethnicity biases and, therefore, is important to objectively evaluate. METHOD: In March 2024, DALL-E 3 was utilised via GPT-4 to generate a series of individual and group images of Australian paramedics, ambulance officers, police officers and firefighters. In total, 82 images were produced including 60 individual-character images, and 22 multiple-character group images. All 326 depicted characters were independently analysed by three reviewers for apparent gender, age, skin tone and ethnicity. RESULTS: Among first responders, 90.8\u00a0% (N\u00a0=\u00a0296) were depicted as male, 90.5\u00a0% (N\u00a0=\u00a0295) as Caucasian, 95.7\u00a0% (N\u00a0=\u00a0312) as a light skin tone, and 94.8\u00a0% (N\u00a0=\u00a0309) as under 55 years of age. For paramedics and police the gender distribution was a statistically significant variation from that of actual Australian workforce data (all p\u00a0<\u00a00.001). Among the images of individual paramedics and ambulance officers (N\u00a0=\u00a032), DALL-E 3 depicted 100\u00a0% as male, 100\u00a0% as Caucasian and 100\u00a0% with light skin tone. CONCLUSION: Gender and ethnicity bias is a significant limitation for text-to-image generative AI using DALL-E 3 among Australian first responders. Generated images have a disproportionately high misrepresentation of males, Caucasians and light skin tones that are not representative of the diversity of paramedics in Australia today.",
      "journal": "Australasian emergency care",
      "year": "2025",
      "doi": "10.1016/j.auec.2024.11.003",
      "authors": "Currie Geoffrey et al.",
      "keywords": "Diversity; First responder; Generative artificial intelligence; Inclusivity",
      "mesh_terms": "Humans; Australia; Male; Female; Allied Health Personnel; Adult; Artificial Intelligence; Sexism; Emergency Responders; Middle Aged; Generative Artificial Intelligence; Paramedics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39627045/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "NLP/LLM; Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Gender and ethnicity bias is a significant limitation for text-to-image generative AI using DALL-E 3 among Australian first responders. Generated images have a disproportionately high misrepresentation of males, Caucasians and light skin tones that are not representative of the diversity of paramedics in Australia today.",
      "ft_include": false,
      "ft_reason": "Not health-related in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39637580",
      "title": "Addressing hidden risks: Systematic review of artificial intelligence biases across racial and ethnic groups in cardiovascular diseases.",
      "abstract": "BACKGROUND: Artificial intelligence (AI)-based models are increasingly being integrated into cardiovascular medicine. Despite promising potential, racial and ethnic biases remain a key concern regarding the development and implementation of AI models in clinical settings. OBJECTIVE: This systematic review offers an overview of the accuracy and clinical applicability of AI models for cardiovascular diagnosis and prognosis across diverse racial and ethnic groups. METHOD: A comprehensive literature search was conducted across four medical and scientific databases: PubMed, MEDLINE via Ovid, Scopus, and the Cochrane Library, to evaluate racial and ethnic disparities in cardiovascular medicine. RESULTS: A total of 1704 references were screened, of which 11 articles were included in the final analysis. Applications of AI-based algorithms across different race/ethnic groups were varied and involved diagnosis, prognosis, and imaging segmentation. Among the 11 studies, 9 (82%) concluded that racial/ethnic bias existed, while 2 (18%) found no differences in the outcomes of AI models across various ethnicities. CONCLUSION: Our results suggest significant differences in how AI models perform in cardiovascular medicine across diverse racial and ethnic groups. CLINICAL RELEVANCE STATEMENT: The increasing integration of AI into cardiovascular medicine highlights the importance of evaluating its performance across diverse populations. This systematic review underscores the critical need to address racial and ethnic disparities in AI-based models to ensure equitable healthcare delivery.",
      "journal": "European journal of radiology",
      "year": "2025",
      "doi": "10.1016/j.ejrad.2024.111867",
      "authors": "Cau Riccardo et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Cardiovascular Diseases; Ethnicity; Racial Groups; Healthcare Disparities",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39637580/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: Our results suggest significant differences in how AI models perform in cardiovascular medicine across diverse racial and ethnic groups. CLINICAL RELEVANCE STATEMENT: The increasing integration of AI into cardiovascular medicine highlights the importance of evaluating its performance across diverse populations. This systematic review underscores the critical need to address racial and ethnic disparities in AI-based models to ensure equitable healthcare delivery.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39694331",
      "title": "Ethical and Bias Considerations in Artificial Intelligence/Machine Learning.",
      "abstract": "As artificial intelligence (AI) gains prominence in pathology and medicine, the ethical implications and potential biases within such integrated AI models will require careful scrutiny. Ethics and bias are important considerations in our practice settings, especially as an increased number of machine learning (ML) systems are being integrated within our various medical domains. Such ML-based systems have demonstrated remarkable capabilities in specified tasks such as, but not limited to, image recognition, natural language processing, and predictive analytics. However, the potential bias that may exist within such AI-ML models can also inadvertently lead to unfair and potentially detrimental outcomes. The source of bias within such ML models can be due to numerous factors but is typically categorized into 3 main buckets (data bias, development bias, and interaction bias). These could be due to the training data, algorithmic bias, feature engineering and selection issues, clinic and institutional bias (ie, practice variability), reporting bias, and temporal bias (ie, changes in technology, clinical practice, or disease patterns). Therefore, despite the potential of these AI-ML applications, their deployment in our day-to-day practice also raises noteworthy ethical concerns. To address ethics and bias in medicine, a comprehensive evaluation process is required, which will encompass all aspects of such systems, from model development through clinical deployment. Addressing these biases is crucial to ensure that AI-ML systems remain fair, transparent, and beneficial to all. This review will discuss the relevant ethical and bias considerations in AI-ML specifically within the pathology and medical domain.",
      "journal": "Modern pathology : an official journal of the United States and Canadian Academy of Pathology, Inc",
      "year": "2025",
      "doi": "10.1016/j.modpat.2024.100686",
      "authors": "Hanna Matthew G et al.",
      "keywords": "artificial intelligence; bias; computational pathology; ethics; machine learning; pathology",
      "mesh_terms": "Humans; Artificial Intelligence; Machine Learning; Bias; Pathology",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39694331/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Pathology",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Data Preprocessing; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "Addressing these biases is crucial to ensure that AI-ML systems remain fair, transparent, and beneficial to all. This review will discuss the relevant ethical and bias considerations in AI-ML specifically within the pathology and medical domain.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39704065",
      "title": "Facilitators and Barriers to Increasing Equity in Cystic Fibrosis Newborn Screening Algorithms.",
      "abstract": "BACKGROUND: Newborn screening (NBS) for cystic fibrosis (CF) was universally implemented in the United States in 2010 to improve disease outcomes. Despite universal screening, disparities in outcomes currently exist between people with CF (PwCF) with Black/African, Asian, Indigenous, and Latino/Hispanic ancestry in comparison to PwCF of European ancestry. This is in part because CFTR panels used for newborn screening are often based on variants common in European ancestries leading to higher rates of false negatives for PwCF from minoritized racial and ethnic groups. METHODS: This study investigated how states evaluate and update their CFNBS algorithms through semi-structured interviews with professionals from four states with ethnically diverse populations and one national consultant. Interviews were transcribed verbatim and analyzed through inductive thematic analysis. RESULTS: Five themes were identified encompassing facilitators, barriers, and motivations for evaluating and updating CF NBS algorithms. Facilitators of effective evaluation and updating of algorithms included effective communication with CF clinical centers and extensive support for CF as compared to other conditions. Although participants stated that their respective NBS programs were aware of the disparate impact of their CF panels on PwCF from minoritized racial and ethnic groups, motivations to decrease this disparity were hampered by a range of funding and logistical barriers, such as limited information about false negative cases and difficulties incorporating next generation sequencing technology. CONCLUSIONS: This study shed light on the experiences of states considering alterations to their CFNBS panels, revealing several key barriers and facilitators to implementing equitable CFNBS algorithms.",
      "journal": "Pediatric pulmonology",
      "year": "2025",
      "doi": "10.1002/ppul.27449",
      "authors": "Madden Kellyn et al.",
      "keywords": "cystic fibrosis; health disparities; health equity; newborn screening",
      "mesh_terms": "Humans; Cystic Fibrosis; Neonatal Screening; Infant, Newborn; Algorithms; United States; Healthcare Disparities; Female; Male; Cystic Fibrosis Transmembrane Conductance Regulator",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39704065/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Pediatrics; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSIONS: This study shed light on the experiences of states considering alterations to their CFNBS panels, revealing several key barriers and facilitators to implementing equitable CFNBS algorithms.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11984462"
    },
    {
      "pmid": "39708960",
      "title": "Health System Purchasing Professionals' Approaches to Considering Equity in Procurement.",
      "abstract": "BACKGROUND: Continuing data on racial bias in pulse oximeters and artificial intelligence have sparked calls for health systems to drive innovation against racial bias in health care device and artificial intelligence markets by incorporating equity concerns explicitly into purchasing decisions. RESEARCH QUESTION: How do health care purchasing professionals integrate equity concerns into purchasing decision-making? STUDY DESIGN AND METHODS: Between August 2023 and March 2024, we conducted semistructured interviews via videoconferencing with health care purchasing professionals about purchasing processes for pulse oximeters and other devices-and whether and where equity concerns arise in decision-making. An abductive approach was used to analyze perspectives on how equity and disparity concerns currently are integrated into health care purchasing decision-making. Health care purchasing professionals (N\u00a0= 30) worked in varied supply chain roles for various health systems and supply chain support and consulting companies across the United States. RESULTS: Health care purchasing professionals described limited considerations of equity in current purchasing processes. They described some receptivity to diversity, equity, and inclusion initiatives, largely focused on diversifying suppliers rather than ensuring that devices and products functioned equitably. Respondents reported that they depended on clinician partners to raise and delineate requirements for equitable performance. Respondents also depicted current sources of evidence used in making purchasing decisions as providing limited information about equitable performance and that large contracts, including with group purchasing organizations, may limit purchasing options. INTERPRETATION: Health system purchasing professionals suggested interest and some nascent successes in diversity, equity, and inclusion considerations in health system purchasing processes, including diverse supplier initiatives, but also expressed a need for strong clinical partnership to ensure equitable performance. Explicit approaches for incorporating equitable performance into health care purchasing likely are needed.",
      "journal": "Chest",
      "year": "2025",
      "doi": "10.1016/j.chest.2024.12.016",
      "authors": "Hauschildt Katrina E et al.",
      "keywords": "biomedical technology assessment; diversity, equity, and inclusion; health equity; health policy; hospital purchasing",
      "mesh_terms": "Humans; United States; Health Personnel; Decision Making",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39708960/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: Health care purchasing professionals described limited considerations of equity in current purchasing processes. They described some receptivity to diversity, equity, and inclusion initiatives, largely focused on diversifying suppliers rather than ensuring that devices and products functioned equitably. Respondents reported that they depended on clinician partners to raise and delineate requirements for equitable performance.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39738559",
      "title": "Generalizability, robustness, and correction bias of segmentations of thoracic organs at risk in CT images.",
      "abstract": "OBJECTIVE: This study aims to assess and compare two state-of-the-art deep learning approaches for segmenting four thoracic organs at risk\u00a0(OAR)-the esophagus, trachea, heart, and aorta-in CT images in the context of radiotherapy planning. MATERIALS AND METHODS: We compare a multi-organ segmentation approach and the fusion of multiple single-organ models, each dedicated to one OAR. All were trained using nnU-Net with the default parameters and the full-resolution configuration. We evaluate their robustness with adversarial perturbations, and their generalizability on external datasets, and explore potential biases introduced by expert corrections compared to fully manual delineations. RESULTS: The two approaches show excellent performance with an average Dice score of 0.928 for the multi-class setting and 0.930 when fusing the four single-organ models. The evaluation of external datasets and common procedural adversarial noise demonstrates the good generalizability of these models. In addition, expert corrections of both models show significant bias to the original automated segmentation. The average Dice score between the two corrections is 0.93, ranging from 0.88 for the trachea to 0.98 for the heart. CONCLUSION: Both approaches demonstrate excellent performance and generalizability in segmenting four thoracic OARs, potentially improving efficiency in radiotherapy planning. However, the multi-organ setting proves advantageous for its efficiency, requiring less training time and fewer resources, making it a preferable choice for this task. Moreover, corrections of AI segmentation by clinicians may lead to biases in the results of AI approaches. A test set, manually annotated, should be used to assess the performance of such methods. KEY POINTS: Question While manual delineation of thoracic organs at risk is labor-intensive, prone to errors, and time-consuming, evaluation of AI models performing this task lacks robustness. Findings The deep-learning model using the nnU-Net framework showed excellent performance, generalizability, and robustness in segmenting thoracic organs in CT, enhancing radiotherapy planning efficiency. Clinical relevance Automatic segmentation of thoracic organs at risk can save clinicians time without compromising the quality of the delineations, and extensive evaluation across diverse settings demonstrates the potential of integrating such models into clinical practice.",
      "journal": "European radiology",
      "year": "2025",
      "doi": "10.1007/s00330-024-11321-2",
      "authors": "Gu\u00e9rendel Corentin et al.",
      "keywords": "Computed tomography; Computer-assisted image analysis; Deep learning; Organs at risk; Thorax",
      "mesh_terms": "Humans; Organs at Risk; Tomography, X-Ray Computed; Deep Learning; Radiotherapy Planning, Computer-Assisted; Esophagus; Heart; Trachea; Radiography, Thoracic; Aorta; Thorax",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39738559/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Generative AI",
      "health_domain": "Radiology/Medical Imaging; Cardiology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Both approaches demonstrate excellent performance and generalizability in segmenting four thoracic OARs, potentially improving efficiency in radiotherapy planning. However, the multi-organ setting proves advantageous for its efficiency, requiring less training time and fewer resources, making it a preferable choice for this task. Moreover, corrections of AI segmentation by clinicians may lead to biases in the results of AI approaches.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39753062",
      "title": "Towards secure and trusted AI in healthcare: A systematic review of emerging innovations and ethical challenges.",
      "abstract": "INTRODUCTION: Artificial Intelligence is in the phase of health care, with transformative innovations in diagnostics, personalized treatment, and operational efficiency. While having potential, critical challenges are apparent in areas of safety, trust, security, and ethical governance. The development of these challenges is important for promoting the responsible adoption of AI technologies into healthcare systems. METHODS: This systematic review of studies published between 2010 and 2023 addressed the applications of AI in healthcare and their implications for safety, transparency, and ethics. A comprehensive search was performed in PubMed, IEEE Xplore, Scopus, and Google Scholar. Those studies that met the inclusion criteria provided empirical evidence, theoretical insights, or systematic evaluations addressing trust, security, and ethical considerations. RESULTS: The analysis brought out both the innovative technologies and the continued challenges. Explainable AI (XAI) emerged as one of the significant developments. It made it possible for healthcare professionals to understand AI-driven recommendations, by this means increasing transparency and trust. Still, challenges in adversarial attacks, algorithmic bias, and variable regulatory frameworks remain strong. According to several studies, more than 60 % of healthcare professionals have expressed their hesitation in adopting AI systems due to a lack of transparency and fear of data insecurity. Moreover, the 2024 WotNot data breach uncovered weaknesses in AI technologies and highlighted the dire requirement for robust cybersecurity. DISCUSSION: Full understanding of the potential of AI will be possible only with putting into practice of ethical and technical maintains in healthcare systems. Effective strategies would include integrating bias mitigation methods, strengthening cybersecurity protocols to prevent breaches. Also by adopting interdisciplinary collaboration with the goal of forming transparent regulatory guidelines. These are very important steps toward earning trust and ensuring that AI systems are safe, reliable, and fair. CONCLUSION: AI can bring transformative opportunities to improve healthcare outcomes, but successful implementation will depend on overcoming the challenges of trust, security, and ethics. Future research should focus on testing these technologies in multiple real-world settings, enhance their scalability, and fine-tune regulations to facilitate accountability. Only by combining technological innovations with ethical principles and strong governance can AI reshape healthcare, ensuring at the same time safety and trustworthiness.",
      "journal": "International journal of medical informatics",
      "year": "2025",
      "doi": "10.1016/j.ijmedinf.2024.105780",
      "authors": "Mohsin Khan Muhammad et al.",
      "keywords": "Artificial Intelligence (AI); Ethics; Explainability; Healthcare; Patient safety; Safety; Transparency; Trust",
      "mesh_terms": "Humans; Artificial Intelligence; Computer Security; Confidentiality; Delivery of Health Care; Trust",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39753062/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Transfer Learning; Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: AI can bring transformative opportunities to improve healthcare outcomes, but successful implementation will depend on overcoming the challenges of trust, security, and ethics. Future research should focus on testing these technologies in multiple real-world settings, enhance their scalability, and fine-tune regulations to facilitate accountability. Only by combining technological innovations with ethical principles and strong governance can AI reshape healthcare, ensuring at the sam...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (1 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39753817",
      "title": "Artificial Intelligence and Cancer Health Equity: Bridging the Divide or Widening the Gap.",
      "abstract": "PURPOSE OF REVIEW: This review aims to evaluate the impact of artificial intelligence (AI) on cancer health equity, specifically investigating whether AI is addressing or widening disparities in cancer outcomes. RECENT FINDINGS: Recent studies demonstrate significant advancements in AI, such as deep learning for cancer diagnosis and predictive analytics for personalized treatment, showing potential for improved precision in care. However, concerns persist about the performance of AI tools across diverse populations due to biased training data. Access to AI technologies also remains limited, particularly in low-income and rural settings. AI holds promise for advancing cancer care, but its current application risks exacerbating existing health disparities. To ensure AI benefits all populations, future research must prioritize inclusive datasets, integrate social determinants of health, and develop ethical frameworks. Addressing these challenges is crucial for AI to contribute positively to cancer health equity and guide future research and policy development.",
      "journal": "Current oncology reports",
      "year": "2025",
      "doi": "10.1007/s11912-024-01627-1",
      "authors": "Dankwa-Mullan Irene et al.",
      "keywords": "AI bias; Artificial intelligence; Cancer health equity; Health disparities; Precision medicine; Predictive analytics",
      "mesh_terms": "Humans; Artificial Intelligence; Health Equity; Neoplasms; Healthcare Disparities",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39753817/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Oncology; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Socioeconomic Status; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Diverse/Representative Data",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "FINDINGS: Recent studies demonstrate significant advancements in AI, such as deep learning for cancer diagnosis and predictive analytics for personalized treatment, showing potential for improved precision in care. However, concerns persist about the performance of AI tools across diverse populations due to biased training data. Access to AI technologies also remains limited, particularly in low-income and rural settings.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39841529",
      "title": "AI Can Be a Powerful Social Innovation for Public Health if Community Engagement Is at the Core.",
      "abstract": "There is a critical need for community engagement in the process of adopting artificial intelligence (AI) technologies in public health. Public health practitioners and researchers have historically innovated in areas like vaccination and sanitation but have been slower in adopting emerging technologies such as generative AI. However, with increasingly complex funding, programming, and research requirements, the field now faces a pivotal moment to enhance its agility and responsiveness to evolving health challenges. Participatory methods and community engagement are key components of many current public health programs and research. The field of public health is well positioned to ensure community engagement is part of AI technologies applied to population health issues. Without such engagement, the adoption of these technologies in public health may exclude significant portions of the population, particularly those with the fewest resources, with the potential to exacerbate health inequities. Risks to privacy and perpetuation of bias are more likely to be avoided if AI technologies in public health are designed with knowledge of community engagement, existing health disparities, and strategies for improving equity. This viewpoint proposes a multifaceted approach to ensure safer and more effective integration of AI in public health with the following call to action: (1) include the basics of AI technology in public health training and professional development; (2) use a community engagement approach to co-design AI technologies in public health; and (3) introduce governance and best practice mechanisms that can guide the use of AI in public health to prevent or mitigate potential harms. These actions will support the application of AI to varied public health domains through a framework for more transparent, responsive, and equitable use of this evolving technology, augmenting the work of public health practitioners and researchers to improve health outcomes while minimizing risks and unintended consequences.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/68198",
      "authors": "Bazzano Alessandra N et al.",
      "keywords": "Artificial Intelligence; Citizen Science; Community Participation; Generative Artificial Intelligence; Innovation Diffusion",
      "mesh_terms": "Artificial Intelligence; Public Health; Humans; Community Participation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39841529/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Generative AI",
      "health_domain": "ICU/Critical Care; Public Health",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "This viewpoint proposes a multifaceted approach to ensure safer and more effective integration of AI in public health with the following call to action: (1) include the basics of AI technology in public health training and professional development; (2) use a community engagement approach to co-design AI technologies in public health; and (3) introduce governance and best practice mechanisms that can guide the use of AI in public health to prevent or mitigate potential harms. These actions will s...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11799803"
    },
    {
      "pmid": "39871015",
      "title": "Assessing online chat-based artificial intelligence models for weight loss recommendation appropriateness and bias in the presence of guideline incongruence.",
      "abstract": "BACKGROUND AND AIM: Managing obesity requires a comprehensive approach that involves therapeutic lifestyle changes, medications, or metabolic surgery. Many patients seek health information from online sources and artificial intelligence models like ChatGPT, Google Gemini, and Microsoft Copilot before consulting health professionals. This study aims to evaluate the appropriateness of the responses of Google Gemini and Microsoft Copilot to questions on pharmacologic and surgical management of obesity and assess for bias in their responses to either the ADA or AACE guidelines. METHODS: Ten questions were compiled into a set and posed separately to the free editions of Google Gemini and Microsoft Copilot. Recommendations for the questions were extracted from the ADA and the AACE websites, and the responses were graded by reviewers for appropriateness, completeness, and bias to any of the guidelines. RESULTS: All responses from Microsoft Copilot and 8/10 (80%) responses from Google Gemini were appropriate. There were no inappropriate responses. Google Gemini refused to respond to two questions and insisted on consulting a physician. Microsoft Copilot (10/10; 100%) provided a higher proportion of complete responses than Google Gemini (5/10; 50%). Of the eight responses from Google Gemini, none were biased towards any of the guidelines, while two of the responses from Microsoft Copilot were biased. CONCLUSION: The study highlights the role of Microsoft Copilot and Google Gemini in weight loss management. The differences in their responses may be attributed to the variation in the quality and scope of their training data and design.",
      "journal": "International journal of obesity (2005)",
      "year": "2025",
      "doi": "10.1038/s41366-025-01717-5",
      "authors": "Annor Eugene et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Obesity; Weight Loss; Internet; Practice Guidelines as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39871015/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "EHR/Health Informatics; Surgery",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: The study highlights the role of Microsoft Copilot and Google Gemini in weight loss management. The differences in their responses may be attributed to the variation in the quality and scope of their training data and design.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39874747",
      "title": "Sex-Based Bias in Artificial Intelligence-Based Segmentation Models in Clinical Oncology.",
      "abstract": "Artificial intelligence (AI) advancements have accelerated applications of imaging in clinical oncology, especially in revolutionizing the safe and accurate delivery of state-of-the-art imaging-guided radiotherapy techniques. However, concerns are growing over the potential for sex-related bias and the omission of female-specific data in multi-organ segmentation algorithm development pipelines. Opportunities exist for addressing sex-specific data as a source of bias, and improving sex inclusion to adequately inform the development of AI-based technologies to ensure their fairness, generalizability and equitable distribution. The goal of this review is to discuss the importance of biological sex for AI-based multi-organ image segmentation in routine clinical and radiation oncology; sources of sex-based bias in data generation, model building and implementation and recommendations to ensure AI equity in this rapidly evolving domain.",
      "journal": "Clinical oncology (Royal College of Radiologists (Great Britain))",
      "year": "2025",
      "doi": "10.1016/j.clon.2025.103758",
      "authors": "Doo F X et al.",
      "keywords": "Algorithmic bias; artificial intelligence; deep learning; medical image segmentation; radiotherapy; sex bias",
      "mesh_terms": "Humans; Animals; Sex Factors; Artificial Intelligence; Medical Oncology; Sexism; Male; Female",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39874747/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Computer Vision/Imaging AI; Generative AI",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Opportunities exist for addressing sex-specific data as a source of bias, and improving sex inclusion to adequately inform the development of AI-based technologies to ensure their fairness, generalizability and equitable distribution. The goal of this review is to discuss the importance of biological sex for AI-based multi-organ image segmentation in routine clinical and radiation oncology; sources of sex-based bias in data generation, model building and implementation and recommendations to ens...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11850178"
    },
    {
      "pmid": "39881067",
      "title": "Fairness in Low Birthweight Predictive Models: Implications of Excluding Race/Ethnicity.",
      "abstract": "CONTEXT: To evaluate algorithmic fairness in low birthweight predictive models. STUDY DESIGN: This study analyzed insurance claims (n\u2009=\u20099,990,990; 2013-2021) linked with birth certificates (n\u2009=\u2009173,035; 2014-2021) from the Arkansas All Payers Claims Database (APCD). METHODS: Low birthweight (<\u20092500\u00a0g) predictive models included four approaches (logistic, elastic net, linear discriminate analysis, and gradient boosting machines [GMB]) with and without racial/ethnic information. Model performance was assessed overall, among Hispanic individuals, and among non-Hispanic White, Black, Native Hawaiian/Other Pacific Islander, and Asian individuals using multiple measures of predictive performance (i.e., AUC [area under the receiver operating characteristic curve] scores, calibration, sensitivity, and specificity). RESULTS: AUC scores were lower (underperformed) for Black and Asian individuals relative to White individuals. In the strongest performing model (i.e., GMB), the AUC scores for Black (0.718 [95% CI: 0.705-0.732]) and Asian (0.655 [95% CI: 0.582-0.728]) populations were lower than the AUC for White individuals (0.764 [95% CI: 0.754-0.775 ]). Model performance measured using AUC was comparable in models that included and excluded race/ethnicity; however, sensitivity (i.e., the percent of records correctly predicted as \"low birthweight\" among those who actually had low birthweight) was lower and calibration was weaker, suggesting underprediction for Black individuals when race/ethnicity were excluded. CONCLUSIONS: This study found that racially blind models resulted in underprediction and reduced algorithmic performance, measured using sensitivity and calibration, for Black populations. Such under prediction could unfairly decrease resource allocation needed to reduce perinatal health inequities. Population health management programs should carefully consider algorithmic fairness in predictive models and associated resource allocation decisions.",
      "journal": "Journal of racial and ethnic health disparities",
      "year": "2025",
      "doi": "10.1007/s40615-025-02296-x",
      "authors": "Brown Clare C et al.",
      "keywords": "Equity; Algorithmic fairness; Low birthweight",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39881067/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "XGBoost/Gradient Boosting; Clinical Prediction Model",
      "health_domain": "Obstetrics/Maternal Health; Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: This study found that racially blind models resulted in underprediction and reduced algorithmic performance, measured using sensitivity and calibration, for Black populations. Such under prediction could unfairly decrease resource allocation needed to reduce perinatal health inequities. Population health management programs should carefully consider algorithmic fairness in predictive models and associated resource allocation decisions.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12304234"
    },
    {
      "pmid": "39901187",
      "title": "Accounting for racial bias and social determinants of health in a model of hypertension control.",
      "abstract": "BACKGROUND: Hypertension control remains a critical problem and most of the existing literature views it from a clinical perspective, overlooking the role of sociodemographic factors. This study aims to identify patients with not well-controlled hypertension using readily available demographic and socioeconomic features and elucidate important predictive variables. METHODS: In this retrospective cohort study, records from 1/1/2012 to 1/1/2020 at the Boston Medical Center were used. Patients with either a hypertension diagnosis or related records (\u2265\u2009130\u00a0mmHg systolic or\u2009\u2265\u200990\u00a0mmHg diastolic, n\u2009=\u2009164,041) were selected. Models were developed to predict which patients had uncontrolled hypertension defined as systolic blood pressure (SBP) records exceeding 160\u00a0mmHg. RESULTS: The predictive model of high SBP reached an Area Under the Receiver Operating Characteristic Curve of 74.49%\u2009\u00b1\u20090.23%. Age, race, Social Determinants of Health (SDoH), mental health, and cigarette use were predictive of high SBP. Being Black or having critical social needs led to higher probability of uncontrolled SBP. To mitigate model bias and elucidate differences in predictive variables, two separate models were trained for Black and White patients. Black patients face a 4.7 \u00d7 higher False Positive Rate (FPR) and a 0.58 \u00d7 lower False Negative Rate (FNR) compared to White patients. Decision threshold differentiation was implemented to equalize FNR. Race-specific models revealed different sets of social variables predicting high SBP, with Black patients being affected by structural barriers (e.g., food and transportation) and White patients by personal and demographic factors (e.g., marital status). CONCLUSIONS: Models using non-clinical factors can predict which patients exhibit poorly controlled hypertension. Racial and SDoH variables are significant predictors but lead to biased predictive models. Race-specific models are not sufficient to resolve such biases and require further decision threshold tuning. A host of structural socioeconomic factors are identified to be targeted to reduce disparities in hypertension control.",
      "journal": "BMC medical informatics and decision making",
      "year": "2025",
      "doi": "10.1186/s12911-025-02873-4",
      "authors": "Hu Yang et al.",
      "keywords": "Hypertension; Machine learning; Racial bias; Social determinants of health",
      "mesh_terms": "Humans; Hypertension; Social Determinants of Health; Female; Middle Aged; Male; Retrospective Studies; Aged; Racism; Adult; Boston; Black or African American; White",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39901187/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Models using non-clinical factors can predict which patients exhibit poorly controlled hypertension. Racial and SDoH variables are significant predictors but lead to biased predictive models. Race-specific models are not sufficient to resolve such biases and require further decision threshold tuning.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11792567"
    },
    {
      "pmid": "39904407",
      "title": "Evaluating the Bias, type I error and statistical power of the prior Knowledge-Guided integrated likelihood estimation (PIE) for bias reduction in EHR based association studies.",
      "abstract": "OBJECTIVES: Binary outcomes in electronic health records (EHR) derived using automated phenotype algorithms may suffer from phenotyping error, resulting in bias in association estimation. Huang et al. [1] proposed the Prior Knowledge-Guided Integrated Likelihood Estimation (PIE) method to mitigate the estimation bias, however, their investigation focused on point estimation without statistical inference, and the evaluation of PIE therein using simulation was a proof-of-concept with only a limited scope of scenarios. This study aims to comprehensively assess PIE's performance including (1) how well PIE performs under a wide spectrum of operating characteristics of phenotyping algorithms under real-world scenarios (e.\u00a0g., low prevalence, low sensitivity, high specificity); (2) beyond point estimation, how much variation of the PIE estimator was introduced by the prior distribution; and (3) from a hypothesis testing point of view, if PIE improves type I error and statistical power relative to the na\u00efve method (i.e., ignoring the phenotyping error). METHODS: Synthetic data and use-case analysis were utilized to evaluate PIE. The synthetic data were generated under diverse outcome prevalence, phenotyping algorithm sensitivity, and association effect sizes. Simulation studies compared PIE under different prior distributions with the na\u00efve method, assessing bias, variance, type I error, and power. Use-case analysis compared the performance of PIE and the na\u00efve method in estimating the association of multiple predictors with COVID-19 infection. RESULTS: PIE exhibited reduced bias compared to the na\u00efve method across varied simulation settings, with comparable type I error and power. As the effect size became larger, the bias reduced by PIE was larger. PIE has superior performance when prior distributions aligned closely with true phenotyping algorithm characteristics. Impact of prior quality was minor for low-prevalence outcomes but large for common outcomes. In use-case analysis, PIE maintains a relatively accurate estimation across different scenarios, particularly outperforming the na\u00efve approach under large effect sizes. CONCLUSION: PIE effectively mitigates estimation bias in a wide spectrum of real-world settings, particularly with accurate prior information. Its main benefit lies in bias reduction rather than hypothesis testing. The impact of the prior is small for low-prevalence outcomes.",
      "journal": "Journal of biomedical informatics",
      "year": "2025",
      "doi": "10.1016/j.jbi.2025.104787",
      "authors": "Jing Naimin et al.",
      "keywords": "Association study; Bias reduction; Electronic health record; Phenotyping error",
      "mesh_terms": "Electronic Health Records; Humans; Algorithms; Likelihood Functions; Bias; COVID-19; Phenotype; SARS-CoV-2; Computer Simulation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39904407/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics; Pulmonology; Infectious Disease",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Data Augmentation",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: PIE effectively mitigates estimation bias in a wide spectrum of real-world settings, particularly with accurate prior information. Its main benefit lies in bias reduction rather than hypothesis testing. The impact of the prior is small for low-prevalence outcomes.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12180398"
    },
    {
      "pmid": "39925011",
      "title": "Is more data always better? On alternative policies to mitigate bias in Artificial Intelligence health systems.",
      "abstract": "The development and implementation of Artificial Intelligence (AI) health systems represent a great power that comes with great responsibility. Their capacity to improve and transform healthcare involves inevitable risks. A major risk in this regard is the propagation of bias throughout the life cycle of the AI system, leading to harmful or discriminatory outcomes. This paper argues that the European medical device regulations may prove inadequate to address this-not only technical but also social challenge. With the advent of new regulatory remedies, it seems that the European policymakers also want to reinforce the current medical device legal framework. In this paper, we analyse different policies to mitigate bias in AI health systems included in the Artificial Intelligence Act and in the proposed European Health Data Space. As we shall see, the different remedies based on processing sensitive data for such purpose devised by the European policymakers may have very different effects both on privacy and on protection against discrimination. We find the focus on mitigation during the pre-commercialisation stages rather weak, and believe that bias control once the system has been implemented in the real world would have merited greater ambition.",
      "journal": "Bioethics",
      "year": "2025",
      "doi": "10.1111/bioe.13398",
      "authors": "Lazcoz Guillermo et al.",
      "keywords": "Artificial Intelligence; Artificial Intelligence Act; bias; healthcare; medical device regulation",
      "mesh_terms": "Artificial Intelligence; Humans; Europe; Delivery of Health Care; Privacy; Bias; Health Policy; Confidentiality",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39925011/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "As we shall see, the different remedies based on processing sensitive data for such purpose devised by the European policymakers may have very different effects both on privacy and on protection against discrimination. We find the focus on mitigation during the pre-commercialisation stages rather weak, and believe that bias control once the system has been implemented in the real world would have merited greater ambition.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39949826",
      "title": "Artificial intelligence in global health: An unfair future for health in Sub-Saharan Africa?",
      "abstract": "Artificial intelligence (AI) holds transformative potential for global health, particularly in underdeveloped regions like Africa. However, the integration of AI into healthcare systems raises significant concerns regarding equity and fairness. This debate paper explores the challenges and risks associated with implementing AI in healthcare in Africa, focusing on the lack of infrastructure, data quality issues, and inadequate governance frameworks. It also explores the geopolitical and economic dynamics that exacerbate these disparities, including the impact of global competition and weakened international institutions. While highlighting the risks, the paper acknowledges the potential benefits of AI, including improved healthcare access, standardization of care, and enhanced health communication. To ensure equitable outcomes, it advocates for targeted policy measures, including infrastructure investment, capacity building, regulatory frameworks, and international collaboration. This comprehensive approach is essential to mitigate risks, harness the benefits of AI, and promote social justice in global health.",
      "journal": "Health affairs scholar",
      "year": "2025",
      "doi": "10.1093/haschl/qxaf023",
      "authors": "Victor Aud\u00eancio",
      "keywords": "Africa; artificial intelligence; global governance; health inequalities; social justice in health",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39949826/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "To ensure equitable outcomes, it advocates for targeted policy measures, including infrastructure investment, capacity building, regulatory frameworks, and international collaboration. This comprehensive approach is essential to mitigate risks, harness the benefits of AI, and promote social justice in global health.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11823112"
    },
    {
      "pmid": "39970491",
      "title": "Predicting cancer survival at different stages: Insights from fair and explainable machine learning approaches.",
      "abstract": "OBJECTIVES: While prior machine learning (ML) models for cancer survivability prediction often treated all cancer stages uniformly, cancer survivability prediction should involve understanding how different stages impact the outcomes. Additionally, the success of ML-powered cancer survival prediction models depends a lot on being fair and easy to understand, especially for different stages of cancer. This study addresses cancer survivability prediction using fair and explainable ML methods. METHODS: Focusing on bladder, breast, and prostate cancers using SEER Program data, we developed and validated fair and explainable ML strategies to train separate models for each stage. These computational strategies also advance the fairness and explainability of the ML models. RESULTS: The current work highlights the important role of ML fairness and explainability in stage-specific cancer survivability prediction, capturing and interpreting the associated factors influencing cancer survivability. CONCLUSIONS: This contribution advocates for integrating fairness and explainability in these ML models to ensure equitable, fair, interpretable, and transparent predictions, ultimately enhancing patient care and shared decision-making in cancer treatment.",
      "journal": "International journal of medical informatics",
      "year": "2025",
      "doi": "10.1016/j.ijmedinf.2025.105822",
      "authors": "Kamble Tejasvi Sanjay et al.",
      "keywords": "Cancer survivability; Machine learning explainability; Machine learning fairness",
      "mesh_terms": "Humans; Machine Learning; Male; Neoplasms; SEER Program; Female; Neoplasm Staging; Prostatic Neoplasms; Breast Neoplasms; Prognosis",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39970491/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: This contribution advocates for integrating fairness and explainability in these ML models to ensure equitable, fair, interpretable, and transparent predictions, ultimately enhancing patient care and shared decision-making in cancer treatment.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39972066",
      "title": "Biases in machine-learning models of human single-cell data.",
      "abstract": "Recent machine-learning (ML)-based advances in single-cell data science have enabled the stratification of human tissue donors at single-cell resolution, promising to provide valuable diagnostic and prognostic insights. However, such insights are susceptible to biases. Here we discuss various biases that emerge along the pipeline of ML-based single-cell analysis, ranging from societal biases affecting whose samples are collected, to clinical and cohort biases that influence the generalizability of single-cell datasets, biases stemming from single-cell sequencing, ML biases specific to (weakly supervised or unsupervised) ML models trained on human single-cell samples and biases during the interpretation of results from ML models. We end by providing methods for single-cell data scientists to assess and mitigate biases, and call for efforts to address the root causes of biases.",
      "journal": "Nature cell biology",
      "year": "2025",
      "doi": "10.1038/s41556-025-01619-8",
      "authors": "Willem Theresa et al.",
      "keywords": "",
      "mesh_terms": "Humans; Single-Cell Analysis; Machine Learning; Bias",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39972066/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clustering",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Here we discuss various biases that emerge along the pipeline of ML-based single-cell analysis, ranging from societal biases affecting whose samples are collected, to clinical and cohort biases that influence the generalizability of single-cell datasets, biases stemming from single-cell sequencing, ML biases specific to (weakly supervised or unsupervised) ML models trained on human single-cell samples and biases during the interpretation of results from ML models. We end by providing methods for...",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39974004",
      "title": "Algorithms to Improve Fairness in Medicare Risk Adjustment.",
      "abstract": "IMPORTANCE: Payment system design creates incentives that impact healthcare spending, access, and outcomes. With Medicare Advantage accounting for more than half of Medicare spending, changes to its risk adjustment algorithm have the potential for broad consequences. OBJECTIVE: To develop risk adjustment algorithms that can achieve fair spending targets, and compare their performance to a baseline that emulates the least squares regression approach used by the Centers for Medicare and Medicaid Services. DESIGN: Retrospective analysis of Traditional Medicare enrollment and claims data between January 2017 and December 2020. Diagnoses in claims were mapped to Hierarchical Condition Categories (HCCs). Algorithms used demographic indicators and HCCs from one calendar year to predict Medicare spending in the subsequent year. SETTING: Data from Medicare beneficiaries with documented residence in the United States or Puerto Rico. PARTICIPANTS: A random 20% sample of beneficiaries enrolled in Traditional Medicare. Included beneficiaries were aged 65 years and older, and did not have Medicaid dual eligibility. Race/ethnicity was assigned using the Research Triangle Institute enhanced indicator. MAIN OUTCOME AND MEASURES: Prospective healthcare spending by Medicare. Overall performance was measured by payment system fit and mean absolute error. Net compensation was used to assess group-level fairness. RESULTS: The main analysis included 4,398,035 Medicare beneficiaries with a mean age of 75.2 years and mean annual Medicare spending of $8,345. Out-of-sample payment system fit for the baseline regression was 12.7%. Constrained regression and post-processing both achieved fair spending targets, while maintaining payment system fit values of 12.6% and 12.7%, respectively. Whereas post-processing only increased mean payments for beneficiaries in minoritized racial/ethnic groups, constrained regression increased mean payments for beneficiaries in minoritized racial/ethnic groups and beneficiaries in other groups residing in counties with greater exposure to socioeconomic factors that can adversely affect health outcomes. CONCLUSIONS AND RELEVANCE: Constrained regression and post-processing can incorporate fairness objectives in the Medicare risk adjustment algorithm with minimal reduction in overall fit.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.01.25.25321057",
      "authors": "Reitsma Marissa B et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39974004/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Post-hoc Correction",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: The main analysis included 4,398,035 Medicare beneficiaries with a mean age of 75.2 years and mean annual Medicare spending of $8,345. Out-of-sample payment system fit for the baseline regression was 12.7%. Constrained regression and post-processing both achieved fair spending targets, while maintaining payment system fit values of 12.6% and 12.7%, respectively.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11838972"
    },
    {
      "pmid": "40000544",
      "title": "Innovating Challenges and Experiences in Emory Health AI Bias Datathon: Experience Report.",
      "abstract": "This paper presents an in-depth analysis of the Emory Health AI (Artificial Intelligence) Bias Datathon held in August 2023, providing insights into the experiences gained during the event. The datathon, focusing on health-related issues, attracted diverse participants, including professionals, researchers, and students from various backgrounds. The paper discusses the preparation, organization, and execution of the datathon, detailing the registration process, team formulation, dataset creation, and logistical aspects. We also explore the achievements and personal experiences of participants, highlighting their resilience, dedication, and innovative contributions. The findings include a breakdown of participant demographics, responses to post-event surveys, and participant backgrounds. Observing the trends, we believe the lessons learned, and the overall impact of the Emory Health AI Bias Datathon on the participants and the field of health data science will contribute significantly in organizing future datathons.",
      "journal": "Journal of imaging informatics in medicine",
      "year": "2025",
      "doi": "10.1007/s10278-024-01367-5",
      "authors": "Paddo Atika Rahman et al.",
      "keywords": "Datathon; Healthcare innovation; Participant experiences; Post-event survey",
      "mesh_terms": "Humans; Artificial Intelligence; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40000544/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The findings include a breakdown of participant demographics, responses to post-event surveys, and participant backgrounds. Observing the trends, we believe the lessons learned, and the overall impact of the Emory Health AI Bias Datathon on the participants and the field of health data science will contribute significantly in organizing future datathons.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12701117"
    },
    {
      "pmid": "40027644",
      "title": "Towards machine learning fairness in classifying multicategory causes of deaths in colorectal or lung cancer patients.",
      "abstract": "Classification of patient multicategory survival outcomes is important for personalized cancer treatments. Machine Learning (ML) algorithms have increasingly been used to inform healthcare decisions, but these models are vulnerable to biases in data collection and algorithm creation. ML models have previously been shown to exhibit racial bias, but their fairness towards patients from different age and sex groups have yet to be studied. Therefore, we compared the multimetric performances of 5 ML models (random forests, multinomial logistic regression, linear support vector classifier, linear discriminant analysis, and multilayer perceptron) when classifying colorectal cancer patients (n=515) of various age, sex, and racial groups using the TCGA data. All five models exhibited biases for these sociodemographic groups. We then repeated the same process on lung adenocarcinoma (n=589) to validate our findings. Surprisingly, most models tended to perform more poorly overall for the largest sociodemographic groups. Methods to optimize model performance, including testing the model on merged age, sex, or racial groups, and creating a model trained on and used for an individual or merged sociodemographic group, show potential to reduce disparities in model performance for different groups. Notably, these methods may be used to improve ML fairness while avoiding penalizing the model for exhibiting bias and thus sacrificing overall performance.",
      "journal": "bioRxiv : the preprint server for biology",
      "year": "2025",
      "doi": "10.1101/2025.02.14.638368",
      "authors": "Feng Catherine H et al.",
      "keywords": "Colorectal cancer; feature selection; machine learning; multilabel classification; survival; transcriptomics",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40027644/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Random Forest; Logistic Regression; Support Vector Machine; Neural Network",
      "health_domain": "Oncology; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Methods to optimize model performance, including testing the model on merged age, sex, or racial groups, and creating a model trained on and used for an individual or merged sociodemographic group, show potential to reduce disparities in model performance for different groups. Notably, these methods may be used to improve ML fairness while avoiding penalizing the model for exhibiting bias and thus sacrificing overall performance.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11870570"
    },
    {
      "pmid": "40031152",
      "title": "FairXAI - A Taxonomy and Framework for Fairness and Explainability Synergy in Machine Learning.",
      "abstract": "Explainable artificial intelligence (XAI) and fair learning have made significant strides in various application domains, including criminal recidivism predictions, healthcare settings, toxic comment detection, automatic speech detection, recommendation systems, and image segmentation. However, these two fields have largely evolved independently. Recent studies have demonstrated that incorporating explanations into decision-making processes enhances the transparency and trustworthiness of AI systems. In light of this, our objective is to conduct a systematic review of FairXAI, which explores the interplay between fairness and explainability frameworks. To commence, we propose a taxonomy of FairXAI that utilizes XAI to mitigate and evaluate bias. This taxonomy will be a base for machine learning researchers operating in diverse domains. Additionally, we will undertake an extensive review of existing articles, taking into account factors such as the purpose of the interaction, target audience, and domain and context. Moreover, we outline an interaction framework for FairXAI considering various fairness perceptions and propose a FairXAI wheel that encompasses four core properties that must be verified and evaluated. This will serve as a practical tool for researchers and practitioners, ensuring the fairness and transparency of their AI systems. Furthermore, we will identify challenges and conflicts in the interactions between fairness and explainability, which could potentially pave the way for enhancing the responsibility of AI systems. As the inaugural review of its kind, we hope that this survey will inspire scholars to address these challenges by scrutinizing current research in their respective domains.",
      "journal": "IEEE transactions on neural networks and learning systems",
      "year": "2025",
      "doi": "10.1109/TNNLS.2025.3528321",
      "authors": "Ramachandranpillai Resmi et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40031152/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Furthermore, we will identify challenges and conflicts in the interactions between fairness and explainability, which could potentially pave the way for enhancing the responsibility of AI systems. As the inaugural review of its kind, we hope that this survey will inspire scholars to address these challenges by scrutinizing current research in their respective domains.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40045476",
      "title": "Ethical and regulatory considerations in the use of AI and machine learning in nursing: A systematic review.",
      "abstract": "AIM: This study systematically explores the ethical and regulatory considerations surrounding the integration of artificial intelligence (AI) and machine learning (ML) in nursing practice, with a focus on patient autonomy, data privacy, algorithmic bias, and accountability. BACKGROUND: AI and ML are transforming nursing practice by enhancing clinical decision-making and operational efficiency. However, these technologies present significant ethical challenges related to\u00a0ensuring patient autonomy, safeguarding data privacy, mitigating algorithmic bias, and ensuring transparency in decision-making processes. Current frameworks are not sufficiently tailored to nursing-specific contexts. METHODS: A systematic review was conducted, adhering to PRISMA guidelines. Six major databases were searched for studies published between 2000 and 2024. Seventeen studies met the inclusion criteria and were included in the final analysis. RESULTS: Five key themes emerged from the review: enhancement of clinical decision-making, promotion of ethical awareness, support for routine nursing tasks, challenges in algorithmic bias, and the importance of public engagement in regulatory frameworks. The review identified critical gaps in nursing-specific ethical guidelines and regulatory oversight for AI integration in practice. DISCUSSION: AI technologies offer substantial benefits for nursing, particularly in decision-making and task efficiency. However, these advantages must be balanced against ethical concerns, including the protection of patient rights, algorithmic transparency, and bias mitigation. Current regulatory frameworks require adaptation to meet the ethical needs of nursing. CONCLUSION AND IMPLICATIONS FOR NURSING AND HEALTH POLICY: The findings emphasize the need for the development of nursing-specific ethical guidelines and robust regulatory frameworks to ensure the responsible integration of AI technologies into nursing practice. AI integration must uphold ethical principles while enhancing the quality of care.",
      "journal": "International nursing review",
      "year": "2025",
      "doi": "10.1111/inr.70010",
      "authors": "Mohammed Sobhia Ahmed Abdel Qader et al.",
      "keywords": "academic subject; credentialing; ethics; informatics; information technology; nursing; nursing regulation; registration; telenursing",
      "mesh_terms": "Artificial Intelligence; Humans; Machine Learning; Ethics, Nursing; Female",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40045476/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "RESULTS: Five key themes emerged from the review: enhancement of clinical decision-making, promotion of ethical awareness, support for routine nursing tasks, challenges in algorithmic bias, and the importance of public engagement in regulatory frameworks. The review identified critical gaps in nursing-specific ethical guidelines and regulatory oversight for AI integration in practice. DISCUSSION: AI technologies offer substantial benefits for nursing, particularly in decision-making and task eff...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (1 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40049040",
      "title": "Cognitive biases in forensic psychiatry: A scoping review.",
      "abstract": "Forensic psychiatry plays a critical role in legal contexts but is highly susceptible to cognitive biases that can undermine the accuracy and objectivity of evaluations. This scoping review, guided by the Arksey and O'Malley framework, aims to identify and analyze cognitive biases within forensic psychiatric practice across criminal, civil, and testimonial domains. A comprehensive search across five databases yielded 7002 records, with 24 studies meeting the inclusion criteria. From these studies, ten distinct cognitive biases were identified, with the most frequently discussed being gender bias (29.2\u00a0%), allegiance bias (20.8\u00a0%), and confirmation bias (20.8\u00a0%), followed by hindsight, cultural, and emotional biases. Most studies focused on criminal settings, with only two addressing civil contexts. Among the mitigation strategies reviewed, structured methodologies and the \"considering the opposite\" technique were the most positively evaluated and widely discussed approaches. Conversely, the self-awareness strategy was criticized for its limited effectiveness in reducing bias. Emerging tools, such as artificial intelligence, offer potential solutions but require robust ethical safeguards to prevent the perpetuation of systemic biases. This scoping review provides a comprehensive overview of the current state of research on biases in forensic psychiatry, underscoring the need for further empirical studies to explore their prevalence, mechanisms, and effective mitigation strategies in greater depth.",
      "journal": "International journal of law and psychiatry",
      "year": "2025",
      "doi": "10.1016/j.ijlp.2025.102083",
      "authors": "Buongiorno L et al.",
      "keywords": "",
      "mesh_terms": "Humans; Forensic Psychiatry; Cognition",
      "pub_types": "Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40049040/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Emerging tools, such as artificial intelligence, offer potential solutions but require robust ethical safeguards to prevent the perpetuation of systemic biases. This scoping review provides a comprehensive overview of the current state of research on biases in forensic psychiatry, underscoring the need for further empirical studies to explore their prevalence, mechanisms, and effective mitigation strategies in greater depth.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40056233",
      "title": "Integration of AI into psychodermatology: concern for racial bias.",
      "abstract": "",
      "journal": "Archives of dermatological research",
      "year": "2025",
      "doi": "10.1007/s00403-025-04012-5",
      "authors": "Modanlo Nina et al.",
      "keywords": "Artificial intelligence; Bias; Dermatology; Psychodermatology; Race",
      "mesh_terms": "",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40056233/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Dermatology",
      "bias_axes": "Race/Ethnicity",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40063843",
      "title": "Gamified Adaptive Approach Bias Modification in Individuals With Methamphetamine Use History From Communities in Sichuan: Pilot Randomized Controlled Trial.",
      "abstract": "BACKGROUND: Cognitive bias modification (CBM) programs have shown promise in treating psychiatric conditions, but they can be perceived as boring and repetitive. Incorporating gamified designs and adaptive algorithms in CBM training may address this issue and enhance engagement and effectiveness. OBJECTIVES: This study aims to gather preliminary data and assess the preliminary efficacy of an adaptive approach bias modification (A-ApBM) paradigm in reducing cue-induced craving in individuals with methamphetamine use history. METHODS: A randomized controlled trial with 3 arms was conducted. Individuals aged 18-60 years with methamphetamine dependence and at least 1 year of methamphetamine use were recruited from 12 community-based rehabilitation centers in Sichuan, China. Individuals with the inability to fluently operate a smartphone and the presence of mental health conditions other than methamphetamine use disorder were excluded. The A-ApBM group engaged in ApBM training using a smartphone app for 4 weeks. The A-ApBM used an adaptive algorithm to dynamically adjust the difficulty level based on individual performance. Cue-induced craving scores and relapses were assessed using a visual analogue scale at baseline, postintervention, and at week-16 follow-up. RESULTS: A total of 136 participants were recruited and randomized: 48 were randomized to the A-ApBM group, 48 were randomized to the static approach bias modification (S-ApBM) group, and 40 were randomized to the no-intervention control group. The A-ApBM group showed a significant reduction in cue-induced craving scores at postintervention compared with baseline (Cohen d=0.34; P<.01; 95% CI 0.03-0.54). The reduction remained significant at the week-16 follow-up (Cohen d=0.40; P=.01; 95% CI 0.18-0.57). No significant changes were observed in the S-ApBM and control groups. CONCLUSIONS: The A-ApBM paradigm with gamified designs and dynamic difficulty adjustments may be an effective intervention for reducing cue-induced craving in individuals with methamphetamine use history. This approach improves engagement and personalization, potentially enhancing the effectiveness of CBM programs. Further research is needed to validate these findings and explore the application of A-ApBM in other psychiatric conditions.",
      "journal": "JMIR serious games",
      "year": "2025",
      "doi": "10.2196/56978",
      "authors": "Shen Danlin et al.",
      "keywords": "cognitive bias modification; digital therapeutics; effectiveness; engagement; game; gamified design; methamphetamine; pilot RCT; psychiatric; randomized controlled trial; smartphone app; substance use disorder",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40063843/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population; Clinical Trial",
      "key_findings": "CONCLUSIONS: The A-ApBM paradigm with gamified designs and dynamic difficulty adjustments may be an effective intervention for reducing cue-induced craving in individuals with methamphetamine use history. This approach improves engagement and personalization, potentially enhancing the effectiveness of CBM programs. Further research is needed to validate these findings and explore the application of A-ApBM in other psychiatric conditions.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11931399"
    },
    {
      "pmid": "40064176",
      "title": "Artificial intelligence and gender equity: An integrated approach for health professional education.",
      "abstract": "INTRODUCTION: As artificial intelligence (AI) increasingly integrates into health workplaces, evidence suggests AI can exacerbate gender inequity. Health professional programmes have a role to play in ensuring graduates grasp the challenges facing working in an AI-mediated world. APPROACH: Drawing from feminist scholars and empirical evidence, this conceptual paper synthesises current and future ways in which AI compounds gender inequities and, in response, proposes foci for an integrated approach to teaching about AI and equity. ANALYSIS: We propose three concerns. Firstly, multiple literature reviews suggest that the gender divide is embedded within AI technologies from both process (AI development) and product (AI output) perspectives. Next, there is emerging evidence that AI is reinforcing already entrenched health workforce inequities, where certain types of roles are seen as being the domain of certain genders. Finally, AI may disassociate health professionals' interactions with an embodied, agentic patient by diverting attention to a gendered digital twin. IMPLICATIONS: Responding to these concerns is not simply a matter of teaching about bias but needs to promote an understanding of AI as a sociotechnical phenomenon. Healthcare curricula could usefully provide clinically relevant educational experiences that illustrate how AI intersects with inequitable gendered knowledge practices. Students can be directed to: (1) explore doubts when working with AI-generated data or decisions; (2) refocus on caring through prioritising embodied connections; and (3) consider how to negotiate gendered workplaces in a time of AI. CONCLUSION: The intersection of gender equity and AI provides an accessible, illustrative case about how changing knowledge practices have the potential to embed inequity and how health professional education programmes might respond.",
      "journal": "Medical education",
      "year": "2025",
      "doi": "10.1111/medu.15657",
      "authors": "Bearman Margaret et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Gender Equity; Female; Male; Health Personnel; Sexism",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40064176/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Intersectional",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: The intersection of gender equity and AI provides an accessible, illustrative case about how changing knowledge practices have the potential to embed inequity and how health professional education programmes might respond.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12437998"
    },
    {
      "pmid": "40064867",
      "title": "Equitable machine learning counteracts ancestral bias in precision medicine.",
      "abstract": "Gold standard genomic datasets severely under-represent non-European populations, leading to inequities and a limited understanding of human disease. Therapeutics and outcomes remain hidden because we lack insights that could be gained from analyzing ancestrally diverse genomic data. To address this significant gap, we present PhyloFrame, a machine learning method for equitable genomic precision medicine. PhyloFrame corrects for ancestral bias by integrating functional interaction networks and population genomics data with transcriptomic training data. Application of PhyloFrame to breast, thyroid, and uterine cancers shows marked improvements in predictive power across all ancestries, less model overfitting, and a higher likelihood of identifying known cancer-related genes. Validation in fourteen ancestrally diverse datasets demonstrates that PhyloFrame is better able to adjust for ancestry bias across all populations. The ability to provide accurate predictions for underrepresented groups, in particular, is substantially increased. Analysis of performance in the most diverse continental ancestry group, African, illustrates how phylogenetic distance from training data negatively impacts model performance, as well as PhyloFrame's capacity to mitigate these effects. These results demonstrate how equitable artificial intelligence (AI) approaches can mitigate ancestral bias in training data and contribute to equitable representation in medical research.",
      "journal": "Nature communications",
      "year": "2025",
      "doi": "10.1038/s41467-025-57216-8",
      "authors": "Smith Leslie A et al.",
      "keywords": "",
      "mesh_terms": "Humans; Machine Learning; Precision Medicine; Genomics; Phylogeny; Female; Breast Neoplasms; Neoplasms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40064867/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; ICU/Critical Care; Genomics/Genetics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Diverse/Representative Data",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "Analysis of performance in the most diverse continental ancestry group, African, illustrates how phylogenetic distance from training data negatively impacts model performance, as well as PhyloFrame's capacity to mitigate these effects. These results demonstrate how equitable artificial intelligence (AI) approaches can mitigate ancestral bias in training data and contribute to equitable representation in medical research.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11894161"
    },
    {
      "pmid": "40111818",
      "title": "Covariate-adjusted inference for doubly adaptive biased coin design.",
      "abstract": "Randomized controlled trials (RCTs) are pivotal for evaluating the efficacy of medical treatments and interventions, serving as a cornerstone in clinical research. In addition to randomization, achieving balances among multiple targets, such as statistical validity, efficiency, and ethical considerations, is also a central issue in RCTs. The doubly-adaptive biased coin design (DBCD) is notable for its high flexibility and efficiency in achieving any predetermined optimal allocation ratio and reducing variance for a given target allocation. However, DBCD does not account for abundant covariates that may be correlated with responses, which could further enhance trial efficiency. To address this limitation, this article explores the use of covariates in the analysis stage and evaluates the benefits of nonlinear covariate adjustment for estimating treatment effects. We propose a general framework to capture the intricate relationship between subjects' covariates and responses, supported by rigorous theoretical derivation and empirical validation via simulation study. Additionally, we introduce the use of sample splitting techniques for machine learning methods under DBCD, demonstrating the effectiveness of the corresponding estimators in high-dimensional cases. This paper aims to advance both the theoretical research and practical application of DBCD, thereby achieving more accurate and ethical clinical trials.",
      "journal": "Statistical methods in medical research",
      "year": "2025",
      "doi": "10.1177/09622802251324750",
      "authors": "Tu Fuyi et al.",
      "keywords": "Response-adaptive randomization; covariate adjustment; doubly-adaptive biased coin design; machine learning; sample splitting",
      "mesh_terms": "Humans; Randomized Controlled Trials as Topic; Research Design; Models, Statistical; Computer Simulation; Machine Learning; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40111818/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Clinical Trial",
      "key_findings": "Additionally, we introduce the use of sample splitting techniques for machine learning methods under DBCD, demonstrating the effectiveness of the corresponding estimators in high-dimensional cases. This paper aims to advance both the theoretical research and practical application of DBCD, thereby achieving more accurate and ethical clinical trials.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40112222",
      "title": "Cracking the Facade: Analyzing Ohio's \"Don't Say Gay\" Legislation as Disguised Discrimination Under the First and Fourteenth Amendments.",
      "abstract": "The Ohio State Legislature is among the growing nationwide trend in attacking LGBTQ+ rights. Chief among these is Ohio House Bill 8, which claims to limit the types of content children encounter in schools. While the drafters cite this noble intent, the bill's actual impact further harms queer students and teachers, who already bear heavier mental health burdens due to such legislation and its societal implications. This type of legislation recently originated in Florida, where it was signed into law by Governor Ron DeSantis in 2022 and garnered national media attention. As Ohio Governor Mike DeWine signed a near-identical bill in January 2025, the outcomes observed in Florida inform the constitutional analyses for the Ohio constituency. As in Florida, Ohio's bill is left intentionally vague, banning \"gender ideology\" and \"sexual concepts\" in classrooms or constraining them to what is deemed age-appropriate without providing sufficient guidelines for what may be acceptable. The disparate impact of this legislation is rooted entirely in gender classifications, triggering intermediate scrutiny. The bill's ambiguity creates a chilling effect on students' First Amendment rights by restricting the ability to express gender non-conformity without the school disclosing such changes to their families, disregarding the child's safety, and limiting the type of instruction children may receive in the classroom. Consequently, this compels schools to treat LGBTQ+ students and age-appropriate content differently from their heteronormative counterparts, inherently relegating those with queer identities as second-class citizens under the Fourteenth Amendment's Equal Protection and Substantive Due Process clauses.",
      "journal": "Journal of law and health",
      "year": "2025",
      "doi": "",
      "authors": "Porter Sydni L",
      "keywords": "",
      "mesh_terms": "Humans; Ohio; Sexual and Gender Minorities; Male; Civil Rights; Female; Schools; Child",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40112222/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry; Pediatrics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation; Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "The bill's ambiguity creates a chilling effect on students' First Amendment rights by restricting the ability to express gender non-conformity without the school disclosing such changes to their families, disregarding the child's safety, and limiting the type of instruction children may receive in the classroom. Consequently, this compels schools to treat LGBTQ+ students and age-appropriate content differently from their heteronormative counterparts, inherently relegating those with queer identi...",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40162166",
      "title": "Artificial intelligence in psychiatry: A systematic review and meta-analysis of diagnostic and therapeutic efficacy.",
      "abstract": "BACKGROUND: Artificial Intelligence (AI) has demonstrated significant potential in transforming psychiatric care by enhancing diagnostic accuracy and therapeutic interventions. Psychiatry faces challenges like overlapping symptoms, subjective diagnostic methods, and personalized treatment requirements. AI, with its advanced data-processing capabilities, offers innovative solutions to these complexities. AIMS: This study systematically reviewed and meta-analyzed the existing literature to evaluate AI's diagnostic accuracy and therapeutic efficacy in psychiatric care, focusing on various psychiatric disorders and AI technologies. METHODS: Adhering to PRISMA guidelines, the study included a comprehensive literature search across multiple databases. Empirical studies investigating AI applications in psychiatry, such as machine learning (ML), deep learning (DL), and hybrid models, were selected based on predefined inclusion criteria. The outcomes of interest were diagnostic accuracy and therapeutic efficacy. Statistical analysis employed fixed- and random-effects models, with subgroup and sensitivity analyses exploring the impact of AI methodologies and study designs. RESULTS: A total of 14 studies met the inclusion criteria, representing diverse AI applications in diagnosing and treating psychiatric disorders. The pooled diagnostic accuracy was 85% (95% CI: 80%-87%), with ML models achieving the highest accuracy, followed by hybrid and DL models. For therapeutic efficacy, the pooled effect size was 84% (95% CI: 82%-86%), with ML excelling in personalized treatment plans and symptom tracking. Moderate heterogeneity was observed, reflecting variability in study designs and populations. The risk of bias assessment indicated high methodological rigor in most studies, though challenges like algorithmic biases and data quality remain. CONCLUSION: AI demonstrates robust diagnostic and therapeutic capabilities in psychiatry, offering a data-driven approach to personalized mental healthcare. Future research should address ethical concerns, standardize methodologies, and explore underrepresented populations to maximize AI's transformative potential in mental health.",
      "journal": "Digital health",
      "year": "2025",
      "doi": "10.1177/20552076251330528",
      "authors": "Rony Moustaq Karim Khan et al.",
      "keywords": "Artificial intelligence; diagnostic accuracy; machine learning; mental health care; psychiatry; therapeutic efficacy",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40162166/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: AI demonstrates robust diagnostic and therapeutic capabilities in psychiatry, offering a data-driven approach to personalized mental healthcare. Future research should address ethical concerns, standardize methodologies, and explore underrepresented populations to maximize AI's transformative potential in mental health.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 2 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11951893"
    },
    {
      "pmid": "40173063",
      "title": "Neighbor-Guided Unbiased Framework for Generalized Category Discovery in Medical Image Classification.",
      "abstract": "Generalized category discovery (GCD) utilizes seen category knowledge to automatically discover new semantic categories that are not defined in the training phase. Nevertheless, there has been no research conducted on identifying new classes using medical images and disease categories, which is essential for understanding and diagnosing specific diseases. Moreover, existing methods still produce predictions that are biased towards seen categories since the model is mainly supervised by labeled seen categories, which in turn leads to sub-optimal clustering performance. In this paper, we propose a new neighbor-guided unbiased framework (NGUF) that leverages neighbor information to mitigate prediction bias to address the GCD problem in medical tasks. Specifically, we devise a neighbor-guided cross-pseudo-clustering strategy, which exploits the knowledge of the nearest-neighbor samples to adjust the model predictions thereby generating unbiased pseudo-clustering supervision. Then, based on the unbiased pseudo-clustering supervision, we use a view-invariant learning strategy to assign labels to all samples. In addition, we propose an adaptive weight learning strategy that dynamically determines the degree of adjustment of the predictions of different samples based on the distance density values. Finally, we further propose a cross-batch knowledge distillation module to utilize information from successive iterations to encourage training consistency. Extensive experiments on four medical image datasets show that NGUF is effective in mitigating the model's prediction bias and has superior performance to other state-of-the-art GCD algorithms. Our code will be released soon.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2025",
      "doi": "10.1109/JBHI.2025.3556984",
      "authors": "Feng Wei et al.",
      "keywords": "",
      "mesh_terms": "Humans; Algorithms; Cluster Analysis; Image Interpretation, Computer-Assisted; Machine Learning; Semantics; Image Processing, Computer-Assisted; Diagnostic Imaging",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40173063/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Computer Vision/Imaging AI; Clustering",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Extensive experiments on four medical image datasets show that NGUF is effective in mitigating the model's prediction bias and has superior performance to other state-of-the-art GCD algorithms. Our code will be released soon.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40182428",
      "title": "Unveiling patient profiles associated with elevated Lp(a) through an unbiased clustering analysis.",
      "abstract": "INTRODUCTION: Lipoprotein(a) [Lp(a)] has been recognized as key factor in cardiovascular research. This study aimed to identify key patient profiles based on the characteristics of a Portuguese cohort of adults who were referred for Lp(a) measurement. METHOD: An unsupervised clustering analysis was performed on 661 Portuguese adults to identify patient profiles associated with lipoprotein a [Lp(a)] based on a range of demographic and clinical indicators. Lp(a) levels were deliberately excluded from the algorithm, to ensure an unbiased cluster formation. RESULTS: The analysis revealed two distinct clusters based on Lp(a) levels. Cluster 1 (n\u2009=\u2009336) exhibited significantly higher median Lp(a) levels than Cluster 2 (n\u2009=\u2009325; p\u2009=\u20090.004), with 46.4% of individuals exceeding the 75\u2005nmol/L (30\u2005mg/dl) risk threshold (p\u2009<\u20090.001). This group was characterized by older age (median 57 vs. 45 years), lower body mass index (27.17 vs. 29.40), and a majority male composition (73.8% vs. 26.5%). Additionally, Cluster 1 displayed a higher prevalence of hypertension (56.5% vs. 31.1%), diabetes mellitus (38.7% vs. 17.2%), and dyslipidemia (88.7% vs. 55.4%). These data suggest that the Cluster 1 profile has a potential increased risk for cardiovascular complications and underscore the importance of considering specific patient profiles for Lp(a) screening and cardiovascular risk assessment. CONCLUSION: Despite the study limitations, including single-institution data and potential selection bias, this study highlights the utility of cluster analysis in identifying clinically meaningful patient profiles and suggests that proactive screening and management of Lp(a) levels, particularly in patients with characteristics resembling those of Cluster 1, may be beneficial.",
      "journal": "Frontiers in cardiovascular medicine",
      "year": "2025",
      "doi": "10.3389/fcvm.2025.1546351",
      "authors": "Saraiva Miguel et al.",
      "keywords": "cardiovascular risk assessment; clustering analysis; lipoprotein(a) levels; patient profiling; unsupervised learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40182428/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clustering",
      "health_domain": "Cardiology; ICU/Critical Care; Endocrinology/Diabetes",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: Despite the study limitations, including single-institution data and potential selection bias, this study highlights the utility of cluster analysis in identifying clinically meaningful patient profiles and suggests that proactive screening and management of Lp(a) levels, particularly in patients with characteristics resembling those of Cluster 1, may be beneficial.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11965613"
    },
    {
      "pmid": "40199255",
      "title": "Artificial Intelligence and Venous Thromboembolism: A Narrative Review of Applications, Benefits, and Limitations.",
      "abstract": "<p>Background: Venous thromboembolism (VTE), including deep vein thrombosis and pulmonary embolism, remains a leading cause of cardiovascular morbidity and mortality. Artificial intelligence (AI) holds promise for potential improvement of risk stratification, diagnosis, and management of VTE. Summary: This narrative review explores the applications, benefits, and limitations of AI in VTE management. AI models were shown to outperform conventional methods in identifying high-risk candidates for VTE prophylaxis treatments in several postsurgical settings. It has also been demonstrated to be efficient in the early detection of VTE events, particularly through point-of-care AI-guided sonography and computer tomography image processing. Data biases, model transparency, and the need for regulatory frameworks remain significant limitations in the full integration of AI into clinical practice. Key Messages: AI has the potential to improve VTE care by enhancing risk stratification and diagnosis. The integration of AI-driven models into clinical workflows has the potential to reduce costs, streamline diagnostic processes, and ensure effective management of VTE. Safe and effective integration of AI into VTE care requires addressing its limitations, such as interpretability, privacy, and algorithmic bias. </p>.",
      "journal": "Acta haematologica",
      "year": "2025",
      "doi": "10.1159/000545760",
      "authors": "Mudrik Aya et al.",
      "keywords": "Artificial intelligence; Deep vein thrombosis; Machine learning; Pulmonary embolism; Venous thromboembolism",
      "mesh_terms": "Humans; Venous Thromboembolism; Artificial Intelligence; Risk Assessment; Tomography, X-Ray Computed; Disease Management",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40199255/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Cardiology; ICU/Critical Care; Surgery; Pulmonology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "Safe and effective integration of AI into VTE care requires addressing its limitations, such as interpretability, privacy, and algorithmic bias. </p>.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12091960"
    },
    {
      "pmid": "40204164",
      "title": "Large Language Models for Global Health Clinics: Opportunities and Challenges.",
      "abstract": "Large language models (LLMs) have emerged as a new wave of artificial intelligence, and their applications could emerge as a pivotal resource capable of reshaping health care communication, research, and informed decision-making processes. These models offer unprecedented potential to swiftly disseminate critical health information and transcend linguistic barriers. However, their integration into health care systems presents formidable challenges, including inherent biases in training data, privacy vulnerabilities, and disparities in digital literacy. Despite these obstacles, LLMs possess unparalleled analytic prowess to inform evidence-based health care policies and clinical practices. Addressing these challenges necessitates the formulation of robust ethical frameworks, bias mitigation strategies, and educational initiatives to ensure equitable access to health care resources globally. By navigating these complexities with meticulous attention and foresight, LLMs stand poised to catalyze substantial advancements in global health outcomes, promoting health equity and improving population health worldwide.",
      "journal": "Journal of the American College of Radiology : JACR",
      "year": "2025",
      "doi": "10.1016/j.jacr.2025.04.007",
      "authors": "Tripathi Satvik et al.",
      "keywords": "Large language models; artificial intelligence; global health",
      "mesh_terms": "Global Health; Humans; Artificial Intelligence; Language; Large Language Models",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40204164/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics; Public Health",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "Addressing these challenges necessitates the formulation of robust ethical frameworks, bias mitigation strategies, and educational initiatives to ensure equitable access to health care resources globally. By navigating these complexities with meticulous attention and foresight, LLMs stand poised to catalyze substantial advancements in global health outcomes, promoting health equity and improving population health worldwide.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (2 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40206564",
      "title": "AI's ongoing impact: Implications of AI's effects on health equity for women's healthcare providers.",
      "abstract": "OBJECTIVE: To assess the effects of the current use of artificial intelligence (AI) in women's health on health equity, specifically in primary and secondary prevention efforts among women. METHODS: Two databases, Scopus and PubMed, were used to conduct this narrative review. The keywords included \"artificial intelligence,\" \"machine learning,\" \"women's health,\" \"screen,\" \"risk factor,\" and \"prevent,\" and papers were filtered only to include those about AI models that general practitioners may use. RESULTS: Of the 18 articles reviewed, 8 articles focused on risk factor modeling under primary prevention, and 10 articles focused on screening tools under secondary prevention. Gaps were found in the ability of AI models to train using large, diverse datasets that were reflective of the population it is intended for. Lack of these datasets was frequently identified as a limitation in the papers reviewed (n = 7). CONCLUSIONS: Minority, low-income women have poor access to health care and are, therefore, not well represented in the datasets AI uses to train, which risks introducing bias in its output. To mitigate this, more datasets should be developed to validate AI models, and AI in women's health should expand to include conditions that affect men and women to provide a gendered lens on these conditions. Public health, medical, and technology entities need to collaborate to regulate the development and use of AI in health care at a standard that reduces bias. OBJETIVO: Evaluar los efectos que el uso actual de la inteligencia artificial (IA) en la salud de las mujeres tiene sobre la equidad en la salud, espec\u00edficamente en las actividades de prevenci\u00f3n primaria y secundaria en las mujeres. M\u00c9TODO: Para realizar esta revisi\u00f3n narrativa se utilizaron dos bases de datos, Scopus y PubMed. En la b\u00fasqueda se utiliz\u00f3 el equivalente en ingl\u00e9s de algunas palabras clave como \u201cinteligencia artificial\u201d, \u201caprendizaje autom\u00e1tico\u201d, \u201csalud de la mujer\u201d, \u201ctamizaje\u201d, \u201cfactor de riesgo\u201d y \u201cprevenir\u201d, y los art\u00edculos solo se filtraron para incluir los que trataban sobre modelos de IA que los m\u00e9dicos generales podr\u00edan utilizar. RESULTADOS: De los 18 art\u00edculos examinados, 8 se centraron en la modelizaci\u00f3n de factores de riesgo en el marco de la prevenci\u00f3n primaria y 10 se centraron en las herramientas de tamizaje en el marco de la prevenci\u00f3n secundaria. Se encontraron brechas en la capacidad para entrenar a los modelos de IA con conjuntos de datos amplios y diversos que reflejen la poblaci\u00f3n a la que est\u00e1n destinados. La falta de estos conjuntos de datos se detect\u00f3 con frecuencia como una limitaci\u00f3n en los art\u00edculos examinados (n = 7). CONCLUSIONES: Las mujeres pertenecientes a grupos minoritarios y de ingresos bajos tienen poco acceso a la atenci\u00f3n de salud y, por lo tanto, no est\u00e1n bien representadas en los conjuntos de datos que se utilizan para entrenar los modelos de AI, lo que podr\u00eda introducir sesgos en sus resultados. Para mitigar esto, deben crearse m\u00e1s conjuntos de datos para validar los modelos de IA, y la IA en la salud de las mujeres debe ampliarse para incluir las afecciones que afectan a hombres y mujeres, a fin de proporcionar una perspectiva de g\u00e9nero al respecto. Las entidades de salud p\u00fablica, medicina y tecnolog\u00eda deben colaborar para regular el desarrollo y el uso de la IA en la atenci\u00f3n de salud de una manera estandarizada que reduzca los sesgos. OBJETIVO: Avaliar os efeitos do atual uso da intelig\u00eancia artificial (IA) na \u00e1rea de sa\u00fade da mulher sobre a equidade em sa\u00fade, especificamente em atividades de preven\u00e7\u00e3o prim\u00e1ria e secund\u00e1ria direcionadas para mulheres. M\u00c9TODOS: Revis\u00e3o narrativa de artigos indexados em duas bases de dados, Scopus e PubMed. As palavras-chave inclu\u00edram \u201cartificial intelligence\u201d, \u201cmachine learning\u201d, \u201cwomen\u2019s health\u201d, \u201cscreen\u201d, \u201crisk factor\u201d e \u201cprevent\u201d, e os artigos foram filtrados de modo a incluir somente artigos sobre modelos de IA para uso por m\u00e9dicos generalistas. RESULTADOS: Dos 18 artigos examinados, 8 se concentraram na modelagem de fatores de risco na preven\u00e7\u00e3o prim\u00e1ria e 10, em ferramentas de rastreamento na preven\u00e7\u00e3o secund\u00e1ria. Foram constatadas lacunas na capacidade de treinar os modelos de IA com conjuntos de dados grandes e diversificados que reflitam as popula\u00e7\u00f5es \u00e0s quais se destinam. A falta de tais conjuntos de dados foi frequentemente identificada como uma limita\u00e7\u00e3o nos artigos examinados (n = 7). CONCLUS\u00d5ES: Mulheres minorit\u00e1rias e de baixa renda t\u00eam acesso limitado \u00e0 aten\u00e7\u00e3o \u00e0 sa\u00fade e, portanto, est\u00e3o sub-representadas nos conjuntos de dados utilizados para treinamento de modelos de IA, o que gera o risco da introdu\u00e7\u00e3o de vi\u00e9s nos resultados. Para mitigar isso, \u00e9 preciso desenvolver mais conjuntos de dados para validar os modelos de IA. Al\u00e9m disso, o uso da IA em sa\u00fade da mulher deve ser expandido para incluir afec\u00e7\u00f5es que afetem homens e mulheres, de modo a proporcionar uma perspectiva de g\u00eanero sobre essas afec\u00e7\u00f5es. As entidades de sa\u00fade p\u00fablica, medicina e tecnologia precisam colaborar para regulamentar o desenvolvimento e o uso da IA na aten\u00e7\u00e3o \u00e0 sa\u00fade de maneira a reduzir o vi\u00e9s.",
      "journal": "Revista panamericana de salud publica = Pan American journal of public health",
      "year": "2025",
      "doi": "10.26633/RPSP.2025.19",
      "authors": "Vadlamani Suman et al.",
      "keywords": "Artificial intelligence; ethics; primary prevention; secondary prevention; women\u2019s health",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40206564/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Diverse/Representative Data",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: Minority, low-income women have poor access to health care and are, therefore, not well represented in the datasets AI uses to train, which risks introducing bias in its output. To mitigate this, more datasets should be developed to validate AI models, and AI in women's health should expand to include conditions that affect men and women to provide a gendered lens on these conditions. Public health, medical, and technology entities need to collaborate to regulate the development and...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11980523"
    },
    {
      "pmid": "40253926",
      "title": "Exploring trade-offs in equitable stroke risk prediction with parity-constrained and race-free models.",
      "abstract": "A recent analysis of common stroke risk prediction models showed that performance differs between Black and White subgroups, and that applying standard machine learning methods does not reduce these disparities. There have been calls in the clinical literature to correct such disparities by removing race as a predictor (i.e., race-free models). Alternatively, a variety of machine learning methods have been proposed to constrain differences in model predictions between racial groups. In this work, we compare these approaches for equitable stroke risk prediction. We begin by proposing a discrete-time, neural network-based time-to-event model that incorporates a parity constraint designed to make predictions more similar between groups. Using harmonized data from Framingham Offspring, MESA, and ARIC studies, we develop both parity-constrained and unconstrained stroke risk prediction models, then compare their performance with race-free models in a held-out test set and a secondary validation set (REGARDS). Our evaluation includes both intra-group and inter-group performance metrics for right-censored time to event outcomes. Results illustrate a fundamental trade-off in which parity-constrained models must sacrifice intra-group calibration to improve inter-group discrimination performance, while the race-free models strike a balance between the two. Consequently, the choice of model must depend on the potential benefits and harms associated with the intended clinical use. All models as well as code implementing our approach are available in a public repository. More broadly, these results provide a roadmap for development of equitable clinical risk prediction models and illustrate both merits and limitations of a race-free approach.",
      "journal": "Artificial intelligence in medicine",
      "year": "2025",
      "doi": "10.1016/j.artmed.2025.103130",
      "authors": "Engelhard Matthew et al.",
      "keywords": "Algorithmic bias; Algorithmic fairness; Data harmonization; Machine learning; Risk prediction; Stroke",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Black or African American; Machine Learning; Neural Networks, Computer; Risk Assessment; Risk Factors; Stroke; White",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40253926/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Neural Network; Clinical Prediction Model",
      "health_domain": "Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration",
      "clinical_setting": "Not specified",
      "key_findings": "All models as well as code implementing our approach are available in a public repository. More broadly, these results provide a roadmap for development of equitable clinical risk prediction models and illustrate both merits and limitations of a race-free approach.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12133243"
    },
    {
      "pmid": "40265187",
      "title": "Advancing clinical biochemistry: addressing gaps and driving future innovations.",
      "abstract": "Modern healthcare depends fundamentally on clinical biochemistry for disease diagnosis and therapeutic guidance. The discipline encounters operational constraints, including sampling inefficiencies, precision limitations, and expansion difficulties. Recent advancements in established technologies, such as mass spectrometry and the development of high-throughput screening and point-of-care technologies, are revolutionizing the industry. Modern biosensor technology and wearable monitors facilitate continuous health tracking, Artificial Intelligence (AI)/machine learning (ML) applications enhance analytical capabilities, generating predictive insights for individualized treatment protocols. However, concerns regarding algorithmic bias, data privacy, lack of transparency in decision-making (\"black box\" models), and over-reliance on automated systems pose significant challenges that must be addressed for responsible AI integration. However, significant limitations remain-substantial implementation expenses, system incompatibility issues, and information security vulnerabilities intersect with ethical considerations regarding algorithmic fairness and protected health information. Addressing these challenges demands coordinated efforts between clinicians, scientists, and technical specialists. This review discusses current challenges in clinical biochemistry, explicitly addressing the limitations of reference intervals and barriers to implementing innovative biomarkers in medical settings. The discussion evaluates how advanced technologies and multidisciplinary collaboration can overcome these constraints while identifying research priorities to enhance diagnostic precision and accessibility for better healthcare delivery.",
      "journal": "Frontiers in medicine",
      "year": "2025",
      "doi": "10.3389/fmed.2025.1521126",
      "authors": "Cao Haiou et al.",
      "keywords": "artificial intelligence; biomarkers; clinical biochemistry; mass spectrometry; personalized medicine; point-of-care system",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40265187/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics; Wearables/Remote Monitoring",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Telehealth/Remote",
      "key_findings": "This review discusses current challenges in clinical biochemistry, explicitly addressing the limitations of reference intervals and barriers to implementing innovative biomarkers in medical settings. The discussion evaluates how advanced technologies and multidisciplinary collaboration can overcome these constraints while identifying research priorities to enhance diagnostic precision and accessibility for better healthcare delivery.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12011881"
    },
    {
      "pmid": "40266052",
      "title": "Artificial Intelligence in Cancer Care: Addressing Challenges and Health Equity.",
      "abstract": "Overdiagnosis in cancer care remains a significant concern, often resulting in unnecessary physical, emotional, and financial burdens on patients. Artificial intelligence (AI) has the potential to address this challenge by enabling more accurate, personalized cancer diagnoses and facilitating tailored treatment plans. Integrating AI with precision medicine can minimize unnecessary treatments and associated adverse effects by optimizing care strategies based on individual patient data. However, the integration of AI in oncology requires rigorous research and validation to ensure its effectiveness across diverse populations and clinical settings. Challenges such as algorithmic bias, data representation, and limited access to technology in resource-constrained settings highlight the need for equitable AI applications in health care. Addressing health equity disparities is critical, as diverse and representative training data sets significantly affects the fairness and efficacy of AI systems. AI also holds promise for advancing cancer care in resource-limited settings by providing cost-effective diagnostic tools, democratizing access to advanced health care technologies, and improving outcomes in low- and middle-income nations. Interdisciplinary and international collaborations between researchers, clinicians, and technologists are crucial to maximizing AI's potential in cancer care. By fostering these partnerships and focusing on the development of accessible, ethical, and patient-centered AI applications, the health care community can revolutionize cancer diagnosis and treatment. The growing role of AI in precision medicine brings hope for equitable, cost-effective, and improved patient outcomes worldwide.",
      "journal": "Oncology (Williston Park, N.Y.)",
      "year": "2025",
      "doi": "10.46883/2025.25921037",
      "authors": "Viviana Cortlana Ms et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Health Equity; Neoplasms; Precision Medicine; Medical Oncology",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40266052/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "By fostering these partnerships and focusing on the development of accessible, ethical, and patient-centered AI applications, the health care community can revolutionize cancer diagnosis and treatment. The growing role of AI in precision medicine brings hope for equitable, cost-effective, and improved patient outcomes worldwide.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40267866",
      "title": "Assessing bias in AI-driven psychiatric recommendations: A comparative cross-sectional study of chatbot-classified and CANMAT 2023 guideline for adjunctive therapy in difficult-to-treat depression.",
      "abstract": "The integration of chatbots into psychiatry introduces a novel approach to support clinical decision-making, but biases in their recommendations pose significant concerns. This study investigates potential biases in chatbot-generated recommendations for adjunctive therapy in difficult-to-treat depression, comparing these outputs with the Canadian Network for Mood and Anxiety Treatments (CANMAT) 2023 guidelines. The analysis involved calculating Cohen's kappa coefficients to measure the overall level of agreement between chatbot-generated classifications and CANMAT guidelines. Differences between chatbot-generated and CANMAT classifications for each medication were assessed using the Wilcoxon signed-rank test. Results reveal substantial agreement for high-performing models, such as Google AI's Gemini 2.0 Flash, which achieved the highest Cohen's kappa value of 0.82 (SE = 0.052). In contrast, OpenAI's o1 model showed a lower agreement of 0.746 (SE = 0.057). Notable discrepancies were observed in the overestimation of medications such as quetiapine and lithium and the underestimation of modafinil and ketamine. Additionally, a distinct bias pattern was observed in OpenAI's chatbots, which demonstrated a tendency to over-recommend lithium and bupropion. Our study highlights both the promise and the challenges of employing AI tools in psychiatric practice, and advocates for multi-model approaches to mitigate bias and improve clinical reliability.",
      "journal": "Psychiatry research",
      "year": "2025",
      "doi": "10.1016/j.psychres.2025.116501",
      "authors": "Chang Yu et al.",
      "keywords": "Artificial intelligence; Depression; Evidence-based medicine; Generative artificial intelligence; Guideline",
      "mesh_terms": "Humans; Cross-Sectional Studies; Practice Guidelines as Topic; Artificial Intelligence; Depressive Disorder, Treatment-Resistant; Antidepressive Agents; Clinical Decision-Making; Canada; Psychiatry; Bias; Generative Artificial Intelligence",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40267866/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry; ICU/Critical Care",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "Additionally, a distinct bias pattern was observed in OpenAI's chatbots, which demonstrated a tendency to over-recommend lithium and bupropion. Our study highlights both the promise and the challenges of employing AI tools in psychiatric practice, and advocates for multi-model approaches to mitigate bias and improve clinical reliability.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40276990",
      "title": "Component Associations of the Healthy Worker Survivor Bias in Medical Radiation Workers.",
      "abstract": "BACKGROUND: The healthy worker survivor bias may vary by sex. This study investigated three component associations necessary for this bias to determine the origins of sex differences in this bias among male and female workers. METHODS: We analyzed a data set of 93,918 South Korean diagnostic medical radiation workers registered in the National Dose Registry from 1996 to 2011, linked with mortality and cancer incidence data. Component associations were assessed using Cox regression to estimate hazard ratios (HRs) and logistic regression with generalized estimating equations to estimate odds ratios (ORs). RESULTS: A significant association between prior cumulative exposure and employment status was observed for all-cause mortality in male (HR 1.06, 95% CI 1.02-1.10), whereas an inverse association was noted in female workers (HR 0.82, 95% CI 0.78-0.87). Adjusted ORs for employment status and subsequent exposure for all-cause mortality, as well as HRs for employment status and survival time, demonstrated associations in the same direction in both males and females. CONCLUSIONS: Our findings demonstrate that sex-specific differences in healthy worker survivor bias were primarily driven by the association between prior exposure and employment status. To improve bias mitigation in occupational cohort studies, sex-specific components should be incorporated.",
      "journal": "American journal of industrial medicine",
      "year": "2025",
      "doi": "10.1002/ajim.23727",
      "authors": "Lee Won Jin et al.",
      "keywords": "bias; cohort; health professionals; ionizing radiation; occupational exposure",
      "mesh_terms": "Humans; Male; Female; Middle Aged; Occupational Exposure; Adult; Republic of Korea; Healthy Worker Effect; Employment; Sex Factors; Registries; Bias; Survivors; Proportional Hazards Models; Aged; Logistic Models; Odds Ratio",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40276990/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Logistic Regression; Survival Analysis",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Our findings demonstrate that sex-specific differences in healthy worker survivor bias were primarily driven by the association between prior exposure and employment status. To improve bias mitigation in occupational cohort studies, sex-specific components should be incorporated.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12070152"
    },
    {
      "pmid": "40291888",
      "title": "Magnetic resonance imaging bias field correction improves tumor prognostic evaluation after transcatheter arterial chemoembolization for liver cancer.",
      "abstract": "BACKGROUND: Transcatheter arterial chemoembolization (TACE) is a key treatment approach for advanced invasive liver cancer (infiltrative hepatocellular carcinoma). However, its therapeutic response can be difficult to evaluate accurately using conventional two-dimensional imaging criteria due to the tumor's diffuse and multifocal growth pattern. Volumetric imaging, especially enhanced tumor volume (ETV), offers a more comprehensive assessment. Nonetheless, bias field inhomogeneity in magnetic resonance imaging (MRI) poses challenges, potentially skewing volumetric measurements and undermining prognostic evaluation. AIM: To investigate whether MRI bias field correction enhances the accuracy of volumetric assessment of infiltrative hepatocellular carcinoma treated with TACE, and to analyze how this improved measurement impacts prognostic prediction. METHODS: We retrospectively collected data from 105 patients with invasive liver cancer who underwent TACE treatment at the Affiliated Hospital of Xuzhou Medical University from January 2020 to January 2024. The improved N4 bias field correction algorithm was applied to process MRI images, and the ETV before and after treatment was calculated. The ETV measurements before and after correction were compared, and their relationship with patient prognosis was analyzed. A Cox proportional hazards model was used to evaluate prognostic factors, with Martingale residual analysis determining the optimal cutoff value, followed by survival analysis. RESULTS: Bias field correction significantly affected ETV measurements, with the corrected baseline ETV mean (505.235 cm\u00b3) being significantly lower than before correction (825.632 cm\u00b3, P < 0.001). Cox analysis showed that the hazard ratio (HR) for corrected baseline ETV (HR = 1.165, 95%CI: 1.069-1.268) was higher than before correction (HR = 1.063, 95%CI: 1.031-1.095). Using 412 cm\u00b3 as the cutoff, the group with baseline ETV < 415 cm\u00b3 had a longer median survival time compared to the \u2265 415 cm\u00b3 group (18.523 months vs 8.926 months, P < 0.001). The group with an ETV reduction rate \u2265 41% had better prognosis than the < 41% group (17.862 months vs 9.235 months, P = 0.006). Multivariate analysis confirmed that ETV reduction rate (HR = 0.412, P < 0.001), Child-Pugh classification (HR = 0.298, P < 0.001), and Barcelona Clinic Liver Cancer stage (HR = 0.578, P = 0.045) were independent prognostic factors. CONCLUSION: Volume imaging based on MRI bias field correction can improve the accuracy of evaluating the efficacy of TACE treatment for invasive liver cancer. The corrected ETV and its reduction rate can serve as independent indicators for predicting patient prognosis, providing important reference for developing individualized treatment strategies.",
      "journal": "World journal of gastrointestinal surgery",
      "year": "2025",
      "doi": "10.4240/wjgs.v17.i4.104187",
      "authors": "Liu Ke et al.",
      "keywords": "Bias field correction; Invasive liver cancer; Magnetic resonance imaging; Transcatheter arterial chemoembolization; Volume imaging",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40291888/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Survival Analysis",
      "health_domain": "Radiology/Medical Imaging; Oncology; ICU/Critical Care; Pediatrics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "Hospital/Inpatient; ICU",
      "key_findings": "CONCLUSION: Volume imaging based on MRI bias field correction can improve the accuracy of evaluating the efficacy of TACE treatment for invasive liver cancer. The corrected ETV and its reduction rate can serve as independent indicators for predicting patient prognosis, providing important reference for developing individualized treatment strategies.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12019036"
    },
    {
      "pmid": "40312417",
      "title": "Building health systems capable of leveraging AI: applying Paul Farmer's 5S framework for equitable global health.",
      "abstract": "The development of artificial intelligence (AI) applications in healthcare is often positioned as a solution to the greatest challenges facing global health. Advocates propose that AI can bridge gaps in care delivery and access, improving healthcare quality and reducing inequity, including in resource-constrained settings. A broad base of critical scholarship has highlighted important issues with healthcare AI, including algorithmic bias and inequitable and inaccurate model outputs. While such criticisms are valid, there exists a much more fundamental challenge that is often overlooked in global health policy debates: the dangerous mismatch between AI's imagined benefits and the material realities of healthcare systems globally. AI cannot be deployed effectively or ethically in contexts lacking sufficient social and material infrastructure and resources to provide effective healthcare services. Continued investments in AI within unprepared, under-resourced contexts risk misallocating resources and potentially causing more harm than good. The article concludes by providing concrete questions to assess AI systemic capacity and socio-technical readiness in global health.",
      "journal": "BMC global and public health",
      "year": "2025",
      "doi": "10.1186/s44263-025-00158-6",
      "authors": "McCoy Liam G et al.",
      "keywords": "Artificial intelligence; Equity; Global health; Health; Health systems; Paul Farmer",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40312417/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Continued investments in AI within unprepared, under-resourced contexts risk misallocating resources and potentially causing more harm than good. The article concludes by providing concrete questions to assess AI systemic capacity and socio-technical readiness in global health.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12046908"
    },
    {
      "pmid": "40344545",
      "title": "Assessing Algorithmic Fairness With a Multimodal Artificial Intelligence Model in Men of African and Non-African Origin on NRG Oncology Prostate Cancer Phase III Trials.",
      "abstract": "PURPOSE: Artificial intelligence (AI) tools could improve clinical decision making or exacerbate inequities because of bias. African American (AA) men reportedly have a worse prognosis for prostate cancer (PCa) and are underrepresented in the development genomic biomarkers. We assess the generalizability of tools developed using a multimodal AI (MMAI) deep learning system using digital histopathology and clinical data from NRG/Radiation Therapy Oncology Group PCa trials across racial subgroups. METHODS: In total, 5,708 patients from five randomized phase III trials were included. Two MMAI algorithms were evaluated: (1) the distant metastasis (DM) MMAI model optimized to predict risk of DM, and (2) the PCa-specific mortality (PCSM) MMAI model optimized to focus on prediction death in the presence of DM (DDM). The prognostic performance of the MMAI algorithms was evaluated in AA and non-AA subgroups using time to DM (primary end point) and time to DDM (secondary end point). Exploratory end points included time to biochemical failure and overall survival with Fine-Gray or Cox proportional hazards models. Cumulative incidence estimates were computed for time-to-event end points and compared using Gray's test. RESULTS: There were 948 (16.6%) AA patients, 4,731 non-AA patients (82.9%), and 29 (0.5%) patients with unknown or missing race status. The DM-MMAI algorithm showed a strong prognostic signal for DM in the AA (subdistribution hazard ratio [sHR], 1.2 [95% CI, 1.0 to 1.3]; P = .007) and non-AA subgroups (sHR, 1.4 [95% CI, 1.3 to 1.5]; P < .001). Similarly, the PCSM-MMAI score showed a strong prognostic signal for DDM in both AA (sHR, 1.3 [95% CI, 1.1 to 1.5]; P = .001) and non-AA subgroups (sHR, 1.5 [95% CI, 1.4 to 1.6]; P < .001), with similar distributions of risk. CONCLUSION: Using cooperative group data sets with a racially diverse population, the MMAI algorithm performed well across racial subgroups without evidence of algorithmic bias.",
      "journal": "JCO clinical cancer informatics",
      "year": "2025",
      "doi": "10.1200/CCI-24-00284",
      "authors": "Roach Mack et al.",
      "keywords": "",
      "mesh_terms": "Aged; Humans; Male; Middle Aged; Algorithms; Artificial Intelligence; Black or African American; Clinical Trials, Phase III as Topic; Prognosis; Prostatic Neoplasms",
      "pub_types": "Clinical Trial, Phase III; Journal Article; Randomized Controlled Trial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40344545/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Oncology; Surgery; Pathology; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Laboratory/Pathology",
      "key_findings": "CONCLUSION: Using cooperative group data sets with a racially diverse population, the MMAI algorithm performed well across racial subgroups without evidence of algorithmic bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12335010"
    },
    {
      "pmid": "40359732",
      "title": "Impact of spectrum bias on deep learning-based stroke MRI analysis.",
      "abstract": "PURPOSE: To evaluate spectrum bias in stroke MRI analysis by excluding cases with uncertain acute ischemic lesions (AIL) and examining patient, imaging, and lesion factors associated with these cases. MATERIALS AND METHODS: This single-center retrospective observational study included adults with brain MRIs for suspected stroke between January 2020 and April 2022. Diagnostic uncertain AIL were identified through reader disagreement or low certainty grading by a radiology resident, a neuroradiologist, and the original radiology report consisting of various neuroradiologists. A commercially available deep learning tool analyzing brain MRIs for AIL was evaluated to assess the impact of excluding uncertain cases on diagnostic odds ratios. Patient-related, MRI acquisition-related, and lesion-related factors were analyzed using the Wilcoxon rank sum test, \u03c72 test, and multiple logistic regression. The study was approved by the National Committee on Health Research Ethics. RESULTS: In 989 patients (median age 73 (IQR: 59-80), 53% female), certain AIL were found in 374 (38%), uncertain AIL in 63 (6%), and no AIL in 552 (56%). Excluding uncertain cases led to a four-fold increase in the diagnostic odds ratio (from 68 to 278), while a simulated case-control design resulted in a six-fold increase compared to the full disease spectrum (from 68 to 431). Independent factors associated with uncertain AIL were MRI artifacts, smaller lesion size, older lesion age, and infratentorial location. CONCLUSION: Excluding uncertain cases leads to a four-fold overestimation of the diagnostic odds ratio. MRI artifacts, smaller lesion size, infratentorial location, and older lesion age are associated with uncertain AIL and should be accounted for in validation studies.",
      "journal": "European journal of radiology",
      "year": "2025",
      "doi": "10.1016/j.ejrad.2025.112161",
      "authors": "Krag Christian Hedeager et al.",
      "keywords": "AI; Deep learning; MRI; Spectrum Bias; Stroke",
      "mesh_terms": "Humans; Female; Male; Deep Learning; Magnetic Resonance Imaging; Aged; Middle Aged; Retrospective Studies; Aged, 80 and over; Stroke; Bias",
      "pub_types": "Journal Article; Observational Study; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40359732/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Deep Learning; Logistic Regression",
      "health_domain": "Radiology/Medical Imaging; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Excluding uncertain cases leads to a four-fold overestimation of the diagnostic odds ratio. MRI artifacts, smaller lesion size, infratentorial location, and older lesion age are associated with uncertain AIL and should be accounted for in validation studies.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40378901",
      "title": "Innovations in maternal-fetal dermatology: A focus on artificial intelligence-driven diagnostics, global disparities, and transformative care.",
      "abstract": "Pregnancy-associated dermatologic conditions emerge from intricate hormonal, immunologic, genetic, and environmental changes, often complicating maternal and neonatal outcomes. Such changes can also impact the course of preexisting skin disease during gestation. There are persistent gaps in clinical research, including the underrepresentation of pregnant patients, limited therapeutic options, and health disparities, that continue to challenge optimal care. Recent advances in artificial intelligence offer promising opportunities for early diagnosis, enhanced risk assessment and drug safety, and expanded care access, particularly in underserved populations. Cultural, demographic, and genetic factors critically shape dermatologic presentations and outcomes, whereas inequities in data representation, algorithmic bias, and health care accessibility remain significant barriers. By integrating interdisciplinary innovations with culturally sensitive strategies, we propose a framework to optimize and globalize maternal-fetal dermatologic care.",
      "journal": "Clinics in dermatology",
      "year": "2025",
      "doi": "10.1016/j.clindermatol.2025.05.013",
      "authors": "Kakish Diala Ra'Ed Kamal et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Pregnancy; Female; Skin Diseases; Dermatology; Healthcare Disparities; Pregnancy Complications; Health Services Accessibility",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40378901/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Dermatology; ICU/Critical Care; Pediatrics; Obstetrics/Maternal Health; Genomics/Genetics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population; Safety-Net/Underserved",
      "key_findings": "Cultural, demographic, and genetic factors critically shape dermatologic presentations and outcomes, whereas inequities in data representation, algorithmic bias, and health care accessibility remain significant barriers. By integrating interdisciplinary innovations with culturally sensitive strategies, we propose a framework to optimize and globalize maternal-fetal dermatologic care.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40379962",
      "title": "Computational challenges arising in algorithmic fairness and health equity with generative AI.",
      "abstract": "",
      "journal": "Nature computational science",
      "year": "2025",
      "doi": "10.1038/s43588-025-00806-9",
      "authors": "Suriyakumar Vinith M et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40379962/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40384063",
      "title": "Large Language Models in Medicine: Clinical Applications, Technical Challenges, and Ethical Considerations.",
      "abstract": "OBJECTIVES: This study presents a comprehensive review of the clinical applications, technical challenges, and ethical considerations associated with using large language models (LLMs) in medicine. METHODS: A literature survey of peer-reviewed articles, technical reports, and expert commentary from relevant medical and artificial intelligence journals was conducted. Key clinical application areas, technical limitations (e.g., accuracy, validation, transparency), and ethical issues (e.g., bias, safety, accountability, privacy) were identified and analyzed. RESULTS: LLMs have potential in clinical documentation assistance, decision support, patient communication, and workflow optimization. The level of supporting evidence varies; documentation support applications are relatively mature, whereas autonomous diagnostics continue to face notable limitations regarding accuracy and validation. Key technical challenges include model hallucination, lack of robust clinical validation, integration issues, and limited transparency. Ethical concerns involve algorithmic bias risking health inequities, threats to patient safety from inaccuracies, unclear accountability, data privacy, and impacts on clinician-patient interactions. CONCLUSIONS: LLMs possess transformative potential for clinical medicine, particularly by augmenting clinician capabilities. However, substantial technical and ethical hurdles necessitate rigorous research, validation, clearly defined guidelines, and human oversight. Existing evidence supports an assistive rather than autonomous role, mandating careful, evidence-based integration that prioritizes patient safety and equity.",
      "journal": "Healthcare informatics research",
      "year": "2025",
      "doi": "10.4258/hir.2025.31.2.114",
      "authors": "Jung Kyu-Hwan",
      "keywords": "Artificial Intelligence; Clinical Decision Support Systems; Medical Ethics; Medical Informatics Applications; Natural Language Processing",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40384063/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSIONS: LLMs possess transformative potential for clinical medicine, particularly by augmenting clinician capabilities. However, substantial technical and ethical hurdles necessitate rigorous research, validation, clearly defined guidelines, and human oversight. Existing evidence supports an assistive rather than autonomous role, mandating careful, evidence-based integration that prioritizes patient safety and equity.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 2 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12086438"
    },
    {
      "pmid": "40387721",
      "title": "Predictive models for low birth weight: a comparative analysis of algorithmic fairness-improving approaches.",
      "abstract": "OBJECTIVE: Evaluating whether common algorithmic fairness-improving approaches can improve low-birth-weight predictive model performance can provide important implications for population health management and health equity. This study aimed to evaluate alternative approaches for improving algorithmic fairness for low-birth-weight predictive models. STUDY DESIGN: Retrospective, cross-sectional study of birth certificates linked with medical insurance claims. METHODS: Birth certificates (n\u2009=\u2009191,943; 2014-2022) were linked with insurance claims (2013-2021) from the Arkansas All-Payer Claims Database to assess alternative approaches for algorithmic fairness in predictive models for low birth weight (<\u20092500 g). We fit an original model and compared 6 fairness-improving approaches using elastic net models trained and tested with 70/30 balanced random split samples and 10-fold cross validation. RESULTS: The original model had lower accuracy (percent predicted correctly) in predicting low birth weight among Black, Native Hawaiian/Other Pacific Islander, Asian, and unknown racial/ethnic populations relative to White individuals. For Black individuals, accuracy increased with all 6 fairness-improving approaches relative to the original model; however, sensitivity (true-positives correctly predicted as low birth weight) significantly declined, as much as 31% (from 0.824 to 0.565), in 5 of 6 approaches. CONCLUSIONS: When developing and implementing decision-making algorithms, it is critical that model performance metrics align with management goals for the predictive tool. In our study, fairness-improving models improved accuracy and area under the curve scores for Black individuals but decreased sensitivity and negative predictive value, suggesting that the original model, although unfair, was not improved. Implementation of unfair models for allocating preventive services could perpetuate racial/ethnic inequities by failing to identify individuals most at risk for a low-birth-weight delivery.",
      "journal": "The American journal of managed care",
      "year": "2025",
      "doi": "10.37765/ajmc.2025.89737",
      "authors": "Brown Clare C et al.",
      "keywords": "",
      "mesh_terms": "Humans; Retrospective Studies; Infant, Newborn; Cross-Sectional Studies; Female; Algorithms; Infant, Low Birth Weight; Male; Birth Certificates; Arkansas; Adult",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40387721/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Pediatrics; Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: When developing and implementing decision-making algorithms, it is critical that model performance metrics align with management goals for the predictive tool. In our study, fairness-improving models improved accuracy and area under the curve scores for Black individuals but decreased sensitivity and negative predictive value, suggesting that the original model, although unfair, was not improved. Implementation of unfair models for allocating preventive services could perpetuate rac...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12109546"
    },
    {
      "pmid": "40392092",
      "title": "Pitfalls and Best Practices in Evaluation of AI Algorithmic Biases in Radiology.",
      "abstract": "Despite growing awareness of problems with fairness in artificial intelligence (AI) models in radiology, evaluation of algorithmic biases, or AI biases, remains challenging due to various complexities. These include incomplete reporting of demographic information in medical imaging datasets, variability in definitions of demographic categories, and inconsistent statistical definitions of bias. To guide the appropriate evaluation of AI biases in radiology, this article summarizes the pitfalls in the evaluation and measurement of algorithmic biases. These pitfalls span the spectrum from the technical (eg, how different statistical definitions of bias impact conclusions about whether an AI model is biased) to those associated with social context (eg, how different conventions of race and ethnicity impact identification or masking of biases). Actionable best practices and future directions to avoid these pitfalls are summarized across three key areas: (a) medical imaging datasets, (b) demographic definitions, and (c) statistical evaluations of bias. Although AI bias in radiology has been broadly reviewed in the recent literature, this article focuses specifically on underrecognized potential pitfalls related to the three key areas. By providing awareness of these pitfalls along with actionable practices to avoid them, exciting AI technologies can be used in radiology for the good of all people.",
      "journal": "Radiology",
      "year": "2025",
      "doi": "10.1148/radiol.241674",
      "authors": "Yi Paul H et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Radiology; Algorithms; Bias; Diagnostic Imaging",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40392092/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Although AI bias in radiology has been broadly reviewed in the recent literature, this article focuses specifically on underrecognized potential pitfalls related to the three key areas. By providing awareness of these pitfalls along with actionable practices to avoid them, exciting AI technologies can be used in radiology for the good of all people.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12127964"
    },
    {
      "pmid": "40397823",
      "title": "International Section Intersectionality in Maternal Health : Gender, Labor, and Structural Barriers With a Focus on Korea.",
      "abstract": "Drawing on intersectional feminist theory, this review interrogates maternal health disparities through lenses of gender, labor, and structural inequality, with a particular focus on South Korea. It integrates global trends with local realities to examine how socioeconomic status, digital divides, and algorithmic bias in artificial intelligence systems could compound maternal vulnerability. By critically evaluating gender gaps in nursing research and leadership, this paper advocates for the adoption of intersectionality as a foundational framework in nursing science to redress inequities and promote inclusive health care innovation.",
      "journal": "ANS. Advances in nursing science",
      "year": "2025",
      "doi": "10.1097/ANS.0000000000000570",
      "authors": "Kim Jeung-Im et al.",
      "keywords": "artificial intelligence; gender equity; intersectional framework; maternal health; nurse; social vulnerability",
      "mesh_terms": "Humans; Republic of Korea; Female; Maternal Health; Male; Pregnancy; Adult; Healthcare Disparities; Feminism",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40397823/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Obstetrics/Maternal Health",
      "bias_axes": "Gender/Sex; Socioeconomic Status; Intersectional",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "It integrates global trends with local realities to examine how socioeconomic status, digital divides, and algorithmic bias in artificial intelligence systems could compound maternal vulnerability. By critically evaluating gender gaps in nursing research and leadership, this paper advocates for the adoption of intersectionality as a foundational framework in nursing science to redress inequities and promote inclusive health care innovation.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40440484",
      "title": "Impact of analytical bias on machine learning models for sepsis prediction using laboratory data.",
      "abstract": "OBJECTIVES: Machine learning (ML) models, using laboratory data, support early sepsis prediction. However, analytical bias in laboratory measurements can compromise their performance and validity in real-world settings. We aimed to evaluate how analytically acceptable bias may affect the validity and generalizability of ML models trained on laboratory\u00a0data. METHODS: A support vector machine model (SVM) for sepsis prediction was developed using complete blood count and erythrocyte sedimentation rate data from outpatients (CS, n=104) and patients from acute inflammatory status wards (SS, n=107). Twenty-six combinations were derived by white blood cells (WBC), platelets (PLT), and erythrocyte sedimentation rate (ESR) biases from analytical performance specifications (APS). The diagnostic performances of the 26 conditions tested were compared to the original dataset. RESULTS: SVM performance of the original dataset was AUC 90.6\u202f% [95\u202f%CI: 80.6-98.7\u202f%]. Minimum, desirable and optimum acceptable biases for WBC were 7.7\u202f, 5.1 and 2.6\u202f%, respectively, for PLT were 6.7\u202f, 4.5 and 2.2\u202f%, respectively and for ESR were 31.6\u202f, 21.1 and 10.5\u202f%, respectively. Across all conditions, AUC varied from 89.8\u202f% [95\u202f%CI: 79.0-97.7\u202f%] (for PLT bias\u00a0-6.7\u202f%), to 89.5\u202f% [95\u202f%CI: 79.1-98.0\u202f%] (for ESR Bias\u00a0+31.6\u202f%) to 90.4\u202f% [95\u202f%CI: 79.3-98.4\u202f%] (for WBC Bias\u00a0-5.1\u202f%). Using a combination of biases, the lowest AUC was 87.8\u202f% [95\u202f%CI: 75.9-96.6\u202f%]. No statistically significant differences were observed for AUC (p>0.05). CONCLUSIONS: Bias can influence model performance depending on the parameters and their combinations. Developing new validation strategies to assess the impact of analytical bias on laboratory data in ML models could improve their reliability.",
      "journal": "Clinical chemistry and laboratory medicine",
      "year": "2025",
      "doi": "10.1515/cclm-2025-0491",
      "authors": "Yesil Meryem Rumeysa et al.",
      "keywords": "analytical bias; artificial intelligence; machine learning; model performance; sepsis",
      "mesh_terms": "Humans; Sepsis; Machine Learning; Blood Sedimentation; Female; Male; Support Vector Machine; Middle Aged; Aged; Leukocyte Count; Bias; Blood Cell Count",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40440484/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Support Vector Machine",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Primary Care/Outpatient; Laboratory/Pathology",
      "key_findings": "CONCLUSIONS: Bias can influence model performance depending on the parameters and their combinations. Developing new validation strategies to assess the impact of analytical bias on laboratory data in ML models could improve their reliability.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40444224",
      "title": "The effectiveness, equity and explainability of health service resource allocation-with applications in kidney transplantation & family planning.",
      "abstract": "INTRODUCTION: Halfway to the deadline of the 2030 agenda, humankind continues to face long-standing yet urgent policy and management challenges to address resource shortages and deliver on Sustainable Development Goal 3; health and well-being for all at all ages. More than half of the global population lacks access to essential health services. Additional resources are required and need to be allocated effectively and equitably. Resource allocation models, however, have struggled to accurately predict effects and to present optimal allocations, thus hampering effectiveness and equity improvement. The current advances in machine learning present opportunities to better predict allocation effects and to prescribe solutions that better balance effectiveness and equity. The most advanced of these models tend to be \"black box\" models that lack explainability. This lack of explainability is problematic as it can clash with professional values and hide biases that negatively impact effectiveness and equity. METHODS: Through a novel theoretical framework and two diverse case studies, this manuscript explores the trade-offs between effectiveness, equity, and explainability. The case studies consider family planning in a low income country and kidney allocation in a high income country. RESULTS: Both case studies find that the least explainable models hardly offer improvements in effectiveness and equity over explainable alternatives. DISCUSSION: As this may more widely apply to health resource allocation decisions, explainable analytics, which are more likely to be trusted and used, might better enable progress towards SDG3 for now. Future research on explainability, also in relation to equity and fairness of allocation policies, can help deliver on the promise of advanced predictive and prescriptive analytics.",
      "journal": "Frontiers in health services",
      "year": "2025",
      "doi": "10.3389/frhs.2025.1545864",
      "authors": "van de Klundert Joris et al.",
      "keywords": "effectiveness; equity; explainability; explainable AI; family planning; healthcare analytics; kidney allocation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40444224/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Nephrology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "RESULTS: Both case studies find that the least explainable models hardly offer improvements in effectiveness and equity over explainable alternatives. DISCUSSION: As this may more widely apply to health resource allocation decisions, explainable analytics, which are more likely to be trusted and used, might better enable progress towards SDG3 for now. Future research on explainability, also in relation to equity and fairness of allocation policies, can help deliver on the promise of advanced pre...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12119484"
    },
    {
      "pmid": "40445911",
      "title": "Bias in Artificial Intelligence: Impact on Breast Imaging.",
      "abstract": "Artificial intelligence (AI) in breast imaging has garnered significant attention given the numerous reports of improved efficiency, accuracy, and the potential to bridge the gap of expanded volume in the face of limited physician resources. While AI models are developed with specific data points, on specific equipment, and in specific populations, the real-world clinical environment is dynamic, and patient populations are diverse, which can impact generalizability and widespread adoption of AI in clinical practice. Implementation of AI models into clinical practice requires focused attention on the potential of AI bias impacting outcomes. The following review presents the concept, sources, and types of AI bias to be considered when implementing AI models and offers suggestions on strategies to mitigate AI bias in practice.",
      "journal": "Journal of breast imaging",
      "year": "2025",
      "doi": "10.1093/jbi/wbaf027",
      "authors": "Net Jose M et al.",
      "keywords": "artificial intelligence; bias; breast imaging; radiology",
      "mesh_terms": "Humans; Artificial Intelligence; Female; Breast Neoplasms; Mammography; Bias; Breast",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40445911/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Oncology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Implementation of AI models into clinical practice requires focused attention on the potential of AI bias impacting outcomes. The following review presents the concept, sources, and types of AI bias to be considered when implementing AI models and offers suggestions on strategies to mitigate AI bias in practice.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40475298",
      "title": "Global trends in the use of artificial intelligence for urological tumor histopathology: A 20-year bibliometric analysis.",
      "abstract": "BACKGROUND: The field of urological tumor histopathology has long relied on subjective pathologist expertise, leading to diagnostic variability. Recent advances in digital pathology and artificial intelligence (AI) offer transformative potential by standardizing diagnoses, improving accuracy, and bridging healthcare disparities. This study conducted a 20-year bibliometric analysis to map global research trends and innovations in AI-driven urological pathology. METHODS: For this bibliometric analysis, literature from 2004 to 2024 was retrieved from the Web of Science Core Collection. CiteSpace, VOSviewer, and Microsoft Excel were used to visualize coauthorship, cocitation, and co-occurrence analyses of countries/regions, institutions, authors, references, and keywords in the field of AI for urological tumor histopathology. RESULTS: A total of 199 papers were included. Research on AI-driven urological tumor pathology has steadily increased since 2005, with a significant surge between 2020 and 2023. The United States made the largest contribution in terms of publications (131), citations (4725), and collaborations. The most productive institution was the University of Southern California, while Patel et al. and Epstein et al. were identified as the most active and most cocited authors, respectively. European Urology led in both publication volume and impact. Keyword analysis identified \"machine learning,\" \"prostate cancer,\" \"deep learning,\" and \"diagnosis\" as major research foci. CONCLUSIONS: The integration of AI into urological tumor pathology demonstrates transformative potential, significantly enhancing diagnostic accuracy and efficiency through automated analysis of whole-slide imaging and Gleason grading, comparable to pathologist-level performance. However, clinical translation encounters critical challenges, including data bias, model interpretability (\"black-box\" limitations), and regulatory-ethical complexities. Future advancements hinge on developing explainable AI frameworks, multimodal systems integrating histopathology, radiomics, and genomics and establishing global collaborative networks to address resource disparities. Prioritizing standardized data protocols, fairness-aware algorithms, and dynamic regulatory guidelines will be essential to ensure equitable, reliable, and clinically actionable AI solutions, ultimately advancing precision oncology in urological malignancies.",
      "journal": "Digital health",
      "year": "2025",
      "doi": "10.1177/20552076251348834",
      "authors": "Dai Fazhong et al.",
      "keywords": "Bibliometric analysis; CiteSpace; VOSviewer; artificial intelligence; pathology; urological tumors",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40475298/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Oncology; Pathology; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "CONCLUSIONS: The integration of AI into urological tumor pathology demonstrates transformative potential, significantly enhancing diagnostic accuracy and efficiency through automated analysis of whole-slide imaging and Gleason grading, comparable to pathologist-level performance. However, clinical translation encounters critical challenges, including data bias, model interpretability (\"black-box\" limitations), and regulatory-ethical complexities. Future advancements hinge on developing explainab...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12138227"
    },
    {
      "pmid": "40476708",
      "title": "Revolutionizing Stroke Management with Wearable Technology: A Narrative Review of Clinical Applications, Opportunities, Challenges, and the Future Path Towards Equitable Implementation.",
      "abstract": "Stroke remains a leading global cause of mortality and disability, necessitating innovative approaches to address gaps in prevention, acute care, and rehabilitation. This narrative review synthesizes evidence on the transformative potential of wearable technology (WT) across the stroke care continuum and identifies critical challenges to its equitable implementation. A systematic search of PubMed/Medical Literature Analysis and Retrieval System Online (MEDLINE), Scopus, and Google Scholar (up to December 2024) identified 50 studies following Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA)-guided screening and thematic analysis. In prevention, WT facilitates primordial strategies through physical activity tracking and sleep monitoring, while in primary prevention, it enables continuous management of hypertension, atrial fibrillation (AF), diabetes, and sleep apnea. Large-scale trials, such as the Apple Heart Study, validate WT's efficacy in AF detection (84% positive predictive value), though confirmatory medical-grade testing remains essential. For acute care, WT demonstrates promise in early stroke detection through accelerometer-based algorithms and remote neurological assessments. Post-stroke rehabilitation strategies utilize wearable sensors, robotics, and virtual reality to enhance motor recovery, with exoskeletons improving gait speed and sensorimotor feedback increasing upper limb function. Despite these advances, critical barriers persist: fragmented evidence from heterogeneous studies, scarce randomized controlled trials evaluating long-term outcomes, and ethical concerns regarding data privacy and algorithmic bias. Socioeconomic disparities further limit access, particularly in low-resource settings where 70% of stroke deaths occur. To realize WT's potential, stakeholders must prioritize pragmatic trials, standardized protocols, affordable designs, and policies aligning innovation with equity. This review underscores WT's role in global stroke care and advocates for collaborative, patient-centered strategies to bridge gaps.",
      "journal": "Annals of Indian Academy of Neurology",
      "year": "2025",
      "doi": "10.4103/aian.aian_156_25",
      "authors": "Vaishnav Anand et al.",
      "keywords": "Stroke; prevention; rehabilitation; technology; wearables",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40476708/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; Emergency Medicine; ICU/Critical Care; Neurology; Endocrinology/Diabetes; Wearables/Remote Monitoring",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Disability; Geographic",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Telehealth/Remote; Clinical Trial; Safety-Net/Underserved",
      "key_findings": "To realize WT's potential, stakeholders must prioritize pragmatic trials, standardized protocols, affordable designs, and policies aligning innovation with equity. This review underscores WT's role in global stroke care and advocates for collaborative, patient-centered strategies to bridge gaps.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12610968"
    },
    {
      "pmid": "40494422",
      "title": "Tailoring task arithmetic to address bias in models trained on multi-institutional datasets.",
      "abstract": "OBJECTIVE: Multi-institutional datasets are widely used for machine learning from clinical data, to increase dataset size and improve generalization. However, deep learning models in particular may learn to recognize the source of a data element, leading to biased predictions. For example, deep learning models for image recognition trained on chest radiographs with COVID-19 positive and negative examples drawn from different data sources can respond to indicators of provenance (e.g., radiological annotations outside the lung area per institution-specific practices) rather than pathology, generalizing poorly beyond their training data. Bias of this sort, called confounding by provenance, is of concern in natural language processing (NLP) because provenance indicators (e.g., institution-specific section headers, or region-specific dialects) are pervasive in language data. Prior work on addressing such bias has focused on statistical methods, without providing a solution for deep learning models for NLP. METHODS: Recent work in representation learning has shown that representing the weights of a trained deep network as task vectors allows for their arithmetic composition to govern model capabilities towards desired behaviors. In this work, we evaluate the extent to which reducing a model's ability to distinguish between contributing sites with such task arithmetic can mitigate confounding by provenance. To do so, we propose two model-agnostic methods, Task Arithmetic for Provenance Effect Reduction (TAPER) and Dominance-Aligned Polarized Provenance Effect Reduction (DAPPER), extending the task vectors approach to a novel problem domain. RESULTS: Evaluation on three datasets shows improved robustness to confounding by provenance for both RoBERTa and Llama-2 models with the task vector approach, with improved performance at the extremes of distribution shift. CONCLUSION: This work emphasizes the importance of adjusting for confounding by provenance, especially in extreme cases of the shift. In use of deep learning models, DAPPER and TAPER show efficiency in mitigating such bias. They provide a novel mitigation strategy for confounding by provenance, with broad applicability to address other sources of bias in composite clinical data sets. Source code is available within the DeconDTN toolkit: https://github.com/LinguisticAnomalies/DeconDTN-toolkit.",
      "journal": "Journal of biomedical informatics",
      "year": "2025",
      "doi": "10.1016/j.jbi.2025.104858",
      "authors": "Ding Xiruo et al.",
      "keywords": "Confounding shift; Large language models; Low-rank adaptation (LoRA); Multi-institutional datasets; Robustness; Task arithmetic",
      "mesh_terms": "Humans; COVID-19; Natural Language Processing; Deep Learning; SARS-CoV-2; Bias; Machine Learning; Datasets as Topic; Databases, Factual",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40494422/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; NLP/LLM",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care; Pathology; Pulmonology",
      "bias_axes": "Gender/Sex; Age; Language; Geographic",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Representation Learning",
      "clinical_setting": "ICU; Laboratory/Pathology",
      "key_findings": "CONCLUSION: This work emphasizes the importance of adjusting for confounding by provenance, especially in extreme cases of the shift. In use of deep learning models, DAPPER and TAPER show efficiency in mitigating such bias. They provide a novel mitigation strategy for confounding by provenance, with broad applicability to address other sources of bias in composite clinical data sets.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12282651"
    },
    {
      "pmid": "40501296",
      "title": "Artificial Intelligence in Aesthetic Medicine: Applications, Challenges, and Future Directions.",
      "abstract": "BACKGROUND: Artificial Intelligence (AI) is transforming healthcare by enhancing diagnostics, treatment personalization, and operational efficiency. In aesthetic medicine-a field blending medical expertise with artistic judgment-AI is increasingly being used to improve precision, optimize treatment outcomes, and personalize patient care. However, its integration presents both opportunities and ethical challenges, necessitating a critical evaluation of its role in this evolving field. OBJECTIVE: This study examines AI's applications in aesthetic medicine, focusing on its role in facial analysis, robotic-assisted procedures, predictive patient outcome modeling, and personalized treatment planning. Additionally, it explores ethical concerns, algorithmic biases, data privacy issues, and regulatory challenges affecting AI adoption in aesthetic practices. METHODS: A comprehensive review of AI-driven technologies in aesthetic medicine was conducted, analyzing literature on machine learning (ML), deep learning, and computer vision applications. Case studies on AI-assisted facial symmetry analysis, robotic hair transplantation, and predictive analytics in patient care were examined to evaluate AI's effectiveness and limitations. RESULTS: AI enhances aesthetic procedures by improving diagnostic accuracy, offering virtual simulations of treatment outcomes, and enabling hyper-personalized treatment plans based on patient data. AI-driven chatbots and virtual assistants streamline patient interactions, while robotic systems assist in precision-based tasks such as laser treatments and hair restoration. However, challenges such as biased training data, lack of transparency in AI decision-making, and inconsistencies in regulatory approvals hinder widespread adoption. The integration of AI in aesthetic medicine presents a paradigm shift from traditional approaches to data-driven, personalized interventions. However, ethical concerns such as data privacy, informed consent, and algorithmic fairness must be addressed. Overreliance on AI may diminish the human-centric approach essential in aesthetic procedures, where patient expectations and subjective perceptions of beauty play a crucial role. Collaboration between technologists, clinicians, and policymakers is necessary to develop standardized AI guidelines that ensure fairness, safety, and efficacy. CONCLUSION: AI has the potential to revolutionize aesthetic medicine by improving precision, efficiency, and patient satisfaction. However, its successful implementation requires balancing technological advancements with ethical considerations and regulatory frameworks. Future research should focus on integrating AI with emerging technologies such as augmented reality (AR) and genomic-based personalization to enhance aesthetic outcomes while maintaining transparency and patient trust.",
      "journal": "Journal of cosmetic dermatology",
      "year": "2025",
      "doi": "10.1111/jocd.70241",
      "authors": "Al-Dhubaibi Mohammed Saleh et al.",
      "keywords": "aesthetic medicine; artificial intelligence; machine learning",
      "mesh_terms": "Humans; Artificial Intelligence; Cosmetic Techniques; Precision Medicine; Esthetics",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40501296/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Deep Learning; Computer Vision/Imaging AI",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: AI has the potential to revolutionize aesthetic medicine by improving precision, efficiency, and patient satisfaction. However, its successful implementation requires balancing technological advancements with ethical considerations and regulatory frameworks. Future research should focus on integrating AI with emerging technologies such as augmented reality (AR) and genomic-based personalization to enhance aesthetic outcomes while maintaining transparency and patient trust.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12159716"
    },
    {
      "pmid": "40502243",
      "title": "A Generalized Tool to Assess Algorithmic Fairness in Disease Phenotype Definitions.",
      "abstract": "For evidence from observational studies to be reliable, researchers must ensure that the patient populations of interest are accurately defined. However, disease definitions can be extremely difficult to standardize and implement accurately across different datasets and study requirements. Furthermore, in this context, they must also ensure that populations are represented fairly to accurately reflect populations' various demographic dynamics and to not overgeneralize across non-applicable populations. In this work, we present a generalized tool to assess the fairness of disease definitions by evaluating their implementation across common fairness metrics. Our approach calculates fairness metrics and provides a robust method to examine coarse and strongly intersecting populations across many characteristics. We highlight workflows when working with disease definitions, provide an example analysis using an OMOP CDM patient database, and discuss potential directions for future improvement and research.",
      "journal": "AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science",
      "year": "2025",
      "doi": "",
      "authors": "Zelko Jacob S et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40502243/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "Our approach calculates fairness metrics and provides a robust method to examine coarse and strongly intersecting populations across many characteristics. We highlight workflows when working with disease definitions, provide an example analysis using an OMOP CDM patient database, and discuss potential directions for future improvement and research.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12150753"
    },
    {
      "pmid": "40517328",
      "title": "FairICP: identifying biases and increasing transparency at the point of care in post-implementation clinical decision support using inductive conformal prediction.",
      "abstract": "OBJECTIVES: Fairness concerns stemming from known and unknown biases in healthcare practices have raised questions about the trustworthiness of Artificial Intelligence (AI)-driven Clinical Decision Support Systems (CDSS). Studies have shown unforeseen performance disparities in subpopulations when applied to clinical settings different from training. Existing unfairness mitigation strategies often struggle with scalability and accessibility, while their pursuit of group-level prediction performance parity does not effectively translate into fairness at the point of care. This study introduces FairICP, a flexible and cost-effective post-implementation framework based on Inductive Conformal Prediction (ICP), to provide users with actionable knowledge of model uncertainty due to subpopulation level biases at the point of care. MATERIALS AND METHODS: FairICP applies ICP to identify the model's scope of competence through group specific calibration, ensuring equitable prediction reliability by filtering predictions that fall within the trusted competence boundaries. We evaluated FairICP against four benchmarks on three medical imaging modalities: (1) Cardiac Magnetic Resonance Imaging (MRI), (2) Chest X-ray and (3) Dermatology Imaging, acquired from both private and large public datasets. Frameworks are assessed on prediction performance enhancement and unfairness mitigation capabilities. RESULTS: Compared to the baseline, FairICP improved prediction accuracy by 7.2% and reduced the accuracy gap between the privileged and unprivileged subpopulations by 2.2% on average across all three datasets. DISCUSSION AND CONCLUSION: Our work provides a robust solution to promote trust and transparency in AI-CDSS, fostering equality and equity in healthcare for diverse patient populations. Such post-process methods are critical to enabling a robust framework for AI-CDSS implementation and monitoring for healthcare settings.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2025",
      "doi": "10.1093/jamia/ocaf095",
      "authors": "Sun Xiaotan et al.",
      "keywords": "artificial intelligence; clinical decision support; fairness; model uncertainty",
      "mesh_terms": "Humans; Decision Support Systems, Clinical; Artificial Intelligence; Point-of-Care Systems; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40517328/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Computer Vision/Imaging AI; Clinical Decision Support",
      "health_domain": "Radiology/Medical Imaging; Dermatology; Cardiology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Subgroup Analysis",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: Our work provides a robust solution to promote trust and transparency in AI-CDSS, fostering equality and equity in healthcare for diverse patient populations. Such post-process methods are critical to enabling a robust framework for AI-CDSS implementation and monitoring for healthcare settings.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12277691"
    },
    {
      "pmid": "40550353",
      "title": "Dental services use prediction among adults in Southern Brazil: A gender and racial fairness-oriented machine learning approach.",
      "abstract": "OBJECTIVE: To develop machine learning models to predict the use of dental services among adults aged 18 and older. METHODS: This is a prospective cohort study that uses data from the survey \"EAI Pelotas?\". The sample consisted of individuals who participated in both the baseline and follow-up, totaling 3461 people. Predictors were collected as baseline and comprised 47 sociodemographic, behavioral, oral and general health characteristics. The outcome was dental service use in the last year assessed during the one-year follow-up. Data was divided into training (80 %) and test (20 %) sets. Five machine learning models were tested. Hyperparameter tuning was optimized through 10-fold cross-validation, utilizing 30 iterations. Model performance was assessed based on the area under the Receiver Operating Characteristic (ROC) curve (AUC), accuracy, recall, precision, and F1-score. RESULTS: The prevalence of dental service use in the follow-up was 47.2 % (95 % CI, 45.5 - 48.9). All models in the test set demonstrated an AUC-ROC between 0.76 and 0.77. The CatBoost Classifier model exhibited the highest performance in the test dataset among the models concerning the AUC metric (AUC = 0.77, CI95 %,[0.73-0.80]), displaying an accuracy = 0.69, recall = 0.69, precision = 0.68, and F1-score = 0.69. Fairness estimations for the best model indicated consistent performance across gender categories. However, disparities were observed among racial groups, AUC = 0.57 for individuals who self-reported mixed (\"pardos\") skin color. The explainability analysis shows that the most important features were the last dental visit at baseline and education level. CONCLUSION: Despite our findings suggesting a sufficient prediction of overall dental services' use, performance varied across racial groups. CLINICAL SIGNIFICANCE: Our findings highlight the potential of machine learning models to predict dental service use with good overall accuracy. However, the significantly lower performance for mixed-race individuals raises concerns about fairness and equity. Therefore, despite promising results, the model requires further refinement before it can be applied in real-world public health settings.",
      "journal": "Journal of dentistry",
      "year": "2025",
      "doi": "10.1016/j.jdent.2025.105929",
      "authors": "Chisini Luiz Alexandre et al.",
      "keywords": "Artificial intelligence; Dental health services; Longitudinal study; Machine learning; Oral health",
      "mesh_terms": "Humans; Adult; Female; Male; Machine Learning; Prospective Studies; Middle Aged; Brazil; Sex Factors; Young Adult; Dental Health Services; Adolescent; Aged; Dental Care; ROC Curve",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40550353/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "XGBoost/Gradient Boosting",
      "health_domain": "Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: Despite our findings suggesting a sufficient prediction of overall dental services' use, performance varied across racial groups. CLINICAL SIGNIFICANCE: Our findings highlight the potential of machine learning models to predict dental service use with good overall accuracy. However, the significantly lower performance for mixed-race individuals raises concerns about fairness and equity.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40553457",
      "title": "Forewarning Artificial Intelligence about Cognitive Biases.",
      "abstract": "Artificial intelligence models display human-like cognitive biases when generating medical recommendations. We tested whether an explicit forewarning, \"Please keep in mind cognitive biases and other pitfalls of reasoning,\" might mitigate biases in OpenAI's generative pretrained transformer large language model. We used 10 clinically nuanced cases to test specific biases with and without a forewarning. Responses from the forewarning group were 50% longer and discussed cognitive biases more than 100 times more frequently compared with responses from the control group. Despite these differences, the forewarning decreased overall bias by only 6.9%, and no bias was extinguished completely. These findings highlight the need for clinician vigilance when interpreting generated responses that might appear seemingly thoughtful and deliberate.HighlightsArtificial intelligence models can be warned to avoid racial and gender bias.Forewarning artificial intelligence models to avoid cognitive biases does not adequately mitigate multiple pitfalls of reasoning.Critical reasoning remains an important clinical skill for practicing physicians.",
      "journal": "Medical decision making : an international journal of the Society for Medical Decision Making",
      "year": "2025",
      "doi": "10.1177/0272989X251346788",
      "authors": "Wang Jonathan et al.",
      "keywords": "algorithm bias; artificial intelligence; bias in medicine; clinical decision making; cognitive psychology; large language model",
      "mesh_terms": "Humans; Artificial Intelligence; Cognition; Female; Male; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40553457/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Despite these differences, the forewarning decreased overall bias by only 6.9%, and no bias was extinguished completely. These findings highlight the need for clinician vigilance when interpreting generated responses that might appear seemingly thoughtful and deliberate.HighlightsArtificial intelligence models can be warned to avoid racial and gender bias.Forewarning artificial intelligence models to avoid cognitive biases does not adequately mitigate multiple pitfalls of reasoning.Critical reas...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12413502"
    },
    {
      "pmid": "40558623",
      "title": "From Data to Decisions: Leveraging Retrieval-Augmented Generation to Balance Citation Bias in Burn Management Literature.",
      "abstract": "(1) Burn injuries demand multidisciplinary, evidence-based care, yet the extensive literature complicates timely decision making. Retrieval-augmented generation (RAG) synthesizes research while addressing inaccuracies in pretrained models. However, citation bias in sourcing for RAG often prioritizes highly cited studies, overlooking less-cited but valuable research. This study examines RAG's performance in burn management, comparing citation levels to enhance evidence synthesis, reduce selection bias, and guide decisions. (2) Two burn management datasets were assembled: 30 highly cited (mean: 303) and 30 less-cited (mean: 21). The Gemini-1.0-Pro-002 RAG model addressed 30 questions, ranging from foundational principles to advanced surgical approaches. Responses were evaluated for accuracy (5-point scale), readability (Flesch-Kincaid metrics), and response time with Wilcoxon rank sum tests (p < 0.05). (3) RAG achieved comparable accuracy (4.6 vs. 4.2, p = 0.49), readability (Flesch Reading Ease: 42.8 vs. 46.5, p = 0.26; Grade Level: 9.9 vs. 9.5, p = 0.29), and response time (2.8 vs. 2.5 s, p = 0.39) for the highly and less-cited datasets. (4) Less-cited research performed similarly to highly cited sources. This equivalence broadens clinicians' access to novel, diverse insights without sacrificing quality. As plastic surgery evolves, RAG's inclusive approach fosters innovation, improves patient care, and reduces cognitive burden by integrating underutilized studies. Embracing RAG could propel the field toward dynamic, forward-thinking care.",
      "journal": "European burn journal",
      "year": "2025",
      "doi": "10.3390/ebj6020028",
      "authors": "Genovese Ariana et al.",
      "keywords": "AI (artificial intelligence); RAG (retrieval-augmented generation); burn; clinical decision support; large language model; plastic surgery",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40558623/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Surgery",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "As plastic surgery evolves, RAG's inclusive approach fosters innovation, improves patient care, and reduces cognitive burden by integrating underutilized studies. Embracing RAG could propel the field toward dynamic, forward-thinking care.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12191722"
    },
    {
      "pmid": "40564882",
      "title": "Precision Medicine in Lung Cancer Screening: A Paradigm Shift in Early Detection-Precision Screening for Lung Cancer.",
      "abstract": "Lung cancer remains the leading cause of cancer-related mortality globally, largely due to late-stage diagnoses. While low-dose computed tomography (LDCT) has improved early detection and reduced mortality in high-risk populations, traditional screening strategies often adopt a one-size-fits-all approach based primarily on age and smoking history. This can lead to limitations, such as overdiagnosis, false positives, and the underrepresentation of non-smokers, which are especially prevalent in Asian populations. Precision medicine offers a transformative solution by tailoring screening protocols to individual risk profiles through the integration of clinical, genetic, environmental, and radiological data. Emerging tools, such as risk prediction models, radiomics, artificial intelligence (AI), and liquid biopsies, enhance the accuracy of screening, allowing for the identification of high-risk individuals who may not meet conventional criteria. Polygenic risk scores (PRSs) and molecular biomarkers further refine stratification, enabling more personalized and effective screening intervals. Incorporating these innovations into clinical workflows, alongside shared decision-making (SDM) and robust data infrastructure, represents a paradigm shift in lung cancer prevention. However, implementation must also address challenges related to health equity, algorithmic bias, and system integration. As precision medicine continues to evolve, it holds the promise of optimizing early detection, minimizing harm, and extending the benefits of lung cancer screening to broader and more diverse populations. This review explores the current landscape and future directions of precision medicine in lung cancer screening, emphasizing the need for interdisciplinary collaboration and population-specific strategies to realize its full potential in reducing the global burden of lung cancer.",
      "journal": "Diagnostics (Basel, Switzerland)",
      "year": "2025",
      "doi": "10.3390/diagnostics15121562",
      "authors": "Chen Hsin-Hung et al.",
      "keywords": "lung cancer screening; precision medicine; prediction model",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40564882/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Oncology; Pulmonology; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "As precision medicine continues to evolve, it holds the promise of optimizing early detection, minimizing harm, and extending the benefits of lung cancer screening to broader and more diverse populations. This review explores the current landscape and future directions of precision medicine in lung cancer screening, emphasizing the need for interdisciplinary collaboration and population-specific strategies to realize its full potential in reducing the global burden of lung cancer.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12192117"
    },
    {
      "pmid": "40578092",
      "title": "Evaluation of nursing students' ethical decision-making biases and attitudes toward artificial intelligence in nursing education.",
      "abstract": "AIM: This study examines nursing students' attitudes toward artificial intelligence (AI) and their association with biases in ethical decision-making processes. BACKGROUND: AI technologies are central to healthcare, particularly clinical decision support systems and simulations. While AI accelerates decision-making processes, it also brings ethical responsibilities. DESIGN: This is a descriptive cross-sectional study. METHOD: 265 nursing students were selected through stratified sampling from two universities in Turkey. Data were collected via an online survey using a demographic information form, the AI Attitude Scale (GAAIS) and the Ethical Decision-Making Bias Scale (EDBS). RESULTS: Most participants were female (n\u202f=\u202f223), with an average age of 20.45 (SD 1.67) years. The results of the GAAIS revealed that students generally had a positive attitude (3.38\u202fSD 0.47). However, 36.2\u202f% expressed distrust toward AI students who trusted AI more successfully to solve ethical issues and used AI tools more effectively. The average score on the EDBS was 2.48\u202fSD 0.41. Additionally, students who encountered ethical decisions more frequently (2.30\u202fSD 0.32) showed lower bias levels than those who experienced them less often (2.50SD0.44). Positive attitudes toward AI were positively associated with students' confidence in ethical decision-making (p\u202f<\u202f0.05). Distrust in AI and difficulty accessing accurate information were identified as significant barriers. CONCLUSIONS: Attitudes toward AI significantly influence students' biases in ethical decision-making processes. The nursing curriculum should include AI ethics, critical thinking and decision-making skills. Integrating ethical decision-making in AI usage within nursing education can ensure that future nurses can provide patient-centered care while maintaining ethical values.",
      "journal": "Nurse education in practice",
      "year": "2025",
      "doi": "10.1016/j.nepr.2025.104432",
      "authors": "Sengul Tuba et al.",
      "keywords": "Artificial Intelligence; Bias; Chat-GPT; Ethical Decision-Making; Nursing Education; Nursing Students",
      "mesh_terms": "Humans; Students, Nursing; Cross-Sectional Studies; Female; Turkey; Artificial Intelligence; Male; Surveys and Questionnaires; Decision Making; Attitude of Health Personnel; Young Adult; Education, Nursing, Baccalaureate; Adult; Ethics, Nursing",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40578092/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Clinical Decision Support",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSIONS: Attitudes toward AI significantly influence students' biases in ethical decision-making processes. The nursing curriculum should include AI ethics, critical thinking and decision-making skills. Integrating ethical decision-making in AI usage within nursing education can ensure that future nurses can provide patient-centered care while maintaining ethical values.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40587474",
      "title": "Racial disparities in continuous glucose monitoring-based 60-min glucose predictions among people with type 1 diabetes.",
      "abstract": "Non-Hispanic white (White) populations are overrepresented in medical studies. Potential healthcare disparities can happen when machine learning models, used in diabetes technologies, are trained on data from primarily White patients. We aimed to evaluate algorithmic fairness in glucose predictions. This study utilized continuous glucose monitoring (CGM) data from 101 White and 104 Black participants with type 1 diabetes collected by the JAEB Center for Health Research, US. Long short-term memory (LSTM) deep learning models were trained on 11 datasets of different proportions of White and Black participants and tailored to each individual using transfer learning to predict glucose 60 minutes ahead based on 60-minute windows. Root mean squared errors (RMSE) were calculated for each participant. Linear mixed-effect models were used to investigate the association between racial composition and RMSE while accounting for age, sex, and training data size. A median of 9 weeks (IQR: 7, 10) of CGM data was available per participant. The divergence in performance (RMSE slope by proportion) was not statistically significant for either group. However, the slope difference (from 0% White and 100% Black to 100% White and 0% Black) between groups was statistically significant (p\u2009=\u20090.02), meaning the RMSE increased 0.04 [0.01, 0.08] mmol/L more for Black participants compared to White participants when the proportion of White participants increased from 0 to 100% in the training data. This difference was attenuated in the transfer learned models (RMSE: 0.02 [-0.01, 0.05] mmol/L, p\u2009=\u20090.20). The racial composition of training data created a small statistically significant difference in the performance of the models, which was not present after using transfer learning. This demonstrates the importance of diversity in datasets and the potential value of transfer learning for developing more fair prediction models.",
      "journal": "PLOS digital health",
      "year": "2025",
      "doi": "10.1371/journal.pdig.0000918",
      "authors": "Thomsen Helene Bei et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40587474/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Clinical Prediction Model",
      "health_domain": "Endocrinology/Diabetes",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Transfer Learning",
      "clinical_setting": "Public Health/Population",
      "key_findings": "The racial composition of training data created a small statistically significant difference in the performance of the models, which was not present after using transfer learning. This demonstrates the importance of diversity in datasets and the potential value of transfer learning for developing more fair prediction models.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12208448"
    },
    {
      "pmid": "40598240",
      "title": "Implicit bias in ICU electronic health record data: measurement frequencies and missing data rates of clinical variables.",
      "abstract": "BACKGROUND: Systematic disparities in data collection within electronic health records (EHRs), defined as non-random patterns in the measurement and recording of clinical variables across demographic groups, can be reflective of underlying implicit bias and may affect patient outcome. Identifying and mitigating these biases is critical for ensuring equitable healthcare. This study aims to develop an analytical framework for measurement patterns, defined as the combination of measurement frequency (how often variables are collected) and missing data rates (the frequency of missing recordings), evaluate the association between them and demographic factors, and assess their impact on in-hospital mortality prediction. METHODS: We conducted a retrospective cohort study using the Medical Information Mart for Intensive Care III (MIMIC-III) database, which includes data on over 40,000 ICU patients from Beth Israel Deaconess Medical Center (2001-2012). Adult patients with ICU stays longer than 24\u00a0h were included. Measurement patterns, including missing data rates and measurement frequencies, were derived from EHR data and analyzed. Targeted Machine Learning (TML) methods were used to assess potential systematic disparities in measurement patterns across demographic factors (age, gender, race/ethnicity) while controlling for confounders such as other demographics and disease severity. The predictive power of measurement patterns on in-hospital mortality was evaluated. RESULTS: Among 23,426 patients, significant demographic systematic disparities were observed in the first 24\u00a0h of ICU stays. Elderly patients (\u2265\u200965 years) had more frequent temperature measurements compared to younger patients, while males had slightly fewer missing temperature measurements than females. Racial disparities were notable: White patients had more frequent blood pressure and oxygen saturation (SpO2) measurements compared to Black and Hispanic patients. Measurement patterns were associated with ICU mortality, with models based solely on these patterns achieving an area under the receiver operating characteristic curve (AUC) of 0.76 (95% CI: 0.74-0.77). CONCLUSIONS: This study underscores the significance of measurement patterns in ICU EHR data, which are associated with patient demographics and ICU mortality. Analyzing patterns of missing data and measurement frequencies provides valuable insights into patient monitoring practices and potential systemic disparities in healthcare delivery. Understanding these disparities is critical for improving the fairness of healthcare delivery and developing more accurate predictive models in critical care settings. CLINICAL TRIAL NUMBER: Not applicable.",
      "journal": "BMC medical informatics and decision making",
      "year": "2025",
      "doi": "10.1186/s12911-025-03058-9",
      "authors": "Shi Junming et al.",
      "keywords": "Bias detection; Critical care; Data completeness; Electronic health records (EHRs); Health equity; Healthcare applications; Implicit bias; MIMIC-III dataset; Measurement frequency; Systematic disparities",
      "mesh_terms": "Humans; Electronic Health Records; Female; Male; Intensive Care Units; Middle Aged; Retrospective Studies; Aged; Hospital Mortality; Adult; Bias; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40598240/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics; Wearables/Remote Monitoring",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; ICU; Clinical Trial",
      "key_findings": "CONCLUSIONS: This study underscores the significance of measurement patterns in ICU EHR data, which are associated with patient demographics and ICU mortality. Analyzing patterns of missing data and measurement frequencies provides valuable insights into patient monitoring practices and potential systemic disparities in healthcare delivery. Understanding these disparities is critical for improving the fairness of healthcare delivery and developing more accurate predictive models in critical care...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12220764"
    },
    {
      "pmid": "40604554",
      "title": "Methodological conduct and risk of bias in studies on prenatal birthweight prediction models using machine learning techniques: a systematic review.",
      "abstract": "OBJECTIVE: To assess the methodological quality and the risk of bias, of studies that developed prediction models using Machine Learning (ML) techniques to estimate prenatal birthweight. STUDY DESIGN AND METHODS: We conducted a systematic review, searching the PubMed databases between 01/01/2018 and 01/08/2022, for studies that developed fetal weight prediction models using ML. We used the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) statement to assess the reporting quality of included publications and the Prediction Model Risk of Bias Assessment Tool (PROBAST) to assess the risk of bias. We measured the overall adherence to the TRIPOD reporting checklist, provided a detailed analysis of the methodological quality of each study, and examined risk of bias in specific domains, including participant, predictor, outcome and analysis. RESULTS: Fourteen studies were included and the adherence to the TRIPOD reporting items ranged from 34.62% to 80.77%, with a median adherence of 63.19%. The studies showed significant variation in their methodological rigor, with a particularly high risk of bias in the selection of participants and predictors. Notably, issues related to missing data, sample size adequacy, performance evaluation, and model validation were prominent across studies. Several studies showed limited model transparency and reproducibility. CONCLUSION: Methodological quality of the ML-based prediction models for prenatal birthweight estimation was generally poor, with most studies at high risk of bias. There is an urgent need for improvements in the design and reporting of these studies. The adaptation of the TRIPOD and PROBAST statements specifically for ML models should be promoted to enhance transparency and reproducibility, which would facilitate the wider clinical application of ML-based prediction models and reduce research waste.",
      "journal": "BMC pregnancy and childbirth",
      "year": "2025",
      "doi": "10.1186/s12884-025-07727-5",
      "authors": "Gao Jing et al.",
      "keywords": "Fetal weight; Machine learning; Methodological quality; Prediction model; Risk of bias; Systematic review",
      "mesh_terms": "Humans; Machine Learning; Pregnancy; Female; Birth Weight; Bias; Reproducibility of Results; Infant, Newborn; Research Design; Fetal Weight",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40604554/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "ICU/Critical Care; Pediatrics; Obstetrics/Maternal Health",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: Methodological quality of the ML-based prediction models for prenatal birthweight estimation was generally poor, with most studies at high risk of bias. There is an urgent need for improvements in the design and reporting of these studies. The adaptation of the TRIPOD and PROBAST statements specifically for ML models should be promoted to enhance transparency and reproducibility, which would facilitate the wider clinical application of ML-based prediction models and reduce research w...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12225210"
    },
    {
      "pmid": "40605830",
      "title": "Generative AI in Medicine: Pioneering Progress or Perpetuating Historical Inaccuracies? Cross-Sectional Study Evaluating Implicit Bias.",
      "abstract": "BACKGROUND: Generative artificial intelligence (gAI) models, such as DALL-E 2, are promising tools that can generate novel images or artwork based on text input. However, caution is warranted, as these tools generate information based on historical data and are thus at risk of propagating past learned inequities. Women in medicine have routinely been underrepresented in academic and clinical medicine and the stereotype of a male physician persists. OBJECTIVE: The primary objective is to evaluate implicit bias among gAI across medical specialties. METHODS: To evaluate for potential implicit bias, 100 photographs for each medical specialty were generated using the gAI platform DALL-E2. For each specialty, DALL-E2 was queried with \"An American [specialty name].\" Our primary endpoint was to compare the gender distribution of gAI photos to the current distribution in the United States. Our secondary endpoint included evaluating the racial distribution. gAI photos were classified according to perceived gender and race based on a unanimous consensus among a diverse group of medical residents. The proportion of gAI women subjects was compared for each medical specialty to the most recent Association of American Medical Colleges report for physician workforce and active residents using \u03c72 analysis. RESULTS: A total of 1900 photos across 19 medical specialties were generated. Compared to physician workforce data, AI significantly overrepresented women in 7/19 specialties and underrepresented women in 6/19 specialties. Women were significantly underrepresented compared to the physician workforce by 18%, 18%, and 27% in internal medicine, family medicine, and pediatrics, respectively. Compared to current residents, AI significantly underrepresented women in 12/19 specialties, ranging from 10% to 36%. Additionally, women represented <50% of the demographic for 17/19 specialties by gAI. CONCLUSIONS: gAI created a sample population of physicians that underrepresented women when compared to both the resident and active physician workforce. Steps must be taken to train datasets in order to represent the diversity of the incoming physician workforce.",
      "journal": "JMIR AI",
      "year": "2025",
      "doi": "10.2196/56891",
      "authors": "Sutera Philip et al.",
      "keywords": "AI bias; Artificial Intelligence; bias; generative artificial intelligence; historical inequity; implicit bias; social inequity; workforce diversity",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40605830/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Generative AI",
      "health_domain": "Primary Care; Pediatrics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: gAI created a sample population of physicians that underrepresented women when compared to both the resident and active physician workforce. Steps must be taken to train datasets in order to represent the diversity of the incoming physician workforce.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12223688"
    },
    {
      "pmid": "40620096",
      "title": "A practical guide for nephrologist peer reviewers: evaluating artificial intelligence and machine learning research in nephrology.",
      "abstract": "Artificial intelligence (AI) and machine learning (ML) are transforming nephrology by enhancing diagnosis, risk prediction, and treatment optimization for conditions such as acute kidney injury (AKI) and chronic kidney disease (CKD). AI-driven models utilize diverse datasets-including electronic health records, imaging, and biomarkers-to improve clinical decision-making. Applications such as convolutional neural networks for kidney biopsy interpretation, and predictive modeling for renal replacement therapies underscore AI's potential. Nonetheless, challenges including data quality, limited external validation, algorithmic bias, and poor interpretability constrain the clinical reliability of AI/ML models. To address these issues, this article offers a structured framework for nephrologist peer reviewers, integrating the TRIPOD-AI (Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis-AI Extension) checklist. Key evaluation criteria include dataset integrity, feature selection, model validation, reporting transparency, ethics, and real-world applicability. This framework promotes rigorous peer review and enhances the reproducibility, clinical relevance, and fairness of AI research in nephrology. Moreover, AI/ML studies must confront biases-data, selection, and algorithmic-that adversely affect model performance. Mitigation strategies such as data diversification, multi-center validation, and fairness-aware algorithms are essential. Overfitting in AI is driven by small patient cohorts faced with thousands of candidate features; our framework spotlights this imbalance and offers concrete remedies. Future directions in AI-driven nephrology include multimodal data fusion for improved predictive modeling, deep learning for automated imaging analysis, wearable-based monitoring, and clinical decision support systems (CDSS) that integrate comprehensive patient data. A visual summary of key manuscript sections is included.",
      "journal": "Renal failure",
      "year": "2025",
      "doi": "10.1080/0886022X.2025.2513002",
      "authors": "Wang Yanni et al.",
      "keywords": "Artificial intelligence; kidney diseases; machine learning; nephrology; peer review; personalized treatment",
      "mesh_terms": "Humans; Acute Kidney Injury; Artificial Intelligence; Machine Learning; Nephrologists; Nephrology; Peer Review, Research; Renal Insufficiency, Chronic; Reproducibility of Results",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40620096/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Deep Learning; Neural Network; Clinical Prediction Model; Clinical Decision Support",
      "health_domain": "EHR/Health Informatics; Pathology; Nephrology; Wearables/Remote Monitoring",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability; Diverse/Representative Data",
      "clinical_setting": "Telehealth/Remote",
      "key_findings": "Future directions in AI-driven nephrology include multimodal data fusion for improved predictive modeling, deep learning for automated imaging analysis, wearable-based monitoring, and clinical decision support systems (CDSS) that integrate comprehensive patient data. A visual summary of key manuscript sections is included.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12239107"
    },
    {
      "pmid": "40629303",
      "title": "Evaluating accountability, transparency, and bias in AI-assisted healthcare decision- making: a qualitative study of healthcare professionals' perspectives in the UK.",
      "abstract": "BACKGROUND: While artificial intelligence (AI) has emerged as a powerful tool for enhancing diagnostic accuracy and streamlining workflows, key ethical questions remain insufficiently explored\u2014particularly around accountability, transparency, and bias. These challenges become especially critical in domains such as pathology and blood sciences, where opaque AI algorithms and non-representative datasets can impact clinical outcomes. The present work focuses on a single NHS context and does not claim broader generalization. METHODS: We conducted a local qualitative study across multiple healthcare facilities in a single NHS Trust in the West Midlands, United Kingdom, to investigate healthcare professionals\u2019 experiences and perceptions of AI-assisted decision-making. Forty participants\u2014including clinicians, healthcare administrators, and AI developers\u2014took part in semi-structured interviews or focus groups. Transcribed data were analyzed using Braun and Clarke\u2019s thematic analysis framework, allowing us to identify core themes relating to the benefits of AI, ethical challenges, and potential mitigation strategies. RESULTS: Participants reported notable gains in diagnostic efficiency and resource allocation, underscoring AI\u2019s potential to reduce turnaround times for routine tests and enhance detection of abnormalities. Nevertheless, accountability surfaced as a pervasive concern: while clinicians felt ultimately liable for patient outcomes, they also relied on AI-generated insights, prompting questions about liability if systems malfunctioned. Transparency emerged as another major theme, with clinicians emphasizing the difficulty of trusting \u201cblack box\u201d models that lack clear rationale or interpretability\u2014particularly for rare or complex cases. Bias was repeatedly cited, especially when algorithms underperformed in minority patient groups or in identifying atypical presentations. These issues raised doubts about the fairness and reliability of AIassisted diagnoses. CONCLUSIONS: Although AI demonstrates promise for improving efficiency and patient care, unresolved ethical complexities around accountability, transparency, and bias may erode stakeholder confidence and compromise patient safety. Participants called for clearer regulatory frameworks, inclusive training datasets, and stronger clinician\u2013developer collaboration. Future research should incorporate patient perspectives, investigate long-term impacts of AI-driven clinical decisions, and refine ethical guidelines to ensure equitable, responsible AI deployment. TRIAL REGISTRATION: : Not applicable. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1186/s12910-025-01243-z.",
      "journal": "BMC medical ethics",
      "year": "2025",
      "doi": "10.1186/s12910-025-01243-z",
      "authors": "Nouis Saoudi Ce et al.",
      "keywords": "Accountability; Artificial intelligence; Bias; Clinical Decision-Making; Electronic health record; Healthcare ethics; Qualitative research; Transparency",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40629303/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Pathology",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability; Diverse/Representative Data",
      "clinical_setting": "ICU; Laboratory/Pathology",
      "key_findings": "CONCLUSIONS: Although AI demonstrates promise for improving efficiency and patient care, unresolved ethical complexities around accountability, transparency, and bias may erode stakeholder confidence and compromise patient safety. Participants called for clearer regulatory frameworks, inclusive training datasets, and stronger clinician\u2013developer collaboration. Future research should incorporate patient perspectives, investigate long-term impacts of AI-driven clinical decisions, and refine ethica...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12235780"
    },
    {
      "pmid": "40633920",
      "title": "Ethical Implications of Artificial Intelligence in Vaccine Equity: Protocol for Exploring Vaccine Distribution Planning and Scheduling in Pandemics in Low- and Middle-Income Countries.",
      "abstract": "BACKGROUND: The COVID-19 pandemic highlighted significant disparities in vaccine distribution, particularly in low- and middle-income countries (LMICs). Artificial intelligence (AI) has emerged as a potential tool to optimize vaccine distribution planning and scheduling. However, its ethical implications, including equity, transparency, bias, and accessibility, remain underexplored. Ensuring ethical AI implementation in vaccine distribution is crucial to addressing health equity challenges worldwide. OBJECTIVE: This study aims to assess the ethical implications of AI-assisted vaccine distribution planning and scheduling in LMICs during pandemics. It seeks to evaluate AI's role in ensuring equitable vaccine access, analyze ethical concerns associated with its deployment, and propose an ethical framework to guide AI-based vaccine distribution strategies. METHODS: Our multiphase qualitative research approach will combine a systematic scoping review, a witness seminar with key stakeholders (health care professionals, AI developers, policymakers, and bioethicists), and a meta-synthesis of findings. The scoping review will follow PRISMA-ScR (Preferred Reporting Items for Systematic reviews and Meta-Analyses Extension for Scoping Reviews) guidelines, focusing on studies from 2019 to 2023. The witness seminar will provide firsthand insights into AI's ethical impact on vaccine equity. Thematic content analysis and qualitative coding will be used for data interpretation, with findings integrated into a policy-driven ethical framework. RESULTS: This study received institutional ethical approval in October 2023. Recruitment commenced in mid-August 2024 through email requests to prospective participants, and recruitment for the witness seminar (focus group discussion) is still ongoing, with 7 expert participants confirmed. Data collection is projected to conclude by August 2025. Preliminary literature analysis from the scoping review is ongoing, and qualitative data analysis from the witness seminar is scheduled for September 2025. The final results and proposed ethical framework are expected to be published in early 2026. CONCLUSIONS: By examining the ethical implications of AI in vaccine distribution, this research will provide actionable recommendations for policymakers, health care organizations, and AI developers. The findings will contribute to the discourse on responsible AI deployment in health care worldwide, ensuring transparency, fairness, and inclusivity in pandemic response strategies. INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): DERR1-10.2196/76634.",
      "journal": "JMIR research protocols",
      "year": "2025",
      "doi": "10.2196/76634",
      "authors": "Akuma Ifeanyichukwu et al.",
      "keywords": "algorithmic bias; digital inequities; digital inequity; ethical decision-making; health equity; vaccine logistics",
      "mesh_terms": "Humans; Artificial Intelligence; Developing Countries; COVID-19; COVID-19 Vaccines; Pandemics; Health Equity; SARS-CoV-2; Vaccination; Research Design",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40633920/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Generative AI",
      "health_domain": "ICU/Critical Care; Pulmonology",
      "bias_axes": "Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Data Collection; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSIONS: By examining the ethical implications of AI in vaccine distribution, this research will provide actionable recommendations for policymakers, health care organizations, and AI developers. The findings will contribute to the discourse on responsible AI deployment in health care worldwide, ensuring transparency, fairness, and inclusivity in pandemic response strategies. INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): DERR1-10.2196/76634.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12287670"
    },
    {
      "pmid": "40658719",
      "title": "Integrating equity, diversity, and inclusion throughout the lifecycle of artificial intelligence for healthcare: a scoping review.",
      "abstract": "The lack of Equity, Diversity, and Inclusion (EDI) principles in the lifecycle of Artificial Intelligence (AI) technologies in healthcare is a growing concern. Despite its importance, there is still a gap in understanding the initiatives undertaken to address this issue. This review aims to explore what and how EDI principles have been integrated into the design, development, and implementation of AI studies in healthcare. We followed the scoping review framework by Levac et al. and the Joanna Briggs Institute. A comprehensive search was conducted until April 29, 2022, across MEDLINE, Embase, PsycInfo, Scopus, and SCI-EXPANDED. Only research studies in which the integration of EDI in AI was the primary focus were included. Non-research articles were excluded. Two independent reviewers screened the abstracts and full texts, resolving disagreements by consensus or by consulting a third reviewer. To synthesize the findings, we conducted a thematic analysis and used a narrative description. We adhered to the PRISMA-ScR checklist for reporting scoping reviews. The search yielded 10,664 records, with 42 studies included. Most studies were conducted on the American population. Previous research has shown that AI models improve when socio-demographic factors such as gender and race are considered. Despite frameworks for EDI integration, no comprehensive approach systematically applies EDI principles in AI model development. Additionally, the integration of EDI into the AI implementation phase remains under-explored, and the representation of EDI within AI teams has been overlooked. This review reports on what and how EDI principles have been integrated into the design, development, and implementation of AI technologies in healthcare. We used a thorough search strategy and rigorous methodology, though we acknowledge limitations such as language and publication bias. A comprehensive framework is needed to ensure that EDI principles are considered throughout the AI lifecycle. Future research could focus on strategies to reduce algorithmic bias, assess the long-term impact of EDI integration, and explore policy implications to ensure that AI technologies are ethical, responsible, and beneficial for all.",
      "journal": "PLOS digital health",
      "year": "2025",
      "doi": "10.1371/journal.pdig.0000941",
      "authors": "Wang Ting et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40658719/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "A comprehensive framework is needed to ensure that EDI principles are considered throughout the AI lifecycle. Future research could focus on strategies to reduce algorithmic bias, assess the long-term impact of EDI integration, and explore policy implications to ensure that AI technologies are ethical, responsible, and beneficial for all.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12258586"
    },
    {
      "pmid": "40661269",
      "title": "Lack of children in public medical imaging data points to growing age bias in biomedical AI.",
      "abstract": "Artificial intelligence (AI) is rapidly transforming healthcare, but its benefits are not reaching all patients equally. Children remain overlooked with only 17% of FDA-approved medical AI devices labeled for pediatric use. In this work, we demonstrate that this exclusion may stem from a fundamental data gap. Our systematic review of 181 public medical imaging datasets reveals that children represent just under 1% of available data, while the majority of machine learning imaging conference papers we surveyed utilized publicly available data for methods development. Much like systematic biases of other kinds in model development, past studies have demonstrated the manner in which pediatric representation in data used for models intended for the pediatric population is essential for model performance in that population. We add to these findings, showing that adult-trained chest radiograph models exhibit significant age bias when applied to pediatric populations, with higher false positive rates in younger children. This work underscores the urgent need for increased pediatric representation in publicly accessible medical datasets. We provide actionable recommendations for researchers, policymakers, and data curators to address this age equity gap and ensure AI benefits patients of all ages.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.06.06.25328913",
      "authors": "Hua Stanley Bryan Zamora et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40661269/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Pediatrics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "This work underscores the urgent need for increased pediatric representation in publicly accessible medical datasets. We provide actionable recommendations for researchers, policymakers, and data curators to address this age equity gap and ensure AI benefits patients of all ages.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12259205"
    },
    {
      "pmid": "40665345",
      "title": "Potential to perpetuate social biases in health care by Chinese large language models: a model evaluation study.",
      "abstract": "BACKGROUND: Large language models (LLMs) may perpetuate or amplify social biases toward patients. We systematically assessed potential biases of three popular Chinese LLMs in clinical application scenarios. METHODS: We tested whether Qwen, Erine, and Baichuan encode social biases for patients of different sex, ethnicity, educational attainment, income level, and health insurance status. First, we prompted LLMs to generate clinical cases for medical education (n\u2009=\u20098,289) and compared the distribution of patient characteristics in LLM-generated cases with national distributions in China. Second, New England Journal of Medicine Healer clinical vignettes were used to prompt LLMs to generate differential diagnoses and treatment plans (n\u2009=\u200945,600), with variations analyzed based on sociodemographic characteristics. Third, we prompted LLMs to assess patient needs (n\u2009=\u200951,039) based on clinical cases, revealing any implicit biases toward patients with different characteristics. RESULTS: The three LLMs showed social biases toward patients with different characteristics to varying degrees in medical education, diagnostic and treatment recommendation, and patient needs assessment. These biases were more frequent in relation to sex, ethnicity, income level, and health insurance status, compared to educational attainment. Overall, the three LLMs failed to appropriately model the sociodemographic diversity of medical conditions, consistently over-representing male, high-education and high-income populations. They also showed a higher referral rate, indicating potential refusal to treat patients, for minority ethnic groups and those without insurance or living with low incomes. The three LLMs were more likely to recommend pain medications for males, and considered patients with higher educational attainment, Han ethnicity, higher income, and those with health insurance as having healthier relationships with others. INTERPRETATION: Our findings broaden the scopes of potential biases inherited in LLMs and highlight the urgent need for systematic and continuous assessments of social biases in LLMs in real-world clinical applications.",
      "journal": "International journal for equity in health",
      "year": "2025",
      "doi": "10.1186/s12939-025-02581-5",
      "authors": "Liu Chenxi et al.",
      "keywords": "Diagnosis and treatment; Large Language Models; Medical Education; Patient Assessment; Social Bias",
      "mesh_terms": "Female; Humans; Male; China; Healthcare Disparities; Large Language Models; East Asian People",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40665345/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Pain Management",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Language; Insurance Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "RESULTS: The three LLMs showed social biases toward patients with different characteristics to varying degrees in medical education, diagnostic and treatment recommendation, and patient needs assessment. These biases were more frequent in relation to sex, ethnicity, income level, and health insurance status, compared to educational attainment. Overall, the three LLMs failed to appropriately model the sociodemographic diversity of medical conditions, consistently over-representing male, high-educ...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12265265"
    },
    {
      "pmid": "40666330",
      "title": "Auditor Models to Suppress Poor AI Predictions Can Improve Human-AI Collaborative Performance.",
      "abstract": "OBJECTIVE: Healthcare decisions are increasingly made with the assistance of machine learning (ML). ML has been known to have unfairness - inconsistent outcomes across subpopulations. Clinicians interacting with these systems can perpetuate such unfairness by overreliance. Recent work exploring ML suppression - silencing predictions based on auditing the ML - shows promise in mitigating performance issues originating from overreliance. This study aims to evaluate the impact of suppression on collaboration fairness and evaluate ML uncertainty as desiderata to audit the ML. MATERIALS AND METHODS: We used data from the Vanderbilt University Medical Center electronic health record (n = 58,817) and the MIMIC-IV-ED dataset (n = 363,145) to predict likelihood of death or ICU transfer and likelihood of 30-day readmission. Our simulation study used gradient-boosted trees as well as an artificially high-performing oracle model. We derived clinician decisions directly from the dataset and simulated clinician acceptance of ML predictions based on previous empirical work on acceptance of CDS alerts. We measured performance as area under the receiver operating characteristic curve and algorithmic fairness using absolute averaged odds difference. RESULTS: When the ML outperforms humans, suppression outperforms the human alone (p < 0.034) and at least does not degrade fairness. When the human outperforms the ML, suppression outperforms the human (p < 5.2 \u00d7 10-5) but the human is fairer than suppression (p < 0.0019). Finally, incorporating uncertainty quantification into suppression approaches can improve performance. CONCLUSION: Suppression of poor-quality ML predictions through an auditor model shows promise in improving collaborative human-AI performance and fairness.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.06.24.25330212",
      "authors": "Brown Katherine E et al.",
      "keywords": "artificial intelligence; human-AI collaboration; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40666330/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics",
      "bias_axes": "Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Subgroup Analysis; Bias Auditing Framework",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: Suppression of poor-quality ML predictions through an auditor model shows promise in improving collaborative human-AI performance and fairness.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 2 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12262782"
    },
    {
      "pmid": "40667329",
      "title": "GhostBuster: A Deep-Learning-based, Literature-Unbiased Gene Prioritization Tool for Gene Annotation Prediction.",
      "abstract": "All genes are not equal before literature. Despite the explosion of genomic data, a significant proportion of human protein-coding genes remain poorly characterized (\"ghost genes\"). Due to sociological dynamics in research, scientific literature disproportionately focuses on already well-annotated genes, reinforcing existing biases (bandwagon effect). This literature bias often permeates machine learning (ML) models trained on gene annotation tasks, leading to predictions that favor well-studied genes. Consequently, standard ML performance metrics may overestimate biological relevance by overfitting literature-derived patterns. To address this challenge, we developed GhostBuster, an encoder-decoder ML platform designed to predict gene functions, disease associations and interactions while minimizing literature bias. We first compared the impact of biased (Gene Ontology) versus unbiased training datasets (LINCS, TCGA, STRING). While literature-biased sources yielded higher ML metrics, they also amplified bias by prioritizing well-characterized genes. In contrast, models trained on unbiased datasets were 2-3\u00d7 more effective at identifying recently discovered gene annotations. Notably, one of the unbiased channels (TCGA), combined minimal amounts of literature bias with robust performance, at a test ROC-AUC of 0.8-0.95. We demonstrate that GhostBuster can be applied to predict novel gene functions, refine pathway memberships, and prioritize intergenic GWAS hits. As the first ML framework explicitly designed to counteract literature bias, GhostBuster offers a powerful tool for uncovering the roles of understudied genes in cellular function, disease, and molecular networks.",
      "journal": "bioRxiv : the preprint server for biology",
      "year": "2025",
      "doi": "10.1101/2025.06.22.660948",
      "authors": "Deangeli Giulio et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40667329/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We demonstrate that GhostBuster can be applied to predict novel gene functions, refine pathway memberships, and prioritize intergenic GWAS hits. As the first ML framework explicitly designed to counteract literature bias, GhostBuster offers a powerful tool for uncovering the roles of understudied genes in cellular function, disease, and molecular networks.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12262676"
    },
    {
      "pmid": "40668069",
      "title": "AI-generated dermatologic images show deficient skin tone diversity and poor diagnostic accuracy: An experimental study.",
      "abstract": "BACKGROUND: Generative AI models are increasingly used in dermatology, yet biases in training datasets may reduce diagnostic accuracy and perpetuate ethnic health disparities. OBJECTIVES: To evaluate two key AI outputs: (1) skin tone representation and (2) diagnostic accuracy of generated dermatologic conditions. METHODS: Using the standard prompt 'Generate a photo of a person with [skin condition],' this cross-sectional study investigated skin tone diversity and accuracy of four leading AI models-Adobe Firefly, ChatGPT-4o, Midjourney and Stable Diffusion-across the 20 most common skin conditions. All images (n\u2009=\u20094000) were evaluated for skin tone representation from June to July 2024. Two independent raters used the Fitzpatrick scale to assess skin tone diversity compared to U.S. Census demographics using \u03c72. Two blinded dermatology residents evaluated a randomized 200-image subset for diagnostic accuracy. An inter-rater kappa statistic was calculated to assess rater agreement. RESULTS: Across all generated images, 89.8% depicted light skin, and 10.2% depicted dark skin. Adobe Firefly demonstrated the highest alignment with U.S. demographic data, with a non-significant chi-square result (38.1% dark skin, \u03c72(1)\u2009=\u20090.320, p\u2009=\u20090.572), indicating no meaningful difference between its generated skin tone diversity and census demographics. ChatGPT-4o, Midjourney and Stable Diffusion significantly underrepresented dark skin with Fitzpatrick scores of >IV (6.0%, 3.9% and 8.7% dark skin, respectively; all p\u2009<\u20090.001). Across all platforms, only 15% of images were identifiable by raters as the intended condition. Adobe Firefly had the lowest accuracy (0.94%), while ChatGPT-4o, Midjourney and Stable Diffusion demonstrated higher but still suboptimal accuracy (22%, 12.2% and 22.5%, respectively). CONCLUSIONS: The study highlights substantial deficiencies in the diversity and accuracy of AI-generated dermatological images. AI programs may exacerbate cognitive bias and health inequity, suggesting the need for ethical AI guidelines and diverse datasets to improve disease diagnosis and dermatologic care.",
      "journal": "Journal of the European Academy of Dermatology and Venereology : JEADV",
      "year": "2025",
      "doi": "10.1111/jdv.20849",
      "authors": "Joerg Lucie et al.",
      "keywords": "artificial intelligence; dermatology; diagnostic accuracy; racial disparities; representation; skin of color",
      "mesh_terms": "Humans; Cross-Sectional Studies; Skin Diseases; Artificial Intelligence; Skin Pigmentation; Female; Male; Photography; Dermatology",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40668069/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM; Generative AI",
      "health_domain": "Dermatology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Diverse/Representative Data",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: The study highlights substantial deficiencies in the diversity and accuracy of AI-generated dermatological images. AI programs may exacerbate cognitive bias and health inequity, suggesting the need for ethical AI guidelines and diverse datasets to improve disease diagnosis and dermatologic care.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40675093",
      "title": "Comparison of synthetic LGE with optimal inversion time vs. conventional LGE via representation learning: Quantification of Bias in Population Analysis.",
      "abstract": "PURPOSE: Late Gadolinium Enhancement (LGE) images are crucial elements of CMR protocols for evaluating myocardial infarct (MI) severity and size. However, these images rely on signal intensity changes and manual inversion time (TI) settings, leading to suboptimal lesion/remote contrast in many cases. Here, we propose an original approach to evaluate the impact of suboptimal TI on the retrospective analysis of ST-elevation MI (STEMI) patients, using a representation learning methodology tailored to consider infarct- and image-based characteristics across the studied population. METHODS: We analyzed 133 pairs of conventional and synthetic LGE short-axis images from the HIBISCUS-STEMI cohort (ClinicalTrials ID: NCT03070496). Optimal TI was identified among co-registered synthetic LGE images, using a mixture of the Mann-Whitney U-test, standard deviation, and saturation of pixel values, while the TI used for conventional LGE image generation was extracted from the DICOM header. Images were realigned to a reference for pixel-wise inter-subject comparisons. Population analysis relied on Attribute-based Regularized Variational Autoencoders which provide a latent representation of the population that is both easier to analyze (lower dimensionality) and ordered by infarct-relevant attributes. RESULTS: Despite visual quality control in the clinic, our study demonstrates that nearly 50% of conventional LGE slices may include a suboptimal TI setting, mostly related to TI settings shorter than the optimal TI determined from synthetic LGE. Additionally, our findings showed that when isolating contrast effects and suboptimal TI settings, contrast had a minimal impact on infarct lesion metrics such as infarct size or transmurality in the latent space. This suggests that other factors than contrast setting are leading (for both cases) to systematic and proportional bias (p<0.05) and loss of precision (respectively \u03c1=0.42 and \u03c1=0.43) in the latent space. CONCLUSION: Suboptimal TI undermines the analysis of infarct patterns in populations. Representation learning is a powerful method to retro-analyze cohorts, enabling the identification of imperfect settings, a crucial step for accurately characterizing representative patterns of a population. Our strategy can be considered a promising candidate for monitoring longitudinal changes and evaluating therapy outcomes on broader populations.",
      "journal": "Computers in biology and medicine",
      "year": "2025",
      "doi": "10.1016/j.compbiomed.2025.110643",
      "authors": "Deleat-Besson Romain et al.",
      "keywords": "Cardiac magnetic resonance; Dimensionality reduction; Late gadolinium enhancement; Myocardial infarction; Representation learning",
      "mesh_terms": "Humans; Male; Female; Middle Aged; Gadolinium; Retrospective Studies; Aged; Myocardial Infarction; Contrast Media; Magnetic Resonance Imaging; Image Processing, Computer-Assisted; ST Elevation Myocardial Infarction; Machine Learning",
      "pub_types": "Journal Article; Comparative Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40675093/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Representation Learning; Regularization",
      "clinical_setting": "Public Health/Population; Telehealth/Remote; Clinical Trial",
      "key_findings": "CONCLUSION: Suboptimal TI undermines the analysis of infarct patterns in populations. Representation learning is a powerful method to retro-analyze cohorts, enabling the identification of imperfect settings, a crucial step for accurately characterizing representative patterns of a population. Our strategy can be considered a promising candidate for monitoring longitudinal changes and evaluating therapy outcomes on broader populations.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40682331",
      "title": "The 2024 Declaration of Helsinki Revision: Relevance to Nursing Research.",
      "abstract": "BACKGROUND: The 2024 revision of the Declaration of Helsinki (DoH) marks a pivotal shift in biomedical research ethics, with significant implications for nursing research. This paper critically evaluates the Declaration's relevance to nursing practice, with particular attention to challenges in low-resource settings. Key updates emphasising global health equity, environmental sustainability, participant-centred consent and artificial intelligence (AI) governance are examined through nursing's ethical lenses of justice, beneficence and patient advocacy. METHODS: Using a multidimensional ethical framework grounded in Virtue Ethics, utilitarianism and phenomenology, the manuscript explores how nurses can ethically engage vulnerable populations, safeguard data privacy and advance inclusive, community-based research. RESULTS: It highlights gaps in the Declaration, particularly regarding algorithmic bias and digital consent and proposes practical strategies for nurse researchers, such as AI governance tools, dynamic consent models and context-sensitive sustainability practices. CONCLUSIONS: Rather than treating ethics as an abstract principle, the paper grounds theory in real-world practice, offering case examples that reflect the lived constraints of nursing researchers in underfunded and culturally diverse environments. By aligning ethical ideals with operational realities, this work reinforces nursing's critical role in shaping equitable and ethically resilient research practices under the revised Declaration.",
      "journal": "Journal of advanced nursing",
      "year": "2025",
      "doi": "10.1111/jan.70082",
      "authors": "Nashwan Abdulqadir J et al.",
      "keywords": "Global Health equity; community engagement; environmental sustainability; ethics; justice and beneficence; research; nursing research; informed consent; artificial intelligence; declaration of Helsinki; virtue ethics",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40682331/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population; Safety-Net/Underserved",
      "key_findings": "CONCLUSIONS: Rather than treating ethics as an abstract principle, the paper grounds theory in real-world practice, offering case examples that reflect the lived constraints of nursing researchers in underfunded and culturally diverse environments. By aligning ethical ideals with operational realities, this work reinforces nursing's critical role in shaping equitable and ethically resilient research practices under the revised Declaration.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40696767",
      "title": "Minimizing Racial Algorithmic Bias when Predicting Electronic Health Record Data Completeness.",
      "abstract": "The previously developed algorithm for identifying subjects with high electronic health record (EHR)-continuity performed suboptimally in racially diverse populations. We aimed to improve the performance by optimizing the race modeling strategy. We randomly divided TriNetX claims-linked EHR dataset from 11 US-based healthcare organizations into training (70%) and testing data (30%) to develop and test models with and without race interactions and race-specific models. We held out a Medicaid-linked EHR dataset as validation data. Study subjects were \u226518\u2009years with \u2265365\u2009days of continuous insurance enrollment overlapping an EHR encounter. We used cross-validated least absolute shrinkage and selection operator (LASSO) to select predictors of high EHR-continuity. We compared the model performance using area under receiver operating curve (AUC). There were 550,859, 236,089, and 65,956 subjects in the training, testing, and validation datasets, respectively. In the validation set, the introduction of race-interaction terms resulted in improved model performance in Black (AUC 0.821 vs. 0.812, P\u2009<\u20090.001) and other non-White race (AUC 0.828 vs. 0.812, P\u2009<\u20090.001) subgroups. The performance of the race-specific models did not differ substantially from that of the models with race-interaction terms in the race subgroups. Using the race interactions model, subjects in the top 50% of predicted EHR-continuity had 2-3-fold lesser misclassification of 40 comparative effectiveness research (CER) relevant variables. The inclusion of race-interaction terms improved model performance in the race subgroups. Using the EHR-continuity prediction algorithm with race-interaction terms can potentially reduce algorithmic bias for racial minorities.",
      "journal": "Clinical pharmacology and therapeutics",
      "year": "2025",
      "doi": "10.1002/cpt.3758",
      "authors": "Anand Priyanka et al.",
      "keywords": "",
      "mesh_terms": "Humans; Electronic Health Records; Algorithms; Male; Female; Adult; United States; Middle Aged; Racial Groups; Medicaid; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40696767/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM; Generative AI",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "The inclusion of race-interaction terms improved model performance in the race subgroups. Using the EHR-continuity prediction algorithm with race-interaction terms can potentially reduce algorithmic bias for racial minorities.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40699955",
      "title": "Mitigating Data Bias in Healthcare AI with Self-Supervised Standardization.",
      "abstract": "The rapid advancement of artificial intelligence (AI) in healthcare has accelerated innovations in medical algorithms, yet its broader adoption faces critical ethical and technical barriers. A key challenge lies in algorithmic bias stemming from heterogeneous medical data across institutions, equipment, and workflows, which may perpetuate disparities in AI-driven diagnoses and exacerbate inequities in patient care. While AI's ability to extract deep features from large-scale data offers transformative potential, its effectiveness heavily depends on standardized, high-quality datasets. Current standardization gaps not only limit model generalizability but also raise concerns about reliability and fairness in real-world clinical settings, particularly for marginalized populations. Addressing these urgent issues, this paper proposes an ethical AI framework centered on a novel self-supervised medical image standardization method. By integrating self-supervised image style conversion, channel attention mechanisms, and contrastive learning-based loss functions, our approach enhances structural and style consistency in diverse datasets while preserving patient privacy through decentralized learning paradigms. Experiments across multi-institutional medical image datasets demonstrate that our method significantly improves AI generalizability without requiring centralized data sharing. By bridging the data standardization gap, this work advances technical foundations for trustworthy AI in healthcare.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2025",
      "doi": "10.1109/JBHI.2025.3588196",
      "authors": "Lan Guipeng et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40699955/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability; Diverse/Representative Data",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "Experiments across multi-institutional medical image datasets demonstrate that our method significantly improves AI generalizability without requiring centralized data sharing. By bridging the data standardization gap, this work advances technical foundations for trustworthy AI in healthcare.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40703534",
      "title": "Ethics, Bias, and Governance in Artificial Intelligence for Hepatology: Toward Building a Safe and Fair Future.",
      "abstract": "Artificial intelligence (AI) is fundamentally changing how modern medicine is practiced with the intent of advancing and accelerating patient care and improving both patient experience and outcomes. AI, however, has been confronted by several challenges, including but not limited to ethics, regulation, and public trust. This paper explores an approach to AI governance in healthcare, specifically in hepatology. As AI continues to grow, it will be crucial for healthcare providers and our community as hepatologists to understand the implications and impact of this growth and what this means for our practice and patients. We draw from existing AI frameworks, principles of medical ethics, as well as quality healthcare principles to propose our framework for AI in hepatology. Our proposed framework includes patient-centered care, non-maleficence and safety, equity, transparency, accountability, and security and privacy. For each of these topics, we discuss examples relevant to hepatology. We also propose an action plan for hepatologists on how each of these principles can be upheld in our day-to-day practice. While many hepatology specific AI applications are currently being tested in research studies, they have not yet made it to \"prime time.\" As a result, the hepatology community has time to consider governance structures to put in place in preparation for the inherent challenges that come with AI implementation and integration into clinical care to ensure that care is responsible, ethical, safe, and secure. With careful and conscientious planning, the inclusion of relevant stakeholders, and laying the groundwork for governance, AI can improve quality of health care in hepatology with efficiency, improved safety, and equity.",
      "journal": "Journal of clinical and experimental hepatology",
      "year": "2025",
      "doi": "10.1016/j.jceh.2025.102628",
      "authors": "Ho Chanda K et al.",
      "keywords": "artificial intelligence; ethics; governance; hepatology; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40703534/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "While many hepatology specific AI applications are currently being tested in research studies, they have not yet made it to \"prime time.\" As a result, the hepatology community has time to consider governance structures to put in place in preparation for the inherent challenges that come with AI implementation and integration into clinical care to ensure that care is responsible, ethical, safe, and secure. With careful and conscientious planning, the inclusion of relevant stakeholders, and laying...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12281515"
    },
    {
      "pmid": "40711509",
      "title": "Artificial Intelligence in Health Profession Education.",
      "abstract": "Artificial Intelligence (AI) is rapidly integrating into medicine, demanding that Health Professional Education (HPE) incorporates AI literacy to prepare future clinicians. AI is driving transformative changes across all sectors. In education, it enables personalized learning and realistic, risk-free simulation training, and for healthcare, it enhances diagnostics, predicts patient outcomes, and improves care delivery. Research benefits from accelerated drug discovery and advanced data analysis, while AI-driven systems streamline administrative tasks. Despite its potential, significant challenges remain to be addressed. Critical concerns include patient data privacy, algorithmic bias that can create health disparities, and the ethical dilemma of accountability for AI errors. The \"black box\" nature of some algorithms, the risk of provider dependency leading to disuse atrophy of skills, and hurdles in technical implementation present major obstacles to effective integration. The future of HPE lies in the responsible adoption of AI. This requires developing robust curricula focused on ethical use, transparency, and navigating limitations of AI. Ultimately, AI should be leveraged as a powerful assistant to augment clinical intelligence and enable smarter workflows, not as a substitute for human judgment. Judicious use, combined with strong regulation and continuous refinement, is essential to harness full potential of AI, safely and effectively, in healthcare.",
      "journal": "Indian pediatrics",
      "year": "2025",
      "doi": "10.1007/s13312-025-00145-y",
      "authors": "Supe Avinash et al.",
      "keywords": "AI; Assessment; Clinical decision; Ethical issues; HPE; Learning; Teaching",
      "mesh_terms": "Artificial Intelligence; Humans; Health Occupations",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40711509/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Drug Discovery/Pharmacology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "Ultimately, AI should be leveraged as a powerful assistant to augment clinical intelligence and enable smarter workflows, not as a substitute for human judgment. Judicious use, combined with strong regulation and continuous refinement, is essential to harness full potential of AI, safely and effectively, in healthcare.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40713312",
      "title": "What black doctors know that AI can't: Confronting algorithmic bias and structural racism in modern medicine.",
      "abstract": "",
      "journal": "Journal of the National Medical Association",
      "year": "2025",
      "doi": "10.1016/j.jnma.2025.07.009",
      "authors": "Gomez Luis Emilio",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40713312/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40729959",
      "title": "Exploring Gender Bias in AI for Personalized Medicine: Focus Group Study With Trans Community Members.",
      "abstract": "BACKGROUND: This paper explores the perception and application of artificial intelligence (AI) for personalized medicine within the trans community, an often-overlooked demographic in the broader scope of precision medicine. Despite growing advancements in AI-driven health care solutions, little research has been dedicated to understanding how these technologies can be tailored to meet the unique health care needs of trans individuals. Addressing this gap is crucial for ensuring that precision medicine is genuinely inclusive and effective for all populations. OBJECTIVE: This study aimed to identify the specific challenges, obstacles, and potential solutions associated with the deployment of AI technologies in the development of personalized medicine for trans people. This research emphasizes a trans-inclusive and multidisciplinary perspective, highlighting the importance of cultural competence and community engagement in the design and implementation of AI-driven health care solutions. METHODS: A communicative methodology was applied in this study, prioritizing the active involvement of end-users and stakeholders through egalitarian dialogue that recognizes and values cultural intelligence. The methodological design included iterative consultations with trans community representatives to cocreate the research workflow and adapt data collection instruments accordingly. This participatory approach ensured that the perspectives and lived experiences of trans individuals were integral to the research process. Data collection was conducted through 3 focus groups with 16 trans adults, aimed at discussing the challenges, risks, and transformative potential of AI in precision medicine. RESULTS: Analysis of the focus group discussions revealed several critical barriers impacting the integration of AI in personalized medicine for trans people, including concerns around data privacy, biases in algorithmic decision-making, and the lack of tailored health care data reflective of trans experiences. Participants expressed apprehensions about potential misdiagnoses or inappropriate treatments due to cisnormative data models. However, they also identified opportunities for AI to enhance health care outcomes, advocating for community-led data collection initiatives and improved algorithmic transparency. Proposed solutions included enhancing datasets with trans-specific health markers, incorporating community voices in AI development processes, and prioritizing ethical frameworks that respect gender diversity. CONCLUSIONS: This study underscores the necessity for a trans-inclusive approach to precision medicine, facilitated by AI technologies that are sensitive to the health care needs and lived realities of trans people. By addressing the identified challenges and adopting community-driven solutions, AI has the potential to bridge existing health care gaps and improve the quality of life for trans individuals. This research contributes to the growing discourse on equitable health care innovation, calling for more inclusive AI design practices that extend the benefits of precision medicine to marginalized communities.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/72325",
      "authors": "Busl\u00f3n Nataly et al.",
      "keywords": "artificial intelligence; artificial intelligence biases; health biases; precision medicine; trans people; transgender medicine",
      "mesh_terms": "Humans; Precision Medicine; Artificial Intelligence; Focus Groups; Male; Female; Adult; Sexism; Transgender Persons; Middle Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40729959/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: This study underscores the necessity for a trans-inclusive approach to precision medicine, facilitated by AI technologies that are sensitive to the health care needs and lived realities of trans people. By addressing the identified challenges and adopting community-driven solutions, AI has the potential to bridge existing health care gaps and improve the quality of life for trans individuals. This research contributes to the growing discourse on equitable health care innovation, cal...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12307004"
    },
    {
      "pmid": "40745627",
      "title": "Using a large language model (ChatGPT) to assess risk of bias in randomized controlled trials of medical interventions: protocol for a pilot study of interrater agreement with human reviewers.",
      "abstract": "BACKGROUND: Risk of bias (RoB) assessment is an essential part of systematic reviews that requires reading and understanding each eligible trial and RoB tools. RoB assessment is subject to human error and is time-consuming. Machine learning-based tools have been developed to automate RoB assessment using simple models trained on limited corpuses. ChatGPT is a conversational agent based on a large language model (LLM) that was trained on an internet-scale corpus and has demonstrated human-like abilities in multiple areas including healthcare. LLMs might be able to support systematic reviewing tasks such as assessing RoB. We aim to assess interrater agreement in overall (rather than domain-level) RoB assessment between human reviewers and ChatGPT, in randomized controlled trials of interventions within medical interventions. METHODS: We will randomly select 100 individually- or cluster-randomized, parallel, two-arm trials of medical interventions from recent Cochrane systematic reviews that have been assessed using the RoB1 or RoB2 family of tools. We will exclude reviews and trials that were performed under emergency conditions (e.g.,\u00a0COVID-19), as well as public health and welfare interventions. We will use 25 of the trials and human RoB assessments to engineer a ChatGPT prompt for assessing overall RoB, based on trial methods text. We will obtain ChatGPT assessments of RoB for the remaining 75 trials and human assessments. We will then estimate interrater agreement using Cohen's \u03ba. RESULTS: The primary outcome for this study is overall human-ChatGPT interrater agreement. We will report observed agreement with an exact 95% confidence interval, expected agreement under random assessment, Cohen's \u03ba, and a p-value testing the null hypothesis of no difference in agreement. Several other analyses are also planned. CONCLUSIONS: This study is likely to provide the first evidence on interrater agreement between human RoB assessments and those provided by LLMs and will inform subsequent research in this area.",
      "journal": "BMC medical research methodology",
      "year": "2025",
      "doi": "10.1186/s12874-025-02631-0",
      "authors": "Rose Christopher James et al.",
      "keywords": "Artificial intelligence; ChatGPT; Large language model; Machine learning; Risk of bias; Systematic reviewing",
      "mesh_terms": "Humans; Bias; Generative Artificial Intelligence; Large Language Models; Machine Learning; Observer Variation; Pilot Projects; Randomized Controlled Trials as Topic; Research Design; Risk Assessment; Systematic Reviews as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40745627/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Pulmonology; Public Health",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Clinical Trial",
      "key_findings": "CONCLUSIONS: This study is likely to provide the first evidence on interrater agreement between human RoB assessments and those provided by LLMs and will inform subsequent research in this area.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12315198"
    },
    {
      "pmid": "40764849",
      "title": "Socioeconomic impact of artificial intelligence-driven point-of-care testing devices for liquid biopsy in the OncoCheck system.",
      "abstract": "Cancer disparities in low- and middle-income countries (LMICs) persist because of socioeconomic inequalities and limited access to screening infrastructure, which requires equitable diagnostic solutions. As researchers, we need to develop interventions which mirror successful strategies from high-income countries (HICs) to address mortality inequalities. Routine cancer diagnosis functions as a fundamental element of effective management yet remains unavailable to numerous populations in LMICs. This review proposes the conceptual \"OncoCheck\" model, which combines the terms Oncology \"Onco\" and Screening \"Check\" as an integrated approach to early cancer detection. It provides a theoretically sound practical approach that combines liquid biopsy with point-of-care testing (POCT) and artificial intelligence (AI) to achieve high-sensitivity diagnostics in resource-limited settings without requiring advanced infrastructure. The review advocates OncoCheck as a promising and practical cancer screening solution which shows potential to increase accessibility and decrease costs while improving survival rates through early detection. Moving beyond technical specifications, the manuscript assesses its socioeconomic impact, showing reduced medical costs and improved treatment outcomes. The paper describes its implementation framework together with a validation strategy and performance benchmarks. The analysis further focuses on the implementation barriers like algorithmic bias mitigation, infrastructure limitations, and ethical AI deployment. The OncoCheck system delivers equitable cancer care by implementing a hospital-at-home model which functions with real-world health systems.",
      "journal": "Cancer metastasis reviews",
      "year": "2025",
      "doi": "10.1007/s10555-025-10281-3",
      "authors": "Singh Sima et al.",
      "keywords": "Artificial intelligence in cancer; Cancer inequities; Liquid biopsy; OncoCheck; Point-of-care testing",
      "mesh_terms": "Humans; Artificial Intelligence; Liquid Biopsy; Neoplasms; Early Detection of Cancer; Point-of-Care Testing; Socioeconomic Factors",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40764849/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; Pathology",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; Public Health/Population",
      "key_findings": "The analysis further focuses on the implementation barriers like algorithmic bias mitigation, infrastructure limitations, and ethical AI deployment. The OncoCheck system delivers equitable cancer care by implementing a hospital-at-home model which functions with real-world health systems.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12325499"
    },
    {
      "pmid": "40768261",
      "title": "Stakeholder Perspectives on Trustworthy AI for Parkinson Disease Management Using a Cocreation Approach: Qualitative Exploratory Study.",
      "abstract": "BACKGROUND: Parkinson disease (PD) is the fastest-growing neurodegenerative disorder in the world, with prevalence expected to exceed 12 million by 2040, which poses significant health care and societal challenges. Artificial intelligence (AI) systems and wearable sensors hold potential for PD diagnosis, personalized symptom monitoring, and progression prediction. Nonetheless, ethical AI adoption requires several core principles, including user trust, transparency, fairness, and human oversight. OBJECTIVE: This study aims to explore and synthesize the perspectives of diverse stakeholders, such as individuals living with PD, health care professionals, AI experts, and bioethicists. The aim was to guide the development of AI-driven digital health solutions, emphasizing transparency, data security, fairness, and bias mitigation while ensuring robust human oversight. These efforts are part of the broader Artificial Intelligence-Based Parkinson's Disease Risk Assessment and Prognosis (AI-PROGNOSIS) European project, dedicated to advancing ethical and effective AI applications in PD diagnosis and management. METHODS: An exploratory qualitative approach, based on 2 datasets constructed from cocreation workshops, engaged key stakeholders with diverse expertise to gather insights, ensuring a broad range of perspectives and enriching the thematic analysis. A total of 24 participants participated in the cocreation workshops, including 11 (46%) people with PD, 6 (25%) health care professionals, 3 (13%) AI technical experts, 1 (4%) bioethics expert, and 3 (13%) facilitators. Using a semistructured guide, key aspects of the discussion centered on trust, fairness, explainability, autonomy, and the psychological impact of AI in PD care. RESULTS: Thematic analysis of the cocreation workshop transcripts identified 5 key main themes, each explored through various corresponding subthemes. AI trust and security (theme 1) was highlighted, focusing on data safety and the accuracy and reliability of the AI systems. AI transparency and education (theme 2) emphasized the need for educational initiatives and the importance of transparency and explainability of AI technologies. AI bias (theme 3) was identified as a critical theme, addressing issues of bias and fairness and ensuring equitable access to AI-driven health care solutions. Human oversight (theme 4) stressed the significance of AI-human collaboration and the essential role of human review in AI processes. Finally, AI's psychological impact (theme 5) examined the emotional impact of AI on patients and how AI is perceived in the context of PD care. CONCLUSIONS: Our findings underline the importance of implementing robust security measures, developing transparent and explainable AI models, reinforcing bias mitigation and reduction strategies and equitable access to treatment, integrating human oversight, and considering the psychological impact of AI-assisted health care. These insights provide actionable guidance for developing trustworthy and effective AI-driven digital PD diagnosis and management solutions.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/73710",
      "authors": "Alves Beatriz et al.",
      "keywords": "Parkinson disease management; advanced care strategies; artificial intelligence; assessment; cocreation; digital health care solutions; disease risk; prognosis; stakeholder insights; trust in AI systems",
      "mesh_terms": "Parkinson Disease; Humans; Artificial Intelligence; Qualitative Research; Trust; Stakeholder Participation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40768261/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Wearables/Remote Monitoring",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Telehealth/Remote",
      "key_findings": "CONCLUSIONS: Our findings underline the importance of implementing robust security measures, developing transparent and explainable AI models, reinforcing bias mitigation and reduction strategies and equitable access to treatment, integrating human oversight, and considering the psychological impact of AI-assisted health care. These insights provide actionable guidance for developing trustworthy and effective AI-driven digital PD diagnosis and management solutions.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 2 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12368464"
    },
    {
      "pmid": "40775971",
      "title": "AI Bias and Confounding Risk in Health Feature Engineering for Machine Learning Classification Task.",
      "abstract": "Recent advancements in machine learning bring unique opportunities in health fields but also pose considerable challenges. Due to stringent ethical considerations and resource constraints, health data can vary in scope, population coverage, and collection granularity, prone to different AI bias and confounding risks in the performance of a classification task. This experimental study explored the impact on hidden confounding risk of model performance in a cardiovascular readmission prediction task using real-life health data from 'Data-derived Risk assessment using the Electronic medical record through Application of Machine Learning' (DREAM). Five commonly used machine learning models-k-nearest neighbors (KNN), random forest (RF), decision tree (DT), Catboost and Xgboost-were selected for this task. Model performance was assessed via the area under the receiver operating characteristics curve (AUC) and F1 score, both before and after propensity score adjustment. Based on density plot comparison of the adjustment, the difference mainly contributed from patients aged 20 and 40. High fluctuation on the model performance has been noted by including and excluding patients under this age group. After reasoning, high-risk pregnant females may serve as a confounding factor in the original model generation. The pregnancy rate in the non-readmitted group is significantly higher than that in the readmitted group (x2 = 10.2, p < 0.001). However, pregnant status required additional information query from a different hospital system. Without carefully consideration of confounding risks, traditional pipeline may generate a less robotic classifier in the clinical setting. Incorporating propensity score matching could be a solution to randomise invisible confounding factors between the classes.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI250953",
      "authors": "Guo Ruihua et al.",
      "keywords": "AI bias; Classification; Confounding bias; Feature Engineering; Machine Learning; Quality Control; Readmission risk prediction",
      "mesh_terms": "Machine Learning; Humans; Electronic Health Records; Female; Risk Assessment; Bias; Adult; Patient Readmission; Cardiovascular Diseases; Pregnancy; Confounding Factors, Epidemiologic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40775971/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Random Forest; XGBoost/Gradient Boosting; Decision Tree",
      "health_domain": "Cardiology; EHR/Health Informatics; Obstetrics/Maternal Health",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; Public Health/Population",
      "key_findings": "Without carefully consideration of confounding risks, traditional pipeline may generate a less robotic classifier in the clinical setting. Incorporating propensity score matching could be a solution to randomise invisible confounding factors between the classes.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40776262",
      "title": "FairFML: A Unified Approach to Algorithmic Fair Federated Learning with Applications to Reducing Gender Disparities in Cardiac Arrest Outcomes.",
      "abstract": "Addressing algorithmic bias in healthcare is crucial for ensuring equity in patient outcomes, particularly in cross-institutional collaborations where privacy constraints often limit data sharing. Federated learning (FL) offers a solution by enabling institutions to collaboratively train models without sharing sensitive data, but challenges related to fairness remain. To tackle this, we propose Fair Federated Machine Learning (FairFML), a model-agnostic framework designed to reduce algorithmic disparities while preserving patient privacy. Validated in a real-world study on gender disparities in cardiac arrest outcomes, FairFML improved fairness by up to 65% compared to centralized models, without compromising predictive performance.",
      "journal": "Studies in health technology and informatics",
      "year": "2025",
      "doi": "10.3233/SHTI251245",
      "authors": "Li Siqi et al.",
      "keywords": "Clinical decision-making; Demographic disparity; Electronic health records; Federated Learning; Model fairness",
      "mesh_terms": "Humans; Machine Learning; Female; Male; Heart Arrest; Algorithms; Healthcare Disparities; Sex Factors; Federated Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40776262/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Federated Learning",
      "health_domain": "Cardiology; ICU/Critical Care",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Federated Learning",
      "clinical_setting": "ICU",
      "key_findings": "To tackle this, we propose Fair Federated Machine Learning (FairFML), a model-agnostic framework designed to reduce algorithmic disparities while preserving patient privacy. Validated in a real-world study on gender disparities in cardiac arrest outcomes, FairFML improved fairness by up to 65% compared to centralized models, without compromising predictive performance.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40794953",
      "title": "Towards machine learning fairness in classifying multicategory causes of deaths in colorectal or lung cancer patients.",
      "abstract": "Classification of patient multicategory survival outcomes is important for personalized cancer treatments. Machine learning (ML) algorithms have increasingly been used to inform healthcare decisions, but these models are vulnerable to biases in data collection and algorithm creation. ML models have previously been shown to exhibit racial bias, but their fairness towards patients from different age and sex groups have yet to be studied. Therefore, we compared the multimetric performances of five ML models (random forests, multinomial logistic regression, linear support vector classifier, linear discriminant analysis, and multilayer perceptron) when classifying colorectal cancer patients (n\u2009=\u2009589) of various age, sex, and racial groups using The Cancer Genome Atlas data. All five models exhibited biases for these sociodemographic groups. We then repeated the same process on lung adenocarcinoma (n\u2009=\u2009515) to validate our findings. Surprisingly, most models tended to perform more poorly overall for the largest sociodemographic groups. Methods to optimize model performance, including testing the model on merged age, sex, or racial groups, and creating a model trained on and used for an individual or merged sociodemographic group, show potential to reduce disparities in model performance for different groups. This is supported by our regression analysis showing associations between model choice and methodology used with reduced performance disparities across demographic subgroups. Notably, these methods may be used to improve ML fairness while avoiding penalizing the model for exhibiting bias and thus sacrificing overall performance.",
      "journal": "Briefings in bioinformatics",
      "year": "2025",
      "doi": "10.1093/bib/bbaf398",
      "authors": "Feng Catherine H et al.",
      "keywords": "colorectal cancer; lung cancer; machine learning; machine learning fairness; multilabel classification; survival",
      "mesh_terms": "Humans; Lung Neoplasms; Machine Learning; Colorectal Neoplasms; Male; Female; Middle Aged; Aged; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40794953/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Random Forest; Logistic Regression; Support Vector Machine; Neural Network",
      "health_domain": "Oncology; Pulmonology; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "This is supported by our regression analysis showing associations between model choice and methodology used with reduced performance disparities across demographic subgroups. Notably, these methods may be used to improve ML fairness while avoiding penalizing the model for exhibiting bias and thus sacrificing overall performance.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12342732"
    },
    {
      "pmid": "40802160",
      "title": "Navigating Healthcare AI Governance: the Comprehensive Algorithmic Oversight and Stewardship Framework for Risk and Equity.",
      "abstract": "Integrating artificial intelligence (AI) in healthcare has sparked innovation but exposed vulnerabilities in regulatory oversight. Unregulated \"shadow\" AI systems, operating outside formal frameworks, pose risks such as algorithmic drift, bias, and disparities. The Comprehensive Algorithmic Oversight and Stewardship (CAOS) Framework addresses these challenges, combining risk assessments, data protection, and equity-focused methodologies to ensure responsible AI implementation. This framework offers a solution to bridge oversight gaps while supporting responsible healthcare innovation. CAOS functions as both a normative governance model and a practical system design, offering a scalable framework for ethical oversight, policy development, and operational implementation of AI systems in healthcare.",
      "journal": "Health care analysis : HCA : journal of health philosophy and policy",
      "year": "2025",
      "doi": "10.1007/s10728-025-00537-y",
      "authors": "Kumar Rahul et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40802160/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "This framework offers a solution to bridge oversight gaps while supporting responsible healthcare innovation. CAOS functions as both a normative governance model and a practical system design, offering a scalable framework for ethical oversight, policy development, and operational implementation of AI systems in healthcare.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40816000",
      "title": "Healthcare and cutting-edge technology: Advancements, challenges, and future prospects.",
      "abstract": "The high-level integration of technology in health care has radically changed the process of patient care, diagnosis, treatment, and health outcomes. This paper discusses significant technological advances: AI for medical imaging to detect early disease stages; robotic surgery with precision and minimally invasive techniques; telemedicine for remote monitoring and virtual consultation; personalized medicine through genomic analysis; and blockchain in secure and transparent handling of health data. Every section in the paper discusses the underlying principles, advantages, and disadvantages associated with such technologies, supported by appropriate case studies like deploying AI in radiology to enhance cancer diagnosis or robotic surgery to enhance accuracy in surgery and blockchain technology in electronic health records to enable data integrity and security. The paper also discusses key ethical issues, including risks to data privacy, algorithmic bias in AI-based diagnosis, patient consent problems in genomic medicine, and regulatory issues blocking the large-scale adoption of digital health solutions. The article also includes some recommended avenues of future research in the spaces where interdisciplinary cooperation, effective cybersecurity frameworks, and policy transformations are urgently required to ensure that new healthcare technology adoption is ethical and responsible. The work is aimed at delivering important information for policymakers and researchers who are interested in the changing roles of technology to improve healthcare provision and patient outcomes, as well as healthcare practitioners.",
      "journal": "Computers in biology and medicine",
      "year": "2025",
      "doi": "10.1016/j.compbiomed.2025.110861",
      "authors": "Singhal Vikas et al.",
      "keywords": "Artificial intelligence; Bioinformatics and virtual reality; Blockchain; Healthcare; Internet of medical things (IoMT); Nanotechnology and drug delivery",
      "mesh_terms": "Humans; Telemedicine; Artificial Intelligence; Computer Security; Delivery of Health Care; Precision Medicine; Electronic Health Records",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40816000/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Oncology; EHR/Health Informatics; Surgery; Genomics/Genetics; Wearables/Remote Monitoring",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Telehealth/Remote",
      "key_findings": "The article also includes some recommended avenues of future research in the spaces where interdisciplinary cooperation, effective cybersecurity frameworks, and policy transformations are urgently required to ensure that new healthcare technology adoption is ethical and responsible. The work is aimed at delivering important information for policymakers and researchers who are interested in the changing roles of technology to improve healthcare provision and patient outcomes, as well as healthcar...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40818896",
      "title": "Inequities in Research and Publication: A Call for Equity, Diversity, and Inclusion.",
      "abstract": "In the realm of academic medicine and scientific research, systemic inequities exist both at the participant or patient level and at the level of the researcher. In this paper, we outline sex, gender, geographical, ethnic, and racial inequities in those who participate as subjects in research, which often leads to the exclusion of vulnerable populations. In addition, we highlight inequities in research funding, mentorship, publications, language, career stage, and leadership roles that lead to disparities in who conducts the research or decides on research priorities. We also report algorithmic bias and artificial intelligence as an emerging source of inequities in research. We conclude by proposing strategies to promote diversity, equity, and inclusion in research and publication and provide a framework for achieving this when conducting clinical trials.",
      "journal": "Advances in kidney disease and health",
      "year": "2025",
      "doi": "10.1053/j.akdh.2025.03.003",
      "authors": "Bajpai Divya et al.",
      "keywords": "Diversity; Inclusion; Inequity; Publication; Research",
      "mesh_terms": "Humans; Biomedical Research; Cultural Diversity; Artificial Intelligence; Diversity, Equity, Inclusion",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40818896/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Clinical Trial",
      "key_findings": "We also report algorithmic bias and artificial intelligence as an emerging source of inequities in research. We conclude by proposing strategies to promote diversity, equity, and inclusion in research and publication and provide a framework for achieving this when conducting clinical trials.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40829151",
      "title": "Exploring the Application of AI and Extended Reality Technologies in Metaverse-Driven Mental Health Solutions: Scoping Review.",
      "abstract": "BACKGROUND: Mental health systems worldwide face unprecedented strain due to rising psychological distress, limited access to care, and an insufficient number of trained professionals. Even in high-income countries, the ratio of patients to health care providers remains inadequate to address demand. Emerging technologies such as artificial intelligence (AI) and extended reality (XR) are being explored to improve access, engagement, and scalability of mental health interventions. When integrated into immersive metaverse environments, these technologies offer the potential to deliver personalized and emotionally responsive mental health care. OBJECTIVE: This scoping review explores the state-of-the-art applications of AI and XR technologies in metaverse frameworks for mental health. It identifies technological capabilities, therapeutic benefits, and ethical limitations, focusing on governance gaps related to data privacy, patient-clinician dynamics, algorithmic bias, digital inequality, and psychological dependency. METHODS: A systematic search was conducted across 5 electronic databases-PubMed, Scopus, IEEE Xplore, PsycINFO, and Google Scholar-for peer-reviewed literature published between January 2014 and October 2024. Search terms included combinations of \"AI,\" \"XR,\" \"VR,\" \"mental health,\" \"psychotherapy,\" and \"metaverse.\" Studies were eligible if they (1) involved mental health interventions; (2) used AI or XR within immersive or metaverse-like environments; and (3) were empirical, peer-reviewed articles in English. Editorials, conference summaries, and articles lacking clinical or technical depth were excluded. Two reviewers independently screened titles, abstracts, and full texts using predefined inclusion and exclusion criteria, with Cohen \u03ba values of 0.85 and 0.80 indicating strong interrater agreement. Risk of bias was not assessed due to the scoping nature of the review. Data synthesis followed a narrative approach. RESULTS: Of 1288 articles identified, 48 studies met the inclusion criteria. The included studies varied in design and scope, with most studies conducted in high-income countries. AI applications included emotion detection, conversational agents, and clinical decision-support systems. XR interventions ranged from virtual reality-based cognitive behavioral therapy and exposure therapy to avatar-guided mindfulness. Several studies reported improvements in patient engagement, symptom reduction, and treatment adherence. However, many studies were limited by small sample sizes, single-institution settings, and lack of longitudinal validation. Ethical risks identified included opaque algorithmic processes, risks of psychological overdependence, weak data governance, and the exclusion of digitally marginalized populations. CONCLUSIONS: AI and XR technologies integrated within metaverse settings represent promising tools for enhancing mental health care delivery through personalization, scalability, and immersive engagement. However, the current evidence base is limited by methodological inconsistencies and a lack of long-term validation. Future research should use disorder-specific frameworks; adopt standardized efficacy measures; and ensure inclusive, ethical, and transparent development practices. Strong interdisciplinary governance models are essential to support the responsible and equitable integration of AI-driven XR technologies into mental health care. The narrative synthesis limits generalizability, and the absence of a risk of bias assessment hinders critical appraisal.",
      "journal": "Journal of medical Internet research",
      "year": "2025",
      "doi": "10.2196/72400",
      "authors": "Tabassum Aliya et al.",
      "keywords": "AR; VR; XR ethics; augmented reality; cognitive behavioral therapy; digital therapeutics; extended reality ethics; immersive technology; mental wellness; psychotherapy; telehealth; virtual interventions; virtual reality",
      "mesh_terms": "Humans; Artificial Intelligence; Mental Health; Mental Health Services; Mental Disorders",
      "pub_types": "Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40829151/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: AI and XR technologies integrated within metaverse settings represent promising tools for enhancing mental health care delivery through personalization, scalability, and immersive engagement. However, the current evidence base is limited by methodological inconsistencies and a lack of long-term validation. Future research should use disorder-specific frameworks; adopt standardized efficacy measures; and ensure inclusive, ethical, and transparent development practices.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 2 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12405795"
    },
    {
      "pmid": "40831669",
      "title": "Fair Text to Medical Image Diffusion Model with Subgroup Distribution Aligned Tuning.",
      "abstract": "The Text to Medical Image (T2MedI) approach using latent diffusion models holds significant promise for addressing the scarcity of medical imaging data and elucidating the appearance distribution of lesions corresponding to specific patient status descriptions. Like natural image synthesis models, our investigations reveal that the T2MedI model may exhibit biases towards certain subgroups, potentially neglecting minority groups present in the training dataset. In this study, we initially developed a T2MedI model adapted from the pre-trained Imagen framework. This model employs a fixed Contrastive Language-Image Pre-training (CLIP) text encoder, with its decoder fine-tuned using medical images from the Radiology Objects in Context (ROCO) dataset. We conduct both qualitative and quantitative analyses to examine its gender bias. To address this issue, we propose a subgroup distribution alignment method during fine-tuning on a target application dataset. Specifically, this process involves an alignment loss, guided by an off-the-shelf sensitivity-subgroup classifier, which aims to synchronize the classification probabilities between the generated images and those expected in the target dataset. Additionally, we preserve image quality through a CLIP-consistency regularization term, based on a knowledge distillation framework. For evaluation purposes, we designated the BraTS18 dataset as the target, and developed a gender classifier based on brain magnetic resonance (MR) imaging slices derived from it. Our methodology significantly mitigates gender representation inconsistencies in the generated MR images, aligning them more closely with the gender distribution in the BraTS18 dataset.",
      "journal": "Proceedings of SPIE--the International Society for Optical Engineering",
      "year": "2025",
      "doi": "10.1117/12.3046450",
      "authors": "Han Xu et al.",
      "keywords": "AI Fairness; Diffusion Model; Text to Medical Image",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40831669/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Transfer Learning; Regularization",
      "clinical_setting": "Not specified",
      "key_findings": "For evaluation purposes, we designated the BraTS18 dataset as the target, and developed a gender classifier based on brain magnetic resonance (MR) imaging slices derived from it. Our methodology significantly mitigates gender representation inconsistencies in the generated MR images, aligning them more closely with the gender distribution in the BraTS18 dataset.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12360154"
    },
    {
      "pmid": "40833534",
      "title": "Evolution and integration of artificial intelligence across the cancer continuum in women: advances in risk assessment, prevention, and early detection.",
      "abstract": "PURPOSE: Artificial Intelligence (AI) is revolutionizing the prevention and control of breast cancer by improving risk assessment, prevention, and early diagnosis.\u00a0Considering an emphasis on AI applications across the women's breast cancer spectrum, this review summarizes developments, existing applications, and future potential prospects. METHODS: We conducted an in-depth review of the literature on AI applications in breast cancer risk prediction, prevention, and early detection from 2000 to 2025, with particular emphasis on Explainable AI (XAI), deep learning (DL), and machine learning (ML). We examined algorithmic fairness, model transparency, dataset representation, and clinical performance indicators. FINDINGS: As compared to traditional methods, AI-based models continuously enhanced risk categorization, screening sensitivity, and early detection (AUCs ranging from 0.65 to 0.975). However, challenges remain in algorithmic bias, underrepresentation of minority populations, and limited external validation. Remarkably, 58% of public datasets focused on mammography, leaving gaps in modalities such as tomosynthesis and histopathology. CONCLUSIONS: AI technologies have an enormous number of opportunities for enhancing the diagnosis and treatment of breast cancer. However, transparent models, inclusive datasets, and standardized frameworks for explainability and external validation should be given the greatest attention in subsequent studies to ensure equitable and effective implementation.",
      "journal": "Cancer causes & control : CCC",
      "year": "2025",
      "doi": "10.1007/s10552-025-02048-6",
      "authors": "Desai Mitali et al.",
      "keywords": "Artificial intelligence; Breast cancer; Early detection; Explainable AI (XAI); Health equity; Prevention; Risk assessment; Women\u2019s health",
      "mesh_terms": "Humans; Female; Early Detection of Cancer; Breast Neoplasms; Artificial Intelligence; Risk Assessment; Mammography",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40833534/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Clinical Prediction Model",
      "health_domain": "Radiology/Medical Imaging; Oncology; ICU/Critical Care; Pathology",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability; Diverse/Representative Data",
      "clinical_setting": "ICU; Public Health/Population; Laboratory/Pathology",
      "key_findings": "CONCLUSIONS: AI technologies have an enormous number of opportunities for enhancing the diagnosis and treatment of breast cancer. However, transparent models, inclusive datasets, and standardized frameworks for explainability and external validation should be given the greatest attention in subsequent studies to ensure equitable and effective implementation.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40833926",
      "title": "Over-reliance on AI for diagnosis: the potential for algorithmic bias and the erosion of clinical skills.",
      "abstract": "",
      "journal": "Journal of medical engineering & technology",
      "year": "2025",
      "doi": "10.1080/03091902.2025.2548478",
      "authors": "Shahzaib Fnu et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40833926/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40834168",
      "title": "Understanding artificial intelligence in critical care: opportunities, risks, and practical applications.",
      "abstract": "Artificial intelligence technologies are rapidly advancing and significantly impacting healthcare, particularly in critical care environments where rapid, precise decision-making is crucial. They promise reductions in clinical errors, enhanced diagnostic accuracy, optimized treatment plans, and better resource allocation. Artificial intelligence applications are widespread across medical fields, with numerous artificial intelligence/machine learning-enabled medical devices approved by regulatory bodies, like the US Food and Drug Administration, aiding in diagnosis, monitoring, and personalized patient care. However, integrating artificial intelligence into healthcare presents challenges, notably the potential to exacerbate existing biases and disparities, especially when systems are trained on homogeneous datasets lacking diversity. Biased artificial intelligence can negatively affect patient outcomes for underrepresented groups, perpetuating health disparities. Additional concerns include data privacy and security, lack of transparency, algorithmic bias, and regulatory hurdles. Addressing these risks requires ensuring diverse and representative datasets, implementing robust auditing and monitoring practices, enhancing transparency, involving diverse perspectives in artificial intelligence development, and promoting critical thinking among healthcare professionals. Furthermore, the environmental impact of artificial intelligence, huge models reliant on energy-intensive data centers, poses challenges due to increased greenhouse gas emissions and resource consumption, disproportionately affecting low-income countries and exacerbating global inequalities. Systemic changes driven by corporate responsibility, government policy, and adopting sustainable artificial intelligence practices within healthcare are necessary. This narrative review explores the current landscape of artificial intelligence in healthcare, highlighting its potential benefits and delineating associated risks and challenges, underscoring the importance of mitigating biases and environmental impacts to ensure equitable and sustainable integration of artificial intelligence technologies in healthcare settings.",
      "journal": "Critical care science",
      "year": "2025",
      "doi": "10.62675/2965-2774.20250380",
      "authors": "Woite Naira Link et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Critical Care",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40834168/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Diverse/Representative Data; Bias Auditing Framework",
      "clinical_setting": "ICU",
      "key_findings": "Systemic changes driven by corporate responsibility, government policy, and adopting sustainable artificial intelligence practices within healthcare are necessary. This narrative review explores the current landscape of artificial intelligence in healthcare, highlighting its potential benefits and delineating associated risks and challenges, underscoring the importance of mitigating biases and environmental impacts to ensure equitable and sustainable integration of artificial intelligence techno...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12614954"
    },
    {
      "pmid": "40844259",
      "title": "Ethical Reflections on Integrating Artificial Intelligence in Care Practices.",
      "abstract": "BackgroundThe integration of artificial intelligence (AI) and robotics into disability care presents transformative opportunities while simultaneously raising pressing ethical concerns. Issues related to autonomy, human dignity, and equitable access require careful consideration, particularly as these technologies reshape the dynamics of care delivery and clinical relationships. PurposeDrawing on an interdisciplinary approach that synthesizes insights from bioethical literature, illustrative case studies, and expert perspectives from healthcare, law, and technology, this reflection examines the ethical landscape of AI-supported rehabilitation and assistance. Particular attention is given to risks such as algorithmic bias, over-reliance on automation, and the potential erosion of the human dimension in care. A biopsychosocial model serves as a guiding framework to analyze how technological systems intersect with the lived experiences of individuals with disabilities. Ethical tensions emerge around personalized care, transparency in decision-making, and the inclusivity of data and design processes.ConclusionsThe analysis emphasizes the need for governance models that embed ethical safeguards and promote fairness, while also encouraging participatory design involving patients, caregivers, and healthcare professionals. By situating technological developments within broader socio-political and clinical contexts, this reflection identifies pathways toward a more equitable and human-centered integration of AI. Recommendations include investment in inclusive datasets, the development of fairness-aware algorithms, and the establishment of regulatory mechanisms that align innovation with fundamental rights and principles of social justice in healthcare.",
      "journal": "Community health equity research & policy",
      "year": "2025",
      "doi": "10.1177/2752535X251370928",
      "authors": "Ricchezze Giulia et al.",
      "keywords": "artificial intelligence; disability; equity; health care; rehabilitation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40844259/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Pathology",
      "bias_axes": "Gender/Sex; Age; Disability",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability; Diverse/Representative Data",
      "clinical_setting": "ICU",
      "key_findings": "By situating technological developments within broader socio-political and clinical contexts, this reflection identifies pathways toward a more equitable and human-centered integration of AI. Recommendations include investment in inclusive datasets, the development of fairness-aware algorithms, and the establishment of regulatory mechanisms that align innovation with fundamental rights and principles of social justice in healthcare.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (1 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40851812",
      "title": "Reconsidering the use of race, sex, and age in clinical algorithms to address bias in practice: A discussion paper.",
      "abstract": "Clinical algorithms are commonly used as decision-support tools, incorporating patient-specific characteristics to predict health outcomes. Risk calculators are clinical algorithms particularly suited for resource allocation based on risk estimation. Although these calculators typically use physiologic data in estimation, they frequently include demographic variables such as race, sex, and age as well. In recent years, the inclusion of race as an input variable has been scrutinized for being reductive, serving as a poor proxy for biological differences, and contributing to the inequitable distribution of services. Little attention has been given to other demographic features, such as sex and age, and their potential to produce similar consequences. By applying a framework for understanding sources of harm throughout the machine learning life cycle and presenting case studies, this paper aims to examine sources of potential harms (i.e. representational and allocative harm) associated with including sex and age in clinical decision-making algorithms, particularly risk calculators. In doing so, this paper demonstrates how systematic discrimination, reductive measurement practices, and observed differences in risk estimation between demographic groups contribute to representational and allocative harm caused by including sex and age in clinical algorithms used for resource distribution. This paper ultimately, urges clinicians to scrutinize the practice of including reductive demographic features (i.e. race, binary-coded sex, and chronological age) as proxies for underlying biological mechanisms in their risk estimations as it violates the bioethical principles of justice and nonmaleficence. Practicing clinicians, including nurses, must have an underlying model literacy to address potential biases introduced in algorithm development, validation, and clinical practice.",
      "journal": "International journal of nursing studies advances",
      "year": "2025",
      "doi": "10.1016/j.ijnsa.2025.100380",
      "authors": "Panagides Reanna et al.",
      "keywords": "Algorithms; Bias; Clinical decision-making; Literacy; Resource allocation; Social Justice",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40851812/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "race, binary-coded sex, and chronological age) as proxies for underlying biological mechanisms in their risk estimations as it violates the bioethical principles of justice and nonmaleficence. Practicing clinicians, including nurses, must have an underlying model literacy to address potential biases introduced in algorithm development, validation, and clinical practice.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12369485"
    },
    {
      "pmid": "40857554",
      "title": "Comparing Multiple Imputation Methods to Address Missing Patient Demographics in Immunization Information Systems: Retrospective Cohort Study.",
      "abstract": "BACKGROUND: Immunization Information Systems (IIS) and surveillance data are essential for public health interventions and programming; however, missing data are often a challenge, potentially introducing bias and impacting the accuracy of vaccine coverage assessments, particularly in addressing disparities. OBJECTIVE: This study aimed to evaluate the performance of 3 multiple imputation methods, Stata's (StataCorp LLC) multiple imputation using chained equations (MICE), scikit-learn's Iterative-Imputer, and Python's miceforest package, in managing missing race and ethnicity data in large-scale surveillance datasets. We compared these methodologies in their ability to preserve demographic distribution, computational efficiency, and performed G-tests on contingency tables to obtain likelihood ratio statistics to assess the association between race and ethnicity and flu vaccination status. METHODS: In this retrospective cohort study, we analyzed 2021-2022 flu vaccination and demographic data from the West Virginia Immunization Information System (N=2,302,036), where race (15%) and ethnicity (34%) were missing. MICE, Iterative Imputer, and miceforest were used to impute missing variables, generating 15 datasets each. Computational efficiency, demographic distribution preservation, and spatial clustering patterns were assessed using G-statistics. RESULTS: After imputation, an additional 780,339 observations were obtained compared with complete case analysis. All imputation methods exhibited significant spatial clustering for race imputation (G-statistics: MICE=26,452.7, Iterative-Imputer=128,280.3, Miceforest=26,891.5; P<.001), while ethnicity imputation showed variable clustering patterns (G-statistics: MICE=1142.2, Iterative-Imputer=1.7, Miceforest=2185.0; P: MICE<.001, Iterative-Imputer=1.7, Miceforest<.001). MICE and miceforest best preserved the proportional distribution of demographics. Computational efficiency varied, with MICE requiring 14 hours, Iterative Imputer 2 minutes, and miceforest 10 minutes for 15 imputations. Postimputation estimates indicated a 0.87%-18% reduction in stratified flu vaccination coverage rates. Overall estimated flu vaccination rates decreased from 26% to 19% after imputations. CONCLUSIONS: Both MICE and Miceforest offer flexible and reliable approaches for imputing missing demographic data while mitigating bias compared with Iterative-Imputer. Our results also highlight that the imputation method can profoundly affect research findings. Though MICE and Miceforest had better effect sizes and reliability, MICE was much more computationally and time-expensive, limiting its use in large, surveillance datasets. Miceforest can use cloud-based computing, which further enhances efficiency by offloading resource-intensive tasks, enabling parallel execution, and minimizing processing delays. The significant decrease in vaccination coverage estimates validates how incomplete or missing data can eclipse real disparities. Our findings support regular application of imputation methods in immunization surveillance to improve health equity evaluations and shape targeted public health interventions and programming.",
      "journal": "JMIR public health and surveillance",
      "year": "2025",
      "doi": "10.2196/73916",
      "authors": "Brown Sara et al.",
      "keywords": "data science; immunization information system; imputation methods; machine learning; missing data; multiple imputation; statistical modeling",
      "mesh_terms": "Retrospective Studies; Humans; Female; Male; Information Systems; Demography; Child, Preschool; Cohort Studies; Adolescent; Child; Immunization; Adult; Infant; Middle Aged",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40857554/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clustering",
      "health_domain": "ICU/Critical Care; Pediatrics; Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSIONS: Both MICE and Miceforest offer flexible and reliable approaches for imputing missing demographic data while mitigating bias compared with Iterative-Imputer. Our results also highlight that the imputation method can profoundly affect research findings. Though MICE and Miceforest had better effect sizes and reliability, MICE was much more computationally and time-expensive, limiting its use in large, surveillance datasets.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12380239"
    },
    {
      "pmid": "40880105",
      "title": "Algorithms to Improve Fairness in Medicare Risk Adjustment.",
      "abstract": "IMPORTANCE: Payment system design creates incentives that affect health care spending, access, and outcomes. With Medicare Advantage accounting for more than half of Medicare spending, changes to its risk adjustment algorithm have the potential for broad consequences. OBJECTIVE: To assess the potential for algorithmic tools to achieve more equitable plan payment for Medicare risk adjustment while maintaining current levels of performance, flexibility, feasibility, transparency, and interpretability. DESIGN, SETTING, AND PARTICIPANTS: This diagnostic study included a retrospective analysis of traditional Medicare enrollment and claims data generated between January 1, 2017, and December 31, 2020, from a random 20% sample of non-dual-eligible Medicare beneficiaries with documented residence in the US or Puerto Rico. Race and ethnicity were designated using the Research Triangle Institute enhanced indicator. Diagnoses in claims were mapped to hierarchical condition categories. Algorithms used demographic indicators and hierarchical condition categories from 1 calendar year to predict Medicare spending in the subsequent year. Data analysis was conducted between August 16, 2023, and January 27, 2025. MAIN OUTCOMES AND MEASURES: The main outcome was prospective health care spending by Medicare. Overall performance was measured by payment system fit and mean absolute error. Net compensation was used to assess group-level fairness. RESULTS: The main analysis of Medicare risk adjustment algorithms included 4\u202f398\u202f035 Medicare beneficiaries with a mean (SD) age of 75.2 (7.4) years and mean (SD) annual Medicare spending of $8345 ($18\u202f581); 44% were men; fewer than 1% were American Indian or Alaska Native, 2% were Asian or Other Pacific Islander, 6% were Black, 3% were Hispanic, 86% were non-Hispanic White, and 1% were part of an additional group (termed as other in the Centers for Medicare & Medicaid Services data). Out-of-sample payment system fit for the baseline regression was 12.7%. Constrained regression and postprocessing both achieved fair spending targets while maintaining payment system fit (constrained regression, 12.6%; postprocessing, 12.7%). Whereas postprocessing increased mean payments for beneficiaries in minoritized racial and ethnic groups (American Indian or Alaska Native, Asian or Other Pacific Islander, Black, and Hispanic individuals) only, constrained regression increased mean payments for beneficiaries in minoritized racial and ethnic groups and beneficiaries in other groups residing in counties with greater exposure to socioeconomic factors that can adversely affect health outcomes. CONCLUSIONS AND RELEVANCE: Results of this study suggest that constrained regression and postprocessing can incorporate fairness objectives into the Medicare risk adjustment algorithm with minimal reduction in overall fit. These feasible changes to the Medicare risk adjustment algorithm could be considered by policymakers aiming to address health care disparities through payment system reform.",
      "journal": "JAMA health forum",
      "year": "2025",
      "doi": "10.1001/jamahealthforum.2025.2640",
      "authors": "Reitsma Marissa B et al.",
      "keywords": "",
      "mesh_terms": "Humans; United States; Algorithms; Risk Adjustment; Medicare; Male; Retrospective Studies; Female; Aged; Health Expenditures; Aged, 80 and over; Medicare Part C",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40880105/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: The main analysis of Medicare risk adjustment algorithms included 4\u202f398\u202f035 Medicare beneficiaries with a mean (SD) age of 75.2 (7.4) years and mean (SD) annual Medicare spending of $8345 ($18\u202f581); 44% were men; fewer than 1% were American Indian or Alaska Native, 2% were Asian or Other Pacific Islander, 6% were Black, 3% were Hispanic, 86% were non-Hispanic White, and 1% were part of an additional group (termed as other in the Centers for Medicare & Medicaid Services data). Out-of-sam...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12397885"
    },
    {
      "pmid": "40895087",
      "title": "Improving the FAIRness and Sustainability of the NHGRI Resources Ecosystem.",
      "abstract": "In 2024, individuals funded by NHGRI to support genomic community resources completed a Self-Assessment Tool (SAT) to evaluate their application of the FAIR (Findable, Accessible, Interoperable, and Reusable) principles and assess their sustainability. By collecting insights from the self-administered questionnaires and conducting personal interviews, a valuable perspective was gained on the FAIRness and sustainability of the NHGRI resources. The results highlighted several challenges and key areas the NHGRI resource community could improve by working together to form recommendations to address these challenges. The next step was the formation of an Organizing Committee to identify which challenges could lead to best practices or guidelines for the community. The workshop's Organizing Committee comprised four members from the NHGRI resource community: Carol Bult, PhD, Chris Mungall, PhD, Heidi Rehm, PhD, and Michael Schatz, PhD. In December 2024, the Organizing Committee engaged with the NHGRI resource community to refine these challenges further, inviting feedback on potential focus areas for a future workshop. This collaborative approach led to two informative webinars in December 2024, highlighting specific challenges in data curation, data processing, metadata tools, and variant identifiers within the NHGRI resources. Throughout the workshop planning process, the four Organizing Committee members worked together to create and develop themes, design breakout sessions, and create a detailed agenda. The workshop's agenda was intentionally structured to ensure participants could generate implementable recommendations for the NHGRI resource community. The two-day workshop was held in Bethesda, MD, on March 3-4, 2025. The challenges received from NHGRI resources were classified into four key categories, forming the basis of the workshop. The four key categories are variant identifiers, data processing, data curation, and metadata tools. They are briefly described below, with greater details on their challenges and recommendations in subsequent sections. Metadata Tools:While metadata is vital for capturing context in genomic datasets, its usage and relevance can vary by domain, making it difficult to standardize usage. While various methods exist for annotating and extracting metadata, incomplete or inconsistent annotations often result in ineffective data sharing and interoperability, further reducing data usability and reproducibility.Data Curation:Curation of annotations for genomics data is critical for FAIR-ness. Scalable curation solutions are challenging because of the multiple components for curation, including harmonizing data sets, data cleaning, and annotation. The workshop focused on identifying which aspects of data curation could be streamlined using computational methods while considering the barriers to increased automation.Variant Identifiers:Variant identifiers are standardized representations of genetic variants, crucial for sharing and interpreting genomic data in research and clinical work. They ensure consistent referencing and enable data aggregation. Standardizing variant identifiers is difficult due to varied formats, complex data, and distinct environments for generating and disseminating data.Data Processing:Data processing is a necessary first step in a FAIR environment. As there are many variant workflows, streamlining this process will ensure greater accuracy, reproducibility, interoperability, and FAIRness, driving advancements in clinical research. The workshop focused on addressing these aspects with a key focus on improvements and best practices around data processing for an NHGRI resource. Several recommendations were made throughout the workshop's interactive sessions with the resources' participants. While many recommendations were specific to data processing, data curation, metadata tools, or variant identifiers, they can be grouped into core recommendations addressing common challenges within the NHGRI resource community. These core recommendations highlight the key themes that emerged across sessions and are listed in the nine recommendations below. Increase transparency to enable effective sharing/reproducibility (documenting, benchmarking, publishing, mapping)Develop entity schema and ontology mapping tools (between models, identifiers, etc.)Annotate tools using resources to increase findability and reuse (Examples: EDAM Ontology of Bioscientific data analysis and data management)Use standard nomenclature and identifiersMake workflows usable by researchers with limited programming expertiseImplement APIs to improve data connectivityPresent data in an interpretable manner, along with machine readabilityDevelop artificial intelligence/machine learning (AI/ML) methods for scaling curation processesAssess the impact of resources using an independent group that can assess return on investment and impact to health and scientific advancement. An additional key collaborative outcome was the development of Appendix A, which outlines ongoing and future efforts, including additional workshops, webinars, and meetings through the listed events provided by the NHGRI resource community. We hope that these activities will enable further advances in the implementation of FAIR standards and continue to foster collaboration and exchange across NHGRI resources and the global community.",
      "journal": "ArXiv",
      "year": "2025",
      "doi": "",
      "authors": "Babb Larry et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40895087/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Generative AI",
      "health_domain": "ICU/Critical Care; Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "An additional key collaborative outcome was the development of Appendix A, which outlines ongoing and future efforts, including additional workshops, webinars, and meetings through the listed events provided by the NHGRI resource community. We hope that these activities will enable further advances in the implementation of FAIR standards and continue to foster collaboration and exchange across NHGRI resources and the global community.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12393232"
    },
    {
      "pmid": "40898015",
      "title": "Artificial intelligence in headache medicine: between automation and the doctor-patient relationship. A systematic review.",
      "abstract": "BACKGROUND: Headache disorders, particularly migraine, are highly prevalent, but often remain underdiagnosed and undertreated. Artificial intelligence (AI) offers promising applications in diagnosis, prediction of attacks, analysis of neuroimaging and neurophysiology data, and treatment selection. Its use in headache medicine raises ethical, regulatory, and clinical questions, including its impact on the doctor-patient relationship. METHODS: A systematic literature search was conducted on April 10, 2025, across PubMed, Cochrane Library, Scopus, Web of Science, and DOAJ, following PRISMA guidelines. Two reviewers independently applied strict inclusion criteria to select studies published from 2000 to 2025 in either English or Spanish. Risk of bias was assessed using validated tools tailored to study design, including the Quality Assessment of Diagnostic Accuracy Studies-2 (QUADAS-2), Prediction Model Risk of Bias Assessment Tool (PROBAST), Newcastle-Ottawa Scale (NOS), and Appraisal Tool for Cross-Sectional Studies (AXIS). RESULTS: A total of 76 studies were included in the qualitative synthesis. The analysis covered AI methodologies, clinical applications, patient perspectives, and ethical implications. AI tools have shown potential to improve diagnostic accuracy, headache subtype classification, and prediction of treatment response, and may help reduce the administrative burden in clinical practice. Emerging technologies such as digital twins, wearable biomarker monitoring, and synthetic data generation support personalized approaches and may reshape clinical research. However, significant challenges remain. These include data quality, model interpretability, algorithmic bias, privacy concerns, and regulatory gaps. Moreover, the evidence base is still developing, with expectations often exceeding the strength of available clinical data. Many studies present methodological limitations due to small sample sizes, selection bias, and lack of external validation, which limit their generalizability to real-world settings. Finally, concerns about depersonalization and transparency affect patient trust in AI, reinforcing the need for both human oversight and a patient-centered approach. CONCLUSIONS: AI holds promise for improving headache care, but evidence supporting its clinical utility is still limited. Integration into practice must be rigorously validated, ethically guided, and carefully designed to prevent depersonalization. Human oversight remains essential as AI should complement, not replace, clinical judgment.",
      "journal": "The journal of headache and pain",
      "year": "2025",
      "doi": "10.1186/s10194-025-02143-8",
      "authors": "Espinoza-Vinces Christian et al.",
      "keywords": "Artificial intelligence; Doctor-Patient relationship; Headache disorders; Human-AI interaction; Machine learning",
      "mesh_terms": "Humans; Artificial Intelligence; Headache; Headache Disorders; Physician-Patient Relations",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40898015/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "ICU/Critical Care; Wearables/Remote Monitoring",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Data Augmentation; Explainability/Interpretability",
      "clinical_setting": "ICU; Telehealth/Remote",
      "key_findings": "CONCLUSIONS: AI holds promise for improving headache care, but evidence supporting its clinical utility is still limited. Integration into practice must be rigorously validated, ethically guided, and carefully designed to prevent depersonalization. Human oversight remains essential as AI should complement, not replace, clinical judgment.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12406602"
    },
    {
      "pmid": "40898192",
      "title": "Exploring nurse perspectives on AI-based shift scheduling for fairness, transparency, and work-life balance.",
      "abstract": "INTRODUCTION: Work-life balance (WLB) is critical to nurse retention and job satisfaction in healthcare. Traditional shift scheduling, characterised by inflexible hours and limited employee control, often leads to stress and perceptions of unfairness, contributing to high turnover rates. AI-based scheduling systems are promoted as a promising solution by enabling fairer and more transparent shift distribution. This study explored the perspectives of nurse leaders, permanent nurses, and temporary nurses on the perceived fairness, transparency, and impact on WLB of AI-based shift scheduling systems, which they had not yet used. METHODS: A qualitative study design was used, with focus group (FG) interviews conducted between May and June 2024. FG interviews were conducted with 21 participants from acute hospitals, home care services, and nursing homes between May and June 2024. The interviews were analyzed using the knowledge mapping method, which allowed for a visual representation of key discussion points and highlighted consensus among participants. The discussions centered on five main themes: (1) experiences with current scheduling systems, (2) requirements for work scheduling, (3) fair and participatory work scheduling, (4) requirements for AI in work scheduling, and (5) perceived advantages and disadvantages of AI-based work scheduling. RESULTS: Participants reported that current scheduling practices often lacked fairness and transparency, leading to dissatisfaction, particularly among permanent nurses. While temporary staff appreciated the flexibility in their schedules, permanent nurses expressed a desire for more autonomy and fairness in shift allocation. AI-based scheduling has the potential to improve shift equity by objectively managing shifts based on pre-defined criteria, thereby reducing bias and administrative burden. However, participants raised concerns about the depersonalisation of scheduling, emphasising the need for human oversight to consider the emotional and contextual factors that AI systems may overlook. CONCLUSION: AI-based scheduling systems were perceived as having the potential to be beneficial in improving fairness, transparency and WLB for nurses. However, the integration of these systems must be accompanied by careful consideration of the human element and ongoing collaboration with healthcare professionals to ensure that the technology is aligned with organisational needs. By striking a balance between AI-driven efficiency and human judgement, healthcare organisations can improve nurse satisfaction and retention, ultimately benefiting patient care and organisational efficiency.",
      "journal": "BMC nursing",
      "year": "2025",
      "doi": "10.1186/s12912-025-03808-0",
      "authors": "Gerlach Maisa et al.",
      "keywords": "Artificial intelligence; Co-creation; Fairness; Nurse retention; Participation; Shift scheduling; Transparency; Work-life balance",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40898192/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Generative AI",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; ICU; Long-term Care",
      "key_findings": "CONCLUSION: AI-based scheduling systems were perceived as having the potential to be beneficial in improving fairness, transparency and WLB for nurses. However, the integration of these systems must be accompanied by careful consideration of the human element and ongoing collaboration with healthcare professionals to ensure that the technology is aligned with organisational needs. By striking a balance between AI-driven efficiency and human judgement, healthcare organisations can improve nurse s...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12406402"
    },
    {
      "pmid": "40926431",
      "title": "The role of AI for improved management of breast cancer: Enhanced diagnosis and health disparity mitigation.",
      "abstract": "Breast Cancer (BC) remains a leading cause of morbidity and mortality among women globally, accounting for 30% of all new cancer cases (with approximately 44,000 women dying), according to recent American Cancer Society reports. Therefore, accurate BC screening, diagnosis, and classification are crucial for timely interventions and improved patient outcomes. The main goal of this paper is to provide a comprehensive review of the latest advancements in BC detection, focusing on diagnostic BC imaging, Artificial Intelligence (AI) driven analysis, and health disparity considerations. We first examine diverse imaging techniques such as Mammography, Ultrasound, and Dynamic Contrast-Enhanced Magnetic Resonance Imaging, and provide an overview of their pros and cons. Then, we provided an intensive review of the State-of-the-Art (SOTA) literature on the role of AI in BC classification and segmentation. Lastly, we examined the role of AI in BC health disparities. A key contribution of this work lies in its integrative approach, consolidating insights from multiple research areas, imaging methods, AI-driven methodologies, and health disparities in a single resource. This paper evaluates the effectiveness of modern AI-based tools in enhancing diagnostic accuracy and discusses their potential to address biases in BC diagnosis, thus promoting equitable healthcare access. By integrating clinical, technical, and equity perspectives, this review aims to inform real-world decision-making, supporting the development of bias-aware AI tools, guiding equitable screening policy, and enhancing clinical practice in breast cancer care. Additionally, our critical analysis and discussion of recent SOTA highlights the strengths, limitations, and knowledge gaps for future directions of AI roles in BC. In total, these findings and future venue suggestions serve as a practical reference for researchers, clinicians, and policymakers, underscoring the need for interdisciplinary collaboration to harness AI's full potential in BC diagnosis and reduce global health disparities.",
      "journal": "Computer methods and programs in biomedicine",
      "year": "2025",
      "doi": "10.1016/j.cmpb.2025.109036",
      "authors": "Akinniyi Oluwatunmise et al.",
      "keywords": "Breast cancer; Health disparity; Mammograms; Multi-modal",
      "mesh_terms": "Humans; Breast Neoplasms; Female; Artificial Intelligence; Mammography; Healthcare Disparities; Magnetic Resonance Imaging; Early Detection of Cancer",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40926431/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Oncology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Additionally, our critical analysis and discussion of recent SOTA highlights the strengths, limitations, and knowledge gaps for future directions of AI roles in BC. In total, these findings and future venue suggestions serve as a practical reference for researchers, clinicians, and policymakers, underscoring the need for interdisciplinary collaboration to harness AI's full potential in BC diagnosis and reduce global health disparities.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12462113"
    },
    {
      "pmid": "40930712",
      "title": "Relational accountability in AI-driven pharmaceutical practices: an ethics approach to bias, inequity and structural harm.",
      "abstract": "The integration of artificial intelligence (AI) into pharmaceutical practices raises critical ethical concerns, including algorithmic bias, data commodification and global health inequities. While existing AI ethics frameworks emphasise transparency and fairness, they often overlook structural vulnerabilities tied to race, gender and socioeconomic status. This paper introduces relational accountability-a feminist ethics framework-to critique AI-driven pharmaceutical practices, arguing that corporate reliance on biased algorithms exacerbates inequalities by design. Through case studies of Pfizer-IBM Watson's immuno-oncology collaboration and Google DeepMind's National Health Service partnership, we demonstrate how AI entrenches disparities in drug pricing, access and development. We propose a causal pathway linking biased training data to inequitable health outcomes, supported by empirical evidence of AI-driven price discrimination and exclusionary clinical trial recruitment algorithms. Policy solutions, including algorithmic audits and equity-centred data governance, are advanced to realign AI with the ethical imperative. This work bridges feminist bioethics and AI governance, offering a novel lens to address structural harm in healthcare innovation.",
      "journal": "Journal of medical ethics",
      "year": "2025",
      "doi": "10.1136/jme-2025-110913",
      "authors": "Biswas Irfan",
      "keywords": "Quality of Health Care",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40930712/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; Drug Discovery/Pharmacology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Bias Auditing Framework",
      "clinical_setting": "Clinical Trial",
      "key_findings": "Policy solutions, including algorithmic audits and equity-centred data governance, are advanced to realign AI with the ethical imperative. This work bridges feminist bioethics and AI governance, offering a novel lens to address structural harm in healthcare innovation.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40932499",
      "title": "Artificial intelligence in gastric cancer: a systematic review of machine learning and deep learning applications.",
      "abstract": "BACKGROUND: Gastric cancer (GC) remains a major global health concern, ranking as the fifth most prevalent malignancy and the fourth leading cause of cancer-related mortality worldwide. Although early detection can increase the 5-year survival rate of early gastric cancer (EGC) to over 90%, more than 80% of cases are diagnosed at advanced stages due to subtle clinical symptoms and diagnostic challenges. Artificial intelligence (AI), particularly machine learning (ML) and deep learning (DL), has shown great promise in addressing these limitations. OBJECTIVES: This systematic review aims to evaluate the performance, applications, and limitations of ML and DL models in GC management, with a focus on their use in detection, diagnosis, treatment planning, and prognosis prediction across diverse clinical imaging and data modalities. METHODS: Following the PRISMA 2020 guidelines, a comprehensive literature search was conducted in MEDLINE, Web of Science, and Scopus for studies published between 2004 and May 2025. Eligible studies applied ML or DL algorithms for diagnostic or prognostic tasks in GC using data from endoscopy, computed tomography (CT), pathology, or multi-modal sources. Two reviewers independently performed study selection, data extraction, and risk of bias assessment. RESULTS: A total of 59 studies met the inclusion criteria. DL models, particularly convolutional neural networks (CNNs), demonstrated strong performance in EGC detection, with reported sensitivities up to 95.3% and Area Under the Curve (AUCs) as high as 0.981, often exceeding expert endoscopists. CT-based radiomics and DL models achieved AUCs ranging from 0.825 to 0.972 for tumor staging and metastasis prediction. Pathology-based models reported accuracies up to 100% for EGC detection and AUCs up to 0.92 for predicting treatment response. Cross-modality approaches combining radiomics and pathomics achieved AUCs up to 0.951. Key challenges included algorithmic bias, limited dataset diversity, interpretability issues, and barriers to clinical integration. CONCLUSION: ML and DL models have demonstrated substantial potential to improve early detection, diagnostic accuracy, and individualized treatment in GC. To advance clinical adoption, future research should prioritize the development of large, diverse datasets, implement explainable AI frameworks, and conduct prospective clinical trials. These efforts will be essential for integrating AI into precision oncology and addressing the increasing global burden of gastric cancer.",
      "journal": "Abdominal radiology (New York)",
      "year": "2025",
      "doi": "10.1007/s00261-025-05181-7",
      "authors": "Alsallal Muna et al.",
      "keywords": "Artificial intelligence; Deep learning; Gastric cancer; Machine learning; Medical imaging; Prognosis prediction",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40932499/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Oncology; ICU/Critical Care; Pathology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability; Diverse/Representative Data",
      "clinical_setting": "ICU; Clinical Trial; Laboratory/Pathology",
      "key_findings": "CONCLUSION: ML and DL models have demonstrated substantial potential to improve early detection, diagnostic accuracy, and individualized treatment in GC. To advance clinical adoption, future research should prioritize the development of large, diverse datasets, implement explainable AI frameworks, and conduct prospective clinical trials. These efforts will be essential for integrating AI into precision oncology and addressing the increasing global burden of gastric cancer.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (1 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40938311",
      "title": "Enhanced guidance on artificial intelligence for medical publication and communication professionals.",
      "abstract": "The International Society for Medical Publication Professionals (ISMPP) position statement and call to action on the use of artificial intelligence (AI), published in 2024, recognized the value of AI while advocating for best practices to guide its use. In this commentary, we offer enhanced guidance on the call to action for ISMPP members and other medical communication professionals on the topics of education and training, implementation and use, and advocacy and community engagement. With AI rapidly revolutionizing scientific communication, members should stay up to date with advancements in the field by completing AI training courses, engaging with ISMPP AI education and training and other external training platforms, developing a practice of lifelong learning, and improving AI literacy. Members can successfully integrate and use AI by complying with organizational policies, ensuring fair access to AI models, complying with authorship guidance, properly disclosing the use of AI models or tools, respecting academic integrity and copyright restrictions, and understanding privacy protections. Members also need to be familiar with the systemic problem of bias with large language models, which can reinforce health inequities, as well as the limits of transparency and explainability with AI models, which can undermine source verification, bias detection, and even scientific integrity. AI models can produce hallucinations, results that are factually incorrect, irrelevant, or nonsensical, which is why all outputs from AI models should be reviewed and verified for accuracy by humans. With respect to advocacy and community engagement, members should advocate for the responsible use of AI, participate in developing AI policy and governance, work with underserved communities to get access to AI tools, and share findings for AI use cases or research results in peer-reviewed journals, conferences, and other professional platforms.",
      "journal": "Current medical research and opinion",
      "year": "2025",
      "doi": "10.1080/03007995.2025.2556012",
      "authors": "Goldman Keith et al.",
      "keywords": "Artificial intelligence; call\u00a0to\u00a0action; enhanced guidance; medical communications; medical publication professional",
      "mesh_terms": "Artificial Intelligence; Humans; Communication",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40938311/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "NLP/LLM; Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population; Safety-Net/Underserved",
      "key_findings": "AI models can produce hallucinations, results that are factually incorrect, irrelevant, or nonsensical, which is why all outputs from AI models should be reviewed and verified for accuracy by humans. With respect to advocacy and community engagement, members should advocate for the responsible use of AI, participate in developing AI policy and governance, work with underserved communities to get access to AI tools, and share findings for AI use cases or research results in peer-reviewed journals...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (1 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40965022",
      "title": "Developments in the Management Strategies for Allergy: Advances in Artificial Intelligence and Future Perspectives.",
      "abstract": "INTRODUCTION: Artificial intelligence (AI) is rapidly transforming biomedical research by offering advanced tools to analyse complex datasets. In the field of allergy studies, however, the translation of AI-generated insights into clinical practice remains limited and underutilised. METHOD: This review critically discussed the current applications of AI in allergy studies. It focuses on the methodological foundations of AI, including machine learning and clustering algorithms, and assesses their practical benefits and limitations. Representative case studies are explored to demonstrate real-world applications, and challenges in data quality, integration, and algorithmic fairness are examined. RESULTS: AI techniques have shown promise in tasks such as disease phenotyping and patient stratification within allergy research. Case studies reveal that AI can uncover immunological insights and support precision medicine approaches. However, the field faces challenges, including fragmented data sources, algorithmic bias, and the limited presence of therapeutic AI tools in clinical practice. DISCUSSION: Despite the demonstrated potential, several barriers hinder the broader adoption of AI in allergy care. These include the need for high-quality, standardised datasets, ethical oversight, and transparent methodologies. The review highlights the importance of these factors in ensuring the reliability, reproducibility, and equity of AI-driven interventions in allergy research. CONCLUSION: AI holds significant promise for improving diagnostic accuracy and enabling personalised treatment strategies in allergy care. Realising its full potential will require robust frameworks, ethical governance, and interdisciplinary collaboration to overcome current limitations and drive clinical translation.",
      "journal": "Anti-inflammatory & anti-allergy agents in medicinal chemistry",
      "year": "2025",
      "doi": "10.2174/0118715230389406250906142050",
      "authors": "Kumar Suraj et al.",
      "keywords": "Allergy; allergy management; artificial intelligence; deep learning; diagnosis; machine learning; precision medicine.",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40965022/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Clustering",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: AI holds significant promise for improving diagnostic accuracy and enabling personalised treatment strategies in allergy care. Realising its full potential will require robust frameworks, ethical governance, and interdisciplinary collaboration to overcome current limitations and drive clinical translation.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40965098",
      "title": "Advancing equity in generative AI dermatology requires representative data and transparent evaluation.",
      "abstract": "",
      "journal": "Journal of the European Academy of Dermatology and Venereology : JEADV",
      "year": "2025",
      "doi": "10.1111/jdv.70052",
      "authors": "Kabakova Margaret et al.",
      "keywords": "algorithmic bias; artificial intelligence; dermatology; health equity; machine learning; skin pigmentation",
      "mesh_terms": "",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40965098/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Generative AI",
      "health_domain": "Dermatology",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40969781",
      "title": "Using a Large Language Model (ChatGPT-4o) to Assess the Risk of Bias in Randomized Controlled Trials of Medical Interventions: Interrater Agreement With Human Reviewers.",
      "abstract": "BACKGROUND: Risk of bias (RoB) assessment is a highly skilled task that is time-consuming and subject to human error. RoB automation tools have previously used machine learning models built using relatively small task-specific training sets. Large language models (LLMs; e.g., ChatGPT) are complex models built using non-task-specific Internet-scale training sets. They demonstrate human-like abilities and might be able to support tasks like RoB assessment. METHODS: Following a published peer-reviewed protocol, we randomly sampled 100 Cochrane reviews. New or updated reviews that evaluated medical interventions, included \u2265\u20091 eligible trial, and presented human consensus assessments using Cochrane RoB1 or RoB2 were eligible. We excluded reviews performed under emergency conditions (e.g., COVID-19), and those on public health or welfare. We randomly sampled one trial from each review. Trials using individual- or cluster-randomized designs were eligible. We extracted human consensus RoB assessments of the trials from the reviews, and methods texts from the trials. We used 25 review-trial pairs to develop a ChatGPT prompt to assess RoB using trial methods text. We used the prompt and the remaining 75 review-trial pairs to estimate human-ChatGPT agreement for \"Overall RoB\" (primary outcome) and \"RoB due to the randomization process\", and ChatGPT-ChatGPT (intrarater) agreement for \"Overall RoB\". We used ChatGPT-4o (February 2025) throughout. RESULTS: The 75 reviews were sampled from 35 Cochrane review groups, and all used RoB1. The 75 trials spanned five decades, and all but one were published in English. Human-ChatGPT agreement for \"Overall RoB\" assessment was 50.7% (95% CI 39.3%-62.0%), substantially higher than expected by chance (p\u2009=\u20090.0015). Human-ChatGPT agreement for \"RoB due to the randomization process\" was 78.7% (95% CI 69.4%-88.0%; p\u2009<\u20090.001). ChatGPT-ChatGPT agreement was 74.7% (95% CI 64.8%-84.6%; p\u2009<\u20090.001). CONCLUSIONS: ChatGPT appears to have some ability to assess RoB and is unlikely to be guessing or \"hallucinating\". The estimated agreement for \"Overall RoB\" is well above estimates of agreement reported for some human reviewers, but below the highest estimates. LLM-based systems for assessing RoB may be able to help streamline and improve evidence synthesis production.",
      "journal": "Cochrane evidence synthesis and methods",
      "year": "2025",
      "doi": "10.1002/cesm.70048",
      "authors": "Rose Christopher James et al.",
      "keywords": "ChatGPT; LLM; RoB; artificial intelligence; evidence synthesis; large language model; risk of bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40969781/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Pulmonology; Public Health",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Clinical Trial",
      "key_findings": "CONCLUSIONS: ChatGPT appears to have some ability to assess RoB and is unlikely to be guessing or \"hallucinating\". The estimated agreement for \"Overall RoB\" is well above estimates of agreement reported for some human reviewers, but below the highest estimates. LLM-based systems for assessing RoB may be able to help streamline and improve evidence synthesis production.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12442625"
    },
    {
      "pmid": "40978155",
      "title": "Can AI Bridge or Widen Maternal Health Inequities?",
      "abstract": "Artificial intelligence (AI) is rapidly transforming maternal healthcare through tools like risk prediction algorithms, telemedicine platforms, and postpartum support chatbots. Although these innovations offer promise, particularly in low- and middle-income countries (LMICs), their impact on health equity remains contested. This commentary explores how AI can either bridge or widen maternal health inequities, depending on how it is designed, governed, and implemented. We introduce a conceptual framework comprising four interdependent domains that shape equity outcomes in maternal health: inclusive data practices, equitable governance, participatory design, and local capacity-building. Drawing from interdisciplinary literature, we situate AI within broader health and social systems and argue for equity-oriented approaches that foreground representation, accountability, and community engagement. By examining both opportunities and risks, this commentary offers practical, context-sensitive recommendations for LMICs to ensure AI serves as a tool for justice in maternal healthcare.",
      "journal": "Public health challenges",
      "year": "2025",
      "doi": "10.1002/puh2.70119",
      "authors": "Laguitan Reuben Victor M et al.",
      "keywords": "algorithmic bias; artificial intelligence (AI) in healthcare; health equity; low\u2010 and middle\u2010income countries (LMICs); maternal health",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40978155/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "ICU/Critical Care; Obstetrics/Maternal Health",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability; Diverse/Representative Data",
      "clinical_setting": "ICU; Public Health/Population; Telehealth/Remote",
      "key_findings": "Drawing from interdisciplinary literature, we situate AI within broader health and social systems and argue for equity-oriented approaches that foreground representation, accountability, and community engagement. By examining both opportunities and risks, this commentary offers practical, context-sensitive recommendations for LMICs to ensure AI serves as a tool for justice in maternal healthcare.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12445195"
    },
    {
      "pmid": "40981366",
      "title": "The FAIR framework: ethical hybrid peer review.",
      "abstract": "OBJECTIVES: Traditional peer review faces critical challenges including systematic bias, prolonged delays, reviewer fatigue, and lack of transparency. These failures violate ethical obligations of beneficence, justice, and autonomy while hindering scientific progress and costing billions annually in academic\u00a0labor. To propose an ethically-guided hybrid peer review system that integrates generative artificial intelligence with human expertise while addressing fundamental shortcomings of current review processes. METHODS: We developed the FAIR Framework (Fairness, Accountability, Integrity, and Responsibility) through systematic analysis of peer review failures and integration of AI capabilities. The framework employs standardized prompt engineering to guide AI evaluation of manuscripts while maintaining human oversight throughout all stages. RESULTS: FAIR addresses bias through algorithmic detection and standardized evaluation protocols, ensures accountability via transparent audit trails and documented decisions, maintains integrity through secure local AI processing and confidentiality safeguards, and upholds responsibility through ethical oversight and constructive feedback mechanisms. The hybrid model automates repetitive tasks including initial screening, methodological verification, and plagiarism detection while preserving human judgment for novelty assessment, ethical evaluation, and final decisions. CONCLUSIONS: The FAIR Framework offers a principled solution to peer review inefficiencies by combining AI-enabled consistency and speed with essential human expertise. This hybrid approach reduces review delays, eliminates systematic bias, and enhances transparency while maintaining confidentiality and editorial control. Implementation could significantly reduce the estimated 100 million hours of global reviewer time annually while improving review quality and equity across diverse research communities.",
      "journal": "Journal of perinatal medicine",
      "year": "2025",
      "doi": "10.1515/jpm-2025-0285",
      "authors": "Gr\u00fcnebaum Amos et al.",
      "keywords": "artificial intelligence; hybrid systems; medical publishing; peer review; research ethics; scientific publishing",
      "mesh_terms": "Humans; Artificial Intelligence; Peer Review, Research",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40981366/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Bias Auditing Framework",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: The FAIR Framework offers a principled solution to peer review inefficiencies by combining AI-enabled consistency and speed with essential human expertise. This hybrid approach reduces review delays, eliminates systematic bias, and enhances transparency while maintaining confidentiality and editorial control. Implementation could significantly reduce the estimated 100 million hours of global reviewer time annually while improving review quality and equity across diverse research com...",
      "ft_include": false,
      "ft_reason": "Not health-related in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41001459",
      "title": "Racial and Ethnic Disparities in Brain Age Algorithm Performance: Investigating Bias Across Six Popular Methods.",
      "abstract": "Brain age algorithms, which estimate biological aging from neuroimaging data, are increasingly used as biomarkers for health and disease. However, most algorithms are trained on datasets with limited racial and ethnic diversity, raising concerns about potential algorithmic bias that could exacerbate health disparities. To probe this potential, we evaluated six popular brain age algorithms using data from the Health and Aging Brain Study-Health Disparities (HABS-HD), comprising 1,123 White American, 1,107 Hispanic American, and 678 African American participants, ages \u226550. Comparing correlations between brain age and chronological age across racial/ethnic groups, relations were consistently weaker for African American participants compared to White and Hispanic American participants across most algorithms (ranging from r=0.51-0.85 for African Americans vs. r=0.57-0.89 for other groups). We also examined error for brain age v. chronological age and found significant differences in median errors across racial/ethnic groups, though specific patterns varied by algorithm. Sensitivity models weighting for age, sex, and scan quality noted similar patterns, with all algorithms maintaining significant differences in correlation or median prediction error between groups. Our findings reveal systematic performance differences in brain age algorithms across racial and ethnic groups, with most algorithms consistently showing reduced algorithm accuracy for African American and/or Hispanic-American participants. These biases, which are likely introduced at multiple stages of algorithm development, could impact clinical utility and diagnostic accuracy. Results highlight the urgent need for more inclusive algorithm development and validation to ensure equitable healthcare applications of neuroimaging biomarkers.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.09.18.25336117",
      "authors": "Adkins Dorthea J et al.",
      "keywords": "algorithm; brain age; chronological age; correlation; ethnicity; median error; race",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41001459/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "These biases, which are likely introduced at multiple stages of algorithm development, could impact clinical utility and diagnostic accuracy. Results highlight the urgent need for more inclusive algorithm development and validation to ensure equitable healthcare applications of neuroimaging biomarkers.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12458499"
    },
    {
      "pmid": "41006145",
      "title": "Not All EPAs Are Created Equal: Fixing Sampling Bias With Utility Modeling.",
      "abstract": "BACKGROUND: Entrustable professional activities (EPAs) are foundational for understanding resident progress towards practice readiness. Unfortunately, when EPAs were initiated manually, EPA assessment completion has been uneven, creating biases from assessment variability across individuals, specialties, and institutions. Therefore, we introduce EPA assessment utility modeling, which can retrospectively correct for and prospectively avoid these biases by informing each attending of the usefulness of each EPA assessment opportunity and highlighting when EPA assessments are most needed. METHODS: We performed a longitudinal analysis of general surgery EPA assessments using an EHR-integrable medical-education platform across 37 institutions. EPA assessment counts were fitted with power law curves to measure skewing. Raw EPA assessment ratings, combined with historical case logs and OR schedules, were analyzed with the platform's large-scale Bayesian network model to quantify each EPA assessment's impact on entrustment learning curves. Lastly, we used Monte Carlo simulations to develop an assessment utility score, as an intuitive label for the predicted benefit of each EPA assessment opportunity, in order to prompt faculty members to complete the most highly useful assessments. RESULTS: From 6/2023 to 5/2025, 444 faculty assessed 532 residents with 17,245 EPA assessments. EPA assessment counts showed substantial skewing across several factors. By EPA type, 52.8% of EPA assessments were of the top 4 (22.2%) types (power law \u03b1\u202f=\u202f0.27, 2 p\u202f\u2248\u202f0). By faculty, 33.5% of EPA assessments were from the most active 15 (4.3%) faculty members (\u03b1\u202f=\u202f0.15, 2 p\u202f\u2248\u202f0). By faculty specialty, 31.0% were from the most active 2 (9.5%) specialties (\u03b1\u202f=\u202f0.24, 2 p\u202f\u2248\u202f0). By resident, 20.1% were received by the 20 (4.5%) most assessed residents (\u03b1\u202f=\u202f0.21, 2 p\u202f\u2248\u202f0). CONCLUSION: EPA assessments were heavily skewed with sampling biases, misrepresenting entrustment levels. To fix these biases and provide a data-driven approach to CBE measurement, we propose an assessment utility framework to optimize EPA assessment timing, assessor, and prioritization.",
      "journal": "Journal of surgical education",
      "year": "2025",
      "doi": "10.1016/j.jsurg.2025.103708",
      "authors": "Jenkins Phillip et al.",
      "keywords": "artificial intelligence; competency-based medical education; entrustable professional activities; human-centered design; surgical education; utility model",
      "mesh_terms": "Internship and Residency; Humans; Clinical Competence; Education, Medical, Graduate; General Surgery; Longitudinal Studies; Educational Measurement; Competency-Based Education; Selection Bias; Retrospective Studies; Bayes Theorem",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41006145/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "EHR/Health Informatics; Surgery",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: EPA assessments were heavily skewed with sampling biases, misrepresenting entrustment levels. To fix these biases and provide a data-driven approach to CBE measurement, we propose an assessment utility framework to optimize EPA assessment timing, assessor, and prioritization.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41018491",
      "title": "A Scoping Review of Methodological Approaches to Detect Bias in the Electronic Health Record.",
      "abstract": "As health systems move to make electronic health records (EHRs) accessible to patients, there is a need to examine if, and the extent to which, bias toward patients may be evident in these records. This scoping review aimed to summarize the scientific literature on methods used to detect biased language about patients in the EHR and the nature and object of the biases detected. A comprehensive literature search was conducted of PubMed, CINAHL, Web of Science, APA PsycInfo, and SOCIndex for peer-reviewed English language studies conducted in the United States published on or before December 22, 2022. Seven studies were included in this review. Four methods were identified: natural language processing methods including machine learning-based (n = 3), Linguistic Inquiry and Word Count (n = 2), and exploratory vocabulary analysis (n = 1), and manual content analysis (n = 2). In four studies, the EHR of Black patients contained significantly greater bias relative to the EHR of White patients. Bias about health conditions (i.e., diabetes, substance use disorder, and chronic pain), women, and preexposure prophylaxis-a medication that prevents HIV infection-were identified. Machine-based learning methods may be best to (a) analyze robust data sampling frames, (b) detect a rare outcome like bias, (c) facilitate inferential analysis, and (d) transcend limitations of manual content analysis. Findings provide an overview of methods that can be used by investigators to analyze EHR records for bias to inform clinical interventions, health policies, and procedures to reduce bias among health care providers.",
      "journal": "Stigma and health",
      "year": "2025",
      "doi": "10.1037/sah0000497",
      "authors": "Kelly Patrick J A et al.",
      "keywords": "bias; electronic health record; health equity; natural language processing; scoping review",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41018491/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "EHR/Health Informatics; Pain Management; Infectious Disease; Endocrinology/Diabetes",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Machine-based learning methods may be best to (a) analyze robust data sampling frames, (b) detect a rare outcome like bias, (c) facilitate inferential analysis, and (d) transcend limitations of manual content analysis. Findings provide an overview of methods that can be used by investigators to analyze EHR records for bias to inform clinical interventions, health policies, and procedures to reduce bias among health care providers.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12470022"
    },
    {
      "pmid": "41038065",
      "title": "Guidance to undertaking systematic evidence maps.",
      "abstract": "Systematic Evidence Maps (SEMs) are a form of evidence synthesis offering structured approaches to categorizing and organizing scientific evidence by identifying trends and gaps. SEMs support researchers and policymakers in navigating complex evidence landscapes. By synthesizing evidence, they lay the foundation for targeted systematic reviews and primary research, supporting evidence-informed decision-making. These outputs can be hosted on websites, providing an interactive tool. In environmental health, SEMs are systematically used to categorize evidence on topics such as pollution control measures, climate change impacts, and health disparities. The methodological framework for conducting SEMs involves defining the research scope, employing a systematic search strategy, screening studies systematically, optionally conducting critical appraisal (risk of bias assessment) when studies are categorized by effect direction or intended to inform subsequent syntheses, and coding data for synthesis and visualization. Narrative synthesis, heatmaps and network diagrams enhance SEMs usability. However, challenges remain, including methodological inconsistencies and the need for standardization. Advances in automation, machine learning, and stakeholder engagement can further refine SEMs methodologies. This commentary situates SEMs within the broader family of evidence synthesis, emphasizing their role in environmental health science. By enhancing methodological clarity and leveraging innovative tools, SEMs can support researchers and decision-makers in navigating complex evidence ecosystems and implementing evidence-based solutions for environmental scientists.",
      "journal": "Environment international",
      "year": "2025",
      "doi": "10.1016/j.envint.2025.109827",
      "authors": "Khalil H et al.",
      "keywords": "Environmental health; Methodology; Public health; Reviews; Systematic evidence map",
      "mesh_terms": "Environmental Health; Climate Change; Decision Making; Humans",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41038065/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Generative AI",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "This commentary situates SEMs within the broader family of evidence synthesis, emphasizing their role in environmental health science. By enhancing methodological clarity and leveraging innovative tools, SEMs can support researchers and decision-makers in navigating complex evidence ecosystems and implementing evidence-based solutions for environmental scientists.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (1 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41040699",
      "title": "Privacy-Enhancing Sequential Learning under Heterogeneous Selection Bias in Multi-Site EHR Data.",
      "abstract": "OBJECTIVE: To develop privacy-enhancing statistical methods for estimation of binary disease risk model association parameters across multiple electronic health record (EHR) sites with heterogeneous selection mechanisms, without sharing raw individual-level data. We illustrate their utility through a cross-biobank analysis of smoking and 97 cancer subtypes using data from the NIH All of Us (AOU) and the Michigan Genomics Initiative (MGI). MATERIALS AND METHODS: Large-scale biobanks often follow heterogeneous recruitment strategies and store data in separate cloud-based platforms, making centralized algorithms infeasible. To address this, we propose two decentralized sequential estimators namely, Sequential Pseudo-likelihood (SPL) and Sequential Augmented Inverse Probability Weighting (SAIPW) that leverage external population-level information to adjust for selection bias, with valid variance estimation. SAIPW additionally protects against misspecification of the selection model using flexible machine learning based auxiliary outcome models. We compare SPL and SAIPW with the existing Sequential Unweighted (SUW) estimator and with centralized and meta learning extensions of IPW and AIPW in simulations under both correctly specified and misspecified selection mechanisms. We apply the methods to harmonized data from MGI ( n = 50,935) and AOU ( n = 241,563) to estimate smoking-cancer associations. RESULTS: In simulations, SUW exhibited substantial bias and poor coverage. SPL and SAIPW yielded unbiased estimates with valid coverage probabilities under correct model specification, with SAIPW remaining robust under selection model misspecification. Both approaches showed no notable efficiency loss relative to centralized methods. Meta-learning methods were efficient for large sites but failed in settings with small cohort sizes and rare outcome prevalence. In real-data analysis, strong associations were consistently identified between smoking and cancers of the lung, bladder, and larynx, aligning with established epidemiological evidence. CONCLUSION: Our framework enables valid, privacy-enhancing inference across EHR cohorts with heterogeneous selection, supporting scalable, decentralized research using real-world data.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.09.26.25336642",
      "authors": "Kundu Ritoban et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41040699/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Generative AI",
      "health_domain": "Oncology; EHR/Health Informatics; Pulmonology; Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: Our framework enables valid, privacy-enhancing inference across EHR cohorts with heterogeneous selection, supporting scalable, decentralized research using real-world data.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12486029"
    },
    {
      "pmid": "41040729",
      "title": "Reducing Inequalities Using an Unbiased Machine Learning Approach to Identify Births with the Highest Risk of Preventable Neonatal Deaths.",
      "abstract": "BACKGROUND: Despite contemporaneous declines in neonatal mortality, recent studies show the existence of left-behind populations that continue to have higher mortality rates than the national averages. Additionally, many of these deaths are from preventable causes. This reality creates the need for more precise methods to identify high-risk births, allowing policymakers to target them more effectively. This study fills this gap by developing unbiased machine-learning approaches to more accurately identify births with a high risk of neonatal deaths from preventable causes. METHODS: We link administrative databases from the Brazilian health ministry to obtain birth and death records in the country from 2015 to 2017. The final dataset comprises 8,797,968 births, of which 59,615 newborns died before reaching 28 days alive (neonatal deaths). These neonatal deaths are categorized into preventable deaths (42,290) and non-preventable deaths (17,325). Our analysis identifies the death risk of the former group, as they are amenable to policy interventions. We train six machine-learning algorithms, test their performance on unseen data, and evaluate them using a new policy-oriented metric. To avoid biased policy recommendations, we also investigate how our approach impacts disadvantaged populations. RESULTS: XGBoost was the best-performing algorithm for our task, with the 5% of births identified as highest risk by the model accounting for over 85% of the observed deaths. Furthermore, the risk predictions exhibit no statistical differences in the proportion of actual preventable deaths from disadvantaged populations, defined by race, education, marital status, and maternal age. These results are similar for other threshold levels. CONCLUSIONS: We show that, by using publicly available administrative data sets and ML methods, it is possible to identify the births with the highest risk of preventable deaths with a high degree of accuracy. This is useful for policymakers as they can target health interventions to those who need them the most and where they can be effective without producing bias against disadvantaged populations. Overall, our approach can guide policymakers in reducing neonatal mortality rates and their health inequalities. Finally, it can be adapted for use in other developing countries.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2024.01.12.24301163",
      "authors": "Ramos Antonio P et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41040729/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "XGBoost/Gradient Boosting; Clinical Prediction Model",
      "health_domain": "Pediatrics; Obstetrics/Maternal Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: We show that, by using publicly available administrative data sets and ML methods, it is possible to identify the births with the highest risk of preventable deaths with a high degree of accuracy. This is useful for policymakers as they can target health interventions to those who need them the most and where they can be effective without producing bias against disadvantaged populations. Overall, our approach can guide policymakers in reducing neonatal mortality rates and their heal...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12486022"
    },
    {
      "pmid": "41043966",
      "title": "Machine learning in gastrointestinal endoscopy: challenges and opportunities.",
      "abstract": "The integration of machine learning (ML) into medical diagnostics has significantly advanced endoscopic examinations for gastrointestinal diseases. By leveraging extensive datasets and sophisticated algorithms, ML technologies enhance diagnostic precision, detect subtle abnormalities, classify diverse pathologies and predict disease progression. However, their widespread adoption is hindered by the inherent heterogeneity of gastrointestinal diseases, technical limitations, limited generalisability across different populations and ethical challenges related to patient privacy, data security and algorithmic bias.This review provides a comprehensive structural analysis of ML approaches in endoscopy, starting with an overview of the classical endoscopic methodology that relies on direct visualisation of the gastrointestinal tract for diagnosis and therapeutic interventions. Then, current ML applications that hold promise for reducing physician-dependent variability, improving diagnostic accuracy and streamlining procedural workflows were explored. Despite these advances, the effectiveness of ML models often remains constrained by the quality and diversity of training data, which can undermine both reliability and generalisability.Ethical considerations - such as safeguarding patient information, upholding data security and mitigating biases embedded in algorithms - are integral to responsibly deploying ML in clinical settings. By examining these technical and ethical barriers, this work contributes to the evolving discourse on integrating advanced ML techniques into gastroenterology. Ultimately, our goal is to pave the way for more effective and reliable ML-driven endoscopic practices that will enhance disease detection, optimise patient care and benefit healthcare providers worldwide.",
      "journal": "BMJ open gastroenterology",
      "year": "2025",
      "doi": "10.1136/bmjgast-2025-001923",
      "authors": "Lobanovs Sergejs et al.",
      "keywords": "DIAGNOSTIC AND THERAPEUTIC ENDOSCOPY; ENDOSCOPIC PROCEDURES; ENDOSCOPY",
      "mesh_terms": "Humans; Machine Learning; Endoscopy, Gastrointestinal; Gastrointestinal Diseases; Algorithms; Reproducibility of Results; Computer Security",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41043966/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "By examining these technical and ethical barriers, this work contributes to the evolving discourse on integrating advanced ML techniques into gastroenterology. Ultimately, our goal is to pave the way for more effective and reliable ML-driven endoscopic practices that will enhance disease detection, optimise patient care and benefit healthcare providers worldwide.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12496122"
    },
    {
      "pmid": "41066646",
      "title": "Global Disparities and Artificial Intelligence in Plastic Surgery: A Narrative Review of Current Applications and Ethical Implications.",
      "abstract": "Global disparities in surgical access remain a major health challenge, with an estimated 5 billion people lacking safe and affordable care. Plastic and reconstructive surgery plays a key role in addressing this gap, particularly through the management of trauma, burns, and congenital conditions such as cleft lip and palate. Although nonprofit organizations and surgical mission trips have delivered high procedure volumes, low- and middle-income countries (LMICs) continue to face substantial unmet need. Concerns over sustainability, long-term outcomes, and reliance on foreign teams highlight the importance of building local capacity, yet workforce shortages and infrastructure limitations remain significant barriers. Artificial intelligence (AI) offers a potential avenue for innovation. In high-income countries, AI has been applied to preoperative planning, postoperative monitoring, patient education, and workforce training. These tools could benefit LMICs by reducing reliance on personnel, improving health literacy, optimizing costs, and supporting surgical training. However, challenges such as data poverty, algorithmic bias, unequal access, and weak regulatory structures raise concerns that AI may widen rather than narrow disparities if not implemented thoughtfully. In addition, ethical considerations regarding equity, inclusion, sustainability, safety, and accountability must be addressed. This narrative review synthesizes emerging evidence at the intersection of plastic surgery, global disparities, and AI. The authors outline opportunities where AI may enhance equity and where it may exacerbate inequities, as well as the ethical considerations critical for its safe and sustainable integration.",
      "journal": "The Journal of craniofacial surgery",
      "year": "2025",
      "doi": "10.1097/SCS.0000000000012054",
      "authors": "Gimenez Lynch Carlota et al.",
      "keywords": "Artificial intelligence; LMIC; disparities; plastic surgery",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41066646/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Generative AI",
      "health_domain": "Emergency Medicine; ICU/Critical Care; Surgery",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "This narrative review synthesizes emerging evidence at the intersection of plastic surgery, global disparities, and AI. The authors outline opportunities where AI may enhance equity and where it may exacerbate inequities, as well as the ethical considerations critical for its safe and sustainable integration.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41072211",
      "title": "Use of artificial intelligence image generation to promote self-reflection and recognition of unconscious bias: A cross-sectional study of nursing students.",
      "abstract": "AIM: To determine the value of an artificial intelligence (AI)-image generation learning sequence on higher-education nursing student self-reflection and recognition of unconscious bias in the context of disability. BACKGROUND: Self-reflection and recognition of bias amongst undergraduate nursing students enhances reasoning skills and self-awareness in clinical situations. Teaching self-reflection to a diverse cohort can be challenging, making it essential to develop and assess innovative technological tools that support engagement in reflective practice. DESIGN: A multi-methods approach was adopted, obtaining both quantitative and qualitative data for analysis through a survey. METHODS: Twenty-nine nursing students from the Australian Catholic University were surveyed. Qualitative data underwent both content and inductive thematic analysis. Quantitative data were summarised using descriptive statistics. The study is reported according to the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) cross-sectional study guideline. RESULTS: AI-image generation aided self-reflection on personal views about disability and recognition of potential personal and society biases towards disability amongst 90\u202f% (n\u202f=\u202f26) and 70\u202f% of participants respectively. Visualisation of thoughts supported self-reflection and identification of generalisations held about disability. Eighty percent of respondents felt AI-image generation prompted them to consider how views and biases about disability may influence nursing practice. AI-image generation was identified to be an interesting and novel tool for self-reflection. CONCLUSION: Findings suggest AI-image generation may be a useful tool in supporting students to practice self-reflection and identify unconscious biases. AI-image generation may assist students to consider how personal views can impact on clinical practice.",
      "journal": "Nurse education in practice",
      "year": "2025",
      "doi": "10.1016/j.nepr.2025.104579",
      "authors": "Mullan Leanne et al.",
      "keywords": "AI; Artificial Intelligence; Higher education; Nursing; Self-reflection; Technology-enabled learning; University",
      "mesh_terms": "Humans; Students, Nursing; Cross-Sectional Studies; Artificial Intelligence; Male; Female; Surveys and Questionnaires; Education, Nursing, Baccalaureate; Adult; Qualitative Research; Australia; Young Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41072211/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Gender/Sex; Age; Disability",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Findings suggest AI-image generation may be a useful tool in supporting students to practice self-reflection and identify unconscious biases. AI-image generation may assist students to consider how personal views can impact on clinical practice.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41088416",
      "title": "PURE: policy-guided unbiased REpresentations for structure-constrained molecular generation.",
      "abstract": "Structure-constrained molecular generation (SCMG) generates novel molecules that are structurally similar to a given molecule and have optimized properties. Deep learning solutions for SCMG are limited in that they are predisposed towards existing knowledge, and they suffer from a natural impedance mismatch problem due to the discrete nature of molecules, while deep learning methods for SCMG often operate in continuous space. Moreover, many task-specific evaluation metrics used during training often bias the model towards a particular metric -\"metric-leakage\". To overcome these shortcomings, we propose Policy-guided Unbiased REpresentations (PURE) for SCMG that learns within a framework simulating molecular transformations for drug synthesis. PURE combines self-supervised learning with a policy-based reinforcement\u00a0learning (RL) framework, thereby avoiding the need for external molecular metrics while learning high-quality representations that incorporate an inherent notion of similarity specific to the given task. Along with a semi-supervised training design, PURE utilizes template-based molecular simulations to better explore and navigate the discrete molecular search space. Despite the lack of metric biases, PURE achieves competitive or superior performance to state-of-the-art methods on multiple benchmarks. Our study emphasizes the importance of reevaluating current approaches for SCMG and developing strategies that naturally align with the problem. Finally, we illustrate how our methodology can be applied to combat drug resistance by identifying sorafenib-like compounds as a case study.",
      "journal": "Journal of cheminformatics",
      "year": "2025",
      "doi": "10.1186/s13321-025-01090-5",
      "authors": "Gupta Abhor et al.",
      "keywords": "Cancer drugs; Deep learning; Drug discovery; Drug resistance; Drug synthesis; Human health; Lead optimization; Machine learning; Product innovation; Reinforcement learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41088416/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Deep Learning",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Intersectional",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "Our study emphasizes the importance of reevaluating current approaches for SCMG and developing strategies that naturally align with the problem. Finally, we illustrate how our methodology can be applied to combat drug resistance by identifying sorafenib-like compounds as a case study.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12522651"
    },
    {
      "pmid": "41090693",
      "title": "Deep Learning-Enabled Unbiased Precision Toxicity Assessment of Zebrafish Organ Development.",
      "abstract": "Precise assessment of toxicological effects remains a key bottleneck in biomedical and environmental health assessments. Traditional toxicology relies on macroscopic end points and manual image analysis, which limit sensitivity to structural damage and introduce subjective bias. We developed an automated deep learning approach based on U-Net for the precise assessment of toxic effects and established a general framework for objective toxicological analysis. Our U-Net model can perform pixel-level segmentation and morphological quantification on thousands of biological images in 1 min without bias. This developed model was then applied to distinguish size-dependent developmental toxicity induced by Ag+, 15 nm, and 100 nm silver nanoparticles (AgNPs) in zebrafish, including the photoreceptor cell layer, inner plexiform layer, skeletal muscle, and spinal cord, which revealed previously undetectable size-dependent and organ-specific toxicity disparities that conventional analytical approaches failed to resolve. The method has the potential to be widely applied to the toxicity assessment of other emerging materials and contaminants. Our model displays great potential to improve toxicity assessment accuracy, efficiency, and reproducibility, providing a scalable application for precise toxicological assessments, including imaging analysis and standardization of assessment processes.",
      "journal": "Environmental science & technology",
      "year": "2025",
      "doi": "10.1021/acs.est.5c10763",
      "authors": "Wang Mengyu et al.",
      "keywords": "AgNPs; deep learning; environmental risk assessment; imaging; toxicity assessment",
      "mesh_terms": "Animals; Zebrafish; Deep Learning; Silver; Metal Nanoparticles; Toxicity Tests",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41090693/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Generative AI",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The method has the potential to be widely applied to the toxicity assessment of other emerging materials and contaminants. Our model displays great potential to improve toxicity assessment accuracy, efficiency, and reproducibility, providing a scalable application for precise toxicological assessments, including imaging analysis and standardization of assessment processes.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41106548",
      "title": "Evaluating Long-Term Health Disparity Impacts of Clinical Algorithms Using a Patient-Level Simulation Framework.",
      "abstract": "OBJECTIVES: This study applies a simulation framework to evaluate the long-term effects of omitting race from a colon cancer decision algorithm for adjuvant chemotherapy, assessing impacts on health outcomes, costs, and disparities while accounting for measurement errors across racial groups. METHODS: We developed a patient-level state-transition model using electronic health records from a large Southern California health system to project outcomes for 4839 adults with stage II and III colon cancer after surgery. We compared 30-year quality-adjusted life-years (QALYs), healthcare costs, and QALY distribution among racial groups under 3 chemotherapy treatment scenarios: (1) current practice, (2) treatment guided by an algorithm that includes race, and (3) the same algorithm with race omitted. An additional health state addressed racial bias in cancer recurrence ascertainment, and probabilistic sensitivity analysis (PSA) assessed uncertainty. RESULTS: The clinical algorithm, compared with current practice, could improve average health by 0.048 QALYs and reduce racial health disparity by 0.20 QALYs at an incremental cost of $3221, with the disparity gap decreasing in 96% of PSA iterations. Omitting race showed minimal effects on overall health or costs but resulted in 13% fewer Black patients receiving treatment, decreasing their QALYs by 0.07 and widening the disparity gap by 0.13 QALY. Health disparity increased in 94% of PSA iterations. CONCLUSIONS: A cancer decision algorithm can improve population health and reduce health disparities, but omitting race may harm disadvantaged groups and limit reductions in disparities. Patient-level simulations can be routinely used to evaluate the potential health disparity impacts of algorithms before implementation.",
      "journal": "Value in health : the journal of the International Society for Pharmacoeconomics and Outcomes Research",
      "year": "2025",
      "doi": "10.1016/j.jval.2025.09.3066",
      "authors": "Khor Sara et al.",
      "keywords": "clinical algorithms; health disparity; microsimulation; patient-level simulation; racial disparity",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41106548/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; EHR/Health Informatics; Surgery; Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: A cancer decision algorithm can improve population health and reduce health disparities, but omitting race may harm disadvantaged groups and limit reductions in disparities. Patient-level simulations can be routinely used to evaluate the potential health disparity impacts of algorithms before implementation.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41108716",
      "title": "Comparative analysis of human-generated versus Artificial Intelligence-drafted summary paragraphs for medical student performance evaluations.",
      "abstract": "PURPOSE: This study evaluated the efficiency and effectiveness of using Generative Artificial Intelligence (GenAI) to draft Medical Student Performance Evaluation (MSPE) summary paragraphs for medical students. MATERIALS AND METHODS: Evaluations on the pediatrics clerkship were used to develop MSPE summary paragraphs. Time to completion was noted for paragraphs drafted by GenAI, created using Microsoft 365 Copilot, and compared to human-generated. Undergraduate Medical Education (UME) leaders were recruited to evaluate 10 randomized pairs of paragraphs through a blinded survey. RESULTS: Copilot-drafted paragraphs required significantly less time to completion compared to human-generated paragraphs (median 6 vs. 12.5\u2009min, p\u2009=\u20090.002). UME leaders showed no significant preference and were unable to consistently identify Copilot vs human authorship. When stratified by perception of authorship, human-generated paragraphs were significantly less likely to be preferred if they were perceived as being Copilot-drafted than if they were perceived as being human-generated (p\u2009=\u20090.017), suggesting an element of anti-AI bias. Competencies were highlighted to a similar degree, and Copilot-drafted paragraphs were perceived as having significantly less biased language by both UME leaders (p\u2009=\u20090.004) and an independent analysis using a validated gender bias calculator (p\u2009=\u20090.029). CONCLUSIONS: Copilot-drafted MSPE summaries are efficient, comparable in quality, and may reduce the introduction of bias.",
      "journal": "Medical teacher",
      "year": "2025",
      "doi": "10.1080/0142159X.2025.2574382",
      "authors": "Maheshwari Atul et al.",
      "keywords": "Artificial Intelligence; Medical student performance evaluations; bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41108716/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "Pediatrics",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Copilot-drafted MSPE summaries are efficient, comparable in quality, and may reduce the introduction of bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41136718",
      "title": "Artificial Intelligence in Population-Level Gastroenterology and Hepatology: A Comprehensive Review of Public Health Applications and Quantitative Impact.",
      "abstract": "BACKGROUND: Artificial intelligence (AI), which includes machine learning and deep learning, is fundamentally changing public health in gastroenterology and hepatology-fields grappling with a significant global disease burden. OBJECTIVE: This review focuses on the population-level applications and impact of AI, highlighting its role in shifting healthcare strategies from reactive treatment to proactive prevention. RESULTS: AI demonstrates substantial improvements across many different areas. In colorectal cancer, AI models significantly boost detection rates, successfully identifying a large majority of high-risk individuals often missed by traditional screening methods. For metabolic dysfunction-associated steatotic liver disease (MASLD), advanced non-invasive tests offer a high degree of reliability in detecting liver fibrosis. The identification of viral hepatitis is enhanced with excellent accuracy, and gastrointestinal infection surveillance benefits from wastewater analysis that provides an early warning system weeks ahead of clinical case reporting. Furthermore, AI improves the diagnosis of upper GI cancers, such as gastric cancer, with higher diagnostic capability, and facilitates precision public health in inflammatory bowel disease (IBD) through highly accurate risk prediction models. CHALLENGES: Despite these important advances, significant hurdles remain. Key challenges include ensuring diverse and representative data to prevent algorithmic bias, protecting patient privacy, establishing robust regulatory frameworks for new technologies, and successfully moving innovations from research settings into practical, real-world deployment. CONCLUSION: The unequal distribution of AI development and access between high-income countries and low- and middle-income countries risks exacerbating existing health disparities. To fully realize AI's transformative potential for global public health in gastroenterology and hepatology, these cross-cutting issues must be actively addressed through ethical design, rigorous validation, and equitable worldwide deployment.",
      "journal": "Digestive diseases and sciences",
      "year": "2025",
      "doi": "10.1007/s10620-025-09452-7",
      "authors": "Bharadwaj Hareesha Rishab et al.",
      "keywords": "Artificial intelligence; Gastroenterology; Hepatology; Machine learning; Public health",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41136718/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Clinical Prediction Model",
      "health_domain": "Oncology; Public Health; Infectious Disease",
      "bias_axes": "Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Diverse/Representative Data",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: The unequal distribution of AI development and access between high-income countries and low- and middle-income countries risks exacerbating existing health disparities. To fully realize AI's transformative potential for global public health in gastroenterology and hepatology, these cross-cutting issues must be actively addressed through ethical design, rigorous validation, and equitable worldwide deployment.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41146192",
      "title": "Reducing inequalities using an unbiased machine learning approach to identify births with the highest risk of preventable neonatal deaths.",
      "abstract": "BACKGROUND: Despite contemporaneous declines in neonatal mortality, recent studies show the existence of left-behind populations that continue to have higher mortality rates than the national averages. Additionally, many of these deaths are from preventable causes. This reality creates the need for more precise methods to identify high-risk births, allowing policymakers to target them more effectively. This study fills this gap by developing unbiased machine-learning approaches to more accurately identify births with a high risk of neonatal deaths from preventable causes. METHODS: We link administrative databases from the Brazilian health ministry to obtain birth and death records in the country from 2015 to 2017. The final dataset comprises 8,797,968 births, of which 59,615 newborns died before reaching 28 days alive (neonatal deaths). These neonatal deaths are categorized into preventable deaths (42,290) and non-preventable deaths (17,325). Our analysis identifies the death risk of the former group, as they are amenable to policy interventions. We train six machine-learning algorithms, test their performance on unseen data, and evaluate them using a new policy-oriented metric. To avoid biased policy recommendations, we also investigate how our approach impacts disadvantaged populations. RESULTS: XGBoost was the best-performing algorithm for our task, with the 5% of births identified as highest risk by the model accounting for over 85% of the observed deaths. Furthermore, the risk predictions exhibit no statistical differences in the proportion of actual preventable deaths from disadvantaged populations, defined by race, education, marital status, and maternal age. These results are similar for other threshold levels. CONCLUSIONS: We show that, by using publicly available administrative data sets and ML methods, it is possible to identify the births with the highest risk of preventable deaths with a high degree of accuracy. This is useful for policymakers as they can target health interventions to those who need them the most and where they can be effective without producing bias against disadvantaged populations. Overall, our approach can guide policymakers in reducing neonatal mortality rates and their health inequalities. Finally, it can be adapted for use in other developing countries.",
      "journal": "Population health metrics",
      "year": "2025",
      "doi": "10.1186/s12963-025-00420-x",
      "authors": "Ramos Antonio P et al.",
      "keywords": "Algorithmic bias; Health inequality; Machine learning; Neonatal mortality; Program targeting",
      "mesh_terms": "Humans; Machine Learning; Infant, Newborn; Brazil; Infant Mortality; Female; Infant; Perinatal Death; Socioeconomic Factors; Male; Cause of Death; Algorithms; Databases, Factual; Risk Factors",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41146192/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "XGBoost/Gradient Boosting; Clinical Prediction Model",
      "health_domain": "Pediatrics; Obstetrics/Maternal Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: We show that, by using publicly available administrative data sets and ML methods, it is possible to identify the births with the highest risk of preventable deaths with a high degree of accuracy. This is useful for policymakers as they can target health interventions to those who need them the most and where they can be effective without producing bias against disadvantaged populations. Overall, our approach can guide policymakers in reducing neonatal mortality rates and their heal...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12557940"
    },
    {
      "pmid": "41163810",
      "title": "Gender and racial bias unveiled: clinical artificial intelligence (AI) and machine learning (ML) algorithms are fanning the flames of inequity.",
      "abstract": "This study aims to advocate for the continued evaluation of published clinical artificial intelligence (AI) and Machine Learning (ML) studies, including the reporting of demographic information, specifically gender, racial composition, and geographic location. Models are often trained on data lacking representation across basic demographics, potentially leading to biased outputs and exacerbating health disparities. Previous research in drug and device development has demonstrated the dangers of underrepresenting women and minority populations. This study aimed to assess the extent to which published clinical AI/ML studies report demographic information, specifically gender and racial composition, in their training datasets. A systematic review was conducted in accordance with PRISMA guidelines. The databases that were used in the study include Ovid MEDLINE, Embase, PsycINFO, Scopus, Web of Science and the Cochrane Library, for clinical AI/ML studies with direct implications for patient care. Inclusion criteria required models to be clinically actionable and not pre-clinical or administrative in scope. Two independent reviewers screened and extracted data using Covidence software, with conflicts resolved by a third reviewer. Out of 390 studies included, 84% of global models did not report the racial composition of their training data, while 31% lacked gender data. US-based models performed slightly better, with 56% reporting race and 77% reporting gender. Only 16% of all models utilized publicly available, non-proprietary datasets. The low frequency of demographic disclosure and limited use of open data raise serious concerns about the transparency, generalizability and fairness of clinical AI/ML models. Standardized reporting of gender and racial composition in training data is urgently needed to ensure ethical and equitable deployment of these technologies.",
      "journal": "Oxford open digital health",
      "year": "2025",
      "doi": "10.1093/oodh/oqaf027",
      "authors": "Otokiti Ahmed Umar et al.",
      "keywords": "artificial intelligence; data transparency; fairness and biases; health equity; machine learning; model updating",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41163810/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Geographic",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "The low frequency of demographic disclosure and limited use of open data raise serious concerns about the transparency, generalizability and fairness of clinical AI/ML models. Standardized reporting of gender and racial composition in training data is urgently needed to ensure ethical and equitable deployment of these technologies.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12560773"
    },
    {
      "pmid": "41180681",
      "title": "Effectiveness and reliability of AI in diagnosis and robot-assisted spinal and cranial surgery: efficient outcomes and ethical worries.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) and machine learning (ML) have significantly advanced medical diagnostics and surgical procedures, particularly in spinal and cranial surgery. Robotic-assisted surgery has emerged as a transformative approach, offering increased precision, reduced intraoperative complications, and improved surgical outcomes. This review examines the effectiveness and reliability of AI in spinal and cranial diagnosis, its integration into robotic-assisted surgical interventions, and the associated ethical concerns. METHOD: An extensive literature search was search was conducted on different search engines such as PubMed, Google Scholar, and Scopus to find relevant articles. RESULT: Findings suggest that AI models exhibit high accuracy in detecting spinal and cranial pathologies. ML algorithms contribute to enhanced prognostic assessments and decision-making in neurosurgery. Robotic-assisted surgeries have superior accuracy, lower radiation exposure, and fewer postoperative complications compared to conventional methods. However, challenges such as data biases, lack of transparency in AI decision-making, regulatory hurdles, and the high costs of AI-driven interventions pose significant barriers to widespread adoption. Ethical concerns, including patient privacy, algorithmic bias, and the potential overreliance on AI, must be addressed to ensure responsible integration into clinical practice. CONCLUSION: Use of AI and machine learning improves the diagnostic outcomes and decreases post op complications in the field of spinal and cranial surgery. But certain challenges such as ethical concerns and technical hurdles should be sorted out with effective planning. Further research is necessary to refine AI-driven interventions, enhance cost-effectiveness, and to make sure ethical and equitable implementation of AI and robotic surgery in neurosurgical care.",
      "journal": "Annals of medicine and surgery (2012)",
      "year": "2025",
      "doi": "10.1097/MS9.0000000000003865",
      "authors": "Shahid Iqra et al.",
      "keywords": "artificial intelligence; cranial surgery; machine learning; robot-assisted surgery; spinal surgery",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41180681/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Surgery",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: Use of AI and machine learning improves the diagnostic outcomes and decreases post op complications in the field of spinal and cranial surgery. But certain challenges such as ethical concerns and technical hurdles should be sorted out with effective planning. Further research is necessary to refine AI-driven interventions, enhance cost-effectiveness, and to make sure ethical and equitable implementation of AI and robotic surgery in neurosurgical care.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12577963"
    },
    {
      "pmid": "41201814",
      "title": "Proactive Bias Mitigation When Using Online Survey Panels for Self-Reported Use of Illicitly Manufactured Fentanyl in the General Adult Population.",
      "abstract": "IMPORTANCE: Illicitly manufactured fentanyl remains a public health threat and trustworthy measurements in prevalence are crucial to public health approaches. Low prevalence behaviors, such as route of administration of illicitly manufactured fentanyl, may have shifted over time, which changes community risk profiles. OBJECTIVE: To assess the impact of bias mitigation methods in an online survey sample and quantify changes in routes of administration in illicitly manufactured fentanyl use over time. DESIGN, SETTING, AND PARTICIPANTS: This repeated cross-sectional survey included US adults 18 years and older in an online, panel-based general population sample fielded twice yearly, in spring and autumn. Corrections for demographic and nondemographic composition bias using calibration weights and removal of misclassification from careless/inattentive responses were applied. Data were collected from April 2022 to October 2024, and data were analyzed in May 2025. MAIN OUTCOMES AND MEASURES: Self-reported use of illicitly manufactured fentanyl in the past 12 months and routes of administration, which included oral, injection, smoking, or snorting. Weighted frequency and percentages were calculated. RESULTS: In the full 2022-2024 sample of 175\u202f058 respondents where misclassification removal and calibration was applied, 50.6% (95% uncertainty interval [UI], 50.3-60.0) were female, 48.1% (95% UI, 47.8-48.4) were male, and 1.3% (95% UI, 1.2-1.3) were transgender, nonbinary, or something else, and the median (IQR) age was 47 (32-62) years. The bias-mitigated prevalence estimate of illicitly manufactured fentanyl use in the last 12 months increased from 0.7% (95% UI, 0.7-0.8) in 2022 to 1.1% (95% UI, 1.0-1.2) in 2024. Oral use of illicitly manufactured fentanyl increased from 35.9% (95% UI, 31.1-40.7) in 2022 to 44.4% (95% UI, 40.3-48.5) in 2024, which was the most common route of administration. In 2024, use by smoking was 37.9% (95% UI, 34.1-41.6), use by snorting was 27.1% (95% UI, 23.5-30.7), and use by injection was 24.5% (95% UI, 21.3-27.7). Importantly, bias mitigation cumulatively reduced the national estimate of illicitly manufactured fentanyl by 70.9% in 2024 (from 3.9% [95% UI, 3.8-4.1] when neither was applied to 1.1% [95% UI, 1.0-1.2]), an important factor when considering prevalence and change over time. CONCLUSIONS AND RELEVANCE: Results of this survey study suggest that fentanyl use has shifted toward oral use, which may contribute to observed lower mortality rates despite an increase in prevalence of use. Methods intended to reduce systematic bias have a strong influence on low prevalence behavior estimates and should be implemented for all survey-based drug use surveillance.",
      "journal": "JAMA health forum",
      "year": "2025",
      "doi": "10.1001/jamahealthforum.2025.4011",
      "authors": "Black Joshua C et al.",
      "keywords": "",
      "mesh_terms": "Humans; Fentanyl; Male; Female; Adult; Cross-Sectional Studies; Self Report; Middle Aged; Bias; United States; Illicit Drugs; Surveys and Questionnaires; Young Adult; Adolescent; Prevalence; Opioid-Related Disorders; Analgesics, Opioid",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41201814/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry; Public Health; Pain Management",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration",
      "clinical_setting": "Public Health/Population",
      "key_findings": "RESULTS: In the full 2022-2024 sample of 175\u202f058 respondents where misclassification removal and calibration was applied, 50.6% (95% uncertainty interval [UI], 50.3-60.0) were female, 48.1% (95% UI, 47.8-48.4) were male, and 1.3% (95% UI, 1.2-1.3) were transgender, nonbinary, or something else, and the median (IQR) age was 47 (32-62) years. The bias-mitigated prevalence estimate of illicitly manufactured fentanyl use in the last 12 months increased from 0.7% (95% UI, 0.7-0.8) in 2022 to 1.1% (95...",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12595535"
    },
    {
      "pmid": "41221373",
      "title": "Using mixture cure models to address algorithmic bias in diagnostic timing: autism as a test case.",
      "abstract": "OBJECTIVES: To address algorithmic bias in clinical prediction models related to the timing of diagnosis, we evaluated the efficacy of mixture cure models that integrate time-to-event and binary classification frameworks to predict diagnoses. MATERIALS AND METHODS: We conducted a simulation and analyzed real-world North Carolina Medicaid data for children born in 2014, followed until 2023. The study evaluated traditional time-to-event and classification models against mixture cure models under scenarios with varied diagnostic timing and censoring. RESULTS: Simulation results demonstrated that traditional models exhibit increased bias as diagnosis timing differences widened, whereas mixture cure models yielded unbiased estimates across varying censoring times. In real-world analyses, significant racial and ethnic variations in autism diagnosis rates were observed, with non-Hispanic White children having higher diagnosis rates compared to other groups. The mixture cure model effectively adjusted for these disparities, providing fairer and more accurate diagnostic predictions across varying levels of censoring. DISCUSSION: Mixture cure models effectively address algorithmic bias by providing unbiased estimates regardless of variations in diagnostic timing and censoring, making them particularly suitable for conditions like autism where not all individuals will receive a diagnosis. This approach shifts focus from when an event will occur to whether it will occur, aligning more closely with clinical needs in early detection of pediatric developmental conditions. CONCLUSION: Mixture cure models offer a promising tool to enhance accuracy and fairness in predictive modeling, especially when the outcome of interest is not uniformly observed across groups.",
      "journal": "JAMIA open",
      "year": "2025",
      "doi": "10.1093/jamiaopen/ooaf148",
      "authors": "Wu Peng et al.",
      "keywords": "algorithmic bias; autism spectrum disorder; clinical prediction models; electronic health records and claims data; mixture cure models",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41221373/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "ICU/Critical Care; Pediatrics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Insurance Status",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: Mixture cure models offer a promising tool to enhance accuracy and fairness in predictive modeling, especially when the outcome of interest is not uniformly observed across groups.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12598640"
    },
    {
      "pmid": "41227046",
      "title": "Artificial Intelligence in Postmenopausal Health: From Risk Prediction to Holistic Care.",
      "abstract": "Background/Objectives: Menopause, marked by permanent cessation of menstruation, is a universal transition associated with vasomotor, genitourinary, psychological, and metabolic changes. These conditions significantly affect health-related quality of life (HRQoL) and increase the risk of chronic diseases. Despite their impact, timely diagnosis and individualized management are often limited by delayed care, fragmented health systems, and cultural barriers. Methods: This review summarizes current applications of artificial intelligence (AI) in postmenopausal health, focusing on risk prediction, early detection, and personalized treatment. Evidence was compiled from studies using biomarkers, imaging, wearable sensors, electronic health records, natural language processing, and digital health platforms. Results: AI enhances disease prediction and diagnosis, including improved accuracy in breast cancer and osteoporosis screening through imaging analysis, and cardiovascular risk stratification via machine learning models. Wearable devices and natural language processing enable real-time monitoring of underreported symptoms such as hot flushes and mood disorders. Digital technologies further support individualized interventions, including lifestyle modification and optimized medication regimens. By improving access to telemedicine and reducing bias, AI also has the potential to narrow healthcare disparities. Conclusions: AI can transform postmenopausal care from reactive to proactive, offering personalized strategies that improve outcomes and quality of life. However, challenges remain, including algorithmic bias, data privacy, and clinical implementation. Ethical frameworks and interdisciplinary collaboration among clinicians, data scientists, and policymakers are essential for safe and equitable adoption.",
      "journal": "Journal of clinical medicine",
      "year": "2025",
      "doi": "10.3390/jcm14217651",
      "authors": "Panjwani Gianeshwaree Alias Rachna et al.",
      "keywords": "artificial intelligence; estrogen; health equity; machine learning; menopause; mental health; postmenopausal complications; risk prediction; telemedicine; wearable sensors",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41227046/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM; Clinical Prediction Model",
      "health_domain": "Cardiology; Oncology; EHR/Health Informatics; Wearables/Remote Monitoring",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Telehealth/Remote",
      "key_findings": "Conclusions: AI can transform postmenopausal care from reactive to proactive, offering personalized strategies that improve outcomes and quality of life. However, challenges remain, including algorithmic bias, data privacy, and clinical implementation. Ethical frameworks and interdisciplinary collaboration among clinicians, data scientists, and policymakers are essential for safe and equitable adoption.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 2 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12608235"
    },
    {
      "pmid": "41230243",
      "title": "FairAlloc: Learning Fair Organ Allocation Policy for Liver Transplant.",
      "abstract": "UNLABELLED: Liver transplantation is a critical treatment for end-stage liver diseases, but ensuring fair and effective allocation of scarce donor organs remains a major challenge in healthcare. Existing allocation policies often struggle to balance clinical outcomes with fairness across patients and demographic groups. To address this, we propose FairAlloc, a learning-based framework that formulates organ allocation as a ranking problem and jointly optimizes for post-transplant outcomes and fairness. Specifically, FairAlloc integrates two fairness objectives-group fairness across sensitive attributes such as gender and race and individual fairness between similar patients-into the ranking process. We evaluate FairAlloc using real-world data from the Organ Procurement and Transplantation Network (OPTN) and compare its performance to six baseline methods. Results show that FairAlloc improves group and individual fairness metrics by up to 37.9% and 39.9%, respectively, while maintaining competitive performance on key post-transplant outcomes such as graft failure and survival rates. This work contributes a novel, fairness-aware decision-making framework to the healthcare informatics community, with the potential to improve equity and efficiency in organ allocation systems. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007/s41666-025-00206-8.",
      "journal": "Journal of healthcare informatics research",
      "year": "2025",
      "doi": "10.1007/s41666-025-00206-8",
      "authors": "Ding Sirui et al.",
      "keywords": "Fairness in healthcare; Multi-objective optimization; Organ allocation; Organ transplant; Reinforcement learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41230243/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "Public Health/Population",
      "key_findings": "This work contributes a novel, fairness-aware decision-making framework to the healthcare informatics community, with the potential to improve equity and efficiency in organ allocation systems. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007/s41666-025-00206-8.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12602758"
    },
    {
      "pmid": "41241583",
      "title": "Leveraging digital technologies to reduce cancer disparities in low-income and middle-income countries.",
      "abstract": "In a rural clinic in southwestern Uganda, Dr Sarah examines cervical images on her smartphone, receiving real-time artificial intelligence-powered guidance from a gynaecologic oncologist located hundreds of miles away. Once imaginary, this scenario now represents a highly probable future of digital health innovation transforming cancer care globally. With over 35 million new cases of cancer estimated by 2050, and up to 70% of deaths anticipated to disproportionately occur in low-income and middle-income countries (LMICs), digital solutions can be leveraged to accelerate the closure of these cancer care gaps. The global oncology community has responded to this imminent crisis by proposing several interventions, including promoting workforce education, mentorship, and task shifting; supporting early diagnosis and referrals through integrated diagnostics; prioritising and implementing prevention strategies such as tobacco cessation, cervical cancer screening, and vaccination; standardising and personalising treatment through increased participation in clinical trials and provision of essential cancer medications; and strengthening health-care systems. Across all these strategic pillars, digital health tools are crucial for advancing cancer care and narrowing existing global and geographical disparities in LMICs. In this Series paper, we evaluate the current status of these digital innovations in the context of cancer care.",
      "journal": "The Lancet. Digital health",
      "year": "2025",
      "doi": "10.1016/j.landig.2025.100937",
      "authors": "Gichoya Judy W et al.",
      "keywords": "",
      "mesh_terms": "Humans; Developing Countries; Digital Technology; Neoplasms; Healthcare Disparities; Female; Early Detection of Cancer; Uterine Cervical Neoplasms; Telemedicine; Artificial Intelligence",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41241583/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Generative AI",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Clinical Trial",
      "key_findings": "Across all these strategic pillars, digital health tools are crucial for advancing cancer care and narrowing existing global and geographical disparities in LMICs. In this Series paper, we evaluate the current status of these digital innovations in the context of cancer care.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41255469",
      "title": "The Target Study: A Conceptual Model and Framework for Measuring Disparity.",
      "abstract": "We present a conceptual model to measure disparity-the target study-where social groups may be similarly situated (i.e., balanced) on allowable covariates. Our model, based on a sampling design, does not intervene to assign social group membership or alter allowable covariates. To address non-random sample selection, we extend our model to generalize or transport disparity or to assess disparity after an intervention on eligibility-related variables that eliminates forms of collider-stratification. To avoid bias from differential timing of enrollment, we aggregate time-specific study results by balancing calendar time of enrollment across social groups. To provide a framework for emulating our model, we discuss study designs, data structures, and G-computation and weighting estimators. We compare our sampling-based model to prominent decomposition-based models used in healthcare and algorithmic fairness. We provide R code for all estimators and apply our methods to measure health system disparities in hypertension control using electronic medical records.",
      "journal": "Sociological methods & research",
      "year": "2025",
      "doi": "10.1177/00491241251314037",
      "authors": "Jackson John W et al.",
      "keywords": "Conceptual Model; Disparity; Equity; Ethics; Fairness; Framework; Target Study Emulation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41255469/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We compare our sampling-based model to prominent decomposition-based models used in healthcare and algorithmic fairness. We provide R code for all estimators and apply our methods to measure health system disparities in hypertension control using electronic medical records.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12622519"
    },
    {
      "pmid": "41255988",
      "title": "Lack of children in public medical imaging data points to growing age bias in biomedical AI.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) is transforming healthcare, but its benefits have not been equitably distributed, with children being particularly overlooked. Only 17% of FDA-approved medical AI devices are labeled for pediatric use. We hypothesized that this disparity may be due to a fundamental data gap in pediatric medical imaging. METHODS: To test this hypothesis, we performed a systematic review of 180 publicly available medical imaging datasets to assess pediatric data representation. To identify the primary data sources used for methods development, we first surveyed papers from a machine learning imaging conference. Finally, we evaluated the performance of adult-trained chest radiograph models when applied to pediatric populations to quantify potential age-related bias. RESULTS: Our systematic review found that children represent less than 1% of the data in public medical imaging datasets. The majority of machine learning conference papers we surveyed relied on publicly available data for model development. Furthermore, we found that adult-trained chest radiograph models exhibit significant age bias when applied to pediatric populations, with higher false positive rates in younger children. DISCUSSION: This study highlights the urgent need for increased pediatric representation in publicly accessible medical datasets. Our findings suggest that the lack of pediatric data may contribute to the scarcity of AI tools for children and the poor performance of adult-trained models in this population. We provide actionable recommendations for researchers, policymakers, and data curators to address this age equity gap and mitigate the potential harms of AI systems not trained on pediatric patients.",
      "journal": "Research square",
      "year": "2025",
      "doi": "10.21203/rs.3.rs-7713694/v1",
      "authors": "Erdman Lauren et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41255988/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care; Pediatrics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "RESULTS: Our systematic review found that children represent less than 1% of the data in public medical imaging datasets. The majority of machine learning conference papers we surveyed relied on publicly available data for model development. Furthermore, we found that adult-trained chest radiograph models exhibit significant age bias when applied to pediatric populations, with higher false positive rates in younger children.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12622155"
    },
    {
      "pmid": "41260014",
      "title": "Knowledge-informed deep learning to mitigate bias in joint air pollutant prediction.",
      "abstract": "Accurate prediction of atmospheric air pollutants is critical for public health protection and environmental management. Traditional machine learning (ML) methods achieve high spatial resolution but lack physicochemical constraints, leading to systematic biases that compromise exposure estimates for epidemiological studies. Chemical transport models incorporate atmospheric physics but require expensive parameterization and often fail to capture local-scale variability crucial for health impact assessment. This gap between data-driven accuracy and physical realism presents a major obstacle to advancing air quality science. We address this challenge through a novel physics-informed deep learning framework that integrates advection-diffusion equations and fluid dynamics constraints directly into neural network architectures for multi-pollutant prediction. Our approach models air pollutant pairs across geographically distinct domains (NO2/NOx for California; PM2.5/PM10 for mainland China), providing a comprehensive framework for physics-constrained atmospheric modeling at high resolution. Through an efficient framework, our methodology demonstrates that incorporating proxy advection and diffusion fields as physical constraints fundamentally alters learning dynamics, reducing generalization error and eliminating systematic bias inherent in data-driven approaches while improving computational efficiency compared to graph networks. Site-based validation reveals unprecedented bias reduction: 21%-42% for nitrogen oxides and 16%-17% for particulate matter compared to the baseline deep learning methods. Our methodology uniquely generates physically interpretable parameters while providing explicit uncertainty quantification through ensemble techniques. The substantial bias reduction coupled with physically interpretable parameters has immediate implications for improving air pollutant exposure assessment and understanding in epidemiological research, potentially transforming health effect evaluations that rely on accurate spatial predictions.",
      "journal": "Environment international",
      "year": "2025",
      "doi": "10.1016/j.envint.2025.109915",
      "authors": "Li Lianfa et al.",
      "keywords": "Air pollution; Bias mitigation; Deep learning; Joint prediction; Knowledge fusion; Physics-informed modeling",
      "mesh_terms": "Deep Learning; Air Pollutants; Air Pollution; Particulate Matter; Environmental Monitoring; California; China; Nitrogen Oxides",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41260014/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Neural Network; Ensemble Methods",
      "health_domain": "ICU/Critical Care; Public Health",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability; Ensemble Methods",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "Our methodology uniquely generates physically interpretable parameters while providing explicit uncertainty quantification through ensemble techniques. The substantial bias reduction coupled with physically interpretable parameters has immediate implications for improving air pollutant exposure assessment and understanding in epidemiological research, potentially transforming health effect evaluations that rely on accurate spatial predictions.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12810719"
    },
    {
      "pmid": "41273675",
      "title": "Equity-promoting integer programming approaches for medical resident rotation scheduling.",
      "abstract": "Motivated by our collaboration with a residency program at an academic health system, we propose new integer programming (IP) approaches for the resident-to-rotation assignment problem (RRAP). Given sets of residents, resident classes, and departments, as well as a block structure for each class, staffing needs, rotation requirements for each class, program rules, and resident vacation requests, the RRAP involves finding a feasible year-long rotation schedule that specifies resident assignments to rotations and vacation times. We first present an IP formulation for the RRAP, which mimics the manual method for generating rotation schedules in practice and can be easily implemented and efficiently solved using off-the-shelf optimization software. However, it can lead to disparities in satisfying vacation requests among residents. To mitigate such disparities, we derive an equity-promoting counterpart that finds an optimal rotation schedule, maximizing the number of satisfied vacation requests while minimizing a measure of disparity in satisfying these requests. Then, we propose a computationally efficient Pareto Search Algorithm capable of finding the complete set of Pareto optimal solutions to the equity-promoting IP within a time that is suitable for practical implementation. Additionally, we present a user-friendly tool that implements the proposed models to automate the generation of the rotation schedule. Finally, we construct diverse RRAP instances based on data from our collaborator and conduct extensive experiments to illustrate the potential practical benefits of our proposed approaches. Our results demonstrate the computational efficiency and implementability of our approaches and underscore their potential to enhance fairness in resident rotation scheduling.",
      "journal": "Health care management science",
      "year": "2025",
      "doi": "10.1007/s10729-025-09736-4",
      "authors": "Li Shutian et al.",
      "keywords": "Fairness; Integer programming; Operations management; Operations research; Optimization; Resident scheduling",
      "mesh_terms": "Internship and Residency; Humans; Personnel Staffing and Scheduling; Algorithms; Academic Medical Centers",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41273675/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Finally, we construct diverse RRAP instances based on data from our collaborator and conduct extensive experiments to illustrate the potential practical benefits of our proposed approaches. Our results demonstrate the computational efficiency and implementability of our approaches and underscore their potential to enhance fairness in resident rotation scheduling.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12743667"
    },
    {
      "pmid": "41274526",
      "title": "Guiding artificial intelligence in public health and medicine with epidemiology: A lifecycle framework for mitigating AI misalignment.",
      "abstract": "Artificial Intelligence (AI) holds immense promise for public health, yet its potential is undermined by alignment failures where systems act contrary to human values, often exacerbating health disparities. This paper challenges the narrow view that algorithmic bias is solely a data problem, arguing instead that misalignment arises at every stage of the AI development lifecycle. We introduce a comprehensive seven-stage framework, spanning problem definition, team assembly, study design, data acquisition, model training, validation, and post-deployment implementation, viewed through an epidemiological lens. This approach systematically integrates core principles such as population representativeness, rigorous study design, bias characterization, and causal reasoning to identify and mitigate alignment risks. For each stage, we define specific alignment failures, from flawed problem formulation to post-market performance degradation, and propose actionable, evidence-based solutions. By embedding epidemiological rigor throughout the entire AI lifecycle, this framework provides a structured, proactive pathway for researchers, developers, and policymakers to create trustworthy, safe, and fair AI systems. This systemic approach is critical to harnessing AI's transformative benefits for population health while preventing the perpetuation of inequity and harm.",
      "journal": "Annals of epidemiology",
      "year": "2025",
      "doi": "10.1016/j.annepidem.2025.11.004",
      "authors": "Hassoon Ahmed et al.",
      "keywords": "AI alignment; Algorithmic bias; Alignment problem; Artificial intelligence; Epidemiology; Health equity; Healthcare technology; Lifecycle management; Machine learning; Public health informatics",
      "mesh_terms": "Artificial Intelligence; Humans; Public Health; Epidemiology; Algorithms",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41274526/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Representation Learning",
      "clinical_setting": "Public Health/Population",
      "key_findings": "By embedding epidemiological rigor throughout the entire AI lifecycle, this framework provides a structured, proactive pathway for researchers, developers, and policymakers to create trustworthy, safe, and fair AI systems. This systemic approach is critical to harnessing AI's transformative benefits for population health while preventing the perpetuation of inequity and harm.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41314754",
      "title": "Future horizons: Innovation, aging, and equity.",
      "abstract": "Precision medicine is on the verge of transforming the treatment of neurodegenerative diseases (NDDs) like Alzheimer's disease (AD) and Parkinson's disease (PD), in response to the intricate interactions of genetic, epigenetic, environmental, and lifestyle factors underlying disease heterogeneity. As the world's aging populations grow, with dementia cases expected to double by 2040 and the costs amounting to over \u20ac130 billion a year in Europe alone, there is an urgent need for novel strategies to stem the socioeconomic costs of NDDs. Conventional \"one-drug-fits-all\" strategies that depend on late-stage symptom treatment are progressively insufficient for disorders that are marked by heterogeneous molecular pathways and unpredictable clinical courses. Recent improvements in artificial intelligence (AI), multi-omics integration, and biomarker research now allow patients to be stratified into subpopulations following their genetic risk profiles, neuroimaging signatures, and fluid biomarkers (e.g., amyloid-beta, tau, \u03b1-synuclein), enabling early diagnosis and focused treatments. For example, artificial intelligence platforms such as the IHI-PROMINENT project are creating forecasting algorithms to chart disease progression and tailor treatment outcomes, and gene therapy and antisense oligonucleotides (ASOs) address precise mutations in familial AD and PD. These advances are supported by pharmacogenomics, which individualizes drug regimens according to metabolic profiles to reduce side effects and maximize efficacy. Still, translating these advances into practice has major barriers to overcome, such as large-scale biomarker validation, multi-omics standardization, and incorporating real-world evidence from digital health technologies. Aging populations only add complexity to this environment, as comorbidities like diabetes and cardiovascular diseases interact with neurodegenerative pathways, requiring system-based, holistic approaches to care. Equity is still a key challenge: differences in access to sophisticated diagnostics (e.g., PET scans, CSF examination) and expensive therapies (e.g., monoclonal antibodies, CAR-T cell therapy) threaten to worsen global health disparities. In retaliation, initiatives such as the JPND research paradigm advance remote clinical trials and telemedicine platforms for the diverse community in decentralized settings, and policies target reducing financial disincentives through risk-sharing strategies and public-private partnerships. Precision medicine in the treatment of NDDs depends on an integrated network among academia, clinics, and industry, by taking advantage of communal biobanks and AI-enabled big data analysis, for refining the drug development process and validating new targets, e.g., neuroinflammatory signaling and gut-brain axis dysfunction. Innovations, like CRISPR-mediated editing and ambient neuroimaging, have innate or potential power to personalize treatment by identifying early-stage and even pre-symptomatic patients and modulating one's lifestyle in light of genetic risk. However ethical considerations around data privacy, algorithmic bias, and informed consent for Sustained therapeutic interventions over a lifetime should guide, not lag, the transformation. With the drive toward preventive rather than delayed care, precision medicine represents a revolutionary paradigm shift in health care, and a possibility to convert NDDs from devastatingly fatal diagnoses to easily managed chronic diseases and render equitable access to innovations possible for the masses. Success will require consistent investment in translational studies, interdisciplinary training, and global regulatory harmonization to translate the promise of precision medicine into tangible improvements in the quality of life for the millions of individuals afflicted with neurodegenerative disorders.",
      "journal": "Progress in brain research",
      "year": "2025",
      "doi": "10.1016/bs.pbr.2025.08.010",
      "authors": "Jana Manoj Kumar et al.",
      "keywords": "Alzheimer\u2019s disease; Artificial intelligence; Biomarkers; Early diagnosis; Gene therapy; Health disparities; Multi-omics integration; Neurodegenerative diseases; Parkinson\u2019s disease; Personalized treatment; Pharmacogenomics; Precision medicine; Translational research",
      "mesh_terms": "Humans; Aging; Precision Medicine; Neurodegenerative Diseases; Artificial Intelligence",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41314754/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; Neurology; Genomics/Genetics; Drug Discovery/Pharmacology; Endocrinology/Diabetes",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Subgroup Analysis",
      "clinical_setting": "Public Health/Population; Telehealth/Remote; Clinical Trial",
      "key_findings": "With the drive toward preventive rather than delayed care, precision medicine represents a revolutionary paradigm shift in health care, and a possibility to convert NDDs from devastatingly fatal diagnoses to easily managed chronic diseases and render equitable access to innovations possible for the masses. Success will require consistent investment in translational studies, interdisciplinary training, and global regulatory harmonization to translate the promise of precision medicine into tangibl...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41332851",
      "title": "Causal modeling of chronic kidney disease in a participatory framework for informing the inclusion of social drivers in health algorithms.",
      "abstract": "Incomplete or incorrect causal theories are a key source of bias in machine learning (ML) algorithms. Community-engaged methodologies provide an avenue for mitigating this bias through incorporating causal insights from community stakeholders into ML development. In health applications, community-engaged approaches can enable the study of social drivers of health (SDOH), which are known to shape health inequities. However, it remains challenging for SDOH to inform ML algorithms, partially because SDOH variables are known to be interrelated, yet it is difficult to elucidate the causal relationships between them. Community based system dynamics is a community-engaged methodology that can be used to co-create formal causal graphs, called causal loop diagrams, with patients. Here, we used community based system dynamics to create a causal graph representing the impacts of SDOH on the progression of chronic kidney disease, a chronic condition with SDOH-driven health disparities. We conducted focus groups with 42 participants and a day-long model building workshop with 11 participants, resulting in a final graph comprising 16 variables, 42 causal links, and 5 subsystems of semantically related SDOH variables. This final graph, representing the causal relationships between social variables relevant to chronic kidney disease, can inform the development of clinical ML algorithms and other technological interventions.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.1101/2025.11.19.25340498",
      "authors": "Foryciarz Agata et al.",
      "keywords": "causal loop diagram; chronic kidney disease; community based systems dynamics; community engaged research; group model building; health inequities; participatory AI; social drivers of health",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41332851/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Nephrology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "We conducted focus groups with 42 participants and a day-long model building workshop with 11 participants, resulting in a final graph comprising 16 variables, 42 causal links, and 5 subsystems of semantically related SDOH variables. This final graph, representing the causal relationships between social variables relevant to chronic kidney disease, can inform the development of clinical ML algorithms and other technological interventions.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12668084"
    },
    {
      "pmid": "41333106",
      "title": "Ethical and practical challenges of generative AI in healthcare and proposed solutions: a survey.",
      "abstract": "BACKGROUND: Generative artificial intelligence (AI) is rapidly transforming healthcare, but its adoption introduces significant ethical and practical challenges. Algorithmic bias, ambiguous liability, lack of transparency, and data privacy risks can undermine patient trust and create health disparities, making their resolution critical for responsible AI integration. OBJECTIVES: This systematic review analyzes the generative AI landscape in healthcare. Our objectives were to: (1) identify AI applications and their associated ethical and practical challenges; (2) evaluate current data-centric, model-centric, and regulatory solutions; and (3) propose a framework for responsible AI deployment. METHODS: Following the PRISMA 2020 statement, we conducted a systematic review of PubMed and Google Scholar for articles published between January 2020 and May 2025. A multi-stage screening process yielded 54 articles, which were analyzed using a thematic narrative synthesis. RESULTS: Our review confirmed AI's growing integration into medical training, research, and clinical practice. Key challenges identified include systemic bias from non-representative data, unresolved legal liability, the \"black box\" nature of complex models, and significant data privacy risks. Proposed solutions are multifaceted, spanning technical (e.g., explainable AI), procedural (e.g., stakeholder oversight), and regulatory strategies. DISCUSSION: Current solutions are fragmented and face significant implementation barriers. Technical fixes are insufficient without robust governance, clear legal guidelines, and comprehensive professional education. Gaps in global regulatory harmonization and frameworks ill-suited for adaptive AI persist. A multi-layered, socio-technical approach is essential to build trust and ensure the safe, equitable, and ethical deployment of generative AI in healthcare. CONCLUSIONS: The review confirmed that generative AI has a growing integration into medical training, research, and clinical practice. Key challenges identified include systemic bias stemming from non-representative data, unresolved legal liability, the \"black box\" nature of complex models, and significant data privacy risks. These challenges can undermine patient trust and create health disparities. Proposed solutions are multifaceted, spanning technical (such as explainable AI), procedural (like stakeholder oversight), and regulatory strategies.",
      "journal": "Frontiers in digital health",
      "year": "2025",
      "doi": "10.3389/fdgth.2025.1692517",
      "authors": "Tung Tina et al.",
      "keywords": "bias mitigation; ethical challenges; generative artificial intelligence; healthcare ethics; large language models; practical challenges; solution strategies; systematic review",
      "mesh_terms": "",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41333106/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability; Diverse/Representative Data",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: The review confirmed that generative AI has a growing integration into medical training, research, and clinical practice. Key challenges identified include systemic bias stemming from non-representative data, unresolved legal liability, the \"black box\" nature of complex models, and significant data privacy risks. These challenges can undermine patient trust and create health disparities.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12665710"
    },
    {
      "pmid": "41336568",
      "title": "ScorER: Exploring Annotation Bias in Vision-Based Neonatal Pain Assessment.",
      "abstract": "Neonatal pain can impair brain development, highlighting the critical clinical importance of accurate pain assessment. In recent years, automated neonatal pain assessment systems have received substantial research attention. However, their advancement is hindered by challenges in data collection and annotation. Vision-based neonatal pain annotation is convenient but susceptible to real-world disturbances that may result in information loss and incorrect annotations. In this study, we propose a deep neural network-based tool, Scoring Error Recognition (ScorER), which aims to automatic identification and intelligent diagnosis of vision-based annotation bias. By integrating the Grad-CAM algorithm, we systematically identify key regions associated with annotation bias and employ a data-driven approach to investigate their underlying causes. This research proposes a novel approach to improve annotation quality and provides new insights for advancing automated neonatal pain assessment systems in future studies.Clinical relevance- ScorER can assist caregivers in achieving high-quality vision-based neonatal pain annotations, supporting the development of large-scale, high-quality clinical neonatal pain databases for real-world applications. This facilitates AI-driven automated neonatal pain assessment research and system development for precise neonatal pain management.",
      "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference",
      "year": "2025",
      "doi": "10.1109/EMBC58623.2025.11253405",
      "authors": "Ni Yuxin et al.",
      "keywords": "",
      "mesh_terms": "Humans; Infant, Newborn; Pain Measurement; Algorithms; Pain; Neural Networks, Computer",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41336568/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Pediatrics; Neurology; Pain Management",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Data Collection; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "This research proposes a novel approach to improve annotation quality and provides new insights for advancing automated neonatal pain assessment systems in future studies.Clinical relevance- ScorER can assist caregivers in achieving high-quality vision-based neonatal pain annotations, supporting the development of large-scale, high-quality clinical neonatal pain databases for real-world applications. This facilitates AI-driven automated neonatal pain assessment research and system development fo...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41416297",
      "title": "Generative Artificial Intelligence in Urology: Navigating the Frontier of Ethical, Legal, and Clinical Challenges.",
      "abstract": "The emergence of generative artificial intelligence (AI), particularly large language models and image-generation tools, is poised to transform the field of urology. These technologies enable innovative applications in medical education, clinical decision support, patient communication, and surgical planning, extending beyond traditional analytical AI by creating new content, from synthetic clinical notes to simulated surgical environments. In urology, these capabilities translate into automated summarization of complex patient histories, generation of patient-specific three-dimensional anatomical reconstructions, and support for differential diagnosis in conditions such as prostate cancer or renal masses. However, the rapid adoption of generative AI also introduces significant ethical, legal, and clinical challenges. Risks are amplified in urological practice, where sensitive imaging data, biomarker profiles, and diagnostic decision pathways may be vulnerable to privacy breaches, algorithmic bias, or erroneous AI-generated recommendations. Hallucinated outputs, such as incorrect treatment summaries or misinterpreted radiologic features, can directly compromise patient safety if not rigorously validated. This review synthesizes the current landscape of generative AI in urology, critically examines these discipline-specific risks, and proposes a structured framework for responsible integration into clinical workflows. We highlight the need for transparent governance, bias mitigation, prospective validation, and interdisciplinary collaboration to ensure that generative AI enhances, rather than undermines, the quality, equity, and safety of urologic care.",
      "journal": "Cureus",
      "year": "2025",
      "doi": "10.7759/cureus.97047",
      "authors": "Khalil Waqas et al.",
      "keywords": "artificial intelligence; artificial intelligence in medicine; artificial intelligence in surgery; endourology; urology; urology department",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41416297/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM; Clinical Decision Support; Generative AI",
      "health_domain": "Oncology; ICU/Critical Care; Surgery; Nephrology",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "This review synthesizes the current landscape of generative AI in urology, critically examines these discipline-specific risks, and proposes a structured framework for responsible integration into clinical workflows. We highlight the need for transparent governance, bias mitigation, prospective validation, and interdisciplinary collaboration to ensure that generative AI enhances, rather than undermines, the quality, equity, and safety of urologic care.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12709556"
    },
    {
      "pmid": "41425665",
      "title": "Human reinforcement learning processes and biases: computational characterization and possible applications to behavioral public policy.",
      "abstract": "The reinforcement learning framework provides a computational and behavioral foundation for understanding how agents learn to maximize rewards and minimize punishments through interaction with their environment. This framework has been widely applied across disciplines, including artificial intelligence, animal psychology, and economics. Over the last decade, a growing body of research has shown that human reinforcement learning often deviates from normative standards, exhibiting systematic biases. The first aim of this paper is to propose a conceptual framework and a\u00a0taxonomy for evaluating computational biases within reinforcement learning. We specifically propose a distinction between praxic biases, characterized by a mismatch between internal representations and selected actions, and epistemic biases, characterized by a mismatch between past experiences and internal representations. Building on this foundation, we characterize\u00a0and discuss two primary types of epistemic biases: relative valuation and biased update. We describe their behavioral signatures\u00a0and discuss their potential adaptive roles. Finally, we eleborate on how these findings may shape future developments in both theoretical and applied domains. Notably, despite being widely used in clinical and educational settings, reinforcement-based interventions have been comparatively neglected in the domains of behavioral\u00a0public policy and decision-making\u00a0improvement, particularly when compared to more popular approaches such as nudges and boosts. In this review, we offer an explanation for this comparative neglect that we believe rooted in common historical and epistemological misconceptions, and advocate for a greater integration of reinforcement learning into the design of behavioral public policy.",
      "journal": "Mind & society",
      "year": "2025",
      "doi": "10.1007/s11299-025-00329-w",
      "authors": "Palminteri Stefano",
      "keywords": "Behavioral public policy; Cognitive biases; Context-dependent valuation; Positivity bias; Reinforcement learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41425665/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Reinforcement Learning",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "Notably, despite being widely used in clinical and educational settings, reinforcement-based interventions have been comparatively neglected in the domains of behavioral\u00a0public policy and decision-making\u00a0improvement, particularly when compared to more popular approaches such as nudges and boosts. In this review, we offer an explanation for this comparative neglect that we believe rooted in common historical and epistemological misconceptions, and advocate for a greater integration of reinforceme...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12713303"
    },
    {
      "pmid": "41445654",
      "title": "Is it possible to vaccinate AI against bias? An exploratory study in epilepsy.",
      "abstract": "IMPORTANCE: Large language models are increasingly used for clinical decision support yet may perpetuate socioeconomic biases. Whether simple prompt-based interventions can mitigate such biases remains unknown. OBJECTIVE: To determine whether a prompt-based 'inoculation' instructing large-language-models (LLMs) to disregard clinically irrelevant information can reduce bias and improve accuracy in recommendations. DESIGN: Experimental study conducted November 21 to December 11, 2025. Each clinical vignette was presented 10 times per condition to account for stochastic variance. SETTING: Publicly available web interfaces of six frontier LLMs with memory features disabled. PARTICIPANTS: No real patients were involved. Two fictional epilepsy vignettes (diagnostic and therapeutic) were created with identical clinical features but differing socioeconomic (SES) descriptors. MAIN OUTCOMES AND MEASURES: Accuracy (proportion of responses concordant with guidelines) and bias (accuracy difference between high and low SES vignettes), assessed via binary scoring based on evidence-based guidelines. RESULTS: A total of 480 LLM responses were analyzed. For diagnosis, base accuracy was 36% (43/120), with 45 percentage point bias gap (high SES 58% vs. low SES 13%); inoculation improved accuracy to 55% (66/120) and reduced bias to 27 percentage points. For treatment, base accuracy was 51% (61/120) with 25 percentage point bias gap; inoculation improved accuracy to 63% (75/120) and reduced bias to 8 percentage points. Responses to inoculation varied considerably: Gemini 3 Pro showed complete diagnostic bias elimination (low SES accuracy 0% \u2192 100%), while Sonnet 4.5 showed paradoxical worsening. CONCLUSIONS AND RELEVANCE: A simple prompt-based intervention overall reduced socioeconomic bias and improved accuracy in LLM clinical recommendations, though effects varied across models. Prompt engineering may offer a practical approach to mitigating specific AI bias in healthcare. KEY POINTS: Question: Can a simple prompt-based \"inoculation\" instructing large language models to ignore clinically irrelevant socioeconomic details reduce bias and improve accuracy in epilepsy diagnosis and treatment recommendations?Findings: In this experimental study of 480 responses from 6 large language models to paired high- vs low-socioeconomic status epilepsy vignettes, base diagnostic and treatment accuracies were 36% and 51%, respectively, with bias gaps of 45 and 25 percentage points, respectively; adding an inoculation prompt increased accuracy to 55% and 63% and reduced bias gaps to 27 and 8 percentage points, though effects varied by model, with some showing near-complete bias elimination and others demonstrating paradoxical worsening in certain conditions.Meaning: Prompt-based inoculation may offer a practical, low-cost strategy to partially mitigate socioeconomic bias and modestly improve the quality of large language model clinical recommendations, but model-specific behavior and residual disparities highlight the need for ongoing oversight and complementary bias-mitigation strategies.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2025",
      "doi": "10.64898/2025.12.18.25342563",
      "authors": "Bhansali Rohan Manish et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Preprint",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41445654/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM; Clinical Decision Support",
      "health_domain": "Neurology",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Language; Disability",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: A total of 480 LLM responses were analyzed. For diagnosis, base accuracy was 36% (43/120), with 45 percentage point bias gap (high SES 58% vs. low SES 13%); inoculation improved accuracy to 55% (66/120) and reduced bias to 27 percentage points.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12723972"
    },
    {
      "pmid": "41472841",
      "title": "Using Under-Represented Subgroup Fine Tuning to Improve Fairness for Disease Prediction.",
      "abstract": "The role of artificial intelligence is growing in healthcare and disease prediction. Because of its potential impact and demographic disparities that have been identified in machine learning models for disease prediction, there are growing concerns about transparency, accountability and fairness of these predictive models. However, very little research has investigated methods for improving model fairness in disease prediction, particularly when the sensitive attribute is multivariate and when the distribution of sensitive attribute groups is highly skewed. In this work, we explore algorithmic fairness when predicting heart disease and Alzheimer's Disease and Related Dementias (ADRD). We propose a fine tuning approach to improve model fairness that takes advantage of observations from the majority groups to build a pre-trained model and uses observations from each underrepresented subgroup to fine tune the pre-trained model, thereby incorporating additional specific knowledge about each subgroup. We find that our fine tuning approach performs better than other algorithmic fairness fixing methods across all subgroups even if the subgroup distribution is very imbalanced and some subgroups are very small. This is an important step toward understanding approaches for improving fairness for healthcare and disease prediction.",
      "journal": "Biomedical engineering systems and technologies, international joint conference, BIOSTEC ... revised selected papers. BIOSTEC (Conference)",
      "year": "2025",
      "doi": "10.5220/0013318600003911",
      "authors": "Wang Yanchen et al.",
      "keywords": "Disease Prediction; Machine Learning Fairness; Model Fine Tuning; Multivariate Sensitive Attribute",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41472841/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Cardiology; ICU/Critical Care; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "We find that our fine tuning approach performs better than other algorithmic fairness fixing methods across all subgroups even if the subgroup distribution is very imbalanced and some subgroups are very small. This is an important step toward understanding approaches for improving fairness for healthcare and disease prediction.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12746336"
    },
    {
      "pmid": "41512215",
      "title": "Understanding algorithmic fairness for clinical prediction in terms of subgroup net benefit and health equity.",
      "abstract": "There are concerns about the fairness of clinical prediction models. 'Fair' models are defined as those for which their performance or predictions are not inappropriately influenced by protected attributes such as ethnicity, gender, or socio-economic status. Researchers have raised concerns that current algorithmic fairness paradigms enforce strict egalitarianism in healthcare, leveling down the performance of models in higher-performing subgroups instead of improving it in lower-performing ones. We propose assessing the fairness of a prediction model by expanding the concept of net benefit, using it to quantify and compare the clinical impact of a model in different subgroups. We use this to explore how a model distributes benefit across a population, its impact on health inequalities, and its role in the achievement of health equity. We show how resource constraints might introduce necessary trade-offs between health equity and other objectives of healthcare systems. We showcase our proposed approach with the development of two clinical prediction models: 1) a prognostic type 2 diabetes model used by clinicians to enrol patients into a preventive care lifestyle intervention programme, and 2) a lung cancer screening algorithm used to allocate diagnostic scans across the population. This approach helps modelers better understand if a model upholds health equity by considering its performance in a clinical and social context.",
      "journal": "Epidemiology (Cambridge, Mass.)",
      "year": "2025",
      "doi": "10.1097/EDE.0000000000001949",
      "authors": "Benitez-Aurioles Jose et al.",
      "keywords": "Clinical prediction models; UK Biobank; fairness; health equity; net benefit",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41512215/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Oncology; Pulmonology; Endocrinology/Diabetes",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We showcase our proposed approach with the development of two clinical prediction models: 1) a prognostic type 2 diabetes model used by clinicians to enrol patients into a preventive care lifestyle intervention programme, and 2) a lung cancer screening algorithm used to allocate diagnostic scans across the population. This approach helps modelers better understand if a model upholds health equity by considering its performance in a clinical and social context.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "41626936",
      "title": "Quantitative bias analysis for unmeasured confounding in unanchored population-adjusted indirect comparisons.",
      "abstract": "Unanchored population-adjusted indirect comparisons (PAICs) such as matching-adjusted indirect comparison (MAIC) and simulated treatment comparison (STC) attracted a significant attention in the health technology assessment field in recent years. These methods allow for indirect comparisons by balancing different patient characteristics in single-arm studies in the case where individual patient-level data are only available for one study. However, the validity of findings from unanchored MAIC/STC analyses is frequently questioned by decision makers, due to the assumption that all potential prognostic factors and effect modifiers are accounted for. Addressing this critical concern, we introduce a sensitivity analysis algorithm for unanchored PAICs by extending quantitative bias analysis techniques traditionally used in epidemiology. Our proposed sensitivity analysis involves simulating important covariates that were not reported by the comparator study when conducting unanchored STC and enables the formal evaluating of the impact of unmeasured confounding in a quantitative manner without additional assumptions. We demonstrate the practical application of this method through a real-world case study of metastatic colorectal cancer, highlighting its utility in enhancing the robustness and credibility of unanchored PAIC results. Our findings emphasise the necessity of formal quantitative sensitivity analysis in interpreting unanchored PAIC results, as it quantifies the robustness of conclusions regarding potential unmeasured confounders and supports more robust, reliable, and informative decision-making in healthcare.",
      "journal": "Research synthesis methods",
      "year": "2025",
      "doi": "10.1017/rsm.2025.13",
      "authors": "Ren Shijie et al.",
      "keywords": "indirect treatment comparison; population-adjustment; quantitative bias analysis; unanchored simulated treatment comparison; unmeasured confounding",
      "mesh_terms": "Humans; Bias; Algorithms; Confounding Factors, Epidemiologic; Colorectal Neoplasms; Computer Simulation; Technology Assessment, Biomedical; Research Design; Reproducibility of Results; Data Interpretation, Statistical; Models, Statistical",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/41626936/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; Public Health",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We demonstrate the practical application of this method through a real-world case study of metastatic colorectal cancer, highlighting its utility in enhancing the robustness and credibility of unanchored PAIC results. Our findings emphasise the necessity of formal quantitative sensitivity analysis in interpreting unanchored PAIC results, as it quantifies the robustness of conclusions regarding potential unmeasured confounders and supports more robust, reliable, and informative decision-making in...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12527536"
    },
    {
      "pmid": "37036329",
      "title": "Craniofacial Soft-Tissue Anthropomorphic Database with Magnetic Resonance Imaging and Unbiased Diffeomorphic Registration.",
      "abstract": "BACKGROUND: Objective assessment of craniofacial surgery outcomes in a pediatric population is challenging because of the complexity of patient presentations, diversity of procedures performed, and rapid craniofacial growth. There is a paucity of robust methods to quantify anatomical measurements by age and objectively compare craniofacial dysmorphology and postoperative outcomes. Here, the authors present data in developing a racially and ethnically sensitive anthropomorphic database, providing plastic and craniofacial surgeons with \"normal\" three-dimensional anatomical parameters with which to appraise and optimize aesthetic and reconstructive outcomes. METHODS: Patients with normal craniofacial anatomy undergoing head magnetic resonance imaging (MRI) scans from 2008 to 2021 were included in this retrospective study. Images were used to construct composite (template) images with diffeomorphic image registration method using the Advanced Normalization Tools package. Composites were thresholded to generate binary three-dimensional segmentations used for anatomical measurements in Materalise Mimics. RESULTS: High-resolution MRI scans from 130 patients generated 12 composites from an average of 10 MRI sequences each: four 3-year-olds, four 4-year-olds, and four 5-year-olds (two male, two female, two Black, and two White). The average head circumference of 3-, 4-, and 5-year-old composites was 50.3, 51.5, and 51.7 cm, respectively, comparable to normative data published by the World Health Organization. CONCLUSIONS: Application of diffeomorphic registration-based image template algorithm to MRI is effective in creating composite templates to represent \"normal\" three-dimensional craniofacial and soft-tissue anatomy. Future research will focus on development of automated computational tools to characterize anatomical normality, generation of indices to grade preoperative severity, and quantification of postoperative results to reduce subjectivity bias.",
      "journal": "Plastic and reconstructive surgery",
      "year": "2024",
      "doi": "10.1097/PRS.0000000000010526",
      "authors": "Villavisanis Dillan F et al.",
      "keywords": "",
      "mesh_terms": "Humans; Child; Male; Female; Child, Preschool; Retrospective Studies; Image Processing, Computer-Assisted; Cephalometry; Algorithms; Magnetic Resonance Imaging; Imaging, Three-Dimensional",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37036329/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Generative AI",
      "health_domain": "Radiology/Medical Imaging; Surgery; Pediatrics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: Application of diffeomorphic registration-based image template algorithm to MRI is effective in creating composite templates to represent \"normal\" three-dimensional craniofacial and soft-tissue anatomy. Future research will focus on development of automated computational tools to characterize anatomical normality, generation of indices to grade preoperative severity, and quantification of postoperative results to reduce subjectivity bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37224362",
      "title": "Decoupled Unbiased Teacher for Source-Free Domain Adaptive Medical Object Detection.",
      "abstract": "Source-free domain adaptation (SFDA) aims to adapt a lightweight pretrained source model to unlabeled new domains without the original labeled source data. Due to the privacy of patients and storage consumption concerns, SFDA is a more practical setting for building a generalized model in medical object detection. Existing methods usually apply the vanilla pseudo-labeling technique, while neglecting the bias issues in SFDA, leading to limited adaptation performance. To this end, we systematically analyze the biases in SFDA medical object detection by constructing a structural causal model (SCM) and propose an unbiased SFDA framework dubbed decoupled unbiased teacher (DUT). Based on the SCM, we derive that the confounding effect causes biases in the SFDA medical object detection task at the sample level, feature level, and prediction level. To prevent the model from emphasizing easy object patterns in the biased dataset, a dual invariance assessment (DIA) strategy is devised to generate counterfactual synthetics. The synthetics are based on unbiased invariant samples in both discrimination and semantic perspectives. To alleviate overfitting to domain-specific features in SFDA, we design a cross-domain feature intervention (CFI) module to explicitly deconfound the domain-specific prior with feature intervention and obtain unbiased features. Besides, we establish a correspondence supervision prioritization (CSP) strategy for addressing the prediction bias caused by coarse pseudo-labels by sample prioritizing and robust box supervision. Through extensive experiments on multiple SFDA medical object detection scenarios, DUT yields superior performance over previous state-of-the-art unsupervised domain adaptation (UDA) and SFDA counterparts, demonstrating the significance of addressing the bias issues in this challenging task. The code is available at https://github.com/CUHK-AIM-Group/Decoupled-Unbiased-Teacher.",
      "journal": "IEEE transactions on neural networks and learning systems",
      "year": "2024",
      "doi": "10.1109/TNNLS.2023.3272389",
      "authors": "Liu Xinyu et al.",
      "keywords": "",
      "mesh_terms": "Humans; Neural Networks, Computer; Algorithms; Machine Learning; Pattern Recognition, Automated",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37224362/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Computer Vision/Imaging AI; Clustering",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Counterfactual Fairness; Transfer Learning",
      "clinical_setting": "Not specified",
      "key_findings": "Through extensive experiments on multiple SFDA medical object detection scenarios, DUT yields superior performance over previous state-of-the-art unsupervised domain adaptation (UDA) and SFDA counterparts, demonstrating the significance of addressing the bias issues in this challenging task. The code is available at https://github.com/CUHK-AIM-Group/Decoupled-Unbiased-Teacher.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37540463",
      "title": "Fairness of artificial intelligence in healthcare: review and recommendations.",
      "abstract": "In this review, we address the issue of fairness in the clinical integration of artificial intelligence (AI) in the medical field. As the clinical adoption of deep learning algorithms, a subfield of AI, progresses, concerns have arisen regarding the impact of AI biases and discrimination on patient health. This review aims to provide a comprehensive overview of concerns associated with AI fairness; discuss strategies to mitigate AI biases; and emphasize the need for cooperation among physicians, AI researchers, AI developers, policymakers, and patients to ensure equitable AI integration. First, we define and introduce the concept of fairness in AI applications in healthcare and radiology, emphasizing the benefits and challenges of incorporating AI into clinical practice. Next, we delve into concerns regarding fairness in healthcare, addressing the various causes of biases in AI and potential concerns such as misdiagnosis, unequal access to treatment, and ethical considerations. We then outline strategies for addressing fairness, such as the importance of diverse and representative data and algorithm audits. Additionally, we discuss ethical and legal considerations such as data privacy, responsibility, accountability, transparency, and explainability in AI. Finally, we present the Fairness of Artificial Intelligence Recommendations in healthcare (FAIR) statement to offer best practices. Through these efforts, we aim to provide a foundation for discussing the responsible and equitable implementation and deployment of AI in healthcare.",
      "journal": "Japanese journal of radiology",
      "year": "2024",
      "doi": "10.1007/s11604-023-01474-3",
      "authors": "Ueda Daiju et al.",
      "keywords": "Artificial intelligence; Bias; Fairness; Healthcare; Medicine; Review",
      "mesh_terms": "Humans; Artificial Intelligence; Algorithms; Radiology; Radiologists; Delivery of Health Care",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37540463/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability; Diverse/Representative Data; Bias Auditing Framework",
      "clinical_setting": "Not specified",
      "key_findings": "Finally, we present the Fairness of Artificial Intelligence Recommendations in healthcare (FAIR) statement to offer best practices. Through these efforts, we aim to provide a foundation for discussing the responsible and equitable implementation and deployment of AI in healthcare.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10764412"
    },
    {
      "pmid": "38031481",
      "title": "Translating ethical and quality principles for the effective, safe and fair development, deployment and use of artificial intelligence technologies in healthcare.",
      "abstract": "OBJECTIVE: The complexity and rapid pace of development of algorithmic technologies pose challenges for their regulation and oversight in healthcare settings. We sought to improve our institution's approach to evaluation and governance of algorithmic technologies used in clinical care and operations by creating an Implementation Guide that standardizes evaluation criteria so that local oversight is performed in an objective fashion. MATERIALS AND METHODS: Building on a framework that applies key ethical and quality principles (clinical value and safety, fairness and equity, usability and adoption, transparency and accountability, and regulatory compliance), we created concrete guidelines for evaluating algorithmic technologies at our institution. RESULTS: An Implementation Guide articulates evaluation criteria used during review of algorithmic technologies and details what evidence supports the implementation of ethical and quality principles for trustworthy health AI. Application of the processes described in the Implementation Guide can lead to algorithms that are safer as well as more effective, fair, and equitable upon implementation, as illustrated through 4 examples of technologies at different phases of the algorithmic lifecycle that underwent evaluation at our academic medical center. DISCUSSION: By providing clear descriptions/definitions of evaluation criteria and embedding them within standardized processes, we streamlined oversight processes and educated communities using and developing algorithmic technologies within our institution. CONCLUSIONS: We developed a scalable, adaptable framework for translating principles into evaluation criteria and specific requirements that support trustworthy implementation of algorithmic technologies in patient care and healthcare operations.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2024",
      "doi": "10.1093/jamia/ocad221",
      "authors": "Economou-Zavlanos Nicoleta J et al.",
      "keywords": "algorithms; artificial intelligence; health equity; quality assurance healthcare; technology assessment",
      "mesh_terms": "Humans; Artificial Intelligence; Health Facilities; Algorithms; Academic Medical Centers; Patient Compliance",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38031481/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Representation Learning",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSIONS: We developed a scalable, adaptable framework for translating principles into evaluation criteria and specific requirements that support trustworthy implementation of algorithmic technologies in patient care and healthcare operations.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10873841"
    },
    {
      "pmid": "38043883",
      "title": "Efficient adversarial debiasing with concept activation vector - Medical image case-studies.",
      "abstract": "BACKGROUND: A major hurdle for the real time deployment of the AI models is ensuring trustworthiness of these models for the unseen population. More often than not, these complex models are black boxes in which promising results are generated. However, when scrutinized, these models begin to reveal implicit biases during the decision making, particularly for the minority subgroups. METHOD: We develop an efficient adversarial de-biasing approach with partial learning by incorporating the existing concept activation vectors (CAV) methodology, to reduce racial disparities while preserving the performance of the targeted task. CAV is originally a model interpretability technique which we adopted to identify convolution layers responsible for learning race and only fine-tune up to that layer instead of fine-tuning the complete network, limiting the drop in performance RESULTS:: The methodology has been evaluated on two independent medical image case-studies - chest X-ray and mammograms, and we also performed external validation on a different racial population. On the external datasets for the chest X-ray use-case, debiased models (averaged AUC 0.87 ) outperformed the baseline convolution models (averaged AUC 0.57 ) as well as the models trained with the popular fine-tuning strategy (averaged AUC 0.81). Moreover, the mammogram models is debiased using a single dataset (white, black and Asian) and improved the performance on an external datasets (averaged AUC 0.8 to 0.86 ) with completely different population (primarily Hispanic patients). CONCLUSION: In this study, we demonstrated that the adversarial models trained only with internal data performed equally or often outperformed the standard fine-tuning strategy with data from an external setting. The adversarial training approach described can be applied regardless of predictor's model architecture, as long as the convolution model is trained using a gradient-based method. We release the training code with academic open-source license - https://github.com/ramon349/JBI2023_TCAV_debiasing.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2023.104548",
      "authors": "Correa Ramon et al.",
      "keywords": "Adversarial fairness; Concept activation vector; Debiasing; Mammogram images; X-ray images",
      "mesh_terms": "Humans; Mammography; Minority Groups; Artificial Intelligence; Clinical Decision-Making; Bias; Healthcare Disparities; Racial Groups; Diagnostic Imaging",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38043883/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Adversarial Debiasing; Transfer Learning; Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: In this study, we demonstrated that the adversarial models trained only with internal data performed equally or often outperformed the standard fine-tuning strategy with data from an external setting. The adversarial training approach described can be applied regardless of predictor's model architecture, as long as the convolution model is trained using a gradient-based method. We release the training code with academic open-source license - https://github.com/ramon349/JBI2023_TCAV_d...",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11192465"
    },
    {
      "pmid": "38070817",
      "title": "Trans-Balance: Reducing demographic disparity for prediction models in the presence of class imbalance.",
      "abstract": "INTRODUCTION: Risk prediction, including early disease detection, prevention, and intervention, is essential to precision medicine. However, systematic bias in risk estimation caused by heterogeneity across different demographic groups can lead to inappropriate or misinformed treatment decisions. In addition, low incidence (class-imbalance) outcomes negatively impact the classification performance of many standard learning algorithms which further exacerbates the racial disparity issues. Therefore, it is crucial to improve the performance of statistical and machine learning models in underrepresented populations in the presence of heavy class imbalance. METHOD: To address demographic disparity in the presence of class imbalance, we develop a novel framework, Trans-Balance, by leveraging recent advances in imbalance learning, transfer learning, and federated learning. We consider a practical setting where data from multiple sites are stored locally under privacy constraints. RESULTS: We show that the proposed Trans-Balance framework improves upon existing approaches by explicitly accounting for heterogeneity across demographic subgroups and cohorts. We demonstrate the feasibility and validity of our methods through numerical experiments and a real application to a multi-cohort study with data from participants of four large, NIH-funded cohorts for stroke risk prediction. CONCLUSION: Our findings indicate that the Trans-Balance approach significantly improves predictive performance, especially in scenarios marked by severe class imbalance and demographic disparity. Given its versatility and effectiveness, Trans-Balance offers a valuable contribution to enhancing risk prediction in biomedical research and related fields.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2023.104532",
      "authors": "Hong Chuan et al.",
      "keywords": "Class imbalance; Demographic disparity; Federated learning; Model fairness; Predictive modeling; Transfer learning",
      "mesh_terms": "Humans; Cohort Studies; Algorithms; Machine Learning; Biomedical Research; Demography",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38070817/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Federated Learning; Clinical Prediction Model",
      "health_domain": "Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Transfer Learning; Federated Learning",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: Our findings indicate that the Trans-Balance approach significantly improves predictive performance, especially in scenarios marked by severe class imbalance and demographic disparity. Given its versatility and effectiveness, Trans-Balance offers a valuable contribution to enhancing risk prediction in biomedical research and related fields.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10850917"
    },
    {
      "pmid": "38123951",
      "title": "Understanding Bias in Artificial Intelligence: A Practice Perspective.",
      "abstract": "In the fall of 2021, several experts in this space delivered a Webinar hosted by the American Society of Neuroradiology (ASNR) Diversity and Inclusion Committee, focused on expanding the understanding of bias in artificial intelligence, with a health equity lens, and provided key concepts for neuroradiologists to approach the evaluation of these tools. In this perspective, we distill key parts of this discussion, including understanding why this topic is important to neuroradiologists and lending insight on how neuroradiologists can develop a framework to assess health equity-related bias in artificial intelligence tools. In addition, we provide examples of clinical workflow implementation of these tools so that we can begin to see how artificial intelligence tools will impact discourse on equitable radiologic care. As continuous learners, we must be engaged in new and rapidly evolving technologies that emerge in our field. The Diversity and Inclusion Committee of the ASNR has addressed this subject matter through its programming content revolving around health equity in neuroradiologic advances.",
      "journal": "AJNR. American journal of neuroradiology",
      "year": "2024",
      "doi": "10.3174/ajnr.A8070",
      "authors": "Davis Melissa A et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Radiologists; Radiology; Workflow",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38123951/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "As continuous learners, we must be engaged in new and rapidly evolving technologies that emerge in our field. The Diversity and Inclusion Committee of the ASNR has addressed this subject matter through its programming content revolving around health equity in neuroradiologic advances.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11288570"
    },
    {
      "pmid": "38127076",
      "title": "Joint radiomics and spatial distribution model for MRI-based discrimination of multiple sclerosis, neuromyelitis optica spectrum disorder, and myelin-oligodendrocyte-glycoprotein-IgG-associated disorder.",
      "abstract": "OBJECTIVE: To develop a discrimination pipeline concerning both radiomics and spatial distribution features of brain lesions for discrimination of multiple sclerosis (MS), aquaporin-4-IgG-seropositive neuromyelitis optica spectrum disorder (NMOSD), and myelin-oligodendrocyte-glycoprotein-IgG-associated disorder (MOGAD). METHODS: Hyperintensity T2 lesions were delineated in 212 brain MRI scans of MS (n\u2009=\u200963), NMOSD (n\u2009=\u200987), and MOGAD (n\u2009=\u200945) patients. To avoid the effect of fixed training/test dataset sampling when developing machine learning models, patients were allocated into 4 sub-groups for cross-validation. For each scan, 351 radiomics and 27 spatial distribution features were extracted. Three models, i.e., multi-lesion radiomics, spatial distribution, and joint models, were constructed using random forest and logistic regression algorithms for differentiating: MS from the others (MS models) and MOGAD from NMOSD (MOG-NMO models), respectively. Then, the joint models were combined with demographic characteristics (i.e., age and sex) to create MS and MOG-NMO discriminators, respectively, based on which a three-disease discrimination pipeline was generated and compared with radiologists. RESULTS: For classification of both MS-others and MOG-NMO, the joint models performed better than radiomics or spatial distribution model solely. The MS discriminator achieved AUC\u2009=\u20090.909\u2009\u00b1\u20090.027 and bias-corrected C-index\u2009=\u20090.909\u2009\u00b1\u20090.027, and the MOG-NMO discriminator achieved AUC\u2009=\u20090.880\u2009\u00b1\u20090.064 and bias-corrected C-index\u2009=\u20090.883\u2009\u00b1\u20090.068. The three-disease discrimination pipeline differentiated MS, NMOSD, and MOGAD patients with 75.0% accuracy, prominently outperforming the three radiologists (47.6%, 56.6%, and 66.0%). CONCLUSIONS: The proposed pipeline integrating multi-lesion radiomics and spatial distribution features could effectively differentiate MS, NMOSD, and MOGAD. CLINICAL RELEVANCE STATEMENT: The discrimination pipeline merging both radiomics and spatial distribution features of brain lesions may facilitate the differential diagnoses of multiple sclerosis, neuromyelitis optica spectrum disorder, and myelin-oligodendrocyte-glycoprotein-IgG-associated disorder. KEY POINTS: \u2022 Our study introduces an approach by combining radiomics and spatial distribution models. \u2022 The joint model exhibited superior performance in distinguishing multiple sclerosis from aquaporin-4-IgG-seropositive neuromyelitis optica spectrum disorder and myelin-oligodendrocyte-glycoprotein-IgG-associated disorder as well as discriminating the latter two diseases. \u2022 The three-disease discrimination pipeline showcased remarkable accuracy, surpassing the performance of experienced radiologists, highlighting its potential as a valuable diagnostic tool.",
      "journal": "European radiology",
      "year": "2024",
      "doi": "10.1007/s00330-023-10529-y",
      "authors": "Luo Xiao et al.",
      "keywords": "Demyelinating autoimmune diseases, CNS; Image processing, computer-assisted; Machine learning; Multiple sclerosis; Neuromyelitis optica",
      "mesh_terms": "Humans; Neuromyelitis Optica; Multiple Sclerosis; Magnetic Resonance Imaging; Female; Male; Adult; Myelin-Oligodendrocyte Glycoprotein; Middle Aged; Immunoglobulin G; Diagnosis, Differential; Brain; Aquaporin 4; Radiomics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38127076/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Random Forest; Logistic Regression",
      "health_domain": "Radiology/Medical Imaging; Ophthalmology; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: The proposed pipeline integrating multi-lesion radiomics and spatial distribution features could effectively differentiate MS, NMOSD, and MOGAD. CLINICAL RELEVANCE STATEMENT: The discrimination pipeline merging both radiomics and spatial distribution features of brain lesions may facilitate the differential diagnoses of multiple sclerosis, neuromyelitis optica spectrum disorder, and myelin-oligodendrocyte-glycoprotein-IgG-associated disorder. KEY POINTS: \u2022 Our study introduces an ap...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38157525",
      "title": "Gender disparities in postoperative outcomes following elective spine surgery: a systematic review and meta-analysis.",
      "abstract": "OBJECTIVE: Several studies have described disparities between male and female patients following spine surgery, but no pooled analyses have performed a robust review characterizing differences in postoperative outcomes based on gender. The purpose of this study was to broadly assess the effects of gender on postoperative outcomes following elective spine surgery. METHODS: Between November 2022 and March 2023, PubMed, MEDLINE, ERIC, and Embase were queried using artificial intelligence-assisted software for relevant cohort studies. Cohort studies with a minimum sample of 100 patients conducted in the United States since 2010 were eligible. Studies related to trauma, tumors, infections, and spinal cord pathology were excluded. Independent extraction by multiple reviewers was performed using Nested Knowledge software. A fixed- or random-effects model was used if heterogeneity among included studies in a meta-analysis was < 50% or \u2265 50%, respectively. Risk of bias was assessed independently by multiple reviewers using the Newcastle-Ottawa Scale. Pooled effect sizes were calculated for readmission, nonroutine discharge (NRD), length of stay (LOS), extended LOS, reoperation, mortality, all medical complications (individual analyses for cardiovascular, deep venous thrombosis/pulmonary embolism, genitourinary, neurological, respiratory, and systemic infection complications), and wound-related complications. For each outcome, two subanalyses were performed with studies that used either center-based (single- or multi-institution) or high-volume (national or state-wide) databases. RESULTS: Across 124 included studies, male patients had an increased incidence of mortality (OR 0.54, p < 0.0001) and all medical complications (OR 0.80, p = 0.0114), specifically cardiovascular (OR 0.68, p < 0.0001) and respiratory (OR 0.76, p = 0.0008) complications. Female patients were more likely to experience a wound-related surgical complication (OR 1.16, p = 0.0183). These findings persisted in the high-volume database subanalyses. Only center-based subanalyses showed that female patients were at greater odds of experiencing an NRD (OR 1.18, p = 0.0476), longer LOS (SMD 0.23, p = 0.0036), and extended LOS (OR 1.28, p < 0.0001). CONCLUSIONS: Males are more likely to experience death and medical complications, whereas females were more likely to face wound-related surgical complications. At the institution level, females more often experience NRD and longer hospital stays. These findings may better inform preoperative expectation management and provide more detailed postoperative risk assessments based on the patient's gender.",
      "journal": "Journal of neurosurgery. Spine",
      "year": "2024",
      "doi": "10.3171/2023.11.SPINE23979",
      "authors": "Kumar Neerav et al.",
      "keywords": "complications; disparities; gender; lumbar; outcomes; spine surgery",
      "mesh_terms": "Humans; Elective Surgical Procedures; Postoperative Complications; Male; Female; Sex Factors; Length of Stay; Spine; Spinal Diseases",
      "pub_types": "Meta-Analysis; Systematic Review; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38157525/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; Oncology; Emergency Medicine; Surgery; Pathology; Pulmonology; Neurology; Infectious Disease",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; Laboratory/Pathology",
      "key_findings": "CONCLUSIONS: Males are more likely to experience death and medical complications, whereas females were more likely to face wound-related surgical complications. At the institution level, females more often experience NRD and longer hospital stays. These findings may better inform preoperative expectation management and provide more detailed postoperative risk assessments based on the patient's gender.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38160296",
      "title": "Quantifying Health Outcome Disparity in Invasive Methicillin-Resistant Staphylococcus aureus Infection using Fairness Algorithms on Real-World Data.",
      "abstract": "This study quantifies health outcome disparities in invasive Methicillin-Resistant Staphylococcus aureus (MRSA) infections by leveraging a novel artificial intelligence (AI) fairness algorithm, the Fairness-Aware Causal paThs (FACTS) decomposition, and applying it to real-world electronic health record (EHR) data. We spatiotemporally linked 9 years of EHRs from a large healthcare provider in Florida, USA, with contextual social determinants of health (SDoH). We first created a causal structure graph connecting SDoH with individual clinical measurements before/upon diagnosis of invasive MRSA infection, treatments, side effects, and outcomes; then, we applied FACTS to quantify outcome potential disparities of different causal pathways including SDoH, clinical and demographic variables. We found moderate disparity with respect to demographics and SDoH, and all the top ranked pathways that led to outcome disparities in age, gender, race, and income, included comorbidity. Prior kidney impairment, vancomycin use, and timing were associated with racial disparity, while income, rurality, and available healthcare facilities contributed to gender disparity. From an intervention standpoint, our results highlight the necessity of devising policies that consider both clinical factors and SDoH. In conclusion, this work demonstrates a practical utility of fairness AI methods in public health settings.",
      "journal": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing",
      "year": "2024",
      "doi": "10.15620/cdc:82532",
      "authors": "Jun Inyoung et al.",
      "keywords": "",
      "mesh_terms": "Humans; Methicillin-Resistant Staphylococcus aureus; Staphylococcal Infections; Artificial Intelligence; Community-Acquired Infections; Computational Biology; Algorithms; Outcome Assessment, Health Care; Anti-Bacterial Agents",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38160296/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "EHR/Health Informatics; Nephrology; Public Health; Infectious Disease",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Disability; Geographic",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "From an intervention standpoint, our results highlight the necessity of devising policies that consider both clinical factors and SDoH. In conclusion, this work demonstrates a practical utility of fairness AI methods in public health settings.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10795837"
    },
    {
      "pmid": "38165430",
      "title": "Reader bias in breast cancer screening related to cancer prevalence and artificial intelligence decision support-a reader study.",
      "abstract": "OBJECTIVES: The aim of our study was to examine how breast radiologists would be affected by high cancer prevalence and the use of artificial intelligence (AI) for decision support. MATERIALS AND METHOD: This reader study was based on selection of screening mammograms, including the original radiologist assessment, acquired in 2010 to 2013 at the Karolinska University Hospital, with a ratio of 1:1 cancer versus healthy based on a 2-year follow-up. A commercial AI system generated an exam-level positive or negative read, and image markers. Double-reading and consensus discussions were first performed without AI and later with AI, with a 6-week wash-out period in between. The chi-squared test was used to test for differences in contingency tables. RESULTS: Mammograms of 758 women were included, half with cancer and half healthy. 52% were 40-55\u00a0years; 48% were 56-75\u00a0years. In the original non-enriched screening setting, the sensitivity was 61% (232/379) at specificity 98% (323/379). In the reader study, the sensitivity without and with AI was 81% (307/379) and 75% (284/379) respectively (p\u2009<\u20090.001). The specificity without and with AI was 67% (255/379) and 86% (326/379) respectively (p\u2009<\u20090.001). The tendency to change assessment from positive to negative based on erroneous AI information differed between readers and was affected by type and number of image signs of malignancy. CONCLUSION: Breast radiologists reading a list with high cancer prevalence performed at considerably higher sensitivity and lower specificity than the original screen-readers. Adding AI information, calibrated to a screening setting, decreased sensitivity and increased specificity. CLINICAL RELEVANCE STATEMENT: Radiologist screening mammography assessments will be biased towards higher sensitivity and lower specificity by high-risk triaging and nudged towards the sensitivity and specificity setting of AI reads. After AI implementation in clinical practice, there is reason to carefully follow screening metrics to ensure the impact is desired. KEY POINTS: \u2022 Breast radiologists' sensitivity and specificity will be affected by changes brought by artificial intelligence. \u2022 Reading in a high cancer prevalence setting markedly increased sensitivity and decreased specificity. \u2022 Reviewing the binary reads by AI, negative or positive, biased screening radiologists towards the sensitivity and specificity of the AI system.",
      "journal": "European radiology",
      "year": "2024",
      "doi": "10.1007/s00330-023-10514-5",
      "authors": "Al-Bazzaz Hanen et al.",
      "keywords": "Artificial intelligence; Bias; Breast; Cancer screening; Mammography",
      "mesh_terms": "Humans; Female; Breast Neoplasms; Artificial Intelligence; Middle Aged; Mammography; Aged; Adult; Prevalence; Early Detection of Cancer; Sensitivity and Specificity; Observer Variation; Decision Support Systems, Clinical",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38165430/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Oncology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "CONCLUSION: Breast radiologists reading a list with high cancer prevalence performed at considerably higher sensitivity and lower specificity than the original screen-readers. Adding AI information, calibrated to a screening setting, decreased sensitivity and increased specificity. CLINICAL RELEVANCE STATEMENT: Radiologist screening mammography assessments will be biased towards higher sensitivity and lower specificity by high-risk triaging and nudged towards the sensitivity and specificity sett...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11255031"
    },
    {
      "pmid": "38232690",
      "title": "Artificial intelligence, ChatGPT, and other large language models for social determinants of health: Current state and future directions.",
      "abstract": "This perspective highlights the importance of addressing social determinants of health (SDOH) in patient health outcomes and health inequity, a global problem exacerbated by the COVID-19 pandemic. We provide a broad discussion on current developments in digital health and artificial intelligence (AI), including large language models (LLMs), as transformative tools in addressing SDOH factors, offering new capabilities for disease surveillance and patient care. Simultaneously, we bring attention to challenges, such as data standardization, infrastructure limitations, digital literacy, and algorithmic bias, that could hinder equitable access to AI benefits. For LLMs, we highlight potential unique challenges and risks including environmental impact, unfair labor practices, inadvertent disinformation or \"hallucinations,\" proliferation of bias, and infringement of copyrights. We propose the need for a multitiered approach to digital inclusion as an SDOH and the development of ethical and responsible AI practice frameworks globally and provide suggestions on bridging the gap from development to implementation of equitable AI technologies.",
      "journal": "Cell reports. Medicine",
      "year": "2024",
      "doi": "10.1016/j.xcrm.2023.101356",
      "authors": "Ong Jasmine Chiat Ling et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Pandemics; Social Determinants of Health; COVID-19; Language",
      "pub_types": "Journal Article; Review; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38232690/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Pulmonology; Public Health",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "For LLMs, we highlight potential unique challenges and risks including environmental impact, unfair labor practices, inadvertent disinformation or \"hallucinations,\" proliferation of bias, and infringement of copyrights. We propose the need for a multitiered approach to digital inclusion as an SDOH and the development of ethical and responsible AI practice frameworks globally and provide suggestions on bridging the gap from development to implementation of equitable AI technologies.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10829781"
    },
    {
      "pmid": "38236216",
      "title": "Attitudes among the Australian public toward AI and CCTV in suicide prevention research: A mixed methods study.",
      "abstract": "Research is underway exploring the use of closed-circuit television (CCTV) cameras and artificial intelligence (AI) for suicide prevention research in public locations where suicides occur. Given the sensitive nature and potential implications of this research, this study explored ethical concerns the public may have about research of this nature. Developed based on the principle of respect, a survey was administered to a representative sample of 1,096 Australians to understand perspectives on the research. The sample was aged 18 and older, 53% female, and 9% ethnic minority. Following an explanatory mixed methods approach, interviews and a focus group were conducted with people with a lived experience of suicide and first responders to contextualize the findings. There were broad levels of acceptance among the Australian public. Younger respondents, females, and those declining to state their ethnicity had lower levels of acceptance of CCTV research using AI for suicide prevention. Those with lived experience of suicide had higher acceptance. Qualitative data indicated concern regarding racial bias in AI and police response to suicidal crises and the need for lived experience involvement in the development and implementation of any resulting interventions. Broad public acceptance of the research aligns with the principle of respect for persons. Beneficence emerged in the context of findings emphasizing the importance of meaningfully including people with lived experience in the development and implementation of interventions resulting from this research, while justice emerged in themes expressing concerns about racial bias in AI and police response to mental health crises. (PsycInfo Database Record (c) 2024 APA, all rights reserved).",
      "journal": "The American psychologist",
      "year": "2024",
      "doi": "10.1037/amp0001215",
      "authors": "Hardy Rebecca C et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Male; Suicide Prevention; Suicide; Artificial Intelligence; Ethnicity; Australia; Minority Groups; Australasian People",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38236216/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Beneficence emerged in the context of findings emphasizing the importance of meaningfully including people with lived experience in the development and implementation of interventions resulting from this research, while justice emerged in themes expressing concerns about racial bias in AI and police response to mental health crises. (PsycInfo Database Record (c) 2024 APA, all rights reserved).",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38240671",
      "title": "Disparities in the Demographic Composition of The Cancer Imaging Archive.",
      "abstract": "Purpose To characterize the demographic distribution of The Cancer Imaging Archive (TCIA) studies and compare them with those of the U.S. cancer population. Materials and Methods In this retrospective study, data from TCIA studies were examined for the inclusion of demographic information. Of 189 studies in TCIA up until April 2023, a total of 83 human cancer studies were found to contain supporting demographic data. The median patient age and the sex, race, and ethnicity proportions of each study were calculated and compared with those of the U.S. cancer population, provided by the Surveillance, Epidemiology, and End Results Program and the Centers for Disease Control and Prevention U.S. Cancer Statistics Data Visualizations Tool. Results The median age of TCIA patients was found to be 6.84 years lower than that of the U.S. cancer population (P = .047) and contained more female than male patients (53% vs 47%). American Indian and Alaska Native, Black or African American, and Hispanic patients were underrepresented in TCIA studies by 47.7%, 35.8%, and 14.7%, respectively, compared with the U.S. cancer population. Conclusion The results demonstrate that the patient demographics of TCIA data sets do not reflect those of the U.S. cancer population, which may decrease the generalizability of artificial intelligence radiology tools developed using these imaging data sets. Keywords: Ethics, Meta-Analysis, Health Disparities, Cancer Health Disparities, Machine Learning, Artificial Intelligence, Race, Ethnicity, Sex, Age, Bias Published under a CC BY 4.0 license.",
      "journal": "Radiology. Imaging cancer",
      "year": "2024",
      "doi": "10.1148/rycan.230100",
      "authors": "Dulaney Aidan et al.",
      "keywords": "Age; Artificial Intelligence; Bias; Cancer Health Disparities; Ethics; Ethnicity; Health Disparities; Machine Learning; Meta-Analysis; Race; Sex",
      "mesh_terms": "Female; Humans; Male; Artificial Intelligence; Ethnicity; Neoplasms; Retrospective Studies; Racial Groups; Datasets as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38240671/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Meta-Analysis",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Oncology; Public Health; Infectious Disease",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "cancer population, which may decrease the generalizability of artificial intelligence radiology tools developed using these imaging data sets. Keywords: Ethics, Meta-Analysis, Health Disparities, Cancer Health Disparities, Machine Learning, Artificial Intelligence, Race, Ethnicity, Sex, Age, Bias Published under a CC BY 4.0 license.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10825717"
    },
    {
      "pmid": "38241250",
      "title": "A scientometric analysis of fairness in health AI literature.",
      "abstract": "Artificial intelligence (AI) and machine learning are central components of today's medical environment. The fairness of AI, i.e. the ability of AI to be free from bias, has repeatedly come into question. This study investigates the diversity of members of academia whose scholarship poses questions about the fairness of AI. The articles that combine the topics of fairness, artificial intelligence, and medicine were selected from Pubmed, Google Scholar, and Embase using keywords. Eligibility and data extraction from the articles were done manually and cross-checked by another author for accuracy. Articles were selected for further analysis, cleaned, and organized in Microsoft Excel; spatial diagrams were generated using Public Tableau. Additional graphs were generated using Matplotlib and Seaborn. Linear and logistic regressions were conducted using Python to measure the relationship between funding status, number of citations, and the gender demographics of the authorship team. We identified 375 eligible publications, including research and review articles concerning AI and fairness in healthcare. Analysis of the bibliographic data revealed that there is an overrepresentation of authors that are white, male, and are from high-income countries, especially in the roles of first and last author. Additionally, analysis showed that papers whose authors are based in higher-income countries were more likely to be cited more often and published in higher impact journals. These findings highlight the lack of diversity among the authors in the AI fairness community whose work gains the largest readership, potentially compromising the very impartiality that the AI fairness community is working towards.",
      "journal": "PLOS global public health",
      "year": "2024",
      "doi": "10.1371/journal.pgph.0002513",
      "authors": "Alberto Isabelle Rose I et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38241250/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Logistic Regression; Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Additionally, analysis showed that papers whose authors are based in higher-income countries were more likely to be cited more often and published in higher impact journals. These findings highlight the lack of diversity among the authors in the AI fairness community whose work gains the largest readership, potentially compromising the very impartiality that the AI fairness community is working towards.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10798451"
    },
    {
      "pmid": "38244317",
      "title": "Evolving paradigms in breast cancer screening: Balancing efficacy, personalization, and equity.",
      "abstract": "Breast cancer remains a significant global health challenge, with projections indicating a troubling increase in incidence. Breast cancer screening programs have long been hailed as life-saving initiatives, yet their true impact on mortality rates is a subject of ongoing debate. Screening poses the risk of false positives and the detection of indolent tumors, potentially leading to overtreatment. Bias factors, including lead time, length time, and selection biases, further complicate the assessment of screening efficacy. Recent studies suggest that AI-driven image analysis may revolutionize breast cancer screening, maintaining diagnostic accuracy while reducing radiologists' workload. However, the generalizability of these findings to diverse populations is a critical consideration. Personalized screening approaches and equitable access to advanced technologies are essential to mitigate disparities. In conclusion, the breast cancer screening landscape is evolving, emphasizing the need for risk stratification, appropriate imaging modalities, and a personalized approach to reduce overdiagnosis and focus on cancers with the potential to impact lives while prioritizing patient-centered care.",
      "journal": "European journal of radiology",
      "year": "2024",
      "doi": "10.1016/j.ejrad.2024.111321",
      "authors": "Pesapane Filippo et al.",
      "keywords": "Artificial intelligence; Breast neoplasms; Diagnostic imaging; Early detection of cancer; Health disparities",
      "mesh_terms": "Humans; Female; Breast Neoplasms; Early Detection of Cancer; Radiologists; Incidence; Mammography; Mass Screening",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38244317/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Oncology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Personalized screening approaches and equitable access to advanced technologies are essential to mitigate disparities. In conclusion, the breast cancer screening landscape is evolving, emphasizing the need for risk stratification, appropriate imaging modalities, and a personalized approach to reduce overdiagnosis and focus on cancers with the potential to impact lives while prioritizing patient-centered care.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38244325",
      "title": "Association between increased Subcutaneous Adipose Tissue Radiodensity and cancer mortality: Automated computation, comparison of cancer types, gender, and scanner bias.",
      "abstract": "PURPOSE: Body composition analysis using computed tomography (CT) is proposed as a predictor of cancer mortality. An association between subcutaneous adipose tissue radiodensity (SATr) and cancer-specific mortality was established, while gender effects and equipment bias were estimated. METHODS: 7,475 CT studies were selected from 17 cohorts containing CT images of untreated cancer patients who underwent follow-up for a period of 2.1-118.8 months. SATr measures were collected from published data (n\u00a0=\u00a06,718) or calculated according to CT images using a deep-learning network (n\u00a0=\u00a0757). The association between SATr and mortality was ascertained for each cohort and gender using the p-value from either logistic regression or ROC analysis. The Kruskal-Wallis test was used to analyze differences between gender distributions, and automatic segmentation was evaluated using the Dice score and five-point Likert quality scale. Gender effect, scanner bias and changes in the Hounsfield unit (HU) to detect hazards were also estimated. RESULTS: Higher SATr was associated with mortality in eight cancer types (p\u00a0<\u00a00.05). Automatic segmentation produced a score of 0.949 while the quality scale measurement was good to excellent. The extent of gender effect was 5.2 HU while the scanner bias was 10.3 HU. The minimum proposed HU change to detect a patient at risk of death was between 5.6 and 8.3 HU. CONCLUSIONS: CT imaging provides valuable assessments of body composition as part of the staging process for several cancer types, saving both time and cost. Gender specific scales and scanner bias adjustments should be carried out to successfully implement SATr measures in clinical practice.",
      "journal": "Applied radiation and isotopes : including data, instrumentation and methods for use in agriculture, industry and medicine",
      "year": "2024",
      "doi": "10.1016/j.apradiso.2024.111181",
      "authors": "Machado Marcos A D et al.",
      "keywords": "Body composition; Computed tomography; Deep-learning; Oncology; Standardization",
      "mesh_terms": "Humans; Neoplasms; Tomography, X-Ray Computed; Subcutaneous Fat; Adipose Tissue",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38244325/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Logistic Regression",
      "health_domain": "Radiology/Medical Imaging; Oncology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: CT imaging provides valuable assessments of body composition as part of the staging process for several cancer types, saving both time and cost. Gender specific scales and scanner bias adjustments should be carried out to successfully implement SATr measures in clinical practice.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38246015",
      "title": "Unveiling biases of artificial intelligence in healthcare: Navigating the promise and pitfalls.",
      "abstract": "",
      "journal": "Injury",
      "year": "2024",
      "doi": "10.1016/j.injury.2024.111358",
      "authors": "Rashid Dawood et al.",
      "keywords": "Artificial intelligence; Bias mitigation; Diverse demographic data; Healthcare disparities; Machine learning",
      "mesh_terms": "Humans; Artificial Intelligence; Bias; Delivery of Health Care",
      "pub_types": "Letter; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38246015/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38264771",
      "title": "Label-free cell classification in holographic flow cytometry through an unbiased learning strategy.",
      "abstract": "Nowadays, label-free imaging flow cytometry at the single-cell level is considered the stepforward lab-on-a-chip technology to address challenges in clinical diagnostics, biology, life sciences and healthcare. In this framework, digital holography in microscopy promises to be a powerful imaging modality thanks to its multi-refocusing and label-free quantitative phase imaging capabilities, along with the encoding of the highest information content within the imaged samples. Moreover, the recent achievements of new data analysis tools for cell classification based on deep/machine learning, combined with holographic imaging, are urging these systems toward the effective implementation of point of care devices. However, the generalization capabilities of learning-based models may be limited from biases caused by data obtained from other holographic imaging settings and/or different processing approaches. In this paper, we propose a combination of a Mask R-CNN to detect the cells, a convolutional auto-encoder, used to the image feature extraction and operating on unlabelled data, thus overcoming the bias due to data coming from different experimental settings, and a feedforward neural network for single cell classification, that operates on the above extracted features. We demonstrate the proposed approach in the challenging classification task related to the identification of drug-resistant endometrial cancer cells.",
      "journal": "Lab on a chip",
      "year": "2024",
      "doi": "10.1039/d3lc00385j",
      "authors": "Ciaparrone Gioele et al.",
      "keywords": "",
      "mesh_terms": "Flow Cytometry; Algorithms; Image Processing, Computer-Assisted; Microscopy; Holography",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38264771/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "In this paper, we propose a combination of a Mask R-CNN to detect the cells, a convolutional auto-encoder, used to the image feature extraction and operating on unlabelled data, thus overcoming the bias due to data coming from different experimental settings, and a feedforward neural network for single cell classification, that operates on the above extracted features. We demonstrate the proposed approach in the challenging classification task related to the identification of drug-resistant endo...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38278614",
      "title": "An intentional approach to managing bias in general purpose embedding models.",
      "abstract": "Advances in machine learning for health care have brought concerns about bias from the research community; specifically, the introduction, perpetuation, or exacerbation of care disparities. Reinforcing these concerns is the finding that medical images often reveal signals about sensitive attributes in ways that are hard to pinpoint by both algorithms and people. This finding raises a question about how to best design general purpose pretrained embeddings (GPPEs, defined as embeddings meant to support a broad array of use cases) for building downstream models that are free from particular types of bias. The downstream model should be carefully evaluated for bias, and audited and improved as appropriate. However, in our view, well intentioned attempts to prevent the upstream components-GPPEs-from learning sensitive attributes can have unintended consequences on the downstream models. Despite producing a veneer of technical neutrality, the resultant end-to-end system might still be biased or poorly performing. We present reasons, by building on previously published data, to support the reasoning that GPPEs should ideally contain as much information as the original data contain, and highlight the perils of trying to remove sensitive attributes from a GPPE. We also emphasise that downstream prediction models trained for specific tasks and settings, whether developed using GPPEs or not, should be carefully designed and evaluated to avoid bias that makes models vulnerable to issues such as distributional shift. These evaluations should be done by a diverse team, including social scientists, on a diverse cohort representing the full breadth of the patient population for which the final model is intended.",
      "journal": "The Lancet. Digital health",
      "year": "2024",
      "doi": "10.1016/S2589-7500(23)00227-3",
      "authors": "Weng Wei-Hung et al.",
      "keywords": "",
      "mesh_terms": "Humans; Machine Learning; Delivery of Health Care; Bias; Algorithms",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38278614/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Representation Learning; Bias Auditing Framework",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "We also emphasise that downstream prediction models trained for specific tasks and settings, whether developed using GPPEs or not, should be carefully designed and evaluated to avoid bias that makes models vulnerable to issues such as distributional shift. These evaluations should be done by a diverse team, including social scientists, on a diverse cohort representing the full breadth of the patient population for which the final model is intended.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38324545",
      "title": "Classification performance bias between training and test sets in a limited mammography dataset.",
      "abstract": "OBJECTIVES: To assess the performance bias caused by sampling data into training and test sets in a mammography radiomics study. METHODS: Mammograms from 700 women were used to study upstaging of ductal carcinoma in situ. The dataset was repeatedly shuffled and split into training (n = 400) and test cases (n = 300) forty times. For each split, cross-validation was used for training, followed by an assessment of the test set. Logistic regression with regularization and support vector machine were used as the machine learning classifiers. For each split and classifier type, multiple models were created based on radiomics and/or clinical features. RESULTS: Area under the curve (AUC) performances varied considerably across the different data splits (e.g., radiomics regression model: train 0.58-0.70, test 0.59-0.73). Performances for regression models showed a tradeoff where better training led to worse testing and vice versa. Cross-validation over all cases reduced this variability, but required samples of 500+ cases to yield representative estimates of performance. CONCLUSIONS: In medical imaging, clinical datasets are often limited to relatively small size. Models built from different training sets may not be representative of the whole dataset. Depending on the selected data split and model, performance bias could lead to inappropriate conclusions that might influence the clinical significance of the findings. ADVANCES IN KNOWLEDGE: Performance bias can result from model testing when using limited datasets. Optimal strategies for test set selection should be developed to ensure study conclusions are appropriate.",
      "journal": "PloS one",
      "year": "2024",
      "doi": "10.1371/journal.pone.0282402",
      "authors": "Hou Rui et al.",
      "keywords": "",
      "mesh_terms": "Humans; Female; Mammography; Machine Learning; Retrospective Studies",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38324545/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Logistic Regression; Support Vector Machine; Computer Vision/Imaging AI; Regression",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Regularization",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: In medical imaging, clinical datasets are often limited to relatively small size. Models built from different training sets may not be representative of the whole dataset. Depending on the selected data split and model, performance bias could lead to inappropriate conclusions that might influence the clinical significance of the findings.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10849231"
    },
    {
      "pmid": "38335791",
      "title": "(Predictable) performance bias in unsupervised anomaly detection.",
      "abstract": "BACKGROUND: With the ever-increasing amount of medical imaging data, the demand for algorithms to assist clinicians has amplified. Unsupervised anomaly detection (UAD) models promise to aid in the crucial first step of disease detection. While previous studies have thoroughly explored fairness in supervised models in healthcare, for UAD, this has so far been unexplored. METHODS: In this study, we evaluated how dataset composition regarding subgroups manifests in disparate performance of UAD models along multiple protected variables on three large-scale publicly available chest X-ray datasets. Our experiments were validated using two state-of-the-art UAD models for medical images. Finally, we introduced subgroup-AUROC (sAUROC), which aids in quantifying fairness in machine learning. FINDINGS: Our experiments revealed empirical \"fairness laws\" (similar to \"scaling laws\" for Transformers) for training-dataset composition: Linear relationships between anomaly detection performance within a subpopulation and its representation in the training data. Our study further revealed performance disparities, even in the case of balanced training data, and compound effects that exacerbate the drop in performance for subjects associated with multiple adversely affected groups. INTERPRETATION: Our study quantified the disparate performance of UAD models against certain demographic subgroups. Importantly, we showed that this unfairness cannot be mitigated by balanced representation alone. Instead, the representation of some subgroups seems harder to learn by UAD models than that of others. The empirical \"fairness laws\" discovered in our study make disparate performance in UAD models easier to estimate and aid in determining the most desirable dataset composition. FUNDING: European Research Council Deep4MI.",
      "journal": "EBioMedicine",
      "year": "2024",
      "doi": "10.1016/j.ebiom.2024.105002",
      "authors": "Meissen Felix et al.",
      "keywords": "Algorithmic bias; Anomaly detection; Artificial intelligence; Machine learning; Subgroup disparities",
      "mesh_terms": "Humans; Algorithms; Hydrolases; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38335791/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "NLP/LLM; Computer Vision/Imaging AI; Clustering",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age; Intersectional",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Subgroup Analysis",
      "clinical_setting": "Public Health/Population",
      "key_findings": "FINDINGS: Our experiments revealed empirical \"fairness laws\" (similar to \"scaling laws\" for Transformers) for training-dataset composition: Linear relationships between anomaly detection performance within a subpopulation and its representation in the training data. Our study further revealed performance disparities, even in the case of balanced training data, and compound effects that exacerbate the drop in performance for subjects associated with multiple adversely affected groups. INTERPRETAT...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10873649"
    },
    {
      "pmid": "38354967",
      "title": "Hierarchical Clustering Applied to Chronic Pain Drawings Identifies Undiagnosed Fibromyalgia: Implications for Busy Clinical Practice.",
      "abstract": "Currently-used assessments for fibromyalgia require clinicians to suspect a fibromyalgia diagnosis, a process susceptible to unintentional bias. Automated assessments of standard patient-reported outcomes (PROs) could be used to prompt formal assessments, potentially reducing bias. We sought to determine whether hierarchical clustering of patient-reported pain distribution on digital body map drawings predicted fibromyalgia diagnosis. Using an observational cohort from the University of Pittsburgh's Patient Outcomes Repository for Treatment registry, which contains PROs and electronic medical record data from 21,423 patients (March 17, 2016-June 25, 2019) presenting to pain management clinics, we tested the hypothesis that hierarchical clustering subgroup was associated with fibromyalgia diagnosis, as determined by ICD-10 code. Logistic regression revealed a significant relationship between the body map cluster subgroup and fibromyalgia diagnosis. The cluster subgroup with the most body areas selected was the most likely to receive a diagnosis of fibromyalgia when controlling for age, gender, anxiety, and depression. Despite this, more than two-thirds of patients in this cluster lacked a clinical fibromyalgia diagnosis. In an exploratory analysis to better understand this apparent underdiagnosis, we developed and applied proxies of fibromyalgia diagnostic criteria. We found that proxy diagnoses were more common than ICD-10 diagnoses, which may be due to less frequent clinical fibromyalgia diagnosis in men. Overall, we find evidence of fibromyalgia underdiagnosis, likely due to gender bias. Coupling PROs that take seconds to complete, such as a digital pain body map, with machine learning is a promising strategy to reduce bias in fibromyalgia diagnosis and improve patient outcomes. PERSPECTIVE: This investigation applies hierarchical clustering to patient-reported, digital pain body maps, finding an association between body map responses and clinical fibromyalgia diagnosis. Rapid, computer-assisted interpretation of pain body maps would be clinically useful in prompting more detailed assessments for fibromyalgia, potentially reducing gender bias.",
      "journal": "The journal of pain",
      "year": "2024",
      "doi": "10.1016/j.jpain.2024.02.003",
      "authors": "Alter Benedict J et al.",
      "keywords": "Chronic pain; Cluster analysis; Fibromyalgia; Machine learning; Pain measurement",
      "mesh_terms": "Humans; Fibromyalgia; Male; Female; Middle Aged; Chronic Pain; Adult; Cluster Analysis; Aged; Patient Reported Outcome Measures; Cohort Studies",
      "pub_types": "Journal Article; Observational Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38354967/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Logistic Regression; Clustering",
      "health_domain": "Mental Health/Psychiatry; Pain Management",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "PERSPECTIVE: This investigation applies hierarchical clustering to patient-reported, digital pain body maps, finding an association between body map responses and clinical fibromyalgia diagnosis. Rapid, computer-assisted interpretation of pain body maps would be clinically useful in prompting more detailed assessments for fibromyalgia, potentially reducing gender bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11180596"
    },
    {
      "pmid": "38377237",
      "title": "Machine Learning and Bias in Medical Imaging: Opportunities and Challenges.",
      "abstract": "Bias in health care has been well documented and results in disparate and worsened outcomes for at-risk groups. Medical imaging plays a critical role in facilitating patient diagnoses but involves multiple sources of bias including factors related to access to imaging modalities, acquisition of images, and assessment (ie, interpretation) of imaging data. Machine learning (ML) applied to diagnostic imaging has demonstrated the potential to improve the quality of imaging-based diagnosis and the precision of measuring imaging-based traits. Algorithms can leverage subtle information not visible to the human eye to detect underdiagnosed conditions or derive new disease phenotypes by linking imaging features with clinical outcomes, all while mitigating cognitive bias in interpretation. Importantly, however, the application of ML to diagnostic imaging has the potential to either reduce or propagate bias. Understanding the potential gain as well as the potential risks requires an understanding of how and what ML models learn. Common risks of propagating bias can arise from unbalanced training, suboptimal architecture design or selection, and uneven application of models. Notwithstanding these risks, ML may yet be applied to improve gain from imaging across all 3A's (access, acquisition, and assessment) for all patients. In this review, we present a framework for understanding the balance of opportunities and challenges for minimizing bias in medical imaging, how ML may improve current approaches to imaging, and what specific design considerations should be made as part of efforts to maximize the quality of health care for all.",
      "journal": "Circulation. Cardiovascular imaging",
      "year": "2024",
      "doi": "10.1161/CIRCIMAGING.123.015495",
      "authors": "Vrudhula Amey et al.",
      "keywords": "artificial intelligence; bias; diagnostic imaging; health equity; machine learning",
      "mesh_terms": "Humans; Machine Learning; Algorithms",
      "pub_types": "Review; Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38377237/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Notwithstanding these risks, ML may yet be applied to improve gain from imaging across all 3A's (access, acquisition, and assessment) for all patients. In this review, we present a framework for understanding the balance of opportunities and challenges for minimizing bias in medical imaging, how ML may improve current approaches to imaging, and what specific design considerations should be made as part of efforts to maximize the quality of health care for all.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10883605"
    },
    {
      "pmid": "38384497",
      "title": "Traditional, complementary, and integrative medicine and artificial intelligence: Novel opportunities in healthcare.",
      "abstract": "The convergence of traditional, complementary, and integrative medicine (TCIM) with artificial intelligence (AI) is a promising frontier in healthcare. TCIM is a patient-centric approach that combines conventional medicine with complementary therapies, emphasizing holistic well-being. AI can revolutionize healthcare through data-driven decision-making and personalized treatment plans. This article explores how AI technologies can complement and enhance TCIM, aligning with the shared objectives of researchers from both fields in improving patient outcomes, enhancing care quality, and promoting holistic wellness. This integration of TCIM and AI introduces exciting opportunities but also noteworthy challenges. AI may augment TCIM by assisting in early disease detection, providing personalized treatment plans, predicting health trends, and enhancing patient engagement. Challenges at the intersection of AI and TCIM include data privacy and security, regulatory complexities, maintaining the human touch in patient-provider relationships, and mitigating bias in AI algorithms. Patients' trust, informed consent, and legal accountability are all essential considerations. Future directions in AI-enhanced TCIM include advanced personalized medicine, understanding the efficacy of herbal remedies, and studying patient-provider interactions. Research on bias mitigation, patient acceptance, and trust in AI-driven TCIM healthcare is crucial. In this article, we outlined that the merging of TCIM and AI holds great promise in enhancing healthcare delivery, personalizing treatment plans, preventive care, and patient engagement. Addressing challenges and fostering collaboration between AI experts, TCIM practitioners, and policymakers, however, is vital to harnessing the full potential of this integration.",
      "journal": "Integrative medicine research",
      "year": "2024",
      "doi": "10.1016/j.imr.2024.101024",
      "authors": "Ng Jeremy Y et al.",
      "keywords": "Artificial intelligence; Complementary and integrative medicine; Novel opportunities in healthcare; Traditional medicine",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38384497/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "In this article, we outlined that the merging of TCIM and AI holds great promise in enhancing healthcare delivery, personalizing treatment plans, preventive care, and patient engagement. Addressing challenges and fostering collaboration between AI experts, TCIM practitioners, and policymakers, however, is vital to harnessing the full potential of this integration.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10879672"
    },
    {
      "pmid": "38388855",
      "title": "Assessing supervisor versus trainee viewpoints of entrustment through cognitive and affective lenses: an artificial intelligence investigation of bias in feedback.",
      "abstract": "The entrustment framework redirects assessment from considering only trainees' competence to decision-making about their readiness to perform clinical tasks independently. Since trainees and supervisors both contribute to entrustment decisions, we examined the cognitive and affective factors that underly their negotiation of trust, and whether trainee demographic characteristics may bias them. Using a document analysis approach, we adapted large language models (LLMs) to examine feedback dialogs (N\u2009=\u200924,187, each with an associated entrustment rating) between medical student trainees and their clinical supervisors. We compared how trainees and supervisors differentially documented feedback dialogs about similar tasks by identifying qualitative themes and quantitatively assessing their correlation with entrustment ratings. Supervisors' themes predominantly reflected skills related to patient presentations, while trainees' themes were broader-including clinical performance and personal qualities. To examine affect, we trained an LLM to measure feedback sentiment. On average, trainees used more negative language (5.3% lower probability of positive sentiment, p\u2009<\u20090.05) compared to supervisors, while documenting higher entrustment ratings (+\u20090.08 on a 1-4 scale, p\u2009<\u20090.05). We also found biases tied to demographic characteristics: trainees' documentation reflected more positive sentiment in the case of male trainees (+\u20091.3%, p\u2009<\u20090.05) and of trainees underrepresented in medicine (UIM) (+\u20091.3%, p\u2009<\u20090.05). Entrustment ratings did not appear to reflect these biases, neither when documented by trainee nor supervisor. As such, bias appeared to influence the emotive language trainees used to document entrustment more than the degree of entrustment they experienced. Mitigating these biases is nonetheless important because they may affect trainees' assimilation into their roles and formation of trusting relationships.",
      "journal": "Advances in health sciences education : theory and practice",
      "year": "2024",
      "doi": "10.1007/s10459-024-10311-9",
      "authors": "Gin Brian C et al.",
      "keywords": "Artificial intelligence; Clinical supervision; Entrustment; Feedback; Gender bias; Large language models; Natural language processing",
      "mesh_terms": "Humans; Trust; Clinical Competence; Students, Medical; Male; Artificial Intelligence; Female; Feedback; Cognition; Bias; Formative Feedback",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38388855/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "As such, bias appeared to influence the emotive language trainees used to document entrustment more than the degree of entrustment they experienced. Mitigating these biases is nonetheless important because they may affect trainees' assimilation into their roles and formation of trusting relationships.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11549112"
    },
    {
      "pmid": "38413380",
      "title": "Vital sign measurements demonstrate terminal digit bias and boundary effects.",
      "abstract": "OBJECTIVE: The measurement and recording of vital signs may be impacted by biases, including preferences for even and round numbers. However, other biases, such as variation due to defined numerical boundaries (also known as boundary effects), may be present in vital signs data and have not yet been investigated in a medical setting. We aimed to assess vital signs data for such biases. These parameters are clinically significant as they influence care escalation. METHODS: Vital signs data (heart rate, respiratory rate, oxygen saturation and systolic blood pressure) were collected from a tertiary hospital electronic medical record over a 2-year period. These data were analysed using polynomial regression with additional terms to assess for underreporting of out-of-range observations and overreporting numbers with terminal digits of 0 (round numbers), 2 (even numbers) and 5. RESULTS: It was found that heart rate, oxygen saturation and systolic blood pressure demonstrated 'boundary effects', with values inside the 'normal' range disproportionately more likely to be recorded. Even number bias was observed in systolic heart rate, respiratory rate and blood pressure. Preference for multiples of 5 was observed for heart rate and blood pressure. Independent overrepresentation of multiples of 10 was demonstrated in heart rate data. CONCLUSION: Although often considered objective, vital signs data are affected by bias. These biases may impact the care patients receive. Additionally, it may have implications for creating and training machine learning models that utilise vital signs data.",
      "journal": "Emergency medicine Australasia : EMA",
      "year": "2024",
      "doi": "10.1111/1742-6723.14395",
      "authors": "Kleinig Oliver et al.",
      "keywords": "bias; boundary effect; bunching; even number; round number; vital sign",
      "mesh_terms": "Humans; Vital Signs; Bias; Female; Male; Electronic Health Records; Middle Aged; Respiratory Rate; Aged; Heart Rate",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38413380/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; EHR/Health Informatics; Pulmonology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "CONCLUSION: Although often considered objective, vital signs data are affected by bias. These biases may impact the care patients receive. Additionally, it may have implications for creating and training machine learning models that utilise vital signs data.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38432311",
      "title": "Assessment of bias in scoring of AI-based radiotherapy segmentation and planning studies using modified TRIPOD and PROBAST guidelines as an example.",
      "abstract": "BACKGROUND AND PURPOSE: Studies investigating the application of Artificial Intelligence (AI) in the field of radiotherapy exhibit substantial variations in terms of quality. The goal of this study was to assess the amount of transparency and bias in scoring articles with a specific focus on AI based segmentation and treatment planning, using modified PROBAST and TRIPOD checklists, in order to provide recommendations for future guideline developers and reviewers. MATERIALS AND METHODS: The TRIPOD and PROBAST checklist items were discussed and modified using a Delphi process. After consensus was reached, 2 groups of 3 co-authors scored 2 articles to evaluate usability and further optimize the adapted checklists. Finally, 10 articles were scored by all co-authors. Fleiss' kappa was calculated to assess the reliability of agreement between observers. RESULTS: Three of the 37 TRIPOD items and 5 of the 32 PROBAST items were deemed irrelevant. General terminology in the items (e.g., multivariable prediction model, predictors) was modified to align with AI-specific terms. After the first scoring round, further improvements of the items were formulated, e.g., by preventing the use of sub-questions or subjective words and adding clarifications on how to score an item. Using the final consensus list to score the 10 articles, only 2 out of the 61 items resulted in a statistically significant kappa of 0.4 or more demonstrating substantial agreement. For 41 items no statistically significant kappa was obtained indicating that the level of agreement among multiple observers is due to chance alone. CONCLUSION: Our study showed low reliability scores with the adapted TRIPOD and PROBAST checklists. Although such checklists have shown great value during development and reporting, this raises concerns about the applicability of such checklists to objectively score scientific articles for AI applications. When developing or revising guidelines, it is essential to consider their applicability to score articles without introducing bias.",
      "journal": "Radiotherapy and oncology : journal of the European Society for Therapeutic Radiology and Oncology",
      "year": "2024",
      "doi": "10.1016/j.radonc.2024.110196",
      "authors": "Hurkmans Coen et al.",
      "keywords": "Artificial intelligence; Bias; Checklists; Distinctiveness; Guidelines; Inter-observer variation; Oncology; Radiation therapy; Transparency",
      "mesh_terms": "Humans; Artificial Intelligence; Radiotherapy Planning, Computer-Assisted; Checklist; Delphi Technique; Practice Guidelines as Topic; Bias; Reproducibility of Results; Neoplasms",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38432311/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Our study showed low reliability scores with the adapted TRIPOD and PROBAST checklists. Although such checklists have shown great value during development and reporting, this raises concerns about the applicability of such checklists to objectively score scientific articles for AI applications. When developing or revising guidelines, it is essential to consider their applicability to score articles without introducing bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38478455",
      "title": "Harvard Glaucoma Fairness: A Retinal Nerve Disease Dataset for Fairness Learning and Fair Identity Normalization.",
      "abstract": "Fairness (also known as equity interchangeably) in machine learning is important for societal well-being, but limited public datasets hinder its progress. Currently, no dedicated public medical datasets with imaging data for fairness learning are available, though underrepresented groups suffer from more health issues. To address this gap, we introduce Harvard Glaucoma Fairness (Harvard-GF), a retinal nerve disease dataset including 3,300 subjects with both 2D and 3D imaging data and balanced racial groups for glaucoma detection. Glaucoma is the leading cause of irreversible blindness globally with Blacks having doubled glaucoma prevalence than other races. We also propose a fair identity normalization (FIN) approach to equalize the feature importance between different identity groups. Our FIN approach is compared with various state-of-the-art fairness learning methods with superior performance in the racial, gender, and ethnicity fairness tasks with 2D and 3D imaging data, demonstrating the utilities of our dataset Harvard-GF for fairness learning. To facilitate fairness comparisons between different models, we propose an equity-scaled performance measure, which can be flexibly used to compare all kinds of performance metrics in the context of fairness. The dataset and code are publicly accessible via https://ophai.hms.harvard.edu/datasets/harvard-gf3300/.",
      "journal": "IEEE transactions on medical imaging",
      "year": "2024",
      "doi": "10.1109/TMI.2024.3377552",
      "authors": "Luo Yan et al.",
      "keywords": "",
      "mesh_terms": "Humans; Glaucoma; Machine Learning; Male; Databases, Factual; Female; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Middle Aged; Aged",
      "pub_types": "Journal Article; Dataset; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38478455/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Ophthalmology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "To facilitate fairness comparisons between different models, we propose an equity-scaled performance measure, which can be flexibly used to compare all kinds of performance metrics in the context of fairness. The dataset and code are publicly accessible via https://ophai.hms.harvard.edu/datasets/harvard-gf3300/.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11251413"
    },
    {
      "pmid": "38486100",
      "title": "Preserving fairness and diagnostic accuracy in private large-scale AI models for medical imaging.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) models are increasingly used in the medical domain. However, as medical data is highly sensitive, special precautions to ensure its protection are required. The gold standard for privacy preservation is the introduction of differential privacy (DP) to model training. Prior work indicates that DP has negative implications on model accuracy and fairness, which are unacceptable in medicine and represent a main barrier to the widespread use of privacy-preserving techniques. In this work, we evaluated the effect of privacy-preserving training of AI models regarding accuracy and fairness compared to non-private training. METHODS: We used two datasets: (1) A large dataset (N\u2009=\u2009193,311) of high quality clinical chest radiographs, and (2) a dataset (N\u2009=\u20091625) of 3D abdominal computed tomography (CT) images, with the task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC). Both were retrospectively collected and manually labeled by experienced radiologists. We then compared non-private deep convolutional neural networks (CNNs) and privacy-preserving (DP) models with respect to privacy-utility trade-offs measured as area under the receiver operating characteristic curve (AUROC), and privacy-fairness trade-offs, measured as Pearson's r or Statistical Parity Difference. RESULTS: We find that, while the privacy-preserving training yields lower accuracy, it largely does not amplify discrimination against age, sex or co-morbidity. However, we find an indication that difficult diagnoses and subgroups suffer stronger performance hits in private training. CONCLUSIONS: Our study shows that - under the challenging realistic circumstances of a real-life clinical dataset - the privacy-preserving training of diagnostic deep learning models is possible with excellent diagnostic accuracy and fairness. Artificial intelligence (AI), in which computers can learn to do tasks that normally require human intelligence, is particularly useful in medical imaging. However, AI should be used in a way that preserves patient privacy. We explored the balance between maintaining patient data privacy and AI performance in medical imaging. We use an approach called differential privacy to protect the privacy of patients\u2019 images. We show that, although training AI with differential privacy leads to a slight decrease in accuracy, it does not substantially increase bias against different age groups, genders, or patients with multiple health conditions. However, we notice that AI faces more challenges in accurately diagnosing complex cases and specific subgroups when trained under these privacy constraints. These findings highlight the importance of designing AI systems that are both privacy-conscious and capable of reliable diagnoses across patient groups.",
      "journal": "Communications medicine",
      "year": "2024",
      "doi": "10.1038/s43856-024-00462-6",
      "authors": "Tayebi Arasteh Soroosh et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38486100/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Neural Network; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSIONS: Our study shows that - under the challenging realistic circumstances of a real-life clinical dataset - the privacy-preserving training of diagnostic deep learning models is possible with excellent diagnostic accuracy and fairness. Artificial intelligence (AI), in which computers can learn to do tasks that normally require human intelligence, is particularly useful in medical imaging. However, AI should be used in a way that preserves patient privacy.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10940659"
    },
    {
      "pmid": "38510858",
      "title": "What Goes In, Must Come Out: Generative Artificial Intelligence Does Not Present Algorithmic Bias Across Race and Gender in Medical Residency Specialties.",
      "abstract": "Objective Artificial Intelligence (AI) has made significant inroads into various domains, including medicine, raising concerns about algorithmic bias. This study investigates the presence of biases in generative AI programs, with a specific focus on gender and racial representations across 19 medical residency specialties. Methodology This comparative study utilized DALL-E2 to generate faces representing 19 distinct residency training specialties, as identified by the Association of American Medical Colleges (AAMC), which were then compared to the AAMC's residency specialty breakdown with respect to race and gender. Results Our findings reveal an alignment between OpenAI's DALL-E2's predictions and the current demographic landscape of medical residents, suggesting an absence of algorithmic bias in this AI model. Conclusion This revelation gives rise to important ethical considerations. While AI excels at pattern recognition, it inherits and mirrors the biases present in its training data. To combat AI bias, addressing real-world disparities is imperative. Initiatives to promote inclusivity and diversity within medicine are commendable and contribute to reshaping medical education. This study underscores the need for ongoing efforts to dismantle barriers and foster inclusivity in historically male-dominated medical fields, particularly for underrepresented populations. Ultimately, our findings underscore the crucial role of real-world data quality in mitigating AI bias. As AI continues to shape healthcare and education, the pursuit of equitable, unbiased AI applications should remain at the forefront of these transformative endeavors.",
      "journal": "Cureus",
      "year": "2024",
      "doi": "10.7759/cureus.54448",
      "authors": "Lin Shu et al.",
      "keywords": "artificial intelligence; bias identification; diversity; healthcare; medical education",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38510858/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Generative AI",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "Ultimately, our findings underscore the crucial role of real-world data quality in mitigating AI bias. As AI continues to shape healthcare and education, the pursuit of equitable, unbiased AI applications should remain at the forefront of these transformative endeavors.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10951939"
    },
    {
      "pmid": "38511501",
      "title": "Leveraging large language models to foster equity in healthcare.",
      "abstract": "OBJECTIVES: Large language models (LLMs) are poised to change care delivery, but their impact on health equity is unclear. While marginalized populations have been historically excluded from early technology developments, LLMs present an opportunity to change our approach to developing, evaluating, and implementing new technologies. In this perspective, we describe the role of LLMs in supporting health equity. MATERIALS AND METHODS: We apply the National Institute on Minority Health and Health Disparities (NIMHD) research framework to explore the use of LLMs for health equity. RESULTS: We present opportunities for how LLMs can improve health equity across individual, family and organizational, community, and population health. We describe emerging concerns including biased data, limited technology diffusion, and privacy. Finally, we highlight recommendations focused on prompt engineering, retrieval augmentation, digital inclusion, transparency, and bias mitigation. CONCLUSION: The potential of LLMs to support health equity depends on making health equity a focus from the start.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2024",
      "doi": "10.1093/jamia/ocae055",
      "authors": "Rodriguez Jorge A et al.",
      "keywords": "artificial intelligence; digital inclusion; health disparities; health equity",
      "mesh_terms": "Health Equity; Humans; United States; Delivery of Health Care; Healthcare Disparities; Language",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38511501/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "NLP/LLM; Generative AI",
      "health_domain": "Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: The potential of LLMs to support health equity depends on making health equity a focus from the start.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11339521"
    },
    {
      "pmid": "38526701",
      "title": "DeepN4: Learning N4ITK Bias Field Correction for T1-weighted Images.",
      "abstract": "T1-weighted (T1w) MRI has low frequency intensity artifacts due to magnetic field inhomogeneities. Removal of these biases in T1w MRI images is a critical preprocessing step to ensure spatially consistent image interpretation. N4ITK bias field correction, the current state-of-the-art, is implemented in such a way that makes it difficult to port between different pipelines and workflows, thus making it hard to reimplement and reproduce results across local, cloud, and edge platforms. Moreover, N4ITK is opaque to optimization before and after its application, meaning that methodological development must work around the inhomogeneity correction step. Given the importance of bias fields correction in structural preprocessing and flexible implementation, we pursue a deep learning approximation / reinterpretation of the N4ITK bias fields correction to create a method which is portable, flexible, and fully differentiable. In this paper, we trained a deep learning network \"DeepN4\" on eight independent cohorts from 72 different scanners and age ranges with N4ITK-corrected T1w MRI and bias field for supervision in log space. We found that we can closely approximate N4ITK bias fields correction with na\u00efve networks. We evaluate the peak signal to noise ratio (PSNR) in test dataset against the N4ITK corrected images. The median PSNR of corrected images between N4ITK and DeepN4 was 47.96\u00a0dB. In addition, we assess the DeepN4 model on eight additional external datasets and show the generalizability of the approach. This study establishes that incompatible N4ITK preprocessing steps can be closely approximated by na\u00efve deep neural networks, facilitating more flexibility. All code and models are released at https://github.com/MASILab/DeepN4 .",
      "journal": "Neuroinformatics",
      "year": "2024",
      "doi": "10.1007/s12021-024-09655-9",
      "authors": "Kanakaraj Praitayini et al.",
      "keywords": "3D U-Net; Bias field correction; Inhomogeneity; N4ITK; T1-weighted images",
      "mesh_terms": "Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Algorithms; Neural Networks, Computer; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38526701/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "This study establishes that incompatible N4ITK preprocessing steps can be closely approximated by na\u00efve deep neural networks, facilitating more flexibility. All code and models are released at https://github.com/MASILab/DeepN4 .",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11182041"
    },
    {
      "pmid": "38568767",
      "title": "De-Biased Disentanglement Learning for Pulmonary Embolism Survival Prediction on Multimodal Data.",
      "abstract": "Health disparities among marginalized populations with lower socioeconomic status significantly impact the fairness and effectiveness of healthcare delivery. The increasing integration of artificial intelligence (AI) into healthcare presents an opportunity to address these inequalities, provided that AI models are free from bias. This paper aims to address the bias challenges by population disparities within healthcare systems, existing in the presentation of and development of algorithms, leading to inequitable medical implementation for conditions such as pulmonary embolism (PE) prognosis. In this study, we explore the diverse bias in healthcare systems, which highlights the demand for a holistic framework to reducing bias by complementary aggregation. By leveraging de-biasing deep survival prediction models, we propose a framework that disentangles identifiable information from images, text reports, and clinical variables to mitigate potential biases within multimodal datasets. Our study offers several advantages over traditional clinical-based survival prediction methods, including richer survival-related characteristics and bias-complementary predicted results. By improving the robustness of survival analysis through this framework, we aim to benefit patients, clinicians, and researchers by enhancing fairness and accuracy in healthcare AI systems.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2024",
      "doi": "10.1109/JBHI.2024.3384848",
      "authors": "Zhong Zhusi et al.",
      "keywords": "",
      "mesh_terms": "Humans; Pulmonary Embolism; Algorithms; Survival Analysis; Female; Male; Middle Aged; Aged; Prognosis; Databases, Factual",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38568767/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Prediction Model; Survival Analysis",
      "health_domain": "Pulmonology",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Representation Learning",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Our study offers several advantages over traditional clinical-based survival prediction methods, including richer survival-related characteristics and bias-complementary predicted results. By improving the robustness of survival analysis through this framework, we aim to benefit patients, clinicians, and researchers by enhancing fairness and accuracy in healthcare AI systems.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38590455",
      "title": "A Gender-Bias-Mitigated, Data-Driven Precision Medicine System to Assist in the Selection of Biological Treatments of Grade 3 and 4 Knee Osteoarthritis: Development and Preliminary Validation of precisionKNEE.",
      "abstract": "Objective To identify key variables predictive of patient responses to microfragmented adipose tissue (MFAT) treatment in knee osteoarthritis (KOA) and evaluate its potential to delay or mitigate the need for total knee replacement (TKR). Methods We utilised a dataset comprising 329 patients treated with MFAT for KOA, incorporating variables such as gender, age, BMI, arthritic aetiology, radiological grade, and Oxford Knee Scores (OKS) pre- and post-treatment. We employed random forest regressors for model training and testing, with gender bias mitigation and outlier detection to enhance prediction accuracy. Model performance was assessed through root mean squared error (RMSE) and mean absolute error (MAE), with further validation in a TKR-suitable patient subset. Results The model achieved a test RMSE of 6.72 and an MAE of 5.38, reflecting moderate predictive accuracy across the patient cohort. Stratification by gender revealed no statistically significant differences between actual and predicted OKS improvements (p-values: males = 0.93, females = 0.92). For the subset of patients suitable for TKR, the model presented an increased RMSE of 9.77 and MAE of 7.81, indicating reduced accuracy in this group. The decision tree analysis identified pre-operative OKS, radiological grade, and gender as significant predictors of post-treatment outcomes, with pre-operative OKS being the most critical determinant. Patients with lower pre-operative OKS showed varying responses based on radiological severity and gender, suggesting a nuanced interaction between these factors in determining treatment efficacy. Conclusion This study highlights the potential of MFAT as a non-surgical alternative for KOA treatment, emphasising the importance of personalised patient assessments. While promising, the predictive model warrants further refinement and validation with a larger, more diverse dataset to improve its utility in clinical decision-making for KOA management.",
      "journal": "Cureus",
      "year": "2024",
      "doi": "10.7759/cureus.55832",
      "authors": "Heidari Nima et al.",
      "keywords": "artificial intelligence in health care; knee osteoarthritis/ koa; microfragmented fat injection; non-surgical treatment; orthopaedic biologics; precision medicine; total knee replacement(tkr)",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38590455/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Random Forest; Decision Tree; Clinical Prediction Model",
      "health_domain": "Surgery",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Diverse/Representative Data",
      "clinical_setting": "Not specified",
      "key_findings": "Conclusion This study highlights the potential of MFAT as a non-surgical alternative for KOA treatment, emphasising the importance of personalised patient assessments. While promising, the predictive model warrants further refinement and validation with a larger, more diverse dataset to improve its utility in clinical decision-making for KOA management.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11000206"
    },
    {
      "pmid": "38600282",
      "title": "Generative models improve fairness of medical classifiers under distribution shifts.",
      "abstract": "Domain generalization is a ubiquitous challenge for machine learning in healthcare. Model performance in real-world conditions might be lower than expected because of discrepancies between the data encountered during deployment and development. Underrepresentation of some groups or conditions during model development is a common cause of this phenomenon. This challenge is often not readily addressed by targeted data acquisition and 'labeling' by expert clinicians, which can be prohibitively expensive or practically impossible because of the rarity of conditions or the available clinical expertise. We hypothesize that advances in generative artificial intelligence can help mitigate this unmet need in a steerable fashion, enriching our training dataset with synthetic examples that address shortfalls of underrepresented conditions or subgroups. We show that diffusion models can automatically learn realistic augmentations from data in a label-efficient manner. We demonstrate that learned augmentations make models more robust and statistically fair in-distribution and out of distribution. To evaluate the generality of our approach, we studied three distinct medical imaging contexts of varying difficulty: (1) histopathology, (2) chest X-ray and (3) dermatology images. Complementing real samples with synthetic ones improved the robustness of models in all three medical tasks and increased fairness by improving the accuracy of clinical diagnosis within underrepresented groups, especially out of distribution.",
      "journal": "Nature medicine",
      "year": "2024",
      "doi": "10.1038/s41591-024-02838-6",
      "authors": "Ktena Ira et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38600282/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Computer Vision/Imaging AI; Generative AI",
      "health_domain": "Radiology/Medical Imaging; Dermatology; ICU/Critical Care; Pathology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Development/Training; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Laboratory/Pathology",
      "key_findings": "To evaluate the generality of our approach, we studied three distinct medical imaging contexts of varying difficulty: (1) histopathology, (2) chest X-ray and (3) dermatology images. Complementing real samples with synthetic ones improved the robustness of models in all three medical tasks and increased fairness by improving the accuracy of clinical diagnosis within underrepresented groups, especially out of distribution.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11031395"
    },
    {
      "pmid": "38606108",
      "title": "Impact of bias field correction on 0.35 T pelvic MR images: evaluation on generative adversarial network-based OARs' auto-segmentation and visual grading assessment.",
      "abstract": "PURPOSE: Magnetic resonance imaging (MRI)-guided radiotherapy enables adaptive treatment plans based on daily anatomical changes and accurate organ visualization. However, the bias field artifact can compromise image quality, affecting diagnostic accuracy and quantitative analyses. This study aims to assess the impact of bias field correction on 0.35 T pelvis MRIs by evaluating clinical anatomy visualization and generative adversarial network (GAN) auto-segmentation performance. MATERIALS AND METHODS: 3D simulation MRIs from 60 prostate cancer patients treated on MR-Linac (0.35 T) were collected and preprocessed with the N4ITK algorithm for bias field correction. A 3D GAN architecture was trained, validated, and tested on 40, 10, and 10 patients, respectively, to auto-segment the organs at risk (OARs) rectum and bladder. The GAN was trained and evaluated either with the original or the bias-corrected MRIs. The Dice similarity coefficient (DSC) and 95th percentile Hausdorff distance (HD95th) were computed for the segmented volumes of each patient. The Wilcoxon signed-rank test assessed the statistical difference of the metrics within OARs, both with and without bias field correction. Five radiation oncologists blindly scored 22 randomly chosen patients in terms of overall image quality and visibility of boundaries (prostate, rectum, bladder, seminal vesicles) of the original and bias-corrected MRIs. Bennett's S score and Fleiss' kappa were used to assess the pairwise interrater agreement and the interrater agreement among all the observers, respectively. RESULTS: In the test set, the GAN trained and evaluated on original and bias-corrected MRIs showed DSC/HD95th of 0.92/5.63\u00a0mm and 0.92/5.91\u00a0mm for the bladder and 0.84/10.61\u00a0mm and 0.83/9.71\u00a0mm for the rectum. No statistical differences in the distribution of the evaluation metrics were found neither for the bladder (DSC: p = 0.07; HD95th: p = 0.35) nor for the rectum (DSC: p = 0.32; HD95th: p = 0.63). From the clinical visual grading assessment, the bias-corrected MRI resulted mostly in either no change or an improvement of the image quality and visualization of the organs' boundaries compared with the original MRI. CONCLUSION: The bias field correction did not improve the anatomy visualization from a clinical point of view and the OARs' auto-segmentation outputs generated by the GAN.",
      "journal": "Frontiers in oncology",
      "year": "2024",
      "doi": "10.3389/fonc.2024.1294252",
      "authors": "Vagni Marica et al.",
      "keywords": "0.35 T MRIgRT; N4ITK algorithm; bias field artifact; generative adversarial networks; prostate cancer; visual grading assessment",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38606108/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Generative AI",
      "health_domain": "Radiology/Medical Imaging; Oncology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: The bias field correction did not improve the anatomy visualization from a clinical point of view and the OARs' auto-segmentation outputs generated by the GAN.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11007142"
    },
    {
      "pmid": "38635456",
      "title": "Understanding and Mitigating Bias in Imaging Artificial Intelligence.",
      "abstract": "Artificial intelligence (AI) algorithms are prone to bias at multiple stages of model development, with potential for exacerbating health disparities. However, bias in imaging AI is a complex topic that encompasses multiple coexisting definitions. Bias may refer to unequal preference to a person or group owing to preexisting attitudes or beliefs, either intentional or unintentional. However, cognitive bias refers to systematic deviation from objective judgment due to reliance on heuristics, and statistical bias refers to differences between true and expected values, commonly manifesting as systematic error in model prediction (ie, a model with output unrepresentative of real-world conditions). Clinical decisions informed by biased models may lead to patient harm due to action on inaccurate AI results or exacerbate health inequities due to differing performance among patient populations. However, while inequitable bias can harm patients in this context, a mindful approach leveraging equitable bias can address underrepresentation of minority groups or rare diseases. Radiologists should also be aware of bias after AI deployment such as automation bias, or a tendency to agree with automated decisions despite contrary evidence. Understanding common sources of imaging AI bias and the consequences of using biased models can guide preventive measures to mitigate its impact. Accordingly, the authors focus on sources of bias at stages along the imaging machine learning life cycle, attempting to simplify potentially intimidating technical terminology for general radiologists using AI tools in practice or collaborating with data scientists and engineers for AI tool development. The authors review definitions of bias in AI, describe common sources of bias, and present recommendations to guide quality control measures to mitigate the impact of bias in imaging AI. Understanding the terms featured in this article will enable a proactive approach to identifying and mitigating bias in imaging AI. Published under a CC BY 4.0 license. Test Your Knowledge questions for this article are available in the supplemental material. See the invited commentary by Rouzrokh and Erickson in this issue.",
      "journal": "Radiographics : a review publication of the Radiological Society of North America, Inc",
      "year": "2024",
      "doi": "10.1148/rg.230067",
      "authors": "Tejani Ali S et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Algorithms; Automation; Machine Learning; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38635456/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Test Your Knowledge questions for this article are available in the supplemental material. See the invited commentary by Rouzrokh and Erickson in this issue.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38645091",
      "title": "A deep learning-based approach for unbiased kinematic analysis in CNS injury.",
      "abstract": "UNLABELLED: Traumatic spinal cord injury (SCI) is a devastating condition that impacts over 300,000 individuals in the US alone. Depending on the severity of the injury, SCI can lead to varying degrees of sensorimotor deficits and paralysis. Despite advances in our understanding of the underlying pathological mechanisms of SCI and the identification of promising molecular targets for repair and functional restoration, few therapies have made it into clinical use. To improve the success rate of clinical translation, more robust, sensitive, and reproducible means of functional assessment are required. The gold standards for the evaluation of locomotion in rodents with SCI are the Basso Beattie Bresnahan (BBB) and Basso Mouse Scale (BMS) tests. To overcome the shortcomings of current methods, we developed two separate marker-less kinematic analysis paradigms in mice, MotorBox and MotoRater, based on deep-learning algorithms generated with the DeepLabCut open-source toolbox. The MotorBox system uses an originally designed, custom-made chamber, and the MotoRater system was implemented on a commercially available MotoRater device. We validated the MotorBox and MotoRater systems by comparing them with the traditional BMS test and extracted metrics of movement and gait that can provide an accurate and sensitive representation of mouse locomotor function post-injury, while eliminating investigator bias and variability. The integration of MotorBox and/or MotoRater assessments with BMS scoring will provide a much wider range of information on specific aspects of locomotion, ensuring the accuracy, rigor, and reproducibility of behavioral outcomes after SCI. HIGHLIGHTS: MotorBox and MotoRater systems are two novel marker-less kinematic analysis paradigms in mice, based on deep-learning algorithms generated with DeepLabCut.MotorBox and MotoRater systems are highly sensitive, accurate and unbiased in analyzing locomotor behavior in mice.MotorBox and MotoRater systems allow for sensitive detection of SCI-induced changes in movement metrics, including range of motion, gait, coordination, and speed.MotorBox and MotoRater systems allow for detection of movement metrics not measurable with the BMS.",
      "journal": "bioRxiv : the preprint server for biology",
      "year": "2024",
      "doi": "10.1101/2024.04.08.588606",
      "authors": "Ascona Maureen et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38645091/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Emergency Medicine",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The integration of MotorBox and/or MotoRater assessments with BMS scoring will provide a much wider range of information on specific aspects of locomotion, ensuring the accuracy, rigor, and reproducibility of behavioral outcomes after SCI. HIGHLIGHTS: MotorBox and MotoRater systems are two novel marker-less kinematic analysis paradigms in mice, based on deep-learning algorithms generated with DeepLabCut.MotorBox and MotoRater systems are highly sensitive, accurate and unbiased in analyzing locom...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11030365"
    },
    {
      "pmid": "38681759",
      "title": "Analyzing the Impact of Personalization on Fairness in Federated Learning for Healthcare.",
      "abstract": "As machine learning (ML) usage becomes more popular in the healthcare sector, there are also increasing concerns about potential biases and risks such as privacy. One countermeasure is to use federated learning (FL) to support collaborative learning without the need for patient data sharing across different organizations. However, the inherent heterogeneity of data distributions among participating FL parties poses challenges for exploring group fairness in FL. While personalization within FL can handle performance degradation caused by data heterogeneity, its influence on group fairness is not fully investigated. Therefore, the primary focus of this study is to rigorously assess the impact of personalized FL on group fairness in the healthcare domain, offering a comprehensive understanding of how personalized FL affects group fairness in clinical outcomes. We conduct an empirical analysis using two prominent real-world Electronic Health Records (EHR) datasets, namely eICU and MIMIC-IV. Our methodology involves a thorough comparison between personalized FL and two baselines: standalone training, where models are developed independently without FL collaboration, and standard FL, which aims to learn a global model via the FedAvg algorithm. We adopt Ditto as our personalized FL approach, which enables each client in FL to develop its own personalized model through multi-task learning. Our assessment is achieved through a series of evaluations, comparing the predictive performance (i.e., AUROC and AUPRC) and fairness gaps (i.e., EOPP, EOD, and DP) of these methods. Personalized FL demonstrates superior predictive accuracy and fairness over standalone training across both datasets. Nevertheless, in comparison with standard FL, personalized FL shows improved predictive accuracy but does not consistently offer better fairness outcomes. For instance, in the 24-h in-hospital mortality prediction task, personalized FL achieves an average EOD of 27.4% across racial groups in the eICU dataset and 47.8% in MIMIC-IV. In comparison, standard FL records a better EOD of 26.2% for eICU and 42.0% for MIMIC-IV, while standalone training yields significantly worse EOD of 69.4% and 54.7% on these datasets, respectively. Our analysis reveals that personalized FL has the potential to enhance fairness in comparison to standalone training, yet it does not consistently ensure fairness improvements compared to standard FL. Our findings also show that while personalization can improve fairness for more biased hospitals (i.e., hospitals having larger fairness gaps in standalone training), it can exacerbate fairness issues for less biased ones. These insights suggest that the integration of personalized FL with additional strategic designs could be key to simultaneously boosting prediction accuracy and reducing fairness disparities. The findings and opportunities outlined in this paper can inform the research agenda for future studies, to overcome the limitations and further advance health equity research.",
      "journal": "Journal of healthcare informatics research",
      "year": "2024",
      "doi": "10.1007/s41666-024-00164-7",
      "authors": "Wang Tongnian et al.",
      "keywords": "Federated learning; Group fairness; Health disparities; Personalization; Privacy",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38681759/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Federated Learning; Generative AI",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Federated Learning; Multi-task Learning",
      "clinical_setting": "Hospital/Inpatient; ICU",
      "key_findings": "These insights suggest that the integration of personalized FL with additional strategic designs could be key to simultaneously boosting prediction accuracy and reducing fairness disparities. The findings and opportunities outlined in this paper can inform the research agenda for future studies, to overcome the limitations and further advance health equity research.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11052754"
    },
    {
      "pmid": "38691911",
      "title": "Use of natural language processing to uncover racial bias in obstetrical documentation.",
      "abstract": "Natural Language Processing (NLP), a form of Artificial Intelligence, allows free-text based clinical documentation to be integrated in ways that facilitate data analysis, data interpretation and formation of individualized medical and obstetrical care. In this cross-sectional study, we identified all births during the study period carrying the radiology-confirmed diagnosis of fibroid uterus in pregnancy (defined as size of largest diameter of >5\u00a0cm) by using an NLP platform and compared it to non-NLP derived data using ICD10 codes of the same diagnosis. We then compared the two sets of data and stratified documentation gaps by race. Using fibroid uterus in pregnancy as a marker, we found that Black patients were more likely to have the diagnosis entered late into the patient's chart or had missing documentation of the diagnosis. With appropriate algorithm definitions, cross referencing and thorough validation steps, NLP can contribute to identifying areas of documentation gaps and improve quality of care.",
      "journal": "Clinical imaging",
      "year": "2024",
      "doi": "10.1016/j.clinimag.2024.110164",
      "authors": "Futterman Itamar D et al.",
      "keywords": "Artificial intelligence; Fibroids; Natural language processing; Racial bias",
      "mesh_terms": "Humans; Natural Language Processing; Female; Pregnancy; Cross-Sectional Studies; Documentation; Uterine Neoplasms; Racism; Leiomyoma; Adult; Obstetrics; Pregnancy Complications, Neoplastic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38691911/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Radiology/Medical Imaging; Oncology; Obstetrics/Maternal Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Using fibroid uterus in pregnancy as a marker, we found that Black patients were more likely to have the diagnosis entered late into the patient's chart or had missing documentation of the diagnosis. With appropriate algorithm definitions, cross referencing and thorough validation steps, NLP can contribute to identifying areas of documentation gaps and improve quality of care.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38702378",
      "title": "Missingness and algorithmic bias: an example from the United States National Outbreak Reporting System, 2009-2019.",
      "abstract": "Growing debates about algorithmic bias in public health surveillance lack specific examples. We tested a common assumption that exposure and illness periods coincide and demonstrated how algorithmic bias can arise due to missingness of critical information related to illness and exposure durations. We examined 9407 outbreaks recorded by the United States National Outbreak Reporting System (NORS) from January 1, 2009 through December 31, 2019 and detected algorithmic bias, a systematic over- or under-estimation of foodborne disease outbreak (FBDO) durations due to missing start and end dates. For 7037 (75%) FBDOs with complete date-time information,\u2009~\u200960% reported that the exposure period ended before the illness period started. For 2079 (87.7%) FBDOs with missing exposure dates, average illness durations were\u2009~\u20095.3 times longer (p\u2009<\u20090.001) than those with complete information, prompting the potential for algorithmic bias. Modern surveillance systems must be equipped with investigative capacities to examine and assess structural data missingness that can lead to bias.",
      "journal": "Journal of public health policy",
      "year": "2024",
      "doi": "10.1057/s41271-024-00477-2",
      "authors": "Diemer Emily et al.",
      "keywords": "Foodborne disease outbreak; Missing data; National Outbreak Reporting System (NORS); Outbreak progression",
      "mesh_terms": "Humans; United States; Disease Outbreaks; Algorithms; Bias; Foodborne Diseases; Public Health Surveillance; Population Surveillance",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38702378/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "For 2079 (87.7%) FBDOs with missing exposure dates, average illness durations were\u2009~\u20095.3 times longer (p\u2009<\u20090.001) than those with complete information, prompting the potential for algorithmic bias. Modern surveillance systems must be equipped with investigative capacities to examine and assess structural data missingness that can lead to bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38740316",
      "title": "A roadmap to artificial intelligence (AI): Methods for designing and building AI ready data to promote fairness.",
      "abstract": "OBJECTIVES: We evaluated methods for preparing electronic health record data to reduce bias before applying artificial intelligence (AI). METHODS: We created methods for transforming raw data into a data framework for applying machine learning and natural language processing techniques for predicting falls and fractures. Strategies such as inclusion and reporting for multiple races, mixed data sources such as outpatient, inpatient, structured codes, and unstructured notes, and addressing missingness were applied to raw data to promote a reduction in bias. The raw data was carefully curated using validated definitions to create data variables such as age, race, gender, and healthcare utilization. For the formation of these variables, clinical, statistical, and data expertise were used. The research team included a variety of experts with diverse professional and demographic backgrounds to include diverse perspectives. RESULTS: For the prediction of falls, information extracted from radiology reports was converted to a matrix for applying machine learning. The processing of the data resulted in an input of 5,377,673 reports to the machine learning algorithm, out of which 45,304 were flagged as positive and 5,332,369 as negative for falls. Processed data resulted in lower missingness and a better representation of race and diagnosis codes. For fractures, specialized algorithms extracted snippets of text around keywork \"femoral\" from dual x-ray absorptiometry (DXA) scans to identify femoral neck T-scores that are important for predicting fracture risk. The natural language processing algorithms yielded 98% accuracy and 2% error rate The methods to prepare data for input to artificial intelligence processes are reproducible and can be applied to other studies. CONCLUSION: The life cycle of data from raw to analytic form includes data governance, cleaning, management, and analysis. When applying artificial intelligence methods, input data must be prepared optimally to reduce algorithmic bias, as biased output is harmful. Building AI-ready data frameworks that improve efficiency can contribute to transparency and reproducibility. The roadmap for the application of AI involves applying specialized techniques to input data, some of which are suggested here. This study highlights data curation aspects to be considered when preparing data for the application of artificial intelligence to reduce bias.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104654",
      "authors": "Kidwai-Khan Farah et al.",
      "keywords": "Algorithms; Artificial Intelligence; Data preparation; Diversity; Fairness; Inclusion",
      "mesh_terms": "Humans; Electronic Health Records; Artificial Intelligence; Natural Language Processing; Accidental Falls; Algorithms; Machine Learning; Fractures, Bone; Female",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38740316/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Radiology/Medical Imaging; EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; Primary Care/Outpatient",
      "key_findings": "CONCLUSION: The life cycle of data from raw to analytic form includes data governance, cleaning, management, and analysis. When applying artificial intelligence methods, input data must be prepared optimally to reduce algorithmic bias, as biased output is harmful. Building AI-ready data frameworks that improve efficiency can contribute to transparency and reproducibility.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11144439"
    },
    {
      "pmid": "38746448",
      "title": "Measuring algorithmic bias to analyze the reliability of AI tools that predict depression risk using smartphone sensed-behavioral data.",
      "abstract": "AI tools intend to transform mental healthcare by providing remote estimates of depression risk using behavioral data collected by sensors embedded in smartphones. While these tools accurately predict elevated symptoms in small, homogenous populations, recent studies show that these tools are less accurate in larger, more diverse populations. In this work, we show that accuracy is reduced because sensed-behaviors are unreliable predictors of depression across individuals; specifically the sensed-behaviors that predict depression risk are inconsistent across demographic and socioeconomic subgroups. We first identified subgroups where a developed AI tool underperformed by measuring algorithmic bias, where subgroups with depression were incorrectly predicted to be at lower risk than healthier subgroups. We then found inconsistencies between sensed-behaviors predictive of depression across these subgroups. Our findings suggest that researchers developing AI tools predicting mental health from behavior should think critically about the generalizability of these tools, and consider tailored solutions for targeted populations.",
      "journal": "Research square",
      "year": "2024",
      "doi": "10.21203/rs.3.rs-3044613/v1",
      "authors": "Adler Daniel A et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38746448/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Telehealth/Remote",
      "key_findings": "We then found inconsistencies between sensed-behaviors predictive of depression across these subgroups. Our findings suggest that researchers developing AI tools predicting mental health from behavior should think critically about the generalizability of these tools, and consider tailored solutions for targeted populations.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11092819"
    },
    {
      "pmid": "38785733",
      "title": "Parkinson's Disease Recognition Using Decorrelated Convolutional Neural Networks: Addressing Imbalance and Scanner Bias in rs-fMRI Data.",
      "abstract": "Parkinson's disease (PD) is a neurodegenerative and progressive disease that impacts the nerve cells in the brain and varies from person to person. The exact cause of PD is still unknown, and the diagnosis of PD does not include a specific objective test with certainty. Although deep learning has made great progress in medical neuroimaging analysis, these methods are very susceptible to biases present in neuroimaging datasets. An innovative decorrelated deep learning technique is introduced to mitigate class bias and scanner bias while simultaneously focusing on finding distinguishing characteristics in resting-state functional MRI (rs-fMRI) data, which assists in recognizing PD with good accuracy. The decorrelation function reduces the nonlinear correlation between features and bias in order to learn bias-invariant features. The publicly available Parkinson's Progression Markers Initiative (PPMI) dataset, referred to as a single-scanner imbalanced dataset in this study, was used to validate our method. The imbalanced dataset problem affects the performance of the deep learning framework by overfitting to the majority class. To resolve this problem, we propose a new decorrelated convolutional neural network (DcCNN) framework by applying decorrelation-based optimization to convolutional neural networks (CNNs). An analysis of evaluation metrics comparisons shows that integrating the decorrelation function boosts the performance of PD recognition by removing class bias. Specifically, our DcCNN models perform significantly better than existing traditional approaches to tackle the imbalance problem. Finally, the same framework can be extended to create scanner-invariant features without significantly impacting the performance of a model. The obtained dataset is a multiscanner dataset, which leads to scanner bias due to the differences in acquisition protocols and scanners. The multiscanner dataset is a combination of two publicly available datasets, namely, PPMI and FTLDNI-the frontotemporal lobar degeneration neuroimaging initiative (NIFD) dataset. The results of t-distributed stochastic neighbor embedding (t-SNE) and scanner classification accuracy of our proposed feature extraction-DcCNN (FE-DcCNN) model validated the effective removal of scanner bias. Our method achieves an average accuracy of 77.80% on a multiscanner dataset for differentiating PD from a healthy control, which is superior to the DcCNN model trained on a single-scanner imbalanced dataset.",
      "journal": "Biosensors",
      "year": "2024",
      "doi": "10.3390/bios14050259",
      "authors": "Patil Pranita et al.",
      "keywords": "DcCNN; FE-DcCNN; Parkinson\u2019s disease; class bias; decorrelation; deep learning; invariant features; rs-fMRI image; scanner bias",
      "mesh_terms": "Parkinson Disease; Humans; Magnetic Resonance Imaging; Neural Networks, Computer; Deep Learning; Brain; Image Processing, Computer-Assisted; Neuroimaging",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38785733/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Radiology/Medical Imaging; Neurology; Genomics/Genetics",
      "bias_axes": "Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Representation Learning; Diverse/Representative Data",
      "clinical_setting": "Not specified",
      "key_findings": "The results of t-distributed stochastic neighbor embedding (t-SNE) and scanner classification accuracy of our proposed feature extraction-DcCNN (FE-DcCNN) model validated the effective removal of scanner bias. Our method achieves an average accuracy of 77.80% on a multiscanner dataset for differentiating PD from a healthy control, which is superior to the DcCNN model trained on a single-scanner imbalanced dataset.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11117585"
    },
    {
      "pmid": "38789452",
      "title": "BOLD: Blood-gas and Oximetry Linked Dataset.",
      "abstract": "Pulse oximeters measure peripheral arterial oxygen saturation (SpO2) noninvasively, while the gold standard (SaO2) involves arterial blood gas measurement. There are known racial and ethnic disparities in their performance. BOLD is a dataset that aims to underscore the importance of addressing biases in pulse oximetry accuracy, which disproportionately affect darker-skinned patients. The dataset was created by harmonizing three Electronic Health Record databases (MIMIC-III, MIMIC-IV, eICU-CRD) comprising Intensive Care Unit stays of US patients. Paired SpO2 and SaO2 measurements were time-aligned and combined with various other sociodemographic and parameters to provide a detailed representation of each patient. BOLD includes 49,099 paired measurements, within a 5-minute window and with oxygen saturation levels between 70-100%. Minority racial and ethnic groups account for ~25% of the data - a proportion seldom achieved in previous studies. The codebase is publicly available. Given the prevalent use of pulse oximeters in the hospital and at home, we hope that BOLD will be leveraged to develop debiasing algorithms that can result in more equitable healthcare solutions.",
      "journal": "Scientific data",
      "year": "2024",
      "doi": "10.1038/s41597-024-03225-z",
      "authors": "Matos Jo\u00e3o et al.",
      "keywords": "",
      "mesh_terms": "Humans; Oximetry; Blood Gas Analysis; Oxygen Saturation; Intensive Care Units; Ethnicity; Oxygen",
      "pub_types": "Dataset; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38789452/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics; Wearables/Remote Monitoring",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; ICU",
      "key_findings": "The codebase is publicly available. Given the prevalent use of pulse oximeters in the hospital and at home, we hope that BOLD will be leveraged to develop debiasing algorithms that can result in more equitable healthcare solutions.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 3 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11126612"
    },
    {
      "pmid": "38820189",
      "title": "Equity and AI governance at academic medical centers.",
      "abstract": "OBJECTIVES: To understand whether and how equity is considered in artificial intelligence/machine learning governance processes at academic medical centers. STUDY DESIGN: Qualitative analysis of interview data. METHODS: We created a database of academic medical centers from the full list of Association of American Medical Colleges hospital and health system members in 2022. Stratifying by census region and restricting to nonfederal and nonspecialty centers, we recruited chief medical informatics officers and similarly positioned individuals from academic medical centers across the country. We created and piloted a semistructured interview guide focused on (1) how academic medical centers govern artificial intelligence and prediction and (2) to what extent equity is considered in these processes. A total of 17 individuals representing 13 institutions across 4 census regions of the US were interviewed. RESULTS: A minority of participants reported considering inequity, racism, or bias in governance. Most participants conceptualized these issues as characteristics of a tool, using frameworks such as algorithmic bias or fairness. Fewer participants conceptualized equity beyond the technology itself and asked broader questions about its implications for patients. Disparities in health information technology resources across health systems were repeatedly identified as a threat to health equity. CONCLUSIONS: We found a lack of consistent equity consideration among academic medical centers as they develop their governance processes for predictive technologies despite considerable national attention to the ways these technologies can cause or reproduce inequities. Health systems and policy makers will need to specifically prioritize equity literacy among health system leadership, design oversight policies, and promote critical engagement with these tools and their implications to prevent the further entrenchment of inequities in digital health care.",
      "journal": "The American journal of managed care",
      "year": "2024",
      "doi": "10.37765/ajmc.2024.89555",
      "authors": "Nong Paige et al.",
      "keywords": "",
      "mesh_terms": "Academic Medical Centers; Humans; United States; Artificial Intelligence; Qualitative Research; Health Equity; Interviews as Topic; Racism",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38820189/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "CONCLUSIONS: We found a lack of consistent equity consideration among academic medical centers as they develop their governance processes for predictive technologies despite considerable national attention to the ways these technologies can cause or reproduce inequities. Health systems and policy makers will need to specifically prioritize equity literacy among health system leadership, design oversight policies, and promote critical engagement with these tools and their implications to prevent ...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38827072",
      "title": "PFERM: A Fair Empirical Risk Minimization Approach with Prior Knowledge.",
      "abstract": "Fairness is crucial in machine learning to prevent bias based on sensitive attributes in classifier predictions. However, the pursuit of strict fairness often sacrifices accuracy, particularly when significant prevalence disparities exist among groups, making classifiers less practical. For example, Alzheimer's disease (AD) is more prevalent in women than men, making equal treatment inequitable for females. Accounting for prevalence ratios among groups is essential for fair decision-making. In this paper, we introduce prior knowledge for fairness, which incorporates prevalence ratio information into the fairness constraint within the Empirical Risk Minimization (ERM) framework. We develop the Prior-knowledge-guided Fair ERM (PFERM) framework, aiming to minimize expected risk within a specified function class while adhering to a prior-knowledge-guided fairness constraint. This approach strikes a flexible balance between accuracy and fairness. Empirical results confirm its effectiveness in preserving fairness without compromising accuracy.",
      "journal": "AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science",
      "year": "2024",
      "doi": "",
      "authors": "Hou Bojian et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38827072/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Neurology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Fairness Constraints",
      "clinical_setting": "ICU",
      "key_findings": "This approach strikes a flexible balance between accuracy and fairness. Empirical results confirm its effectiveness in preserving fairness without compromising accuracy.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11141835"
    },
    {
      "pmid": "38849530",
      "title": "Towards equitable AI in oncology.",
      "abstract": "Artificial intelligence (AI) stands at the threshold of revolutionizing clinical oncology, with considerable potential to improve early cancer detection and risk assessment, and to enable more accurate personalized treatment recommendations. However, a notable imbalance exists in the distribution of the benefits of AI, which disproportionately favour those living in specific geographical locations and in specific populations. In this Perspective, we discuss the need to foster the development of equitable AI tools that are both accurate in and accessible to a diverse range of patient populations, including those in low-income to middle-income countries. We also discuss some of the challenges and potential solutions in attaining equitable AI, including addressing the historically limited representation of diverse populations in existing clinical datasets and the use of inadequate clinical validation methods. Additionally, we focus on extant sources of inequity including the type of model approach (such as deep learning, and feature engineering-based methods), the implications of dataset curation strategies, the need for rigorous validation across a variety of populations and settings, and the risk of introducing contextual bias that comes with developing tools predominantly in high-income countries.",
      "journal": "Nature reviews. Clinical oncology",
      "year": "2024",
      "doi": "10.1038/s41571-024-00909-8",
      "authors": "Viswanathan Vidya Sankar et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Medical Oncology; Neoplasms; Developing Countries",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38849530/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex; Socioeconomic Status; Geographic",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We also discuss some of the challenges and potential solutions in attaining equitable AI, including addressing the historically limited representation of diverse populations in existing clinical datasets and the use of inadequate clinical validation methods. Additionally, we focus on extant sources of inequity including the type of model approach (such as deep learning, and feature engineering-based methods), the implications of dataset curation strategies, the need for rigorous validation acros...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "38851444",
      "title": "Enhancing Clinical Decision Support in Nephrology: Addressing Algorithmic Bias Through Artificial Intelligence Governance.",
      "abstract": "There has been a steady rise in the use of clinical decision support (CDS) tools to guide nephrology as well as general clinical care. Through guidance set by federal agencies and concerns raised by clinical investigators, there has been an equal rise in understanding whether such tools exhibit algorithmic bias leading to unfairness. This has spurred the more fundamental question of whether sensitive variables such as race should be included in CDS tools. In order to properly answer this question, it is necessary to understand how algorithmic bias arises. We break down 3 sources of bias encountered when using electronic health record data to develop CDS tools: (1) use of proxy variables, (2) observability concerns and (3) underlying heterogeneity. We discuss how answering the question of whether to include sensitive variables like race often hinges more on qualitative considerations than on quantitative analysis, dependent on the function that the sensitive variable serves. Based on our experience with our own institution's CDS governance group, we show how health system-based governance committees play a central role in guiding these difficult and important considerations. Ultimately, our goal is to foster a community practice of model development and governance teams that emphasizes consciousness about sensitive variables and prioritizes equity.",
      "journal": "American journal of kidney diseases : the official journal of the National Kidney Foundation",
      "year": "2024",
      "doi": "10.1053/j.ajkd.2024.04.008",
      "authors": "Goldstein Benjamin A et al.",
      "keywords": "Algorithmic bias; artificial intelligence; clinical decision support; governance; machine learning",
      "mesh_terms": "Humans; Decision Support Systems, Clinical; Nephrology; Artificial Intelligence; Algorithms; Bias; Electronic Health Records",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38851444/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Clinical Decision Support",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics; Nephrology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "Based on our experience with our own institution's CDS governance group, we show how health system-based governance committees play a central role in guiding these difficult and important considerations. Ultimately, our goal is to foster a community practice of model development and governance teams that emphasizes consciousness about sensitive variables and prioritizes equity.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11585446"
    },
    {
      "pmid": "38876558",
      "title": "Assessing and attenuating the impact of selection bias on spatial cluster detection studies.",
      "abstract": "Spatial cluster analyses are commonly used in epidemiologic studies of case-control data to detect whether certain areas in a study region have an excess of disease risk. Case-control studies are susceptible to potential biases including selection bias, which can result from non-participation of eligible subjects in the study. However, there has been no systematic evaluation of the effects of non-participation on the findings of spatial cluster analyses. In this paper, we perform a simulation study assessing the effect of non-participation on spatial cluster analysis using the local spatial scan statistic under a variety of scenarios that vary the location and rates of study non-participation and the presence and intensity of a zone of elevated risk for disease for simulated case-control studies. We find that geographic areas of lower participation among controls than cases can greatly inflate false-positive rates for identification of artificial spatial clusters. Additionally, we find that even modest non-participation outside of a true zone of elevated risk can decrease spatial power to identify the true zone. We propose a spatial algorithm to correct for potentially spatially structured non-participation that compares the spatial distributions of the observed sample and underlying population. We demonstrate its ability to markedly decrease false positive rates in the absence of elevated risk and resist decreasing spatial sensitivity to detect true zones of elevated risk. We apply our method to a case-control study of non-Hodgkin lymphoma. Our findings suggest that greater attention should be paid to the potential effects of non-participation in spatial cluster studies.",
      "journal": "Spatial and spatio-temporal epidemiology",
      "year": "2024",
      "doi": "10.1016/j.sste.2024.100659",
      "authors": "Boyle Joseph et al.",
      "keywords": "Case-control study; Epidemiology; Local spatial scan; Non-participation; Selection bias; Spatial cluster",
      "mesh_terms": "Humans; Cluster Analysis; Case-Control Studies; Spatial Analysis; Selection Bias; Computer Simulation; Algorithms; Lymphoma, Non-Hodgkin",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, N.I.H., Intramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38876558/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Geographic",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We apply our method to a case-control study of non-Hodgkin lymphoma. Our findings suggest that greater attention should be paid to the potential effects of non-participation in spatial cluster studies.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11180222"
    },
    {
      "pmid": "38887420",
      "title": "Assessing Risk of Bias Using ChatGPT-4 and Cochrane ROB2 Tool.",
      "abstract": "In the world of evidence-based medicine, systematic reviews have long been the gold standard. But they have had a problem-they take forever. That is where ChatGPT-4 and automation come in. They are like a breath of fresh air, speeding things up and making the process more reliable. ChatGPT-4 is like having a super-smart assistant who can quickly assess bias risk in research studies. It is a game-changer, especially in a field where getting the latest research quickly can mean life or death for patients. Sure, it is not perfect, and we still need humans to keep an eye on things and ensure everything's ethical. But the future looks bright. With ChatGPT-4 and automation, evidence-based medicine is on the fast track to success.",
      "journal": "Medical science educator",
      "year": "2024",
      "doi": "10.1007/s40670-024-02034-8",
      "authors": "Trevi\u00f1o-Juarez Angel Sebastian",
      "keywords": "Artificial intelligence; Bias; Medical education; Systematic review",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38887420/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "But the future looks bright. With ChatGPT-4 and automation, evidence-based medicine is on the fast track to success.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11180068"
    },
    {
      "pmid": "38931955",
      "title": "Single-Cell Transcriptomic and Targeted Genomic Profiling Adjusted for Inflammation and Therapy Bias Reveal CRTAM and PLCB1 as Novel Hub Genes for Anti-Tumor Necrosis Factor Alpha Therapy Response in Crohn's Disease.",
      "abstract": "The lack of reliable biomarkers in response to anti-TNF\u03b1 biologicals hinders personalized therapy for Crohn's disease (CD) patients. The motivation behind our study is to shift the paradigm of anti-TNF\u03b1 biomarker discovery toward specific immune cell sub-populations using single-cell RNA sequencing and an innovative approach designed to uncover PBMCs gene expression signals, which may be masked due to the treatment or ongoing inflammation; Methods: The single-cell RNA sequencing was performed on PBMC samples from CD patients either na\u00efve to biological therapy, in remission while on adalimumab, or while on ustekinumab but previously non-responsive to adalimumab. Sieves for stringent downstream gene selection consisted of gene ontology and independent cohort genomic profiling. Replication and meta-analyses were performed using publicly available raw RNA sequencing files of sorted immune cells and an association analysis summary. Machine learning, Mendelian randomization, and oligogenic risk score methods were deployed to validate DEGs highly relevant to anti-TNF\u03b1 therapy response; Results: This study found PLCB1 in CD4+ T cells and CRTAM in double-negative T cells, which met the stringent statistical thresholds throughout the analyses. An additional assessment proved causal inference of both genes in response to anti-TNF\u03b1 therapy; Conclusions: This study, jointly with an innovative design, uncovered novel candidate genes in the anti-TNF\u03b1 response landscape of CD, potentially obscured by therapy or inflammation.",
      "journal": "Pharmaceutics",
      "year": "2024",
      "doi": "10.3390/pharmaceutics16060835",
      "authors": "Gorenjak Mario et al.",
      "keywords": "Crohn\u2019s disease; adalimumab; inflammatory bowel diseases; single-cell gene expression analysis; tumor necrosis factor alpha",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38931955/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Oncology; Genomics/Genetics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Threshold Adjustment; Counterfactual Fairness",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Conclusions: This study, jointly with an innovative design, uncovered novel candidate genes in the anti-TNF\u03b1 response landscape of CD, potentially obscured by therapy or inflammation.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11207411"
    },
    {
      "pmid": "38960729",
      "title": "Fair prediction of 2-year stroke risk in patients with atrial fibrillation.",
      "abstract": "OBJECTIVE: This study aims to develop machine learning models that provide both accurate and equitable predictions of 2-year stroke risk for patients with atrial fibrillation across diverse racial groups. MATERIALS AND METHODS: Our study utilized structured electronic health records (EHR) data from the All of Us Research Program. Machine learning models (LightGBM) were utilized to capture the relations between stroke risks and the predictors used by the widely recognized CHADS2 and CHA2DS2-VASc scores. We mitigated the racial disparity by creating a representative tuning set, customizing tuning criteria, and setting binary thresholds separately for subgroups. We constructed a hold-out test set that not only supports temporal validation but also includes a larger proportion of Black/African Americans for fairness validation. RESULTS: Compared to the original CHADS2 and CHA2DS2-VASc scores, significant improvements were achieved by modeling their predictors using machine learning models (Area Under the Receiver Operating Characteristic curve from near 0.70 to above 0.80). Furthermore, applying our disparity mitigation strategies can effectively enhance model fairness compared to the conventional cross-validation approach. DISCUSSION: Modeling CHADS2 and CHA2DS2-VASc risk factors with LightGBM and our disparity mitigation strategies achieved decent discriminative performance and excellent fairness performance. In addition, this approach can provide a complete interpretation of each predictor. These highlight its potential utility in clinical practice. CONCLUSIONS: Our research presents a practical example of addressing clinical challenges through the All of Us Research Program data. The disparity mitigation framework we proposed is adaptable across various models and data modalities, demonstrating broad potential in clinical informatics.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2024",
      "doi": "10.1093/jamia/ocae170",
      "authors": "Gao Jifan et al.",
      "keywords": "atrial fibrillation; bias; fairness; machine learning; stroke",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Atrial Fibrillation; Black or African American; Electronic Health Records; Machine Learning; Risk Assessment; Risk Factors; ROC Curve; Stroke; Racial Groups",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38960729/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "XGBoost/Gradient Boosting",
      "health_domain": "Cardiology; EHR/Health Informatics; Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Our research presents a practical example of addressing clinical challenges through the All of Us Research Program data. The disparity mitigation framework we proposed is adaptable across various models and data modalities, demonstrating broad potential in clinical informatics.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11631105"
    },
    {
      "pmid": "38981215",
      "title": "Promoting fairness in activity recognition algorithms for patient's monitoring and evaluation systems in healthcare.",
      "abstract": "Researchers face the challenge of defining subject selection criteria when training algorithms for human activity recognition tasks. The ongoing uncertainty revolves around which characteristics should be considered to ensure algorithmic robustness across diverse populations. This study aims to address this challenge by conducting an analysis of heterogeneity in the training data to assess the impact of physical characteristics and soft-biometric attributes on activity recognition performance. The performance of various state-of-the-art deep neural network architectures (tCNN, hybrid-LSTM, Transformer model) processing time-series data using the IntelliRehab (IRDS) dataset was evaluated. By intentionally introducing bias into the training data based on human characteristics, the objective is to identify the characteristics that influence algorithms in motion analysis. Experimental findings reveal that the CNN-LSTM model achieved the highest accuracy, reaching 88%. Moreover, models trained on heterogeneous distributions of disability attributes exhibited notably higher accuracy, reaching 51%, compared to those not considering such factors, which scored an average of 33%. These evaluations underscore the significant influence of subjects' characteristics on activity recognition performance, providing valuable insights into the algorithm's robustness across diverse populations. This study represents a significant step forward in promoting fairness and trustworthiness in artificial intelligence by quantifying representation bias in multi-channel time-series activity recognition data within the healthcare domain.",
      "journal": "Computers in biology and medicine",
      "year": "2024",
      "doi": "10.1016/j.compbiomed.2024.108826",
      "authors": "Mennella Ciro et al.",
      "keywords": "Artificial intelligence; Bias; Deep learning; Motion analysis; Rehabilitation; Time-series",
      "mesh_terms": "Humans; Algorithms; Male; Female; Neural Networks, Computer; Adult; Middle Aged; Human Activities; Aged",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38981215/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; NLP/LLM; Neural Network",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Disability",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "These evaluations underscore the significant influence of subjects' characteristics on activity recognition performance, providing valuable insights into the algorithm's robustness across diverse populations. This study represents a significant step forward in promoting fairness and trustworthiness in artificial intelligence by quantifying representation bias in multi-channel time-series activity recognition data within the healthcare domain.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39019301",
      "title": "Recommendations to promote fairness and inclusion in biomedical AI research and clinical use.",
      "abstract": "OBJECTIVE: Understanding and quantifying biases when designing and implementing actionable approaches to increase fairness and inclusion is critical for artificial intelligence (AI) in biomedical applications. METHODS: In this Special Communication, we discuss how bias is introduced at different stages of the development and use of AI applications in biomedical sciences and health care. We describe various AI applications and their implications for fairness and inclusion in sections on 1) Bias in Data Source Landscapes, 2) Algorithmic Fairness, 3) Uncertainty in AI Predictions, 4) Explainable AI for Fairness and Equity, and 5) Sociological/Ethnographic Issues in Data and Results Representation. RESULTS: We provide recommendations to address biases when developing and using AI in clinical applications. CONCLUSION: These recommendations can be applied to informatics research and practice to foster more equitable and inclusive health care systems and research discoveries.",
      "journal": "Journal of biomedical informatics",
      "year": "2024",
      "doi": "10.1016/j.jbi.2024.104693",
      "authors": "Griffin Ashley C et al.",
      "keywords": "Artificial Intelligence; Bias; Digital Divide; Equity; Fairness; Inclusion; Uncertainty Quantification",
      "mesh_terms": "Artificial Intelligence; Humans; Biomedical Research; Algorithms; Bias; Medical Informatics; Delivery of Health Care",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39019301/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: These recommendations can be applied to informatics research and practice to foster more equitable and inclusive health care systems and research discoveries.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11402591"
    },
    {
      "pmid": "39121467",
      "title": "Twenty-Five Years of Progress-Lessons Learned From JMIR Publications to Address Gender Parity in Digital Health Authorships: Bibliometric Analysis.",
      "abstract": "BACKGROUND: Digital health research plays a vital role in advancing equitable health care. The diversity of research teams is thereby instrumental in capturing societal challenges, increasing productivity, and reducing bias in algorithms. Despite its importance, the gender distribution within digital health authorship remains largely unexplored. OBJECTIVE: This study aimed to investigate the gender distribution among first and last authors in digital health research, thereby identifying predicting factors of female authorship. METHODS: This bibliometric analysis examined the gender distribution across 59,980 publications from 1999 to 2023, spanning 42 digital health journals indexed in the Web of Science. To identify strategies ensuring equality in research, a detailed comparison of gender representation in JMIR journals was conducted within the field, as well as against a matched sample. Two-tailed Welch 2-sample t tests, Wilcoxon rank sum tests, and chi-square tests were used to assess differences. In addition, odds ratios were calculated to identify predictors of female authorship. RESULTS: The analysis revealed that 37% of first authors and 30% of last authors in digital health were female. JMIR journals demonstrated a higher representation, with 49% of first authors and 38% of last authors being female, yielding odds ratios of 1.96 (95% CI 1.90-2.03; P<.001) and 1.78 (95% CI 1.71-1.84; P<.001), respectively. Since 2008, JMIR journals have consistently featured a greater proportion of female first authors than male counterparts. Other factors that predicted female authorship included having female authors in other relevant positions and gender discordance, given the higher rate of male last authors in the field. CONCLUSIONS: There was an evident shift toward gender parity across publications in digital health, particularly from the publisher JMIR Publications. The specialized focus of its sister journals, equitable editorial policies, and transparency in the review process might contribute to these achievements. Further research is imperative to establish causality, enabling the replication of these successful strategies across other scientific fields to bridge the gender gap in digital health effectively.",
      "journal": "Journal of medical Internet research",
      "year": "2024",
      "doi": "10.2196/58950",
      "authors": "Meyer Annika et al.",
      "keywords": "JMIR Publications; Web of Science; algorithmic bias reduction; article; articles; author; authors; authorships; bibliometric; bibliometric analysis; comparative analysis; comparison; control group; digital health; diversity; equality; gender; gender distribution; gender gap; gender representation",
      "mesh_terms": "Authorship; Bibliometrics; Humans; Female; Male; Periodicals as Topic; Sex Factors; Digital Health",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39121467/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSIONS: There was an evident shift toward gender parity across publications in digital health, particularly from the publisher JMIR Publications. The specialized focus of its sister journals, equitable editorial policies, and transparency in the review process might contribute to these achievements. Further research is imperative to establish causality, enabling the replication of these successful strategies across other scientific fields to bridge the gender gap in digital health effective...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11344179"
    },
    {
      "pmid": "39126673",
      "title": "Predictive roles of cognitive biases in health anxiety: A machine learning approach.",
      "abstract": "Prior work suggests that cognitive biases may contribute to health anxiety. Yet there is little research investigating how biased attention, interpretation, and memory for health threats are collectively associated with health anxiety, as well as the relative importance of these cognitive processes in predicting health anxiety. This study aimed to build a prediction model for health anxiety with multiple cognitive biases as potential predictors and to identify the biased cognitive processes that best predict individual differences in health anxiety. A machine learning algorithm (elastic net) was performed to recognise the predictors of health anxiety, using various tasks of attention, interpretation, and memory measured across behavioural, self-reported, and computational modelling approaches. Participants were 196 university students with a range of health anxiety severity from mild to severe. The results showed that only the interpretation bias for illness and the attention bias towards symptoms significantly contributed to the prediction model of health anxiety, with both biases having positive weights and the former being the most important predictor. These findings underscore the central role of illness-related interpretation bias and suggest that combined cognitive bias modification may be a promising method for alleviating health anxiety.",
      "journal": "Stress and health : journal of the International Society for the Investigation of Stress",
      "year": "2024",
      "doi": "10.1002/smi.3463",
      "authors": "Shi Congrong et al.",
      "keywords": "attention bias; health anxiety; interpretation bias; machine learning; memory bias",
      "mesh_terms": "Humans; Machine Learning; Male; Female; Young Adult; Adult; Anxiety; Cognition; Adolescent; Attention",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39126673/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "The results showed that only the interpretation bias for illness and the attention bias towards symptoms significantly contributed to the prediction model of health anxiety, with both biases having positive weights and the former being the most important predictor. These findings underscore the central role of illness-related interpretation bias and suggest that combined cognitive bias modification may be a promising method for alleviating health anxiety.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39166973",
      "title": "Improving Fairness of Automated Chest Radiograph Diagnosis by Contrastive Learning.",
      "abstract": "Purpose To develop an artificial intelligence model that uses supervised contrastive learning (SCL) to minimize bias in chest radiograph diagnosis. Materials and Methods In this retrospective study, the proposed method was evaluated on two datasets: the Medical Imaging and Data Resource Center (MIDRC) dataset with 77\u2009887 chest radiographs in 27\u2009796 patients collected as of April 20, 2023, for COVID-19 diagnosis and the National Institutes of Health ChestX-ray14 dataset with 112\u2009120 chest radiographs in 30\u2009805 patients collected between 1992 and 2015. In the ChestX-ray14 dataset, thoracic abnormalities included atelectasis, cardiomegaly, effusion, infiltration, mass, nodule, pneumonia, pneumothorax, consolidation, edema, emphysema, fibrosis, pleural thickening, and hernia. The proposed method used SCL with carefully selected positive and negative samples to generate fair image embeddings, which were fine-tuned for subsequent tasks to reduce bias in chest radiograph diagnosis. The method was evaluated using the marginal area under the receiver operating characteristic curve difference (\u2206mAUC). Results The proposed model showed a significant decrease in bias across all subgroups compared with the baseline models, as evidenced by a paired t test (P < .001). The \u2206mAUCs obtained by the proposed method were 0.01 (95% CI: 0.01, 0.01), 0.21 (95% CI: 0.21, 0.21), and 0.10 (95% CI: 0.10, 0.10) for sex, race, and age subgroups, respectively, on the MIDRC dataset and 0.01 (95% CI: 0.01, 0.01) and 0.05 (95% CI: 0.05, 0.05) for sex and age subgroups, respectively, on the ChestX-ray14 dataset. Conclusion Employing SCL can mitigate bias in chest radiograph diagnosis, addressing concerns of fairness and reliability in deep learning-based diagnostic methods. Keywords: Thorax, Diagnosis, Supervised Learning, Convolutional Neural Network (CNN), Computer-aided Diagnosis (CAD) Supplemental material is available for this article. \u00a9 RSNA, 2024 See also the commentary by Johnson in this issue.",
      "journal": "Radiology. Artificial intelligence",
      "year": "2024",
      "doi": "10.1148/ryai.230342",
      "authors": "Lin Mingquan et al.",
      "keywords": "Computer-aided Diagnosis (CAD); Convolutional Neural Network (CNN); Diagnosis; Supervised Learning; Thorax",
      "mesh_terms": "Humans; Radiography, Thoracic; Retrospective Studies; Female; Male; Middle Aged; Aged; COVID-19; Adult; Artificial Intelligence; SARS-CoV-2; Radiographic Image Interpretation, Computer-Assisted; Supervised Machine Learning; Adolescent; Young Adult",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39166973/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Deep Learning; Neural Network; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Representation Learning; Transfer Learning",
      "clinical_setting": "Not specified",
      "key_findings": "Keywords: Thorax, Diagnosis, Supervised Learning, Convolutional Neural Network (CNN), Computer-aided Diagnosis (CAD) Supplemental material is available for this article. \u00a9 RSNA, 2024 See also the commentary by Johnson in this issue.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11449211"
    },
    {
      "pmid": "39174723",
      "title": "Evaluating EHR-Integrated Digital Technologies for Medication-Related Outcomes and Health Equity in Hospitalised Adults: A Scoping Review.",
      "abstract": "The purpose of this scoping review is to identify and evaluate studies that examine the effectiveness and implementation strategies of Electronic Health Record (EHR)-integrated digital technologies aimed at improving medication-related outcomes and promoting health equity among hospitalised adults. Using the Consolidated Framework for Implementation Research (CFIR), the implementation methods and outcomes of the studies were evaluated, as was the assessment of methodological quality and risk of bias. Searches through Medline, Embase, Web of Science, and CINAHL Plus yielded 23 relevant studies from 1,232 abstracts, spanning 11 countries and from 2008 to 2022, with varied research designs. Integrated digital tools such as alert systems, clinical decision support systems, predictive analytics, risk assessment, and real-time screening and surveillance within EHRs demonstrated potential in reducing medication errors, adverse events, and inappropriate medication use, particularly in older patients. Challenges include alert fatigue, clinician acceptance, workflow integration, cost, data integrity, interoperability, and the potential for algorithmic bias, with a call for long-term and ongoing monitoring of patient safety and health equity outcomes. This review, guided by the CFIR framework, highlights the importance of designing health technology based on evidence and user-centred practices. Quality assessments identified eligibility and representativeness issues that affected the reliability and generalisability of the findings. This review also highlights a critical research gap on whether EHR-integrated digital tools can address or worsen health inequities among hospitalised patients. Recognising the growing role of Artificial Intelligence (AI) and Machine Learning (ML), this review calls for further research on its influence on medication management and health equity through integration of EHR and digital technology.",
      "journal": "Journal of medical systems",
      "year": "2024",
      "doi": "10.1007/s10916-024-02097-5",
      "authors": "Murthi Sreyon et al.",
      "keywords": "Digital technology; Electronic health records; Health equity; Hospital; Medication management; Patient safety",
      "mesh_terms": "Humans; Electronic Health Records; Health Equity; Digital Technology; Medication Errors; Decision Support Systems, Clinical; Hospitalization; Adult",
      "pub_types": "Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39174723/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Clinical Decision Support",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics; Public Health",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; ICU; Public Health/Population",
      "key_findings": "This review also highlights a critical research gap on whether EHR-integrated digital tools can address or worsen health inequities among hospitalised patients. Recognising the growing role of Artificial Intelligence (AI) and Machine Learning (ML), this review calls for further research on its influence on medication management and health equity through integration of EHR and digital technology.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11341601"
    },
    {
      "pmid": "39176803",
      "title": "Exploring the Intersection of AI and Inclusive Design for People with Disabilities.",
      "abstract": "This scoping review examines current research on AI for inclusive design for people with disabilities. We identified both advantages and challenges of AI-based solutions and suggested future research directions. Our search of four online databases for studies from the last five years revealed promising AI applications in education, daily living, home environments, workplaces, and healthcare. However, limitations include limited research, lack of user involvement, potential data bias, and reporting deficiencies. We stress the importance of future research prioritizing user-centered design, inclusive participation, AI bias mitigation, consideration of diverse populations, and ensuring user-friendly performance to fully realize AI's potential for accessibility and inclusion.",
      "journal": "Studies in health technology and informatics",
      "year": "2024",
      "doi": "10.3233/SHTI240475",
      "authors": "El Morr Christo et al.",
      "keywords": "Artificial Intelligence; Disabled Persons; Machine Learning; Social Discrimination; Social Inclusion; Universal Design; User-Centered Design",
      "mesh_terms": "Humans; Artificial Intelligence; Persons with Disabilities; User-Centered Design",
      "pub_types": "Review; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39176803/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "However, limitations include limited research, lack of user involvement, potential data bias, and reporting deficiencies. We stress the importance of future research prioritizing user-centered design, inclusive participation, AI bias mitigation, consideration of diverse populations, and ensuring user-friendly performance to fully realize AI's potential for accessibility and inclusion.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39176898",
      "title": "How Data Infrastructure Deals with Bias Problems in Medical Imaging.",
      "abstract": "The paper discusses biases in medical imaging analysis, particularly focusing on the challenges posed by the development of machine learning algorithms and generative models. It introduces a taxonomy of bias problems and addresses them through a data infrastructure initiative: the PADME (Platform for Analytics and Distributed Machine-Learning for Enterprises), which is a part of the National Research Data Infrastructure for Personal Health Data (NFDI4Health) project. The PADME facilitates the structuring and sharing of health data while ensuring privacy and adherence to FAIR principles. The paper presents experimental results that show that generative methods can be effective in data augmentation. Complying with PADME infrastructure, this work proposes a solution framework to deal with bias in the different data stations and preserve privacy when transferring images. It highlights the importance of standardized data infrastructure in mitigating biases and promoting FAIR, reusable, and privacy-preserving research environments in healthcare.",
      "journal": "Studies in health technology and informatics",
      "year": "2024",
      "doi": "10.3233/SHTI240517",
      "authors": "Li Feifei et al.",
      "keywords": "Bias; Data Infrastructure; Differential Privacy; Federated Learning; Machine Learning; Medical Imaging",
      "mesh_terms": "Diagnostic Imaging; Humans; Machine Learning; Bias; Algorithms; Confidentiality; Computer Security",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39176898/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Computer Vision/Imaging AI; Generative AI",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Data Augmentation",
      "clinical_setting": "ICU",
      "key_findings": "Complying with PADME infrastructure, this work proposes a solution framework to deal with bias in the different data stations and preserve privacy when transferring images. It highlights the importance of standardized data infrastructure in mitigating biases and promoting FAIR, reusable, and privacy-preserving research environments in healthcare.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39184970",
      "title": "Computational Simulation of Virtual Patients Reduces Dataset Bias and Improves Machine Learning-Based Detection of ARDS from Noisy Heterogeneous ICU Datasets.",
      "abstract": "Goal: Machine learning (ML) technologies that leverage large-scale patient data are promising tools predicting disease evolution in individual patients. However, the limited generalizability of ML models developed on single-center datasets, and their unproven performance in real-world settings, remain significant constraints to their widespread adoption in clinical practice. One approach to tackle this issue is to base learning on large multi-center datasets. However, such heterogeneous datasets can introduce further biases driven by data origin, as data structures and patient cohorts may differ between hospitals. Methods: In this paper, we demonstrate how mechanistic virtual patient (VP) modeling can be used to capture specific features of patients' states and dynamics, while reducing biases introduced by heterogeneous datasets. We show how VP modeling can be used for data augmentation through identification of individualized model parameters approximating disease states of patients with suspected acute respiratory distress syndrome (ARDS) from observational data of mixed origin. We compare the results of an unsupervised learning method (clustering) in two cases: where the learning is based on original patient data and on data derived in the matching procedure of the VP model to real patient data. Results: More robust cluster configurations were observed in clustering using the model-derived data. VP model-based clustering also reduced biases introduced by the inclusion of data from different hospitals and was able to discover an additional cluster with significant ARDS enrichment. Conclusions: Our results indicate that mechanistic VP modeling can be used to significantly reduce biases introduced by learning from heterogeneous datasets and to allow improved discovery of patient cohorts driven exclusively by medical conditions.",
      "journal": "IEEE open journal of engineering in medicine and biology",
      "year": "2024",
      "doi": "10.1109/OJEMB.2023.3243190",
      "authors": "Sharafutdinov Konstantin et al.",
      "keywords": "ARDS; computational simulation; dataset bias; machine learning; virtual patients",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39184970/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clustering",
      "health_domain": "ICU/Critical Care; Pulmonology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Data Augmentation",
      "clinical_setting": "Hospital/Inpatient; ICU",
      "key_findings": "Conclusions: Our results indicate that mechanistic VP modeling can be used to significantly reduce biases introduced by learning from heterogeneous datasets and to allow improved discovery of patient cohorts driven exclusively by medical conditions.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11342939"
    },
    {
      "pmid": "39199522",
      "title": "Leveraging Artificial Intelligence to Optimize Transcranial Direct Current Stimulation for Long COVID Management: A Forward-Looking Perspective.",
      "abstract": "Long COVID (Coronavirus disease), affecting millions globally, presents unprecedented challenges to healthcare systems due to its complex, multifaceted nature and the lack of effective treatments. This perspective review explores the potential of artificial intelligence (AI)-guided transcranial direct current stimulation (tDCS) as an innovative approach to address the urgent need for effective Long COVID management. The authors examine how AI could optimize tDCS protocols, enhance clinical trial design, and facilitate personalized treatment for the heterogeneous manifestations of Long COVID. Key areas discussed include AI-driven personalization of tDCS parameters based on individual patient characteristics and real-time symptom fluctuations, the use of machine learning for patient stratification, and the development of more sensitive outcome measures in clinical trials. This perspective addresses ethical considerations surrounding data privacy, algorithmic bias, and equitable access to AI-enhanced treatments. It also explores challenges and opportunities for implementing AI-guided tDCS across diverse healthcare settings globally. Future research directions are outlined, including the need for large-scale validation studies and investigations of long-term efficacy and safety. The authors argue that while AI-guided tDCS shows promise for addressing the complex nature of Long COVID, significant technical, ethical, and practical challenges remain. They emphasize the importance of interdisciplinary collaboration, patient-centered approaches, and a commitment to global health equity in realizing the potential of this technology. This perspective article provides a roadmap for researchers, clinicians, and policymakers involved in developing and implementing AI-guided neuromodulation therapies for Long COVID and potentially other neurological and psychiatric conditions.",
      "journal": "Brain sciences",
      "year": "2024",
      "doi": "10.3390/brainsci14080831",
      "authors": "Rudroff Thorsten et al.",
      "keywords": "artificial intelligence; brain stimulation; long COVID; neuroimaging",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39199522/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Clinical Trial",
      "key_findings": "They emphasize the importance of interdisciplinary collaboration, patient-centered approaches, and a commitment to global health equity in realizing the potential of this technology. This perspective article provides a roadmap for researchers, clinicians, and policymakers involved in developing and implementing AI-guided neuromodulation therapies for Long COVID and potentially other neurological and psychiatric conditions.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11353063"
    },
    {
      "pmid": "39207356",
      "title": "Understanding AI bias in clinical practice.",
      "abstract": "",
      "journal": "Heart rhythm",
      "year": "2024",
      "doi": "10.1016/j.hrthm.2024.08.004",
      "authors": "Adedinsewo Demilade et al.",
      "keywords": "Artificial intelligence; Bias; Cardiac electrophysiology; Cardiovascular disease; Clinical algorithms",
      "mesh_terms": "Humans; Artificial Intelligence; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39207356/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": "PMC12268356"
    },
    {
      "pmid": "39214762",
      "title": "Mitigating the risk of artificial intelligence bias in cardiovascular care.",
      "abstract": "Digital health technologies can generate data that can be used to train artificial intelligence (AI) algorithms, which have been particularly transformative in cardiovascular health-care delivery. However, digital and health-care data repositories that are used to train AI algorithms can introduce bias when data are homogeneous and health-care processes are inequitable. AI bias can also be introduced during algorithm development, testing, implementation, and post-implementation processes. The consequences of AI algorithmic bias can be considerable, including missed diagnoses, misclassification of disease, incorrect risk prediction, and inappropriate treatment recommendations. This bias can disproportionately affect marginalised demographic groups. In this Series paper, we provide a brief overview of AI applications in cardiovascular health care, discuss stages of algorithm development and associated sources of bias, and provide examples of harm from biased algorithms. We propose strategies that can be applied during the training, testing, and implementation of AI algorithms to mitigate bias so that all those at risk for or living with cardiovascular disease might benefit equally from AI.",
      "journal": "The Lancet. Digital health",
      "year": "2024",
      "doi": "10.1016/S2589-7500(24)00155-9",
      "authors": "Mihan Ariana et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Cardiovascular Diseases; Bias; Algorithms; Delivery of Health Care",
      "pub_types": "Journal Article; Review; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39214762/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Cardiology; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "In this Series paper, we provide a brief overview of AI applications in cardiovascular health care, discuss stages of algorithm development and associated sources of bias, and provide examples of harm from biased algorithms. We propose strategies that can be applied during the training, testing, and implementation of AI algorithms to mitigate bias so that all those at risk for or living with cardiovascular disease might benefit equally from AI.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39230911",
      "title": "Addressing AI Algorithmic Bias in Health Care.",
      "abstract": "This Viewpoint discusses the bias that exists in artificial intelligence (AI) algorithms used in health care despite recent federal rules to prohibit discriminatory outcomes from AI and recommends ways in which health care facilities, AI developers, and regulators could share responsibilities and actions to address bias.",
      "journal": "JAMA",
      "year": "2024",
      "doi": "10.1001/jama.2024.13486",
      "authors": "Ratwani Raj M et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Bias; Digital Health; Decision Support Systems, Clinical; United States Dept. of Health and Human Services; Software Design; Certification",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39230911/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "This Viewpoint discusses the bias that exists in artificial intelligence (AI) algorithms used in health care despite recent federal rules to prohibit discriminatory outcomes from AI and recommends ways in which health care facilities, AI developers, and regulators could share responsibilities and actions to address bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39232303",
      "title": "Bias Perpetuates Bias: ChatGPT Learns Gender Inequities in Academic Surgery Promotions.",
      "abstract": "OBJECTIVE: Gender inequities persist in academic surgery with implicit bias impacting hiring and promotion at all levels. We hypothesized that creating letters of recommendation for both female and male candidates for academic promotion in surgery using an AI platform, ChatGPT, would elucidate the entrained gender biases already present in the promotion process. DESIGN: Using ChatGPT, we generated 6 letters of recommendation for \"a phenomenal surgeon applying for job promotion to associate professor position\", specifying \"female\" or \"male\" before surgeon in the prompt. We compared 3 \"female\" letters to 3 \"male\" letters for differences in length, language, and tone. RESULTS: The letters written for females averaged 298 words compared to 314 for males. Female letters more frequently referred to \"compassion\", \"empathy\", and \"inclusivity\"; whereas male letters referred to \"respect\", \"reputation\", and \"skill\". CONCLUSIONS: These findings highlight the gender bias present in promotion letters generated by ChatGPT, reiterating existing literature regarding real letters of recommendation in academic surgery. Our study suggests that surgeons should use AI tools, such as ChatGPT, with caution when writing LORs for academic surgery faculty promotion.",
      "journal": "Journal of surgical education",
      "year": "2024",
      "doi": "10.1016/j.jsurg.2024.07.023",
      "authors": "Desai Pooja et al.",
      "keywords": "ChatGPT; Gender disparities in medicine; academic promotions; artificial intelligence; implicit bias; letters of recommendation",
      "mesh_terms": "Female; Humans; Male; Sexism; Faculty, Medical; General Surgery; Career Mobility; Personnel Selection; Correspondence as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39232303/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Surgery",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: These findings highlight the gender bias present in promotion letters generated by ChatGPT, reiterating existing literature regarding real letters of recommendation in academic surgery. Our study suggests that surgeons should use AI tools, such as ChatGPT, with caution when writing LORs for academic surgery faculty promotion.",
      "ft_include": false,
      "ft_reason": "Not health-related in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39242068",
      "title": "A deep learning-based approach for unbiased kinematic analysis in CNS injury.",
      "abstract": "Traumatic spinal cord injury (SCI) is a devastating condition that impacts over 300,000 individuals in the US alone. Depending on the severity of the injury, SCI can lead to varying degrees of sensorimotor deficits and paralysis. Despite advances in our understanding of the underlying pathological mechanisms of SCI and the identification of promising molecular targets for repair and functional restoration, few therapies have made it into clinical use. To improve the success rate of clinical translation, more robust, sensitive, and reproducible means of functional assessment are required. The gold standards for the evaluation of locomotion in rodents with SCI are the Basso Beattie Bresnahan (BBB) scale and Basso Mouse Scale (BMS). To overcome the shortcomings of current methods, we developed two separate markerless kinematic analysis paradigms in mice, MotorBox and MotoRater, based on deep-learning algorithms generated with the DeepLabCut open-source toolbox. The MotorBox system uses an originally designed, custom-made chamber, and the MotoRater system was implemented on a commercially available MotoRater device. We validated the MotorBox and MotoRater systems by comparing them with the traditional BMS test and extracted metrics of movement and gait that can provide an accurate and sensitive representation of mouse locomotor function post-injury, while eliminating investigator bias and variability. The integration of MotorBox and/or MotoRater assessments with BMS scoring will provide a much wider range of information on specific aspects of locomotion, ensuring the accuracy, rigor, and reproducibility of behavioral outcomes after SCI.",
      "journal": "Experimental neurology",
      "year": "2024",
      "doi": "10.1016/j.expneurol.2024.114944",
      "authors": "Ascona Maureen C et al.",
      "keywords": "DeepLabCut; Kinematics; Machine learning; Spinal cord injury; Traumatic brain injury",
      "mesh_terms": "Animals; Deep Learning; Mice; Biomechanical Phenomena; Spinal Cord Injuries; Mice, Inbred C57BL; Female; Locomotion",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39242068/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Emergency Medicine",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We validated the MotorBox and MotoRater systems by comparing them with the traditional BMS test and extracted metrics of movement and gait that can provide an accurate and sensitive representation of mouse locomotor function post-injury, while eliminating investigator bias and variability. The integration of MotorBox and/or MotoRater assessments with BMS scoring will provide a much wider range of information on specific aspects of locomotion, ensuring the accuracy, rigor, and reproducibility of ...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12576851"
    },
    {
      "pmid": "39312289",
      "title": "Evaluating the Bias in Hospital Data: Automatic Preprocessing of Patient Pathways Algorithm Development and Validation Study.",
      "abstract": "BACKGROUND: The optimization of patient care pathways is crucial for hospital managers in the context of a scarcity of medical resources. Assuming unlimited capacities, the pathway of a patient would only be governed by pure medical logic to meet at best the patient's needs. However, logistical limitations (eg, resources such as inpatient beds) are often associated with delayed treatments and may ultimately affect patient pathways. This is especially true for unscheduled patients-when a patient in the emergency department needs to be admitted to another medical unit without disturbing the flow of planned hospitalizations. OBJECTIVE: In this study, we proposed a new framework to automatically detect activities in patient pathways that may be unrelated to patients' needs but rather induced by logistical limitations. METHODS: The scientific contribution lies in a method that transforms a database of historical pathways with bias into 2 databases: a labeled pathway database where each activity is labeled as relevant (related to a patient's needs) or irrelevant (induced by logistical limitations) and a corrected pathway database where each activity corresponds to the activity that would occur assuming unlimited resources. The labeling algorithm was assessed through medical expertise. In total, 2 case studies quantified the impact of our method of preprocessing health care data using process mining and discrete event simulation. RESULTS: Focusing on unscheduled patient pathways, we collected data covering 12 months of activity at the Groupe Hospitalier Bretagne Sud in France. Our algorithm had 87% accuracy and demonstrated its usefulness for preprocessing traces and obtaining a clean database. The 2 case studies showed the importance of our preprocessing step before any analysis. The process graphs of the processed data had, on average, 40% (SD 10%) fewer variants than the raw data. The simulation revealed that 30% of the medical units had >1 bed difference in capacity between the processed and raw data. CONCLUSIONS: Patient pathway data reflect the actual activity of hospitals that is governed by medical requirements and logistical limitations. Before using these data, these limitations should be identified and corrected. We anticipate that our approach can be generalized to obtain unbiased analyses of patient pathways for other hospitals.",
      "journal": "JMIR medical informatics",
      "year": "2024",
      "doi": "10.2196/58978",
      "authors": "Uhl Laura et al.",
      "keywords": "bed management; framework; health care data; patient pathway; preprocessing",
      "mesh_terms": "Humans; Algorithms; Critical Pathways; Data Mining; Bias; Emergency Service, Hospital; Databases, Factual",
      "pub_types": "Journal Article; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39312289/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Emergency Medicine; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; Emergency Department",
      "key_findings": "CONCLUSIONS: Patient pathway data reflect the actual activity of hospitals that is governed by medical requirements and logistical limitations. Before using these data, these limitations should be identified and corrected. We anticipate that our approach can be generalized to obtain unbiased analyses of patient pathways for other hospitals.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11459108"
    },
    {
      "pmid": "39316436",
      "title": "Equity in Digital Mental Health Interventions in the United States: Where to Next?",
      "abstract": "Health care technologies have the ability to bridge or hinder equitable care. Advocates of digital mental health interventions (DMHIs) report that such technologies are poised to reduce the documented gross health care inequities that have plagued generations of people seeking care in the United States. This is due to a multitude of factors such as their potential to revolutionize access; mitigate logistical barriers to in-person mental health care; and leverage patient inputs to formulate tailored, responsive, and personalized experiences. Although we agree with the potential of DMHIs to advance health equity, we articulate several steps essential to mobilize and sustain meaningful forward progression in this endeavor, reflecting on decades of research and learnings drawn from multiple fields of expertise and real-world experience. First, DMHI manufacturers must build diversity, equity, inclusion, and belonging (DEIB) processes into the full spectrum of product evolution itself (eg, product design, evidence generation) as well as into the fabric of internal company practices (eg, talent recruitment, communication principles, and advisory boards). Second, awareness of the DEIB efforts-or lack thereof-in DMHI research trials is needed to refine and optimize future study design for inclusivity as well as proactively address potential barriers to doing so. Trials should incorporate thoughtful, inclusive, and creative approaches to recruitment, enrollment, and measurement of social determinants of health and self-identity, as well as a prioritization of planned and exploratory analyses examining outcomes across various groups of people. Third, mental health care advocacy, research funding policies, and local and federal legislation can advance these pursuits, with directives from the US Preventive Services Taskforce, National Institutes of Health, and Food and Drug Administration applied as poignant examples. For products with artificial intelligence/machine learning, maintaining a \"human in the loop\" as well as prespecified and adaptive analytic frameworks to monitor and remediate potential algorithmic bias can reduce the risk of increasing inequity. Last, but certainly not least, is a call for partnership and transparency within and across ecosystems (academic, industry, payer, provider, regulatory agencies, and value-based care organizations) to reliably build health equity into real-world DMHI product deployments and evidence-generation strategies. All these considerations should also extend into the context of an equity-informed commercial strategy for DMHI manufacturers and health care organizations alike. The potential to advance health equity in innovation with DMHI is apparent. We advocate the field's thoughtful and evergreen advancement in inclusivity, thereby redefining the mental health care experience for this generation and those to come.",
      "journal": "Journal of medical Internet research",
      "year": "2024",
      "doi": "10.2196/59939",
      "authors": "Robinson Athena et al.",
      "keywords": "Digital Mental Health Interventions; access to health care; health equity; health plan implementations; mental health",
      "mesh_terms": "Humans; United States; Mental Health Services; Mental Health; Health Equity; Telemedicine; Healthcare Disparities",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39316436/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "NLP/LLM; Generative AI",
      "health_domain": "Mental Health/Psychiatry; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Insurance Status",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "The potential to advance health equity in innovation with DMHI is apparent. We advocate the field's thoughtful and evergreen advancement in inclusivity, thereby redefining the mental health care experience for this generation and those to come.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11462105"
    },
    {
      "pmid": "39331661",
      "title": "Trust as moral currency: Perspectives of health researchers in sub-Saharan Africa on strategies to promote equitable data sharing.",
      "abstract": "Groundbreaking data-sharing techniques and quick access to stored research data from the African continent are highly beneficial to create diverse unbiased datasets to inform digital health technologies and artificial intelligence in healthcare. Yet health researchers in sub-Saharan Africa (SSA) experience individual and collective challenges that render them cautious and even hesitant to share data despite acknowledging the public health benefits of sharing. This qualitative study reports on the perspectives of health researchers regarding strategies to mitigate these challenges. In-depth interviews were conducted via Microsoft Teams with 16 researchers from 16 different countries across SSA between July 2022 and April 2023. Purposive and snowball sampling techniques were used to invite participants via email. Recorded interviews were transcribed, cleaned, coded and managed through Atlas.ti.22. Thematic Analysis was used to analyse the data. Three recurrent themes and several subthemes emerged around strategies to improve governance of data sharing. The main themes identified were (1) Strategies for change at a policy level: guideline development, (2) Strengthening data governance to improve data quality and (3) Reciprocity: towards equitable data sharing. Building trust is central to the promotion of data sharing amongst researchers on the African continent and with global partners. This can be achieved by enhancing research integrity and strengthening micro and macro level governance. Substantial resources are required from funders and governments to enhance data governance practices, to improve data literacy and to enhance data quality. High quality data from Africa will afford diversity to global data sets, reducing bias in algorithms built for artificial intelligence technologies in healthcare. Engagement with multiple stakeholders including researchers and research communities is necessary to establish an equitable data sharing approach based on reciprocity and mutual benefit.",
      "journal": "PLOS digital health",
      "year": "2024",
      "doi": "10.1371/journal.pdig.0000551",
      "authors": "Brown Qunita et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39331661/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "High quality data from Africa will afford diversity to global data sets, reducing bias in algorithms built for artificial intelligence technologies in healthcare. Engagement with multiple stakeholders including researchers and research communities is necessary to establish an equitable data sharing approach based on reciprocity and mutual benefit.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11432837"
    },
    {
      "pmid": "39362934",
      "title": "A scoping review of reporting gaps in FDA-approved AI medical devices.",
      "abstract": "Machine learning and artificial intelligence (AI/ML) models in healthcare may exacerbate health biases. Regulatory oversight is critical in evaluating the safety and effectiveness of AI/ML devices in clinical settings. We conducted a scoping review on the 692 FDA-approved AI/ML-enabled medical devices approved from 1995-2023 to examine transparency, safety reporting, and sociodemographic representation. Only 3.6% of approvals reported race/ethnicity, 99.1% provided no socioeconomic data. 81.6% did not report the age of study subjects. Only 46.1% provided comprehensive detailed results of performance studies; only 1.9% included a link to a scientific publication with safety and efficacy data. Only 9.0% contained a prospective study for post-market surveillance. Despite the growing number of market-approved medical devices, our data shows that FDA reporting data remains inconsistent. Demographic and socioeconomic characteristics are underreported, exacerbating the risk of algorithmic bias and health disparity.",
      "journal": "NPJ digital medicine",
      "year": "2024",
      "doi": "10.1038/s41746-024-01270-x",
      "authors": "Muralidharan Vijaytha et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39362934/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Race/Ethnicity; Age; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Despite the growing number of market-approved medical devices, our data shows that FDA reporting data remains inconsistent. Demographic and socioeconomic characteristics are underreported, exacerbating the risk of algorithmic bias and health disparity.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 2 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11450195"
    },
    {
      "pmid": "39372817",
      "title": "Socio-ethical challenges and opportunities for advancing diversity, equity, and inclusion in digital medicine.",
      "abstract": "Digitalization in medicine offers a significant opportunity to transform healthcare systems by providing novel digital tools and services to guide personalized prevention, prediction, diagnosis, treatment and disease management. This transformation raises a number of novel socio-ethical considerations for individuals and society as a whole, which need to be appropriately addressed to ensure that digital medical devices (DMDs) are widely adopted and benefit all patients as well as healthcare service providers. In this narrative review, based on a broad literature search in PubMed, Web of Science, Google Scholar, we outline five core socio-ethical considerations in digital medicine that intersect with the notions of equity and digital inclusion: (i) access, use and engagement with DMDs, (ii) inclusiveness in DMD clinical trials, (iii) algorithm fairness, (iv) surveillance and datafication, and (v) data privacy and trust. By integrating literature from multidisciplinary fields, including social, medical, and computer sciences, we shed light on challenges and opportunities related to the development and adoption of DMDs. We begin with an overview of the different types of DMDs, followed by in-depth discussions of five socio-ethical implications associated with their deployment. Concluding our review, we provide evidence-based multilevel recommendations aimed at fostering a more inclusive digital landscape to ensure that the development and integration of DMDs in healthcare mitigate rather than cause, maintain or exacerbate health inequities.",
      "journal": "Digital health",
      "year": "2024",
      "doi": "10.1177/20552076241277705",
      "authors": "Paccoud Ivana et al.",
      "keywords": "AI fairness; Socio-ethical considerations; datafication, privacy and trust\u200c; digital determinants of health; digital health equity; diversity, equity and digital inclusion",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39372817/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Clinical Trial",
      "key_findings": "We begin with an overview of the different types of DMDs, followed by in-depth discussions of five socio-ethical implications associated with their deployment. Concluding our review, we provide evidence-based multilevel recommendations aimed at fostering a more inclusive digital landscape to ensure that the development and integration of DMDs in healthcare mitigate rather than cause, maintain or exacerbate health inequities.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11450794"
    },
    {
      "pmid": "39376670",
      "title": "Analyzing Racial Differences in Imaging Joint Replacement Registries Using Generative Artificial Intelligence: Advancing Orthopaedic Data Equity.",
      "abstract": "BACKGROUND: Discrepancies in medical data sets can perpetuate bias, especially when training deep learning models, potentially leading to biased outcomes in clinical applications. Understanding these biases is crucial for the development of equitable healthcare technologies. This study employs generative deep learning technology to explore and understand radiographic differences based on race among patients undergoing total hip arthroplasty. METHODS: Utilizing a large institutional registry, we retrospectively analyzed pelvic radiographs from total hip arthroplasty patients, characterized by demographics and image features. Denoising diffusion probabilistic models generated radiographs conditioned on demographic and imaging characteristics. Fr\u00e9chet Inception Distance assessed the generated image quality, showing the diversity and realism of the generated images. Sixty transition videos were generated that showed transforming White pelvises to their closest African American counterparts and vice versa while controlling for patients' sex, age, and body mass index. Two expert surgeons and 2 radiologists carefully studied these videos to understand the systematic differences that are present in the 2 races' radiographs. RESULTS: Our data set included 480,407 pelvic radiographs, with a predominance of White patients over African Americans. The generative denoising diffusion probabilistic model created high-quality images and reached an Fr\u00e9chet Inception Distance of 6.8. Experts identified 6 characteristics differentiating races, including interacetabular distance, osteoarthritis degree, obturator foramina shape, femoral neck-shaft angle, pelvic ring shape, and femoral cortical thickness. CONCLUSIONS: This study demonstrates the potential of generative models for understanding disparities in medical imaging data sets. By visualizing race-based differences, this method aids in identifying bias in downstream tasks, fostering the development of fairer healthcare practices.",
      "journal": "Arthroplasty today",
      "year": "2024",
      "doi": "10.1016/j.artd.2024.101503",
      "authors": "Khosravi Bardia et al.",
      "keywords": "Bias; Dataset curation; Equity; Explainability; Generative AI",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39376670/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Computer Vision/Imaging AI; Generative AI",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: This study demonstrates the potential of generative models for understanding disparities in medical imaging data sets. By visualizing race-based differences, this method aids in identifying bias in downstream tasks, fostering the development of fairer healthcare practices.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11456877"
    },
    {
      "pmid": "39437384",
      "title": "Gender Bias in AI's Perception of Cardiovascular Risk.",
      "abstract": "The study investigated gender bias in GPT-4's assessment of coronary artery disease risk by presenting identical clinical vignettes of men and women with and without psychiatric comorbidities. Results suggest that psychiatric conditions may influence GPT-4's coronary artery disease risk assessment among men and women.",
      "journal": "Journal of medical Internet research",
      "year": "2024",
      "doi": "10.2196/54242",
      "authors": "Achtari Margaux et al.",
      "keywords": "AI; CAD; artery; artificial intelligence; cardiovascular; chatbot: health care; coronary; coronary artery disease; gender; gender bias; gender equity; men: women; risk",
      "mesh_terms": "Humans; Female; Male; Sexism; Cardiovascular Diseases; Middle Aged; Risk Assessment; Artificial Intelligence; Adult; Heart Disease Risk Factors; Coronary Artery Disease",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39437384/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Cardiology; Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The study investigated gender bias in GPT-4's assessment of coronary artery disease risk by presenting identical clinical vignettes of men and women with and without psychiatric comorbidities. Results suggest that psychiatric conditions may influence GPT-4's coronary artery disease risk assessment among men and women.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11538872"
    },
    {
      "pmid": "39488166",
      "title": "Mitigating biases in feature selection and importance assessments in predictive models using LASSO regression.",
      "abstract": "Yuan et al. developed a predictive model for early response using sub-regional radiomic features from multi-sequence MRI alongside clinical factors. However, biases in feature selection and assessment may lead to misleading conclusions regarding feature importance. This paper elucidates the biases induced by machine learning models and advocates for a robust methodology utilizing statistical techniques, such as Chi-squared tests and p-values, to uncover true associations. By emphasizing the vital distinction between true and model-specific associations, we promote a comprehensive approach that integrates multiple modeling techniques. This strategy enhances the reliability of predictive models in medical imaging, ensuring that outcomes are based on objective relationships and ultimately improving patient care.",
      "journal": "Oral oncology",
      "year": "2024",
      "doi": "10.1016/j.oraloncology.2024.107090",
      "authors": "Takefuji Yoshiyasu",
      "keywords": "Feature selection; LASSO regression; Machine learning; Predictive modeling; Statistical methods",
      "mesh_terms": "Humans; Machine Learning; Bias; Magnetic Resonance Imaging; Models, Statistical",
      "pub_types": "Letter",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39488166/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Computer Vision/Imaging AI; Clinical Prediction Model",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "By emphasizing the vital distinction between true and model-specific associations, we promote a comprehensive approach that integrates multiple modeling techniques. This strategy enhances the reliability of predictive models in medical imaging, ensuring that outcomes are based on objective relationships and ultimately improving patient care.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39500908",
      "title": "Artificial intelligence tools trained on human-labeled data reflect human biases: a case study in a large clinical consecutive knee osteoarthritis cohort.",
      "abstract": "Humans have been shown to have biases when reading medical images, raising questions about whether humans are uniform in their disease gradings. Artificial intelligence (AI) tools trained on human-labeled data may have inherent human non-uniformity. In this study, we used a radiographic knee osteoarthritis external validation dataset of 50 patients and a six-year retrospective consecutive clinical cohort of 8,273 patients. An FDA-approved and CE-marked AI tool was tested for potential non-uniformity in Kellgren-Lawrence grades between the right and left sides of the images. We flipped the images horizontally so that a left knee looked like a right knee and vice versa. According to human review, the AI tool showed non-uniformity with 20-22% disagreements on the external validation dataset and 13.6% on the cohort. However, we found no evidence of a significant difference in the accuracy compared to senior radiologists on the external validation dataset, or age bias or sex bias on the cohort. AI non-uniformity can boost the evaluated performance against humans, but image areas with inferior performance should be investigated.",
      "journal": "Scientific reports",
      "year": "2024",
      "doi": "10.1038/s41598-024-75752-z",
      "authors": "Lenskjold Anders et al.",
      "keywords": "Laterality; Artificial intelligence; Bias; Clinical data; Knee osteoarthritis; Uniform performance",
      "mesh_terms": "Humans; Osteoarthritis, Knee; Artificial Intelligence; Male; Female; Aged; Middle Aged; Retrospective Studies; Cohort Studies; Bias; Radiography",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39500908/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "However, we found no evidence of a significant difference in the accuracy compared to senior radiologists on the external validation dataset, or age bias or sex bias on the cohort. AI non-uniformity can boost the evaluated performance against humans, but image areas with inferior performance should be investigated.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11538298"
    },
    {
      "pmid": "39535532",
      "title": "Mitigating bias in radiology: The promise of topological data analysis and simplicial complexes.",
      "abstract": "Topological Data Analysis (TDA) and simplicial complexes offer a novel approach to address biases in AI-assisted radiology. By capturing complex structures, n-way interactions, and geometric relationships in medical images, TDA enhances feature extraction, improves representation robustness, and increases interpretability. This mathematical framework has the potential to significantly improve the accuracy and fairness of radiological assessments, paving the way for more equitable patient care.",
      "journal": "Oncotarget",
      "year": "2024",
      "doi": "10.18632/oncotarget.28668",
      "authors": "Singh Yashbir et al.",
      "keywords": "medical imaging; radiology; simplicial complexes; topological data analysis",
      "mesh_terms": "Humans; Radiology; Bias; Data Analysis; Artificial Intelligence; Algorithms",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39535532/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "By capturing complex structures, n-way interactions, and geometric relationships in medical images, TDA enhances feature extraction, improves representation robustness, and increases interpretability. This mathematical framework has the potential to significantly improve the accuracy and fairness of radiological assessments, paving the way for more equitable patient care.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11559658"
    },
    {
      "pmid": "39561363",
      "title": "Mitigating Cognitive Biases in Clinical Decision-Making Through Multi-Agent Conversations Using Large Language Models: Simulation Study.",
      "abstract": "BACKGROUND: Cognitive biases in clinical decision-making significantly contribute to errors in diagnosis and suboptimal patient outcomes. Addressing these biases presents a formidable challenge in the medical field. OBJECTIVE: This study aimed to explore the role of large language models (LLMs) in mitigating these biases through the use of the multi-agent framework. We simulate the clinical decision-making processes through multi-agent conversation and evaluate its efficacy in improving diagnostic accuracy compared with humans. METHODS: A total of 16 published and unpublished case reports where cognitive biases have resulted in misdiagnoses were identified from the literature. In the multi-agent framework, we leveraged GPT-4 (OpenAI) to facilitate interactions among different simulated agents to replicate clinical team dynamics. Each agent was assigned a distinct role: (1) making the final diagnosis after considering the discussions, (2) acting as a devil's advocate to correct confirmation and anchoring biases, (3) serving as a field expert in the required medical subspecialty, (4) facilitating discussions to mitigate premature closure bias, and (5) recording and summarizing findings. We tested varying combinations of these agents within the framework to determine which configuration yielded the highest rate of correct final diagnoses. Each scenario was repeated 5 times for consistency. The accuracy of the initial diagnoses and the final differential diagnoses were evaluated, and comparisons with human-generated answers were made using the Fisher exact test. RESULTS: A total of 240 responses were evaluated (3 different multi-agent frameworks). The initial diagnosis had an accuracy of 0% (0/80). However, following multi-agent discussions, the accuracy for the top 2 differential diagnoses increased to 76% (61/80) for the best-performing multi-agent framework (Framework 4-C). This was significantly higher compared with the accuracy achieved by human evaluators (odds ratio 3.49; P=.002). CONCLUSIONS: The multi-agent framework demonstrated an ability to re-evaluate and correct misconceptions, even in scenarios with misleading initial investigations. In addition, the LLM-driven, multi-agent conversation framework shows promise in enhancing diagnostic accuracy in diagnostically challenging medical scenarios.",
      "journal": "Journal of medical Internet research",
      "year": "2024",
      "doi": "10.2196/59439",
      "authors": "Ke Yuhe et al.",
      "keywords": "clinical decision-making; cognitive bias; generative artificial intelligence; large language model; multi-agent",
      "mesh_terms": "Humans; Clinical Decision-Making; Cognition; Bias; Language; Computer Simulation; Diagnostic Errors; Communication",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39561363/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: The multi-agent framework demonstrated an ability to re-evaluate and correct misconceptions, even in scenarios with misleading initial investigations. In addition, the LLM-driven, multi-agent conversation framework shows promise in enhancing diagnostic accuracy in diagnostically challenging medical scenarios.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11615553"
    },
    {
      "pmid": "39565825",
      "title": "Learning patterns of HIV-1 resistance to broadly neutralizing antibodies with reduced subtype bias using multi-task learning.",
      "abstract": "The ability to predict HIV-1 resistance to broadly neutralizing antibodies (bnAbs) will increase bnAb therapeutic benefits. Machine learning is a powerful approach for such prediction. One challenge is that some HIV-1 subtypes in currently available training datasets are underrepresented, which likely affects models' generalizability across subtypes. A second challenge is that combinations of bnAbs are required to avoid the inevitable resistance to a single bnAb, and computationally determining optimal combinations of bnAbs is an unsolved problem. Recently, machine learning models trained using resistance outcomes for multiple antibodies at once, a strategy called multi-task learning (MTL), have been shown to improve predictions. We develop a new model and show that, beyond the boost in performance, MTL also helps address the previous two challenges. Specifically, we demonstrate empirically that MTL can mitigate bias from underrepresented subtypes, and that MTL allows the model to learn patterns of co-resistance to combinations of antibodies, thus providing tools to predict antibodies' epitopes and to potentially select optimal bnAb combinations. Our analyses, publicly available at https://github.com/iaime/LBUM, can be adapted to other infectious diseases that are treated with antibody therapy.",
      "journal": "PLoS computational biology",
      "year": "2024",
      "doi": "10.1371/journal.pcbi.1012618",
      "authors": "Igiraneza Aime Bienfait et al.",
      "keywords": "",
      "mesh_terms": "HIV-1; Humans; HIV Antibodies; HIV Infections; Machine Learning; Computational Biology; Antibodies, Neutralizing; Broadly Neutralizing Antibodies; Epitopes",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39565825/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Infectious Disease",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Multi-task Learning",
      "clinical_setting": "Not specified",
      "key_findings": "Specifically, we demonstrate empirically that MTL can mitigate bias from underrepresented subtypes, and that MTL allows the model to learn patterns of co-resistance to combinations of antibodies, thus providing tools to predict antibodies' epitopes and to potentially select optimal bnAb combinations. Our analyses, publicly available at https://github.com/iaime/LBUM, can be adapted to other infectious diseases that are treated with antibody therapy.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11616810"
    },
    {
      "pmid": "39652394",
      "title": "Use of ChatGPT to Explore Gender and Geographic Disparities in Scientific Peer Review.",
      "abstract": "BACKGROUND: In the realm of scientific research, peer review serves as a cornerstone for ensuring the quality and integrity of scholarly papers. Recent trends in promoting transparency and accountability has led some journals to publish peer-review reports alongside papers. OBJECTIVE: ChatGPT-4 (OpenAI) was used to quantitatively assess sentiment and politeness in peer-review reports from high-impact medical journals. The objective was to explore gender and geographical disparities to enhance inclusivity within the peer-review process. METHODS: All 9 general medical journals with an impact factor >2 that publish peer-review reports were identified. A total of 12 research papers per journal were randomly selected, all published in 2023. The names of the first and last authors along with the first author's country of affiliation were collected, and the gender of both the first and last authors was determined. For each review, ChatGPT-4 was asked to evaluate the \"sentiment score,\" ranging from -100 (negative) to 0 (neutral) to +100 (positive), and the \"politeness score,\" ranging from -100 (rude) to 0 (neutral) to +100 (polite). The measurements were repeated 5 times and the minimum and maximum values were removed. The mean sentiment and politeness scores for each review were computed and then summarized using the median and interquartile range. Statistical analyses included Wilcoxon rank-sum tests, Kruskal-Wallis rank tests, and negative binomial regressions. RESULTS: Analysis of 291 peer-review reports corresponding to 108 papers unveiled notable regional disparities. Papers from the Middle East, Latin America, or Africa exhibited lower sentiment and politeness scores compared to those from North America, Europe, or Pacific and Asia (sentiment scores: 27 vs 60 and 62 respectively; politeness scores: 43.5 vs 67 and 65 respectively, adjusted P=.02). No significant differences based on authors' gender were observed (all P>.05). CONCLUSIONS: Notable regional disparities were found, with papers from the Middle East, Latin America, and Africa demonstrating significantly lower scores, while no discernible differences were observed based on authors' gender. The absence of gender-based differences suggests that gender biases may not manifest as prominently as other forms of bias within the context of peer review. The study underscores the need for targeted interventions to address regional disparities in peer review and advocates for ongoing efforts to promote equity and inclusivity in scholarly communication.",
      "journal": "Journal of medical Internet research",
      "year": "2024",
      "doi": "10.2196/57667",
      "authors": "Sebo Paul",
      "keywords": "Africa; ChatGPT; artificial intelligence; assessment; communication; consultation; discrimination; disparity; gender; gender bias; geographic; global south; inequality; peer review; researcher; sentiment analysis; woman",
      "mesh_terms": "Humans; Male; Female; Peer Review; Journal Impact Factor; Periodicals as Topic; Sex Factors; Peer Review, Research",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39652394/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Notable regional disparities were found, with papers from the Middle East, Latin America, and Africa demonstrating significantly lower scores, while no discernible differences were observed based on authors' gender. The absence of gender-based differences suggests that gender biases may not manifest as prominently as other forms of bias within the context of peer review. The study underscores the need for targeted interventions to address regional disparities in peer review and advo...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11667125"
    },
    {
      "pmid": "39666377",
      "title": "Language disparities in pandemic information: Autocomplete analysis of COVID-19 searches in New York.",
      "abstract": "Objective: To audit and compare search autocomplete results in Spanish and English during the early COVID-19 pandemic in the New York metropolitan area. The pandemic led to significant online search activity about the disease, its spread, and remedies. As gatekeepers, search engines like Google can influence public opinion. Autocomplete predictions help users complete searches faster but may also shape their views. Understanding these differences is crucial to identify biases and ensure equitable information dissemination. Methods: The study tracked autocomplete results daily for five COVID-19 related search terms in English and Spanish over 100+ days in 2020, yielding a total of 9164 autocomplete predictions. Results: Queries in Spanish yielded fewer autocomplete options and often included more negative content than English autocompletes. The topical coverage differed, with Spanish autocompletes including themes related to religion and spirituality that were absent in the English search autocompletes. Conclusion: The contrast in search autocomplete results could lead to divergent impressions about the pandemic and remedial actions among different sections of society. Continuous auditing of autocompletes by public health stakeholders and search engine organizations is recommended to reduce potential bias and misinformation.",
      "journal": "Health informatics journal",
      "year": "2024",
      "doi": "10.1177/14604582241307836",
      "authors": "Singh Vivek K et al.",
      "keywords": "COVID health information; algorithmic bias; health disparity; search autocompletes; search bias",
      "mesh_terms": "COVID-19; Humans; Language; Search Engine; Pandemics; New York; SARS-CoV-2; New York City; Information Seeking Behavior; Information Dissemination",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39666377/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Generative AI",
      "health_domain": "Pulmonology; Public Health",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability; Bias Auditing Framework",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Conclusion: The contrast in search autocomplete results could lead to divergent impressions about the pandemic and remedial actions among different sections of society. Continuous auditing of autocompletes by public health stakeholders and search engine organizations is recommended to reduce potential bias and misinformation.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "39679140",
      "title": "Evaluating Large Language Model-Supported Instructions for Medication Use: First Steps Toward a Comprehensive Model.",
      "abstract": "OBJECTIVE: To assess the support of large language models (LLMs) in generating clearer and more personalized medication instructions to enhance e-prescription. PATIENTS AND METHODS: We established patient-centered guidelines for adequate, acceptable, and personalized directions to enhance e-prescription. A dataset comprising 104 outpatient scenarios, with an array of medications, administration routes, and patient conditions, was developed following the Brazilian national e-prescribing standard. Three prompts were submitted to a closed-source LLM. The first prompt involved a generic command, the second one was calibrated for content enhancement and personalization, and the third one requested bias mitigation. The third prompt was submitted to an open-source LLM. Outputs were assessed using automated metrics and human evaluation. We conducted the study between March 1, 2024 and September 10, 2024. RESULTS: Adequacy scores of our closed-source LLM's output showed the third prompt outperforming the first and second one. Full and partial acceptability was achieved in 94.3% of texts with the third prompt. Personalization was rated highly, especially with the second and third prompts. The 2 LLMs showed similar adequacy results. Lack of scientific evidence and factual errors were infrequent and unrelated to a particular prompt or LLM. The frequency of hallucinations was different for each LLM and concerned prescriptions issued upon symptom manifestation and medications requiring dosage adjustment or involving intermittent use. Gender bias was found in our closed-source LLM's output for the first and second prompts, with the third one being bias-free. The second LLM's output was bias-free. CONCLUSION: This study demonstrates the potential of LLM-supported generation to produce prescription directions and improve communication between health professionals and patients within the e-prescribing system.",
      "journal": "Mayo Clinic proceedings. Digital health",
      "year": "2024",
      "doi": "10.1016/j.mcpdig.2024.09.006",
      "authors": "Reis Zilma Silveira Nogueira et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39679140/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Primary Care/Outpatient",
      "key_findings": "CONCLUSION: This study demonstrates the potential of LLM-supported generation to produce prescription directions and improve communication between health professionals and patients within the e-prescribing system.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11638470"
    },
    {
      "pmid": "39707939",
      "title": "Accountability for Reasonableness as a Framework for the Promotion of Fair and Equitable Research.",
      "abstract": "Despite increased efforts to ensure diversity in genomic research, the exclusion of minority groups from data analyses and publications remains a critical issue. This paper addresses the ethical implications of these exclusions and proposes accountability for reasonableness (A4R) as a framework to promote fairness and equity in research. Originally conceived by Norman Daniels and James Sabin to guide resource allocation in the context of health policy, A4R emphasizes publicity, relevance of reasons, enforcement, and revision as essential for legitimacy and trust in the decision-making process. The authors argue that A4R is also relevant to resource allocation in research and that, if adequately informed and incentivized by funding agencies, institutional review boards, and scientific journals, researchers are well-positioned to assess data-selection justifications. The A4R framework provides a promising foundation for fostering accountability in genomics and other fields, including artificial intelligence, where lack of diversity and pervasive biases threaten equitable benefit sharing.",
      "journal": "The Hastings Center report",
      "year": "2024",
      "doi": "10.1002/hast.4931",
      "authors": "Dupras Charles et al.",
      "keywords": "A4R; accountability for reasonableness; diversity; equity and fairness; exclusion; genomic research; procedural justice; research ethics",
      "mesh_terms": "Humans; Social Responsibility; Minority Groups; Genomics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39707939/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The authors argue that A4R is also relevant to resource allocation in research and that, if adequately informed and incentivized by funding agencies, institutional review boards, and scientific journals, researchers are well-positioned to assess data-selection justifications. The A4R framework provides a promising foundation for fostering accountability in genomics and other fields, including artificial intelligence, where lack of diversity and pervasive biases threaten equitable benefit sharing...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11662771"
    },
    {
      "pmid": "39767481",
      "title": "AI in Biomedicine-A Forward-Looking Perspective on Health Equity.",
      "abstract": "As new artificial intelligence (AI) tools are being developed and as AI continues to revolutionize healthcare, its potential to advance health equity is increasingly recognized. The 2024 Research Centers in Minority Institutions (RCMI) Consortium National Conference session titled \"Artificial Intelligence: Safely, Ethically, and Responsibly\" brought together experts from diverse institutions to explore AI's role and challenges in advancing health equity. This report summarizes presentations and discussions from the conference focused on AI's potential and its challenges, particularly algorithmic bias, transparency, and the under-representation of minority groups in AI datasets. Key topics included AI's predictive and generative capabilities in healthcare, ethical governance, and key national initiatives, like AIM-AHEAD. The session highlighted the critical role of RCMI institutions in fostering diverse AI/machine learning research and in developing culturally competent AI tools. Other discussions included AI's capacity to improve patient outcomes, especially for underserved communities, and underscored the necessity for robust ethical standards, a diverse AI and scientific workforce, transparency, and inclusive data practices. The engagement of RCMI institutions is critical to ensure practices in AI development and deployment which prioritize health equity, thus paving the way for a more inclusive AI-driven healthcare system.",
      "journal": "International journal of environmental research and public health",
      "year": "2024",
      "doi": "10.3390/ijerph21121642",
      "authors": "Kumar Deepak et al.",
      "keywords": "RCMI; artificial intelligence; augmented intelligence; health disparities; health equity; health ethics; machine learning",
      "mesh_terms": "Health Equity; Artificial Intelligence; Humans; Biomedical Research",
      "pub_types": "Journal Article; Conference Proceedings",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39767481/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Diverse/Representative Data",
      "clinical_setting": "ICU; Safety-Net/Underserved",
      "key_findings": "Other discussions included AI's capacity to improve patient outcomes, especially for underserved communities, and underscored the necessity for robust ethical standards, a diverse AI and scientific workforce, transparency, and inclusive data practices. The engagement of RCMI institutions is critical to ensure practices in AI development and deployment which prioritize health equity, thus paving the way for a more inclusive AI-driven healthcare system.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11675414"
    },
    {
      "pmid": "39831110",
      "title": "Bias in Prediction Models to Identify Patients With Colorectal Cancer at High Risk for Readmission After Resection.",
      "abstract": "PURPOSE: Machine learning algorithms are used for predictive modeling in medicine, but studies often do not evaluate or report on the potential biases of the models. Our purpose was to develop clinical prediction models for readmission after surgery in colorectal cancer (CRC) patients and to examine their potential for racial bias. METHODS: We used the 2012-2020 American College of Surgeons' National Surgical Quality Improvement Program (ACS-NSQIP) Participant Use File and Targeted Colectomy File. Patients were categorized into four race groups - White, Black or African American, Other, and Unknown/Not Reported. Potential predictive features were identified from studies of risk factors of 30-day readmission in CRC patients. We compared four machine learning-based methods - logistic regression (LR), multilayer perceptron (MLP), random forest (RF), and XGBoost (XGB). Model bias was assessed using false negative rate (FNR) difference, false positive rate (FPR) difference, and disparate impact. RESULTS: In all, 112,077 patients were included, 67.2% of whom were White, 9.2% Black, 5.6% Other race, and 18% with race not recorded. There were significant differences in the AUROC, FPR and FNR between race groups across all models. Notably, patients in the 'Other' race category had higher FNR compared to Black patients in all but the XGB model, while Black patients had higher FPR than White patients in some models. Patients in the 'Other' category consistently had the lowest FPR. Applying the 80% rule for disparate impact, the models consistently met the threshold for unfairness for the 'Other' race category. CONCLUSION: Predictive models for 30-day readmission after colorectal surgery may perform unequally for different race groups, potentially propagating to inequalities in delivery of care and patient outcomes if the predictions from these models are used to direct care.",
      "journal": "JCO clinical cancer informatics",
      "year": "2024",
      "doi": "10.1200/CCI.23.00194",
      "authors": "Lucas Mary M et al.",
      "keywords": "",
      "mesh_terms": "Aged; Female; Humans; Male; Middle Aged; Bias; Black or African American; Colectomy; Colorectal Neoplasms; Machine Learning; Patient Readmission; Risk Assessment; Risk Factors; White",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39831110/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Random Forest; Logistic Regression; XGBoost/Gradient Boosting; Neural Network; Clinical Prediction Model",
      "health_domain": "Oncology; Surgery",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Threshold Adjustment; Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Predictive models for 30-day readmission after colorectal surgery may perform unequally for different race groups, potentially propagating to inequalities in delivery of care and patient outcomes if the predictions from these models are used to direct care.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11741203"
    },
    {
      "pmid": "40039481",
      "title": "Understanding Bias in Multispectral Autofluorescence Lifetime Imaging: Are Models Sensitive to Oral Location?",
      "abstract": "While bias in artificial intelligence is gaining attention across applications, model fairness is especially concerning in medical applications because a person's health may depend on the model outcome. Sources of bias in medical applications include age, gender, race, and social history. However, in oral cancer diagnosis, the oral location may be a source of bias. Variability in performance based on the oral location has been reported but is not well understood. To help ensure that models perform equitably regardless of location, we design three experiments to study the effect of oral location on model performance. We show that multispectral autofluorescence images retain tissue-type characteristics, but that the tissue-specific information is degraded in lesion images. Furthermore, we show that the tissue-specific features are not disentangled from the disease-associated features. Our results show that automated diagnosis models need to be thoughtfully designed to remove bias from the oral location to ensure equitable performance. Based on these insights, we propose a tissue-specific fine-tuning approach that increases overall performance and lowers the fairness gap by over 5%.Clinical relevance- This paper explores sources of offtarget variance in multispectral autofluorescence images. By understanding sources of bias in multispectral autofluorescence images, fairer and more robust models for oral cancer diagnosis and margin delineation can be developed, leading to greater clinical acceptance and more equitable patient outcomes.",
      "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference",
      "year": "2024",
      "doi": "10.1109/EMBC53108.2024.10781827",
      "authors": "Caughlin Kayla et al.",
      "keywords": "",
      "mesh_terms": "Humans; Optical Imaging; Mouth Neoplasms; Mouth; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40039481/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; Ophthalmology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Representation Learning; Transfer Learning; Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Based on these insights, we propose a tissue-specific fine-tuning approach that increases overall performance and lowers the fairness gap by over 5%.Clinical relevance- This paper explores sources of offtarget variance in multispectral autofluorescence images. By understanding sources of bias in multispectral autofluorescence images, fairer and more robust models for oral cancer diagnosis and margin delineation can be developed, leading to greater clinical acceptance and more equitable patient o...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "40144330",
      "title": "Artificial Intelligence to Promote Racial and Ethnic Cardiovascular Health Equity.",
      "abstract": "PURPOSE OF REVIEW: The integration of artificial intelligence (AI) in medicine holds promise for transformative advancements aimed at improving healthcare outcomes. Amidst this promise, AI has been envisioned as a tool to detect and mitigate racial and ethnic inequity known to plague current cardiovascular care. However, this enthusiasm is dampened by the recognition that AI itself can harbor and propagate biases, necessitating a careful approach to ensure equity. This review highlights topics in the landscape of AI in cardiology, its role in identifying and addressing healthcare inequities, promoting diversity in research, concerns surrounding its applications, and proposed strategies for fostering equitable utilization. RECENT FINDINGS: Artificial intelligence has proven to be a valuable tool for clinicians in diagnosing and mitigating racial and ethnic inequities in cardiology, as well as the promotion of diversity in research. This promise is counterbalanced by the cautionary reality that AI can inadvertently perpetuate existent biases stemming from limited diversity in training data, inherent biases within datasets, and inadequate bias detection and monitoring mechanisms. Recognizing these concerns, experts emphasize the need for rigorous efforts to address these limitations in the development and deployment of AI within medicine. SUMMARY: Implementing AI in cardiovascular care to identify and address racial and ethnic inequities requires careful design and execution, beginning with meticulous data collection and a thorough review of training datasets. Furthermore, ensuring equitable performance involves rigorous testing and continuous surveillance of algorithms. Lastly, the promotion of diversity in the AI workforce and engagement of stakeholders are crucial to the advancement of equity to ultimately realize the potential for artificial intelligence for cardiovascular health equity.",
      "journal": "Current cardiovascular risk reports",
      "year": "2024",
      "doi": "10.1007/s12170-024-00745-6",
      "authors": "Amponsah Daniel et al.",
      "keywords": "Artificial Intelligence; Cardiovascular Equity; Disparities; Diversity; Machine Learning; Racial and Ethnic Inequity",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40144330/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; ICU/Critical Care; Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "FINDINGS: Artificial intelligence has proven to be a valuable tool for clinicians in diagnosing and mitigating racial and ethnic inequities in cardiology, as well as the promotion of diversity in research. This promise is counterbalanced by the cautionary reality that AI can inadvertently perpetuate existent biases stemming from limited diversity in training data, inherent biases within datasets, and inadequate bias detection and monitoring mechanisms. Recognizing these concerns, experts emphasi...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11938301"
    },
    {
      "pmid": "40417489",
      "title": "Ensuring Fairness in Detecting Mild Cognitive Impairment with MRI.",
      "abstract": "Machine learning (ML) algorithms play a crucial role in the early and accurate diagnosis of Alzheimer's Disease (AD), which is essential for effective treatment planning. However, existing methods are not well-suited for identifying Mild Cognitive Impairment (MCI), a critical transitional stage between normal aging and AD. This inadequacy is primarily due to label imbalance and bias from different sensitve attributes in MCI classification. To overcome these challenges, we have designed an end-to-end fairness-aware approach for label-imbalanced classification, tailored specifically for neuroimaging data. This method, built on the recently developed FACIMS framework, integrates into STREAMLINE, an automated ML environment. We evaluated our approach against nine other ML algorithms and found that it achieves comparable balanced accuracy to other methods while prioritizing fairness in classifications with five different sensitive attributes. This analysis contributes to the development of equitable and reliable ML diagnostics for MCI detection.",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "year": "2024",
      "doi": "",
      "authors": "Tong Boning et al.",
      "keywords": "",
      "mesh_terms": "Cognitive Dysfunction; Humans; Magnetic Resonance Imaging; Machine Learning; Algorithms; Alzheimer Disease",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40417489/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Neurology",
      "bias_axes": "Gender/Sex; Age; Disability",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We evaluated our approach against nine other ML algorithms and found that it achieves comparable balanced accuracy to other methods while prioritizing fairness in classifications with five different sensitive attributes. This analysis contributes to the development of equitable and reliable ML diagnostics for MCI detection.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12099326"
    },
    {
      "pmid": "40574796",
      "title": "From Biased Selective Labels to Pseudo-Labels: An Expectation-Maximization Framework for Learning from Biased Decisions.",
      "abstract": "Selective labels occur when label observations are subject to a decision-making process; e.g., diagnoses that depend on the administration of laboratory tests. We study a clinically-inspired selective label problem called disparate censorship, where labeling biases vary across subgroups and unlabeled individuals are imputed as \"negative\" (i.e., no diagnostic test = no illness). Machine learning models na\u00efvely trained on such labels could amplify labeling bias. Inspired by causal models of selective labels, we propose Disparate Censorship Expectation-Maximization (DCEM), an algorithm for learning in the presence of disparate censorship. We theoretically analyze how DCEM mitigates the effects of disparate censorship on model performance. We validate DCEM on synthetic data, showing that it improves bias mitigation (area between ROC curves) without sacrificing discriminative performance (AUC) compared to baselines. We achieve similar results in a sepsis classification task using clinical data.",
      "journal": "Proceedings of machine learning research",
      "year": "2024",
      "doi": "",
      "authors": "Chang Trenton et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40574796/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Data Augmentation",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "We validate DCEM on synthetic data, showing that it improves bias mitigation (area between ROC curves) without sacrificing discriminative performance (AUC) compared to baselines. We achieve similar results in a sepsis classification task using clinical data.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12199211"
    },
    {
      "pmid": "35142367",
      "title": "A joint fairness model with applications to risk predictions for underrepresented populations.",
      "abstract": "In data collection for predictive modeling, underrepresentation of certain groups, based on gender, race/ethnicity, or age, may yield less accurate predictions for these groups. Recently, this issue of fairness in predictions has attracted significant attention, as data-driven models are increasingly utilized to perform crucial decision-making tasks. Existing methods to achieve fairness in the machine learning literature typically build a single prediction model in a manner that encourages fair prediction performance for all groups. These approaches have two major limitations: (i) fairness is often achieved by compromising accuracy for some groups; (ii) the underlying relationship between dependent and independent variables may not be the same across groups. We propose a joint fairness model (JFM) approach for logistic regression models for binary outcomes that estimates group-specific classifiers using a joint modeling objective function that incorporates fairness criteria for prediction. We introduce an accelerated smoothing proximal gradient algorithm to solve the convex objective function, and present the key asymptotic properties of the JFM estimates. Through simulations, we demonstrate the efficacy of the JFM in achieving good prediction performance and across-group parity, in comparison with the single fairness model, group-separate model, and group-ignorant model, especially when the minority group's sample size is small. Finally, we demonstrate the utility of the JFM method in a real-world example to obtain fair risk predictions for underrepresented older patients diagnosed with coronavirus disease 2019 (COVID-19).",
      "journal": "Biometrics",
      "year": "2023",
      "doi": "10.1111/biom.13632",
      "authors": "Do Hyungrok et al.",
      "keywords": "algorithmic bias; algorithmic fairness; joint estimation; underrepresented population",
      "mesh_terms": "Humans; COVID-19; Logistic Models; Algorithms",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35142367/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Logistic Regression; Clinical Prediction Model; Regression",
      "health_domain": "Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Deployment",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Through simulations, we demonstrate the efficacy of the JFM in achieving good prediction performance and across-group parity, in comparison with the single fairness model, group-separate model, and group-ignorant model, especially when the minority group's sample size is small. Finally, we demonstrate the utility of the JFM method in a real-world example to obtain fair risk predictions for underrepresented older patients diagnosed with coronavirus disease 2019 (COVID-19).",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9363518"
    },
    {
      "pmid": "36490369",
      "title": "A Call to Action on Assessing and Mitigating Bias in Artificial Intelligence Applications for Mental Health.",
      "abstract": "Advances in computer science and data-analytic methods are driving a new era in mental health research and application. Artificial intelligence (AI) technologies hold the potential to enhance the assessment, diagnosis, and treatment of people experiencing mental health problems and to increase the reach and impact of mental health care. However, AI applications will not mitigate mental health disparities if they are built from historical data that reflect underlying social biases and inequities. AI models biased against sensitive classes could reinforce and even perpetuate existing inequities if these models create legacies that differentially impact who is diagnosed and treated, and how effectively. The current article reviews the health-equity implications of applying AI to mental health problems, outlines state-of-the-art methods for assessing and mitigating algorithmic bias, and presents a call to action to guide the development of fair-aware AI in psychological science.",
      "journal": "Perspectives on psychological science : a journal of the Association for Psychological Science",
      "year": "2023",
      "doi": "10.1177/17456916221134490",
      "authors": "Timmons Adela C et al.",
      "keywords": "artificial intelligence; bias; fair aware; mental health equity",
      "mesh_terms": "Humans; Artificial Intelligence; Mental Health; Awareness; Bias; Technology",
      "pub_types": "Review; Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36490369/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "AI models biased against sensitive classes could reinforce and even perpetuate existing inequities if these models create legacies that differentially impact who is diagnosed and treated, and how effectively. The current article reviews the health-equity implications of applying AI to mental health problems, outlines state-of-the-art methods for assessing and mitigating algorithmic bias, and presents a call to action to guide the development of fair-aware AI in psychological science.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10250563"
    },
    {
      "pmid": "36527795",
      "title": "Artificial intelligence in cancer research and precision medicine: Applications, limitations and priorities to drive transformation in the delivery of equitable and unbiased care.",
      "abstract": "Artificial intelligence (AI) has experienced explosive growth in oncology and related specialties in recent years. The improved expertise in data capture, the increased capacity for data aggregation and analytic power, along with decreasing costs of genome sequencing and related biologic \"omics\", set the foundation and need for novel tools that can meaningfully process these data from multiple sources and of varying types. These advances provide value across biomedical discovery, diagnosis, prognosis, treatment, and prevention, in a multimodal fashion. However, while big data and AI tools have already revolutionized many fields, medicine has partially lagged due to its complexity and multi-dimensionality, leading to technical challenges in developing and validating solutions that generalize to diverse populations. Indeed, inner biases and miseducation of algorithms, in view of their implementation in daily clinical practice, are increasingly relevant concerns; critically, it is possible for AI to mirror the unconscious biases of the humans who generated these algorithms. Therefore, to avoid worsening existing health disparities, it is critical to employ a thoughtful, transparent, and inclusive approach that involves addressing bias in algorithm design and implementation along the cancer care continuum. In this review, a broad landscape of major applications of AI in cancer care is provided, with a focus on cancer research and precision medicine. Major challenges posed by the implementation of AI in the clinical setting will be discussed. Potentially feasible solutions for mitigating bias are provided, in the light of promoting cancer health equity.",
      "journal": "Cancer treatment reviews",
      "year": "2023",
      "doi": "10.1016/j.ctrv.2022.102498",
      "authors": "Corti Chiara et al.",
      "keywords": "Artificial intelligence; Bias; Decision support; Equity; Outcome prediction; Precision medicine",
      "mesh_terms": "Humans; Artificial Intelligence; Precision Medicine; Algorithms; Prognosis; Neoplasms",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36527795/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; Genomics/Genetics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Major challenges posed by the implementation of AI in the clinical setting will be discussed. Potentially feasible solutions for mitigating bias are provided, in the light of promoting cancer health equity.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36533881",
      "title": "Quantification of MR spectra by deep learning in an idealized setting: Investigation of forms of input, network architectures, optimization by ensembles of networks, and training bias.",
      "abstract": "PURPOSE: The aims of this work are (1) to explore deep learning (DL) architectures, spectroscopic input types, and learning designs toward optimal quantification in MR spectroscopy of simulated pathological spectra; and (2) to demonstrate accuracy and precision of DL predictions in view of inherent bias toward the training distribution. METHODS: Simulated 1D spectra and 2D spectrograms that mimic an extensive range of pathological in vivo conditions are used to train and test 24 different DL architectures. Active learning through altered training and testing data distributions is probed to optimize quantification performance. Ensembles of networks are explored to improve DL robustness and reduce the variance of estimates. A set of scores compares performances of DL predictions and traditional model fitting (MF). RESULTS: Ensembles of heterogeneous networks that combine 1D frequency-domain and 2D time-frequency domain spectrograms as input perform best. Dataset augmentation with active learning can improve performance, but gains are limited. MF is more accurate, although DL appears to be more precise at low SNR. However, this overall improved precision originates from a strong bias for cases with high uncertainty toward the dataset the network has been trained with, tending toward its average value. CONCLUSION: MF mostly performs better compared to the faster DL approach. Potential intrinsic biases on training sets are dangerous in a clinical context that requires the algorithm to be unbiased to outliers (i.e., pathological data). Active learning and ensemble of networks are good strategies to improve prediction performances. However, data quality (sufficient SNR) has proven as a bottleneck for adequate unbiased performance-like in the case of MF.",
      "journal": "Magnetic resonance in medicine",
      "year": "2023",
      "doi": "10.1002/mrm.29561",
      "authors": "Rizzo Rudy et al.",
      "keywords": "active learning; bias; deep learning; ensemble of networks; magnetic resonance spectroscopy; model fitting; quantification",
      "mesh_terms": "Deep Learning; Algorithms; Bias",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36533881/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Ensemble Methods",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Ensemble Methods",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: MF mostly performs better compared to the faster DL approach. Potential intrinsic biases on training sets are dangerous in a clinical context that requires the algorithm to be unbiased to outliers (i.e., pathological data). Active learning and ensemble of networks are good strategies to improve prediction performances.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36540977",
      "title": "FairPRS: adjusting for admixed populations in polygenic risk scores using invariant risk minimization.",
      "abstract": "Polygenic risk scores (PRS) are increasingly used to estimate the personal risk of a trait based on genetics. However, most genomic cohorts are of European populations, with a strong under-representation of non-European groups. Given that PRS poorly transport across racial groups, this has the potential to exacerbate health disparities if used in clinical care. Hence there is a need to generate PRS that perform comparably across ethnic groups. Borrowing from recent advancements in the domain adaption field of machine learning, we propose FairPRS - an Invariant Risk Minimization (IRM) approach for estimating fair PRS or debiasing a pre-computed PRS. We test our method on both a diverse set of synthetic data and real data from the UK Biobank. We show our method can create ancestry-invariant PRS distributions that are both racially unbiased and largely improve phenotype prediction. We hope that FairPRS will contribute to a fairer characterization of patients by genetics rather than by race.",
      "journal": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing",
      "year": "2023",
      "doi": "",
      "authors": "Machado Reyes Diego et al.",
      "keywords": "",
      "mesh_terms": "Humans; Genetic Predisposition to Disease; Genome-Wide Association Study; Computational Biology; Risk Factors; Phenotype; Multifactorial Inheritance",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36540977/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Data Augmentation",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We show our method can create ancestry-invariant PRS distributions that are both racially unbiased and largely improve phenotype prediction. We hope that FairPRS will contribute to a fairer characterization of patients by genetics rather than by race.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 2 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10804441"
    },
    {
      "pmid": "36541005",
      "title": "Algorithmic Fairness in the Roberts Court Era.",
      "abstract": "Scientists and policymakers alike have increasingly been interested in exploring ways to advance algorithmic fairness, recognizing not only the potential utility of algorithms in biomedical and digital health contexts but also that the unique challenges that algorithms-in a datafied culture such as the United States-pose for civil rights (including, but not limited to, privacy and nondiscrimination). In addition to the technical complexities, separation of powers issues are making the task even more daunting for policymakers-issues that might seem obscure to many scientists and technologists. While administrative agencies (such as the Federal Trade Commission) and legislators have been working to advance algorithmic fairness (in large part through comprehensive data privacy reform), recent judicial activism by the Roberts Court threaten to undermine those efforts. Scientists need to understand these legal developments so they can take appropriate action when contributing to a biomedical data ecosystem and designing, deploying, and maintaining algorithms for digital health. Here I highlight some of the recent actions taken by policymakers. I then review three recent Supreme Court cases (and foreshadow a fourth case) that illustrate the radical power grab by the Roberts Court, explaining for scientists how these drastic shifts in law will frustrate governmental approaches to algorithmic fairness and necessitate increased reliance by scientists on self-governance strategies to promote responsible and ethical practices.",
      "journal": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing",
      "year": "2023",
      "doi": "10.1145/3194770.3194776",
      "authors": "Wagner Jennifer K",
      "keywords": "",
      "mesh_terms": "United States; Humans; Ecosystem; Computational Biology; Privacy",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36541005/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Here I highlight some of the recent actions taken by policymakers. I then review three recent Supreme Court cases (and foreshadow a fourth case) that illustrate the radical power grab by the Roberts Court, explaining for scientists how these drastic shifts in law will frustrate governmental approaches to algorithmic fairness and necessitate increased reliance by scientists on self-governance strategies to promote responsible and ethical practices.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9782697"
    },
    {
      "pmid": "36611079",
      "title": "Fast, accurate, and racially unbiased pan-cancer tumor-only variant calling with tabular machine learning.",
      "abstract": "Accurately identifying somatic mutations is essential for precision oncology and crucial for calculating tumor-mutational burden (TMB), an important predictor of response to immunotherapy. For tumor-only variant calling (i.e., when the cancer biopsy but not the patient's normal tissue sample is sequenced), accurately distinguishing somatic mutations from germline variants is a challenging problem that, when unaddressed, results in unreliable, biased, and inflated TMB estimates. Here, we apply machine learning to the task of somatic vs germline classification in tumor-only solid tumor samples using TabNet, XGBoost, and LightGBM, three machine-learning models for tabular data. We constructed a training set for supervised classification using features derived exclusively from tumor-only variant calling and drawing somatic and germline truth labels from an independent pipeline using the patient-matched normal samples. All three trained models achieved state-of-the-art performance on two holdout test datasets: a TCGA dataset including sarcoma, breast adenocarcinoma, and endometrial carcinoma samples (AUC\u2009>\u200994%), and a metastatic melanoma dataset (AUC\u2009>\u200985%). Concordance between matched-normal and tumor-only TMB improves from R2\u2009=\u20090.006 to 0.71-0.76 with the addition of a machine-learning classifier, with LightGBM performing best. Notably, these machine-learning models generalize across cancer subtypes and capture kits with a call rate of 100%. We reproduce the recent finding that tumor-only TMB estimates for Black patients are extremely inflated relative to that of white patients due to the racial biases of germline databases. We show that our approach with XGBoost and LightGBM eliminates this significant racial bias in tumor-only variant calling.",
      "journal": "NPJ precision oncology",
      "year": "2023",
      "doi": "10.1038/s41698-022-00340-1",
      "authors": "McLaughlin R Tyler et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36611079/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "XGBoost/Gradient Boosting",
      "health_domain": "Dermatology; Oncology; Pathology; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We reproduce the recent finding that tumor-only TMB estimates for Black patients are extremely inflated relative to that of white patients due to the racial biases of germline databases. We show that our approach with XGBoost and LightGBM eliminates this significant racial bias in tumor-only variant calling.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9825621"
    },
    {
      "pmid": "36634514",
      "title": "Age-level bias correction in brain age prediction.",
      "abstract": "The predicted age difference (PAD) between an individual's predicted brain age and chronological age has been commonly viewed as a meaningful phenotype relating to aging and brain diseases. However, the systematic bias appears in the PAD achieved using machine learning methods. Recent studies have designed diverse bias correction methods to eliminate it for further downstream studies. Strikingly, here we demonstrate that bias still exists in the PAD of samples with the same age even after kind of correction. Therefore, current PAD may not be taken as a reliable phenotype and more investigations are needed to solve this fundamental defect. To this end, we propose an age-level bias correction method and demonstrate its efficacy in numerical experiments.",
      "journal": "NeuroImage. Clinical",
      "year": "2023",
      "doi": "10.1016/j.nicl.2023.103319",
      "authors": "Zhang Biao et al.",
      "keywords": "Age prediction; Bias correction; Human brain; MRI; Machine learning",
      "mesh_terms": "Magnetic Resonance Imaging; Brain; Machine Learning",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36634514/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Therefore, current PAD may not be taken as a reliable phenotype and more investigations are needed to solve this fundamental defect. To this end, we propose an age-level bias correction method and demonstrate its efficacy in numerical experiments.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9860514"
    },
    {
      "pmid": "36639799",
      "title": "Ethics and governance of trustworthy medical artificial intelligence.",
      "abstract": "BACKGROUND: The growing application of artificial intelligence (AI) in healthcare has brought technological breakthroughs to traditional diagnosis and treatment, but it is accompanied by many risks and challenges. These adverse effects are also seen as ethical issues and affect trustworthiness in medical AI and need to be managed through identification, prognosis and monitoring. METHODS: We adopted a multidisciplinary approach and summarized five subjects that influence the trustworthiness of medical AI: data quality, algorithmic bias, opacity, safety and security, and responsibility attribution, and discussed these factors from the perspectives of technology, law, and healthcare stakeholders and institutions. The ethical framework of ethical values-ethical principles-ethical norms is used to propose corresponding ethical governance countermeasures for trustworthy medical AI from the ethical, legal, and regulatory aspects. RESULTS: Medical data are primarily unstructured, lacking uniform and standardized annotation, and data quality will directly affect the quality of medical AI algorithm models. Algorithmic bias can affect AI clinical predictions and exacerbate health disparities. The opacity of algorithms affects patients' and doctors' trust in medical AI, and algorithmic errors or security vulnerabilities can pose significant risks and harm to patients. The involvement of medical AI in clinical practices may threaten doctors 'and patients' autonomy and dignity. When accidents occur with medical AI, the responsibility attribution is not clear. All these factors affect people's trust in medical AI. CONCLUSIONS: In order to make medical AI trustworthy, at the ethical level, the ethical value orientation of promoting human health should first and foremost be considered as the top-level design. At the legal level, current medical AI does not have moral status and humans remain the duty bearers. At the regulatory level, strengthening data quality management, improving algorithm transparency and traceability to reduce algorithm bias, and regulating and reviewing the whole process of the AI industry to control risks are proposed. It is also necessary to encourage multiple parties to discuss and assess AI risks and social impacts, and to strengthen international cooperation and communication.",
      "journal": "BMC medical informatics and decision making",
      "year": "2023",
      "doi": "10.1186/s12911-023-02103-9",
      "authors": "Zhang Jie et al.",
      "keywords": "Algorithms; Artificial intelligence; Data; Ethics; Governance; Healthcare; Regulation; Responsibility attribution",
      "mesh_terms": "Humans; Artificial Intelligence; Algorithms; Delivery of Health Care; Prognosis; Data Management",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36639799/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: In order to make medical AI trustworthy, at the ethical level, the ethical value orientation of promoting human health should first and foremost be considered as the top-level design. At the legal level, current medical AI does not have moral status and humans remain the duty bearers. At the regulatory level, strengthening data quality management, improving algorithm transparency and traceability to reduce algorithm bias, and regulating and reviewing the whole process of the AI indu...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9840286"
    },
    {
      "pmid": "36651834",
      "title": "Bias and Non-Diversity of Big Data in Artificial Intelligence: Focus on Retinal Diseases.",
      "abstract": "Artificial intelligence (AI) applications in healthcare will have a potentially far-reaching impact on patient care, however issues regarding algorithmic bias and fairness have recently surfaced. There is a recognized lack of diversity in the available ophthalmic datasets, with 45% of the global population having no readily accessible representative images, leading to potential misrepresentations of their unique anatomic features and ocular pathology. AI applications in retinal disease may show less accuracy with underrepresented populations that may further widen the gap of health inequality if left unaddressed. Beyond disease symptomatology, social determinants of health must be integrated into our current paradigms of disease understanding, with the goal of more personalized care. AI has the potential to decrease global healthcare inequality, but it will need to be based on a more diverse, transparent and responsible use of healthcare data.",
      "journal": "Seminars in ophthalmology",
      "year": "2023",
      "doi": "10.1080/08820538.2023.2168486",
      "authors": "Jacoba Cris Martin P et al.",
      "keywords": "Artificial Intelligence; Big Data; Diversity; Retina; Social Determinants of Health",
      "mesh_terms": "Humans; Big Data; Artificial Intelligence; Health Status Disparities; Retinal Diseases; Eye",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36651834/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Ophthalmology; Pathology",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population; Laboratory/Pathology",
      "key_findings": "Beyond disease symptomatology, social determinants of health must be integrated into our current paradigms of disease understanding, with the goal of more personalized care. AI has the potential to decrease global healthcare inequality, but it will need to be based on a more diverse, transparent and responsible use of healthcare data.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36653067",
      "title": "Evaluation of race/ethnicity-specific survival machine learning models for Hispanic and Black patients with breast cancer.",
      "abstract": "OBJECTIVES: Survival machine learning (ML) has been suggested as a useful approach for forecasting future events, but a growing concern exists that ML models have the potential to cause racial disparities through the data used to train them. This study aims to develop race/ethnicity-specific survival ML models for Hispanic and black women diagnosed with breast cancer to examine whether race/ethnicity-specific ML models outperform the general models trained with all races/ethnicity data. METHODS: We used the data from the US National Cancer Institute's Surveillance, Epidemiology and End Results programme registries. We developed the Hispanic-specific and black-specific models and compared them with the general model using the Cox proportional-hazards model, Gradient Boost Tree, survival tree and survival support vector machine. RESULTS: A total of 322\u2009348 female patients who had breast cancer diagnoses between 1 January 2000 and 31 December 2017 were identified. The race/ethnicity-specific models for Hispanic and black women consistently outperformed the general model when predicting the outcomes of specific race/ethnicity. DISCUSSION: Accurately predicting the survival outcome of a patient is critical in determining treatment options and providing appropriate cancer care. The high-performing models developed in this study can contribute to providing individualised oncology care and improving the survival outcome of black and Hispanic women. CONCLUSION: Predicting the individualised survival outcome of breast cancer can provide the evidence necessary for determining treatment options and high-quality, patient-centred cancer care delivery for under-represented populations. Also, the race/ethnicity-specific ML models can mitigate representation bias and contribute to addressing health disparities.",
      "journal": "BMJ health & care informatics",
      "year": "2023",
      "doi": "10.1136/bmjhci-2022-100666",
      "authors": "Park Jung In et al.",
      "keywords": "artificial intelligence; health equity; informatics; machine learning",
      "mesh_terms": "Humans; Female; Ethnicity; Breast Neoplasms; Hispanic or Latino; Black People; Proportional Hazards Models",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36653067/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Support Vector Machine",
      "health_domain": "Oncology; Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: Predicting the individualised survival outcome of breast cancer can provide the evidence necessary for determining treatment options and high-quality, patient-centred cancer care delivery for under-represented populations. Also, the race/ethnicity-specific ML models can mitigate representation bias and contribute to addressing health disparities.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9853120"
    },
    {
      "pmid": "36716733",
      "title": "Do not treat Bill Gates for prostate cancer! Algorithmic bias and causality in medical prediction.",
      "abstract": "",
      "journal": "BJU international",
      "year": "2023",
      "doi": "10.1111/bju.15951",
      "authors": "Vickers Andrew",
      "keywords": "",
      "mesh_terms": "Male; Humans; Bias; Prostatic Neoplasms",
      "pub_types": "Editorial; Research Support, N.I.H., Extramural; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36716733/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36717257",
      "title": "IARC-NCI workshop on an epidemiological toolkit to assess biases in human cancer studies for hazard identification: beyond the algorithm.",
      "abstract": "",
      "journal": "Occupational and environmental medicine",
      "year": "2023",
      "doi": "10.1136/oemed-2022-108724",
      "authors": "Schubauer-Berigan Mary K et al.",
      "keywords": "Environment; Epidemiology; Occupational Health; Statistics",
      "mesh_terms": "Humans; Neoplasms; Carcinogens; Bias; Algorithms",
      "pub_types": "Editorial; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36717257/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36719717",
      "title": "Black and Latinx Primary Caregiver Considerations for Developing and Implementing a Machine Learning-Based Model for Detecting Child Abuse and Neglect With Implications for Racial Bias Reduction: Qualitative Interview Study With Primary Caregivers.",
      "abstract": "BACKGROUND: Child abuse and neglect, once viewed as a social problem, is now an epidemic. Moreover, health providers agree that existing stereotypes may link racial and social class issues to child abuse. The broad adoption of electronic health records (EHRs) in clinical settings offers a new avenue for addressing this epidemic. To reduce racial bias and improve the development, implementation, and outcomes of machine learning (ML)-based models that use EHR data, it is crucial to involve marginalized members of the community in the process. OBJECTIVE: This study elicited Black and Latinx primary caregivers' viewpoints regarding child abuse and neglect while living in underserved communities to highlight considerations for designing an ML-based model for detecting child abuse and neglect in emergency departments (EDs) with implications for racial bias reduction and future interventions. METHODS: We conducted a qualitative study using in-depth interviews with 20 Black and Latinx primary caregivers whose children were cared for at a single pediatric tertiary-care ED to gain insights about child abuse and neglect and their experiences with health providers. RESULTS: Three central themes were developed in the coding process: (1) primary caregivers' perspectives on the definition of child abuse and neglect, (2) primary caregivers' experiences with health providers and medical documentation, and (3) primary caregivers' perceptions of child protective services. CONCLUSIONS: Our findings highlight essential considerations from primary caregivers for developing an ML-based model for detecting child abuse and neglect in ED settings. This includes how to define child abuse and neglect from a primary caregiver lens. Miscommunication between patients and health providers can potentially lead to a misdiagnosis, and therefore, have a negative impact on medical documentation. Additionally, the outcome and application of the ML-based models for detecting abuse and neglect may cause additional harm than expected to the community. Further research is needed to validate these findings and integrate them into creating an ML-based model.",
      "journal": "JMIR formative research",
      "year": "2023",
      "doi": "10.2196/40194",
      "authors": "Landau Aviv Y et al.",
      "keywords": "abuse; child; child abuse and neglect; community; development; electronic health records; epidemic; implementation; machine learning; machine learning\u2013based risk models; model; neglect; pediatric emergency departments; primary caregivers",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36719717/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "Emergency Medicine; EHR/Health Informatics; Primary Care; Pediatrics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Emergency Department; Primary Care/Outpatient; Public Health/Population; Safety-Net/Underserved",
      "key_findings": "CONCLUSIONS: Our findings highlight essential considerations from primary caregivers for developing an ML-based model for detecting child abuse and neglect in ED settings. This includes how to define child abuse and neglect from a primary caregiver lens. Miscommunication between patients and health providers can potentially lead to a misdiagnosis, and therefore, have a negative impact on medical documentation.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9929722"
    },
    {
      "pmid": "36754076",
      "title": "How Should Clinicians and Health Care Organizations Promote Equity in Child Abuse and Neglect Suspicion, Evaluation, and Reporting?",
      "abstract": "Victims of child abuse and neglect come from every racial, ethnic, and socioeconomic background, yet clinical evaluation, reporting to child protective services, and responses to reports inequitably harm Black children and malign families of color. Racial bias and inequity in suspicion, reporting, and substantiation of abuse and neglect and in services offered and delivered, foster care placement, and criminal prosecution are widely documented. In response, clinicians and health care organizations should promote equity by educating clinicians about racial bias, standardizing evaluation using clinical decision support tools, and working with policy makers to support prevention services. If we decide that it is ethically justifiable for clinicians to err on the side of overreporting, we must ensure fair distribution of associated benefits and harms among all children and families.",
      "journal": "AMA journal of ethics",
      "year": "2023",
      "doi": "10.1001/amajethics.2023.133",
      "authors": "Lane Wendy G et al.",
      "keywords": "",
      "mesh_terms": "Child; Humans; Child Abuse; Child Welfare; Racial Groups; Delivery of Health Care",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36754076/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Decision Support; Generative AI",
      "health_domain": "Pediatrics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "In response, clinicians and health care organizations should promote equity by educating clinicians about racial bias, standardizing evaluation using clinical decision support tools, and working with policy makers to support prevention services. If we decide that it is ethically justifiable for clinicians to err on the side of overreporting, we must ensure fair distribution of associated benefits and harms among all children and families.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36768092",
      "title": "Evaluation of AIML + HDR-A Course to Enhance Data Science Workforce Capacity for Hispanic Biomedical Researchers.",
      "abstract": "Artificial intelligence (AI) and machine learning (ML) facilitate the creation of revolutionary medical techniques. Unfortunately, biases in current AI and ML approaches are perpetuating minority health inequity. One of the strategies to solve this problem is training a diverse workforce. For this reason, we created the course \"Artificial Intelligence and Machine Learning applied to Health Disparities Research (AIML + HDR)\" which applied general Data Science (DS) approaches to health disparities research with an emphasis on Hispanic populations. Some technical topics covered included the Jupyter Notebook Framework, coding with R and Python to manipulate data, and ML libraries to create predictive models. Some health disparities topics covered included Electronic Health Records, Social Determinants of Health, and Bias in Data. As a result, the course was taught to 34 selected Hispanic participants and evaluated by a survey on a Likert scale (0-4). The surveys showed high satisfaction (more than 80% of participants agreed) regarding the course organization, activities, and covered topics. The students strongly agreed that the activities were relevant to the course and promoted their learning (3.71 \u00b1 0.21). The students strongly agreed that the course was helpful for their professional development (3.76 \u00b1 0.18). The open question was quantitatively analyzed and showed that seventy-five percent of the comments received from the participants confirmed their great satisfaction.",
      "journal": "International journal of environmental research and public health",
      "year": "2023",
      "doi": "10.3390/ijerph20032726",
      "authors": "Heredia-Negron Frances et al.",
      "keywords": "artificial intelligence; data science; health disparities; hispanic biomedical research; machine learning",
      "mesh_terms": "Humans; Artificial Intelligence; Data Science; Hispanic or Latino; Machine Learning; Workforce; Biomedical Research",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36768092/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Clinical Prediction Model; Generative AI",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "The students strongly agreed that the course was helpful for their professional development (3.76 \u00b1 0.18). The open question was quantitatively analyzed and showed that seventy-five percent of the comments received from the participants confirmed their great satisfaction.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9914971"
    },
    {
      "pmid": "36865183",
      "title": "Classification performance bias between training and test sets in a limited mammography dataset.",
      "abstract": "OBJECTIVES: To assess the performance bias caused by sampling data into training and test sets in a mammography radiomics study. METHODS: Mammograms from 700 women were used to study upstaging of ductal carcinoma in situ. The dataset was repeatedly shuffled and split into training (n=400) and test cases (n=300) forty times. For each split, cross-validation was used for training, followed by an assessment of the test set. Logistic regression with regularization and support vector machine were used as the machine learning classifiers. For each split and classifier type, multiple models were created based on radiomics and/or clinical features. RESULTS: Area under the curve (AUC) performances varied considerably across the different data splits (e.g., radiomics regression model: train 0.58-0.70, test 0.59-0.73). Performances for regression models showed a tradeoff where better training led to worse testing and vice versa. Cross-validation over all cases reduced this variability, but required samples of 500+ cases to yield representative estimates of performance. CONCLUSIONS: In medical imaging, clinical datasets are often limited to relatively small size. Models built from different training sets may not be representative of the whole dataset. Depending on the selected data split and model, performance bias could lead to inappropriate conclusions that might influence the clinical significance of the findings. Optimal strategies for test set selection should be developed to ensure study conclusions are appropriate.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2023",
      "doi": "10.1101/2023.02.15.23285985",
      "authors": "Hou Rui et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36865183/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Logistic Regression; Support Vector Machine; Computer Vision/Imaging AI; Regression",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Regularization",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: In medical imaging, clinical datasets are often limited to relatively small size. Models built from different training sets may not be representative of the whole dataset. Depending on the selected data split and model, performance bias could lead to inappropriate conclusions that might influence the clinical significance of the findings.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9980247"
    },
    {
      "pmid": "36939724",
      "title": "Effects of Racial Bias in Pulse Oximetry on Children and How to Address Algorithmic Bias in Clinical Medicine.",
      "abstract": "",
      "journal": "JAMA pediatrics",
      "year": "2023",
      "doi": "10.1001/jamapediatrics.2023.0077",
      "authors": "Gray Keyaria D et al.",
      "keywords": "",
      "mesh_terms": "Child; Humans; Racism; Oximetry; Oxygen; Bias; Clinical Medicine",
      "pub_types": "Editorial; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36939724/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Pediatrics; Wearables/Remote Monitoring",
      "bias_axes": "Race/Ethnicity",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36947701",
      "title": "Intelligent decision support in medical triage: are people robust to biased advice?",
      "abstract": "BACKGROUND: Intelligent artificial agents ('agents') have emerged in various domains of human society (healthcare, legal, social). Since using intelligent agents can lead to biases, a common proposed solution is to keep the human in the loop. Will this be enough to ensure unbiased decision making? METHODS: To address this question, an experimental testbed was developed in which a human participant and an agent collaboratively conduct triage on patients during a pandemic crisis. The agent uses data to support the human by providing advice and extra information about the patients. In one condition, the agent provided sound advice; the agent in the other condition gave biased advice. The research question was whether participants neutralized bias from the biased artificial agent. RESULTS: Although it was an exploratory study, the data suggest that human participants may not be sufficiently in control to correct the agent's bias. CONCLUSIONS: This research shows how important it is to design and test for human control in concrete human-machine collaboration contexts. It suggests that insufficient human control can potentially result in people being unable to detect biases in machines and thus unable to prevent machine biases from affecting decisions.",
      "journal": "Journal of public health (Oxford, England)",
      "year": "2023",
      "doi": "10.1093/pubmed/fdad005",
      "authors": "van der Stigchel Birgit et al.",
      "keywords": "emergency care; ethics; health intelligence",
      "mesh_terms": "Humans; Triage; Decision Support Systems, Clinical; Artificial Intelligence",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36947701/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Emergency Medicine",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: This research shows how important it is to design and test for human control in concrete human-machine collaboration contexts. It suggests that insufficient human control can potentially result in people being unable to detect biases in machines and thus unable to prevent machine biases from affecting decisions.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10470333"
    },
    {
      "pmid": "36964821",
      "title": "Brief Review: Racial and Ethnic Disparities in Cardiovascular Care with a Focus on Congenital Heart Disease and Precision Medicine.",
      "abstract": "PURPOSE OF REVIEW: This is a brief review about racial and ethnic disparities in healthcare with focused attention to less frequently covered areas in the literature such as adult congenital heart disease, artificial intelligence, and precision medicine. Although diverse racial and ethnic populations such as Black and Hispanic groups are at an increased risk for CHD and have worse related outcomes, they are woefully underrepresented in large clinical trials. Additionally, although artificial intelligence and its application to precision medicine are touted as a means to individualize cardiovascular treatment and eliminate racial and ethnic bias, serious concerns exist about insufficient and inadequate available information from diverse racial and ethnic groups to facilitate accurate care. This review discusses relevant data to the aforementioned topics and the associated nuances. RECENT FINDINGS: Recent studies have shown that racial and ethnic minorities have increased morbidity and mortality related to congenital heart disease. Artificial intelligence, one of the chief methods used in precision medicine, can exacerbate racial and ethnic bias especially if inappropriate algorithms are utilized from populations that lack racial and ethnic diversity. Dedicated resources are needed to engage diverse populations to facilitate participation in clinical and population-based studies to eliminate racial and ethnic healthcare disparities in adult congenital disease and the utilization of artificial intelligence to improve health outcomes in all populations.",
      "journal": "Current atherosclerosis reports",
      "year": "2023",
      "doi": "10.1007/s11883-023-01093-3",
      "authors": "Bayne Joseph et al.",
      "keywords": "Adult congenital heart disease; Cardiovascular disease; Precision medicine; Racial and ethnic disparities; Social determinants of health",
      "mesh_terms": "Humans; Adult; United States; Precision Medicine; Artificial Intelligence; Heart Defects, Congenital; Ethnicity; Healthcare Disparities",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36964821/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population; Clinical Trial",
      "key_findings": "FINDINGS: Recent studies have shown that racial and ethnic minorities have increased morbidity and mortality related to congenital heart disease. Artificial intelligence, one of the chief methods used in precision medicine, can exacerbate racial and ethnic bias especially if inappropriate algorithms are utilized from populations that lack racial and ethnic diversity. Dedicated resources are needed to engage diverse populations to facilitate participation in clinical and population-based studies ...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10039345"
    },
    {
      "pmid": "36969013",
      "title": "Detection of missed deaths in cancer registry data to reduce bias in long-term survival estimation.",
      "abstract": "BACKGROUND: Population-based cancer survival estimates can provide insight into the real-world impacts of healthcare interventions and preventive services. However, estimation of survival rates obtained from population-based cancer registries can be biased due to missed incidence or incomplete vital status data. Long-term survival estimates in particular are prone to overestimation, since the proportion of deaths that are missed, for example through unregistered emigration, increases with follow-up time. This also applies to registry-based long-term prevalence estimates. The aim of this report is to introduce a method to detect missed deaths within cancer registry data such that long-term survival of cancer patients does not exceed survival in the general population. METHODS: We analyzed data from 15 German epidemiologic cancer registries covering the years 1970-2016 and from Surveillance, Epidemiology, and End Results (SEER)-18 registries covering 1975-2015. The method is based on comparing survival times until exit (death or follow-up end) and ages at exit between deceased patients and surviving patients, stratified by diagnosis group, sex, age group and stage. Deceased patients with both follow-up time and age at exit in the highest percentile were regarded as outliers and used to fit a logistic regression. The regression was then used to classify each surviving patient as a survivor or a missed death. The procedure was repeated for lower percentile thresholds regarding deceased persons until long-term survival rates no longer exceeded the survival rates in the general population. RESULTS: For the German cancer registry data, 0.9% of total deaths were classified as having been missed. Excluding these missed deaths reduced 20-year relative survival estimates for all cancers combined from 140% to 51%. For the whites in SEER data, classified missed deaths amounted to 0.02% of total deaths, resulting in 0.4 percent points lower 20-year relative survival rate for all cancers combined. CONCLUSION: The method described here classified a relatively small proportion of missed deaths yet reduced long-term survival estimates to more plausible levels. The effects of missed deaths should be considered when calculating long-term survival or prevalence estimates.",
      "journal": "Frontiers in oncology",
      "year": "2023",
      "doi": "10.3389/fonc.2023.1088657",
      "authors": "Dahm Stefan et al.",
      "keywords": "cancer registry data; classification algorithm; long-term survival; missed deaths; relative survival",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36969013/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Logistic Regression",
      "health_domain": "Oncology; ICU/Critical Care; Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: The method described here classified a relatively small proportion of missed deaths yet reduced long-term survival estimates to more plausible levels. The effects of missed deaths should be considered when calculating long-term survival or prevalence estimates.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10034313"
    },
    {
      "pmid": "36997578",
      "title": "A multi-institutional study using artificial intelligence to provide reliable and fair feedback to surgeons.",
      "abstract": "BACKGROUND: Surgeons who receive reliable feedback on their performance quickly master the skills necessary for surgery. Such performance-based feedback can be provided by a recently-developed artificial intelligence (AI) system that assesses a surgeon's skills based on a surgical video while simultaneously highlighting aspects of the video most pertinent to the assessment. However, it remains an open question whether these highlights, or explanations, are equally reliable for all surgeons. METHODS: Here, we systematically quantify the reliability of AI-based explanations on surgical videos from three hospitals across two continents by comparing them to explanations generated by humans experts. To improve the reliability of AI-based explanations, we propose the strategy of training with explanations -TWIX -which uses human explanations as supervision to explicitly teach an AI system to highlight important video frames. RESULTS: We show that while AI-based explanations often align with human explanations, they are not equally reliable for different sub-cohorts of surgeons (e.g., novices vs. experts), a phenomenon we refer to as an explanation bias. We also show that TWIX enhances the reliability of AI-based explanations, mitigates the explanation bias, and improves the performance of AI systems across hospitals. These findings extend to a training environment where medical students can be provided with feedback today. CONCLUSIONS: Our study informs the impending implementation of AI-augmented surgical training and surgeon credentialing programs, and contributes to the safe and fair democratization of surgery. Surgeons aim to master skills necessary for surgery. One such skill is suturing which involves connecting objects together through a series of stitches. Mastering these surgical skills can be improved by providing surgeons with feedback on the quality of their performance. However, such feedback is often absent from surgical practice. Although performance-based feedback can be provided, in theory, by recently-developed artificial intelligence (AI) systems that use a computational model to assess a surgeon\u2019s skill, the reliability of this feedback remains unknown. Here, we compare AI-based feedback to that provided by human experts and demonstrate that they often overlap with one another. We also show that explicitly teaching an AI system to align with human feedback further improves the reliability of AI-based feedback on new videos of surgery. Our findings outline the potential of AI systems to support the training of surgeons by providing feedback that is reliable and focused on a particular skill, and guide programs that give surgeons qualifications by complementing skill assessments with explanations that increase the trustworthiness of such assessments.",
      "journal": "Communications medicine",
      "year": "2023",
      "doi": "10.1038/s43856-023-00263-3",
      "authors": "Kiyasseh Dani et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36997578/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Surgery",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; ICU",
      "key_findings": "CONCLUSIONS: Our study informs the impending implementation of AI-augmented surgical training and surgeon credentialing programs, and contributes to the safe and fair democratization of surgery. Surgeons aim to master skills necessary for surgery. One such skill is suturing which involves connecting objects together through a series of stitches.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10063640"
    },
    {
      "pmid": "36997714",
      "title": "AI 'fairness' research held back by lack of diversity.",
      "abstract": "",
      "journal": "Nature",
      "year": "2023",
      "doi": "10.1038/d41586-023-00935-z",
      "authors": "Wong Carissa",
      "keywords": "Health care; Machine learning; Scientific community",
      "mesh_terms": "",
      "pub_types": "News",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36997714/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36998311",
      "title": "A fair and interpretable network for clinical risk prediction: a regularized multi-view multi-task learning approach.",
      "abstract": "In healthcare domain, complication risk profiling which can be seen as multiple clinical risk prediction tasks is challenging due to the complex interaction between heterogeneous clinical entities. With the availability of real-world data, many deep learning methods are proposed for complication risk profiling. However, the existing methods face three open challenges. First, they leverage clinical data from a single view and then lead to suboptimal models. Second, most existing methods lack an effective mechanism to interpret predictions. Third, models learned from clinical data may have inherent pre-existing biases and exhibit discrimination against certain social groups. We then propose a multi-view multi-task network (MuViTaNet) to tackle these issues. MuViTaNet complements patient representation by using a multi-view encoder to exploit more information. Moreover, it uses a multi-task learning to generate more generalized representations using both labeled and unlabeled datasets. Last, a fairness variant (F-MuViTaNet) is proposed to mitigate the unfairness issues and promote healthcare equity. The experiments show that MuViTaNet outperforms existing methods for cardiac complication profiling. Its architecture also provides an effective mechanism for interpreting the predictions, which helps clinicians discover the underlying mechanism triggering the complication onsets. F-MuViTaNet can also effectively mitigate the unfairness with only negligible impact on accuracy.",
      "journal": "Knowledge and information systems",
      "year": "2023",
      "doi": "10.1007/s10115-022-01813-2",
      "authors": "Pham Thai-Hoang et al.",
      "keywords": "Attention; Complication risk profiling; Contrastive learning; Equal opportunity; Fairness; Multi-task; Multi-view; Regularization",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36998311/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Clinical Prediction Model",
      "health_domain": "Cardiology; Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Multi-task Learning",
      "clinical_setting": "Not specified",
      "key_findings": "Its architecture also provides an effective mechanism for interpreting the predictions, which helps clinicians discover the underlying mechanism triggering the complication onsets. F-MuViTaNet can also effectively mitigate the unfairness with only negligible impact on accuracy.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10046420"
    },
    {
      "pmid": "37018335",
      "title": "Proportionally Fair Hospital Collaborations in Federated Learning of Histopathology Images.",
      "abstract": "Medical centers and healthcare providers have concerns and hence restrictions around sharing data with external collaborators. Federated learning, as a privacy-preserving method, involves learning a site-independent model without having direct access to patient-sensitive data in a distributed collaborative fashion. The federated approach relies on decentralized data distribution from various hospitals and clinics. The collaboratively learned global model is supposed to have acceptable performance for the individual sites. However, existing methods focus on minimizing the average of the aggregated loss functions, leading to a biased model that performs perfectly for some hospitals while exhibiting undesirable performance for other sites. In this paper, we improve model \"fairness\" among participating hospitals by proposing a novel federated learning scheme called Proportionally Fair Federated Learning, short Prop-FFL. Prop-FFL is based on a novel optimization objective function to decrease the performance variations among participating hospitals. This function encourages a fair model, providing us with more uniform performance across participating hospitals. We validate the proposed Prop-FFL on two histopathology datasets as well as two general datasets to shed light on its inherent capabilities. The experimental results suggest promising performance in terms of learning speed, accuracy, and fairness.",
      "journal": "IEEE transactions on medical imaging",
      "year": "2023",
      "doi": "10.1109/TMI.2023.3234450",
      "authors": "Hosseini S Maryam et al.",
      "keywords": "",
      "mesh_terms": "Humans; Hospitals; Supervised Machine Learning; Pathology",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37018335/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Federated Learning",
      "health_domain": "Pathology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Federated Learning",
      "clinical_setting": "Hospital/Inpatient; Laboratory/Pathology",
      "key_findings": "We validate the proposed Prop-FFL on two histopathology datasets as well as two general datasets to shed light on its inherent capabilities. The experimental results suggest promising performance in terms of learning speed, accuracy, and fairness.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37027665",
      "title": "Hierarchical Bias Mitigation for Semi-Supervised Medical Image Classification.",
      "abstract": "Semi-supervised learning (SSL) has demonstrated remarkable advances on medical image classification, by harvesting beneficial knowledge from abundant unlabeled samples. The pseudo labeling dominates current SSL approaches, however, it suffers from intrinsic biases within the process. In this paper, we retrospect the pseudo labeling and identify three hierarchical biases: perception bias, selection bias and confirmation bias, at feature extraction, pseudo label selection and momentum optimization stages, respectively. In this regard, we propose a HierArchical BIas miTigation (HABIT) framework to amend these biases, which consists of three customized modules including Mutual Reconciliation Network (MRNet), Recalibrated Feature Compensation (RFC) and Consistency-aware Momentum Heredity (CMH). Firstly, in the feature extraction, MRNet is devised to jointly utilize convolution and permutator-based paths with a mutual information transfer module to exchanges features and reconcile spatial perception bias for better representations. To address pseudo label selection bias, RFC adaptively recalibrates the strong and weak augmented distributions to be a rational discrepancy and augments features for minority categories to achieve the balanced training. Finally, in the momentum optimization stage, in order to reduce the confirmation bias, CMH models the consistency among different sample augmentations into network updating process to improve the dependability of the model. Extensive experiments on three semi-supervised medical image classification datasets demonstrate that HABIT mitigates three biases and achieves state-of-the-art performance. Our codes are available at https://github.com/CityU-AIM-Group/HABIT.",
      "journal": "IEEE transactions on medical imaging",
      "year": "2023",
      "doi": "10.1109/TMI.2023.3247440",
      "authors": "Yang Qiushi et al.",
      "keywords": "",
      "mesh_terms": "Bias; Motion; Supervised Machine Learning",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37027665/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Extensive experiments on three semi-supervised medical image classification datasets demonstrate that HABIT mitigates three biases and achieves state-of-the-art performance. Our codes are available at https://github.com/CityU-AIM-Group/HABIT.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37081528",
      "title": "Higher performance for women than men in MRI-based Alzheimer's disease detection.",
      "abstract": "INTRODUCTION: Although machine learning classifiers have been frequently used to detect Alzheimer's disease (AD) based on structural brain MRI data, potential bias with respect to sex and age has not yet been addressed. Here, we examine a state-of-the-art AD classifier for potential sex and age bias even in the case of balanced training data. METHODS: Based on an age- and sex-balanced cohort of 432 subjects (306 healthy controls, 126 subjects with AD) extracted from the ADNI data base, we trained a convolutional neural network to detect AD in MRI brain scans and performed ten different random training-validation-test splits to increase robustness of the results. Classifier decisions for single subjects were explained using layer-wise relevance propagation. RESULTS: The classifier performed significantly better for women (balanced accuracy [Formula: see text]) than for men ([Formula: see text]). No significant differences were found in clinical AD scores, ruling out a disparity in disease severity as a cause for the performance difference. Analysis of the explanations revealed a larger variance in regional brain areas for male subjects compared to female subjects. DISCUSSION: The identified sex differences cannot be attributed to an imbalanced training dataset and therefore point to the importance of examining and reporting classifier performance across population subgroups to increase transparency and algorithmic fairness. Collecting more data especially among underrepresented subgroups and balancing the dataset are important but do not always guarantee a fair outcome.",
      "journal": "Alzheimer's research & therapy",
      "year": "2023",
      "doi": "10.1186/s13195-023-01225-6",
      "authors": "Klingenberg Malte et al.",
      "keywords": "Alzheimer\u2019s disease; Bias; Deep learning; MRI; Sex",
      "mesh_terms": "Humans; Male; Female; Alzheimer Disease; Cognitive Dysfunction; Magnetic Resonance Imaging; Neuroimaging; Machine Learning",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37081528/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Radiology/Medical Imaging; Neurology",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "RESULTS: The classifier performed significantly better for women (balanced accuracy [Formula: see text]) than for men ([Formula: see text]). No significant differences were found in clinical AD scores, ruling out a disparity in disease severity as a cause for the performance difference. Analysis of the explanations revealed a larger variance in regional brain areas for male subjects compared to female subjects.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10116672"
    },
    {
      "pmid": "37099916",
      "title": "Automatic sleep staging for the young and the old - Evaluating age bias in deep learning.",
      "abstract": "BACKGROUND: Various deep-learning systems have been proposed for automated sleep staging. Still, the significance of age-specific underrepresentation in training data and the resulting errors in clinically used sleep metrics are unknown. METHODS: We adopted XSleepNet2, a deep neural network for automated sleep staging, to train and test models using polysomnograms of 1232 children (7.0\u00a0\u00b1\u00a01.4 years) and 3757 adults (56.9\u00a0\u00b1\u00a019.4 years) and 2788 older adults (mean 80.7\u00a0\u00b1\u00a04.2 years). We developed four separate sleep stage classifiers using exclusively pediatric (P), adult (A), older adults (O) as well as PSG from mixed cohorts: pediatric, adult, and older adult (PAO). Results were compared against an alternative sleep stager (DeepSleepNet) for validation purposes. RESULTS: When pediatric PSG was classified by XSleepNet2 exclusively trained on pediatric PSG, the overall accuracy was 88.9%, dropping to 78.9% when subjected to a system trained exclusively on adult PSG. Errors performed by the system staging PSG of older people were comparably lower. However, all systems produced significant errors in clinical markers when considering individual PSG. Results obtained with DeepSleepNet showed similar patterns. CONCLUSION: Underrepresentation of age groups, in particular children, can significantly lower the performance of automatic deep-learning sleep stagers. In general, automated sleep stagers may behave unexpectedly, limiting clinical use. Future evaluation of automated systems must pay attention to PSG-level performance and overall accuracy.",
      "journal": "Sleep medicine",
      "year": "2023",
      "doi": "10.1016/j.sleep.2023.04.002",
      "authors": "Baumert Mathias et al.",
      "keywords": "Sleep staging; machine learning; polysomnography",
      "mesh_terms": "Humans; Child; Aged; Deep Learning; Sleep Stages; Sleep; Neural Networks, Computer; Polysomnography",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37099916/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "ICU/Critical Care; Pediatrics",
      "bias_axes": "Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: Underrepresentation of age groups, in particular children, can significantly lower the performance of automatic deep-learning sleep stagers. In general, automated sleep stagers may behave unexpectedly, limiting clinical use. Future evaluation of automated systems must pay attention to PSG-level performance and overall accuracy.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37140902",
      "title": "Association of Biomarker-Based Artificial Intelligence With Risk of Racial Bias in Retinal Images.",
      "abstract": "IMPORTANCE: Although race is a social construct, it is associated with variations in skin and retinal pigmentation. Image-based medical artificial intelligence (AI) algorithms that use images of these organs have the potential to learn features associated with self-reported race (SRR), which increases the risk of racially biased performance in diagnostic tasks; understanding whether this information can be removed, without affecting the performance of AI algorithms, is critical in reducing the risk of racial bias in medical AI. OBJECTIVE: To evaluate whether converting color fundus photographs to retinal vessel maps (RVMs) of infants screened for retinopathy of prematurity (ROP) removes the risk for racial bias. DESIGN, SETTING, AND PARTICIPANTS: The retinal fundus images (RFIs) of neonates with parent-reported Black or White race were collected for this study. A u-net, a convolutional neural network (CNN) that provides precise segmentation for biomedical images, was used to segment the major arteries and veins in RFIs into grayscale RVMs, which were subsequently thresholded, binarized, and/or skeletonized. CNNs were trained with patients' SRR labels on color RFIs, raw RVMs, and thresholded, binarized, or skeletonized RVMs. Study data were analyzed from July 1 to September 28, 2021. MAIN OUTCOMES AND MEASURES: Area under the precision-recall curve (AUC-PR) and area under the receiver operating characteristic curve (AUROC) at both the image and eye level for classification of SRR. RESULTS: A total of 4095 RFIs were collected from 245 neonates with parent-reported Black (94 [38.4%]; mean [SD] age, 27.2 [2.3] weeks; 55 majority sex [58.5%]) or White (151 [61.6%]; mean [SD] age, 27.6 [2.3] weeks, 80 majority sex [53.0%]) race. CNNs inferred SRR from RFIs nearly perfectly (image-level AUC-PR, 0.999; 95% CI, 0.999-1.000; infant-level AUC-PR, 1.000; 95% CI, 0.999-1.000). Raw RVMs were nearly as informative as color RFIs (image-level AUC-PR, 0.938; 95% CI, 0.926-0.950; infant-level AUC-PR, 0.995; 95% CI, 0.992-0.998). Ultimately, CNNs were able to learn whether RFIs or RVMs were from Black or White infants regardless of whether images contained color, vessel segmentation brightness differences were nullified, or vessel segmentation widths were uniform. CONCLUSIONS AND RELEVANCE: Results of this diagnostic study suggest that it can be very challenging to remove information relevant to SRR from fundus photographs. As a result, AI algorithms trained on fundus photographs have the potential for biased performance in practice, even if based on biomarkers rather than raw images. Regardless of the methodology used for training AI, evaluating performance in relevant subpopulations is critical.",
      "journal": "JAMA ophthalmology",
      "year": "2023",
      "doi": "10.1001/jamaophthalmol.2023.1310",
      "authors": "Coyner Aaron S et al.",
      "keywords": "",
      "mesh_terms": "Infant, Newborn; Infant; Humans; Adult; Artificial Intelligence; Racism; Retina; Neural Networks, Computer; Algorithms",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37140902/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Neural Network; Generative AI",
      "health_domain": "Ophthalmology; Pediatrics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment; Subgroup Analysis",
      "clinical_setting": "Public Health/Population",
      "key_findings": "RESULTS: A total of 4095 RFIs were collected from 245 neonates with parent-reported Black (94 [38.4%]; mean [SD] age, 27.2 [2.3] weeks; 55 majority sex [58.5%]) or White (151 [61.6%]; mean [SD] age, 27.6 [2.3] weeks, 80 majority sex [53.0%]) race. CNNs inferred SRR from RFIs nearly perfectly (image-level AUC-PR, 0.999; 95% CI, 0.999-1.000; infant-level AUC-PR, 1.000; 95% CI, 0.999-1.000). Raw RVMs were nearly as informative as color RFIs (image-level AUC-PR, 0.938; 95% CI, 0.926-0.950; infant-le...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10160994"
    },
    {
      "pmid": "37198691",
      "title": "Biased data, biased AI: deep networks predict the acquisition site of TCGA images.",
      "abstract": "BACKGROUND: Deep learning models applied to healthcare applications including digital pathology have been increasing their scope and importance in recent years. Many of these models have been trained on The Cancer Genome Atlas (TCGA) atlas of digital images, or use it as a validation source. One crucial factor that seems to have been widely ignored is the internal bias that originates from the institutions that contributed WSIs to the TCGA dataset, and its effects on models trained on this dataset. METHODS: 8,579 paraffin-embedded, hematoxylin and eosin stained, digital slides were selected from the TCGA dataset. More than 140 medical institutions (acquisition sites) contributed to this dataset. Two deep neural networks (DenseNet121 and KimiaNet were used to extract deep features at 20\u00d7 magnification. DenseNet was pre-trained on non-medical objects. KimiaNet has the same structure but trained for cancer type classification on TCGA images. The extracted deep features were later used to detect each slide's acquisition site, and also for slide representation in image search. RESULTS: DenseNet's deep features could distinguish acquisition sites with 70% accuracy whereas KimiaNet's deep features could reveal acquisition sites with more than 86% accuracy. These findings suggest that there are acquisition site specific patterns that could be picked up by deep neural networks. It has also been shown that these medically irrelevant patterns can interfere with other applications of deep learning in digital pathology, namely image search. This study shows that there are acquisition site specific patterns that can be used to identify tissue acquisition sites without any explicit training. Furthermore, it was observed that a model trained for cancer subtype classification has exploited such medically irrelevant patterns to classify cancer types. Digital scanner configuration and noise, tissue stain variation and artifacts, and source site patient demographics are among factors that likely account for the observed bias. Therefore, researchers should be cautious of such bias when using histopathology datasets for developing and training deep networks.",
      "journal": "Diagnostic pathology",
      "year": "2023",
      "doi": "10.1186/s13000-023-01355-3",
      "authors": "Dehkharghanian Taher et al.",
      "keywords": "AI bias; AI ethics; Cancer; Deep Learning; Digital pathology; TCGA",
      "mesh_terms": "Humans; Neoplasms; Neural Networks, Computer; Coloring Agents; Hematoxylin; Eosine Yellowish-(YS)",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37198691/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Oncology; Pathology; Genomics/Genetics",
      "bias_axes": "Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "RESULTS: DenseNet's deep features could distinguish acquisition sites with 70% accuracy whereas KimiaNet's deep features could reveal acquisition sites with more than 86% accuracy. These findings suggest that there are acquisition site specific patterns that could be picked up by deep neural networks. It has also been shown that these medically irrelevant patterns can interfere with other applications of deep learning in digital pathology, namely image search.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10189924"
    },
    {
      "pmid": "37203610",
      "title": "Assessing the FAIRness of Deep Learning Models in Cardiovascular Disease Using Computed Tomography Images: Data and Code Perspective.",
      "abstract": "The interest in the application of AI in medicine has intensely increased over the past decade with most of the changes in the past five years. Most recently, the application of deep learning algorithms in prediction and classification of cardiovascular diseases (CVD) using computed tomography (CT) images showed promising results. The notable and exciting advancement in this area of study is, however, associated with different challenges related to the findability (F), accessibility(A), interoperability(I), reusability(R) of both data and source code. The aim of this work is to identify reoccurring missing FAIR-related features and to assess the level of FAIRness of data and models used to predict/diagnose cardiovascular diseases from CT images. We evaluated the FAIRness of data and models in published studies using the RDA (Research Data Alliance) FAIR Data maturity model and FAIRshake toolkit. The finding showed that although AI is anticipated to bring ground breaking solutions for complex medical problems, the findability, accessibility, interoperability and reusability of data/metadata/code is still a prominent challenge.",
      "journal": "Studies in health technology and informatics",
      "year": "2023",
      "doi": "10.3233/SHTI230065",
      "authors": "Shiferaw Kirubel Biruk et al.",
      "keywords": "Deep learning; FAIR Principles; RDA FAIR Data maturity model; cardiovascular disease; computed tomography",
      "mesh_terms": "Humans; Deep Learning; Cardiovascular Diseases; Tomography, X-Ray Computed; Software; Algorithms",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37203610/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Radiology/Medical Imaging; Cardiology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We evaluated the FAIRness of data and models in published studies using the RDA (Research Data Alliance) FAIR Data maturity model and FAIRshake toolkit. The finding showed that although AI is anticipated to bring ground breaking solutions for complex medical problems, the findability, accessibility, interoperability and reusability of data/metadata/code is still a prominent challenge.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37211215",
      "title": "Availability of information needed to evaluate algorithmic fairness - A systematic review of publicly accessible critical care databases.",
      "abstract": "BACKGROUND: Machine learning (ML) may improve clinical decision-making in critical care settings, but intrinsic biases in datasets can introduce bias into predictive models. This study aims to determine if publicly available critical care datasets provide relevant information to identify historically marginalized populations. METHOD: We conducted a review to identify the manuscripts that report the training/validation of ML algorithms using publicly accessible critical care electronic medical record (EMR) datasets. The datasets were reviewed to determine if the following 12 variables were available: age, sex, gender identity, race and/or ethnicity, self-identification as an indigenous person, payor, primary language, religion, place of residence, education, occupation, and income. RESULTS: 7 publicly available databases were identified. Medical Information Mart for Intensive Care (MIMIC) reports information on 7 of the 12 variables of interest, Sistema de Informa\u00e7\u00e3o de Vigil\u00e2ncia Epidemiol\u00f3gica da Gripe (SIVEP-Gripe) on 7, COVID-19 Mexican Open Repository on 4, and eICU on 4. Other datasets report information on 2 or fewer variables. All 7 databases included information about sex and age. Four databases (57%) included information about whether a patient identified as native or indigenous. Only 3 (43%) included data about race and/or ethnicity. Two databases (29%) included information about residence, and one (14%) included information about payor, language, and religion. One database (14%) included information about education and patient occupation. No databases included information on gender identity and income. CONCLUSION: This review demonstrates that critical care publicly available data used to train AI algorithms do not include enough information to properly look for intrinsic bias and fairness issues towards historically marginalized populations.",
      "journal": "Anaesthesia, critical care & pain medicine",
      "year": "2023",
      "doi": "10.1016/j.accpm.2023.101248",
      "authors": "Fong Nicholas et al.",
      "keywords": "Artificial Intelligence; Bias; Dataset; Fairness; Machine learning; Publicly available",
      "mesh_terms": "Humans; Male; Female; COVID-19; Gender Identity; Algorithms; Critical Care; Machine Learning",
      "pub_types": "Systematic Review; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37211215/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics; Pulmonology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: This review demonstrates that critical care publicly available data used to train AI algorithms do not include enough information to properly look for intrinsic bias and fairness issues towards historically marginalized populations.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37258078",
      "title": "Social bias in artificial intelligence algorithms designed to improve cardiovascular risk assessment relative to the Framingham Risk Score: a protocol for a systematic review.",
      "abstract": "INTRODUCTION: Cardiovascular disease (CVD) prevention relies on timely identification of and intervention for individuals at risk. Risk assessment models such as the Framingham Risk Score (FRS) have been shown to over-estimate or under-estimate risk in certain groups, such as socioeconomically disadvantaged populations. Artificial intelligence (AI) and machine learning (ML) could be used to address such equity gaps to improve risk assessment; however, critical appraisal is warranted before ML-informed clinical decision-making is implemented. METHODS AND ANALYSIS: This study will employ an equity-lens to identify sources of bias (ie, race/ethnicity, gender and social stratum) in ML algorithms designed to improve CVD risk assessment relative to the FRS. A comprehensive literature search will be completed using MEDLINE, Embase and IEEE to answer the research question: do AI algorithms that are designed for the estimation of CVD risk and that compare performance with the FRS address the sources of bias inherent in the FRS? No study date filters will be imposed on the search, but English language filters will be applied. Studies describing a specific algorithm or ML approach that provided a risk assessment output for coronary artery disease, heart failure, cardiac arrhythmias (ie, atrial fibrillation), stroke or a global CVD risk score, and that compared performance with the FRS are eligible for inclusion. Papers describing algorithms for the diagnosis rather than the prevention of CVD will be excluded. A structured narrative review analysis of included studies will be completed. ETHICS AND DISSEMINATION: Ethics approval was not required. Ethics exemption was formally received from the General Research Ethics Board at Queen's University. The completed systematic review will be submitted to a peer-reviewed journal and parts of the work will be presented at relevant conferences.",
      "journal": "BMJ open",
      "year": "2023",
      "doi": "10.1136/bmjopen-2022-067638",
      "authors": "Garcha Ivneet et al.",
      "keywords": "cardiology; preventive medicine; primary care; social medicine",
      "mesh_terms": "Humans; Algorithms; Artificial Intelligence; Cardiovascular Diseases; Heart Disease Risk Factors; Risk Assessment; Risk Factors; Systematic Reviews as Topic",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37258078/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Cardiology; Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Ethics exemption was formally received from the General Research Ethics Board at Queen's University. The completed systematic review will be submitted to a peer-reviewed journal and parts of the work will be presented at relevant conferences.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10254950"
    },
    {
      "pmid": "37312237",
      "title": "Testing for an ignorable sampling bias under random double truncation.",
      "abstract": "In clinical and epidemiological research doubly truncated data often appear. This is the case, for instance, when the data registry is formed by interval sampling. Double truncation generally induces a sampling bias on the target variable, so proper corrections of ordinary estimation and inference procedures must be used. Unfortunately, the nonparametric maximum likelihood estimator of a doubly truncated distribution has several drawbacks, like potential nonexistence and nonuniqueness issues, or large estimation variance. Interestingly, no correction for double truncation is needed when the sampling bias is ignorable, which may occur with interval sampling and other sampling designs. In such a case the ordinary empirical distribution function is a consistent and fully efficient estimator that generally brings remarkable variance improvements compared to the nonparametric maximum likelihood estimator. Thus, identification of such situations is critical for the simple and efficient estimation of the target distribution. In this article, we introduce for the first time formal testing procedures for the null hypothesis of ignorable sampling bias with doubly truncated data. The asymptotic properties of the proposed test statistic are investigated. A bootstrap algorithm to approximate the null distribution of the test in practice is introduced. The finite sample performance of the method is studied in simulated scenarios. Finally, applications to data on onset for childhood cancer and Parkinson's disease are given. Variance improvements in estimation are discussed and illustrated.",
      "journal": "Statistics in medicine",
      "year": "2023",
      "doi": "10.1002/sim.9828",
      "authors": "de U\u00f1a-\u00c1lvarez Jacobo",
      "keywords": "bootstrap; goodness-of-fit; interval sampling; nonparametric statistics; survival analysis",
      "mesh_terms": "Humans; Child; Selection Bias; Likelihood Functions; Computer Simulation; Research Design; Algorithms; Bias",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37312237/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; Pediatrics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Finally, applications to data on onset for childhood cancer and Parkinson's disease are given. Variance improvements in estimation are discussed and illustrated.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37312937",
      "title": "Benzodiazepine-related dementia risks and protopathic biases revealed by multiple-kernel learning with electronic medical records.",
      "abstract": "OBJECTIVES: To simultaneously estimate how the risk of incident dementia nonlinearly varies with the administration period and cumulative dose of benzodiazepines, the duration of disorders with an indication for benzodiazepines, and other potential confounders, with the goal of settling the controversy over the role of benzodiazepines in the development of dementia. METHODS: The classical hazard model was extended using the techniques of multiple-kernel learning. Regularised maximum-likelihood estimation, including determination of hyperparameter values with 10-fold cross-validation, bootstrap goodness-of-fit test, and bootstrap estimation of confidence intervals, was applied to cohorts retrospectively extracted from electronic medical records of our university hospitals between 1 November 2004 and 31 July 2020. The analysis was mainly focused on 8160 patients aged 40 or older with new onset of insomnia, affective disorders, or anxiety disorders, who were followed up for 4.10\u00b13.47 years. RESULTS: Besides previously reported risk associations, we detected significant nonlinear risk variations over 2-4 years attributable to the duration of insomnia and anxiety disorders, and to the administration period of short-acting benzodiazepines. After nonlinear adjustment for potential confounders, we observed no significant risk associations with long-term use of benzodiazepines. CONCLUSIONS: The pattern of the detected nonlinear risk variations suggested reverse causation and confounding. Their putative bias effects over 2-4 years suggested similar biases in previously reported results. These results, together with the lack of significant risk associations with long-term use of benzodiazepines, suggested the need to reconsider previous results and methods for future analysis.",
      "journal": "Digital health",
      "year": "2023",
      "doi": "10.1177/20552076231178577",
      "authors": "Hayakawa Takashi et al.",
      "keywords": "Dementia; benzodiazepines; electronic medical records; kernel method; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37312937/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Survival Analysis",
      "health_domain": "Mental Health/Psychiatry; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "CONCLUSIONS: The pattern of the detected nonlinear risk variations suggested reverse causation and confounding. Their putative bias effects over 2-4 years suggested similar biases in previously reported results. These results, together with the lack of significant risk associations with long-term use of benzodiazepines, suggested the need to reconsider previous results and methods for future analysis.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10259140"
    },
    {
      "pmid": "37318857",
      "title": "Algorithmic bias and research integrity; the role of nonhuman authors in shaping scientific knowledge with respect to artificial intelligence: a perspective.",
      "abstract": "Artificial intelligence technologies were developed to assist authors in bettering the organization and caliber of their published papers, which are both growing in quantity and sophistication. Even though the usage of artificial intelligence tools in particular ChatGPT's natural language processing systems has been shown to be beneficial in research, there are still concerns about accuracy, responsibility, and transparency when it comes to the norms regarding authorship credit and contributions. Genomic algorithms quickly examine large amounts of genetic data to identify potential disease-causing mutations. By analyzing millions of medications for potential therapeutic benefits, they can quickly and relatively economically find novel approaches to treatment. Researchers from several fields can collaborate on difficult tasks with the assistance of nonhuman writers, promoting interdisciplinary research. Sadly, there are a number of significant disadvantages associated with employing nonhuman authors, including the potential for algorithmic prejudice. Biased data may be reinforced by the algorithm since machine learning algorithms can only be as objective as the data they are trained on. It is overdue that scholars bring forth basic moral concerns in the fight against algorithmic prejudice. Overall, even if the use of nonhuman authors has the potential to significantly improve scientific research, it is crucial for scientists to be aware of these drawbacks and take precautions to avoid bias and limits. To provide accurate and objective results, algorithms must be carefully designed and implemented, and researchers need to be mindful of the larger ethical ramifications of their usage.",
      "journal": "International journal of surgery (London, England)",
      "year": "2023",
      "doi": "10.1097/JS9.0000000000000552",
      "authors": "Oduoye Malik Olatunde et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Algorithms; Authorship; Awareness; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37318857/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "NLP/LLM; Generative AI",
      "health_domain": "ICU/Critical Care; Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "Overall, even if the use of nonhuman authors has the potential to significantly improve scientific research, it is crucial for scientists to be aware of these drawbacks and take precautions to avoid bias and limits. To provide accurate and objective results, algorithms must be carefully designed and implemented, and researchers need to be mindful of the larger ethical ramifications of their usage.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10583945"
    },
    {
      "pmid": "37350883",
      "title": "Avoiding Biased Clinical Machine Learning Model Performance Estimates in the Presence of Label Selection.",
      "abstract": "When evaluating the performance of clinical machine learning models, one must consider the deployment population. When the population of patients with observed labels is only a subset of the deployment population (label selection), standard model performance estimates on the observed population may be misleading. In this study we describe three classes of label selection and simulate five causally distinct scenarios to assess how particular selection mechanisms bias a suite of commonly reported binary machine learning model performance metrics. Simulations reveal that when selection is affected by observed features, naive estimates of model discrimination may be misleading. When selection is affected by labels, naive estimates of calibration fail to reflect reality. We borrow traditional weighting estimators from causal inference literature and find that when selection probabilities are properly specified, they recover full population estimates. We then tackle the real-world task of monitoring the performance of deployed machine learning models whose interactions with clinicians feed-back and affect the selection mechanism of the labels. We train three machine learning models to flag low-yield laboratory diagnostics, and simulate their intended consequence of reducing wasteful laboratory utilization. We find that naive estimates of AUROC on the observed population undershoot actual performance by up to 20%. Such a disparity could be large enough to lead to the wrongful termination of a successful clinical decision support tool. We propose an altered deployment procedure, one that combines injected randomization with traditional weighted estimates, and find it recovers true model performance.",
      "journal": "AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science",
      "year": "2023",
      "doi": "",
      "authors": "Corbin Conor K et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37350883/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clinical Decision Support",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Counterfactual Fairness",
      "clinical_setting": "ICU; Public Health/Population; Laboratory/Pathology",
      "key_findings": "Such a disparity could be large enough to lead to the wrongful termination of a successful clinical decision support tool. We propose an altered deployment procedure, one that combines injected randomization with traditional weighted estimates, and find it recovers true model performance.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10283136"
    },
    {
      "pmid": "37350910",
      "title": "Evaluate underdiagnosis and overdiagnosis bias of deep learning model on primary open-angle glaucoma diagnosis in under-served populations.",
      "abstract": "In the United States, primary open-angle glaucoma (POAG) is the leading cause of blindness, especially among African American and Hispanic individuals. Deep learning has been widely used to detect POAG using fundus images as its performance is comparable to or even surpasses diagnosis by clinicians. However, human bias in clinical diagnosis may be reflected and amplified in the widely-used deep learning models, thus impacting their performance. Biases may cause (1) underdiagnosis, increasing the risks of delayed or inadequate treatment, and (2) overdiagnosis, which may increase individuals' stress, fear, well-being, and unnecessary/costly treatment. In this study, we examined the underdiagnosis and overdiagnosis when applying deep learning in POAG detection based on the Ocular Hypertension Treatment Study (OHTS) from 22 centers across 16 states in the United States. Our results show that the widely-used deep learning model can underdiagnose or overdiagnose under-served populations. The most underdiagnosed group is female younger (< 60 yrs) group, and the most overdiagnosed group is Black older (\u2265 60 yrs) group. Biased diagnosis through traditional deep learning methods may delay disease detection, treatment and create burdens among under-served populations, thereby, raising ethical concerns about using deep learning models in ophthalmology clinics.",
      "journal": "AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science",
      "year": "2023",
      "doi": "",
      "authors": "Lin Mingquan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37350910/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Ophthalmology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "The most underdiagnosed group is female younger (< 60 yrs) group, and the most overdiagnosed group is Black older (\u2265 60 yrs) group. Biased diagnosis through traditional deep learning methods may delay disease detection, treatment and create burdens among under-served populations, thereby, raising ethical concerns about using deep learning models in ophthalmology clinics.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10283103"
    },
    {
      "pmid": "37380750",
      "title": "Algorithmic fairness in artificial intelligence for medicine and healthcare.",
      "abstract": "In healthcare, the development and deployment of insufficiently fair systems of artificial intelligence (AI) can undermine the delivery of equitable care. Assessments of AI models stratified across subpopulations have revealed inequalities in how patients are diagnosed, treated and billed. In this Perspective, we outline fairness in machine learning through the lens of healthcare, and discuss how algorithmic biases (in data acquisition, genetic variation and intra-observer labelling variability, in particular) arise in clinical workflows and the resulting healthcare disparities. We also review emerging technology for mitigating biases via disentanglement, federated learning and model explainability, and their role in the development of AI-based software as a medical device.",
      "journal": "Nature biomedical engineering",
      "year": "2023",
      "doi": "10.1038/s41551-023-01056-8",
      "authors": "Chen Richard J et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Medicine; Software; Machine Learning; Delivery of Health Care",
      "pub_types": "Journal Article; Review; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37380750/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Federated Learning",
      "health_domain": "ICU/Critical Care; Genomics/Genetics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Collection; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Subgroup Analysis; Representation Learning; Federated Learning; Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "In this Perspective, we outline fairness in machine learning through the lens of healthcare, and discuss how algorithmic biases (in data acquisition, genetic variation and intra-observer labelling variability, in particular) arise in clinical workflows and the resulting healthcare disparities. We also review emerging technology for mitigating biases via disentanglement, federated learning and model explainability, and their role in the development of AI-based software as a medical device.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10632090"
    },
    {
      "pmid": "37405023",
      "title": "Cardiovascular disease/stroke risk stratification in deep learning framework: a review.",
      "abstract": "The global mortality rate is known to be the highest due to cardiovascular disease (CVD). Thus, preventive, and early CVD risk identification in a non-invasive manner is vital as healthcare cost is increasing day by day. Conventional methods for risk prediction of CVD lack robustness due to the non-linear relationship between risk factors and cardiovascular events in multi-ethnic cohorts. Few recently proposed machine learning-based risk stratification reviews without deep learning (DL) integration. The proposed study focuses on CVD risk stratification by the use of techniques mainly solo deep learning (SDL) and hybrid deep learning (HDL). Using a PRISMA model, 286 DL-based CVD studies were selected and analyzed. The databases included were Science Direct, IEEE Xplore, PubMed, and Google Scholar. This review is focused on different SDL and HDL architectures, their characteristics, applications, scientific and clinical validation, along with plaque tissue characterization for CVD/stroke risk stratification. Since signal processing methods are also crucial, the study further briefly presented Electrocardiogram (ECG)-based solutions. Finally, the study presented the risk due to bias in AI systems. The risk of bias tools used were (I) ranking method (RBS), (II) region-based map (RBM), (III) radial bias area (RBA), (IV) prediction model risk of bias assessment tool (PROBAST), and (V) risk of bias in non-randomized studies-of interventions (ROBINS-I). The surrogate carotid ultrasound image was mostly used in the UNet-based DL framework for arterial wall segmentation. Ground truth (GT) selection is vital for reducing the risk of bias (RoB) for CVD risk stratification. It was observed that the convolutional neural network (CNN) algorithms were widely used since the feature extraction process was automated. The ensemble-based DL techniques for risk stratification in CVD are likely to supersede the SDL and HDL paradigms. Due to the reliability, high accuracy, and faster execution on dedicated hardware, these DL methods for CVD risk assessment are powerful and promising. The risk of bias in DL methods can be best reduced by considering multicentre data collection and clinical evaluation.",
      "journal": "Cardiovascular diagnosis and therapy",
      "year": "2023",
      "doi": "10.21037/cdt-22-438",
      "authors": "Bhagawati Mrinalini et al.",
      "keywords": "Cardiovascular disease; bias; deep learning; risk stratification",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37405023/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Deep Learning; Neural Network; Ensemble Methods; Clinical Prediction Model",
      "health_domain": "Radiology/Medical Imaging; Cardiology; Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Ensemble Methods",
      "clinical_setting": "Not specified",
      "key_findings": "Due to the reliability, high accuracy, and faster execution on dedicated hardware, these DL methods for CVD risk assessment are powerful and promising. The risk of bias in DL methods can be best reduced by considering multicentre data collection and clinical evaluation.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 2 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10315429"
    },
    {
      "pmid": "37414033",
      "title": "Health Equity in Clinical Research Informatics.",
      "abstract": "OBJECTIVES: Through a scoping review, we examine in this survey what ways health equity has been promoted in clinical research informatics with patient implications and especially published in the year of 2021 (and some in 2022). METHOD: A scoping review was conducted guided by using methods described in the Joanna Briggs Institute Manual. The review process consisted of five stages: 1) development of aim and research question, 2) literature search, 3) literature screening and selection, 4) data extraction, and 5) accumulate and report results. RESULTS: From the 478 identified papers in 2021 on the topic of clinical research informatics with focus on health equity as a patient implication, 8 papers met our inclusion criteria. All included papers focused on artificial intelligence (AI) technology. The papers addressed health equity in clinical research informatics either through the exposure of inequity in AI-based solutions or using AI as a tool for promoting health equity in the delivery of healthcare services. While algorithmic bias poses a risk to health equity within AI-based solutions, AI has also uncovered inequity in traditional treatment and demonstrated effective complements and alternatives that promotes health equity. CONCLUSIONS: Clinical research informatics with implications for patients still face challenges of ethical nature and clinical value. However, used prudently-for the right purpose in the right context-clinical research informatics could bring powerful tools in advancing health equity in patient care.",
      "journal": "Yearbook of medical informatics",
      "year": "2023",
      "doi": "10.1055/s-0043-1768720",
      "authors": "Maurud Sigurd et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Health Equity; Medical Informatics; Academies and Institutes",
      "pub_types": "Review; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37414033/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Clinical research informatics with implications for patients still face challenges of ethical nature and clinical value. However, used prudently-for the right purpose in the right context-clinical research informatics could bring powerful tools in advancing health equity in patient care.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10751137"
    },
    {
      "pmid": "37423238",
      "title": "Making decisions: Bias in artificial intelligence and data\u2011driven diagnostic tools.",
      "abstract": "BACKGROUND: Although numerous studies have shown the potential of artificial intelligence (AI) systems in drastically improving clinical practice, there are concerns that these AI systems could replicate existing biases. OBJECTIVE: This paper provides a brief overview of\u00a0'algorithmic bias', which refers to the\u00a0tendency of some AI systems to\u00a0perform poorly for disadvantaged or\u00a0marginalised groups. DISCUSSION: AI relies on data generated, collected, recorded and labelled by humans. If AI systems remain unchecked, whatever biases that exist in the real world that are embedded in data will be incorporated into the AI algorithms. Algorithmic bias can be considered as an extension, if not a new manifestation, of existing social biases, understood as negative attitudes towards or the discriminatory treatment of some groups. In medicine, algorithmic bias can compromise patient safety and risks perpetuating disparities in care and outcome. Thus, clinicians should consider the risk of bias when deploying AI-enabled tools in their practice.",
      "journal": "Australian journal of general practice",
      "year": "2023",
      "doi": "10.31128/AJGP-12-22-6630",
      "authors": "Saint James Aquino Yves",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Decision Making; Bias; Medicine; Patient Safety",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37423238/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "DISCUSSION: AI relies on data generated, collected, recorded and labelled by humans. If AI systems remain unchecked, whatever biases that exist in the real world that are embedded in data will be incorporated into the AI algorithms. Algorithmic bias can be considered as an extension, if not a new manifestation, of existing social biases, understood as negative attitudes towards or the discriminatory treatment of some groups.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37444068",
      "title": "Occupational Safety and Health Equity Impacts of Artificial Intelligence: A Scoping Review.",
      "abstract": "Artificial intelligence (AI) has the potential to either reduce or exacerbate occupational safety and health (OSH) inequities in the workplace, and its impact will be mediated by numerous factors. This paper anticipates challenges to ensuring that the OSH benefits of technological advances are equitably distributed among social groups, industries, job arrangements, and geographical regions. A scoping review was completed to summarize the recent literature on AI's role in promoting OSH equity. The scoping review was designed around three concepts: artificial intelligence, OSH, and health equity. Scoping results revealed 113 articles relevant for inclusion. The ways in which AI presents barriers and facilitators to OSH equity are outlined along with priority focus areas and best practices in reducing OSH disparities and knowledge gaps. The scoping review uncovered priority focus areas. In conclusion, AI's role in OSH equity is vastly understudied. An urgent need exists for multidisciplinary research that addresses where and how AI is being adopted and evaluated and how its use is affecting OSH across industries, wage categories, and sociodemographic groups. OSH professionals can play a significant role in identifying strategies that ensure the benefits of AI in promoting workforce health and wellbeing are equitably distributed.",
      "journal": "International journal of environmental research and public health",
      "year": "2023",
      "doi": "10.3390/ijerph20136221",
      "authors": "Fisher Elizabeth et al.",
      "keywords": "algorithmic bias; algorithmic integrity; artificial intelligence; future of work; health equity; occupational safety and health",
      "mesh_terms": "Humans; Occupational Health; Artificial Intelligence; Health Equity; Workplace; Salaries and Fringe Benefits",
      "pub_types": "Journal Article; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37444068/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "An urgent need exists for multidisciplinary research that addresses where and how AI is being adopted and evaluated and how its use is affecting OSH across industries, wage categories, and sociodemographic groups. OSH professionals can play a significant role in identifying strategies that ensure the benefits of AI in promoting workforce health and wellbeing are equitably distributed.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10340692"
    },
    {
      "pmid": "37450282",
      "title": "Retinal Vessel Caliber Measurement Bias in Fundus Images in the Presence of the Central Light Reflex.",
      "abstract": "PURPOSE: To investigate the agreement between a fundus camera and a scanning laser ophthalmoscope in retinal vessel caliber measurements and to identify whether the presence of the central light reflex (CLR) explains potential discrepancies. METHODS: For this cross-sectional study, we obtained fundus camera and scanning laser ophthalmoscope images from 85 eyes of 85 healthy individuals (aged 50-65 years) with different blood pressure status. We measured the central retinal artery equivalent (CRAE) and central retinal artery vein equivalent (CRVE) with the Knudtson-Parr-Hubbard algorithm and assessed the CLR using a semiautomatic grading method. We used Bland-Altman plots, 95% limits of agreement, and the two-way mixed effects intraclass correlation coefficient for consistency [ICC(3,1)] to describe interdevice agreement. We used multivariable regression to identify factors associated with differences in between-device measurements. RESULTS: The between-device difference in CRAE (9.5 \u00a0\u00b5m; 95% confidence interval, 8.0-11.1 \u00a0\u00b5m) was larger than the between-device difference in CRVE (2.9 \u00a0\u00b5m; 95% confidence interval, 1.3-4.5 \u00a0\u00b5m), with the fundus camera yielding higher measurements (both P < 0.001). The 95% fundus camera-scanning laser ophthalmoscope limits of agreement were -4.8 to 23.9 \u00a0\u00b5m for CRAE and -12.0 to 17.8 \u00a0\u00b5m for CRVE. The corresponding ICCs(3,1) were 0.89 (95% confidence interval, 0.83-0.92) and 0.91 (95% confidence interval, 0.86-0.94). The between-device CRAE difference was positively associated with the presence of a CLR (P = 0.002). CONCLUSIONS: Fundus cameras and scanning laser ophthalmoscopes yield correlated but not interchangeable caliber measurements. The CLR induces bias in arteriolar caliber in fundus camera images, compared with scanning laser ophthalmoscope images. TRANSLATIONAL RELEVANCE: Refined measurements could yield better estimates of the association between retinal vessel caliber and ophthalmic or systemic disease.",
      "journal": "Translational vision science & technology",
      "year": "2023",
      "doi": "10.1167/tvst.12.7.16",
      "authors": "Pappelis Konstantinos et al.",
      "keywords": "",
      "mesh_terms": "Humans; Cross-Sectional Studies; Retinal Vessels; Retinal Vein; Retinal Artery; Reflex",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37450282/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Ophthalmology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Fundus cameras and scanning laser ophthalmoscopes yield correlated but not interchangeable caliber measurements. The CLR induces bias in arteriolar caliber in fundus camera images, compared with scanning laser ophthalmoscope images. TRANSLATIONAL RELEVANCE: Refined measurements could yield better estimates of the association between retinal vessel caliber and ophthalmic or systemic disease.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10353742"
    },
    {
      "pmid": "37485306",
      "title": "Accelerating voxelwise annotation of cross-sectional imaging through AI collaborative labeling with quality assurance and bias mitigation.",
      "abstract": "BACKGROUND: precision-medicine quantitative tools for cross-sectional imaging require painstaking labeling of targets that vary considerably in volume, prohibiting scaling of data annotation efforts and supervised training to large datasets for robust and generalizable clinical performance. A straight-forward time-saving strategy involves manual editing of AI-generated labels, which we call AI-collaborative labeling (AICL). Factors affecting the efficacy and utility of such an approach are unknown. Reduction in time effort is not well documented. Further, edited AI labels may be prone to automation bias. PURPOSE: In this pilot, using a cohort of CTs with intracavitary hemorrhage, we evaluate both time savings and AICL label quality and propose criteria that must be met for using AICL annotations as a high-throughput, high-quality ground truth. METHODS: 57 CT scans of patients with traumatic intracavitary hemorrhage were included. No participant recruited for this study had previously interpreted the scans. nnU-net models trained on small existing datasets for each feature (hemothorax/hemoperitoneum/pelvic hematoma; n = 77-253) were used in inference. Two common scenarios served as baseline comparison- de novo expert manual labeling, and expert edits of trained staff labels. Parameters included time effort and image quality graded by a blinded independent expert using a 9-point scale. The observer also attempted to discriminate AICL and expert labels in a random subset (n = 18). Data were compared with ANOVA and post-hoc paired signed rank tests with Bonferroni correction. RESULTS: AICL reduced time effort 2.8-fold compared to staff label editing, and 8.7-fold compared to expert labeling (corrected p < 0.0006). Mean Likert grades for AICL (8.4, SD:0.6) were significantly higher than for expert labels (7.8, SD:0.9) and edited staff labels (7.7, SD:0.8) (corrected p < 0.0006). The independent observer failed to correctly discriminate AI and human labels. CONCLUSION: For our use case and annotators, AICL facilitates rapid large-scale curation of high-quality ground truth. The proposed quality control regime can be employed by other investigators prior to embarking on AICL for segmentation tasks in large datasets.",
      "journal": "Frontiers in radiology",
      "year": "2023",
      "doi": "10.3389/fradi.2023.1202412",
      "authors": "Dreizin David et al.",
      "keywords": "AI assisted annotation; AI-collaborative labeling; CT volumetry; artificial intelligence\u2014AI; computed tomography; human in the loop (HITL); quantitative visualization; trauma",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37485306/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Emergency Medicine; Pain Management",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Post-hoc Correction",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: For our use case and annotators, AICL facilitates rapid large-scale curation of high-quality ground truth. The proposed quality control regime can be employed by other investigators prior to embarking on AICL for segmentation tasks in large datasets.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10362988"
    },
    {
      "pmid": "37506964",
      "title": "\"Shortcuts\" Causing Bias in Radiology Artificial Intelligence: Causes, Evaluation, and Mitigation.",
      "abstract": "Despite the expert-level performance of artificial intelligence (AI) models for various medical imaging tasks, real-world performance failures with disparate outputs for various subgroups limit the usefulness of AI in improving patients' lives. Many definitions of fairness have been proposed, with discussions of various tensions that arise in the choice of an appropriate metric to use to evaluate bias; for example, should one aim for individual or group fairness? One central observation is that AI models apply \"shortcut learning\" whereby spurious features (such as chest tubes and portable radiographic markers on intensive care unit chest radiography) on medical images are used for prediction instead of identifying true pathology. Moreover, AI has been shown to have a remarkable ability to detect protected attributes of age, sex, and race, while the same models demonstrate bias against historically underserved subgroups of age, sex, and race in disease diagnosis. Therefore, an AI model may take shortcut predictions from these correlations and subsequently generate an outcome that is biased toward certain subgroups even when protected attributes are not explicitly used as inputs into the model. As a result, these subgroups became nonprivileged subgroups. In this review, the authors discuss the various types of bias from shortcut learning that may occur at different phases of AI model development, including data bias, modeling bias, and inference bias. The authors thereafter summarize various tool kits that can be used to evaluate and mitigate bias and note that these have largely been applied to nonmedical domains and require more evaluation for medical AI. The authors then summarize current techniques for mitigating bias from preprocessing (data-centric solutions) and during model development (computational solutions) and postprocessing (recalibration of learning). Ongoing legal changes where the use of a biased model will be penalized highlight the necessity of understanding, detecting, and mitigating biases from shortcut learning and will require diverse research teams looking at the whole AI pipeline.",
      "journal": "Journal of the American College of Radiology : JACR",
      "year": "2023",
      "doi": "10.1016/j.jacr.2023.06.025",
      "authors": "Banerjee Imon et al.",
      "keywords": "Artificial Intelligence; machine learning",
      "mesh_terms": "Humans; Artificial Intelligence; Radiography; Radiology; Causality; Bias",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37506964/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care; Pathology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration",
      "clinical_setting": "ICU; Laboratory/Pathology; Safety-Net/Underserved",
      "key_findings": "The authors then summarize current techniques for mitigating bias from preprocessing (data-centric solutions) and during model development (computational solutions) and postprocessing (recalibration of learning). Ongoing legal changes where the use of a biased model will be penalized highlight the necessity of understanding, detecting, and mitigating biases from shortcut learning and will require diverse research teams looking at the whole AI pipeline.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11192466"
    },
    {
      "pmid": "37546907",
      "title": "Equitable machine learning counteracts ancestral bias in precision medicine, improving outcomes for all.",
      "abstract": "Gold standard genomic datasets severely under-represent non-European populations, leading to inequities and a limited understanding of human disease [1-8]. Therapeutics and outcomes remain hidden because we lack insights that we could gain from analyzing ancestry-unbiased genomic data. To address this significant gap, we present PhyloFrame, the first-ever machine learning method for equitable genomic precision medicine. PhyloFrame corrects for ancestral bias by integrating big data tissue-specific functional interaction networks, global population variation data, and disease-relevant transcriptomic data. Application of PhyloFrame to breast, thyroid, and uterine cancers shows marked improvements in predictive power across all ancestries, less model overfitting, and a higher likelihood of identifying known cancer-related genes. The ability to provide accurate predictions for underrepresented groups, in particular, is substantially increased. These results demonstrate how AI can mitigate ancestral bias in training data and contribute to equitable representation in medical research.",
      "journal": "Research square",
      "year": "2023",
      "doi": "10.21203/rs.3.rs-3168446/v1",
      "authors": "Smith Leslie A et al.",
      "keywords": "ancestry; artificial intelligence; cancer; equitable AI; genomics",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37546907/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; ICU/Critical Care; Genomics/Genetics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "The ability to provide accurate predictions for underrepresented groups, in particular, is substantially increased. These results demonstrate how AI can mitigate ancestral bias in training data and contribute to equitable representation in medical research.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10402189"
    },
    {
      "pmid": "37561535",
      "title": "Artificial intelligence suppression as a strategy to mitigate artificial intelligence automation bias.",
      "abstract": "BACKGROUND: Incorporating artificial intelligence (AI) into clinics brings the risk of automation bias, which potentially misleads the clinician's decision-making. The purpose of this study was to propose a potential strategy to mitigate automation bias. METHODS: This was a laboratory study with a randomized cross-over design. The diagnosis of anterior cruciate ligament (ACL) rupture, a common injury, on magnetic resonance imaging (MRI) was used as an example. Forty clinicians were invited to diagnose 200 ACLs with and without AI assistance. The AI's correcting and misleading (automation bias) effects on the clinicians' decision-making processes were analyzed. An ordinal logistic regression model was employed to predict the correcting and misleading probabilities of the AI. We further proposed an AI suppression strategy that retracted AI diagnoses with a higher misleading probability and provided AI diagnoses with a higher correcting probability. RESULTS: The AI significantly increased clinicians' accuracy from 87.2%\u00b113.1% to 96.4%\u00b11.9% (P\u2009<\u2009.001). However, the clinicians' errors in the AI-assisted round were associated with automation bias, accounting for 45.5% of the total mistakes. The automation bias was found to affect clinicians of all levels of expertise. Using a logistic regression model, we identified an AI output zone with higher probability to generate misleading diagnoses. The proposed AI suppression strategy was estimated to decrease clinicians' automation bias by 41.7%. CONCLUSION: Although AI improved clinicians' diagnostic performance, automation bias was a serious problem that should be addressed in clinical practice. The proposed AI suppression strategy is a practical method for decreasing automation bias.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2023",
      "doi": "10.1093/jamia/ocad118",
      "authors": "Wang Ding-Yu et al.",
      "keywords": "AI suppression; automation bias; clinician-AI interaction; deep learning",
      "mesh_terms": "Artificial Intelligence; Magnetic Resonance Imaging; Clinical Decision-Making; Humans; Anterior Cruciate Ligament Injuries; Diagnosis, Computer-Assisted",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37561535/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Logistic Regression; Regression",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "CONCLUSION: Although AI improved clinicians' diagnostic performance, automation bias was a serious problem that should be addressed in clinical practice. The proposed AI suppression strategy is a practical method for decreasing automation bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10531198"
    },
    {
      "pmid": "37566454",
      "title": "Ethical Considerations of Using ChatGPT in Health Care.",
      "abstract": "ChatGPT has promising applications in health care, but potential ethical issues need to be addressed proactively to prevent harm. ChatGPT presents potential ethical challenges from legal, humanistic, algorithmic, and informational perspectives. Legal ethics concerns arise from the unclear allocation of responsibility when patient harm occurs and from potential breaches of patient privacy due to data collection. Clear rules and legal boundaries are needed to properly allocate liability and protect users. Humanistic ethics concerns arise from the potential disruption of the physician-patient relationship, humanistic care, and issues of integrity. Overreliance on artificial intelligence (AI) can undermine compassion and erode trust. Transparency and disclosure of AI-generated content are critical to maintaining integrity. Algorithmic ethics raise concerns about algorithmic bias, responsibility, transparency and explainability, as well as validation and evaluation. Information ethics include data bias, validity, and effectiveness. Biased training data can lead to biased output, and overreliance on ChatGPT can reduce patient adherence and encourage self-diagnosis. Ensuring the accuracy, reliability, and validity of ChatGPT-generated content requires rigorous validation and ongoing updates based on clinical practice. To navigate the evolving ethical landscape of AI, AI in health care must adhere to the strictest ethical standards. Through comprehensive ethical guidelines, health care professionals can ensure the responsible use of ChatGPT, promote accurate and reliable information exchange, protect patient privacy, and empower patients to make informed decisions about their health care.",
      "journal": "Journal of medical Internet research",
      "year": "2023",
      "doi": "10.2196/48009",
      "authors": "Wang Changyu et al.",
      "keywords": "AI; ChatGPT; algorithm; artificial intelligence; artificial intelligence development; development; ethics; health care; large language models; patient privacy; patient safety; privacy; safety",
      "mesh_terms": "Humans; Artificial Intelligence; Reproducibility of Results; Data Collection; Disclosure; Patient Compliance",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37566454/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Age",
      "lifecycle_stage": "Data Collection; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "To navigate the evolving ethical landscape of AI, AI in health care must adhere to the strictest ethical standards. Through comprehensive ethical guidelines, health care professionals can ensure the responsible use of ChatGPT, promote accurate and reliable information exchange, protect patient privacy, and empower patients to make informed decisions about their health care.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10457697"
    },
    {
      "pmid": "37579574",
      "title": "Algorithmic bias in artificial intelligence is a problem-And the root issue is power.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) in health care continues to expand at a rapid rate, impacting both nurses and communities we accompany in care. PURPOSE: We argue algorithmic bias is but a symptom of a more systemic and longstanding problem: power imbalances related to the creation, development, and use of health care technologies. METHODS: This commentary responds to Drs. O'Connor and Booth's 2022 article, \"Algorithmic bias in health care: Opportunities for nurses to improve equality in the age of artificial intelligence.\" DISCUSSION: Nurses need not 'reinvent the wheel' when it comes to AI policy, curricula, or ethics. We can and should follow the lead of communities already working 'from the margins' who provide ample guidance. CONCLUSION: Its neither feasible nor just to expect individual nurses to counter systemic injustice in health care through individual actions, more technocentric curricula, or industry partnerships. We need disciplinary supports for collective action to renegotiate power for AI tech.",
      "journal": "Nursing outlook",
      "year": "2023",
      "doi": "10.1016/j.outlook.2023.102023",
      "authors": "Walker Rae et al.",
      "keywords": "Algorithms; Artificial intelligence; Bias; Machine learning; Nursing ethics; Racism",
      "mesh_terms": "Humans; Artificial Intelligence; Delivery of Health Care",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37579574/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: Its neither feasible nor just to expect individual nurses to counter systemic injustice in health care through individual actions, more technocentric curricula, or industry partnerships. We need disciplinary supports for collective action to renegotiate power for AI tech.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37595374",
      "title": "Multidisciplinary considerations of fairness in medical AI: A scoping review.",
      "abstract": "INTRODUCTION: Artificial Intelligence (AI) technology has been developed significantly in recent years. The fairness of medical AI is of great concern due to its direct relation to human life and health. This review aims to analyze the existing research literature on fairness in medical AI from the perspectives of computer science, medical science, and social science (including law and ethics). The objective of the review is to examine the similarities and differences in the understanding of fairness, explore influencing factors, and investigate potential measures to implement fairness in medical AI across English and Chinese literature. METHODS: This study employed a scoping review methodology and selected the following databases: Web of Science, MEDLINE, Pubmed, OVID, CNKI, WANFANG Data, etc., for the fairness issues in medical AI through February 2023. The search was conducted using various keywords such as \"artificial intelligence,\" \"machine learning,\" \"medical,\" \"algorithm,\" \"fairness,\" \"decision-making,\" and \"bias.\" The collected data were charted, synthesized, and subjected to descriptive and thematic analysis. RESULTS: After reviewing 468 English papers and 356 Chinese papers, 53 and 42 were included in the final analysis. Our results show the three different disciplines all show significant differences in the research on the core issues. Data is the foundation that affects medical AI fairness in addition to algorithmic bias and human bias. Legal, ethical, and technological measures all promote the implementation of medical AI fairness. CONCLUSIONS: Our review indicates a consensus regarding the importance of data fairness as the foundation for achieving fairness in medical AI across multidisciplinary perspectives. However, there are substantial discrepancies in core aspects such as the concept, influencing factors, and implementation measures of fairness in medical AI. Consequently, future research should facilitate interdisciplinary discussions to bridge the cognitive gaps between different fields and enhance the practical implementation of fairness in medical AI.",
      "journal": "International journal of medical informatics",
      "year": "2023",
      "doi": "10.1016/j.ijmedinf.2023.105175",
      "authors": "Wang Yue et al.",
      "keywords": "Algorithm; Artificial Intelligence; Bias; Data; Fairness; Medical AI",
      "mesh_terms": "Humans; Artificial Intelligence; Algorithms; Machine Learning; Consensus; Data Collection",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37595374/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Our review indicates a consensus regarding the importance of data fairness as the foundation for achieving fairness in medical AI across multidisciplinary perspectives. However, there are substantial discrepancies in core aspects such as the concept, influencing factors, and implementation measures of fairness in medical AI. Consequently, future research should facilitate interdisciplinary discussions to bridge the cognitive gaps between different fields and enhance the practical im...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37595864",
      "title": "Advancing AI in healthcare: A comprehensive review of best practices.",
      "abstract": "Artificial Intelligence (AI) and Machine Learning (ML) are powerful tools shaping the healthcare sector. This review considers twelve key aspects of AI in clinical practice: 1) Ethical AI; 2) Explainable AI; 3) Health Equity and Bias in AI; 4) Sponsorship Bias; 5) Data Privacy; 6) Genomics and Privacy; 7) Insufficient Sample Size and Self-Serving Bias; 8) Bridging the Gap Between Training Datasets and Real-World Scenarios; 9) Open Source and Collaborative Development; 10) Dataset Bias and Synthetic Data; 11) Measurement Bias; 12) Reproducibility in AI Research. These categories represent both the challenges and opportunities of AI implementation in healthcare. While AI holds significant potential for improving patient care, it also presents risks and challenges, such as ensuring privacy, combating bias, and maintaining transparency and ethics. The review underscores the necessity of developing comprehensive best practices for healthcare organizations and fostering a diverse dialogue involving data scientists, clinicians, patient advocates, ethicists, economists, and policymakers. We are at the precipice of significant transformation in healthcare powered by AI. By continuing to reassess and refine our approach, we can ensure that AI is implemented responsibly and ethically, maximizing its benefit to patient care and public health.",
      "journal": "Clinica chimica acta; international journal of clinical chemistry",
      "year": "2023",
      "doi": "10.1016/j.cca.2023.117519",
      "authors": "Polevikov Sergei",
      "keywords": "AI biases; AI ethics; Artificial intelligence; Best practices; Health equity; Machine learning",
      "mesh_terms": "Humans; Artificial Intelligence; Reproducibility of Results; Machine Learning; Genomics; Delivery of Health Care",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37595864/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Generative AI",
      "health_domain": "Genomics/Genetics; Public Health",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Data Augmentation; Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We are at the precipice of significant transformation in healthcare powered by AI. By continuing to reassess and refine our approach, we can ensure that AI is implemented responsibly and ethically, maximizing its benefit to patient care and public health.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37600144",
      "title": "Translating Intersectionality to Fair Machine Learning in Health Sciences.",
      "abstract": "Fairness approaches in machine learning should involve more than assessment of performance metrics across groups. Shifting the focus away from model metrics, we reframe fairness through the lens of intersectionality, a Black feminist theoretical framework that contextualizes individuals in interacting systems of power and oppression.",
      "journal": "Nature machine intelligence",
      "year": "2023",
      "doi": "10.1038/s42256-023-00651-3",
      "authors": "Lett Elle et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37600144/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Intersectional",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Fairness approaches in machine learning should involve more than assessment of performance metrics across groups. Shifting the focus away from model metrics, we reframe fairness through the lens of intersectionality, a Black feminist theoretical framework that contextualizes individuals in interacting systems of power and oppression.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10437125"
    },
    {
      "pmid": "37605208",
      "title": "Fairness and generalizability of OCT normative databases: a comparative analysis.",
      "abstract": "PURPOSE: In supervised Machine Learning algorithms, labels and reports are important in model development. To provide a normality assessment, the OCT has an in-built normative database that provides a color base scale from the measurement database comparison. This article aims to evaluate and compare normative databases of different OCT machines, analyzing patient demographic, contrast inclusion and exclusion criteria, diversity index, and statistical approach to assess their fairness and generalizability. METHODS: Data were retrieved from Cirrus, Avanti, Spectralis, and Triton's FDA-approval and equipment manual. The following variables were compared: number of eyes and patients, inclusion and exclusion criteria, statistical approach, sex, race and ethnicity, age, participant country, and diversity index. RESULTS: Avanti OCT has the largest normative database (640 eyes). In every database, the inclusion and exclusion criteria were similar, including adult patients and excluding pathological eyes. Spectralis has the largest White (79.7%) proportionately representation, Cirrus has the largest Asian (24%), and Triton has the largest Black (22%) patient representation. In all databases, the statistical analysis applied was Regression models. The sex diversity index is similar in all datasets, and comparable to the ten most populous contries. Avanti dataset has the highest diversity index in terms of race, followed by Cirrus, Triton, and Spectralis. CONCLUSION: In all analyzed databases, the data framework is static, with limited upgrade options and lacking normative databases for new modules. As a result, caution in OCT normality interpretation is warranted. To address these limitations, there is a need for more diverse, representative, and open-access datasets that take into account patient demographics, especially considering the development of supervised Machine Learning algorithms in healthcare.",
      "journal": "International journal of retina and vitreous",
      "year": "2023",
      "doi": "10.1186/s40942-023-00459-8",
      "authors": "Nakayama Luis Filipe et al.",
      "keywords": "Database; Fairness; Generalizability; Optical coherence tomography; Supervised machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37605208/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Regression",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: In all analyzed databases, the data framework is static, with limited upgrade options and lacking normative databases for new modules. As a result, caution in OCT normality interpretation is warranted. To address these limitations, there is a need for more diverse, representative, and open-access datasets that take into account patient demographics, especially considering the development of supervised Machine Learning algorithms in healthcare.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10440930"
    },
    {
      "pmid": "37609150",
      "title": "Toward MR protocol-agnostic, bias-corrected brain age predicted from clinical-grade MRIs.",
      "abstract": "The predicted brain age minus the chronological age ('brain-PAD') could become a clinical biomarker. However, most brain age methods were developed to use research-grade high-resolution T1-weighted MRIs, limiting their applicability to clinical-grade MRIs from multiple protocols. To overcome this, we adopted a double transfer learning approach to develop a brain age model agnostic to modality, resolution, or slice orientation. Using 6,224 clinical MRIs among 7 modalities, scanned from 1,540 patients using 8 scanners among 15 + facilities of the University of Florida's Health System, we retrained a convolutional neural network (CNN) to predict brain age from synthetic research-grade magnetization-prepared rapid gradient-echo MRIs (MPRAGEs) generated by a deep learning-trained 'super-resolution' method. We also modeled the \"regression dilution bias\", a typical overestimation of younger ages and underestimation of older ages, which correction is paramount for personalized brain age-based biomarkers. This bias was independent of modality or scanner and generalizable to new samples, allowing us to add a bias-correction layer to the CNN. The mean absolute error in test samples was 4.67-6.47 years across modalities, with similar accuracy between original MPRAGEs and their synthetic counterparts. Brain-PAD was also reliable across modalities. We demonstrate the feasibility of clinical-grade brain age predictions, contributing to personalized medicine.",
      "journal": "Research square",
      "year": "2023",
      "doi": "10.21203/rs.3.rs-3229072/v1",
      "authors": "Valdes-Hernandez Pedro et al.",
      "keywords": "Clinical Multimodal MRI; DeepBrainNet; Synthetic MPRAGE; brain age gap; brain-PAD; research-grade MRI; transfer learning",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37609150/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Radiology/Medical Imaging; Neurology",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Transfer Learning",
      "clinical_setting": "Not specified",
      "key_findings": "Brain-PAD was also reliable across modalities. We demonstrate the feasibility of clinical-grade brain age predictions, contributing to personalized medicine.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10441510"
    },
    {
      "pmid": "37610808",
      "title": "Can AI Mitigate Bias in Writing Letters of Recommendation?",
      "abstract": "Letters of recommendation play a significant role in higher education and career progression, particularly for women and underrepresented groups in medicine and science. Already, there is evidence to suggest that written letters of recommendation contain language that expresses implicit biases, or unconscious biases, and that these biases occur for all recommenders regardless of the recommender's sex. Given that all individuals have implicit biases that may influence language use, there may be opportunities to apply contemporary technologies, such as large language models or other forms of generative artificial intelligence (AI), to augment and potentially reduce implicit biases in the written language of letters of recommendation. In this editorial, we provide a brief overview of existing literature on the manifestations of implicit bias in letters of recommendation, with a focus on academia and medical education. We then highlight potential opportunities and drawbacks of applying this emerging technology in augmenting the focused, professional task of writing letters of recommendation. We also offer best practices for integrating their use into the routine writing of letters of recommendation and conclude with our outlook for the future of generative AI applications in supporting this task.",
      "journal": "JMIR medical education",
      "year": "2023",
      "doi": "10.2196/51494",
      "authors": "Leung Tiffany I et al.",
      "keywords": "artificial intelligence; bias; career advancement; gender bias; implicit bias; large language models; leadership; letters of recommendation; medical education; promotion; sponsorship; tenure and promotion",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37610808/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "NLP/LLM; Generative AI",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "We then highlight potential opportunities and drawbacks of applying this emerging technology in augmenting the focused, professional task of writing letters of recommendation. We also offer best practices for integrating their use into the routine writing of letters of recommendation and conclude with our outlook for the future of generative AI applications in supporting this task.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10483302"
    },
    {
      "pmid": "37615359",
      "title": "Toward Advancing Precision Environmental Health: Developing a Customized Exposure Burden Score to PFAS Mixtures to Enable Equitable Comparisons Across Population Subgroups, Using Mixture Item Response Theory.",
      "abstract": "Quantifying a person's cumulative exposure burden to per- and polyfluoroalkyl substances (PFAS) mixtures is important for risk assessment, biomonitoring, and reporting of results to participants. However, different people may be exposed to different sets of PFASs due to heterogeneity in the exposure sources and patterns. Applying a single measurement model for the entire population (e.g., by summing concentrations of all PFAS analytes) assumes that each PFAS analyte is equally informative to PFAS exposure burden for all individuals. This assumption may not hold if PFAS exposure sources systematically differ within the population. However, the sociodemographic, dietary, and behavioral characteristics that underlie systematic exposure differences may not be known, or may be due to a combination of these factors. Therefore, we used mixture item response theory, an unsupervised psychometrics and data science method, to develop a customized PFAS exposure burden scoring algorithm. This scoring algorithm ensures that PFAS burden scores can be equitably compared across population subgroups. We applied our methods to PFAS biomonitoring data from the United States National Health and Nutrition Examination Survey (2013-2018). Using mixture item response theory, we found that participants with higher household incomes had higher PFAS burden scores. Asian Americans had significantly higher PFAS burden compared with non-Hispanic Whites and other race/ethnicity groups. However, some disparities were masked when using summed PFAS concentrations as the exposure metric. This work demonstrates that our summary PFAS burden metric, accounting for sources of exposure variation, may be a more fair and informative estimate of PFAS exposure.",
      "journal": "Environmental science & technology",
      "year": "2023",
      "doi": "10.1021/acs.est.3c00343",
      "authors": "Liu Shelley H et al.",
      "keywords": "algorithmic bias; chemical mixtures; fairness; item response theory; latent variables; per- and polyfluoroalkyl substances; precision environmental health; psychometrics",
      "mesh_terms": "Humans; United States; Environmental Pollutants; Alkanesulfonic Acids; Nutrition Surveys; Fluorocarbons; Environmental Health",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37615359/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Clustering",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "However, some disparities were masked when using summed PFAS concentrations as the exposure metric. This work demonstrates that our summary PFAS burden metric, accounting for sources of exposure variation, may be a more fair and informative estimate of PFAS exposure.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11106720"
    },
    {
      "pmid": "37649910",
      "title": "Performance of Off-the-Shelf Machine Learning Architectures and Biases in Detection of Low Left Ventricular Ejection Fraction.",
      "abstract": "Artificial intelligence - machine learning (AI-ML) is a computational technique that has been demonstrated to be able to extract meaningful clinical information from diagnostic data that are not available using either human interpretation or more simple analysis methods. Recent developments have shown that AI-ML approaches applied to ECGs can accurately predict different patient characteristics and pathologies not detectable by expert physician readers. There is an extensive body of literature surrounding the use of AI-ML in other fields, which has given rise to an array of predefined open-source AI-ML architectures which can be translated to new problems in an \"off-the-shelf\" manner. Applying \"off-the-shelf\" AI-ML architectures to ECG-based datasets opens the door for rapid development and identification of previously unknown disease biomarkers. Despite the excellent opportunity, the ideal open-source AI-ML architecture for ECG related problems is not known. Furthermore, there has been limited investigation on how and when these AI-ML approaches fail and possible bias or disparities associated with particular network architectures. In this study, we aimed to: (1) determine if open-source, \"off-the-shelf\" AI-ML architectures could be trained to classify low LVEF from ECGs, (2) assess the accuracy of different AI-ML architectures compared to each other, and (3) to identify which, if any, patient characteristics are associated with poor AI-ML performance.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2023",
      "doi": "10.1101/2023.06.10.23291237",
      "authors": "Bergquist Jake A et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37649910/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; ICU/Critical Care",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "Furthermore, there has been limited investigation on how and when these AI-ML approaches fail and possible bias or disparities associated with particular network architectures. In this study, we aimed to: (1) determine if open-source, \"off-the-shelf\" AI-ML architectures could be trained to classify low LVEF from ECGs, (2) assess the accuracy of different AI-ML architectures compared to each other, and (3) to identify which, if any, patient characteristics are associated with poor AI-ML performan...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10465010"
    },
    {
      "pmid": "37652578",
      "title": "Ethical Considerations and Fairness in the Use of Artificial Intelligence for Neuroradiology.",
      "abstract": "In this review, concepts of algorithmic bias and fairness are defined qualitatively and mathematically. Illustrative examples are given of what can go wrong when unintended bias or unfairness in algorithmic development occurs. The importance of explainability, accountability, and transparency with respect to artificial intelligence algorithm development and clinical deployment is discussed. These are grounded in the concept of \"primum no nocere\" (first, do no harm). Steps to mitigate unfairness and bias in task definition, data collection, model definition, training, testing, deployment, and feedback are provided. Discussions on the implementation of fairness criteria that maximize benefit and minimize unfairness and harm to neuroradiology patients will be provided, including suggestions for neuroradiologists to consider as artificial intelligence algorithms gain acceptance into neuroradiology practice and become incorporated into routine clinical workflow.",
      "journal": "AJNR. American journal of neuroradiology",
      "year": "2023",
      "doi": "10.3174/ajnr.A7963",
      "authors": "Filippi C G et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Algorithms; Radiologists; Workflow",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37652578/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Collection; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Steps to mitigate unfairness and bias in task definition, data collection, model definition, training, testing, deployment, and feedback are provided. Discussions on the implementation of fairness criteria that maximize benefit and minimize unfairness and harm to neuroradiology patients will be provided, including suggestions for neuroradiologists to consider as artificial intelligence algorithms gain acceptance into neuroradiology practice and become incorporated into routine clinical workflow.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10631523"
    },
    {
      "pmid": "37674957",
      "title": "Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation.",
      "abstract": "This editorial explores the innovative application of large language Models (LLMs) in conducting systematic reviews, specifically focusing on quality assessment and risk-of-bias evaluation. As integral components of systematic reviews, these tasks traditionally require extensive human effort, subjectivity, and time. Integrating LLMs can revolutionize this process, providing an objective, consistent, and rapid methodology for quality assessment and risk-of-bias evaluation. With their ability to comprehend context, predict semantic relationships, and extract relevant information, LLMs can effectively appraise study quality and risk of bias. However, careful consideration must be given to potential risks and limitations associated with over-reliance on machine learning models and inherent biases in training data. An optimal balance and combination between human expertise and automated LLM evaluation might offer the most effective approach to advance and streamline the field of evidence synthesis.",
      "journal": "Cureus",
      "year": "2023",
      "doi": "10.7759/cureus.43023",
      "authors": "Nashwan Abdulqadir J et al.",
      "keywords": "artificial intelligence; evidence synthesis; large language models; machine learning; quality assessment; risk of bias; systematic reviews",
      "mesh_terms": "",
      "pub_types": "Editorial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37674957/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "However, careful consideration must be given to potential risks and limitations associated with over-reliance on machine learning models and inherent biases in training data. An optimal balance and combination between human expertise and automated LLM evaluation might offer the most effective approach to advance and streamline the field of evidence synthesis.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10478591"
    },
    {
      "pmid": "37704341",
      "title": "Critical Bias in Critical Care Devices.",
      "abstract": "Critical care data contain information about the most physiologically fragile patients in the hospital, who require a significant level of monitoring. However, medical devices used for patient monitoring suffer from measurement biases that have been largely underreported. This article explores sources of bias in commonly used clinical devices, including pulse oximeters, thermometers, and sphygmomanometers. Further, it provides a framework for mitigating these biases and key principles to achieve more equitable health care delivery.",
      "journal": "Critical care clinics",
      "year": "2023",
      "doi": "10.1016/j.ccc.2023.02.005",
      "authors": "Charpignon Marie-Laure et al.",
      "keywords": "Artificial intelligence; Bias; Critical care; Medical devices",
      "mesh_terms": "Humans; Bias; Critical Care",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37704341/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; ICU",
      "key_findings": "This article explores sources of bias in commonly used clinical devices, including pulse oximeters, thermometers, and sphygmomanometers. Further, it provides a framework for mitigating these biases and key principles to achieve more equitable health care delivery.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37746321",
      "title": "Raising awareness of sex and gender bias in artificial intelligence and health.",
      "abstract": "Historically, biomedical research has been led by and focused on men. The recent introduction of Artificial Intelligence (AI) in this area has further proven this practice to be discriminatory for other sexes and genders, more noticeably for women. To move towards a fair AI development, it is essential to include sex and gender diversity both in research practices and in the workplace. In this context, the Bioinfo4women (B4W) program of the Barcelona Supercomputing Center (i) promotes the participation of women scientists by improving their visibility, (ii) fosters international collaborations between institutions and programs and (iii) advances research on sex and gender bias in AI and health. In this article, we discuss methodology and results of a series of conferences, titled \u00e2\u20ac\u0153Sex and Gender Bias in Artificial Intelligence and Health, organized by B4W and La Caixa Foundation from March to June 2021 in Barcelona, Spain. The series consisted of nine hybrid events, composed of keynote sessions and seminars open to the general audience, and two working groups with invited experts from different professional backgrounds (academic fields such as biology, engineering, and sociology, as well as NGOs, journalists, lawyers, policymakers, industry). Based on this awareness-raising action, we distilled key recommendations to facilitate the inclusion of sex and gender perspective into public policies, educational programs, industry, and biomedical research, among other sectors, and help overcome sex and gender biases in AI and health.",
      "journal": "Frontiers in global women's health",
      "year": "2023",
      "doi": "10.3389/fgwh.2023.970312",
      "authors": "Busl\u00f3n Nataly et al.",
      "keywords": "AI; bias in science; gender bias; gender policies; health",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37746321/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Generative AI",
      "health_domain": "Pain Management",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The series consisted of nine hybrid events, composed of keynote sessions and seminars open to the general audience, and two working groups with invited experts from different professional backgrounds (academic fields such as biology, engineering, and sociology, as well as NGOs, journalists, lawyers, policymakers, industry). Based on this awareness-raising action, we distilled key recommendations to facilitate the inclusion of sex and gender perspective into public policies, educational programs,...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10512182"
    },
    {
      "pmid": "37789032",
      "title": "Humans inherit artificial intelligence biases.",
      "abstract": "Artificial intelligence recommendations are sometimes erroneous and biased. In our research, we hypothesized that people who perform a (simulated) medical diagnostic task assisted by a biased AI system will reproduce the model's bias in their own decisions, even when they move to a context without AI support. In three experiments, participants completed a medical-themed classification task with or without the help of a biased AI system. The biased recommendations by the AI influenced participants' decisions. Moreover, when those participants, assisted by the AI, moved on to perform the task without assistance, they made the same errors as the AI had made during the previous phase. Thus, participants' responses mimicked AI bias even when the AI was no longer making suggestions. These results provide evidence of human inheritance of AI bias.",
      "journal": "Scientific reports",
      "year": "2023",
      "doi": "10.1038/s41598-023-42384-8",
      "authors": "Vicente Luc\u00eda et al.",
      "keywords": "",
      "mesh_terms": "Humans; Artificial Intelligence; Bias; Inheritance Patterns; Suggestion",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37789032/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Thus, participants' responses mimicked AI bias even when the AI was no longer making suggestions. These results provide evidence of human inheritance of AI bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10547752"
    },
    {
      "pmid": "37790341",
      "title": "Evaluating and Improving Health Equity and Fairness of Polygenic Scores.",
      "abstract": "Polygenic scores (PGS) are quantitative metrics for predicting phenotypic values, such as human height or disease status. Some PGS methods require only summary statistics of a relevant genome-wide association study (GWAS) for their score. One such method is Lassosum, which inherits the model selection advantages of Lasso to select a meaningful subset of the GWAS single nucleotide polymorphisms as predictors from their association statistics. However, even efficient scores like Lassosum, when derived from European-based GWAS, are poor predictors of phenotype for subjects of non-European ancestry; that is, they have limited portability to other ancestries. To increase the portability of Lassosum, when GWAS information and estimates of linkage disequilibrium are available for both ancestries, we propose Joint-Lassosum. In the simulation settings we explore, Joint-Lassosum provides more accurate PGS compared with other methods, especially when measured in terms of fairness. Like all PGS methods, Joint-Lassosum requires selection of predictors, which are determined by data-driven tuning parameters. We describe a new approach to selecting tuning parameters and note its relevance for model selection for any PGS. We also draw connections to the literature on algorithmic fairness and discuss how Joint-Lassosum can help mitigate fairness-related harms that might result from the use of PGS scores in clinical settings. While no PGS method is likely to be universally portable, due to the diversity of human populations and unequal information content of GWAS for different ancestries, Joint-Lassosum is an effective approach for enhancing portability and reducing predictive bias.",
      "journal": "bioRxiv : the preprint server for biology",
      "year": "2023",
      "doi": "10.1101/2023.09.22.559051",
      "authors": "Zhang Tianyu et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37790341/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We also draw connections to the literature on algorithmic fairness and discuss how Joint-Lassosum can help mitigate fairness-related harms that might result from the use of PGS scores in clinical settings. While no PGS method is likely to be universally portable, due to the diversity of human populations and unequal information content of GWAS for different ancestries, Joint-Lassosum is an effective approach for enhancing portability and reducing predictive bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10542523"
    },
    {
      "pmid": "37794418",
      "title": "Sources of bias and limitations of thrombinography: inner filter effect and substrate depletion at the edge of failure algorithm.",
      "abstract": "BACKGROUND: Fluorogenic thrombin generation (TG) is a global hemostasis assay that provides an overall representation of hemostasis potential. However, the accurate detection of thrombin activity in plasma may be affected by artifacts inherent to the assay-associated fluorogenic substrate. The significance of the fluorogenic artifacts or their corrections has not been studied in hemophilia treatment applications. METHODS: We sought to investigate TG in hemophilia plasma samples under typical and worst-case fluorogenic artifact conditions and assess the performance of artifact correction algorithms. Severe hemophilic plasma with or without added Factor VIII (FVIII) was evaluated using commercially available and in-house TG reagents, instruments, and software packages. The inner filter effect (IFE) was induced by spiking elevated amounts of fluorophore 7-amino-4-methylcoumarin (AMC) into plasma prior to the TG experiment. Substrate consumption was modeled by adding decreasing amounts of Z-Gly-Gly-Arg-AMC (ZGGR-AMC) to plasma or performing TG in antithrombin deficient plasma. RESULTS: All algorithms corrected the AMC-induced IFE and antithrombin-deficiency induced substrate consumption up to a certain level of either artifact (edge of failure) upon which TG results were not returned or overestimated. TG values in FVIII deficient (FVIII-DP) or supplemented plasma were affected similarly. Normalization of FVIII-DP resulted in a more accurate correction of substrate artifacts than algorithmic methods. CONCLUSIONS: Correction algorithms may be effective in situations of moderate fluorogenic substrate artifacts inherent to highly procoagulant samples, but correction may not be required under typical conditions for hemophilia treatment studies if TG parameters can be normalized to a reference plasma sample.",
      "journal": "Thrombosis journal",
      "year": "2023",
      "doi": "10.1186/s12959-023-00549-5",
      "authors": "Jackson Joseph W et al.",
      "keywords": "Factor VIII; Hemophilia A; Hemostasis; Thrombin, Blood coagulation test",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37794418/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Correction algorithms may be effective in situations of moderate fluorogenic substrate artifacts inherent to highly procoagulant samples, but correction may not be required under typical conditions for hemophilia treatment studies if TG parameters can be normalized to a reference plasma sample.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10548689"
    },
    {
      "pmid": "37846615",
      "title": "Call for algorithmic fairness to mitigate amplification of racial biases in artificial intelligence models used in orthodontics and craniofacial health.",
      "abstract": "Machine Learning (ML), a subfield of Artificial Intelligence (AI), is being increasingly used in Orthodontics and craniofacial health for predicting clinical outcomes. Current ML/AI models are prone to accentuate racial disparities. The objective of this narrative review is to provide an overview of how AI/ML models perpetuate racial biases and how we can mitigate this situation. A narrative review of articles published in the medical literature on racial biases and the use of AI/ML models was undertaken. Current AI/ML models are built on homogenous clinical datasets that have a gross underrepresentation of historically disadvantages demographic groups, especially the ethno-racial minorities. The consequence of such AI/ML models is that they perform poorly when deployed on ethno-racial minorities thus further amplifying racial biases. Healthcare providers, policymakers, AI developers and all stakeholders should pay close attention to various steps in the pipeline of building AI/ML models and every effort must be made to establish algorithmic fairness to redress inequities.",
      "journal": "Orthodontics & craniofacial research",
      "year": "2023",
      "doi": "10.1111/ocr.12721",
      "authors": "Allareddy Veerasathpurush et al.",
      "keywords": "algorithms; artificial intelligence; ethnoracial disparities; health disparities; machine learning; racial bias",
      "mesh_terms": "Artificial Intelligence; Machine Learning; Bias",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37846615/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "The consequence of such AI/ML models is that they perform poorly when deployed on ethno-racial minorities thus further amplifying racial biases. Healthcare providers, policymakers, AI developers and all stakeholders should pay close attention to various steps in the pipeline of building AI/ML models and every effort must be made to establish algorithmic fairness to redress inequities.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "37873343",
      "title": "BOLD: Blood-gas and Oximetry Linked Dataset - Open Source Research.",
      "abstract": "Pulse oximeters measure peripheral arterial oxygen saturation (SpO 2 ) noninvasively, while the gold standard (SaO 2 ) involves arterial blood gas measurement. There are known racial and ethnic disparities in their performance. BOLD is a new comprehensive dataset that aims to underscore the importance of addressing biases in pulse oximetry accuracy, which disproportionately affect darker-skinned patients. The dataset was created by harmonizing three Electronic Health Record databases (MIMIC-III, MIMIC-IV, eICU-CRD) comprising Intensive Care Unit stays of US patients. Paired SpO 2 and SaO 2 measurements were time-aligned and combined with various other sociodemographic and parameters to provide a detailed representation of each patient. BOLD includes 49,099 paired measurements, within a 5-minute window and with oxygen saturation levels between 70-100%. Minority racial and ethnic groups account for \u223c25% of the data - a proportion seldom achieved in previous studies. The codebase is publicly available. Given the prevalent use of pulse oximeters in the hospital and at home, we hope that BOLD will be leveraged to develop debiasing algorithms that can result in more equitable healthcare solutions.",
      "journal": "medRxiv : the preprint server for health sciences",
      "year": "2023",
      "doi": "10.1101/2023.10.03.23296485",
      "authors": "Matos Jo\u00e3o et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Preprint; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37873343/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; EHR/Health Informatics; Wearables/Remote Monitoring",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; ICU",
      "key_findings": "The codebase is publicly available. Given the prevalent use of pulse oximeters in the hospital and at home, we hope that BOLD will be leveraged to develop debiasing algorithms that can result in more equitable healthcare solutions.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 2 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10593048"
    },
    {
      "pmid": "37885556",
      "title": "Bias and Inaccuracy in AI Chatbot Ophthalmologist Recommendations.",
      "abstract": "PURPOSE AND DESIGN: To evaluate the accuracy and bias of ophthalmologist recommendations made by three AI chatbots, namely\u00a0ChatGPT 3.5 (OpenAI, San Francisco, CA, USA),\u00a0Bing Chat (Microsoft Corp., Redmond, WA, USA), and\u00a0Google Bard (Alphabet Inc., Mountain View, CA, USA). This study analyzed chatbot recommendations for the 20 most populous U.S. cities. METHODS: Each chatbot returned 80 total recommendations when given the prompt\u00a0\"Find me four good ophthalmologists in (city).\"\u00a0Characteristics of the physicians, including specialty, location, gender, practice type, and fellowship, were collected. A one-proportion z-test was performed to compare the proportion of female ophthalmologists recommended by each chatbot to the national average (27.2% per the\u00a0Association of American Medical Colleges\u00a0(AAMC)). Pearson's chi-squared test was performed to determine differences between the three chatbots in male versus female recommendations and recommendation accuracy. RESULTS: Female ophthalmologists recommended by Bing Chat (1.61%) and Bard (8.0%) were significantly less than the national proportion of 27.2% practicing female ophthalmologists (p<0.001, p<0.01, respectively). ChatGPT recommended fewer female (29.5%) than male ophthalmologists (p<0.722). ChatGPT (73.8%), Bing Chat (67.5%), and Bard (62.5%) gave high rates of inaccurate recommendations. Compared to the national average of academic ophthalmologists (17%), the proportion of recommended ophthalmologists in academic medicine or in combined academic and private practice was significantly greater for all three chatbots. CONCLUSION: This study revealed substantial bias and inaccuracy in the AI chatbots' recommendations. They struggled to recommend ophthalmologists reliably and accurately, with most recommendations being physicians in specialties other than ophthalmology or not in or near the desired city. Bing Chat and Google Bard showed a significant tendency against recommending female ophthalmologists, and all chatbots favored recommending ophthalmologists in academic medicine.",
      "journal": "Cureus",
      "year": "2023",
      "doi": "10.7759/cureus.45911",
      "authors": "Oca Michael C et al.",
      "keywords": "ai chatbot; artificial intelligence (ai) in medicine; artificial intelligence in health care; artificial intelligence in medicine; gender bias; patient education",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37885556/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Ophthalmology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: This study revealed substantial bias and inaccuracy in the AI chatbots' recommendations. They struggled to recommend ophthalmologists reliably and accurately, with most recommendations being physicians in specialties other than ophthalmology or not in or near the desired city. Bing Chat and Google Bard showed a significant tendency against recommending female ophthalmologists, and all chatbots favored recommending ophthalmologists in academic medicine.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10599183"
    },
    {
      "pmid": "37917120",
      "title": "The Accuracy and Potential Racial and Ethnic Biases of GPT-4 in the Diagnosis and Triage of Health Conditions: Evaluation Study.",
      "abstract": "BACKGROUND: Whether GPT-4, the conversational artificial intelligence, can accurately diagnose and triage health conditions and whether it presents racial and ethnic biases in its decisions remain unclear. OBJECTIVE: We aim to assess the accuracy of GPT-4 in the diagnosis and triage of health conditions and whether its performance varies by patient race and ethnicity. METHODS: We compared the performance of GPT-4 and physicians, using 45 typical clinical vignettes, each with a correct diagnosis and triage level, in February and March 2023. For each of the 45 clinical vignettes, GPT-4 and 3 board-certified physicians provided the most likely primary diagnosis and triage level (emergency, nonemergency, or self-care). Independent reviewers evaluated the diagnoses as \"correct\" or \"incorrect.\" Physician diagnosis was defined as the consensus of the 3 physicians. We evaluated whether the performance of GPT-4 varies by patient race and ethnicity, by adding the information on patient race and ethnicity to the clinical vignettes. RESULTS: The accuracy of diagnosis was comparable between GPT-4 and physicians (the percentage of correct diagnosis was 97.8% (44/45; 95% CI 88.2%-99.9%) for GPT-4 and 91.1% (41/45; 95% CI 78.8%-97.5%) for physicians; P=.38). GPT-4 provided appropriate reasoning for 97.8% (44/45) of the vignettes. The appropriateness of triage was comparable between GPT-4 and physicians (GPT-4: 30/45, 66.7%; 95% CI 51.0%-80.0%; physicians: 30/45, 66.7%; 95% CI 51.0%-80.0%; P=.99). The performance of GPT-4 in diagnosing health conditions did not vary among different races and ethnicities (Black, White, Asian, and Hispanic), with an accuracy of 100% (95% CI 78.2%-100%). P values, compared to the GPT-4 output without incorporating race and ethnicity information, were all .99. The accuracy of triage was not significantly different even if patients' race and ethnicity information was added. The accuracy of triage was 62.2% (95% CI 46.5%-76.2%; P=.50) for Black patients; 66.7% (95% CI 51.0%-80.0%; P=.99) for White patients; 66.7% (95% CI 51.0%-80.0%; P=.99) for Asian patients, and 62.2% (95% CI 46.5%-76.2%; P=.69) for Hispanic patients. P values were calculated by comparing the outputs with and without conditioning on race and ethnicity. CONCLUSIONS: GPT-4's ability to diagnose and triage typical clinical vignettes was comparable to that of board-certified physicians. The performance of GPT-4 did not vary by patient race and ethnicity. These findings should be informative for health systems looking to introduce conversational artificial intelligence to improve the efficiency of patient diagnosis and triage.",
      "journal": "JMIR medical education",
      "year": "2023",
      "doi": "10.2196/47532",
      "authors": "Ito Naoki et al.",
      "keywords": "AI; GPT; GPT-4; artificial intelligence; bias; clinical vignettes; decision-making; diagnosis; efficiency; physician; race; racial and ethnic bias; triage; typical clinical vignettes",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37917120/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Emergency Medicine",
      "bias_axes": "Race/Ethnicity; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: GPT-4's ability to diagnose and triage typical clinical vignettes was comparable to that of board-certified physicians. The performance of GPT-4 did not vary by patient race and ethnicity. These findings should be informative for health systems looking to introduce conversational artificial intelligence to improve the efficiency of patient diagnosis and triage.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10654908"
    },
    {
      "pmid": "37963683",
      "title": "Algorithmic fairness in cardiovascular disease risk prediction: overcoming inequalities.",
      "abstract": "The main purpose of prognostic risk prediction models is to identify individuals who are at risk of disease, to enable early intervention. Current prognostic cardiovascular risk prediction models, such as the Systematic COronary Risk Evaluation (SCORE2) and the SCORE2-Older Persons (SCORE2-OP) models, which represent the clinically used gold standard in assessing patient risk for major cardiovascular events in the European Union (EU), generally overlook socioeconomic determinants, leading to disparities in risk prediction and resource allocation. A central recommendation of this article is the explicit inclusion of individual-level socioeconomic determinants of cardiovascular disease in risk prediction models. The question of whether prognostic risk prediction models can promote health equity remains to be answered through experimental research, potential clinical implementation and public health analysis. This paper introduces four distinct fairness concepts in cardiovascular disease prediction and their potential to narrow existing disparities in cardiometabolic health.",
      "journal": "Open heart",
      "year": "2023",
      "doi": "10.1136/openhrt-2023-002395",
      "authors": "Varga Tibor V",
      "keywords": "biomarkers; delivery of health care; epidemiology; ethics, medical",
      "mesh_terms": "Humans; Aged; Aged, 80 and over; Health Promotion; Cardiovascular Diseases; Socioeconomic Factors; Prognosis",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37963683/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Cardiology; Public Health",
      "bias_axes": "Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "The question of whether prognostic risk prediction models can promote health equity remains to be answered through experimental research, potential clinical implementation and public health analysis. This paper introduces four distinct fairness concepts in cardiovascular disease prediction and their potential to narrow existing disparities in cardiometabolic health.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10649900"
    },
    {
      "pmid": "38020160",
      "title": "Artificial intelligence in global health equity: an evaluation and discussion on the application of ChatGPT, in the Chinese National Medical Licensing Examination.",
      "abstract": "BACKGROUND: The demand for healthcare is increasing globally, with notable disparities in access to resources, especially in Asia, Africa, and Latin America. The rapid development of Artificial Intelligence (AI) technologies, such as OpenAI's ChatGPT, has shown promise in revolutionizing healthcare. However, potential challenges, including the need for specialized medical training, privacy concerns, and language bias, require attention. METHODS: To assess the applicability and limitations of ChatGPT in Chinese and English settings, we designed an experiment evaluating its performance in the 2022 National Medical Licensing Examination (NMLE) in China. For a standardized evaluation, we used the comprehensive written part of the NMLE, translated into English by a bilingual expert. All questions were input into ChatGPT, which provided answers and reasons for choosing them. Responses were evaluated for \"information quality\" using the Likert scale. RESULTS: ChatGPT demonstrated a correct response rate of 81.25% for Chinese and 86.25% for English questions. Logistic regression analysis showed that neither the difficulty nor the subject matter of the questions was a significant factor in AI errors. The Brier Scores, indicating predictive accuracy, were 0.19 for Chinese and 0.14 for English, indicating good predictive performance. The average quality score for English responses was excellent (4.43 point), slightly higher than for Chinese (4.34 point). CONCLUSION: While AI language models like ChatGPT show promise for global healthcare, language bias is a key challenge. Ensuring that such technologies are robustly trained and sensitive to multiple languages and cultures is vital. Further research into AI's role in healthcare, particularly in areas with limited resources, is warranted.",
      "journal": "Frontiers in medicine",
      "year": "2023",
      "doi": "10.3389/fmed.2023.1237432",
      "authors": "Tong Wenting et al.",
      "keywords": "ChatGPT; artificial intelligence; equity; global healthcare; language bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38020160/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM; Logistic Regression",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: While AI language models like ChatGPT show promise for global healthcare, language bias is a key challenge. Ensuring that such technologies are robustly trained and sensitive to multiple languages and cultures is vital. Further research into AI's role in healthcare, particularly in areas with limited resources, is warranted.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10656681"
    },
    {
      "pmid": "38074628",
      "title": "Deep learning based tomosynthesis denoising: a bias investigation across different breast types.",
      "abstract": "PURPOSE: High noise levels due to low X-ray dose are a challenge in digital breast tomosynthesis (DBT) reconstruction. Deep learning algorithms show promise in reducing this noise. However, these algorithms can be complex and biased toward certain patient groups if the training data are not representative. It is important to thoroughly evaluate deep learning-based denoising algorithms before they are applied in the medical field to ensure their effectiveness and fairness. In this work, we present a deep learning-based denoising algorithm and examine potential biases with respect to breast density, thickness, and noise level. APPROACH: We use physics-driven data augmentation to generate low-dose images from full field digital mammography and train an encoder-decoder network. The rectified linear unit (ReLU)-loss, specifically designed for mammographic denoising, is utilized as the objective function. To evaluate our algorithm for potential biases, we tested it on both clinical and simulated data generated with the virtual imaging clinical trial for regulatory evaluation pipeline. Simulated data allowed us to generate X-ray dose distributions not present in clinical data, enabling us to separate the influence of breast types and X-ray dose on the denoising performance. RESULTS: Our results show that the denoising performance is proportional to the noise level. We found a bias toward certain breast groups on simulated data; however, on clinical data, our algorithm denoises different breast types equally well with respect to structural similarity index. CONCLUSIONS: We propose a robust deep learning-based denoising algorithm that reduces DBT projection noise levels and subject it to an extensive test that provides information about its strengths and weaknesses.",
      "journal": "Journal of medical imaging (Bellingham, Wash.)",
      "year": "2023",
      "doi": "10.1117/1.JMI.10.6.064003",
      "authors": "Eckert Dominik et al.",
      "keywords": "deep learning; denoising; mammography; noise simulation; tomosynthesis",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38074628/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Data Augmentation",
      "clinical_setting": "Clinical Trial",
      "key_findings": "CONCLUSIONS: We propose a robust deep learning-based denoising algorithm that reduces DBT projection noise levels and subject it to an extensive test that provides information about its strengths and weaknesses.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10704268"
    },
    {
      "pmid": "38076222",
      "title": "Race Correction and Algorithmic Bias in Atrial Fibrillation Wearable Technologies.",
      "abstract": "Stakeholders in biomedicine are evaluating how race corrections in clinical algorithms inequitably allocate health care resources on the basis of a misunderstanding of race-as-genetic difference. Ostensibly used to intervene on persistent disparities in health outcomes across different racial groups, these troubling corrections in risk assessments embed essentialist ideas of race as a biological reality, rather than a social and political construct that reproduces a racial hierarchy, into practice guidelines. This article explores the harms of such race corrections by considering how the technologies we use to account for disparities in health outcomes can actually innovate and amplify these harms. Focusing on the design of wearable digital health technologies that use photoplethysmographic sensors to detect atrial fibrillation, we argue that these devices, which are notoriously poor in accurately functioning on users with darker skin tones, embed a subtle form of race correction that presupposes the need for explicit adjustments in the clinical interpretation of their data outputs. We point to research on responsible innovation in health, and its commitment to being responsive in addressing inequities and harms, as a way forward for those invested in the elimination of race correction.",
      "journal": "Health equity",
      "year": "2023",
      "doi": "10.1089/heq.2023.0034",
      "authors": "Merid Beza et al.",
      "keywords": "African American; cardiovascular health; health disparities; technology",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38076222/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; Genomics/Genetics; Wearables/Remote Monitoring",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Telehealth/Remote",
      "key_findings": "Focusing on the design of wearable digital health technologies that use photoplethysmographic sensors to detect atrial fibrillation, we argue that these devices, which are notoriously poor in accurately functioning on users with darker skin tones, embed a subtle form of race correction that presupposes the need for explicit adjustments in the clinical interpretation of their data outputs. We point to research on responsible innovation in health, and its commitment to being responsive in addressi...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10698766"
    },
    {
      "pmid": "38106815",
      "title": "Validating racial and ethnic non-bias of artificial intelligence decision support for diagnostic breast ultrasound evaluation.",
      "abstract": "PURPOSE: Breast ultrasound suffers from low positive predictive value and specificity. Artificial intelligence (AI) proposes to improve accuracy, reduce false negatives, reduce inter- and intra-observer variability and decrease the rate of benign biopsies. Perpetuating racial/ethnic disparities in healthcare and patient outcome is a potential risk when incorporating AI-based models into clinical practice; therefore, it is necessary to validate its non-bias before clinical use. APPROACH: Our retrospective review assesses whether our AI decision support (DS) system demonstrates racial/ethnic bias by evaluating its performance on 1810 biopsy proven cases from nine breast imaging facilities within our health system from January 1, 2018 to October 28, 2021. Patient age, gender, race/ethnicity, AI DS output, and pathology results were obtained. RESULTS: Significant differences in breast pathology incidence were seen across different racial and ethnic groups. Stratified analysis showed that the difference in output by our AI DS system was due to underlying differences in pathology incidence for our specific cohort and did not demonstrate statistically significant bias in output among race/ethnic groups, suggesting similar effectiveness of our AI DS system among different races (p>0.05 for all). CONCLUSIONS: Our study shows promise that an AI DS system may serve as a valuable second opinion in the detection of breast cancer on diagnostic ultrasound without significant racial or ethnic bias. AI tools are not meant to replace the radiologist, but rather to aid in screening and diagnosis without perpetuating racial/ethnic disparities.",
      "journal": "Journal of medical imaging (Bellingham, Wash.)",
      "year": "2023",
      "doi": "10.1117/1.JMI.10.6.061108",
      "authors": "Koo Clara et al.",
      "keywords": "artificial intelligence; breast cancer; breast ultrasound; deep learning; racial and ethnicity bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38106815/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Oncology; Pathology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Subgroup Analysis",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "CONCLUSIONS: Our study shows promise that an AI DS system may serve as a valuable second opinion in the detection of breast cancer on diagnostic ultrasound without significant racial or ethnic bias. AI tools are not meant to replace the radiologist, but rather to aid in screening and diagnosis without perpetuating racial/ethnic disparities.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10721939"
    },
    {
      "pmid": "38153778",
      "title": "Empathy and Equity: Key Considerations for Large Language Model Adoption in Health Care.",
      "abstract": "The growing presence of large language models (LLMs) in health care applications holds significant promise for innovative advancements in patient care. However, concerns about ethical implications and potential biases have been raised by various stakeholders. Here, we evaluate the ethics of LLMs in medicine along 2 key axes: empathy and equity. We outline the importance of these factors in novel models of care and develop frameworks for addressing these alongside LLM deployment.",
      "journal": "JMIR medical education",
      "year": "2023",
      "doi": "10.2196/51199",
      "authors": "Koranteng Erica et al.",
      "keywords": "AI; ChatGPT; LLMs; artificial intelligence; bias; care; development; empathy; equity; ethical implication; ethics; framework; health care application; language model; large language models; model; patient care",
      "mesh_terms": "Humans; Empathy; Health Facilities; Language; Medicine; Delivery of Health Care",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38153778/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Here, we evaluate the ethics of LLMs in medicine along 2 key axes: empathy and equity. We outline the importance of these factors in novel models of care and develop frameworks for addressing these alongside LLM deployment.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10884892"
    },
    {
      "pmid": "38222427",
      "title": "Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint.",
      "abstract": "Clinical trials are indispensable in developing new treatments, but they face obstacles in patient recruitment and retention, hindering the enrollment of necessary participants. To tackle these challenges, deep learning frameworks have been created to match patients to trials. These frameworks calculate the similarity between patients and clinical trial eligibility criteria, considering the discrepancy between inclusion and exclusion criteria. Recent studies have shown that these frameworks outperform earlier approaches. However, deep learning models may raise fairness issues in patient-trial matching when certain sensitive groups of individuals are underrepresented in clinical trials, leading to incomplete or inaccurate data and potential harm. To tackle the issue of fairness, this work proposes a fair patient-trial matching framework by generating a patient-criterion level fairness constraint. The proposed framework considers the inconsistency between the embedding of inclusion and exclusion criteria among patients of different sensitive groups. The experimental results on real-world patient-trial and patient-criterion matching tasks demonstrate that the proposed framework can successfully alleviate the predictions that tend to be biased.",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "year": "2023",
      "doi": "",
      "authors": "Chang Chia-Yuan et al.",
      "keywords": "",
      "mesh_terms": "Humans; Patient Selection; Clinical Trials as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38222427/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; NLP/LLM",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Fairness Constraints; Representation Learning",
      "clinical_setting": "Clinical Trial",
      "key_findings": "The proposed framework considers the inconsistency between the embedding of inclusion and exclusion criteria among patients of different sensitive groups. The experimental results on real-world patient-trial and patient-criterion matching tasks demonstrate that the proposed framework can successfully alleviate the predictions that tend to be biased.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10785912"
    },
    {
      "pmid": "38567296",
      "title": "Muffin: A Framework Toward Multi-Dimension AI Fairness by Uniting Off-the-Shelf Models.",
      "abstract": "Model fairness (a.k.a., bias) has become one of the most critical problems in a wide range of AI applications. An unfair model in autonomous driving may cause a traffic accident if corner cases (e.g., extreme weather) cannot be fairly regarded; or it will incur healthcare disparities if the AI model misdiagnoses a certain group of people (e.g., brown and black skin). In recent years, there are emerging research works on addressing unfairness, and they mainly focus on a single unfair attribute, like skin tone; however, real-world data commonly have multiple attributes, among which unfairness can exist in more than one attribute, called \"multi-dimensional fairness\". In this paper, we first reveal a strong correlation between the different unfair attributes, i.e., optimizing fairness on one attribute will lead to the collapse of others. Then, we propose a novel Multi-Dimension Fairness framework, namely Muffin, which includes an automatic tool to unite off-the-shelf models to improve the fairness on multiple attributes simultaneously. Case studies on dermatology datasets with two unfair attributes show that the existing approach can achieve 21.05% fairness improvement on the first attribute while it makes the second attribute unfair by 1.85%. On the other hand, the proposed Muffin can unite multiple models to achieve simultaneously 26.32% and 20.37% fairness improvement on both attributes; meanwhile, it obtains 5.58% accuracy gain.",
      "journal": "Proceedings. Design Automation Conference",
      "year": "2023",
      "doi": "10.1109/dac56929.2023.10247765",
      "authors": "Sheng Yi et al.",
      "keywords": "model fusing; multi-dimensional fairness; parameters; reinforcement learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38567296/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Dermatology",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Case studies on dermatology datasets with two unfair attributes show that the existing approach can achieve 21.05% fairness improvement on the first attribute while it makes the second attribute unfair by 1.85%. On the other hand, the proposed Muffin can unite multiple models to achieve simultaneously 26.32% and 20.37% fairness improvement on both attributes; meanwhile, it obtains 5.58% accuracy gain.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10987014"
    },
    {
      "pmid": "38875540",
      "title": "Developing Ethics and Equity Principles, Terms, and Engagement Tools to Advance Health Equity and Researcher Diversity in AI and Machine Learning: Modified Delphi Approach.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) and machine learning (ML) technology design and development continues to be rapid, despite major limitations in its current form as a practice and discipline to address all sociohumanitarian issues and complexities. From these limitations emerges an imperative to strengthen AI and ML literacy in underserved communities and build a more diverse AI and ML design and development workforce engaged in health research. OBJECTIVE: AI and ML has the potential to account for and assess a variety of factors that contribute to health and disease and to improve prevention, diagnosis, and therapy. Here, we describe recent activities within the Artificial Intelligence/Machine Learning Consortium to Advance Health Equity and Researcher Diversity (AIM-AHEAD) Ethics and Equity Workgroup (EEWG) that led to the development of deliverables that will help put ethics and fairness at the forefront of AI and ML applications to build equity in biomedical research, education, and health care. METHODS: The AIM-AHEAD EEWG was created in 2021 with 3 cochairs and 51 members in year 1 and 2 cochairs and ~40 members in year 2. Members in both years included AIM-AHEAD principal investigators, coinvestigators, leadership fellows, and research fellows. The EEWG used a modified Delphi approach using polling, ranking, and other exercises to facilitate discussions around tangible steps, key terms, and definitions needed to ensure that ethics and fairness are at the forefront of AI and ML applications to build equity in biomedical research, education, and health care. RESULTS: The EEWG developed a set of ethics and equity principles, a glossary, and an interview guide. The ethics and equity principles comprise 5 core principles, each with subparts, which articulate best practices for working with stakeholders from historically and presently underrepresented communities. The glossary contains 12 terms and definitions, with particular emphasis on optimal development, refinement, and implementation of AI and ML in health equity research. To accompany the glossary, the EEWG developed a concept relationship diagram that describes the logical flow of and relationship between the definitional concepts. Lastly, the interview guide provides questions that can be used or adapted to garner stakeholder and community perspectives on the principles and glossary. CONCLUSIONS: Ongoing engagement is needed around our principles and glossary to identify and predict potential limitations in their uses in AI and ML research settings, especially for institutions with limited resources. This requires time, careful consideration, and honest discussions around what classifies an engagement incentive as meaningful to support and sustain their full engagement. By slowing down to meet historically and presently underresourced institutions and communities where they are and where they are capable of engaging and competing, there is higher potential to achieve needed diversity, ethics, and equity in AI and ML implementation in health research.",
      "journal": "JMIR AI",
      "year": "2023",
      "doi": "10.2196/52888",
      "authors": "Hendricks-Sturrup Rachele et al.",
      "keywords": "AI; Delphi; ML; artificial intelligence; disparities; disparity; engagement; equitable; equities; equity; ethic; ethical; ethics; fair; fairness; health disparities; health equity; humanitarian; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38875540/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population; Safety-Net/Underserved",
      "key_findings": "CONCLUSIONS: Ongoing engagement is needed around our principles and glossary to identify and predict potential limitations in their uses in AI and ML research settings, especially for institutions with limited resources. This requires time, careful consideration, and honest discussions around what classifies an engagement incentive as meaningful to support and sustain their full engagement. By slowing down to meet historically and presently underresourced institutions and communities where they ...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11041493"
    },
    {
      "pmid": "39435256",
      "title": "Ethicara for Responsible AI in Healthcare: A System for Bias Detection and AI Risk Management.",
      "abstract": "The increasing torrents of health AI innovations hold promise for facilitating the delivery of patient-centered care. Yet the enablement and adoption of AI innovations in the healthcare and life science industries can be challenging with the rising concerns of AI risks and the potential harms to health equity. This paper describes Ethicara, a system that enables health AI risk assessment for responsible AI model development. Ethicara works by orchestrating a collection of self-analytics services that detect and mitigate bias and increase model transparency from harmonized data models. For the lack of risk controls currently in the health AI development and deployment process, the self-analytics tools enhanced by Ethicara are expected to provide repeatable and measurable controls to operationalize voluntary risk management frameworks and guidelines (e.g., NIST RMF, FDA GMLP) and regulatory requirements emerging from the upcoming AI regulations (e.g., EU AI Act, US Blueprint for an AI Bill of Rights). In addition, Ethicara provides plug-ins via which analytics results are incorporated into healthcare applications. This paper provides an overview of Ethicara's architecture, pipeline, and technical components and showcases the system's capability to facilitate responsible AI use, and exemplifies the types of AI risk controls it enables in the healthcare and life science industry.",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "year": "2023",
      "doi": "",
      "authors": "Kritharidou Maria et al.",
      "keywords": "",
      "mesh_terms": "Risk Management; Artificial Intelligence; Humans; Delivery of Health Care; Risk Assessment; Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/39435256/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Neural Network",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "In addition, Ethicara provides plug-ins via which analytics results are incorporated into healthcare applications. This paper provides an overview of Ethicara's architecture, pipeline, and technical components and showcases the system's capability to facilitate responsible AI use, and exemplifies the types of AI risk controls it enables in the healthcare and life science industry.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11492113"
    },
    {
      "pmid": "40046549",
      "title": "Misguided Artificial Intelligence: How Racial Bias is Built Into Clinical Models.",
      "abstract": "Artificial Intelligence is being used today to solve a myriad of problems. While there is significant promise that AI can help us address many healthcare issues, there is also concern that health inequities can be exacerbated. This article looks specifically at predictive models in regards to racial bias. Each phase of the model building process including raw data collection and processing, data labelling, and implementation of the model can be subject to racial bias. This article aims to explore some of the ways in which this occurs.",
      "journal": "The Brown journal of hospital medicine",
      "year": "2023",
      "doi": "10.56305/001c.38021",
      "authors": "Jindal Atin",
      "keywords": "artificial intelligence; bias; health equity; machine learning; racial bias; racism",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40046549/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Each phase of the model building process including raw data collection and processing, data labelling, and implementation of the model can be subject to racial bias. This article aims to explore some of the ways in which this occurs.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11878858"
    },
    {
      "pmid": "40933771",
      "title": "Bias correction for nonignorable missing counts of areal HIV new diagnosis.",
      "abstract": "Public health data, such as HIV new diagnoses, are often left-censored due to confidentiality issues. Standard analysis approaches that assume censored values as missing at random often lead to biased estimates and inferior predictions. Motivated by the Philadelphia areal counts of HIV new diagnosis for which all values less than or equal to 5 are suppressed, we propose two methods to reduce the adverse influence of missingness on predictions and imputation of areal HIV new diagnoses. One is the likelihood-based method that integrates the missing mechanism into the likelihood function, and the other is a nonparametric algorithm for matrix factorization imputation. Numerical studies and the Philadelphia data analysis demonstrate that the two proposed methods can significantly improve prediction and imputation based on left-censored HIV data. We also compare the two methods on their robustness to model misspecification and find that both methods appear to be robust for prediction, while their performance for imputation depends on model specification.",
      "journal": "Stat",
      "year": "2023",
      "doi": "10.1002/sta4.555",
      "authors": "Qu Tianyi et al.",
      "keywords": "left-censored; likelihood; matrix factorization; missing value; spatiotemporal data",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/40933771/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health; Infectious Disease",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Numerical studies and the Philadelphia data analysis demonstrate that the two proposed methods can significantly improve prediction and imputation based on left-censored HIV data. We also compare the two methods on their robustness to model misspecification and find that both methods appear to be robust for prediction, while their performance for imputation depends on model specification.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12419480"
    },
    {
      "pmid": "34460977",
      "title": "Contextual bias, the democratization of healthcare, and medical artificial intelligence in low- and middle-income countries.",
      "abstract": "Medical artificial intelligence (MAI) creates an opportunity to radically expand access to healthcare across the globe by allowing us to overcome the persistent labor shortages that limit healthcare access. This democratization of healthcare is the greatest moral promise of MAI. Whatever comes of the enthusiastic discourse about the ability of MAI to improve the state-of-the-art in high-income countries (HICs), it will be far less impactful than improving the desperate state-of-the-actual in low- and middle-income countries (LMICs). However, the almost exclusive development of MAI in HICs risks this promise being thwarted by contextual bias, an algorithmic bias that arises when the context of the training data is significantly dissimilar from potential contexts of application, which makes the unreflective application of HIC-based MAI in LMIC contexts dangerous. The use of MAI in LMICs demands careful attention to context. In this paper, I aim to provide that attention. First, I illustrate the dire state of healthcare in LMICs and the hope that MAI may help us to improve this state. Next, I show that the radical differences between the health contexts of HICs and those of LMICs create an extraordinary risk of contextual bias. Then, I explore ethical challenges raised by this risk, and propose policies that will help to overcome those challenges. Finally, I sketch a wide range of related issues that need to be addressed to ensure that MAI has a positive impact on LMICs-and is able to improve, rather than worsen, global health equity.",
      "journal": "Bioethics",
      "year": "2022",
      "doi": "10.1111/bioe.12927",
      "authors": "Weissglass Daniel E",
      "keywords": "contextual bias; health disparity; health equity; low- and middle-income countries; medical artificial intelligence",
      "mesh_terms": "Artificial Intelligence; Developing Countries; Health Equity; Health Services Accessibility; Humans; Poverty",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34460977/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Then, I explore ethical challenges raised by this risk, and propose policies that will help to overcome those challenges. Finally, I sketch a wide range of related issues that need to be addressed to ensure that MAI has a positive impact on LMICs-and is able to improve, rather than worsen, global health equity.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "34542183",
      "title": "Highlighting psychological pain avoidance and decision-making bias as key predictors of suicide attempt in major depressive disorder-A novel investigative approach using machine learning.",
      "abstract": "OBJECTIVE: Predicting suicide is notoriously difficult and complex, but a serious public health issue. An innovative approach utilizing machine learning (ML) that incorporates features of psychological mechanisms and decision-making characteristics related to suicidality could create an improved model for identifying suicide risk in patients with major depressive disorder (MDD). METHOD: Forty-four patients with MDD and past suicide attempts (MDD_SA, N\u2009=\u200944); 48 patients with MDD but without past suicide attempts (MDD_NS, N\u2009=\u200948-42 of whom with suicide ideation [MDD_SI, N\u2009=\u200942]), and healthy controls (HCs, N\u2009=\u200951) completed seven psychometric assessments including the Three-dimensional\u2002Psychological Pain Scale (TDPPS), and one behavioral assessment, the Balloon Analogue Risk Task (BART). Descriptive statistics, group comparisons, logistic regressions, and ML were used to explore and compare the groups and generate predictors of suicidal acts. RESULTS: MDD_SA and MDD_NS differed in TDPPS\u2002total score, pain arousal and avoidance subscale scores, suicidal ideation scores, and relevant decision-making indicators in BART. Logistic regression tests linked suicide attempts to psychological pain avoidance and a risk decision-making indicator. The resultant key ML model distinguished MDD_SA/MDD_NS with 88.2% accuracy. The model could also distinguish MDD_SA/MDD_SI with 81.25% accuracy. The ML model using hopelessness could classify MDD_SI/HC with 94.4% accuracy. CONCLUSION: ML analyses showed that motivation to avoid intolerable psychological pain, coupled with impaired decision-making bias toward under-valuing life's worth are highly predictive of suicide attempts. Analyses also demonstrated that suicidal ideation and attempts differed in potential mechanisms, as suicidal ideation was more related to hopelessness. ML algorithms show useful promises as a predictive instrument.",
      "journal": "Journal of clinical psychology",
      "year": "2022",
      "doi": "10.1002/jclp.23246",
      "authors": "Ji Xinlei et al.",
      "keywords": "machine learning; major depressive disorder; psychological pain; risk decision-making; suicide",
      "mesh_terms": "Major Depressive Disorder; Humans; Machine Learning; Pain; Suicidal Ideation; Suicide, Attempted",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34542183/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Logistic Regression",
      "health_domain": "Mental Health/Psychiatry; ICU/Critical Care; Public Health; Pain Management",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: ML analyses showed that motivation to avoid intolerable psychological pain, coupled with impaired decision-making bias toward under-valuing life's worth are highly predictive of suicide attempts. Analyses also demonstrated that suicidal ideation and attempts differed in potential mechanisms, as suicidal ideation was more related to hopelessness. ML algorithms show useful promises as a predictive instrument.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "34637384",
      "title": "Mix-and-Interpolate: A Training Strategy to Deal With Source-Biased Medical Data.",
      "abstract": "Till March 31st, 2021, the coronavirus disease 2019 (COVID-19) had reportedly infected more than 127 million people and caused over 2.5 million deaths worldwide. Timely diagnosis of COVID-19 is crucial for management of individual patients as well as containment of the highly contagious disease. Having realized the clinical value of non-contrast chest computed tomography (CT) for diagnosis of COVID-19, deep learning (DL) based automated methods have been proposed to aid the radiologists in reading the huge quantities of CT exams as a result of the pandemic. In this work, we address an overlooked problem for training deep convolutional neural networks for COVID-19 classification using real-world multi-source data, namely, the data source bias problem. The data source bias problem refers to the situation in which certain sources of data comprise only a single class of data, and training with such source-biased data may make the DL models learn to distinguish data sources instead of COVID-19. To overcome this problem, we propose MIx-aNd-Interpolate (MINI), a conceptually simple, easy-to-implement, efficient yet effective training strategy. The proposed MINI approach generates volumes of the absent class by combining the samples collected from different hospitals, which enlarges the sample space of the original source-biased dataset. Experimental results on a large collection of real patient data (1,221 COVID-19 and 1,520 negative CT images, and the latter consisting of 786 community acquired pneumonia and 734 non-pneumonia) from eight hospitals and health institutions show that: 1) MINI can improve COVID-19 classification performance upon the baseline (which does not deal with the source bias), and 2) MINI is superior to competing methods in terms of the extent of improvement.",
      "journal": "IEEE journal of biomedical and health informatics",
      "year": "2022",
      "doi": "10.1109/JBHI.2021.3119325",
      "authors": "Li Yuexiang et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; COVID-19; Deep Learning; Humans; Pandemics; SARS-CoV-2",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34637384/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Pulmonology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; Public Health/Population",
      "key_findings": "The proposed MINI approach generates volumes of the absent class by combining the samples collected from different hospitals, which enlarges the sample space of the original source-biased dataset. Experimental results on a large collection of real patient data (1,221 COVID-19 and 1,520 negative CT images, and the latter consisting of 786 community acquired pneumonia and 734 non-pneumonia) from eight hospitals and health institutions show that: 1) MINI can improve COVID-19 classification performa...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8908883"
    },
    {
      "pmid": "34895784",
      "title": "Willingness to vaccinate against SARS-CoV-2: The role of reasoning biases and conspiracist ideation.",
      "abstract": "UNLABELLED: BACKGR1OUND: Widespread vaccine hesitancy and refusal complicate containment of the SARS-CoV-2 pandemic. Extant research indicates that biased reasoning and conspiracist ideation discourage vaccination. However, causal pathways from these constructs to vaccine hesitancy and refusal remain underspecified, impeding efforts to intervene and increase vaccine uptake. METHOD: 554 participants who denied prior SARS-CoV-2 vaccination completed self-report measures of SARS-CoV-2 vaccine intentions, conspiracist ideation, and constructs from the Health Belief Model of medical decision-making (such as perceived vaccine dangerousness) along with tasks measuring reasoning biases (such as those concerning data gathering behavior). Cutting-edge machine learning algorithms (Greedy Fast Causal Inference) and psychometric network analysis were used to elucidate causal pathways to (and from) vaccine intentions. RESULTS: Results indicated that a bias toward reduced data gathering during reasoning may cause paranoia, increasing the perceived dangerousness of vaccines and thereby reducing willingness to vaccinate. Existing interventions that target data gathering and paranoia therefore hold promise for encouraging vaccination. Additionally, reduced willingness to vaccinate was identified as a likely cause of belief in conspiracy theories, subverting the common assumption that the opposite causal relation exists. Finally, perceived severity of SARS-CoV-2 infection and perceived vaccine dangerousness (but not effectiveness) were potential direct causes of willingness to vaccinate, providing partial support for the Health Belief Model's applicability to SARS-CoV-2 vaccine decisions. CONCLUSIONS: These insights significantly advance our understanding of the underpinnings of vaccine intentions and should scaffold efforts to prepare more effective interventions on hesitancy for deployment during future pandemics.",
      "journal": "Vaccine",
      "year": "2022",
      "doi": "10.1016/j.vaccine.2021.11.079",
      "authors": "Bronstein Michael V et al.",
      "keywords": "COVID-19; Conspiracy theories; GFCI; Reasoning; SARS-CoV-2; Vaccines",
      "mesh_terms": "Bias; COVID-19; COVID-19 Vaccines; Humans; SARS-CoV-2; Vaccination; Vaccination Hesitancy",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34895784/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Pulmonology; Infectious Disease",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Counterfactual Fairness",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: These insights significantly advance our understanding of the underpinnings of vaccine intentions and should scaffold efforts to prepare more effective interventions on hesitancy for deployment during future pandemics.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8642163"
    },
    {
      "pmid": "34918101",
      "title": "Gender-sensitive word embeddings for healthcare.",
      "abstract": "OBJECTIVE: To analyze gender bias in clinical trials, to design an algorithm that mitigates the effects of biases of gender representation on natural-language (NLP) systems trained on text drawn from clinical trials, and to evaluate its performance. MATERIALS AND METHODS: We analyze gender bias in clinical trials described by 16\u00a0772 PubMed abstracts (2008-2018). We present a method to augment word embeddings, the core building block of NLP-centric representations, by weighting abstracts by the number of women participants in the trial. We evaluate the resulting gender-sensitive embeddings performance on several clinical prediction tasks: comorbidity classification, hospital length of stay prediction, and intensive care unit (ICU) readmission prediction. RESULTS: For female patients, the gender-sensitive model area under the receiver-operator characteristic (AUROC) is 0.86 versus the baseline of 0.81 for comorbidity classification, mean absolute error 4.59 versus the baseline of 4.66 for length of stay prediction, and AUROC 0.69 versus 0.67 for ICU readmission. All results are statistically significant. DISCUSSION: Women have been underrepresented in clinical trials. Thus, using the broad clinical trials literature as training data for statistical language models could result in biased models, with deficits in knowledge about women. The method presented enables gender-sensitive use of publications as training data for word embeddings. In experiments, the gender-sensitive embeddings show better performance than baseline embeddings for the clinical tasks studied. The results highlight opportunities for recognizing and addressing gender and other representational biases in the clinical trials literature. CONCLUSION: Addressing representational biases in data for training NLP embeddings can lead to better results on downstream tasks for underrepresented populations.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2022",
      "doi": "10.1093/jamia/ocab279",
      "authors": "Agmon Shunit et al.",
      "keywords": "algorithms; bias; gender; statistical models; word embeddings",
      "mesh_terms": "Clinical Trials as Topic; Delivery of Health Care; Female; Humans; Male; Natural Language Processing; PubMed; Sexism",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34918101/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "NLP/LLM; Clinical Prediction Model",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Representation Learning",
      "clinical_setting": "Hospital/Inpatient; ICU; Public Health/Population; Clinical Trial",
      "key_findings": "CONCLUSION: Addressing representational biases in data for training NLP embeddings can lead to better results on downstream tasks for underrepresented populations.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8800511"
    },
    {
      "pmid": "34951033",
      "title": "Automatic coronavirus disease 2019 diagnosis based on chest radiography and deep learning - Success story or dataset bias?",
      "abstract": "PURPOSE: Over the last 2 years, the artificial intelligence (AI) community has presented several automatic screening tools for coronavirus disease 2019 (COVID-19) based on chest radiography (CXR), with reported accuracies often well over 90%. However, it has been noted that many of these studies have likely suffered from dataset bias, leading to overly optimistic results. The purpose of this study was to thoroughly investigate to what extent biases have influenced the performance of a range of previously proposed and promising convolutional neural networks (CNNs), and to determine what performance can be expected with current CNNs on a realistic and unbiased dataset. METHODS: Five CNNs for COVID-19 positive/negative classification were implemented for evaluation, namely VGG19, ResNet50, InceptionV3, DenseNet201, and COVID-Net. To perform both internal and cross-dataset evaluations, four datasets were created. The first dataset Valencian Region Medical Image Bank (BIMCV) followed strict reverse transcriptase-polymerase chain reaction (RT-PCR) test criteria and was created from a single reliable open access databank, while the second dataset (COVIDxB8) was created through a combination of six online CXR repositories. The third and fourth datasets were created by combining the opposing classes from the BIMCV and COVIDxB8 datasets. To decrease inter-dataset variability, a pre-processing workflow of resizing, normalization, and histogram equalization were applied to all datasets. Classification performance was evaluated on unseen test sets using precision and recall. A qualitative sanity check was performed by evaluating saliency maps displaying the top 5%, 10%, and 20% most salient segments in the input CXRs, to evaluate whether the CNNs were using relevant information for decision making. In an additional experiment and to further investigate the origin of potential dataset bias, all pixel values outside the lungs were set to zero through automatic lung segmentation before training and testing. RESULTS: When trained and evaluated on the single online source dataset (BIMCV), the performance of all CNNs is relatively low (precision: 0.65-0.72, recall: 0.59-0.71), but remains relatively consistent during external evaluation (precision: 0.58-0.82, recall: 0.57-0.72). On the contrary, when trained and internally evaluated on the combinatory datasets, all CNNs performed well across all metrics (precision: 0.94-1.00, recall: 0.77-1.00). However, when subsequently evaluated cross-dataset, results dropped substantially (precision: 0.10-0.61, recall: 0.04-0.80). For all datasets, saliency maps revealed the CNNs rarely focus on areas inside the lungs for their decision-making. However, even when setting all pixel values outside the lungs to zero, classification performance does not change and dataset bias remains. CONCLUSIONS: Results in this study confirm that when trained on a combinatory dataset, CNNs tend to learn the origin of the CXRs rather than the presence or absence of disease, a behavior known as short-cut learning. The bias is shown to originate from differences in overall pixel values rather than embedded text or symbols, despite consistent image pre-processing. When trained on a reliable, and realistic single-source dataset in which non-lung pixels have been masked, CNNs currently show limited sensitivity\u00a0(<70%) for COVID-19 infection in CXR, questioning their use as a reliable automatic screening tool.",
      "journal": "Medical physics",
      "year": "2022",
      "doi": "10.1002/mp.15419",
      "authors": "Dhont Jennifer et al.",
      "keywords": "COVID-19; X-ray imaging; artificial intelligence; dataset bias",
      "mesh_terms": "Artificial Intelligence; Bias; COVID-19; Deep Learning; Humans; Radiography; SARS-CoV-2",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34951033/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Radiology/Medical Imaging; Pulmonology; Infectious Disease",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Data Preprocessing; Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: Results in this study confirm that when trained on a combinatory dataset, CNNs tend to learn the origin of the CXRs rather than the presence or absence of disease, a behavior known as short-cut learning. The bias is shown to originate from differences in overall pixel values rather than embedded text or symbols, despite consistent image pre-processing. When trained on a reliable, and realistic single-source dataset in which non-lung pixels have been masked, CNNs currently show limit...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9015341"
    },
    {
      "pmid": "35016819",
      "title": "Investigation of biases in convolutional neural networks for semantic segmentation using performance sensitivity analysis.",
      "abstract": "The application of deep neural networks for segmentation in medical imaging has gained substantial interest in recent years. In many cases, this variant of machine learning has been shown to outperform other conventional segmentation approaches. However, little is known about its general applicability. Especially the robustness against image modifications (e.g., intensity variations, contrast variations, spatial alignment) has hardly been investigated. Data augmentation is often used to compensate for sensitivity to such changes, although its effectiveness has not yet been studied. Therefore, the goal of this study was to systematically investigate the sensitivity to variations in input data with respect to segmentation of medical images using deep learning. This approach was tested with two publicly available segmentation frameworks (DeepMedic and TractSeg). In the case of DeepMedic, the performance was tested using ground truth data, while in the case of TractSeg, the STAPLE technique was employed. In both cases, sensitivity analysis revealed significant dependence of the segmentation performance on input variations. The effects of different data augmentation strategies were also shown, making this type of analysis a useful tool for selecting the right parameters for augmentation. The proposed analysis should be applied to any deep learning image segmentation approach, unless the assessment of sensitivity to input variations can be directly derived from the network.",
      "journal": "Zeitschrift fur medizinische Physik",
      "year": "2022",
      "doi": "10.1016/j.zemedi.2021.11.004",
      "authors": "G\u00fcllmar Daniel et al.",
      "keywords": "Convolutional neural network; Data augmentation; Semantic image segmentation; Sensitivity analysis",
      "mesh_terms": "Bias; Image Processing, Computer-Assisted; Machine Learning; Neural Networks, Computer; Semantics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35016819/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Neural Network; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Data Augmentation",
      "clinical_setting": "Not specified",
      "key_findings": "The effects of different data augmentation strategies were also shown, making this type of analysis a useful tool for selecting the right parameters for augmentation. The proposed analysis should be applied to any deep learning image segmentation approach, unless the assessment of sensitivity to input variations can be directly derived from the network.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9948839"
    },
    {
      "pmid": "35024857",
      "title": "Considerations for development of child abuse and neglect phenotype with implications for reduction of racial bias: a qualitative study.",
      "abstract": "OBJECTIVE: The study provides considerations for generating a phenotype of child abuse and neglect in Emergency Departments (ED) using secondary data from electronic health records (EHR). Implications will be provided for racial bias reduction and the development of further decision support tools to assist in identifying child abuse and neglect. MATERIALS AND METHODS: We conducted a qualitative study using in-depth interviews with 20 pediatric clinicians working in a single pediatric ED to gain insights about generating an EHR-based phenotype to identify children at risk for abuse and neglect. RESULTS: Three central themes emerged from the interviews: (1) Challenges in diagnosing child abuse and neglect, (2) Health Discipline Differences in Documentation Styles in EHR, and (3) Identification of potential racial bias through documentation. DISCUSSION: Our findings highlight important considerations for generating a phenotype for child abuse and neglect using EHR data. First, information-related challenges include lack of proper previous visit history due to limited information exchanges and scattered documentation within EHRs. Second, there are differences in documentation styles by health disciplines, and clinicians tend to document abuse in different document types within EHRs. Finally, documentation can help identify potential racial bias in suspicion of child abuse and neglect by revealing potential discrepancies in quality of care, and in the language used to document abuse and neglect. CONCLUSIONS: Our findings highlight challenges in building an EHR-based risk phenotype for child abuse and neglect. Further research is needed to validate these findings and integrate them into creation of an EHR-based risk phenotype.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2022",
      "doi": "10.1093/jamia/ocab275",
      "authors": "Landau Aviv Y et al.",
      "keywords": "child abuse and neglect; clinical decision support tool; electronic health records; pediatric emergency departments; racial bias",
      "mesh_terms": "Child; Child Abuse; Documentation; Electronic Health Records; Humans; Phenotype; Qualitative Research; Racism",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35024857/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "Emergency Medicine; EHR/Health Informatics; Pediatrics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Emergency Department",
      "key_findings": "CONCLUSIONS: Our findings highlight challenges in building an EHR-based risk phenotype for child abuse and neglect. Further research is needed to validate these findings and integrate them into creation of an EHR-based risk phenotype.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8800508"
    },
    {
      "pmid": "35033879",
      "title": "Understanding the bias in machine learning systems for cardiovascular disease risk assessment: The first of its kind review.",
      "abstract": "BACKGROUND: Artificial Intelligence (AI), in particular, machine learning (ML) has shown promising results in coronary artery disease (CAD) or cardiovascular disease (CVD) risk prediction. Bias in ML systems is of great interest due to its over-performance and poor clinical delivery. The main objective is to understand the nature of risk-of-bias (RoB) in ML and non-ML studies for CVD risk prediction. METHODS: PRISMA model was used to shortlisting 117 studies, which were analyzed to understand the RoB in ML and non-ML using 46 and 32 attributes, respectively. The mean score for each study was computed and then ranked into three ML and non-ML bias categories, namely low-bias (LB), moderate-bias (MB), and high-bias (HB), derived using two cutoffs. Further, bias computation was validated using the analytical slope method. RESULTS: Five types of the gold standard were identified in the ML design for CAD/CVD risk prediction. The low-moderate and moderate-high bias cutoffs for 24\u00a0ML studies (5, 10, and 9 studies for each LB, MB, and HB) and 14 non-ML (3, 4, and 7 studies for each LB, MB, and HB) were in the range of 1.5 to 1.95. BiasML<\u00a0Biasnon-ML by \u223c43%. A set of recommendations were proposed for lowering RoB. CONCLUSION: ML showed a lower bias compared to non-ML. For a robust ML-based CAD/CVD prediction design, it is vital to have (i) stronger outcomes like death or CAC score or coronary artery stenosis; (ii) ensuring scientific/clinical validation; (iii) adaptation of multiethnic groups while practicing unseen AI; (iv) amalgamation of conventional, laboratory, image-based and medication-based biomarkers.",
      "journal": "Computers in biology and medicine",
      "year": "2022",
      "doi": "10.1016/j.compbiomed.2021.105204",
      "authors": "Suri Jasjit S et al.",
      "keywords": "And unseen data; Artificial intelligence; Bias; Carotid ultrasound; Coronary artery disease; Gold standard; Risk prediction",
      "mesh_terms": "Artificial Intelligence; Cardiovascular Diseases; Coronary Artery Disease; Coronary Stenosis; Humans; Machine Learning; Risk Assessment",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35033879/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Cardiology; ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "ICU; Laboratory/Pathology",
      "key_findings": "CONCLUSION: ML showed a lower bias compared to non-ML. For a robust ML-based CAD/CVD prediction design, it is vital to have (i) stronger outcomes like death or CAC score or coronary artery stenosis; (ii) ensuring scientific/clinical validation; (iii) adaptation of multiethnic groups while practicing unseen AI; (iv) amalgamation of conventional, laboratory, image-based and medication-based biomarkers.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "35044842",
      "title": "Negative Patient Descriptors: Documenting Racial Bias In The Electronic Health Record.",
      "abstract": "Little is known about how racism and bias may be communicated in the medical record. This study used machine learning to analyze electronic health records (EHRs) from an urban academic medical center and to investigate whether providers' use of negative patient descriptors varied by patient race or ethnicity. We analyzed a sample of 40,113 history and physical notes (January 2019-October 2020) from 18,459 patients for sentences containing a negative descriptor (for example, resistant or noncompliant) of the patient or the patient's behavior. We used mixed effects logistic regression to determine the odds of finding at least one negative descriptor as a function of the patient's race or ethnicity, controlling for sociodemographic and health characteristics. Compared with White patients, Black patients had 2.54 times the odds of having at least one negative descriptor in the history and physical notes. Our findings raise concerns about stigmatizing language in the EHR and its potential to exacerbate racial and ethnic health care disparities.",
      "journal": "Health affairs (Project Hope)",
      "year": "2022",
      "doi": "10.1377/hlthaff.2021.01423",
      "authors": "Sun Michael et al.",
      "keywords": "",
      "mesh_terms": "Black People; Electronic Health Records; Ethnicity; Healthcare Disparities; Humans; Racism",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35044842/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Logistic Regression",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Language; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Compared with White patients, Black patients had 2.54 times the odds of having at least one negative descriptor in the history and physical notes. Our findings raise concerns about stigmatizing language in the EHR and its potential to exacerbate racial and ethnic health care disparities.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8973827"
    },
    {
      "pmid": "35048111",
      "title": "Digital Ageism: Challenges and Opportunities in Artificial Intelligence for Older Adults.",
      "abstract": "Artificial intelligence (AI) and machine learning are changing our world through their impact on sectors including health care, education, employment, finance, and law. AI systems are developed using data that reflect the implicit and explicit biases of society, and there are significant concerns about how the predictive models in AI systems amplify inequity, privilege, and power in society. The widespread applications of AI have led to mainstream discourse about how AI systems are perpetuating racism, sexism, and classism; yet, concerns about ageism have been largely absent in the AI bias literature. Given the globally aging population and proliferation of AI, there is a need to critically examine the presence of age-related bias in AI systems. This forum article discusses ageism in AI systems and introduces a conceptual model that outlines intersecting pathways of technology development that can produce and reinforce digital ageism in AI systems. We also describe the broader ethical and legal implications and considerations for future directions in digital ageism research to advance knowledge in the field and deepen our understanding of how ageism in AI is fostered by broader cycles of injustice.",
      "journal": "The Gerontologist",
      "year": "2022",
      "doi": "10.1093/geront/gnab167",
      "authors": "Chu Charlene H et al.",
      "keywords": "Bias; Gerontology; Machine learning; Technology",
      "mesh_terms": "Aged; Ageism; Artificial Intelligence; Delivery of Health Care; Humans; Machine Learning; Racism",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35048111/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "This forum article discusses ageism in AI systems and introduces a conceptual model that outlines intersecting pathways of technology development that can produce and reinforce digital ageism in AI systems. We also describe the broader ethical and legal implications and considerations for future directions in digital ageism research to advance knowledge in the field and deepen our understanding of how ageism in AI is fostered by broader cycles of injustice.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9372891"
    },
    {
      "pmid": "35049447",
      "title": "On Algorithmic Fairness in Medical Practice.",
      "abstract": "The application of machine-learning technologies to medical practice promises to enhance the capabilities of healthcare professionals in the assessment, diagnosis, and treatment, of medical conditions. However, there is growing concern that algorithmic bias may perpetuate or exacerbate existing health inequalities. Hence, it matters that we make precise the different respects in which algorithmic bias can arise in medicine, and also make clear the normative relevance of these different kinds of algorithmic bias for broader questions about justice and fairness in healthcare. In this paper, we provide the building blocks for an account of algorithmic bias and its normative relevance in medicine.",
      "journal": "Cambridge quarterly of healthcare ethics : CQ : the international journal of healthcare ethics committees",
      "year": "2022",
      "doi": "10.1017/S0963180121000839",
      "authors": "Grote Thomas et al.",
      "keywords": "algorithmic bias; discrimination; fairness; machine learning; medical practice",
      "mesh_terms": "Data Collection; Delivery of Health Care; Humans; Machine Learning; Social Justice",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35049447/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Hence, it matters that we make precise the different respects in which algorithmic bias can arise in medicine, and also make clear the normative relevance of these different kinds of algorithmic bias for broader questions about justice and fairness in healthcare. In this paper, we provide the building blocks for an account of algorithmic bias and its normative relevance in medicine.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "35054333",
      "title": "Bias Investigation in Artificial Intelligence Systems for Early Detection of Parkinson's Disease: A Narrative Review.",
      "abstract": "UNLABELLED: Background and Motivation: Diagnosis of Parkinson's disease (PD) is often based on medical attention and clinical signs. It is subjective and does not have a good prognosis. Artificial Intelligence (AI) has played a promising role in the diagnosis of PD. However, it introduces bias due to lack of sample size, poor validation, clinical evaluation, and lack of big data configuration. The purpose of this study is to compute the risk of bias (RoB) automatically. METHOD: The PRISMA search strategy was adopted to select the best 39 AI studies out of 85 PD studies closely associated with early diagnosis PD. The studies were used to compute 30 AI attributes (based on 6 AI clusters), using AP(ai)Bias 1.0 (AtheroPointTM, Roseville, CA, USA), and the mean aggregate score was computed. The studies were ranked and two cutoffs (Moderate-Low (ML) and High-Moderate (MH)) were determined to segregate the studies into three bins: low-, moderate-, and high-bias. RESULT: The ML and HM cutoffs were 3.50 and 2.33, respectively, which constituted 7, 13, and 6 for low-, moderate-, and high-bias studies. The best and worst architectures were \"deep learning with sketches as outcomes\" and \"machine learning with Electroencephalography,\" respectively. We recommend (i) the usage of power analysis in big data framework, (ii) that it must undergo scientific validation using unseen AI models, and (iii) that it should be taken towards clinical evaluation for reliability and stability tests. CONCLUSION: The AI is a vital component for the diagnosis of early PD and the recommendations must be followed to lower the RoB.",
      "journal": "Diagnostics (Basel, Switzerland)",
      "year": "2022",
      "doi": "10.3390/diagnostics12010166",
      "authors": "Paul Sudip et al.",
      "keywords": "AI; PD; bias; cutoff; mean score; recommendations",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35054333/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Deep Learning",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Threshold Adjustment; Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: The AI is a vital component for the diagnosis of early PD and the recommendations must be followed to lower the RoB.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8774851"
    },
    {
      "pmid": "35110547",
      "title": "A robust method for collider bias correction in conditional genome-wide association studies.",
      "abstract": "Estimated genetic associations with prognosis, or conditional on a phenotype (e.g. disease incidence), may be affected by collider bias, whereby conditioning on the phenotype induces associations between causes of the phenotype and prognosis. We propose a method, 'Slope-Hunter', that uses model-based clustering to identify and utilise the class of variants only affecting the phenotype to estimate the adjustment factor, assuming this class explains more variation in the phenotype than any other variant classes. Simulation studies show that our approach eliminates the bias and outperforms alternatives even in the presence of genetic correlation. In a study of fasting blood insulin levels (FI) conditional on body mass index, we eliminate paradoxical associations of the underweight loci: COBLLI; PPARG with increased FI, and reveal an association for the locus rs1421085 (FTO). In an analysis of a case-only study for breast cancer mortality, a single region remains associated with more pronounced results.",
      "journal": "Nature communications",
      "year": "2022",
      "doi": "10.1038/s41467-022-28119-9",
      "authors": "Mahmoud Osama et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Alpha-Ketoglutarate-Dependent Dioxygenase FTO; Bias; Body Mass Index; Breast Neoplasms; Computational Biology; Fasting; Genetic Predisposition to Disease; Genome-Wide Association Study; Humans; Machine Learning; Phenotype; Polymorphism, Single Nucleotide; Risk Factors",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35110547/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clustering",
      "health_domain": "Oncology; Genomics/Genetics; Endocrinology/Diabetes",
      "bias_axes": "Gender/Sex; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "In a study of fasting blood insulin levels (FI) conditional on body mass index, we eliminate paradoxical associations of the underweight loci: COBLLI; PPARG with increased FI, and reveal an association for the locus rs1421085 (FTO). In an analysis of a case-only study for breast cancer mortality, a single region remains associated with more pronounced results.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8810923"
    },
    {
      "pmid": "35143823",
      "title": "Framework for Integrating Equity Into Machine Learning Models: A Case Study.",
      "abstract": "Predictive analytic models leveraging machine learning methods increasingly have become vital to health care organizations hoping to improve clinical outcomes and the efficiency of care delivery for all patients. Unfortunately, predictive models could harm populations that have experienced interpersonal, institutional, and structural biases. Models learn from historically collected data that could be biased. In addition, bias impacts a model's development, application, and interpretation. We present a strategy to evaluate for and mitigate biases in machine learning models that potentially could create harm. We recommend analyzing for disparities between less and more socially advantaged populations across model performance metrics (eg, accuracy, positive predictive value), patient outcomes, and resource allocation and then identify root causes of the disparities (eg, biased data, interpretation) and brainstorm solutions to address the disparities. This strategy follows the lifecycle of machine learning models in health care, namely, identifying the clinical problem, model design, data collection, model training, model validation, model deployment, and monitoring after deployment. To illustrate this approach, we use a hypothetical case of a health system developing and deploying a machine learning model to predict the risk of mortality in 6\u00a0months for patients admitted to the hospital to target a hospital's delivery of palliative care services to those with the highest mortality risk. The core ethical concepts of equity and transparency guide our proposed framework to help ensure the safe and effective use of predictive algorithms in health care to help everyone achieve their best possible health.",
      "journal": "Chest",
      "year": "2022",
      "doi": "10.1016/j.chest.2022.02.001",
      "authors": "Rojas Juan C et al.",
      "keywords": "bias; disparities; equity; framework; machine learning",
      "mesh_terms": "Algorithms; Hospitalization; Humans; Machine Learning; Predictive Value of Tests",
      "pub_types": "Journal Article; Review; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35143823/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Prediction Model; Generative AI",
      "health_domain": "Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; Public Health/Population",
      "key_findings": "To illustrate this approach, we use a hypothetical case of a health system developing and deploying a machine learning model to predict the risk of mortality in 6\u00a0months for patients admitted to the hospital to target a hospital's delivery of palliative care services to those with the highest mortality risk. The core ethical concepts of equity and transparency guide our proposed framework to help ensure the safe and effective use of predictive algorithms in health care to help everyone achieve t...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9424327"
    },
    {
      "pmid": "35211742",
      "title": "Observability and its impact on differential bias for clinical prediction models.",
      "abstract": "OBJECTIVE: Electronic health records have incomplete capture of patient outcomes. We consider the case when observability is differential across a predictor. Including such a predictor (sensitive variable) can lead to algorithmic bias, potentially exacerbating health inequities. MATERIALS AND METHODS: We define bias for a clinical prediction model (CPM) as the difference between the true and estimated risk, and differential bias as bias that differs across a sensitive variable. We illustrate the genesis of differential bias via a 2-stage process, where conditional on having the outcome of interest, the outcome is differentially observed. We use simulations and a real-data example to demonstrate the possible impact of including a sensitive variable in a CPM. RESULTS: If there is differential observability based on a sensitive variable, including it in a CPM can induce differential bias. However, if the sensitive variable impacts the outcome but not observability, it is better to include it. When a sensitive variable impacts both observability and the outcome no simple recommendation can be provided. We show that one cannot use observed data to detect differential bias. DISCUSSION: Our study furthers the literature on observability, showing that differential observability can lead to algorithmic bias. This highlights the importance of considering whether to include sensitive variables in CPMs. CONCLUSION: Including a sensitive variable in a CPM depends on whether it truly affects the outcome or just the observability of the outcome. Since this cannot be distinguished with observed data, observability is an implicit assumption of CPMs.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2022",
      "doi": "10.1093/jamia/ocac019",
      "authors": "Yan Mengying et al.",
      "keywords": "algorithmic bias; clinical prediction models; electronic health record; health equity; observability",
      "mesh_terms": "Bias; Humans; Models, Statistical; Prognosis",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35211742/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Including a sensitive variable in a CPM depends on whether it truly affects the outcome or just the observability of the outcome. Since this cannot be distinguished with observed data, observability is an implicit assumption of CPMs.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9006687"
    },
    {
      "pmid": "35228172",
      "title": "An artificial intelligence framework and its bias for brain tumor segmentation: A narrative review.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) has become a prominent technique for medical diagnosis and represents an essential role in detecting brain tumors. Although AI-based models are widely used in brain lesion segmentation (BLS), understanding their effectiveness is challenging due to their complexity and diversity. Several reviews on brain tumor segmentation are available, but none of them describe a link between the threats due to risk-of-bias (RoB) in AI and its architectures. In our review, we focused on linking RoB and different AI-based architectural Cluster in popular DL framework. Further, due to variance in these designs and input data types in medical imaging, it is necessary to present a narrative review considering all facets of BLS. APPROACH: The proposed study uses a PRISMA strategy based on 75 relevant studies found by searching PubMed, Scopus, and Google Scholar. Based on the architectural evolution, DL studies were subsequently categorized into four classes: convolutional neural network (CNN)-based, encoder-decoder (ED)-based, transfer learning (TL)-based, and hybrid DL (HDL)-based architectures. These studies were then analyzed considering 32 AI attributes, with clusters including AI architecture, imaging modalities, hyper-parameters, performance evaluation metrics, and clinical evaluation. Then, after these studies were scored for all attributes, a composite score was computed, normalized, and ranked. Thereafter, a bias cutoff (AP(ai)Bias 1.0, AtheroPoint, Roseville, CA, USA) was established to detect low-, moderate- and high-bias studies. CONCLUSION: The four classes of architectures, from best-to worst-performing, are TL\u00a0>\u00a0ED\u00a0>\u00a0CNN\u00a0>\u00a0HDL. ED-based models had the lowest AI bias for BLS. This study presents a set of three primary and six secondary recommendations for lowering the RoB.",
      "journal": "Computers in biology and medicine",
      "year": "2022",
      "doi": "10.1016/j.compbiomed.2022.105273",
      "authors": "Das Suchismita et al.",
      "keywords": "AI; BTS; DL; Hybrid deep learning; RoB; U-Net",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35228172/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Deep Learning; Neural Network; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Oncology; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Threshold Adjustment; Transfer Learning",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: The four classes of architectures, from best-to worst-performing, are TL\u00a0>\u00a0ED\u00a0>\u00a0CNN\u00a0>\u00a0HDL. ED-based models had the lowest AI bias for BLS. This study presents a set of three primary and six secondary recommendations for lowering the RoB.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "35239737",
      "title": "A test of affect processing bias in response to affect regulation.",
      "abstract": "In this study we merged methods from machine learning and human neuroimaging to test the role of self-induced affect processing states in biasing the affect processing of subsequent image stimuli. To test this relationship we developed a novel paradigm in which (n = 40) healthy adult participants observed affective neural decodings of their real-time functional magnetic resonance image (rtfMRI) responses as feedback to guide explicit regulation of their brain (and corollary affect processing) state towards a positive valence goal state. By this method individual differences in affect regulation ability were controlled. Attaining this brain-affect goal state triggered the presentation of pseudo-randomly selected affectively congruent (positive valence) or incongruent (negative valence) image stimuli drawn from the International Affective Picture Set. Separately, subjects passively viewed randomly triggered positively and negatively valent image stimuli during fMRI acquisition. Multivariate neural decodings of the affect processing induced by these stimuli were modeled using the task trial type (state- versus randomly-triggered) as the fixed-effect of a general linear mixed-effects model. Random effects were modeled subject-wise. We found that self-induction of a positive valence brain state significantly positively biased valence processing of subsequent stimuli. As a manipulation check, we validated affect processing state induction achieved by the image stimuli using independent psychophysiological response measures of hedonic valence and autonomic arousal. We also validated the predictive fidelity of the trained neural decoding models using brain states induced by an out-of-sample set of image stimuli. Beyond its contribution to our understanding of the neural mechanisms that bias affect processing, this work demonstrated the viability of novel experimental paradigms triggered by pre-defined cognitive states. This line of individual differences research potentially provides neuroimaging scientists with a valuable tool for exploring the roles and identities of intrinsic cognitive processing mechanisms that shape our perceptual processing of sensory stimuli.",
      "journal": "PloS one",
      "year": "2022",
      "doi": "10.1371/journal.pone.0264758",
      "authors": "Bush Keith A et al.",
      "keywords": "",
      "mesh_terms": "Adult; Affect; Arousal; Brain; Brain Mapping; Emotions; Humans; Magnetic Resonance Imaging; Neuroimaging",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35239737/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Beyond its contribution to our understanding of the neural mechanisms that bias affect processing, this work demonstrated the viability of novel experimental paradigms triggered by pre-defined cognitive states. This line of individual differences research potentially provides neuroimaging scientists with a valuable tool for exploring the roles and identities of intrinsic cognitive processing mechanisms that shape our perceptual processing of sensory stimuli.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8893671"
    },
    {
      "pmid": "35243993",
      "title": "Mitigating Racial Bias in Machine Learning.",
      "abstract": "When applied in the health sector, AI-based applications raise not only ethical but legal and safety concerns, where algorithms trained on data from majority populations can generate less accurate or reliable results for minorities and other disadvantaged groups.",
      "journal": "The Journal of law, medicine & ethics : a journal of the American Society of Law, Medicine & Ethics",
      "year": "2022",
      "doi": "10.1017/jme.2022.13",
      "authors": "Kostick-Quenet Kristin M et al.",
      "keywords": "Algorithmic Bias; Artificial Intelligence; Ethics; Machine Learning; Racial Bias",
      "mesh_terms": "Artificial Intelligence; Humans; Machine Learning; Racism",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35243993/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "When applied in the health sector, AI-based applications raise not only ethical but legal and safety concerns, where algorithms trained on data from majority populations can generate less accurate or reliable results for minorities and other disadvantaged groups.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12140104"
    },
    {
      "pmid": "35285816",
      "title": "Machine Learning-Based Short-Term Mortality Prediction Models for Patients With Cancer Using Electronic Health Record Data: Systematic Review and Critical Appraisal.",
      "abstract": "BACKGROUND: In the United States, national guidelines suggest that aggressive cancer care should be avoided in the final months of life. However, guideline compliance currently requires clinicians to make judgments based on their experience as to when a patient is nearing the end of their life. Machine learning (ML) algorithms may facilitate improved end-of-life care provision for patients with cancer by identifying patients at risk of short-term mortality. OBJECTIVE: This study aims to summarize the evidence for applying ML in \u22641-year cancer mortality prediction to assist with the transition to end-of-life care for patients with cancer. METHODS: We searched MEDLINE, Embase, Scopus, Web of Science, and IEEE to identify relevant articles. We included studies describing ML algorithms predicting \u22641-year mortality in patients of oncology. We used the prediction model risk of bias assessment tool to assess the quality of the included studies. RESULTS: We included 15 articles involving 110,058 patients in the final synthesis. Of the 15 studies, 12 (80%) had a high or unclear risk of bias. The model performance was good: the area under the receiver operating characteristic curve ranged from 0.72 to 0.92. We identified common issues leading to biased models, including using a single performance metric, incomplete reporting of or inappropriate modeling practice, and small sample size. CONCLUSIONS: We found encouraging signs of ML performance in predicting short-term cancer mortality. Nevertheless, no included ML algorithms are suitable for clinical practice at the current stage because of the high risk of bias and uncertainty regarding real-world performance. Further research is needed to develop ML models using the modern standards of algorithm development and reporting.",
      "journal": "JMIR medical informatics",
      "year": "2022",
      "doi": "10.2196/33182",
      "authors": "Lu Sheng-Chieh et al.",
      "keywords": "artificial intelligence; cancer mortality; clinical prediction models; end-of-life care; machine learning",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35285816/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Oncology; EHR/Health Informatics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: We found encouraging signs of ML performance in predicting short-term cancer mortality. Nevertheless, no included ML algorithms are suitable for clinical practice at the current stage because of the high risk of bias and uncertainty regarding real-world performance. Further research is needed to develop ML models using the modern standards of algorithm development and reporting.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 2 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8961346"
    },
    {
      "pmid": "35361577",
      "title": "Machine Learning in Differentiating Gliomas from Primary CNS Lymphomas: A Systematic Review, Reporting Quality, and Risk of Bias Assessment.",
      "abstract": "BACKGROUND: Differentiating gliomas and primary CNS lymphoma represents a diagnostic challenge with important therapeutic ramifications. Biopsy is the preferred method of diagnosis, while MR imaging in conjunction with machine learning has shown promising results in differentiating these tumors. PURPOSE: Our aim was to evaluate the quality of reporting and risk of bias, assess data bases with which the machine learning classification algorithms were developed, the algorithms themselves, and their performance. DATA SOURCES: Ovid EMBASE, Ovid MEDLINE, Cochrane Central Register of Controlled Trials, and the Web of Science Core Collection were searched according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. STUDY SELECTION: From 11,727 studies, 23 peer-reviewed studies used machine learning to differentiate primary CNS lymphoma from gliomas in 2276 patients. DATA ANALYSIS: Characteristics of data sets and machine learning algorithms were extracted. A meta-analysis on a subset of studies was performed. Reporting quality and risk of bias were assessed using the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) and Prediction Model Study Risk Of Bias Assessment Tool. DATA SYNTHESIS: The highest area under the receiver operating characteristic curve (0.961) and accuracy (91.2%) in external validation were achieved by logistic regression and support vector machines models using conventional radiomic features. Meta-analysis of machine learning classifiers using these features yielded a mean area under the receiver operating characteristic curve of 0.944 (95% CI, 0.898-0.99). The median TRIPOD score was 51.7%. The risk of bias was high for 16 studies. LIMITATIONS: Exclusion of abstracts decreased the sensitivity in evaluating all published studies. Meta-analysis had high heterogeneity. CONCLUSIONS: Machine learning-based methods of differentiating primary CNS lymphoma from gliomas have shown great potential, but most studies lack large, balanced data sets and external validation. Assessment of the studies identified multiple deficiencies in reporting quality and risk of bias. These factors reduce the generalizability and reproducibility of the findings.",
      "journal": "AJNR. American journal of neuroradiology",
      "year": "2022",
      "doi": "10.3174/ajnr.A7473",
      "authors": "Cassinelli Petersen G I et al.",
      "keywords": "",
      "mesh_terms": "Glioma; Humans; Lymphoma; Machine Learning; Magnetic Resonance Imaging; Reproducibility of Results",
      "pub_types": "Journal Article; Meta-Analysis; Systematic Review; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35361577/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Logistic Regression; Support Vector Machine; Clinical Prediction Model",
      "health_domain": "Oncology; Pathology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Machine learning-based methods of differentiating primary CNS lymphoma from gliomas have shown great potential, but most studies lack large, balanced data sets and external validation. Assessment of the studies identified multiple deficiencies in reporting quality and risk of bias. These factors reduce the generalizability and reproducibility of the findings.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8993193"
    },
    {
      "pmid": "35396996",
      "title": "Assessing socioeconomic bias in machine learning algorithms in health care: a case study of the HOUSES index.",
      "abstract": "OBJECTIVE: Artificial intelligence (AI) models may propagate harmful biases in performance and hence negatively affect the underserved. We aimed to assess the degree to which data quality of electronic health records (EHRs) affected by inequities related to low socioeconomic status (SES), results in differential performance of AI models across SES. MATERIALS AND METHODS: This study utilized existing machine learning models for predicting asthma exacerbation in children with asthma. We compared balanced error rate (BER) against different SES levels measured by HOUsing-based SocioEconomic Status measure (HOUSES) index. As a possible mechanism for differential performance, we also compared incompleteness of EHR information relevant to asthma care by SES. RESULTS: Asthmatic children with lower SES had larger BER than those with higher SES (eg, ratio = 1.35 for HOUSES Q1 vs Q2-Q4) and had a higher proportion of missing information relevant to asthma care (eg, 41% vs 24% for missing asthma severity and 12% vs 9.8% for undiagnosed asthma despite meeting asthma criteria). DISCUSSION: Our study suggests that lower SES is associated with worse predictive model performance. It also highlights the potential role of incomplete EHR data in this differential performance and suggests a way to mitigate this bias. CONCLUSION: The HOUSES index allows AI researchers to assess bias in predictive model performance by SES. Although our case study was based on a small sample size and a single-site study, the study results highlight a potential strategy for identifying bias by using an innovative SES measure.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2022",
      "doi": "10.1093/jamia/ocac052",
      "authors": "Juhn Young J et al.",
      "keywords": "HOUSES; algorithmic bias; artificial intelligence; electronic health records; social determinants of health",
      "mesh_terms": "Artificial Intelligence; Asthma; Bias; Child; Delivery of Health Care; Humans; Machine Learning; Social Class",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35396996/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "EHR/Health Informatics; Pediatrics; Pulmonology",
      "bias_axes": "Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Safety-Net/Underserved",
      "key_findings": "CONCLUSION: The HOUSES index allows AI researchers to assess bias in predictive model performance by SES. Although our case study was based on a small sample size and a single-site study, the study results highlight a potential strategy for identifying bias by using an innovative SES measure.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9196683"
    },
    {
      "pmid": "35471566",
      "title": "Analysis of Race and Sex Bias in the Autism Diagnostic Observation Schedule (ADOS-2).",
      "abstract": "IMPORTANCE: There are long-standing disparities in the prevalence of autism spectrum disorder (ASD) across race and sex. Surprisingly, few studies have examined whether these disparities arise partially out of systematic biases in the Autism Diagnostic Observation Schedule, Second Edition (ADOS-2), the reference standard measure of ASD. OBJECTIVE: To examine differential item functioning (DIF) of ADOS-2 items across sex and race. DESIGN, SETTING, AND PARTICIPANTS: This is a cross-sectional study of children who were evaluated for ASD between 2014 and 2020 at a specialty outpatient clinic located in the Mid-Atlantic region of the US. Data were analyzed from July 2021 to February 2022. EXPOSURES: Child race (Black/African American vs White) and sex (female vs male). MAIN OUTCOMES AND MEASURES: Item-level biases across ADOS-2 harmonized algorithm items, including social affect (SA; 10 items) and repetitive/restricted behaviors (RRBs; 4 items), were evaluated across 3 modules. Measurement bias was identified by examining DIF and differential test functioning (DTF), within a graded response, item response theory framework. Statistical significance was determined by a likelihood ratio \u03c72 test, and a series of metrics was used to examine the magnitude of DIF and DTF. RESULTS: A total of 6269 children (mean [SD] age, 6.77 [3.27] years; 1619 Black/African American [25.9%], 3151 White [50.3%], and 4970 male [79.4%]), were included in this study. Overall, 16 of 140 ADOS-2 diagnostic items (11%) had a significant DIF. For race, 8 items had a significant DIF, 6 of which involved SA. No single item showed DIF consistently across all modules. Most items with DIF had greater difficulty and poorer discrimination in Black/African American children compared with White children. For sex, 5 items showed significant DIF. DIF was split across SA and RRB. However, hand mannerisms evidenced DIF across all 5 algorithms, with generally greater difficulty. The magnitude of DIF was only moderate to large for 2 items: hand mannerisms (among female children) and repetitive interests (among Black/African American children). The overall estimated effect of DIF on total DTF was not large. CONCLUSIONS AND RELEVANCE: These findings suggest that the ADOS-2 does not have widespread systematic measurement bias across race or sex. However, the findings raise some concerns around underdetection that warrant further research.",
      "journal": "JAMA network open",
      "year": "2022",
      "doi": "10.1001/jamanetworkopen.2022.9498",
      "authors": "Kalb Luther G et al.",
      "keywords": "",
      "mesh_terms": "Autism Spectrum Disorder; Autistic Disorder; Child; Cross-Sectional Studies; Female; Humans; Male; Racial Groups; Sexism",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35471566/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Pediatrics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Primary Care/Outpatient",
      "key_findings": "RESULTS: A total of 6269 children (mean [SD] age, 6.77 [3.27] years; 1619 Black/African American [25.9%], 3151 White [50.3%], and 4970 male [79.4%]), were included in this study. Overall, 16 of 140 ADOS-2 diagnostic items (11%) had a significant DIF. For race, 8 items had a significant DIF, 6 of which involved SA.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9044110"
    },
    {
      "pmid": "35477689",
      "title": "Operationalising fairness in medical AI adoption: detection of early Alzheimer's disease with 2D CNN.",
      "abstract": "OBJECTIVES: To operationalise fairness in the adoption of medical artificial intelligence (AI) algorithms in terms of access to computational resources, the proposed approach is based on a two-dimensional (2D) convolutional neural networks (CNN), which provides a faster, cheaper and accurate-enough detection of early Alzheimer's disease (AD) and mild cognitive impairment (MCI), without the need for use of large training data sets or costly high-performance computing (HPC) infrastructures. METHODS: The standardised Alzheimer's Disease Neuroimaging Initiative (ADNI) data sets are used for the proposed model, with additional skull stripping, using the Brain Extraction Tool V.2approach. The 2D CNN architecture is based on LeNet-5, the Leaky Rectified Linear Unit activation function and a Sigmoid function were used, and batch normalisation was added after every convolutional layer to stabilise the learning process. The model was optimised by manually tuning all its hyperparameters. RESULTS: The model was evaluated in terms of accuracy, recall, precision and f1-score. The results demonstrate that the model predicted MCI with an accuracy of 0.735, passing the random guessing baseline of 0.521 and predicted AD with an accuracy of 0.837, passing the random guessing baseline of 0.536. DISCUSSION: The proposed approach can assist clinicians in the early diagnosis of AD and MCI, with high-enough accuracy, based on relatively smaller data sets, and without the need of HPC infrastructures. Such an approach can alleviate disparities and operationalise fairness in the adoption of medical algorithms. CONCLUSION: Medical AI algorithms should not be focused solely on accuracy but should also be evaluated with respect to how they might impact disparities and operationalise fairness in their adoption.",
      "journal": "BMJ health & care informatics",
      "year": "2022",
      "doi": "10.1136/bmjhci-2021-100485",
      "authors": "Heising Luca et al.",
      "keywords": "artificial intelligence; medical informatics applications; neural networks, computer",
      "mesh_terms": "Alzheimer Disease; Artificial Intelligence; Cognitive Dysfunction; Humans; Magnetic Resonance Imaging; Neural Networks, Computer",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35477689/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Neurology",
      "bias_axes": "Gender/Sex; Age; Disability",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Medical AI algorithms should not be focused solely on accuracy but should also be evaluated with respect to how they might impact disparities and operationalise fairness in their adoption.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9047889"
    },
    {
      "pmid": "35496981",
      "title": "Ethics-by-design: efficient, fair and inclusive resource allocation using machine learning.",
      "abstract": "The distribution of crucial medical goods and services in conditions of scarcity is among the most important, albeit contested, areas of public policy development. Policymakers must strike a balance between multiple efficiency and fairness objectives, while reconciling disparate value judgments from a diverse set of stakeholders. We present a general framework for combining ethical theory, data modeling, and stakeholder input in this process and illustrate through a case study on designing organ transplant allocation policies. We develop a novel analytical tool, based on machine learning and optimization, designed to facilitate efficient and wide-ranging exploration of policy outcomes across multiple objectives. Such a tool enables all stakeholders, regardless of their technical expertise, to more effectively engage in the policymaking process by developing evidence-based value judgments based on relevant tradeoffs.",
      "journal": "Journal of law and the biosciences",
      "year": "2022",
      "doi": "10.1093/jlb/lsac012",
      "authors": "Papalexopoulos Theodore P et al.",
      "keywords": "Analytics; ethics by design; machine learning; organ allocation; organ transplantation; resource allocation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35496981/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We develop a novel analytical tool, based on machine learning and optimization, designed to facilitate efficient and wide-ranging exploration of policy outcomes across multiple objectives. Such a tool enables all stakeholders, regardless of their technical expertise, to more effectively engage in the policymaking process by developing evidence-based value judgments based on relevant tradeoffs.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9050238"
    },
    {
      "pmid": "35506520",
      "title": "Bias field correction in hyperpolarized 129 Xe ventilation MRI using templates derived by RF-depolarization mapping.",
      "abstract": "PURPOSE: To correct for RF inhomogeneity for in vivo 129 Xe ventilation MRI using flip-angle mapping enabled by randomized 3D radial acquisitions. To extend this RF-depolarization mapping approach to create a flip-angle map template applicable to arbitrary acquisition strategies, and to compare these approaches to conventional bias field correction. METHODS: RF-depolarization mapping was evaluated first in digital simulations and then in 51 subjects who had undergone radial 129 Xe ventilation MRI in the supine position at 3T (views\u00a0=\u00a03600; samples/view\u00a0=\u00a0128; TR/TE\u00a0=\u00a04.5/0.45\u2009ms; flip angle\u00a0=\u00a01.5; FOV\u00a0=\u00a040\u2009cm). The images were corrected using newly developed RF-depolarization and templated-based methods and the resulting quantitative ventilation metrics (mean, coefficient of variation, and gradient) were compared to those resulting from N4ITK correction. RESULTS: RF-depolarization and template-based mapping methods yielded a pattern of RF-inhomogeneity consistent with the expected variation based on coil architecture. The resulting corrected images were visually similar, but meaningfully distinct from those generated using standard N4ITK correction. The N4ITK algorithm eliminated the physiologically expected anterior-posterior gradient (-0.04\u2009\u00b1\u20091.56%/cm, P\u2009<\u20090.001). These 2 newly introduced methods of RF-depolarization and template correction retained the physiologically expected anterior-posterior ventilation gradient in healthy subjects (2.77\u2009\u00b1\u20092.09%/cm and 2.01\u2009\u00b1\u20092.73%/cm, respectively). CONCLUSIONS: Randomized 3D 129 Xe MRI ventilation acquisitions can inherently be corrected for bias field, and this technique can be extended to create flip angle templates capable of correcting images from a given coil regardless of acquisition strategy. These methods may be more favorable than the de facto standard N4ITK because they can remove undesirable heterogeneity caused by RF effects while retaining results from known physiology.",
      "journal": "Magnetic resonance in medicine",
      "year": "2022",
      "doi": "10.1002/mrm.29254",
      "authors": "Lu Junlan et al.",
      "keywords": "bias field correction; hyperpolarized 129Xe MRI; ventilation defect percentage",
      "mesh_terms": "Algorithms; Humans; Lung; Magnetic Resonance Imaging; Respiration; Xenon Isotopes",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35506520/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Pulmonology",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Randomized 3D 129 Xe MRI ventilation acquisitions can inherently be corrected for bias field, and this technique can be extended to create flip angle templates capable of correcting images from a given coil regardless of acquisition strategy. These methods may be more favorable than the de facto standard N4ITK because they can remove undesirable heterogeneity caused by RF effects while retaining results from known physiology.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9248357"
    },
    {
      "pmid": "35574857",
      "title": "Accounting for publication bias using a bivariate trim and fill meta-analysis procedure.",
      "abstract": "In research synthesis, publication bias (PB) refers to the phenomenon that the publication of a study is associated with the direction and statistical significance of its results. Consequently, it may lead to biased (commonly optimistic) estimates of treatment effects. Visualization tools such as funnel plots have been widely used to investigate PB in univariate meta-analyses. The trim and fill procedure is a nonparametric method to identify and adjust for PB. It is popular among applied scientists due to its simplicity. However, most visualization tools and PB correction methods focus on univariate outcomes. For a meta-analysis with multiple outcomes, the conventional univariate trim and fill method can only account for different outcomes separately and thus may lead to inconsistent conclusions. In this article, we propose a bivariate trim and fill procedure to simultaneously account for PB in the presence of two outcomes that are possibly associated. Based on a recently developed galaxy plot for bivariate meta-analysis, the proposed procedure uses a data-driven imputation algorithm to detect and adjust PB. The method relies on the symmetry of the galaxy plot and assumes that some studies are suppressed based on a linear combination of outcomes. The method projects bivariate outcomes along a particular direction, uses the univariate trim and fill method to estimate the number of trimmed and filled studies, and yields consistent conclusions about PB. The proposed approach is validated using simulated data and is applied to a meta-analysis of the efficacy and safety of antidepressant drugs.",
      "journal": "Statistics in medicine",
      "year": "2022",
      "doi": "10.1002/sim.9428",
      "authors": "Luo Chongliang et al.",
      "keywords": "antidepressant drug; bivariate meta-analysis; galaxy plot; publication bias; trim and fill",
      "mesh_terms": "Humans; Publication Bias",
      "pub_types": "Journal Article; Meta-Analysis; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35574857/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Meta-Analysis",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "The method projects bivariate outcomes along a particular direction, uses the univariate trim and fill method to estimate the number of trimmed and filled studies, and yields consistent conclusions about PB. The proposed approach is validated using simulated data and is applied to a meta-analysis of the efficacy and safety of antidepressant drugs.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC12208082"
    },
    {
      "pmid": "35592084",
      "title": "Operationalizing \"One Health\" as \"One Digital Health\" Through a Global Framework That Emphasizes Fair and Equitable Sharing of Benefits From the Use of Artificial Intelligence and Related Digital Technologies.",
      "abstract": "The operationalization of One Health (OH) through digitalization is a means to deploy digital technologies (including Artificial Intelligence (AI), big data and related digital technologies) to better capacitate us to deal with growing climate exigency and related threats to human, animal and plant health. With reference to the concept of One Digital Health (ODH), this paper considers how digital capabilities can help to overcome 'operational brakes' in OH through new and deeper insights, better predictions, and more targeted or precise preventive strategies and public health countermeasures. However, the data landscape is fragmented and access to certain types of data is increasingly restrictive as individuals, communities and countries seek to assert greater control over data taken from them. This paper proposes for a dedicated global ODH framework-centered on fairness and equity-to be established to promote data-sharing across all the key knowledge domains of OH and to devise data-driven solutions to challenges in the human-animal-ecosystems interface. It first considers the data landscape in relation to: (1) Human and population health; (2) Pathogens; (3) Animal and plant health; and (4) Ecosystems and biodiversity. The complexification from the application of advance genetic sequencing technology is then considered, with focus on current debates over whether certain types of data like digital (genetic) sequencing information (DSI) should remain openly and freely accessible. The proposed ODH framework must augment the existing access and benefit sharing (ABS) framework currently prescribed under the Nagoya Protocol to the Convention on Biological Diversity (CBD) in at least three different ways. First, the ODH framework should apply to all genetic resources and data, including DSI, whether from humans or non-humans. Second, the FAIRER principles should be implemented, with focus on fair and equitable benefit-sharing. Third, the ODH framework should adopt multilateral approaches to data sharing (such as through federated data systems) and to ABS. By operationalizing OH as ODH, we are more likely to be able to protect and restore natural habitats, secure the health and well-being of all living things, and thereby realize the goals set out in the post-2020 Global Biodiversity Framework under the CBD.",
      "journal": "Frontiers in public health",
      "year": "2022",
      "doi": "10.3389/fpubh.2022.768977",
      "authors": "Ho Calvin Wai-Loon",
      "keywords": "Convention on Biological Diversity; FAIR; International Health Regulations; One Health; artificial intelligence; benefit sharing; data sharing; digital",
      "mesh_terms": "Artificial Intelligence; Biodiversity; Digital Technology; Ecosystem; One Health",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35592084/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Genomics/Genetics; Public Health",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Third, the ODH framework should adopt multilateral approaches to data sharing (such as through federated data systems) and to ABS. By operationalizing OH as ODH, we are more likely to be able to protect and restore natural habitats, secure the health and well-being of all living things, and thereby realize the goals set out in the post-2020 Global Biodiversity Framework under the CBD.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9110679"
    },
    {
      "pmid": "35679118",
      "title": "Ageism and Artificial Intelligence: Protocol for a Scoping Review.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) has emerged as a major driver of technological development in the 21st century, yet little attention has been paid to algorithmic biases toward older adults. OBJECTIVE: This paper documents the search strategy and process for a scoping review exploring how age-related bias is encoded or amplified in AI systems as well as the corresponding legal and ethical implications. METHODS: The scoping review follows a 6-stage methodology framework developed by Arksey and O'Malley. The search strategy has been established in 6 databases. We will investigate the legal implications of ageism in AI by searching grey literature databases, targeted websites, and popular search engines and using an iterative search strategy. Studies meet the inclusion criteria if they are in English, peer-reviewed, available electronically in full text, and meet one of the following two additional criteria: (1) include \"bias\" related to AI in any application (eg, facial recognition) and (2) discuss bias related to the concept of old age or ageism. At least two reviewers will independently conduct the title, abstract, and full-text screening. Search results will be reported using the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) reporting guideline. We will chart data on a structured form and conduct a thematic analysis to highlight the societal, legal, and ethical implications reported in the literature. RESULTS: The database searches resulted in 7595 records when the searches were piloted in November 2021. The scoping review will be completed by December 2022. CONCLUSIONS: The findings will provide interdisciplinary insights into the extent of age-related bias in AI systems. The results will contribute foundational knowledge that can encourage multisectoral cooperation to ensure that AI is developed and deployed in a manner consistent with ethical values and human rights legislation as it relates to an older and aging population. We will publish the review findings in peer-reviewed journals and disseminate the key results with stakeholders via workshops and webinars. TRIAL REGISTRATION: OSF Registries AMG5P; https://osf.io/amg5p. INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): DERR1-10.2196/33211.",
      "journal": "JMIR research protocols",
      "year": "2022",
      "doi": "10.2196/33211",
      "authors": "Chu Charlene H et al.",
      "keywords": "age-related biases; ageism; algorithms; artificial intelligence; ethics; gerontology; health database; human rights; search strategy",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35679118/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: The findings will provide interdisciplinary insights into the extent of age-related bias in AI systems. The results will contribute foundational knowledge that can encourage multisectoral cooperation to ensure that AI is developed and deployed in a manner consistent with ethical values and human rights legislation as it relates to an older and aging population. We will publish the review findings in peer-reviewed journals and disseminate the key results with stakeholders via worksho...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9227654"
    },
    {
      "pmid": "35688612",
      "title": "Artificial intelligence in gastroenterology and hepatology: how to advance clinical practice while ensuring health equity.",
      "abstract": "Artificial intelligence (AI) and machine learning (ML) systems are increasingly used in medicine to improve clinical decision-making and healthcare delivery. In gastroenterology and hepatology, studies have explored a myriad of opportunities for AI/ML applications which are already making the transition to bedside. Despite these advances, there is a risk that biases and health inequities can be introduced or exacerbated by these technologies. If unrecognised, these technologies could generate or worsen systematic racial, ethnic and sex disparities when deployed on a large scale. There are several mechanisms through which AI/ML could contribute to health inequities in gastroenterology and hepatology, including diagnosis of oesophageal cancer, management of inflammatory bowel disease (IBD), liver transplantation, colorectal cancer screening and many others. This review adapts a framework for ethical AI/ML development and application to gastroenterology and hepatology such that clinical practice is advanced while minimising bias and optimising health equity.",
      "journal": "Gut",
      "year": "2022",
      "doi": "10.1136/gutjnl-2021-326271",
      "authors": "Uche-Anya Eugenia et al.",
      "keywords": "IBD; colorectal cancer; liver transplantation; oesophageal cancer",
      "mesh_terms": "Artificial Intelligence; Clinical Decision-Making; Gastroenterology; Health Equity; Humans; Machine Learning",
      "pub_types": "Journal Article; Review; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35688612/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "There are several mechanisms through which AI/ML could contribute to health inequities in gastroenterology and hepatology, including diagnosis of oesophageal cancer, management of inflammatory bowel disease (IBD), liver transplantation, colorectal cancer screening and many others. This review adapts a framework for ethical AI/ML development and application to gastroenterology and hepatology such that clinical practice is advanced while minimising bias and optimising health equity.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10323754"
    },
    {
      "pmid": "35756093",
      "title": "Machine Learning Models to Predict In-Hospital Mortality among Inpatients with COVID-19: Underestimation and Overestimation Bias Analysis in Subgroup Populations.",
      "abstract": "Prediction of the death among COVID-19 patients can help healthcare providers manage the patients better. We aimed to develop machine learning models to predict in-hospital death among these patients. We developed different models using different feature sets and datasets developed using the data balancing method. We used demographic and clinical data from a multicenter COVID-19 registry. We extracted 10,657 records for confirmed patients with PCR or CT scans, who were hospitalized at least for 24 hours at the end of March 2021. The death rate was 16.06%. Generally, models with 60 and 40 features performed better. Among the 240 models, the C5 models with 60 and 40 features performed well. The C5 model with 60 features outperformed the rest based on all evaluation metrics; however, in external validation, C5 with 32 features performed better. This model had high accuracy (91.18%), F-score (0.916), Area under the Curve (0.96), sensitivity (94.2%), and specificity (88%). The model suggested in this study uses simple and available data and can be applied to predict death among COVID-19 patients. Furthermore, we concluded that machine learning models may perform differently in different subpopulations in terms of gender and age groups.",
      "journal": "Journal of healthcare engineering",
      "year": "2022",
      "doi": "10.1155/2022/1644910",
      "authors": "Zarei Javad et al.",
      "keywords": "",
      "mesh_terms": "COVID-19; Hospital Mortality; Humans; Inpatients; Machine Learning; ROC Curve",
      "pub_types": "Journal Article; Multicenter Study; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35756093/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Pulmonology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Subgroup Analysis",
      "clinical_setting": "Hospital/Inpatient; Public Health/Population",
      "key_findings": "The model suggested in this study uses simple and available data and can be applied to predict death among COVID-19 patients. Furthermore, we concluded that machine learning models may perform differently in different subpopulations in terms of gender and age groups.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9226971"
    },
    {
      "pmid": "35801177",
      "title": "Transcranial Direct Current Stimulation Over the Left Dorsolateral Prefrontal Cortex Reduced Attention Bias Toward Negative Facial Expression: A Pilot Study in Healthy Subjects.",
      "abstract": "Research in the cognitive neuroscience field has shown that individuals with a stronger attention bias for negative information had higher depression risk, which may be the underlying pathogenesis of depression. This dysfunction of affect-biased attention also represents a decline in emotion regulation ability. Clinical studies have suggested that transcranial direct current stimulation (tDCS) treatment can improve the symptoms of depression, yet the neural mechanism behind this improvement is still veiled. This study aims to investigate the effects of tDCS on affect-biased attention. A sample of healthy participants received 20 min active (n = 22) or sham tDCS (n = 19) over the left dorsolateral prefrontal cortex (DLPFC) for 7 consecutive days. Electroencephalographic (EEG) signals were recorded while performing the rest task and emotional oddball task. The oddball task required response to pictures of the target (positive or negative) emotional facial stimuli and neglecting distracter (negative or positive) or standard (neutral) stimuli. Welch power spectrum estimation algorithm was applied to calculate frontal alpha asymmetry (FAA) in the rest task, and the overlapping averaging method was used to extract event-related potentials (ERP) components in the oddball task. Compared to sham tDCS, active tDCS caused an obvious increment in FAA in connection with emotion regulation (p < 0.05). Also, participants in the active tDCS group show greater P3 amplitudes following positive targets (p < 0.05) and greater N2 amplitudes following negative distracters (p < 0.05), reflecting emotion-related attention biases. These results offer valuable insights into the relationship between affect-biased attention and the effects of tDCS, which may be of assistance in exploring the neuropathological mechanism of depression and anxiety and new treatment strategies for tDCS.",
      "journal": "Frontiers in neuroscience",
      "year": "2022",
      "doi": "10.3389/fnins.2022.894798",
      "authors": "Liu Shuang et al.",
      "keywords": "DLPFC; ERP; attention bias; emotion regulation; tDCS",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35801177/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Also, participants in the active tDCS group show greater P3 amplitudes following positive targets (p < 0.05) and greater N2 amplitudes following negative distracters (p < 0.05), reflecting emotion-related attention biases. These results offer valuable insights into the relationship between affect-biased attention and the effects of tDCS, which may be of assistance in exploring the neuropathological mechanism of depression and anxiety and new treatment strategies for tDCS.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9256464"
    },
    {
      "pmid": "35878534",
      "title": "Age, sex and race bias in automated arrhythmia detectors.",
      "abstract": "Despite the recent explosion of machine learning applied to medical data, very few studies have examined algorithmic bias in any meaningful manner, comparing across algorithms, databases, and assessment metrics. In this study, we compared the biases in sex, age, and race of 56 algorithms on over 130,000 electrocardiograms (ECGs) using several metrics and propose a machine learning model design to reduce bias. Participants of the 2021 PhysioNet Challenge designed and implemented working, open-source algorithms to identify clinical diagnosis from 2- lead ECG recordings. We grouped the data from the training, validation, and test datasets by sex (male vs female), age (binned by decade), and race (Asian, Black, White, and Other) whenever possible. We computed recording-wise accuracy, area under the receiver operating characteristic curve (AUROC), area under the precision recall curve (AUPRC), F-measure, and the Challenge Score for each of the 56 algorithms. The Mann-Whitney U and the Kruskal-Wallis tests assessed the performance differences of algorithms across these demographic groups. Group trends revealed similar values for the AUROC, AUPRC, and F-measure for both male and female groups across the training, validation, and test sets. However, recording-wise accuracies were 20% higher (p\u00a0<\u00a00.01) and the Challenge Score 12% lower (p\u00a0=\u00a00.02) for female subjects on the test set. AUPRC, F-measure, and the Challenge Score increased with age, while recording-wise accuracy and AUROC decreased with age. The results were similar for the training and test sets, but only recording-wise accuracy (12% decrease per decade, p\u00a0<\u00a00.01), Challenge Score (1% increase per decade, p\u00a0<\u00a00.01), and AUROC (1% decrease per decade, p\u00a0<\u00a00.01) were statistically different on the test set. We observed similar AUROC, AUPRC, Challenge Score, and F-measure values across the different race categories. But, recording-wise accuracies were significantly lower for Black subjects and higher for Asian subjects on the training (31% difference, p\u00a0<\u00a00.01) and test (39% difference, p\u00a0<\u00a00.01) sets. A top performing model was then retrained using an additional constraint which simultaneously minimized differences in performance across sex, race and age. This resulted in a modest reduction in performance, with a significant reduction in bias. This work provides a demonstration that biases manifest as a function of model architecture, population, cost function and optimization metric, all of which should be closely examined in any model.",
      "journal": "Journal of electrocardiology",
      "year": "2022",
      "doi": "10.1016/j.jelectrocard.2022.07.007",
      "authors": "Perez Alday Erick A et al.",
      "keywords": "Age; Bias; Healthcare; Machine learning; Race; Sex",
      "mesh_terms": "Female; Humans; Male; Electrocardiography; Arrhythmias, Cardiac; Sex Factors; Age Factors",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35878534/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "This resulted in a modest reduction in performance, with a significant reduction in bias. This work provides a demonstration that biases manifest as a function of model architecture, population, cost function and optimization metric, all of which should be closely examined in any model.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC11486543"
    },
    {
      "pmid": "35914194",
      "title": "Predicting Race And Ethnicity To Ensure Equitable Algorithms For Health Care Decision Making.",
      "abstract": "Algorithms are currently used to assist in a wide array of health care decisions. Despite the general utility of these health care algorithms, there is growing recognition that they may lead to unintended racially discriminatory practices, raising concerns about the potential for algorithmic bias. An intuitive precaution against such bias is to remove race and ethnicity information as an input to health care algorithms, mimicking the idea of \"race-blind\" decisions. However, we argue that this approach is misguided. Knowledge, not ignorance, of race and ethnicity is necessary to combat algorithmic bias. When race and ethnicity are observed, many methodological approaches can be used to enforce equitable algorithmic performance. When race and ethnicity information is unavailable, which is often the case, imputing them can expand opportunities to not only identify and assess algorithmic bias but also combat it in both clinical and nonclinical settings. A valid imputation method, such as Bayesian Improved Surname Geocoding, can be applied to standard data collected by public and private payers and provider entities. We describe two applications in which imputation of race and ethnicity can help mitigate potential algorithmic biases: equitable disease screening algorithms using machine learning and equitable pay-for-performance incentives.",
      "journal": "Health affairs (Project Hope)",
      "year": "2022",
      "doi": "10.1377/hlthaff.2022.00095",
      "authors": "Cabreros Irineo et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Bayes Theorem; Decision Making; Delivery of Health Care; Ethnicity; Humans; Reimbursement, Incentive",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35914194/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Insurance Status",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "A valid imputation method, such as Bayesian Improved Surname Geocoding, can be applied to standard data collected by public and private payers and provider entities. We describe two applications in which imputation of race and ethnicity can help mitigate potential algorithmic biases: equitable disease screening algorithms using machine learning and equitable pay-for-performance incentives.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "35920547",
      "title": "Using Cg05575921 methylation to predict lung cancer risk: a potentially bias-free precision epigenetics approach.",
      "abstract": "The decision to engage in lung cancer screening (LCS) necessitates weighing benefits versus harms. Previously, clinicians in the United States have used the PLCOM2012 algorithm to guide LCS decision-making. However, that formula contains race and gender-based variables. Previously, using data from a European study, Bojesen and colleagues have suggested that cg05575921 methylation could guide decision-making. To test this hypothesis in a more diverse American population, we examined DNA and clinical data from 3081 subjects from the National Lung Screening Trial (NLST) study. Using survival analysis, we found a simple linear predictor consisting of age, pack-year consumption and cg05575921, to have the best predictive power among several alternatives (AUC\u00a0=\u00a00.66). Results showed that the highest quartile of risk was more than 2-fold more likely to develop lung cancer than those in the lowest quartile. Race, ethnicity, and gender had no effect on prediction with both cg05575921 and pack years contributing equally (both p <\u00a00.003) to risk prediction. Current smokers had considerably lower methylation than former smokers (46% vs 67%; p <\u00a00.001) with the average methylation of those who quit approaching 80% after 25\u00a0years of cessation. Finally, current male smokers had lower mean cg05575921 percentage than female smokers (46% vs 49%; p <\u00a00.001). We conclude that cg05575921 (along with age and pack years) can be used to guide LCS decision-making, and additional studies might focus on how best to use methylation to inform decision-making.",
      "journal": "Epigenetics",
      "year": "2022",
      "doi": "10.1080/15592294.2022.2108082",
      "authors": "Philibert Rob et al.",
      "keywords": "AHRR; DNA methylation; cg05575921; lung cancer; smoking",
      "mesh_terms": "Humans; Male; Female; United States; Lung Neoplasms; Early Detection of Cancer; DNA Methylation; Smoking; Epigenesis, Genetic; Lung",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35920547/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model; Survival Analysis",
      "health_domain": "Oncology; Pulmonology; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Finally, current male smokers had lower mean cg05575921 percentage than female smokers (46% vs 49%; p <\u00a00.001). We conclude that cg05575921 (along with age and pack years) can be used to guide LCS decision-making, and additional studies might focus on how best to use methylation to inform decision-making.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9665144"
    },
    {
      "pmid": "35932546",
      "title": "Interpretability-Guided Inductive Bias For Deep Learning Based Medical Image.",
      "abstract": "Deep learning methods provide state of the art performance for supervised learning based medical image analysis. However it is essential that trained models extract clinically relevant features for downstream tasks as, otherwise, shortcut learning and generalization issues can occur. Furthermore in the medical field, trustability and transparency of current deep learning systems is a much desired property. In this paper we propose an interpretability-guided inductive bias approach enforcing that learned features yield more distinctive and spatially consistent saliency maps for different class labels of trained models, leading to improved model performance. We achieve our objectives by incorporating a class-distinctiveness loss and a spatial-consistency regularization loss term. Experimental results for medical image classification and segmentation tasks show our proposed approach outperforms conventional methods, while yielding saliency maps in higher agreement with clinical experts. Additionally, we show how information from unlabeled images can be used to further boost performance. In summary, the proposed approach is modular, applicable to existing network architectures used for medical imaging applications, and yields improved learning rates, model robustness, and model interpretability.",
      "journal": "Medical image analysis",
      "year": "2022",
      "doi": "10.1016/j.media.2022.102551",
      "authors": "Mahapatra Dwarikanath et al.",
      "keywords": "Inductive bias; Interpretability; Medical image classification; Medical image segmentation",
      "mesh_terms": "Deep Learning; Diagnostic Imaging; Humans; Image Processing, Computer-Assisted",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35932546/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Explainability/Interpretability; Regularization",
      "clinical_setting": "Not specified",
      "key_findings": "Additionally, we show how information from unlabeled images can be used to further boost performance. In summary, the proposed approach is modular, applicable to existing network architectures used for medical imaging applications, and yields improved learning rates, model robustness, and model interpretability.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36010243",
      "title": "A Fair Performance Comparison between Complex-Valued and Real-Valued Neural Networks for Disease Detection.",
      "abstract": "Our aim is to contribute to the classification of anomalous patterns in biosignals using this novel approach. We specifically focus on melanoma and heart murmurs. We use a comparative study of two convolution networks in the Complex and Real numerical domains. The idea is to obtain a powerful approach for building portable systems for early disease detection. Two similar algorithmic structures were chosen so that there is no bias determined by the number of parameters to train. Three clinical data sets, ISIC2017, PH2, and Pascal, were used to carry out the experiments. Mean comparison hypothesis tests were performed to ensure statistical objectivity in the conclusions. In all cases, complex-valued networks presented a superior performance for the Precision, Recall, F1 Score, Accuracy, and Specificity metrics in the detection of associated anomalies. The best complex number-based classifier obtained in the Receiving Operating Characteristic (ROC) space presents a Euclidean distance of 0.26127 with respect to the ideal classifier, as opposed to the best real number-based classifier, whose Euclidean distance to the ideal is 0.36022 for the same task of melanoma detection. The 27.46% superiority in this metric, as in the others reported in this work, suggests that complex-valued networks have a greater ability to extract features for more efficient discrimination in the dataset.",
      "journal": "Diagnostics (Basel, Switzerland)",
      "year": "2022",
      "doi": "10.3390/diagnostics12081893",
      "authors": "Jojoa Mario et al.",
      "keywords": "complex numbers; complex-valued convolution neural networks; complex-valued deep learning; fair performance comparison; real-valued neural networks",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36010243/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Neural Network",
      "health_domain": "Dermatology; Cardiology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The best complex number-based classifier obtained in the Receiving Operating Characteristic (ROC) space presents a Euclidean distance of 0.26127 with respect to the ideal classifier, as opposed to the best real number-based classifier, whose Euclidean distance to the ideal is 0.36022 for the same task of melanoma detection. The 27.46% superiority in this metric, as in the others reported in this work, suggests that complex-valued networks have a greater ability to extract features for more effic...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9406326"
    },
    {
      "pmid": "36046104",
      "title": "Fairness-related performance and explainability effects in deep learning models for brain image analysis.",
      "abstract": "Purpose: Explainability and fairness are two key factors for the effective and ethical clinical implementation of deep learning-based machine learning models in healthcare settings. However, there has been limited work on investigating how unfair performance manifests in explainable artificial intelligence (XAI) methods, and how XAI can be used to investigate potential reasons for unfairness. Thus, the aim of this work was to analyze the effects of previously established sociodemographic-related confounders on classifier performance and explainability methods. Approach: A convolutional neural network (CNN) was trained to predict biological sex from T1-weighted brain MRI datasets of 4547 9- to 10-year-old adolescents from the Adolescent Brain Cognitive Development study. Performance disparities of the trained CNN between White and Black subjects were analyzed and saliency maps were generated for each subgroup at the intersection of sex and race. Results: The classification model demonstrated a significant difference in the percentage of correctly classified White male ( 90.3 % \u00b1 1.7 %  ) and Black male ( 81.1 % \u00b1 4.5 %  ) children. Conversely, slightly higher performance was found for Black female ( 89.3 % \u00b1 4.8 %  ) compared with White female ( 86.5 % \u00b1 2.0 %  ) children. Saliency maps showed subgroup-specific differences, corresponding to brain regions previously associated with pubertal development. In line with this finding, average pubertal development scores of subjects used in this study were significantly different between Black and White females ( p < 0.001  ) and males ( p < 0.001  ). Conclusions: We demonstrate that a CNN with significantly different sex classification performance between Black and White adolescents can identify different important brain regions when comparing subgroup saliency maps. Importance scores vary substantially between subgroups within brain structures associated with pubertal development, a race-associated confounder for predicting sex. We illustrate that unfair models can produce different XAI results between subgroups and that these results may explain potential reasons for biased performance.",
      "journal": "Journal of medical imaging (Bellingham, Wash.)",
      "year": "2022",
      "doi": "10.1117/1.JMI.9.6.061102",
      "authors": "Stanley Emma A M et al.",
      "keywords": "adolescent brain cognitive development study; bias; explainable artificial intelligence; fairness; machine learning; magnetic resonance imaging",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36046104/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; NLP/LLM; Neural Network",
      "health_domain": "Radiology/Medical Imaging; Pediatrics; Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Geographic",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Conclusions: We demonstrate that a CNN with significantly different sex classification performance between Black and White adolescents can identify different important brain regions when comparing subgroup saliency maps. Importance scores vary substantially between subgroups within brain structures associated with pubertal development, a race-associated confounder for predicting sex. We illustrate that unfair models can produce different XAI results between subgroups and that these results may e...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9412191"
    },
    {
      "pmid": "36054492",
      "title": "A Machine Learning-Based Approach to Discrimination of Tauopathies Using [18 F]PM-PBB3 PET Images.",
      "abstract": "BACKGROUND: We recently developed a positron emission tomography (PET) probe, [18 F]PM-PBB3, to detect tau lesions in diverse tauopathies, including mixed three-repeat and four-repeat (3R\u2009+\u20094R) tau fibrils in Alzheimer's disease (AD) and 4R tau aggregates in progressive supranuclear palsy (PSP). For wider availability of this technology for clinical settings, bias-free quantitative evaluation of tau images without a priori disease information is needed. OBJECTIVE: We aimed to establish tau PET pathology indices to characterize PSP and AD using a machine learning approach and test their validity and tracer capabilities. METHODS: Data were obtained from 50 healthy control subjects, 46 patients with PSP Richardson syndrome, and 37 patients on the AD continuum. Tau PET data from 114 regions of interest were subjected to Elastic Net cross-validation linear classification analysis with a one-versus-the-rest multiclass strategy to obtain a linear function that discriminates diseases by maximizing the area under the receiver operating characteristic curve. We defined PSP- and AD-tau scores for each participant as values of the functions optimized for differentiating PSP (4R) and AD (3R\u2009+\u20094R), respectively, from others. RESULTS: The discriminatory ability of PSP- and AD-tau scores assessed as the area under the receiver operating characteristic curve was 0.98 and 1.00, respectively. PSP-tau scores correlated with the PSP rating scale in patients with PSP, and AD-tau scores correlated with Mini-Mental State Examination scores in healthy control-AD continuum patients. The globus pallidus and amygdala were highlighted as regions with high weight coefficients for determining PSP- and AD-tau scores, respectively. CONCLUSIONS: These findings highlight our technology's unbiased capability to identify topologies of 3R\u2009+\u20094R versus 4R tau deposits. \u00a9 2022 The Authors. Movement Disorders published by Wiley Periodicals LLC on behalf of International Parkinson and Movement Disorder Society.",
      "journal": "Movement disorders : official journal of the Movement Disorder Society",
      "year": "2022",
      "doi": "10.1002/mds.29173",
      "authors": "Endo Hironobu et al.",
      "keywords": "Alzheimer's disease; machine learning; progressive supranuclear palsy; tau PET; tauopathy",
      "mesh_terms": "Humans; tau Proteins; Brain; Tauopathies; Supranuclear Palsy, Progressive; Positron-Emission Tomography; Alzheimer Disease; Movement Disorders; Machine Learning",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36054492/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Pathology; Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "CONCLUSIONS: These findings highlight our technology's unbiased capability to identify topologies of 3R\u2009+\u20094R versus 4R tau deposits. \u00a9 2022 The Authors. Movement Disorders published by Wiley Periodicals LLC on behalf of International Parkinson and Movement Disorder Society.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9805085"
    },
    {
      "pmid": "36086432",
      "title": "Fair and Privacy-Preserving Alzheimer's Disease Diagnosis Based on Spontaneous Speech Analysis via Federated Learning.",
      "abstract": "As the most common neurodegenerative disease among older adults, Alzheimer's disease (AD) would lead to loss of memory, impaired language and judgment, gait disorders, and other cognitive deficits severe enough to interfere with daily activities and significantly diminish quality of life. Recent research has shown promising results in automatic AD diagnosis via speech, leveraging the advances of deep learning in the audio domain. However, most existing studies rely on a centralized learning framework which requires subjects' voice data to be gathered to a central server, raising severe privacy concerns. To resolve this, in this paper, we propose the first federated-learning-based approach for achieving automatic AD diagnosis via spontaneous speech analysis while ensuring the subjects' data privacy. Extensive experiments under various federated learning settings on the ADReSS challenge dataset show that the proposed model can achieve high accuracy for AD detection while achieving privacy preservation. To ensure fairness of the model performance across clients in federated settings, we further deploy fair aggregation mechanisms, particularly q-FEDAvg and q-FEDSgd, which greatly reduces the algorithmic biases due to the data heterogeneity among the clients. Clinical Relevance -The experiments were conducted on publicly available clinical datasets. No humans or animals were involved.",
      "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference",
      "year": "2022",
      "doi": "10.1109/EMBC48229.2022.9871204",
      "authors": "Ali Meerza Syed Irfan et al.",
      "keywords": "",
      "mesh_terms": "Alzheimer Disease; Humans; Neurodegenerative Diseases; Privacy; Quality of Life; Speech",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36086432/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Federated Learning",
      "health_domain": "ICU/Critical Care; Neurology",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Federated Learning",
      "clinical_setting": "ICU",
      "key_findings": "Clinical Relevance -The experiments were conducted on publicly available clinical datasets. No humans or animals were involved.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36126552",
      "title": "Jointly estimating bias field and reconstructing uniform MRI image by deep learning.",
      "abstract": "Bias field is one of the main artifacts that degrade the quality of magnetic resonance images. It introduces intensity inhomogeneity and affects image analysis such as segmentation. In this work, we proposed a deep learning approach to jointly estimate bias field and reconstruct uniform image. By modeling the quality degradation process as the product of a spatially varying field and a uniform image, the network was trained on 800 images with true bias fields from 12 healthy subjects. A network structure of bias field estimation and uniform image reconstruction was designed to compensate for the intensity loss. To further evaluate the benefit of bias field correction, a quantitative analysis was made on image segmentation. Experimental results show that the proposed BFCNet improves the image uniformity by 8.3% and 10.1%, the segmentation accuracy by 4.1% and 6.8% on white and grey matter in T2-weighted brain images. Moreover, BFCNet outperforms the state-of-the-art traditional methods and deep learning methods on estimating bias field and preserving image structure, and BFCNet is robust to different levels of bias field and noise.",
      "journal": "Journal of magnetic resonance (San Diego, Calif. : 1997)",
      "year": "2022",
      "doi": "10.1016/j.jmr.2022.107301",
      "authors": "Song Wenke et al.",
      "keywords": "Bias field correction; Deep learning; Intensity inhomogeneity",
      "mesh_terms": "Humans; Algorithms; Deep Learning; Magnetic Resonance Imaging; Image Processing, Computer-Assisted; Artifacts; Brain",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36126552/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Experimental results show that the proposed BFCNet improves the image uniformity by 8.3% and 10.1%, the segmentation accuracy by 4.1% and 6.8% on white and grey matter in T2-weighted brain images. Moreover, BFCNet outperforms the state-of-the-art traditional methods and deep learning methods on estimating bias field and preserving image structure, and BFCNet is robust to different levels of bias field and noise.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36204539",
      "title": "Mitigating Bias in Radiology Machine Learning: 3. Performance Metrics.",
      "abstract": "The increasing use of machine learning (ML) algorithms in clinical settings raises concerns about bias in ML models. Bias can arise at any step of ML creation, including data handling, model development, and performance evaluation. Potential biases in the ML model can be minimized by implementing these steps correctly. This report focuses on performance evaluation and discusses model fitness, as well as a set of performance evaluation toolboxes: namely, performance metrics, performance interpretation maps, and uncertainty quantification. By discussing the strengths and limitations of each toolbox, our report highlights strategies and considerations to mitigate and detect biases during performance evaluations of radiology artificial intelligence models. Keywords: Segmentation, Diagnosis, Convolutional Neural Network (CNN) \u00a9 RSNA, 2022.",
      "journal": "Radiology. Artificial intelligence",
      "year": "2022",
      "doi": "10.1148/ryai.220061",
      "authors": "Faghani Shahriar et al.",
      "keywords": "Convolutional Neural Network (CNN); Diagnosis; Segmentation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36204539/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Development/Training; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "By discussing the strengths and limitations of each toolbox, our report highlights strategies and considerations to mitigate and detect biases during performance evaluations of radiology artificial intelligence models. Keywords: Segmentation, Diagnosis, Convolutional Neural Network (CNN) \u00a9 RSNA, 2022.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9530766"
    },
    {
      "pmid": "36204544",
      "title": "Mitigating Bias in Radiology Machine Learning: 1. Data Handling.",
      "abstract": "Minimizing bias is critical to adoption and implementation of machine learning (ML) in clinical practice. Systematic mathematical biases produce consistent and reproducible differences between the observed and expected performance of ML systems, resulting in suboptimal performance. Such biases can be traced back to various phases of ML development: data handling, model development, and performance evaluation. This report presents 12 suboptimal practices during data handling of an ML study, explains how those practices can lead to biases, and describes what may be done to mitigate them. Authors employ an arbitrary and simplified framework that splits ML data handling into four steps: data collection, data investigation, data splitting, and feature engineering. Examples from the available research literature are provided. A Google Colaboratory Jupyter notebook includes code examples to demonstrate the suboptimal practices and steps to prevent them. Keywords: Data Handling, Bias, Machine Learning, Deep Learning, Convolutional Neural Network (CNN), Computer-aided Diagnosis (CAD) \u00a9 RSNA, 2022.",
      "journal": "Radiology. Artificial intelligence",
      "year": "2022",
      "doi": "10.1148/ryai.210290",
      "authors": "Rouzrokh Pouria et al.",
      "keywords": "Bias; Computer-aided Diagnosis (CAD); Convolutional Neural Network (CNN); Data Handling; Deep Learning; Machine Learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36204544/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Development/Training; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "A Google Colaboratory Jupyter notebook includes code examples to demonstrate the suboptimal practices and steps to prevent them. Keywords: Data Handling, Bias, Machine Learning, Deep Learning, Convolutional Neural Network (CNN), Computer-aided Diagnosis (CAD) \u00a9 RSNA, 2022.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9533091"
    },
    {
      "pmid": "36247412",
      "title": "Real-world performance, long-term efficacy, and absence of bias in the artificial intelligence enhanced electrocardiogram to detect left ventricular systolic dysfunction.",
      "abstract": "AIMS: Some artificial intelligence models applied in medical practice require ongoing retraining, introduce unintended racial bias, or have variable performance among different subgroups of patients. We assessed the real-world performance of the artificial intelligence-enhanced electrocardiogram to detect left ventricular systolic dysfunction with respect to multiple patient and electrocardiogram variables to determine the algorithm's long-term efficacy and potential bias in the absence of retraining. METHODS AND RESULTS: Electrocardiograms acquired in 2019 at Mayo Clinic in Minnesota, Arizona, and Florida with an echocardiogram performed within 14 days were analyzed (n = 44 986 unique patients). The area under the curve (AUC) was calculated to evaluate performance of the algorithm among age groups, racial and ethnic groups, patient encounter location, electrocardiogram features, and over time. The artificial intelligence-enhanced electrocardiogram to detect left ventricular systolic dysfunction had an AUC of 0.903 for the total cohort. Time series analysis of the model validated its temporal stability. Areas under the curve were similar for all racial and ethnic groups (0.90-0.92) with minimal performance difference between sexes. Patients with a 'normal sinus rhythm' electrocardiogram (n = 37 047) exhibited an AUC of 0.91. All other electrocardiogram features had areas under the curve between 0.79 and 0.91, with the lowest performance occurring in the left bundle branch block group (0.79). CONCLUSION: The artificial intelligence-enhanced electrocardiogram to detect left ventricular systolic dysfunction is stable over time in the absence of retraining and robust with respect to multiple variables including time, patient race, and electrocardiogram features.",
      "journal": "European heart journal. Digital health",
      "year": "2022",
      "doi": "10.1093/ehjdh/ztac028",
      "authors": "Harmon David M et al.",
      "keywords": "Arrhythmia; Artificial intelligence; Deep learning; Digital medicine; ECG; Heart failure",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36247412/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: The artificial intelligence-enhanced electrocardiogram to detect left ventricular systolic dysfunction is stable over time in the absence of retraining and robust with respect to multiple variables including time, patient race, and electrocardiogram features.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9558265"
    },
    {
      "pmid": "36284736",
      "title": "Ethical Redress of Racial Inequities in AI: Lessons from Decoupling Machine Learning from Optimization in Medical Appointment Scheduling.",
      "abstract": "An Artificial Intelligence algorithm trained on data that reflect racial biases may yield racially biased outputs, even if the algorithm on its own is unbiased. For example, algorithms used to schedule medical appointments in the USA predict that Black patients are at a higher risk of no-show than non-Black patients, though technically accurate given existing data that prediction results in Black patients being overwhelmingly scheduled in appointment slots that cause longer wait times than non-Black patients. This perpetuates racial inequity, in this case lesser access to medical care. This gives rise to one type of Accuracy-Fairness trade-off: preserve the efficiency offered by using AI to schedule appointments or discard that efficiency in order to avoid perpetuating ethno-racial disparities. Similar trade-offs arise in a range of AI applications including others in medicine, as well as in education, judicial systems, and public security, among others. This article presents a framework for addressing such trade-offs where Machine Learning and Optimization components of the algorithm are decoupled. Applied to medical appointment scheduling, our framework articulates four approaches intervening in different ways on different components of the algorithm. Each yields specific results, in one case preserving accuracy comparable to the current state-of-the-art while eliminating the disparity.",
      "journal": "Philosophy & technology",
      "year": "2022",
      "doi": "10.1007/s13347-022-00590-8",
      "authors": "Shanklin Robert et al.",
      "keywords": "Artificial Intelligence; Bias; Ethics; Healthcare; Machine Learning; Racial\u00a0Disparities",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36284736/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "Applied to medical appointment scheduling, our framework articulates four approaches intervening in different ways on different components of the algorithm. Each yields specific results, in one case preserving accuracy comparable to the current state-of-the-art while eliminating the disparity.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9584259"
    },
    {
      "pmid": "36301408",
      "title": "Equity in AgeTech for Ageing Well in Technology-Driven Places: The Role of Social Determinants in Designing AI-based Assistive Technologies.",
      "abstract": "AgeTech involves the use of emerging technologies to support the health, well-being and independent living of older adults. In this paper we focus on how AgeTech based on artificial intelligence (AI) may better support older adults to remain in their own living environment for longer, provide social connectedness, support wellbeing and mental health, and enable social participation. In order to assess and better understand the positive as well as negative outcomes of AI-based AgeTech, a critical analysis of ethical design, digital equity, and policy pathways is required. A crucial question is how AI-based AgeTech may drive practical, equitable, and inclusive multilevel solutions to support healthy, active ageing.In our paper, we aim to show that a focus on equity is key for AI-based AgeTech if it is to realize its full potential. We propose that equity should not just be an extra benefit or minimum requirement, but the explicit aim of designing AI-based health tech. This means that social determinants that affect the use of or access to these technologies have to be addressed. We will explore how complexity management as a crucial element of AI-based AgeTech may potentially create and exacerbate social inequities by marginalising or ignoring social determinants. We identify bias, standardization, and access as main ethical issues in this context and subsequently, make recommendations as to how inequities that stem form AI-based AgeTech can be addressed.",
      "journal": "Science and engineering ethics",
      "year": "2022",
      "doi": "10.1007/s11948-022-00397-y",
      "authors": "Rubeis Giovanni et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Social Determinants of Health; Healthy Aging; Self-Help Devices; Technology",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36301408/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We will explore how complexity management as a crucial element of AI-based AgeTech may potentially create and exacerbate social inequities by marginalising or ignoring social determinants. We identify bias, standardization, and access as main ethical issues in this context and subsequently, make recommendations as to how inequities that stem form AI-based AgeTech can be addressed.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9613787"
    },
    {
      "pmid": "36313215",
      "title": "Turing test-inspired method for analysis of biases prevalent in artificial intelligence-based medical imaging.",
      "abstract": "Due to the growing need to provide better global healthcare, computer-based and robotic healthcare equipment that depend on artificial intelligence has seen an increase in development. In order to evaluate artificial intelligence (AI) in computer technology, the Turing test was created. For evaluating the future generation of medical diagnostics and medical robots, it remains an essential qualitative instrument. We propose a novel methodology to assess AI-based healthcare technology that provided verifiable diagnostic accuracy and statistical robustness. In order to run our test, we used a state-of-the-art AI model and compared it to radiologists for checking how generalized the model is and if any biases are prevalent. We achieved results that can evaluate the performance of our chosen model for this study in a clinical setting and we also applied a quantifiable method for evaluating our modified Turing test results using a meta-analytical evaluation framework. His test provides a translational standard for upcoming AI modalities. Our modified Turing test is a notably strong standard to measure the actual performance of the AI model on a variety of edge cases and normal cases and also helps in detecting if the algorithm is biased towards any one type of case. This method extends the flexibility to detect any prevalent biases and also classify the type of bias.",
      "journal": "AI and ethics",
      "year": "2022",
      "doi": "10.1007/s43681-022-00227-8",
      "authors": "Tripathi Satvik et al.",
      "keywords": "Artificial intelligence; Diagnostic tests; Fairness; Healthcare; Turing test",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36313215/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Our modified Turing test is a notably strong standard to measure the actual performance of the AI model on a variety of edge cases and normal cases and also helps in detecting if the algorithm is biased towards any one type of case. This method extends the flexibility to detect any prevalent biases and also classify the type of bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9590390"
    },
    {
      "pmid": "36378761",
      "title": "Improving Fairness in the Prediction of Heart Failure Length of Stay and Mortality by Integrating Social Determinants of Health.",
      "abstract": "BACKGROUND: Machine learning (ML) approaches have been broadly applied to the prediction of length of stay and mortality in hospitalized patients. ML may also reduce societal health burdens, assist in health resources planning and improve health outcomes. However, the fairness of these ML models across ethnoracial or socioeconomic subgroups is rarely assessed or discussed. In this study, we aim (1) to quantify the algorithmic bias of ML models when predicting the probability of long-term hospitalization or in-hospital mortality for different heart failure (HF) subpopulations, and (2) to propose a novel method that can improve the fairness of our models without compromising predictive power. METHODS: We built 5 ML classifiers to predict the composite outcome of hospitalization length-of-stay and in-hospital mortality for 210\u2009368 HF patients extracted from the Get With The Guidelines-Heart Failure registry data set. We integrated 15 social determinants of health variables, including the Social Deprivation Index and the Area Deprivation Index, into the feature space of ML models based on patients' geographies to mitigate the algorithmic bias. RESULTS: The best-performing random forest model demonstrated modest predictive power but selectively underdiagnosed underserved subpopulations, for example, female, Black, and socioeconomically disadvantaged patients. The integration of social determinants of health variables can significantly improve fairness without compromising model performance. CONCLUSIONS: We quantified algorithmic bias against underserved subpopulations in the prediction of the composite outcome for HF patients. We provide a potential direction to reduce disparities of ML-based predictive models by integrating social determinants of health variables. We urge fellow researchers to strongly consider ML fairness when developing predictive models for HF patients.",
      "journal": "Circulation. Heart failure",
      "year": "2022",
      "doi": "10.1161/CIRCHEARTFAILURE.122.009473",
      "authors": "Li Yikuan et al.",
      "keywords": "bias; healthcare disparities; heart failure; machine learning; social determinants of health",
      "mesh_terms": "Humans; Female; Heart Failure; Length of Stay; Social Determinants of Health; Hospitalization; Hospital Mortality",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36378761/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Random Forest; Clinical Prediction Model",
      "health_domain": "Cardiology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Subgroup Analysis",
      "clinical_setting": "Hospital/Inpatient; Public Health/Population; Safety-Net/Underserved",
      "key_findings": "CONCLUSIONS: We quantified algorithmic bias against underserved subpopulations in the prediction of the composite outcome for HF patients. We provide a potential direction to reduce disparities of ML-based predictive models by integrating social determinants of health variables. We urge fellow researchers to strongly consider ML fairness when developing predictive models for HF patients.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9673161"
    },
    {
      "pmid": "36387013",
      "title": "Application of convex hull analysis for the evaluation of data heterogeneity between patient populations of different origin and implications of hospital bias in downstream machine-learning-based data processing: A comparison of 4 critical-care patient datasets.",
      "abstract": "Machine learning (ML) models are developed on a learning dataset covering only a small part of the data of interest. If model predictions are accurate for the learning dataset but fail for unseen data then generalization error is considered high. This problem manifests itself within all major sub-fields of ML but is especially relevant in medical applications. Clinical data structures, patient cohorts, and clinical protocols may be highly biased among hospitals such that sampling of representative learning datasets to learn ML models remains a challenge. As ML models exhibit poor predictive performance over data ranges sparsely or not covered by the learning dataset, in this study, we propose a novel method to assess their generalization capability among different hospitals based on the convex hull (CH) overlap between multivariate datasets. To reduce dimensionality effects, we used a two-step approach. First, CH analysis was applied to find mean CH coverage between each of the two datasets, resulting in an upper bound of the prediction range. Second, 4 types of ML models were trained to classify the origin of a dataset (i.e., from which hospital) and to estimate differences in datasets with respect to underlying distributions. To demonstrate the applicability of our method, we used 4 critical-care patient datasets from different hospitals in Germany and USA. We estimated the similarity of these populations and investigated whether ML models developed on one dataset can be reliably applied to another one. We show that the strongest drop in performance was associated with the poor intersection of convex hulls in the corresponding hospitals' datasets and with a high performance of ML methods for dataset discrimination. Hence, we suggest the application of our pipeline as a first tool to assess the transferability of trained models. We emphasize that datasets from different hospitals represent heterogeneous data sources, and the transfer from one database to another should be performed with utmost care to avoid implications during real-world applications of the developed models. Further research is needed to develop methods for the adaptation of ML models to new hospitals. In addition, more work should be aimed at the creation of gold-standard datasets that are large and diverse with data from varied application sites.",
      "journal": "Frontiers in big data",
      "year": "2022",
      "doi": "10.3389/fdata.2022.603429",
      "authors": "Sharafutdinov Konstantin et al.",
      "keywords": "ARDS; convex hull (CH); data pooling; dataset-bias; generalization error",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36387013/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; Public Health/Population",
      "key_findings": "Further research is needed to develop methods for the adaptation of ML models to new hospitals. In addition, more work should be aimed at the creation of gold-standard datasets that are large and diverse with data from varied application sites.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9659720"
    },
    {
      "pmid": "36396503",
      "title": "Algorithmic bias in health care: Opportunities for nurses to improve equality in the age of artificial intelligence.",
      "abstract": "",
      "journal": "Nursing outlook",
      "year": "2022",
      "doi": "10.1016/j.outlook.2022.09.003",
      "authors": "O'Connor Siobhan et al.",
      "keywords": "Algorithms; Artificial Intelligence; Bias; Health care; Machine learning; Natural language processing; Neural networks; Nursing",
      "mesh_terms": "Humans; Artificial Intelligence; Bias; Delivery of Health Care",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36396503/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "36414774",
      "title": "Mitigating the impact of biased artificial intelligence in emergency decision-making.",
      "abstract": "BACKGROUND: Prior research has shown that artificial intelligence (AI) systems often encode biases against minority subgroups. However, little work has focused on ways to mitigate the harm discriminatory algorithms can cause in high-stakes settings such as medicine. METHODS: In this study, we experimentally evaluated the impact biased AI recommendations have on emergency decisions, where participants respond to mental health crises by calling for either medical or police assistance. We recruited 438 clinicians and 516 non-experts to participate in our web-based experiment. We evaluated participant decision-making with and without advice from biased and unbiased AI systems. We also varied the style of the AI advice, framing it either as prescriptive recommendations or descriptive flags. RESULTS: Participant decisions are unbiased without AI advice. However, both clinicians and non-experts are influenced by prescriptive recommendations from a biased algorithm, choosing police help more often in emergencies involving African-American or Muslim men. Crucially, using descriptive flags rather than prescriptive recommendations allows respondents to retain their original, unbiased decision-making. CONCLUSIONS: Our work demonstrates the practical danger of using biased models in health contexts, and suggests that appropriately framing decision support can mitigate the effects of AI bias. These findings must be carefully considered in the many real-world clinical scenarios where inaccurate or biased models may be used to inform important decisions. Artificial intelligence (AI) systems that make decisions based on historical data are increasingly common in health care settings. However, many AI models exhibit problematic biases, as data often reflect human prejudices against minority groups. In this study, we used a web-based experiment to evaluate the impact biased models can have when used to inform human decisions. We found that though participants were not inherently biased, they were strongly influenced by advice from a biased model if it was offered prescriptively (i.e., \u201cyou should do X\u201d). This adherence led their decisions to be biased against African-American and Muslims individuals. However, framing the same advice descriptively (i.e., without recommending a specific action) allowed participants to remain fair. These results demonstrate that though discriminatory AI can lead to poor outcomes for minority groups, appropriately framing advice can help mitigate its effects.",
      "journal": "Communications medicine",
      "year": "2022",
      "doi": "10.1038/s43856-022-00214-4",
      "authors": "Adam Hammaad et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36414774/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Our work demonstrates the practical danger of using biased models in health contexts, and suggests that appropriately framing decision support can mitigate the effects of AI bias. These findings must be carefully considered in the many real-world clinical scenarios where inaccurate or biased models may be used to inform important decisions. Artificial intelligence (AI) systems that make decisions based on historical data are increasingly common in health care settings.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9681767"
    },
    {
      "pmid": "36450391",
      "title": "Artificial intelligence and health inequities in primary care: a systematic scoping review and framework.",
      "abstract": "OBJECTIVE: Artificial intelligence (AI) will have a significant impact on healthcare over the coming decade. At the same time, health inequity remains one of the biggest challenges. Primary care is both a driver and a mitigator of health inequities and with AI gaining traction in primary care, there is a need for a holistic understanding of how AI affect health inequities, through the act of providing care and through potential system effects. This paper presents a systematic scoping review of the ways AI implementation in primary care may impact health inequity. DESIGN: Following a systematic scoping review approach, we searched for literature related to AI, health inequity, and implementation challenges of AI in primary care. In addition, articles from primary exploratory searches were added, and through reference screening.The results were thematically summarised and used to produce both a narrative and conceptual model for the mechanisms by which social determinants of health and AI in primary care could interact to either improve or worsen health inequities.Two public advisors were involved in the review process. ELIGIBILITY CRITERIA: Peer-reviewed publications and grey literature in English and Scandinavian languages. INFORMATION SOURCES: PubMed, SCOPUS and JSTOR. RESULTS: A total of 1529 publications were identified, of which 86 met the inclusion criteria. The findings were summarised under six different domains, covering both positive and negative effects: (1) access, (2) trust, (3) dehumanisation, (4) agency for self-care, (5) algorithmic bias and (6) external effects. The five first domains cover aspects of the interface between the patient and the primary care system, while the last domain covers care system-wide and societal effects of AI in primary care. A graphical model has been produced to illustrate this. Community involvement throughout the whole process of designing and implementing of AI in primary care was a common suggestion to mitigate the potential negative effects of AI. CONCLUSION: AI has the potential to affect health inequities through a multitude of ways, both directly in the patient consultation and through transformative system effects. This review summarises these effects from a system tive and provides a base for future research into responsible implementation.",
      "journal": "Family medicine and community health",
      "year": "2022",
      "doi": "10.1136/fmch-2022-001670",
      "authors": "d'Elia Alexander et al.",
      "keywords": "General Practice; Health Equity; Healthcare Disparities",
      "mesh_terms": "Humans; Artificial Intelligence; Health Inequities; Gray Literature; PubMed; Primary Health Care",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36450391/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Primary Care",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Primary Care/Outpatient; Public Health/Population",
      "key_findings": "CONCLUSION: AI has the potential to affect health inequities through a multitude of ways, both directly in the patient consultation and through transformative system effects. This review summarises these effects from a system tive and provides a base for future research into responsible implementation.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9716837"
    },
    {
      "pmid": "36463866",
      "title": "Special Section on Inclusive Digital Health: Notable Papers on Addressing Bias, Equity, and Literacy to Strengthen Health Systems.",
      "abstract": "OBJECTIVE: To summarize significant research contributions on addressing bias, equity, and literacy in health delivery systems published in 2021. METHODS: An extensive search using PubMed and Scopus was conducted to identify peer-reviewed articles published in 2021 that examined ways that informatics methods, approaches, and tools could address bias, equity, and literacy in health systems and care delivery processes. The selection process comprised three steps: (1) 15 candidate best papers were first selected by the two section editors; (2) external reviewers from internationally renowned research teams reviewed each candidate best paper; and (3) the final selection of three best papers was conducted by the editorial committee of the Yearbook. RESULTS: Selected best papers represent studies that characterized significant challenges facing biomedical informatics with respect to equity and practices that support equity and literacy in the design of health information systems. Selected papers represent the full spectrum of this year's yearbook theme. In general, papers identified in the search fell into one of the following categories: (1) descriptive accounts of algorithmic bias in medical software or machine learning approaches; (2) enabling health information systems to appropriately encode for gender identity and sex; (3) approaches to support health literacy among individuals who interact with information systems and mobile applications; and (4) approaches to engage diverse populations in the use of health information systems and the biomedical informatics workforce CONCLUSIONS: : Although the selected papers are notable, our collective efforts as a biomedical informatics community to address equity, literacy, and bias remain nascent. More work is needed to ensure health information systems are just in their use of advanced computing approaches and all persons have equal access to health care and informatics tools.",
      "journal": "Yearbook of medical informatics",
      "year": "2022",
      "doi": "10.1055/s-0042-1742536",
      "authors": "Dixon Brian E et al.",
      "keywords": "",
      "mesh_terms": "Female; Humans; Male; Gender Identity; Bias; Health Literacy; Health Information Systems; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36463866/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "EHR/Health Informatics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSIONS: : Although the selected papers are notable, our collective efforts as a biomedical informatics community to address equity, literacy, and bias remain nascent. More work is needed to ensure health information systems are just in their use of advanced computing approaches and all persons have equal access to health care and informatics tools.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9719755"
    },
    {
      "pmid": "36577851",
      "title": "AI in the hands of imperfect users.",
      "abstract": "As the use of artificial intelligence and machine learning (AI/ML) continues to expand in healthcare, much attention has been given to mitigating bias in algorithms to ensure they are employed fairly and transparently. Less attention has fallen to addressing potential bias among AI/ML's human users or factors that influence user reliance. We argue for a systematic approach to identifying the existence and impacts of user biases while using AI/ML tools and call for the development of embedded interface design features, drawing on insights from decision science and behavioral economics, to nudge users towards more critical and reflective decision making using AI/ML.",
      "journal": "NPJ digital medicine",
      "year": "2022",
      "doi": "10.1038/s41746-022-00737-z",
      "authors": "Kostick-Quenet Kristin M et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36577851/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Less attention has fallen to addressing potential bias among AI/ML's human users or factors that influence user reliance. We argue for a systematic approach to identifying the existence and impacts of user biases while using AI/ML tools and call for the development of embedded interface design features, drawing on insights from decision science and behavioral economics, to nudge users towards more critical and reflective decision making using AI/ML.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 4 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9795935"
    },
    {
      "pmid": "36577946",
      "title": "External control arm analysis: an evaluation of propensity score approaches, G-computation, and doubly debiased machine learning.",
      "abstract": "BACKGROUND: An external control arm is a cohort of control patients that are collected from data external to a single-arm trial. To provide an unbiased estimation of efficacy, the clinical profiles of patients from single and external arms should be aligned, typically using propensity score approaches. There are alternative approaches to infer efficacy based on comparisons between outcomes of single-arm patients and machine-learning predictions of control patient outcomes. These methods include G-computation and Doubly Debiased Machine Learning (DDML) and their evaluation for External Control Arms (ECA) analysis is insufficient. METHODS: We consider both numerical simulations and a trial replication procedure to evaluate the different statistical approaches: propensity score matching, Inverse Probability of Treatment Weighting (IPTW), G-computation, and DDML. The replication study relies on five type 2 diabetes randomized clinical trials granted by the Yale University Open Data Access (YODA) project. From the pool of five trials, observational experiments are artificially built by replacing a control arm from one trial by an arm originating from another trial and containing similarly-treated patients. RESULTS: Among the different statistical approaches, numerical simulations show that DDML has the smallest bias followed by G-computation. In terms of mean squared error, G-computation usually minimizes mean squared error. Compared to other methods, DDML has varying Mean Squared Error performances that improves with increasing sample sizes. For hypothesis testing, all methods control type I error and DDML is the most conservative. G-computation is the best method in terms of statistical power, and DDML has comparable power at [Formula: see text] but inferior ones for smaller sample sizes. The replication procedure also indicates that G-computation minimizes mean squared error whereas DDML has intermediate performances in between G-computation and propensity score approaches. The confidence intervals of G-computation are the narrowest whereas confidence intervals obtained with DDML are the widest for small sample sizes, which confirms its conservative nature. CONCLUSIONS: For external control arm analyses, methods based on outcome prediction models can reduce estimation error and increase statistical power compared to propensity score approaches.",
      "journal": "BMC medical research methodology",
      "year": "2022",
      "doi": "10.1186/s12874-022-01799-z",
      "authors": "Loiseau Nicolas et al.",
      "keywords": "Average treatment effect; Confounding variables; Counterfactual; Doubly robust; Observational study; Propensity score; Replication study",
      "mesh_terms": "Humans; Bias; Computer Simulation; Diabetes Mellitus, Type 2; Machine Learning; Propensity Score; Research Design; Randomized Controlled Trials as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36577946/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Endocrinology/Diabetes",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Clinical Trial",
      "key_findings": "CONCLUSIONS: For external control arm analyses, methods based on outcome prediction models can reduce estimation error and increase statistical power compared to propensity score approaches.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9795588"
    },
    {
      "pmid": "36601036",
      "title": "Imputation Strategies Under Clinical Presence: Impact on Algorithmic Fairness.",
      "abstract": "Biases have marked medical history, leading to unequal care affecting marginalised groups. The patterns of missingness in observational data often reflect these group discrepancies, but the algorithmic fairness implications of group-specific missingness are not well understood. Despite its potential impact, imputation is too often an overlooked preprocessing step. When explicitly considered, attention is placed on overall performance, ignoring how this preprocessing can reinforce groupspecific inequities. Our work questions this choice by studying how imputation affects downstream algorithmic fairness. First, we provide a structured view of the relationship between clinical presence mechanisms and groupspecific missingness patterns. Then, through simulations and real-world experiments, we demonstrate that the imputation choice influences marginalised group performance and that no imputation strategy consistently reduces disparities. Importantly, our results show that current practices may endanger health equity as similarly performing imputation strategies at the population level can affect marginalised groups differently. Finally, we propose recommendations for mitigating inequities that may stem from a neglected step of the machine learning pipeline.",
      "journal": "Proceedings of machine learning research",
      "year": "2022",
      "doi": "",
      "authors": "Jeanselme Vincent et al.",
      "keywords": "Clinical Presence; Fairness; Imputation",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36601036/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Preprocessing; Deployment",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Importantly, our results show that current practices may endanger health equity as similarly performing imputation strategies at the population level can affect marginalised groups differently. Finally, we propose recommendations for mitigating inequities that may stem from a neglected step of the machine learning pipeline.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7614014"
    },
    {
      "pmid": "36743404",
      "title": "A machine learning approach to quantify gender bias in collaboration practices of mathematicians.",
      "abstract": "Collaboration practices have been shown to be crucial determinants of scientific careers. We examine the effect of gender on coauthorship-based collaboration in mathematics, a discipline in which women continue to be underrepresented, especially in higher academic positions. We focus on two key aspects of scientific collaboration-the number of different coauthors and the number of single authorships. A higher number of coauthors has a positive effect on, e.g., the number of citations and productivity, while single authorships, for example, serve as evidence of scientific maturity and help to send a clear signal of one's proficiency to the community. Using machine learning-based methods, we show that collaboration networks of female mathematicians are slightly larger than those of their male colleagues when potential confounders such as seniority or total number of publications are controlled, while they author significantly fewer papers on their own. This confirms previous descriptive explorations and provides more precise models for the role of gender in collaboration in mathematics.",
      "journal": "Frontiers in big data",
      "year": "2022",
      "doi": "10.3389/fdata.2022.989469",
      "authors": "Steinfeldt Christian et al.",
      "keywords": "authorship; coauthorship; collaboration networks; gender in mathematics; machine learning; regression-based analysis; scientific publishing; single-authored publications",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36743404/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Using machine learning-based methods, we show that collaboration networks of female mathematicians are slightly larger than those of their male colleagues when potential confounders such as seniority or total number of publications are controlled, while they author significantly fewer papers on their own. This confirms previous descriptive explorations and provides more precise models for the role of gender in collaboration in mathematics.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9889827"
    },
    {
      "pmid": "36755564",
      "title": "AI revolution in healthcare and medicine and the (re-)emergence of inequalities and disadvantages for ageing population.",
      "abstract": "AI systems in medicine and healthcare are being extensively explored in prevention, diagnosis, novel drug designs and after-care. The application of AI technology in healthcare systems promises impressive outcomes such as equalising healthcare, reducing mortality rate and human error, reducing medical costs, as well as reducing reliance on social services. In the light of the WHO \"Decade of Healthy Ageing\", AI applications are designed as digital innovations to support the quality of life for older persons. However, the emergence of evidence of different types of algorithmic bias in AI applications, ageism in the use of digital devices and platforms, as well as age bias in digital data suggests that the use of AI might have discriminatory effects on older population or even cause harm. This paper addresses the issue of age biases and age discrimination in AI applications in medicine and healthcare systems and try to identify main challenges in this area. It will reflect on the potential of AI applications to amplify the already existing health inequalities by discussing two levels where potential negative impact of AI on age inequalities might be observed. Firstly, we will address the technical level of age bias in algorithms and digital datasets (especially health data). Secondly, we will discuss the potential disparate outcomes of automatic decision-making systems (ADMs) used in healthcare on the older population. These examples will demonstrate, although only partially, how AI systems may create new structures of age inequalities and novel dimensions of exclusion in healthcare and medicine.",
      "journal": "Frontiers in sociology",
      "year": "2022",
      "doi": "10.3389/fsoc.2022.1038854",
      "authors": "Stypi\u0144ska Justyna et al.",
      "keywords": "ageing population; ageism; artificial intelligence; automatic decision making; health care",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36755564/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Secondly, we will discuss the potential disparate outcomes of automatic decision-making systems (ADMs) used in healthcare on the older population. These examples will demonstrate, although only partially, how AI systems may create new structures of age inequalities and novel dimensions of exclusion in healthcare and medicine.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9899925"
    },
    {
      "pmid": "37128361",
      "title": "Assessing Phenotype Definitions for Algorithmic Fairness.",
      "abstract": "Phenotyping is a core, routine activity in observational health research. Cohorts impact downstream analyses, such as how a condition is characterized, how patient risk is defined, and what treatments are studied. It is thus critical to ensure that cohorts are representative of all patients, independently of their demographics or social determinants of health. In this paper, we propose a set of best practices to assess the fairness of phenotype definitions. We leverage established fairness metrics commonly used in predictive models and relate them to commonly used epidemiological metrics. We describe an empirical study for Crohn's disease and diabetes type 2, each with multiple phenotype definitions taken from the literature across gender and race. We show that the different phenotype definitions exhibit widely varying and disparate performance according to the different fairness metrics and subgroups. We hope that the proposed best practices can help in constructing fair and inclusive phenotype definitions.",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "year": "2022",
      "doi": "",
      "authors": "Sun Tony Y et al.",
      "keywords": "",
      "mesh_terms": "Humans; Crohn Disease; Phenotype",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37128361/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Endocrinology/Diabetes",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Fairness Metrics Evaluation",
      "clinical_setting": "Not specified",
      "key_findings": "We show that the different phenotype definitions exhibit widely varying and disparate performance according to the different fairness metrics and subgroups. We hope that the proposed best practices can help in constructing fair and inclusive phenotype definitions.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10148336"
    },
    {
      "pmid": "37128420",
      "title": "Fairly Predicting Graft Failure in Liver Transplant for Organ Assigning.",
      "abstract": "Liver transplant is an essential therapy performed for severe liver diseases. The fact of scarce liver resources makes the organ assigning crucial. Model for End-stage Liver Disease (MELD) score is a widely adopted criterion when making organ distribution decisions. However, it ignores post-transplant outcomes and organ/donor features. These limitations motivate the emergence of machine learning (ML) models. Unfortunately, ML models could be unfair and trigger bias against certain groups of people. To tackle this problem, this work proposes a fair machine learning framework targeting graft failure prediction in liver transplant. Specifically, knowledge distillation is employed to handle dense and sparse features by combining the advantages of tree models and neural networks. A two-step debiasing method is tailored for this framework to enhance fairness. Experiments are conducted to analyze unfairness issues in existing models and demonstrate the superiority of our method in both prediction and fairness performance.",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "year": "2022",
      "doi": "",
      "authors": "Ding Sirui et al.",
      "keywords": "",
      "mesh_terms": "Humans; End Stage Liver Disease; Liver Transplantation; Severity of Illness Index; Neural Networks, Computer; Machine Learning; Retrospective Studies",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/37128420/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Neural Network; Generative AI",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "A two-step debiasing method is tailored for this framework to enhance fairness. Experiments are conducted to analyze unfairness issues in existing models and demonstrate the superiority of our method in both prediction and fairness performance.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 3 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10148275"
    },
    {
      "pmid": "33152602",
      "title": "Detect and correct bias in multi-site neuroimaging datasets.",
      "abstract": "The desire to train complex machine learning algorithms and to increase the statistical power in association studies drives neuroimaging research to use ever-larger datasets. The most obvious way to increase sample size is by pooling scans from independent studies. However, simple pooling is often ill-advised as selection, measurement, and confounding biases may creep in and yield spurious correlations. In this work, we combine 35,320 magnetic resonance images of the brain from 17 studies to examine bias in neuroimaging. In the first experiment, Name That Dataset, we provide empirical evidence for the presence of bias by showing that scans can be correctly assigned to their respective dataset with 71.5% accuracy. Given such evidence, we take a closer look at confounding bias, which is often viewed as the main shortcoming in observational studies. In practice, we neither know all potential confounders nor do we have data on them. Hence, we model confounders as unknown, latent variables. Kolmogorov complexity is then used to decide whether the confounded or the causal model provides the simplest factorization of the graphical model. Finally, we present methods for dataset harmonization and study their ability to remove bias in imaging features. In particular, we propose an extension of the recently introduced ComBat algorithm to control for global variation across image features, inspired by adjusting for unknown population stratification in genetics. Our results demonstrate that harmonization can reduce dataset-specific information in image features. Further, confounding bias can be reduced and even turned into a causal relationship. However, harmonization also requires caution as it can easily remove relevant subject-specific information. Code is available at https://github.com/ai-med/Dataset-Bias.",
      "journal": "Medical image analysis",
      "year": "2021",
      "doi": "10.1016/j.media.2020.101879",
      "authors": "Wachinger Christian et al.",
      "keywords": "Bias; Big data; Causal inference; Harmonization; MRI",
      "mesh_terms": "Algorithms; Bias; Humans; Machine Learning; Magnetic Resonance Imaging; Neuroimaging",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33152602/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Neurology; Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "However, harmonization also requires caution as it can easily remove relevant subject-specific information. Code is available at https://github.com/ai-med/Dataset-Bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "33236066",
      "title": "Addressing bias in prediction models by improving subpopulation calibration.",
      "abstract": "OBJECTIVE: To illustrate the problem of subpopulation miscalibration, to adapt an algorithm for recalibration of the predictions, and to validate its performance. MATERIALS AND METHODS: In this retrospective cohort study, we evaluated the calibration of predictions based on the Pooled Cohort Equations (PCE) and the fracture risk assessment tool (FRAX) in the overall population and in subpopulations defined by the intersection of age, sex, ethnicity, socioeconomic status, and immigration history. We next applied the recalibration algorithm and assessed the change in calibration metrics, including calibration-in-the-large. RESULTS: 1\u00a0021\u00a0041 patients were included in the PCE population, and 1\u00a0116\u00a0324 patients were included in the FRAX population. Baseline overall model calibration of the 2 tested models was good, but calibration in a substantial portion of the subpopulations was poor. After applying the algorithm, subpopulation calibration statistics were greatly improved, with the variance of the calibration-in-the-large values across all subpopulations reduced by 98.8% and 94.3% in the PCE and FRAX models, respectively. DISCUSSION: Prediction models in medicine are increasingly common. Calibration, the agreement between predicted and observed risks, is commonly poor for subpopulations that were underrepresented in the development set of the models, resulting in bias and reduced performance for these subpopulations. In this work, we empirically evaluated an adapted version of the fairness algorithm designed by Hebert-Johnson et al. (2017) and demonstrated its use in improving subpopulation miscalibration. CONCLUSION: A postprocessing and model-independent fairness algorithm for recalibration of predictive models greatly decreases the bias of subpopulation miscalibration and thus increases fairness and equality.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2021",
      "doi": "10.1093/jamia/ocaa283",
      "authors": "Barda Noam et al.",
      "keywords": "Predictive\u00a0; models, algorithmic fairness, calibration, model bias, cardiovascular disease, osteoporosis",
      "mesh_terms": "Adult; Aged; Algorithms; Bias; Female; Humans; Male; Middle Aged; Models, Statistical; Multivariate Analysis; Prognosis; Proportional Hazards Models; Retrospective Studies; Risk Assessment",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33236066/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM; Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration; Subgroup Analysis",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: A postprocessing and model-independent fairness algorithm for recalibration of predictive models greatly decreases the bias of subpopulation miscalibration and thus increases fairness and equality.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7936516"
    },
    {
      "pmid": "33422469",
      "title": "Cloud-Based Functional Magnetic Resonance Imaging Neurofeedback to Reduce the Negative Attentional Bias in Depression: A Proof-of-Concept Study.",
      "abstract": "Individuals with depression show an attentional bias toward negatively valenced stimuli and thoughts. In this proof-of-concept study, we present a novel closed-loop neurofeedback procedure intended to remediate this bias. Internal attentional states were detected in real time by applying machine learning techniques to functional magnetic resonance imaging data on a cloud server; these attentional states were externalized using a visual stimulus that the participant could learn to control. We trained 15 participants with major depressive disorder and 12 healthy control participants over 3 functional magnetic resonance imaging sessions. Exploratory analysis showed that participants with major depressive disorder were initially more likely than healthy control participants to get stuck in negative attentional states, but this diminished with neurofeedback training relative to controls. Depression severity also decreased from pre- to posttraining. These results demonstrate that our method is sensitive to the negative attentional bias in major depressive disorder and showcase the potential of this novel technique as a treatment that can be evaluated in future clinical trials.",
      "journal": "Biological psychiatry. Cognitive neuroscience and neuroimaging",
      "year": "2021",
      "doi": "10.1016/j.bpsc.2020.10.006",
      "authors": "Mennen Anne C et al.",
      "keywords": "Attentional bias; Brain-machine interface; Cloud computing; Cognitive training; Depression; Real-time fMRI",
      "mesh_terms": "Attentional Bias; Cloud Computing; Depression; Major Depressive Disorder; Humans; Magnetic Resonance Imaging; Neurofeedback",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33422469/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Clinical Trial",
      "key_findings": "Depression severity also decreased from pre- to posttraining. These results demonstrate that our method is sensitive to the negative attentional bias in major depressive disorder and showcase the potential of this novel technique as a treatment that can be evaluated in future clinical trials.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8035170"
    },
    {
      "pmid": "33484133",
      "title": "The risk of racial bias while tracking influenza-related content on social media using machine learning.",
      "abstract": "OBJECTIVE: Machine learning is used to understand and track influenza-related content on social media. Because these systems are used at scale, they have the potential to adversely impact the people they are built to help. In this study, we explore the biases of different machine learning methods for the specific task of detecting influenza-related content. We compare the performance of each model on tweets written in Standard American English (SAE) vs African American English (AAE). MATERIALS AND METHODS: Two influenza-related datasets are used to train 3 text classification models (support vector machine, convolutional neural network, bidirectional long short-term memory) with different feature sets. The datasets match real-world scenarios in which there is a large imbalance between SAE and AAE examples. The number of AAE examples for each class ranges from 2% to 5% in both datasets. We also evaluate each model's performance using a balanced dataset via undersampling. RESULTS: We find that all of the tested machine learning methods are biased on both datasets. The difference in false positive rates between SAE and AAE examples ranges from 0.01 to 0.35. The difference in the false negative rates ranges from 0.01 to 0.23. We also find that the neural network methods generally has more unfair results than the linear support vector machine on the chosen datasets. CONCLUSIONS: The models that result in the most unfair predictions may vary from dataset to dataset. Practitioners should be aware of the potential harms related to applying machine learning to health-related social media data. At a minimum, we recommend evaluating fairness along with traditional evaluation metrics.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2021",
      "doi": "10.1093/jamia/ocaa326",
      "authors": "Lwowski Brandon et al.",
      "keywords": "classification; deep learning; fairness; machine learning; social network",
      "mesh_terms": "Black or African American; Datasets as Topic; Humans; Influenza Vaccines; Influenza, Human; Machine Learning; Neural Networks, Computer; Racism; Social Media; Support Vector Machine",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33484133/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; NLP/LLM; Support Vector Machine; Neural Network",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Data Collection; Data Preprocessing; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Reweighting/Resampling; Diverse/Representative Data",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: The models that result in the most unfair predictions may vary from dataset to dataset. Practitioners should be aware of the potential harms related to applying machine learning to health-related social media data. At a minimum, we recommend evaluating fairness along with traditional evaluation metrics.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7973478"
    },
    {
      "pmid": "33565982",
      "title": "The Need for Ethnoracial Equity in Artificial Intelligence for Diabetes Management: Review and Recommendations.",
      "abstract": "There is clear evidence to suggest that diabetes does not affect all populations equally. Among adults living with diabetes, those from ethnoracial minority communities-foreign-born, immigrant, refugee, and culturally marginalized-are at increased risk of poor health outcomes. Artificial intelligence (AI) is actively being researched as a means of improving diabetes management and care; however, several factors may predispose AI to ethnoracial bias. To better understand whether diabetes AI interventions are being designed in an ethnoracially equitable manner, we conducted a secondary analysis of 141 articles included in a 2018 review by Contreras and Vehi entitled \"Artificial Intelligence for Diabetes Management and Decision Support: Literature Review.\" Two members of our research team independently reviewed each article and selected those reporting ethnoracial data for further analysis. Only 10 articles (7.1%) were ultimately selected for secondary analysis in our case study. Of the 131 excluded articles, 118 (90.1%) failed to mention participants' ethnic or racial backgrounds. The included articles reported ethnoracial data under various categories, including race (n=6), ethnicity (n=2), race/ethnicity (n=3), and percentage of Caucasian participants (n=1). Among articles specifically reporting race, the average distribution was 69.5% White, 17.1% Black, and 3.7% Asian. Only 2 articles reported inclusion of Native American participants. Given the clear ethnic and racial differences in diabetes biomarkers, prevalence, and outcomes, the inclusion of ethnoracial training data is likely to improve the accuracy of predictive models. Such considerations are imperative in AI-based tools, which are predisposed to negative biases due to their black-box nature and proneness to distributional shift. Based on our findings, we propose a short questionnaire to assess ethnoracial equity in research describing AI-based diabetes interventions. At this unprecedented time in history, AI can either mitigate or exacerbate disparities in health care. Future accounts of the infancy of diabetes AI must reflect our early and decisive action to confront ethnoracial inequities before they are coded into our systems and perpetuate the very biases we aim to eliminate. If we take deliberate and meaningful steps now toward training our algorithms to be ethnoracially inclusive, we can architect innovations in diabetes care that are bound by the diverse fabric of our society.",
      "journal": "Journal of medical Internet research",
      "year": "2021",
      "doi": "10.2196/22320",
      "authors": "Pham Quynh et al.",
      "keywords": "artificial intelligence; diabetes; digital health; ethnicity; ethnoracial equity; race",
      "mesh_terms": "Artificial Intelligence; Diabetes Mellitus; Ethnicity; Humans; Minority Groups",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33565982/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "Endocrinology/Diabetes",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Future accounts of the infancy of diabetes AI must reflect our early and decisive action to confront ethnoracial inequities before they are coded into our systems and perpetuate the very biases we aim to eliminate. If we take deliberate and meaningful steps now toward training our algorithms to be ethnoracially inclusive, we can architect innovations in diabetes care that are bound by the diverse fabric of our society.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7904401"
    },
    {
      "pmid": "33665879",
      "title": "Accounting for selection bias due to death in estimating the effect of wealth shock on cognition for the Health and Retirement Study.",
      "abstract": "The Health and Retirement Study (HRS) is a longitudinal study of U.S. adults enrolled at age 50 and older. We were interested in investigating the effect of a sudden large decline in wealth on the cognitive ability of subjects measured using a dataset provided composite score. However, our analysis was complicated by the lack of randomization, time-dependent confounding, and a substantial fraction of the sample and population will die during follow-up leading to some of our outcomes being censored. The common method to handle this type of problem is marginal structural models (MSM). Although MSM produces valid estimates, this may not be the most appropriate method to reflect a useful real-world situation because MSM upweights subjects who are more likely to die to obtain a hypothetical population that over time, resembles that would have been obtained in the absence of death. A more refined and practical framework, principal stratification (PS), would be to restrict analysis to the strata of the population that would survive regardless of negative wealth shock experience. In this work, we propose a new algorithm for the estimation of the treatment effect under PS by imputing the counterfactual survival status and outcomes. Simulation studies suggest that our algorithm works well in various scenarios. We found no evidence that a negative wealth shock experience would affect the cognitive score of HRS subjects.",
      "journal": "Statistics in medicine",
      "year": "2021",
      "doi": "10.1002/sim.8921",
      "authors": "Tan Yaoyuan Vincent et al.",
      "keywords": "Bayesian additive regression trees; causal inference; longitudinal study; missing data; penalized spline of propensity methods in treatment comparisons; time-dependent confounding",
      "mesh_terms": "Humans; Middle Aged; Bias; Cognition; Longitudinal Studies; Retirement; Selection Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33665879/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Counterfactual Fairness",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Simulation studies suggest that our algorithm works well in various scenarios. We found no evidence that a negative wealth shock experience would affect the cognitive score of HRS subjects.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "33678038",
      "title": "A case study of ascertainment bias for the primary outcome in the Strategies to Reduce Injuries and Develop Confidence in Elders (STRIDE) trial.",
      "abstract": "BACKGROUND/AIM: In clinical trials, there is potential for bias from unblinded observers that may influence ascertainment of outcomes. This issue arose in the Strategies to Reduce Injuries and Develop Confidence in Elders trial, a cluster randomized trial to test a multicomponent intervention versus enhanced usual care (control) to prevent serious fall injuries, originally defined as a fall injury leading to medical attention. An unblinded nurse falls care manager administered the intervention, while the usual care arm did not involve contact with a falls care manager. Thus, there was an opportunity for falls care managers to refer participants reporting falls to seek medical attention. Since this type of observer bias could not occur in the usual care arm, there was potential for additional falls to be reported in the intervention arm, leading to dilution of the intervention effect and a reduction in study power. We describe the clinical basis for ascertainment bias, the statistical approach used to assess it, and its effect on study power. METHODS: The prespecified interim monitoring plan included a decision algorithm for assessing ascertainment bias and adapting (revising) the primary outcome definition, if necessary. The original definition categorized serious fall injuries requiring medical attention into Type 1 (fracture other than thoracic/lumbar vertebral, joint dislocation, cut requiring closure) and Type 2 (head injury, sprain or strain, bruising or swelling, other). The revised definition, proposed by the monitoring plan, excluded Type 2 injuries that did not necessarily require an overnight hospitalization since these would be most subject to bias. These injuries were categorized into those with (Type 2b) and without (Type 2c) medical attention. The remaining Type 2a injuries required medical attention and an overnight hospitalization. We used the ratio of 2b/(2b\u2009+\u20092c) in intervention versus control as a measure of ascertainment bias; ratios\u2009>\u20091 indicated the likelihood of falls care manager bias. We determined the effect of ascertainment bias on study power for the revised (Types 1 and 2a) versus original definition (Types 1, 2a, and 2b). RESULTS: The estimate of ascertainment bias was 1.14 (95% confidence interval: 0.98, 1.30), providing evidence of the likelihood of falls care manager bias. We estimated that this bias diluted the hazard ratio from the hypothesized 0.80 to 0.86 and reduced power to under 80% for the original primary outcome definition. In contrast, adapting the revised definition maintained study power at nearly 90%. CONCLUSION: There was evidence of ascertainment bias in the Strategies to Reduce Injuries and Develop Confidence in Elders trial. The decision to adapt the primary outcome definition reduced the likelihood of this bias while preserving the intervention effect and study power.",
      "journal": "Clinical trials (London, England)",
      "year": "2021",
      "doi": "10.1177/1740774520980070",
      "authors": "Esserman Denise A et al.",
      "keywords": "Cluster randomized trial; adjudication; ascertainment bias; hazard ratio; power",
      "mesh_terms": "Accidental Falls; Aged; Bias; Fractures, Bone; Hospitalization; Humans; Randomized Controlled Trials as Topic",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33678038/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Hospital/Inpatient; Clinical Trial",
      "key_findings": "CONCLUSION: There was evidence of ascertainment bias in the Strategies to Reduce Injuries and Develop Confidence in Elders trial. The decision to adapt the primary outcome definition reduced the likelihood of this bias while preserving the intervention effect and study power.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8009806"
    },
    {
      "pmid": "33679476",
      "title": "Correlation Constraints for Regression Models: Controlling Bias in Brain Age Prediction.",
      "abstract": "In neuroimaging, the difference between chronological age and predicted brain age, also known as brain age delta, has been proposed as a pathology marker linked to a range of phenotypes. Brain age delta is estimated using regression, which involves a frequently observed bias due to a negative correlation between chronological age and brain age delta. In brain age prediction models, this correlation can manifest as an overprediction of the age of young brains and an underprediction for elderly ones. We show that this bias can be controlled for by adding correlation constraints to the model training procedure. We develop an analytical solution to this constrained optimization problem for Linear, Ridge, and Kernel Ridge regression. The solution is optimal in the least-squares sense i.e., there is no other model that satisfies the correlation constraints and has a better fit. Analyses on the PAC2019 competition data demonstrate that this approach produces optimal unbiased predictive models with a number of advantages over existing approaches. Finally, we introduce regression toolboxes for Python and MATLAB that implement our algorithm.",
      "journal": "Frontiers in psychiatry",
      "year": "2021",
      "doi": "10.3389/fpsyt.2021.615754",
      "authors": "Treder Matthias S et al.",
      "keywords": "age; brain; correlation; optimization; prediction; regression",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33679476/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Clinical Prediction Model; Regression",
      "health_domain": "Pathology; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "Analyses on the PAC2019 competition data demonstrate that this approach produces optimal unbiased predictive models with a number of advantages over existing approaches. Finally, we introduce regression toolboxes for Python and MATLAB that implement our algorithm.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7930839"
    },
    {
      "pmid": "33713239",
      "title": "Towards a pragmatist dealing with algorithmic bias in medical machine learning.",
      "abstract": "Machine Learning (ML) is on the rise in medicine, promising improved diagnostic, therapeutic and prognostic clinical tools. While these technological innovations are bound to transform health care, they also bring new ethical concerns to the forefront. One particularly elusive challenge regards discriminatory algorithmic judgements based on biases inherent in the training data. A common line of reasoning distinguishes between justified differential treatments that mirror true disparities between socially salient groups, and unjustified biases which do not, leading to misdiagnosis and erroneous treatment. In the curation of training data this strategy runs into severe problems though, since distinguishing between the two can be next to impossible. We thus plead for a pragmatist dealing with algorithmic bias in healthcare environments. By recurring to a recent reformulation of William James's pragmatist understanding of truth, we recommend that, instead of aiming at a supposedly objective truth, outcome-based therapeutic usefulness should serve as the guiding principle for assessing ML applications in medicine.",
      "journal": "Medicine, health care, and philosophy",
      "year": "2021",
      "doi": "10.1007/s11019-021-10008-5",
      "authors": "Starke Georg et al.",
      "keywords": "Algorithmic bias; Artificial intelligence; Fairness; Machine learning; Philosophy of Science; Pragmatism",
      "mesh_terms": "Bias; Delivery of Health Care; Education, Medical; Humans; Machine Learning; Morals",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33713239/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "We thus plead for a pragmatist dealing with algorithmic bias in healthcare environments. By recurring to a recent reformulation of William James's pragmatist understanding of truth, we recommend that, instead of aiming at a supposedly objective truth, outcome-based therapeutic usefulness should serve as the guiding principle for assessing ML applications in medicine.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7955212"
    },
    {
      "pmid": "33746859",
      "title": "A Pictorial Dot Probe Task to Assess Food-Related Attentional Bias in Youth With and Without Obesity: Overview of Indices and Evaluation of Their Reliability.",
      "abstract": "Several versions of the dot probe detection task are frequently used to assess maladaptive attentional processes associated with a broad range of psychopathology and health behavior, including eating behavior and weight. However, there are serious concerns about the reliability of the indices derived from the paradigm as measurement of attentional bias toward or away from salient stimuli. The present paper gives an overview of different attentional bias indices used in psychopathology research and scrutinizes three types of indices (the traditional attentional bias score, the dynamic trial-level base scores, and the probability index) calculated from a pictorial version of the dot probe task to assess food-related attentional biases in children and youngsters with and without obesity. Correlational analyses reveal that dynamic scores (but not the traditional and probability indices) are dependent on general response speed. Reliability estimates are low for the traditional and probability indices. The higher reliability for the dynamic indices is at least partially explained by general response speed. No significant group differences between youth with and without obesity are found, and correlations with weight are also non-significant. Taken together, results cast doubt on the applicability of this specific task for both experimental and individual differences research on food-related attentional biases in youth. However, researchers are encouraged to make and test adaptations to the procedure or computational algorithm in an effort to increase psychometric quality of the task and to report psychometric characteristics of their version of the task for their specific sample.",
      "journal": "Frontiers in psychology",
      "year": "2021",
      "doi": "10.3389/fpsyg.2021.644512",
      "authors": "Vervoort Leentje et al.",
      "keywords": "attentional bias; children and adolescent; dot probe paradigm; obesity; reliability",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33746859/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Pathology; Pediatrics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "Taken together, results cast doubt on the applicability of this specific task for both experimental and individual differences research on food-related attentional biases in youth. However, researchers are encouraged to make and test adaptations to the procedure or computational algorithm in an effort to increase psychometric quality of the task and to report psychometric characteristics of their version of the task for their specific sample.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7965983"
    },
    {
      "pmid": "33947843",
      "title": "Interrogation of gender disparity uncovers androgen receptor as the transcriptional activator for oncogenic miR-125b in gastric cancer.",
      "abstract": "There is a male preponderance in gastric cancer (GC), which suggests a role of androgen and androgen receptor (AR). However, the mechanism of AR signaling in GC especially in female patients remains obscure. We sought to identify the AR signaling pathway that might be related to prognosis and examine the potential clinical utility of the AR antagonist for treatment. Deep learning and gene set enrichment analysis was used to identify potential critical factors associated with gender bias in GC (n\u2009=\u20091390). Gene expression profile analysis was performed to screen differentially expressed genes associated with AR expression in the Tianjin discovery set (n\u2009=\u200990) and TCGA validation set (n\u2009=\u2009341). Predictors of survival were identified via lasso regression analyses and validated in the expanded Tianjin cohort (n\u2009=\u2009373). In vitro and in vivo experiments were established to determine the drug effect. The GC gender bias was attributable to sex chromosome abnormalities and AR signaling dysregulation. The candidates for AR-related gene sets were screened, and AR combined with miR-125b was associated with poor prognosis, particularly among female patients. AR was confirmed to directly regulate miR-125b expression. AR-miR-125b signaling pathway inhibited apoptosis and promoted proliferation. AR antagonist, bicalutamide, exerted anti-tumor activities and induced apoptosis both in vitro and in vivo, using GC cell lines and female patient-derived xenograft (PDX) model. We have shed light on gender differences by revealing a hormone-regulated oncogenic signaling pathway in GC. Our preclinical studies suggest that AR is a potential therapeutic target for this deadly cancer type, especially in female patients.",
      "journal": "Cell death & disease",
      "year": "2021",
      "doi": "10.1038/s41419-021-03727-3",
      "authors": "Liu Ben et al.",
      "keywords": "",
      "mesh_terms": "Animals; Female; Heterografts; Humans; Male; Mice; Mice, Inbred BALB C; Mice, Nude; MicroRNAs; Receptors, Androgen; Sex Factors; Stomach Neoplasms; Transcriptome",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33947843/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Oncology; ICU/Critical Care; Genomics/Genetics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "We have shed light on gender differences by revealing a hormone-regulated oncogenic signaling pathway in GC. Our preclinical studies suggest that AR is a potential therapeutic target for this deadly cancer type, especially in female patients.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8096848"
    },
    {
      "pmid": "33997972",
      "title": "BiasCorrector: Fast and accurate correction of all types of experimental biases in quantitative DNA methylation data derived by different technologies.",
      "abstract": "Quantification of DNA methylation in neoplastic cells is crucial both from mechanistic and diagnostic perspectives. However, such measurements are prone to different experimental biases. Polymerase chain reaction (PCR) bias results in an unequal recovery of methylated and unmethylated alleles at the sample preparation step. Post-PCR biases get introduced additionally by the readout processes. Correcting the biases is more practicable than optimising experimental conditions, as demonstrated previously. However, utilisation of our earlier developed algorithm strongly necessitates automation. Here, we present two R packages: rBiasCorrection, the core algorithms to correct biases; and BiasCorrector, its web-based graphical user interface frontend. The software detects and analyses experimental biases in calibration DNA samples at a single base resolution by using cubic polynomial and hyperbolic regression. The correction coefficients from the best regression type are employed to compensate for the bias. Three common technologies-bisulphite pyrosequencing, next-generation sequencing and oligonucleotide microarrays-were used to comprehensively test BiasCorrector. We demonstrate the accuracy of BiasCorrector's performance and reveal technology-specific PCR- and post-PCR biases. BiasCorrector effectively eliminates biases regardless of their nature, locus, the number of interrogated methylation sites and the detection method, thus representing a user-friendly tool for producing accurate epigenetic results.",
      "journal": "International journal of cancer",
      "year": "2021",
      "doi": "10.1002/ijc.33681",
      "authors": "Kapsner Lorenz A et al.",
      "keywords": "BiasCorrector; DNA methylation; PCR-bias; cancer; post-PCR bias",
      "mesh_terms": "Algorithms; Bias; CpG Islands; DNA Methylation; Humans; Neoplasms; Polymerase Chain Reaction; Sequence Analysis, DNA; Software; Technology",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33997972/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Both",
      "approach_method": "Calibration",
      "clinical_setting": "Not specified",
      "key_findings": "We demonstrate the accuracy of BiasCorrector's performance and reveal technology-specific PCR- and post-PCR biases. BiasCorrector effectively eliminates biases regardless of their nature, locus, the number of interrogated methylation sites and the detection method, thus representing a user-friendly tool for producing accurate epigenetic results.",
      "ft_include": false,
      "ft_reason": "Not health-related in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "34022618",
      "title": "Gender disparities in clozapine prescription in a cohort of treatment-resistant schizophrenia in the South London and Maudsley case register.",
      "abstract": "BACKGROUND: Gender disparities in treatment are apparent across many areas of healthcare. There has been little research into whether clozapine prescription, the first-line treatment for treatment-resistant schizophrenia (TRS), is affected by patient gender. METHODS: This retrospective cohort study identified 2244 patients with TRS within the South London and Maudsley NHS Trust, by using a bespoke method validated against a gold-standard, manually coded, dataset of TRS cases. The outcome and exposures were identified from the free-text using natural language processing applications (including machine learning and rules-based approaches) and from information entered in structured fields. Multivariable logistic regression was carried out to calculate the odds ratios for clozapine prescription according to patients' gender, and adjusting for numerous potential confounders including sociodemographic, clinical (e.g., psychiatric comorbidities and substance use), neutropenia, functional factors (e.g., problems with occupation), and clinical monitoring. RESULTS: Clozapine was prescribed to 77% of the women and 85% of the men with TRS. Women had reduced odds of being prescribed clozapine as compared to men after adjusting for all factors included in the present study (adjusted OR: 0.66; 95% CI 0.44-0.97; p = 0.037). CONCLUSION: Women with TRS are less likely to be prescribed clozapine than men with TRS, even when considering the effects of multiple clinical and functional factors. This finding suggests there could be gender bias in clozapine prescription, which carries ramifications for the relatively poorer care of women with TRS regarding many outcomes such as increased hospitalisation, mortality, and poorer quality of life.",
      "journal": "Schizophrenia research",
      "year": "2021",
      "doi": "10.1016/j.schres.2021.05.006",
      "authors": "Wellesley Wesley Emma et al.",
      "keywords": "Healthcare inequality; Psychopharmacology; Refractory psychosis; Sex",
      "mesh_terms": "Antipsychotic Agents; Clozapine; Cohort Studies; Female; Humans; London; Male; Prescriptions; Quality of Life; Retrospective Studies; Schizophrenia; Sexism",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34022618/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "NLP/LLM; Logistic Regression",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status; Language",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "CONCLUSION: Women with TRS are less likely to be prescribed clozapine than men with TRS, even when considering the effects of multiple clinical and functional factors. This finding suggests there could be gender bias in clozapine prescription, which carries ramifications for the relatively poorer care of women with TRS regarding many outcomes such as increased hospitalisation, mortality, and poorer quality of life.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "34083423",
      "title": "On the Ethics and Practicalities of Artificial Intelligence, Risk Assessment, and Race.",
      "abstract": "Artificial intelligence (AI) has been put forth as a potential means of improving and expediting violence risk assessment in forensic psychiatry. Furthermore, it has been proffered as a means of mitigating bias by replacing subjective human judgements with unadulterated data-driven predictions. A recent ethics analysis of AI-informed violence risk assessment enumerated some potential benefits, ethics concerns, and recommendations for further discussion. The current review builds on this previous work by highlighting additional important practical and ethics considerations. These include extant technology for violence risk assessment, paradigmatic concerns with the application of AI to risk assessment and management, and empirical evidence of racial bias in the criminal justice system. Emphasis is given to problems of informed consent, maleficence (e.g., the known iatrogenic effects of overly punitive sanctions), and justice (particularly racial justice). AI appears well suited to certain medical applications, such as the interpretation of diagnostic images, and may well surpass human judgement in accuracy or efficiency with respect to some important tasks. Caution is necessary, however, when applying AI to processes like violence risk assessment that do not conform clearly to simple classification paradigms.",
      "journal": "The journal of the American Academy of Psychiatry and the Law",
      "year": "2021",
      "doi": "10.29158/JAAPL.200116-20",
      "authors": "Hogan Neil R et al.",
      "keywords": "artificial intelligence; ethics; race; risk assessment",
      "mesh_terms": "Artificial Intelligence; Forensic Psychiatry; Humans; Informed Consent; Risk Assessment; Violence",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34083423/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry; ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "AI appears well suited to certain medical applications, such as the interpretation of diagnostic images, and may well surpass human judgement in accuracy or efficiency with respect to some important tasks. Caution is necessary, however, when applying AI to processes like violence risk assessment that do not conform clearly to simple classification paradigms.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "34083673",
      "title": "Automatic and unbiased segmentation and quantification of myofibers in skeletal muscle.",
      "abstract": "Skeletal muscle has the remarkable ability to regenerate. However, with age and disease muscle strength and function decline. Myofiber size, which is affected by injury and disease, is a critical measurement to assess muscle health. Here, we test and apply Cellpose, a recently developed deep learning algorithm, to automatically segment myofibers within murine skeletal muscle. We first show that tissue fixation is necessary to preserve cellular structures such as primary cilia, small cellular antennae, and adipocyte lipid droplets. However, fixation generates heterogeneous myofiber labeling, which impedes intensity-based segmentation. We demonstrate that Cellpose efficiently delineates thousands of individual myofibers outlined by a variety of markers, even within fixed tissue with highly uneven myofiber staining. We created a novel ImageJ plugin (LabelsToRois) that allows processing\u00a0of multiple Cellpose segmentation images in batch. The plugin also contains a semi-automatic erosion function to correct for the area bias introduced by the different stainings, thereby\u00a0identifying myofibers as accurately as human experts. We successfully applied our segmentation pipeline to uncover myofiber regeneration differences between two different muscle injury models, cardiotoxin and glycerol. Thus, Cellpose combined with LabelsToRois allows for fast, unbiased, and reproducible myofiber quantification for a variety of staining and fixation conditions.",
      "journal": "Scientific reports",
      "year": "2021",
      "doi": "10.1038/s41598-021-91191-6",
      "authors": "Waisman Ariel et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Animals; Computational Biology; Histocytochemistry; Image Processing, Computer-Assisted; Mice; Microscopy; Muscle Fibers, Skeletal; Muscle, Skeletal; Software",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34083673/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Deep Learning",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We successfully applied our segmentation pipeline to uncover myofiber regeneration differences between two different muscle injury models, cardiotoxin and glycerol. Thus, Cellpose combined with LabelsToRois allows for fast, unbiased, and reproducible myofiber quantification for a variety of staining and fixation conditions.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8175575"
    },
    {
      "pmid": "34278590",
      "title": "Optimized bias and signal inference in diffusion-weighted image analysis (OBSIDIAN).",
      "abstract": "PURPOSE: Correction of Rician signal bias in magnitude MR images. METHODS: A model-based, iterative fitting procedure is used to simultaneously estimate true signal and underlying Gaussian noise with standard deviation \u03c3g on a pixel-by-pixel basis in magnitude MR images. A precomputed function that relates absolute residuals between measured signals and model fit to \u03c3g is used to iteratively estimate \u03c3g . The feasibility of the method is evaluated and compared to maximum likelihood estimation (MLE) for diffusion signal decay simulations and diffusion-weighted images of the prostate considering 21 linearly spaced b-values from 0 to 3000\u00a0s/mm2 . A multidirectional analysis was performed with publically available brain data. RESULTS: Model simulations show that the Rician bias correction algorithm is fast, with an accuracy and precision that is on par to model-based MLE and direct fitting in the case of pure Gaussian noise. Increased accuracy in parameter prediction in a low signal-to-noise ratio (SNR) scenario is ideally achieved by using a composite of multiple signal decays from neighboring voxels as input for the algorithm. For patient data, good agreement with high SNR reference data of diffusion in prostate is achieved. CONCLUSIONS: OBSIDIAN is a novel, alternative, simple to implement approach for rapid Rician bias correction applicable in any case where differences between true signal decay and underlying model function can be considered negligible in comparison to noise. The proposed composite fitting approach permits accurate parameter estimation even in typical clinical scenarios with low SNR, which significantly simplifies comparison of complex diffusion parameters among studies.",
      "journal": "Magnetic resonance in medicine",
      "year": "2021",
      "doi": "10.1002/mrm.28773",
      "authors": "Kuczera Stefan et al.",
      "keywords": "Rician bias correction; diffusion MRI; noise; prostate",
      "mesh_terms": "Algorithms; Brain; Diffusion; Diffusion Magnetic Resonance Imaging; Humans; Image Processing, Computer-Assisted; Normal Distribution; Signal-To-Noise Ratio",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34278590/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: OBSIDIAN is a novel, alternative, simple to implement approach for rapid Rician bias correction applicable in any case where differences between true signal decay and underlying model function can be considered negligible in comparison to noise. The proposed composite fitting approach permits accurate parameter estimation even in typical clinical scenarios with low SNR, which significantly simplifies comparison of complex diffusion parameters among studies.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9009782"
    },
    {
      "pmid": "34285218",
      "title": "The impact of site-specific digital histology signatures on deep learning model accuracy and bias.",
      "abstract": "The Cancer Genome Atlas (TCGA) is one of the largest biorepositories of digital histology. Deep learning (DL) models have been trained on TCGA to predict numerous features directly from histology, including survival, gene expression patterns, and driver mutations. However, we demonstrate that these features vary substantially across tissue submitting sites in TCGA for over 3,000 patients with six cancer subtypes. Additionally, we show that histologic image differences between submitting sites can easily be identified with DL. Site detection remains possible despite commonly used color normalization and augmentation methods, and we quantify the image characteristics constituting this site-specific digital histology signature. We demonstrate that these site-specific signatures lead to biased accuracy for prediction of features including survival, genomic mutations, and tumor stage. Furthermore, ethnicity can also be inferred from site-specific signatures, which must be accounted for to ensure equitable application of DL. These site-specific signatures can lead to overoptimistic estimates of model performance, and we propose a quadratic programming method that abrogates this bias by ensuring models are not trained and validated on samples from the same site.",
      "journal": "Nature communications",
      "year": "2021",
      "doi": "10.1038/s41467-021-24698-1",
      "authors": "Howard Frederick M et al.",
      "keywords": "",
      "mesh_terms": "Biomarkers, Tumor; DNA Mutational Analysis; Data Accuracy; Deep Learning; Gene Expression Profiling; Humans; Image Processing, Computer-Assisted; Mutation; Neoplasm Staging; Neoplasms; Risk Assessment; Specimen Handling",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34285218/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Oncology; Genomics/Genetics",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Furthermore, ethnicity can also be inferred from site-specific signatures, which must be accounted for to ensure equitable application of DL. These site-specific signatures can lead to overoptimistic estimates of model performance, and we propose a quadratic programming method that abrogates this bias by ensuring models are not trained and validated on samples from the same site.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8292530"
    },
    {
      "pmid": "34451100",
      "title": "Explainable Artificial Intelligence for Bias Detection in COVID CT-Scan Classifiers.",
      "abstract": "PROBLEM: An application of Explainable Artificial Intelligence Methods for COVID CT-Scan classifiers is presented. MOTIVATION: It is possible that classifiers are using spurious artifacts in dataset images to achieve high performances, and such explainable techniques can help identify this issue. AIM: For this purpose, several approaches were used in tandem, in order to create a complete overview of the classificatios. METHODOLOGY: The techniques used included GradCAM, LIME, RISE, Squaregrid, and direct Gradient approaches (Vanilla, Smooth, Integrated). MAIN RESULTS: Among the deep neural networks architectures evaluated for this image classification task, VGG16 was shown to be most affected by biases towards spurious artifacts, while DenseNet was notably more robust against them. Further impacts: Results further show that small differences in validation accuracies can cause drastic changes in explanation heatmaps for DenseNet architectures, indicating that small changes in validation accuracy may have large impacts on the biases learned by the networks. Notably, it is important to notice that the strong performance metrics achieved by all these networks (Accuracy, F1 score, AUC all in the 80 to 90% range) could give users the erroneous impression that there is no bias. However, the analysis of the explanation heatmaps highlights the bias.",
      "journal": "Sensors (Basel, Switzerland)",
      "year": "2021",
      "doi": "10.3390/s21165657",
      "authors": "Palatnik de Sousa Iam et al.",
      "keywords": "Computerized Tomography; Covid 19; Explainable AI; computer vision; image classification; medical imaging",
      "mesh_terms": "Artificial Intelligence; Bias; COVID-19; Humans; SARS-CoV-2; Tomography, X-Ray Computed",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34451100/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Neural Network; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; Pulmonology",
      "bias_axes": "Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: Among the deep neural networks architectures evaluated for this image classification task, VGG16 was shown to be most affected by biases towards spurious artifacts, while DenseNet was notably more robust against them. Further impacts: Results further show that small differences in validation accuracies can cause drastic changes in explanation heatmaps for DenseNet architectures, indicating that small changes in validation accuracy may have large impacts on the biases learned by the netw...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8402377"
    },
    {
      "pmid": "34522832",
      "title": "Representation Learning with Statistical Independence to Mitigate Bias.",
      "abstract": "Presence of bias (in datasets or tasks) is inarguably one of the most critical challenges in machine learning applications that has alluded to pivotal debates in recent years. Such challenges range from spurious associations between variables in medical studies to the bias of race in gender or face recognition systems. Controlling for all types of biases in the dataset curation stage is cumbersome and sometimes impossible. The alternative is to use the available data and build models incorporating fair representation learning. In this paper, we propose such a model based on adversarial training with two competing objectives to learn features that have (1) maximum discriminative power with respect to the task and (2) minimal statistical mean dependence with the protected (bias) variable(s). Our approach does so by incorporating a new adversarial loss function that encourages a vanished correlation between the bias and the learned features. We apply our method to synthetic data, medical images (containing task bias), and a dataset for gender classification (containing dataset bias). Our results show that the learned features by our method not only result in superior prediction performance but also are unbiased.",
      "journal": "IEEE Winter Conference on Applications of Computer Vision. IEEE Winter Conference on Applications of Computer Vision",
      "year": "2021",
      "doi": "10.1109/wacv48630.2021.00256",
      "authors": "Adeli Ehsan et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34522832/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Adversarial Debiasing; Data Augmentation; Representation Learning",
      "clinical_setting": "Not specified",
      "key_findings": "We apply our method to synthetic data, medical images (containing task bias), and a dataset for gender classification (containing dataset bias). Our results show that the learned features by our method not only result in superior prediction performance but also are unbiased.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8436589"
    },
    {
      "pmid": "34544367",
      "title": "Minimizing bias in massive multi-arm observational studies with BCAUS: balancing covariates automatically using supervision.",
      "abstract": "BACKGROUND: Observational studies are increasingly being used to provide supplementary evidence in addition to Randomized Control Trials (RCTs) because they provide a scale and diversity of participants and outcomes that would be infeasible in an RCT. Additionally, they more closely reflect the settings in which the studied interventions will be applied in the future. Well-established propensity-score-based methods exist to overcome the challenges of working with observational data to estimate causal effects. These methods also provide quality assurance diagnostics to evaluate the degree to which bias has been removed and the estimates can be trusted. In large medical datasets it is common to find the same underlying health condition being treated with a variety of distinct drugs or drug combinations. Conventional methods require a manual iterative workflow, making them scale poorly to studies with many intervention arms. In such situations, automated causal inference methods that are compatible with traditional propensity-score-based workflows are highly desirable. METHODS: We introduce an automated causal inference method BCAUS, that features a deep-neural-network-based propensity model that is trained with a loss which penalizes both the incorrect prediction of the assigned treatment as well as the degree of imbalance between the inverse probability weighted covariates. The network is trained end-to-end by dynamically adjusting the loss term for each training batch such that the relative contributions from the two loss components are held fixed. Trained BCAUS models can be used in conjunction with traditional propensity-score-based methods to estimate causal treatment effects. RESULTS: We tested BCAUS on the semi-synthetic Infant Health & Development Program dataset with a single intervention arm, and a real-world observational study of diabetes interventions with over 100,000 individuals spread across more than a hundred intervention arms. When compared against other recently proposed automated causal inference methods, BCAUS had competitive accuracy for estimating synthetic treatment effects and provided highly concordant estimates on the real-world dataset but was an order-of-magnitude faster. CONCLUSIONS: BCAUS is directly compatible with trusted protocols to estimate treatment effects and diagnose the quality of those estimates, while making the established approaches automatically scalable to an arbitrary number of simultaneous intervention arms without any need for manual iteration.",
      "journal": "BMC medical research methodology",
      "year": "2021",
      "doi": "10.1186/s12874-021-01383-x",
      "authors": "Belthangady Chinmay et al.",
      "keywords": "Causal inference; Deep learning; Neural networks; Observational studies",
      "mesh_terms": "Bias; Causality; Humans; Propensity Score",
      "pub_types": "Journal Article; Observational Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34544367/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Endocrinology/Diabetes",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Counterfactual Fairness",
      "clinical_setting": "Clinical Trial",
      "key_findings": "CONCLUSIONS: BCAUS is directly compatible with trusted protocols to estimate treatment effects and diagnose the quality of those estimates, while making the established approaches automatically scalable to an arbitrary number of simultaneous intervention arms without any need for manual iteration.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8454087"
    },
    {
      "pmid": "34550305",
      "title": "Lack of Transparency and Potential Bias in Artificial Intelligence Data Sets and Algorithms: A Scoping Review.",
      "abstract": "IMPORTANCE: Clinical artificial intelligence (AI) algorithms have the potential to improve clinical care, but fair, generalizable algorithms depend on the clinical data on which they are trained and tested. OBJECTIVE: To assess whether data sets used for training diagnostic AI algorithms addressing skin disease are adequately described and to identify potential sources of bias in these data sets. DATA SOURCES: In this scoping review, PubMed was used to search for peer-reviewed research articles published between January 1, 2015, and November 1, 2020, with the following paired search terms: deep learning and dermatology, artificial intelligence and dermatology, deep learning and dermatologist, and artificial intelligence and dermatologist. STUDY SELECTION: Studies that developed or tested an existing deep learning algorithm for triage, diagnosis, or monitoring using clinical or dermoscopic images of skin disease were selected, and the articles were independently reviewed by 2 investigators to verify that they met selection criteria. CONSENSUS PROCESS: Data set audit criteria were determined by consensus of all authors after reviewing existing literature to highlight data set transparency and sources of bias. RESULTS: A total of 70 unique studies were included. Among these studies, 1\u202f065\u202f291 images were used to develop or test AI algorithms, of which only 257\u202f372 (24.2%) were publicly available. Only 14 studies (20.0%) included descriptions of patient ethnicity or race in at least 1 data set used. Only 7 studies (10.0%) included any information about skin tone in at least 1 data set used. Thirty-six of the 56 studies developing new AI algorithms for cutaneous malignant neoplasms (64.3%) met the gold standard criteria for disease labeling. Public data sets were cited more often than private data sets, suggesting that public data sets contribute more to new development and benchmarks. CONCLUSIONS AND RELEVANCE: This scoping review identified 3 issues in data sets that are used to develop and test clinical AI algorithms for skin disease that should be addressed before clinical translation: (1) sparsity of data set characterization and lack of transparency, (2) nonstandard and unverified disease labels, and (3) inability to fully assess patient diversity used for algorithm development and testing.",
      "journal": "JAMA dermatology",
      "year": "2021",
      "doi": "10.1001/jamadermatol.2021.3129",
      "authors": "Daneshjou Roxana et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artificial Intelligence; Humans; Neoplasms; Triage",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.; Scoping Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34550305/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Scoping Review",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Dermatology; Oncology; Emergency Medicine",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Data Collection; Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Bias Auditing Framework",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: A total of 70 unique studies were included. Among these studies, 1\u202f065\u202f291 images were used to develop or test AI algorithms, of which only 257\u202f372 (24.2%) were publicly available. Only 14 studies (20.0%) included descriptions of patient ethnicity or race in at least 1 data set used.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9379852"
    },
    {
      "pmid": "34553213",
      "title": "Bias-invariant RNA-sequencing metadata annotation.",
      "abstract": "BACKGROUND: Recent technological advances have resulted in an unprecedented increase in publicly available biomedical data, yet the reuse of the data is often precluded by experimental bias and a lack of annotation depth and consistency. Missing annotations makes it impossible for researchers to find datasets specific to their needs. FINDINGS: Here, we investigate RNA-sequencing metadata prediction based on gene expression values. We present a deep-learning-based domain adaptation algorithm for the automatic annotation of RNA-sequencing metadata. We show, in multiple experiments, that our model is better at integrating heterogeneous training data compared with existing linear regression-based approaches, resulting in improved tissue type classification. By using a model architecture similar to Siamese networks, the algorithm can learn biases from datasets with few samples. CONCLUSION: Using our novel domain adaptation approach, we achieved metadata annotation accuracies up to 15.7% better than a previously published method. Using the best model, we provide a list of >10,000 novel tissue and sex label annotations for 8,495 unique SRA samples. Our approach has the potential to revive idle datasets by automated annotation making them more searchable.",
      "journal": "GigaScience",
      "year": "2021",
      "doi": "10.1093/gigascience/giab064",
      "authors": "Wartmann Hannes et al.",
      "keywords": "RNA-seq metadata; automated annotation; bias invariance; deep learning; computational biology; bioinformatics; data reusability; domain adaptation; machine learning",
      "mesh_terms": "Algorithms; Bias; Metadata; Molecular Sequence Annotation; RNA; Sequence Analysis, RNA",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34553213/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Regression",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Transfer Learning",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Using our novel domain adaptation approach, we achieved metadata annotation accuracies up to 15.7% better than a previously published method. Using the best model, we provide a list of >10,000 novel tissue and sex label annotations for 8,495 unique SRA samples. Our approach has the potential to revive idle datasets by automated annotation making them more searchable.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8559615"
    },
    {
      "pmid": "34567966",
      "title": "Ethics of artificial intelligence in global health: Explainability, algorithmic bias and trust.",
      "abstract": "AI has the potential to disrupt and transform the way we deliver care globally. It is reputed to be able to improve the accuracy of diagnoses and treatments, and make the provision of services more efficient and effective. In surgery, AI systems could lead to more accurate diagnoses of health problems and help surgeons better care for their patients. In the context of lower-and-middle-income-countries (LMICs), where access to healthcare still remains a global problem, AI could facilitate access to healthcare professionals and services, even specialist services, for millions of people. The ability of AI to deliver on its promises, however, depends on successfully resolving the ethical and practical issues identified, including that of explainability and algorithmic bias. Even though such issues might appear as being merely practical or technical ones, their closer examination uncovers questions of value, fairness and trust. It should not be left to AI developers, being research institutions or global tech companies, to decide how to resolve these ethical questions. Particularly, relying only on the trustworthiness of companies and institutions to address ethical issues relating to justice, fairness and health equality would be unsuitable and unwise. The pathway to a fair, appropriate and relevant AI necessitates the development, and critically, successful implementation of national and international rules and regulations that define the parameters and set the boundaries of operation and engagement.",
      "journal": "Journal of oral biology and craniofacial research",
      "year": "2021",
      "doi": "10.1016/j.jobcr.2021.09.004",
      "authors": "Kerasidou Angeliki",
      "keywords": "Algorithmic bias; Artificial intelligence; Explainability; Global health; Lower-middle-income-countries (LMICS); Trust",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34567966/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Surgery",
      "bias_axes": "Gender/Sex; Age; Socioeconomic Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "Particularly, relying only on the trustworthiness of companies and institutions to address ethical issues relating to justice, fairness and health equality would be unsuitable and unwise. The pathway to a fair, appropriate and relevant AI necessitates the development, and critically, successful implementation of national and international rules and regulations that define the parameters and set the boundaries of operation and engagement.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8449079"
    },
    {
      "pmid": "34615890",
      "title": "Optimization of an appointment scheduling problem for healthcare systems based on the quality of fairness service using whale optimization algorithm and NSGA-II.",
      "abstract": "Effective appointment scheduling (EAS) is essential for the quality and patient satisfaction in hospital management. Healthcare schedulers typically refer patients to a suitable period of service before the admission call closes. The appointment date can no longer be adjusted. This research presents the whale optimization algorithm (WOA) based on the Pareto archive and NSGA-II algorithm to solve the appointment scheduling model by considering the simulation approach. Based on these two algorithms, this paper has addressed the multi-criteria method in appointment scheduling. This paper computes WOA and NSGA with various hypotheses to meet the analysis and different factors related to patients in the hospital. In the last part of the model, this paper has analyzed NSGA and WOA with three cases. Fairness policy first come first serve (FCFS) considers the most priority factor to obtain from figure to strategies optimized solution for best satisfaction results. In the proposed NSGA, the FCFS approach and the WOA approach are contrasted. Numerical results indicate that both the FCFS and WOA approaches outperform the strategy optimized by the proposed algorithm.",
      "journal": "Scientific reports",
      "year": "2021",
      "doi": "10.1038/s41598-021-98851-7",
      "authors": "Ala Ali et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Appointments and Schedules; Decision Trees; Delivery of Health Care; Humans; Models, Theoretical; Quality Improvement; Quality of Health Care",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34615890/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Infectious Disease",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "In the proposed NSGA, the FCFS approach and the WOA approach are contrasted. Numerical results indicate that both the FCFS and WOA approaches outperform the strategy optimized by the proposed algorithm.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8494746"
    },
    {
      "pmid": "34674847",
      "title": "Use of artificial intelligence for gender bias analysis in letters of recommendation for general surgery residency candidates.",
      "abstract": "BACKGROUND: Letters of recommendation (LoRs) play an important role in resident selection. Author language varies implicitly toward male and female applicants. We examined gender bias in LoRs written for surgical residency candidates across three decades at one institution. METHODS: Retrospective analysis of LoRs written for general surgery residency candidates between 1980 and 2011 using artificial intelligence (AI) to conduct natural language processing (NLP) and sentiment analysis, and computer-based algorithms to detect gender bias. Applicants were grouped by scaled clerkship grades and USMLE scores. Data were analyzed among groups with t-tests, ANOVA, and non-parametric tests, as appropriate. RESULTS: A total of 611 LoRs were analyzed for 171 applicants (16.4% female), and 95.3% of letter authors were male. Scaled USMLE scores and clerkship grades (SCG) were similar for both genders (p\u00a0>\u00a00.05 for both). Average word count for all letters was 290 words and was not significantly different between genders (p\u00a0=\u00a00.18). LoRs written before 2000 were significantly shorter than those written after, among applicants of both genders (female p\u00a0=\u00a00.004; male p\u00a0<\u00a00.001). Gender bias analysis of female LoRs revealed more gendered wording compared to male LoRs (p\u00a0=\u00a00.04) and was most prominent among females with lower SCG (9.5 vs 5.1, p\u00a0=\u00a00.01). Sentiment analysis revealed male LoRs with female authors had significantly more positive sentiment compared to female LoRs (p\u00a0=\u00a00.02), and males with higher SCG had more positive sentiment compared to those with lower SCG (9.4 vs 8.2, p\u00a0=\u00a00.03). NLP detected more \"fear\" in male LoRs with lower SCGs (0.11 vs 0.09, p\u00a0=\u00a00.02). Female LoRs with higher SCGs had more positive sentiment (0.78 vs 0.83, p\u00a0=\u00a00.03) and \"joy\" (0.60 vs 0.63, p\u00a0=\u00a00.02), although those written before 2000 had less \"joy\" (0.5 vs 0.63, p\u00a0=\u00a00.006). CONCLUSION: AI and computer-based algorithms detected linguistic differences and gender bias in LoRs written for general surgery residency applicants, even following stratification by clerkship grades and when analyzed by decade.",
      "journal": "American journal of surgery",
      "year": "2021",
      "doi": "10.1016/j.amjsurg.2021.09.034",
      "authors": "Sarraf Daniel et al.",
      "keywords": "Gender bias; General surgery residency; Graduate medical education; LoRs",
      "mesh_terms": "Artificial Intelligence; Correspondence as Topic; Female; General Surgery; Humans; Internship and Residency; Male; School Admission Criteria; Sexism",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34674847/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "NLP/LLM",
      "health_domain": "Surgery",
      "bias_axes": "Gender/Sex; Age; Language",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: AI and computer-based algorithms detected linguistic differences and gender bias in LoRs written for general surgery residency applicants, even following stratification by clerkship grades and when analyzed by decade.",
      "ft_include": false,
      "ft_reason": "Not health-related in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "34679619",
      "title": "Detection Accuracy and Latency of Colorectal Lesions with Computer-Aided Detection System Based on Low-Bias Evaluation.",
      "abstract": "We developed a computer-aided detection (CADe) system to detect and localize colorectal lesions by modifying You-Only-Look-Once version 3 (YOLO v3) and evaluated its performance in two different settings. The test dataset was obtained from 20 randomly selected patients who underwent endoscopic resection for 69 colorectal lesions at the Jikei University Hospital between June 2017 and February 2018. First, we evaluated the diagnostic performances using still images randomly and automatically extracted from video recordings of the entire endoscopic procedure at intervals of 5 s, without eliminating poor quality images. Second, the latency of lesion detection by the CADe system from the initial appearance of lesions was investigated by reviewing the videos. A total of 6531 images, including 662 images with a lesion, were studied in the image-based analysis. The AUC, sensitivity, specificity, positive predictive value, negative predictive value, and accuracy were 0.983, 94.6%, 95.2%, 68.8%, 99.4%, and 95.1%, respectively. The median time for detecting colorectal lesions measured in the lesion-based analysis was 0.67 s. In conclusion, we proved that the originally developed CADe system based on YOLO v3 could accurately and instantaneously detect colorectal lesions using the test dataset obtained from videos, mitigating operator selection biases.",
      "journal": "Diagnostics (Basel, Switzerland)",
      "year": "2021",
      "doi": "10.3390/diagnostics11101922",
      "authors": "Matsui Hiroaki et al.",
      "keywords": "artificial intelligence; colonoscopy; colorectal lesions; computer-aided detection; deep learning",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34679619/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "The median time for detecting colorectal lesions measured in the lesion-based analysis was 0.67 s. In conclusion, we proved that the originally developed CADe system based on YOLO v3 could accurately and instantaneously detect colorectal lesions using the test dataset obtained from videos, mitigating operator selection biases.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8534444"
    },
    {
      "pmid": "34693373",
      "title": "Addressing bias in big data and AI for health care: A call for open science.",
      "abstract": "Artificial intelligence (AI) has an astonishing potential in assisting clinical decision making and revolutionizing the field of health care. A major open challenge that AI will need to address before its integration in the clinical routine is that of algorithmic bias. Most AI algorithms need big datasets to learn from, but several groups of the human population have a long history of being absent or misrepresented in existing biomedical datasets. If the training data is misrepresentative of the population variability, AI is prone to reinforcing bias, which can lead to fatal outcomes, misdiagnoses, and lack of generalization. Here, we describe the challenges in rendering AI algorithms fairer, and we propose concrete steps for addressing bias using tools from the field of open science.",
      "journal": "Patterns (New York, N.Y.)",
      "year": "2021",
      "doi": "10.1016/j.patter.2021.100347",
      "authors": "Norori Natalia et al.",
      "keywords": "artificial intelligence; bias; data standards; deep learning; health care; open science; participatory science",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34693373/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "If the training data is misrepresentative of the population variability, AI is prone to reinforcing bias, which can lead to fatal outcomes, misdiagnoses, and lack of generalization. Here, we describe the challenges in rendering AI algorithms fairer, and we propose concrete steps for addressing bias using tools from the field of open science.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8515002"
    },
    {
      "pmid": "34912242",
      "title": "Gender Bias in Artificial Intelligence: Severity Prediction at an Early Stage of COVID-19.",
      "abstract": "Artificial intelligence (AI) technologies have been applied in various medical domains to predict patient outcomes with high accuracy. As AI becomes more widely adopted, the problem of model bias is increasingly apparent. In this study, we investigate the model bias that can occur when training a model using datasets for only one particular gender and aim to present new insights into the bias issue. For the investigation, we considered an AI model that predicts severity at an early stage based on the medical records of coronavirus disease (COVID-19) patients. For 5,601 confirmed COVID-19 patients, we used 37 medical records, namely, basic patient information, physical index, initial examination findings, clinical findings, comorbidity diseases, and general blood test results at an early stage. To investigate the gender-based AI model bias, we trained and evaluated two separate models-one that was trained using only the male group, and the other using only the female group. When the model trained by the male-group data was applied to the female testing data, the overall accuracy decreased-sensitivity from 0.93 to 0.86, specificity from 0.92 to 0.86, accuracy from 0.92 to 0.86, balanced accuracy from 0.93 to 0.86, and area under the curve (AUC) from 0.97 to 0.94. Similarly, when the model trained by the female-group data was applied to the male testing data, once again, the overall accuracy decreased-sensitivity from 0.97 to 0.90, specificity from 0.96 to 0.91, accuracy from 0.96 to 0.91, balanced accuracy from 0.96 to 0.90, and AUC from 0.97 to 0.95. Furthermore, when we evaluated each gender-dependent model with the test data from the same gender used for training, the resultant accuracy was also lower than that from the unbiased model.",
      "journal": "Frontiers in physiology",
      "year": "2021",
      "doi": "10.3389/fphys.2021.778720",
      "authors": "Chung Heewon et al.",
      "keywords": "COVID-19; artificial intelligence bias; feature importance; gender dependent bias; severity prediction",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34912242/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Pulmonology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "Similarly, when the model trained by the female-group data was applied to the male testing data, once again, the overall accuracy decreased-sensitivity from 0.97 to 0.90, specificity from 0.96 to 0.91, accuracy from 0.96 to 0.91, balanced accuracy from 0.96 to 0.90, and AUC from 0.97 to 0.95. Furthermore, when we evaluated each gender-dependent model with the test data from the same gender used for training, the resultant accuracy was also lower than that from the unbiased model.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8667070"
    },
    {
      "pmid": "35308985",
      "title": "Data and Model Biases in Social Media Analyses: A Case Study of COVID-19 Tweets.",
      "abstract": "During the coronavirus disease pandemic (COVID-19), social media platforms such as Twitter have become a venue for individuals, health professionals, and government agencies to share COVID-19 information. Twitter has been a popular source of data for researchers, especially for public health studies. However, the use of Twitter data for research also has drawbacks and barriers. Biases appear everywhere from data collection methods to modeling approaches, and those biases have not been systematically assessed. In this study, we examined six different data collection methods and three different machine learning (ML) models-commonly used in social media analysis-to assess data collection bias and measure ML models' sensitivity to data collection bias. We showed that (1) publicly available Twitter data collection endpoints with appropriate strategies can collect data that is reasonably representative of the Twitter universe; and (2) careful examinations of ML models' sensitivity to data collection bias are critical.",
      "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "year": "2021",
      "doi": "10.1145/3400806.3400839",
      "authors": "Zhao Yunpeng et al.",
      "keywords": "",
      "mesh_terms": "Bias; COVID-19; Data Collection; Humans; Machine Learning; Social Media",
      "pub_types": "Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/35308985/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Pulmonology; Public Health",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "In this study, we examined six different data collection methods and three different machine learning (ML) models-commonly used in social media analysis-to assess data collection bias and measure ML models' sensitivity to data collection bias. We showed that (1) publicly available Twitter data collection endpoints with appropriate strategies can collect data that is reasonably representative of the Twitter universe; and (2) careful examinations of ML models' sensitivity to data collection bias a...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8861742"
    },
    {
      "pmid": "36713099",
      "title": "Spectrum bias in algorithms derived by artificial intelligence: a case study in detecting aortic stenosis using electrocardiograms.",
      "abstract": "AIMS: Spectrum bias can arise when a diagnostic test is derived from study populations with different disease spectra than the target population, resulting in poor generalizability. We used a real-world artificial intelligence (AI)-derived algorithm to detect severe aortic stenosis (AS) to experimentally assess the effect of spectrum bias on test performance. METHODS AND RESULTS: All adult patients at the Mayo Clinic between 1 January 1989 and 30 September 2019 with transthoracic echocardiograms within 180 days after electrocardiogram (ECG) were identified. Two models were developed from two distinct patient cohorts: a whole-spectrum cohort comparing severe AS to any non-severe AS and an extreme-spectrum cohort comparing severe AS to no AS at all. Model performance was assessed. Overall, 258 607 patients had valid ECG and echocardiograms pairs. The area under the receiver operator curve was 0.87 and 0.91 for the whole-spectrum and extreme-spectrum models, respectively. Sensitivity and specificity for the whole-spectrum model was 80% and 81%, respectively, while for the extreme-spectrum model it was 84% and 84%, respectively. When applying the AI-ECG derived from the extreme-spectrum cohort to patients in the whole-spectrum cohort, the sensitivity, specificity, and area under the curve dropped to 83%, 73%, and 0.86, respectively. CONCLUSION: While the algorithm performed robustly in identifying severe AS, this study shows that limiting datasets to clearly positive or negative labels leads to overestimation of test performance when testing an AI algorithm in the setting of classifying severe AS using ECG data. While the effect of the bias may be modest in this example, clinicians should be aware of the existence of such a bias in AI-derived algorithms.",
      "journal": "European heart journal. Digital health",
      "year": "2021",
      "doi": "10.1093/ehjdh/ztab061",
      "authors": "Tseng Andrew S et al.",
      "keywords": "Aortic stenosis; Artificial intelligence; Electrocardiogram; Spectrum bias",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36713099/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "CONCLUSION: While the algorithm performed robustly in identifying severe AS, this study shows that limiting datasets to clearly positive or negative labels leads to overestimation of test performance when testing an AI algorithm in the setting of classifying severe AS using ECG data. While the effect of the bias may be modest in this example, clinicians should be aware of the existence of such a bias in AI-derived algorithms.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC9707965"
    },
    {
      "pmid": "38223304",
      "title": "Double/debiased machine learning for logistic partially linear model.",
      "abstract": "We propose double/debiased machine learning approaches to infer a parametric component of a logistic partially linear model. Our framework is based on a Neyman orthogonal score equation consisting of two nuisance models for the nonparametric component of the logistic model and conditional mean of the exposure with the control group. To estimate the nuisance models, we separately consider the use of high dimensional (HD) sparse regression and (nonparametric) machine learning (ML) methods. In the HD case, we derive certain moment equations to calibrate the first order bias of the nuisance models, which preserves the model double robustness property. In the ML case, we handle the nonlinearity of the logit link through a novel and easy-to-implement 'full model refitting' procedure. We evaluate our methods through simulation and apply them in assessing the effect of the emergency contraceptive pill on early gestation and new births based on a 2008 policy reform in Chile.",
      "journal": "The econometrics journal",
      "year": "2021",
      "doi": "10.1093/ectj/utab019",
      "authors": "Liu Molei et al.",
      "keywords": "C14; Logistic partially linear model; calibration; double machine learning; double robustness; regularized regression",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38223304/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "In the ML case, we handle the nonlinearity of the logit link through a novel and easy-to-implement 'full model refitting' procedure. We evaluate our methods through simulation and apply them in assessing the effect of the emergency contraceptive pill on early gestation and new births based on a 2008 policy reform in Chile.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10786638"
    },
    {
      "pmid": "31828959",
      "title": "MUBD-DecoyMaker 2.0: A Python GUI Application to Generate Maximal Unbiased Benchmarking Data Sets for Virtual Drug Screening.",
      "abstract": "Ligand enrichment assessment based on benchmarking data sets has become a necessity for the rational selection of the best-suited approach for prospective data mining of drug-like molecules. Up to now, a variety of benchmarking data sets had been generated and frequently used. Among them, MUBD-HDACs from our prior research efforts was regarded as one of five state-of-the-art benchmarks in 2017 by Frontiers in Pharmacology. This benchmarking set was generated by one of our unique de-biasing algorithms. It also rendered quite a few other cases of successful applications in recent years, thus is expected to have more impact in modern drug discovery. To make our algorithm amenable to more users, we developed a Python GUI application called MUBD-DecoyMaker 2.0. Moreover, it has two new additional functional modules, i.\u2009e. \"Detect 2D Bias\" and \"Quality Control\". This new GUI version had been proved to be easy to use while generate benchmarking data sets of the same quality. MUBD-DecoyMaker 2.0 is freely available at https://github.com/jwxia2014/MUBD-DecoyMaker2.0, along with its manual and testcase.",
      "journal": "Molecular informatics",
      "year": "2020",
      "doi": "10.1002/minf.201900151",
      "authors": "Xia Jie et al.",
      "keywords": "Python; drug discovery; ligand enrichment; unbiased benchmark; virtual screening",
      "mesh_terms": "Algorithms; Databases, Pharmaceutical; Datasets as Topic; Drug Discovery; Drug Evaluation, Preclinical; Pharmaceutical Preparations; Programming Languages; User-Computer Interface",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31828959/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Generative AI",
      "health_domain": "Drug Discovery/Pharmacology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "This new GUI version had been proved to be easy to use while generate benchmarking data sets of the same quality. MUBD-DecoyMaker 2.0 is freely available at https://github.com/jwxia2014/MUBD-DecoyMaker2.0, along with its manual and testcase.",
      "ft_include": false,
      "ft_reason": "Not health-related in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "32012005",
      "title": "Unsupervised Domain Adaptation to Classify Medical Images Using Zero-Bias Convolutional Auto-Encoders and Context-Based Feature Augmentation.",
      "abstract": "The accuracy and robustness of image classification with supervised deep learning are dependent on the availability of large-scale labelled training data. In medical imaging, these large labelled datasets are sparse, mainly related to the complexity in manual annotation. Deep convolutional neural networks (CNNs), with transferable knowledge, have been employed as a solution to limited annotated data through: 1) fine-tuning generic knowledge with a relatively smaller amount of labelled medical imaging data, and 2) learning image representation that is invariant to different domains. These approaches, however, are still reliant on labelled medical image data. Our aim is to use a new hierarchical unsupervised feature extractor to reduce reliance on annotated training data. Our unsupervised approach uses a multi-layer zero-bias convolutional auto-encoder that constrains the transformation of generic features from a pre-trained CNN (for natural images) to non-redundant and locally relevant features for the medical image data. We also propose a context-based feature augmentation scheme to improve the discriminative power of the feature representation. We evaluated our approach on 3 public medical image datasets and compared it to other state-of-the-art supervised CNNs. Our unsupervised approach achieved better accuracy when compared to other conventional unsupervised methods and baseline fine-tuned CNNs.",
      "journal": "IEEE transactions on medical imaging",
      "year": "2020",
      "doi": "10.1109/TMI.2020.2971258",
      "authors": "Ahn Euijoon et al.",
      "keywords": "",
      "mesh_terms": "Diagnostic Imaging; Neural Networks, Computer",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32012005/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Neural Network; Computer Vision/Imaging AI; Clustering",
      "health_domain": "Radiology/Medical Imaging; Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Transfer Learning",
      "clinical_setting": "Not specified",
      "key_findings": "We evaluated our approach on 3 public medical image datasets and compared it to other state-of-the-art supervised CNNs. Our unsupervised approach achieved better accuracy when compared to other conventional unsupervised methods and baseline fine-tuned CNNs.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "32036418",
      "title": "Similar revision rates in clinical studies and arthroplasty registers and no bias for developer publications in unicompartmental knee arthroplasty.",
      "abstract": "PURPOSE: Our aim was to assess the outcome with respect to cumulative revision rates of unicompartmental knee arthroplasty (UKA) by comparing published literature and arthroplasty registry data. Our hypothesis was that there is a superior outcome of UKA described in dependent clinical studies compared to independent studies or arthroplasty registers. METHODS: A systematic review of all clinical studies on UKA in the past decade was conducted with the main endpoint revision rate. Revision rate was calculated as \"revision per 100 component years (CY)\". The respective data were analysed with regard to a potential difference of the percentage of performed revision surgeries as described in dependent and independent clinical studies. Clinical data were further compared to arthroplasty registers in a systematic search algorithm. RESULTS: In total, 48 study cohorts fulfilled our inclusion criteria and revealed 1.11 revisions per 100 CY. This corresponds to a revision rate of 11.1% after 10\u00a0years. No deviations with regard to revision rates for UKA among dependent and independent clinical literature were detected. Data from four arthroplasty registers showed lower survival rates after 10\u00a0years compared to published literature without being significant. CONCLUSIONS: The outcomes of UKA in dependent and independent clinical studies do not differ significantly and are in line with arthroplasty register datasets. We cannot confirm biased results and the authors recommend the use of UKAs in properly selected patients by experts in their field.",
      "journal": "Archives of orthopaedic and trauma surgery",
      "year": "2020",
      "doi": "10.1007/s00402-020-03336-3",
      "authors": "Hauer Georg et al.",
      "keywords": "Arthroplasty register; Revision rate; Systematic review; Unicompartmental knee arthroplasty",
      "mesh_terms": "Arthroplasty, Replacement, Knee; Humans; Knee; Registries; Reoperation; Treatment Outcome",
      "pub_types": "Journal Article; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32036418/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: The outcomes of UKA in dependent and independent clinical studies do not differ significantly and are in line with arthroplasty register datasets. We cannot confirm biased results and the authors recommend the use of UKAs in properly selected patients by experts in their field.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7109167"
    },
    {
      "pmid": "32064914",
      "title": "Assessing and Mitigating Bias in Medical Artificial Intelligence: The Effects of Race and Ethnicity on a Deep Learning Model for ECG Analysis.",
      "abstract": "BACKGROUND: Deep learning algorithms derived in homogeneous populations may be poorly generalizable and have the potential to reflect, perpetuate, and even exacerbate racial/ethnic disparities in health and health care. In this study, we aimed to (1) assess whether the performance of a deep learning algorithm designed to detect low left ventricular ejection fraction using the 12-lead ECG varies by race/ethnicity and to (2) determine whether its performance is determined by the derivation population or by racial variation in the ECG. METHODS: We performed a retrospective cohort analysis that included 97 829 patients with paired ECGs and echocardiograms. We tested the model performance by race/ethnicity for convolutional neural network designed to identify patients with a left ventricular ejection fraction \u226435% from the 12-lead ECG. RESULTS: The convolutional neural network that was previously derived in a homogeneous population (derivation cohort, n=44 959; 96.2% non-Hispanic white) demonstrated consistent performance to detect low left ventricular ejection fraction across a range of racial/ethnic subgroups in a separate testing cohort (n=52 870): non-Hispanic white (n=44 524; area under the curve [AUC], 0.931), Asian (n=557; AUC, 0.961), black/African American (n=651; AUC, 0.937), Hispanic/Latino (n=331; AUC, 0.937), and American Indian/Native Alaskan (n=223; AUC, 0.938). In secondary analyses, a separate neural network was able to discern racial subgroup category (black/African American [AUC, 0.84], and white, non-Hispanic [AUC, 0.76] in a 5-class classifier), and a network trained only in non-Hispanic whites from the original derivation cohort performed similarly well across a range of racial/ethnic subgroups in the testing cohort with an AUC of at least 0.930 in all racial/ethnic subgroups. CONCLUSIONS: Our study demonstrates that while ECG characteristics vary by race, this did not impact the ability of a convolutional neural network to predict low left ventricular ejection fraction from the ECG. We recommend reporting of performance among diverse ethnic, racial, age, and sex groups for all new artificial intelligence tools to ensure responsible use of artificial intelligence in medicine.",
      "journal": "Circulation. Arrhythmia and electrophysiology",
      "year": "2020",
      "doi": "10.1161/CIRCEP.119.007988",
      "authors": "Noseworthy Peter A et al.",
      "keywords": "United States; artificial intelligence; electrocardiography; humans; machine learning",
      "mesh_terms": "Artificial Intelligence; Deep Learning; Electrocardiography; Ethnicity; Female; Follow-Up Studies; Heart Ventricles; Humans; Male; Middle Aged; Racial Groups; Retrospective Studies; Ventricular Function, Left",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32064914/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Cardiology; ICU/Critical Care",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSIONS: Our study demonstrates that while ECG characteristics vary by race, this did not impact the ability of a convolutional neural network to predict low left ventricular ejection fraction from the ECG. We recommend reporting of performance among diverse ethnic, racial, age, and sex groups for all new artificial intelligence tools to ensure responsible use of artificial intelligence in medicine.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7158877"
    },
    {
      "pmid": "32188481",
      "title": "A geographic identifier assignment algorithm with Bayesian variable selection to identify neighborhood factors associated with emergency department visit disparities for asthma.",
      "abstract": "BACKGROUND: Ecologic health studies often rely on outcomes from health service utilization data that are limited by relatively coarse spatial resolutions and missing geographic information, particularly neighborhood level identifiers. When fine-scale geographic data are missing, the ramifications and strategies for addressing them are not well researched or developed. This study illustrates a novel spatio-temporal framework that combines a geographic identifier assignment (i.e., geographic imputation) algorithm with predictive Bayesian variable selection to identify neighborhood factors associated with disparities in emergency department (ED) visits for asthma. METHODS: ED visit records with missing fine-scale spatial identifiers (~\u200920%) were geocoded using information from known, coarser, misaligned spatial units using an innovative geographic identifier assignment algorithm. We then employed systematic variable selection in a spatio-temporal Bayesian hierarchical model (BHM) predictive framework within the NIMBLE package in R. Our novel methodology is illustrated in an ecologic case study aimed at identifying neighborhood-level predictors of asthma ED visits in South Carolina, United States, from 1999 to 2015. The health outcome was annual ED visit counts in small areas (i.e., census tracts) with primary diagnoses of asthma (ICD9 codes 493.XX) among children ages 5 to 19\u00a0years. RESULTS: We maintained 96% of ED visit records for this analysis. When the algorithm used areal proportions as probabilities for assignment, which addressed differential missingness of census tract identifiers in rural areas, variable selection consistently identified significant neighborhood-level predictors of asthma ED visit risk including pharmacy proximity, average household size, and carbon monoxide interactions. Contrasted with common solutions of removing geographically incomplete records or scaling up analyses, our methodology identified critical differences in parameters estimated, predictors selected, and inferences. We posit that the differences were attributable to improved data resolution, resulting in greater power and less bias. Importantly, without this methodology, we would have inaccurately identified predictors of risk for asthma ED visits, particularly in rural areas. CONCLUSIONS: Our approach innovatively addressed several issues in ecologic health studies, including missing small-area geographic information, multiple correlated neighborhood covariates, and multiscale unmeasured confounding factors. Our methodology could be widely applied to other small-area studies, useful to a range of researchers throughout the world.",
      "journal": "International journal of health geographics",
      "year": "2020",
      "doi": "10.1186/s12942-020-00203-7",
      "authors": "Bozigar Matthew et al.",
      "keywords": "Air pollution; Bayesian spatio-temporal modeling; Geographic imputation; Hospitalization record data; Respiratory diseases; Rural health; SEA-AIR Study; Social determinants of health; Urban health",
      "mesh_terms": "Adolescent; Algorithms; Asthma; Bayes Theorem; Child; Child, Preschool; Emergency Service, Hospital; Geographic Information Systems; Geography; Health Status Disparities; Humans; Residence Characteristics; South Carolina; Young Adult",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32188481/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Emergency Medicine; ICU/Critical Care; Pediatrics; Pulmonology",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU; Emergency Department",
      "key_findings": "CONCLUSIONS: Our approach innovatively addressed several issues in ecologic health studies, including missing small-area geographic information, multiple correlated neighborhood covariates, and multiscale unmeasured confounding factors. Our methodology could be widely applied to other small-area studies, useful to a range of researchers throughout the world.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7081565"
    },
    {
      "pmid": "32209237",
      "title": "Integrated longitudinal analysis does not compromise precision and reduces bias in the study of imaging outcomes: A comparative 5-year analysis in the DESIR cohort.",
      "abstract": "OBJECTIVE: To assess if an integrated longitudinal analysis using all available imaging data affects the precision of estimates of change in patients with axial spondyloarthritis (axSpA), with completers analysis as reference standard. METHODS: Patients from the DESIR cohort fulfilling the ASAS axSpA criteria were included. Radiographs and MRIs of the sacroiliac joints and spine were obtained at baseline, 1, 2 and 5 years. Each image was scored by 2 or 3 readers in 3 'reading-waves' (or campaigns). Each outcome was analyzed: i. According to a 'combination algorithm' (e.g. '2 out of 3' for binary scores); and ii. Per reader. Change over time was analyzed with generalized estimating equations by 3 approaches: (a)'integrated-analysis' (all patients with \u22651 score from \u22651 reader from all waves); (b1)Completers-only analysis (patients with 5-year follow-up, using scores from individual readers); (b2)Completers analysis using a 'combination algorithm' (as (b1) but with combined scores). Approaches (b1) and (b2) were considered the 'reference'. RESULTS: In total, 413 patients were included. The 'integrated analysis' was more inclusive with similar levels of precision of the change estimates as compared to both completers analyses. In fact, for low-incident outcomes (e.g.% mNY-positive over 5-years), an increased incidence was 'captured', with more precision, by the 'integrated analysis' compared to the completers analysis with combined scores (% change/year (95%CI): 1.1 (0.7; 1.5) vs 1.2 (0.5; 1.8), respectively). CONCLUSION: An efficient and entirely assumption-free 'integrated analysis' does not jeopardize precision of the estimates of change in imaging parameters and may yield increased statistical power for detecting changes with low incidence.",
      "journal": "Seminars in arthritis and rheumatism",
      "year": "2020",
      "doi": "10.1016/j.semarthrit.2020.02.017",
      "authors": "Sepriano Alexandre et al.",
      "keywords": "Axial spondyloarthritis; Imaging; Statistical methods",
      "mesh_terms": "Cohort Studies; Humans; Magnetic Resonance Imaging; Radiography; Sacroiliac Joint; Spine; Spondylarthritis",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32209237/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: An efficient and entirely assumption-free 'integrated analysis' does not jeopardize precision of the estimates of change in imaging parameters and may yield increased statistical power for detecting changes with low incidence.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "32457147",
      "title": "Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis.",
      "abstract": "Artificial intelligence (AI) systems for computer-aided diagnosis and image-based screening are being adopted worldwide by medical institutions. In such a context, generating fair and unbiased classifiers becomes of paramount importance. The research community of medical image computing is making great efforts in developing more accurate algorithms to assist medical doctors in the difficult task of disease diagnosis. However, little attention is paid to the way databases are collected and how this may influence the performance of AI systems. Our study sheds light on the importance of gender balance in medical imaging datasets used to train AI systems for computer-assisted diagnosis. We provide empirical evidence supported by a large-scale study, based on three deep neural network architectures and two well-known publicly available X-ray image datasets used to diagnose various thoracic diseases under different gender imbalance conditions. We found a consistent decrease in performance for underrepresented genders when a minimum balance is not fulfilled. This raises the alarm for national agencies in charge of regulating and approving computer-assisted diagnosis systems, which should include explicit gender balance and diversity recommendations. We also establish an open problem for the academic medical image computing community which needs to be addressed by novel algorithms endowed with robustness to gender imbalance.",
      "journal": "Proceedings of the National Academy of Sciences of the United States of America",
      "year": "2020",
      "doi": "10.1073/pnas.1919012117",
      "authors": "Larrazabal Agostina J et al.",
      "keywords": "computer-aided diagnosis; deep learning; gender bias; gendered innovations; medical image analysis",
      "mesh_terms": "Bias; Datasets as Topic; Deep Learning; Female; Humans; Male; Radiographic Image Interpretation, Computer-Assisted; Radiography, Thoracic; Reference Standards; Sex Factors",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32457147/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Deep Learning; Neural Network; Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "This raises the alarm for national agencies in charge of regulating and approving computer-assisted diagnosis systems, which should include explicit gender balance and diversity recommendations. We also establish an open problem for the academic medical image computing community which needs to be addressed by novel algorithms endowed with robustness to gender imbalance.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7293650"
    },
    {
      "pmid": "32548642",
      "title": "Empirical assessment of bias in machine learning diagnostic test accuracy studies.",
      "abstract": "OBJECTIVE: Machine learning (ML) diagnostic tools have significant potential to improve health care. However, methodological pitfalls may affect diagnostic test accuracy studies used to appraise such tools. We aimed to evaluate the prevalence and reporting of design characteristics within the literature. Further, we sought to empirically assess whether design features may be associated with different estimates of diagnostic accuracy. MATERIALS AND METHODS: We systematically retrieved 2 \u00d7 2 tables (n\u2009=\u2009281) describing the performance of ML diagnostic tools, derived from 114 publications in 38 meta-analyses, from PubMed. Data extracted included test performance, sample sizes, and design features. A mixed-effects metaregression was run to quantify the association between design features and diagnostic accuracy. RESULTS: Participant ethnicity and blinding in test interpretation was unreported in 90% and 60% of studies, respectively. Reporting was occasionally lacking for rudimentary characteristics such as study design (28% unreported). Internal validation without appropriate safeguards was used in 44% of studies. Several design features were associated with larger estimates of accuracy, including having unreported (relative diagnostic odds ratio [RDOR], 2.11; 95% confidence interval [CI], 1.43-3.1) or case-control study designs (RDOR, 1.27; 95% CI, 0.97-1.66), and recruiting participants for the index test (RDOR, 1.67; 95% CI, 1.08-2.59). DISCUSSION: Significant underreporting of experimental details was present. Study design features may affect estimates of diagnostic performance in the ML diagnostic test accuracy literature. CONCLUSIONS: The present study identifies pitfalls that threaten the validity, generalizability, and clinical value of ML diagnostic tools and provides recommendations for improvement.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2020",
      "doi": "10.1093/jamia/ocaa075",
      "authors": "Crowley Ryan J et al.",
      "keywords": "bias; diagnostic techniques and procedures; machine learning; research design; sensitivity and specificity",
      "mesh_terms": "Bias; Biomedical Research; Diagnostic Techniques and Procedures; Humans; Machine Learning; Publications; Sensitivity and Specificity; Systematic Reviews as Topic",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Systematic Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32548642/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: The present study identifies pitfalls that threaten the validity, generalizability, and clinical value of ML diagnostic tools and provides recommendations for improvement.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7647361"
    },
    {
      "pmid": "32585698",
      "title": "Patient safety and quality improvement: Ethical principles for a regulatory approach to bias in healthcare machine learning.",
      "abstract": "Accumulating evidence demonstrates the impact of bias that reflects social inequality on the performance of machine learning (ML) models in health care. Given their intended placement within healthcare decision making more broadly, ML tools require attention to adequately quantify the impact of bias and reduce its potential to exacerbate inequalities. We suggest that taking a patient safety and quality improvement approach to bias can support the quantification of bias-related effects on ML. Drawing from the ethical principles underpinning these approaches, we argue that patient safety and quality improvement lenses support the quantification of relevant performance metrics, in order to minimize harm while promoting accountability, justice, and transparency. We identify specific methods for operationalizing these principles with the goal of attending to bias to support better decision making in light of controllable and uncontrollable factors.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2020",
      "doi": "10.1093/jamia/ocaa085",
      "authors": "McCradden Melissa D et al.",
      "keywords": "healthcare delivery; machine learning; patient safety; quality improvement; systematic bias",
      "mesh_terms": "Artificial Intelligence; Data Collection; Government Regulation; Healthcare Disparities; Humans; Patient Safety; Prejudice; Quality Improvement; Social Determinants of Health",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32585698/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Drawing from the ethical principles underpinning these approaches, we argue that patient safety and quality improvement lenses support the quantification of relevant performance metrics, in order to minimize harm while promoting accountability, justice, and transparency. We identify specific methods for operationalizing these principles with the goal of attending to bias to support better decision making in light of controllable and uncontrollable factors.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7727331"
    },
    {
      "pmid": "32607482",
      "title": "Stigma, biomarkers, and algorithmic bias: recommendations for precision behavioral health with artificial intelligence.",
      "abstract": "Effective implementation of artificial intelligence in behavioral healthcare delivery depends on overcoming challenges that are pronounced in this domain. Self and social stigma contribute to under-reported symptoms, and under-coding worsens ascertainment. Health disparities contribute to algorithmic bias. Lack of reliable biological and clinical markers hinders model development, and model explainability challenges impede trust among users. In this perspective, we describe these challenges and discuss design and implementation recommendations to overcome them in intelligent systems for behavioral and mental health.",
      "journal": "JAMIA open",
      "year": "2020",
      "doi": "10.1093/jamiaopen/ooz054",
      "authors": "Walsh Colin G et al.",
      "keywords": "artificial intelligence; behavioral health; ethics; health disparities, algorithms,\u00a0mental health; precision medicine; predictive modeling",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32607482/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Development/Training",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "Lack of reliable biological and clinical markers hinders model development, and model explainability challenges impede trust among users. In this perspective, we describe these challenges and discuss design and implementation recommendations to overcome them in intelligent systems for behavioral and mental health.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7309258"
    },
    {
      "pmid": "32671340",
      "title": "Towards accurate and unbiased imaging-based differentiation of Parkinson's disease, progressive supranuclear palsy and corticobasal syndrome.",
      "abstract": "The early and accurate differential diagnosis of parkinsonian disorders is still a significant challenge for clinicians. In recent years, a number of studies have used magnetic resonance imaging data combined with machine learning and statistical classifiers to successfully differentiate between different forms of Parkinsonism. However, several questions and methodological issues remain, to minimize bias and artefact-driven classification. In this study, we compared different approaches for feature selection, as well as different magnetic resonance imaging modalities, with well-matched patient groups and tightly controlling for data quality issues related to patient motion. Our sample was drawn from a cohort of 69 healthy controls, and patients with idiopathic Parkinson's disease (n\u2009=\u200935), progressive supranuclear palsy Richardson's syndrome (n\u2009=\u200952) and corticobasal syndrome (n\u2009=\u200936). Participants underwent standardized T1-weighted and diffusion-weighted magnetic resonance imaging. Strict data quality control and group matching reduced the control and patient numbers to 43, 32, 33 and 26, respectively. We compared two different methods for feature selection and dimensionality reduction: whole-brain principal components analysis, and an anatomical region-of-interest based approach. In both cases, support vector machines were used to construct a statistical model for pairwise classification of healthy controls and patients. The accuracy of each model was estimated using a leave-two-out cross-validation approach, as well as an independent validation using a different set of subjects. Our cross-validation results suggest that using principal components analysis for feature extraction provides higher classification accuracies when compared to a region-of-interest based approach. However, the differences between the two feature extraction methods were significantly reduced when an independent sample was used for validation, suggesting that the principal components analysis approach may be more vulnerable to overfitting with cross-validation. Both T1-weighted and diffusion magnetic resonance imaging data could be used to successfully differentiate between subject groups, with neither modality outperforming the other across all pairwise comparisons in the cross-validation analysis. However, features obtained from diffusion magnetic resonance imaging data resulted in significantly higher classification accuracies when an independent validation cohort was used. Overall, our results support the use of statistical classification approaches for differential diagnosis of parkinsonian disorders. However, classification accuracy can be affected by group size, age, sex and movement artefacts. With appropriate controls and out-of-sample cross validation, diagnostic biomarker evaluation including magnetic resonance imaging based classifiers may be an important adjunct to clinical evaluation.",
      "journal": "Brain communications",
      "year": "2020",
      "doi": "10.1093/braincomms/fcaa051",
      "authors": "Correia Marta M et al.",
      "keywords": "Parkinson\u2019s disease; corticobasal degeneration syndrome; magnetic resonance imaging; progressive supranuclear palsy; support vector machine",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32671340/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Support Vector Machine",
      "health_domain": "Neurology",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "However, classification accuracy can be affected by group size, age, sex and movement artefacts. With appropriate controls and out-of-sample cross validation, diagnostic biomarker evaluation including magnetic resonance imaging based classifiers may be an important adjunct to clinical evaluation.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7325838"
    },
    {
      "pmid": "32838582",
      "title": "Diagnostic accuracy of faecal calprotectin in a symptom-based algorithm for early diagnosis of inflammatory bowel disease adjusting for differential verification bias using a Bayesian approach.",
      "abstract": "BACKGROUND: Diagnostic delay in IBD is a major problem and diagnosis is frequently arrived when irreversible damage has already occurred. This study evaluated accuracy of faecal calprotectin (fCAL) integrated with diagnostic criteria for early diagnosis of IBD in a primary care setting. METHODS: General practitioners (GPs) were trained to recognize alarm symptoms for IBD classified as major and minor criteria. Fulfilment of one major or at least two minor criteria was followed by free fCAL testing and a visit by an IBD specialist and follow-up over 12\u00a0months. All patients with positive fCAL testing, i.e., \u226570\u2009\u03bcg/g underwent colonoscopy. The diagnostic accuracy of fCAL was estimated after adjusting for differential-verification bias following a Bayesian approach. RESULTS: Thirty-four GPs participated in the study and 133 patients were tested for fCAL between July 2016 and August 2017. Positivity of fCAL was seen in 45/133 patients (34%) and a final IBD diagnosis was made in 10/45 (22%). According to the threshold of 70\u2009\u03bcg/g, fCAL achieved a sensitivity of 74.8% (95%CI: 39.10-96.01%), a specificity of 70.4% (95%CI: 61.76-78.16%) and an overall diagnostic accuracy of 70.6% (95%CI: 61.04-78.37%). As for prognostic accuracy, despite positive predictive value being low, 21.9% (95%CI: 11.74-35.18%), the negative predictive value was definitely higher: 96.2% (95%CI: 84.96-99.51%). CONCLUSIONS: fCAL with a threshold set at 70\u2009\u03bcg/g seems to represent a potentially reliable negative test to be used in primary care settings for patients with symptoms suggestive of IBD.",
      "journal": "Scandinavian journal of gastroenterology",
      "year": "2020",
      "doi": "10.1080/00365521.2020.1807599",
      "authors": "Viola Anna et al.",
      "keywords": "Diagnostic delay; diagnostic accuracy; early diagnosis",
      "mesh_terms": "Algorithms; Bayes Theorem; Biomarkers; Delayed Diagnosis; Early Diagnosis; Feces; Humans; Inflammatory Bowel Diseases; Leukocyte L1 Antigen Complex",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32838582/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Primary Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "Primary Care/Outpatient",
      "key_findings": "CONCLUSIONS: fCAL with a threshold set at 70\u2009\u03bcg/g seems to represent a potentially reliable negative test to be used in primary care settings for patients with symptoms suggestive of IBD.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "32880609",
      "title": "Low-Shot Deep Learning of Diabetic Retinopathy With Potential Applications to Address Artificial Intelligence Bias in Retinal Diagnostics and Rare Ophthalmic Diseases.",
      "abstract": "IMPORTANCE: Recent studies have demonstrated the successful application of artificial intelligence (AI) for automated retinal disease diagnostics but have not addressed a fundamental challenge for deep learning systems: the current need for large, criterion standard-annotated retinal data sets for training. Low-shot learning algorithms, aiming to learn from a relatively low number of training data, may be beneficial for clinical situations involving rare retinal diseases or when addressing potential bias resulting from data that may not adequately represent certain groups for training, such as individuals older than 85 years. OBJECTIVE: To evaluate whether low-shot deep learning methods are beneficial when using small training data sets for automated retinal diagnostics. DESIGN, SETTING, AND PARTICIPANTS: This cross-sectional study, conducted from July 1, 2019, to June 21, 2020, compared different diabetic retinopathy classification algorithms, traditional and low-shot, for 2-class designations (diabetic retinopathy warranting referral vs not warranting referral). The public domain EyePACS data set was used, which originally included 88\u202f692 fundi from 44\u202f346 individuals. Statistical analysis was performed from February 1 to June 21, 2020. MAIN OUTCOMES AND MEASURES: The performance (95% CIs) of the various AI algorithms was measured via receiver operating curves and their area under the curve (AUC), precision recall curves, accuracy, and F1 score, evaluated for different training data sizes, ranging from 5120 to 10 samples per class. RESULTS: Deep learning algorithms, when trained with sufficiently large data sets (5120 samples per class), yielded comparable performance, with an AUC of 0.8330 (95% CI, 0.8140-0.8520) for a traditional approach (eg, fined-tuned ResNet), compared with low-shot methods (AUC, 0.8348 [95% CI, 0.8159-0.8537]) (using self-supervised Deep InfoMax [our method denoted as DIM]). However, when far fewer training images were available (n\u2009=\u2009160), the traditional deep learning approach had an AUC decreasing to 0.6585 (95% CI, 0.6332-0.6838) and was outperformed by a low-shot method using self-supervision with an AUC of 0.7467 (95% CI, 0.7239-0.7695). At very low shots (n\u2009=\u200910), the traditional approach had performance close to chance, with an AUC of 0.5178 (95% CI, 0.4909-0.5447) compared with the best low-shot method (AUC, 0.5778 [95% CI, 0.5512-0.6044]). CONCLUSIONS AND RELEVANCE: These findings suggest the potential benefits of using low-shot methods for AI retinal diagnostics when a limited number of annotated training retinal images are available (eg, with rare ophthalmic diseases or when addressing potential AI bias).",
      "journal": "JAMA ophthalmology",
      "year": "2020",
      "doi": "10.1001/jamaophthalmol.2020.3269",
      "authors": "Burlina Philippe et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artificial Intelligence; Cross-Sectional Studies; Deep Learning; Diabetic Retinopathy; Female; Humans; Male; Neural Networks, Computer; ROC Curve; Rare Diseases; Retrospective Studies",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/32880609/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Ophthalmology; Endocrinology/Diabetes",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: Deep learning algorithms, when trained with sufficiently large data sets (5120 samples per class), yielded comparable performance, with an AUC of 0.8330 (95% CI, 0.8140-0.8520) for a traditional approach (eg, fined-tuned ResNet), compared with low-shot methods (AUC, 0.8348 [95% CI, 0.8159-0.8537]) (using self-supervised Deep InfoMax [our method denoted as DIM]). However, when far fewer training images were available (n\u2009=\u2009160), the traditional deep learning approach had an AUC decreasing...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7489388"
    },
    {
      "pmid": "33002153",
      "title": "Estimated Risk for Insulin Dose Error Among Hospital Patients Due to Glucose Meter Hematocrit Bias in 2020.",
      "abstract": "CONTEXT.\u2014: Glycemic control requires accurate blood glucose testing. The extent of hematocrit interference is difficult to assess to assure quality patient care. OBJECTIVE.\u2014: To predict the effect of patient hematocrit on the performance of a glucose meter and its corresponding impact on insulin-dosing error. DESIGN.\u2014: Multilevel mixed regression was conducted to assess the extent that patient hematocrit influences Roche Accu-Chek Inform II glucose meters, using the Radiometer ABL 837 as a reference method collected during validation of 35 new meters. Regression coefficients of fixed effects for reference glucose, hematocrit, an interaction term, and random error were applied to 4 months of patient reference method results extracted from the laboratory information system. A hospital inpatient insulin dose algorithm was used to determine the frequency of insulin dose error between reference glucose and meter glucose results. RESULTS.\u2014: Fixed effects regression for method and hematocrit predicted biases to glucose meter results that met the \"95% within \u00b112%\" for the US Food and Drug Administration goal, but combinations of fixed and random effects exceeded that target in emergency and hospital inpatient units. Insulin dose errors were predicted from the meter results. Twenty-eight percent of intensive care unit, 20.8% of hospital inpatient, and 17.7% of emergency department results were predicted to trigger a \u00b11 insulin dose error by fixed and random effects. CONCLUSIONS.\u2014: The current extent of hematocrit interference on glucose meter performance is anticipated to cause insulin error by 1-dose category, which is likely associated with low patient risk.",
      "journal": "Archives of pathology & laboratory medicine",
      "year": "2020",
      "doi": "10.5858/arpa.2020-0101-RA",
      "authors": "Inman Mark et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Blood Glucose; Hematocrit; Humans; Hypoglycemic Agents; Insulin; Medical Errors; Risk Assessment; United States",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33002153/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Emergency Medicine; ICU/Critical Care; Endocrinology/Diabetes",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; ICU; Emergency Department; Laboratory/Pathology",
      "key_findings": "Twenty-eight percent of intensive care unit, 20.8% of hospital inpatient, and 17.7% of emergency department results were predicted to trigger a \u00b11 insulin dose error by fixed and random effects. CONCLUSIONS.\u2014: The current extent of hematocrit interference on glucose meter performance is anticipated to cause insulin error by 1-dose category, which is likely associated with low patient risk.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "33328054",
      "title": "Ethical limitations of algorithmic fairness solutions in health care machine learning.",
      "abstract": "",
      "journal": "The Lancet. Digital health",
      "year": "2020",
      "doi": "10.1016/S2589-7500(20)30065-0",
      "authors": "McCradden Melissa D et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Delivery of Health Care; Female; Health Equity; Humans; Machine Learning; Male; Models, Biological; Social Justice",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33328054/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "33404333",
      "title": "Using Precision Public Health to Manage Climate Change: Opportunities, Challenges, and Health Justice.",
      "abstract": "Amid public health concerns over climate change, \"precision public health\" (PPH) is emerging in next generation approaches to practice. These novel methods promise to augment public health operations by using ever larger and more robust health datasets combined with new tools for collecting and analyzing data. Precision strategies to protecting the public health could more effectively or efficiently address the systemic threats of climate change, but may also propagate or exacerbate health disparities for the populations most vulnerable in a changing climate. How PPH interventions collect and aggregate data, decide what to measure, and analyze data pose potential issues around privacy, neglecting social determinants of health, and introducing algorithmic bias into climate responses. Adopting a health justice framework, guided by broader social and climate justice tenets, can reveal principles and policy actions which may guide more responsible implementation of PPH in climate responses.",
      "journal": "The Journal of law, medicine & ethics : a journal of the American Society of Law, Medicine & Ethics",
      "year": "2020",
      "doi": "10.1177/1073110520979374",
      "authors": "Johnson Walter G",
      "keywords": "",
      "mesh_terms": "Big Data; Climate Change; Data Analysis; Data Collection; Data Science; Health Equity; Healthcare Disparities; Humans; Precision Medicine; Public Health; Social Determinants of Health; Social Justice",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33404333/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "Public Health",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "How PPH interventions collect and aggregate data, decide what to measure, and analyze data pose potential issues around privacy, neglecting social determinants of health, and introducing algorithmic bias into climate responses. Adopting a health justice framework, guided by broader social and climate justice tenets, can reveal principles and policy actions which may guide more responsible implementation of PPH in climate responses.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "34213259",
      "title": "[Establishment of chromatographic fingerprint of Squama Manis and its applications in animal source identification and quality grade discrimination].",
      "abstract": "Squama Manis, or \"Chuanshanjia\" in Chinese, is a traditional Chinese medicine (TCM) for promoting blood circulation and reducing swelling and discharge; the only animal source used in TCM is the scales of Manis pentadactyla. However, in today's pharmaceutical market, there are many scales from other species of the same genus that are difficult to distinguish from Squama Manis. High-quality and low-quality scales are also severely confused. To solve the above problems, various analytical methods have been developed, such as thin-layer chromatography, mass spectrometry and DNA detection. Owing to their low resolving ability, high equipment cost, and inconvenient operation, none of these methods are appropriate for routine identification of Squama Manis. A chromatographic fingerprint can comprehensively reflect the synergic action of multiple chemical compositions in TCM and has been widely used for the quality control of TCM. In the present study, we established a fingerprint of Squama Manis and explored its feasibility in identifying the origin and quality grade of scales. First, Squama Manis powder was hydrolyzed by hydrochloric acid (1 mol/L). Next, the extract was analyzed on a Symmetry 300 C18 column by linear gradient elution, using 0.1% trifluoroacetic acid (v/v) in water and 0.1% trifluoroacetic acid (v/v) in acetonitrile as the mobile phase and 280 nm as the detection wavelength. The established method was systematically validated, demonstrating good precision, repeatability and sample stability (relative standard deviation (RSD)<5%). Subsequently, samples of different sources and quality grades were distinguished by similarity evaluation and discrimination analysis based on the fingerprint data. In the similarity evaluation, the reference fingerprint was defined as the average fingerprint of twelve first-class samples, and seventeen chromatographic peaks were identified as common peaks. Similarities between the reference fingerprint and fingerprints with different base sources and quality grades were calculated using the absolute area of common peaks as original data. The similarities between Squama Manis and scales from other animals were all less than 0.776, while the similarities between Squama Manis of different grades overlapped significantly, varying from 0.988 to 0.996 for first-class samples and 0.950 to 0.995 for general samples. The results reflected the feasibility of similarity evaluation for discriminating base source and its limitation in the distinguishing between quality grades. Nonetheless, first-class scales showed higher average similarity and lower RSD than general scales, which indicates some level of revelation between fingerprint similarity and quality grade. Thus, a better algorithm or discriminant model is required to distinguish between quality grades. Therefore, a supervised chemometric technique, kernel-based support vector machine (SVM), was applied to construct predictive models. The SVM is a common discriminant model that classifies samples by constructing a separate hyperplane in n-dimensional space, maximizing the margin between classes. Combination with a kernel function can effectively avoid \"dimension disaster\" when dealing with nonlinear data. In the model, the quality grade was defined as a sample label, and the absolute peak areas constituted the data matrix. Verified by 10-fold cross-validation, the unbiased prediction accuracy was up to 95.83%. The predicted results were highly consistent with the actual classifications. The results indicate the high feasibility of the established model for determining quality grade, as it performed significantly better than the similarity evaluation. Samples from batches A and B were completely discriminated and only two samples from batch S were incorrectly classified. Given the batch bias, we believe that model error may have been caused by man-made tag errors rather than the model itself. In conclusion, we established a chromatographic fingerprint for Squama Manis quality analysis and demonstrated its feasibility in animal source identification and quality determination by combining different data analysis methods. The established strategy may provide a new method for improving the the validity and accuracy of Squama Manis in clinical use.",
      "journal": "Se pu = Chinese journal of chromatography",
      "year": "2020",
      "doi": "10.3724/SP.J.1123.2020.04007",
      "authors": "Qiao Yali et al.",
      "keywords": "Squama Manis; base resource; chromatographic fingerprint; discrimination analysis; quality grade; similarity evaluation; traditional Chinese medicine",
      "mesh_terms": "Animals; Biological Products; Chromatography, High Pressure Liquid; Mass Spectrometry; Medicine, Chinese Traditional; Pangolins; Powders; Quality Control",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/34213259/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Support Vector Machine; Clinical Prediction Model",
      "health_domain": "ICU/Critical Care; Drug Discovery/Pharmacology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "In conclusion, we established a chromatographic fingerprint for Squama Manis quality analysis and demonstrated its feasibility in animal source identification and quality determination by combining different data analysis methods. The established strategy may provide a new method for improving the the validity and accuracy of Squama Manis in clinical use.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "29267847",
      "title": "Adjusting for bias introduced by instrumental variable estimation in the Cox proportional hazards model.",
      "abstract": "Instrumental variable (IV) methods are widely used for estimating average treatment effects in the presence of unmeasured confounders. However, the capability of existing IV procedures, and most notably the two-stage residual inclusion (2SRI) algorithm recommended for use in non-linear contexts, to account for unmeasured confounders in the Cox proportional hazard model is unclear. We show that instrumenting an endogenous treatment induces an unmeasured covariate, referred to as an individual frailty in survival analysis parlance, which if not accounted for leads to bias. We propose a new procedure that augments 2SRI with an individual frailty and prove that it is consistent under certain conditions. The finite sample-size behavior is studied across a broad set of conditions via Monte Carlo simulations. Finally, the proposed methodology is used to estimate the average effect of carotid endarterectomy versus carotid stenting on the mortality of patients suffering from carotid artery disease. Results suggest that the 2SRI-frailty estimator generally reduces the bias of both point and interval estimators compared to traditional 2SRI.",
      "journal": "Biostatistics (Oxford, England)",
      "year": "2019",
      "doi": "10.1093/biostatistics/kxx062",
      "authors": "Mart\u00ednez-Camblor Pablo et al.",
      "keywords": "",
      "mesh_terms": "Bias; Biostatistics; Data Interpretation, Statistical; Humans; Monte Carlo Method; Proportional Hazards Models",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29267847/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Survival Analysis",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Finally, the proposed methodology is used to estimate the average effect of carotid endarterectomy versus carotid stenting on the mortality of patients suffering from carotid artery disease. Results suggest that the 2SRI-frailty estimator generally reduces the bias of both point and interval estimators compared to traditional 2SRI.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "30062910",
      "title": "Copas-like selection model to correct publication bias in systematic review of diagnostic test studies.",
      "abstract": "The accuracy of a diagnostic test, which is often quantified by a pair of measures such as sensitivity and specificity, is critical for medical decision making. Separate studies of an investigational diagnostic test can be combined through meta-analysis; however, such an analysis can be threatened by publication bias. To the best of our knowledge, there is no existing method that accounts for publication bias in the meta-analysis of diagnostic tests involving bivariate outcomes. In this paper, we extend the Copas selection model from univariate outcomes to bivariate outcomes for the correction of publication bias when the probability of a study being published can depend on its sensitivity, specificity, and the associated standard errors. We develop an expectation-maximization algorithm for the maximum likelihood estimation under the proposed selection model. We investigate the finite sample performance of the proposed method through simulation studies and illustrate the method by assessing a meta-analysis of 17 published studies of a rapid diagnostic test for influenza.",
      "journal": "Statistical methods in medical research",
      "year": "2019",
      "doi": "10.1177/0962280218791602",
      "authors": "Piao Jin et al.",
      "keywords": "Copas selection model; diagnostic test accuracy; linear mixed model; meta-analysis; systematic reviews",
      "mesh_terms": "Humans; Diagnostic Tests, Routine; Models, Statistical; Publication Bias; Sensitivity and Specificity; Systematic Reviews as Topic",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30062910/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We develop an expectation-maximization algorithm for the maximum likelihood estimation under the proposed selection model. We investigate the finite sample performance of the proposed method through simulation studies and illustrate the method by assessing a meta-analysis of 17 published studies of a rapid diagnostic test for influenza.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC6443505"
    },
    {
      "pmid": "30267539",
      "title": "Semiparametric regression analysis of length-biased interval-censored data.",
      "abstract": "In prevalent cohort design, subjects who have experienced an initial event but not the failure event are preferentially enrolled and the observed failure times are often length-biased. Moreover, the prospective follow-up may not be continuously monitored and failure times are subject to interval censoring. We study the nonparametric maximum likelihood estimation for the proportional hazards model with length-biased interval-censored data. Direct maximization of likelihood function is intractable, thus we develop a computationally simple and stable expectation-maximization algorithm through introducing two layers of data augmentation. We establish the strong consistency, asymptotic normality and efficiency of the proposed estimator and provide an inferential procedure through profile likelihood. We assess the performance of the proposed methods through extensive simulations and apply the proposed methods to the Massachusetts Health Care Panel Study.",
      "journal": "Biometrics",
      "year": "2019",
      "doi": "10.1111/biom.12970",
      "authors": "Gao Fei et al.",
      "keywords": "Nonparametric maximum likelihood estimation; left truncation; proportional hazards model; semiparametric efficiency",
      "mesh_terms": "Activities of Daily Living; Aged; Aged, 80 and over; Algorithms; Bias; Computer Simulation; Data Interpretation, Statistical; Humans; Likelihood Functions; Proportional Hazards Models; Regression Analysis; Survival Analysis",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30267539/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Data Augmentation",
      "clinical_setting": "Not specified",
      "key_findings": "We establish the strong consistency, asymptotic normality and efficiency of the proposed estimator and provide an inferential procedure through profile likelihood. We assess the performance of the proposed methods through extensive simulations and apply the proposed methods to the Massachusetts Health Care Panel Study.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC8614128"
    },
    {
      "pmid": "30652884",
      "title": "Association between negative cognitive bias and depression: A symptom-level approach.",
      "abstract": "Cognitive models of depression posit that negatively biased self-referent processing and attention have important roles in the disorder. However, depression is a heterogeneous collection of symptoms and all symptoms are unlikely to be associated with these negative cognitive biases. The current study involved 218 community adults whose depression ranged from no symptoms to clinical levels of depression. Random forest machine learning was used to identify the most important depression symptom predictors of each negative cognitive bias. Depression symptoms were measured with the Beck Depression Inventory-II. Model performance was evaluated using predictive R-squared (Rpred2), the expected variance explained in data not used to train the algorithm, estimated by 10 repetitions of 10-fold cross-validation. Using the self-referent encoding task (SRET), depression symptoms explained 34% to 45% of the variance in negative self-referent processing. The symptoms of sadness, self-dislike, pessimism, feelings of punishment, and indecision were most important. Notably, many depression symptoms made virtually no contribution to this prediction. In contrast, for attention bias for sad stimuli, measured with the dot-probe task using behavioral reaction time (RT) and eye gaze metrics, no reliable symptom predictors were identified. Findings indicate that a symptom-level approach may provide new insights into which symptoms, if any, are associated with negative cognitive biases in depression. (PsycINFO Database Record (c) 2019 APA, all rights reserved).",
      "journal": "Journal of abnormal psychology",
      "year": "2019",
      "doi": "10.1037/abn0000405",
      "authors": "Beevers Christopher G et al.",
      "keywords": "",
      "mesh_terms": "Adolescent; Adult; Attention; Attentional Bias; Cognition Disorders; Depression; Depressive Disorder; Emotions; Female; Fixation, Ocular; Humans; Male; Personality Inventory; Reaction Time; Research Design; Young Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30652884/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Random Forest",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Findings indicate that a symptom-level approach may provide new insights into which symptoms, if any, are associated with negative cognitive biases in depression. (PsycINFO Database Record (c) 2019 APA, all rights reserved).",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC6449499"
    },
    {
      "pmid": "30669741",
      "title": "[Introduction on 'assessing the risk of bias of individual studies' in systematic review of health-care intervention programs revised by the Agency for Healthcare Research and Quality].",
      "abstract": "This paper summarizes the Risk of Bias of Individual Studies in Systematic Reviews of Health Care Interventions revised by the Agency for Healthcare Research and Quality (AHRQ) and introduces how to use Revman software make risk of bias graph or risk of bias summary. AHRQ tool can be used to evaluate following study designs: RCTs, cohort study, case-control study (including nested case-control), case series study and cross-sectional study. The tool evaluates the risk of bias of individual studies from selection bias, performance bias, attrition bias, detection bias and reporting bias. Each of the bias domains contains different items, and each item is available for the assessment of one or more study designs. It is worth noting that the appropriate items should be selected for evaluation different study designs instead of using all items to directly assess the risk of bias. AHRQ tool can be used to evaluate risk of bias individual studies when systematic reviews of health care interventions is including different study designs. Moreover, the tool items are relatively easy to understand and the assessment process is not complicated. AHRQ recommends the use of high, medium and low risk classification methods to assess the overall risk of bias of individual studies. However, AHRQ gives no recommendations on how to determine the overall bias grade. It is expected that future research will give corresponding recommendations. \u672c\u6587\u5bf9\u7f8e\u56fd\u536b\u751f\u4fdd\u5065\u7814\u7a76\u548c\u8d28\u91cf\u673a\u6784\uff08Agency for Healthcare Research and Quality\uff0cAHRQ\uff09\u63a8\u8350\u7684\u5e72\u9884\u6027\u7814\u7a76\u504f\u501a\u98ce\u9669\u8bc4\u4ef7\u5de5\u5177\u7684\u4e3b\u8981\u5185\u5bb9\u8fdb\u884c\u8be6\u7ec6\u89e3\u8bfb\uff0c\u5e76\u5c55\u793a\u5982\u4f55\u4f7f\u7528Revman\u8f6f\u4ef6\u5236\u4f5c\u504f\u501a\u98ce\u9669\u8bc4\u4ef7\u56fe\u3002AHRQ\u504f\u501a\u98ce\u9669\u8bc4\u4ef7\u5de5\u5177\u662f\u4e00\u79cd\u7efc\u5408\u8bc4\u4ef7\u5de5\u5177\uff0c\u53ef\u7528\u6765\u8bc4\u4ef7\u5e38\u89c1\u7684\u7814\u7a76\u8bbe\u8ba1\u7c7b\u578b\uff08\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u7814\u7a76\u3001\u6709\u5bf9\u7167\u7684\u4e34\u5e8a\u8bd5\u9a8c\u7814\u7a76\u3001\u961f\u5217\u7814\u7a76\u3001\u75c5\u4f8b\u5bf9\u7167\u7814\u7a76\u3001\u75c5\u4f8b\u7cfb\u5217\u7814\u7a76\u3001\u6a2a\u65ad\u9762\u7814\u7a76\uff09\u7684\u504f\u501a\u98ce\u9669\u3002\u8be5\u5de5\u5177\u4ece\u9009\u62e9\u504f\u501a\u3001\u5b9e\u65bd\u504f\u501a\u3001\u968f\u8bbf\u504f\u501a\u3001\u6d4b\u91cf\u504f\u501a\u3001\u62a5\u544a\u504f\u501a5\u4e2a\u9886\u57df\u6765\u8bc4\u4ef7\u7814\u7a76\u7684\u504f\u501a\u98ce\u9669\uff0c\u6bcf\u4e2a\u9886\u57df\u542b\u6709\u4e0d\u540c\u7684\u6761\u76ee\uff0c\u6bcf\u4e2a\u6761\u76ee\u9002\u7528\u4e8e\u8bc4\u4ef71\u79cd\u6216\u51e0\u79cd\u7814\u7a76\u8bbe\u8ba1\u7c7b\u578b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\u5e94\u6839\u636e\u4e0d\u540c\u7684\u7814\u7a76\u8bbe\u8ba1\u7c7b\u578b\u9009\u62e9\u76f8\u5e94\u7684\u6761\u76ee\u8fdb\u884c\u8bc4\u4ef7\u800c\u4e0d\u662f\u76f4\u63a5\u4f7f\u7528\u6240\u6709\u6761\u76ee\u8fdb\u884c\u504f\u501a\u98ce\u9669\u8bc4\u4ef7\u3002\u5f531\u4e2a\u5e72\u9884\u6027\u7814\u7a76\u7684\u7cfb\u7edf\u7efc\u8ff0\u7eb3\u5165\u4e86\u591a\u79cd\u7814\u7a76\u8bbe\u8ba1\u7c7b\u578b\u65f6\uff0c\u53ea\u9700\u8981\u7528AHRQ\u5de5\u5177\u5c31\u53ef\u4ee5\u8bc4\u4ef7\u7eb3\u5165\u7814\u7a76\u5b58\u5728\u7684\u5e38\u89c1\u504f\u501a\u98ce\u9669\uff0c\u7701\u53bb\u4e86\u4f7f\u7528\u4e0d\u540c\u504f\u501a\u98ce\u9669\u5de5\u5177\u8fdb\u884c\u504f\u501a\u98ce\u9669\u8bc4\u4ef7\u7684\u7e41\u7410\u8fc7\u7a0b\u3002\u8be5\u5de5\u5177\u6761\u76ee\u76f8\u5bf9\u7b80\u5355\u6613\u61c2\uff0c\u8bc4\u4ef7\u6d41\u7a0b\u4e0d\u590d\u6742\u3002AHRQ\u63a8\u8350\u4f7f\u7528\u9ad8\u3001\u4e2d\u3001\u4f4e\u7684\u504f\u501a\u98ce\u9669\u5206\u7c7b\u65b9\u6cd5\u8bc4\u4ef7\u7eb3\u5165\u7814\u7a76\u603b\u4f53\u504f\u501a\u98ce\u9669\u7684\u9ad8\u4f4e\uff0c\u4f46\u662f\uff0c\u5176\u5bf9\u5982\u4f55\u5224\u5b9a\u603b\u4f53\u504f\u501a\u98ce\u9669\u7684\u9ad8\u4f4e\u6ca1\u6709\u7ed9\u51fa\u63a8\u8350\u610f\u89c1\uff0c\u5982\u4f55\u5177\u4f53\u5224\u5b9a\u5e72\u9884\u6027\u7814\u7a76\u504f\u501a\u98ce\u9669\u7b49\u7ea7\u7684\u754c\u503c\uff0c\u4ecd\u6709\u5f85\u66f4\u591a\u8fd9\u65b9\u9762\u7684\u7814\u7a76\u7ed3\u679c\u3002.",
      "journal": "Zhonghua liu xing bing xue za zhi = Zhonghua liuxingbingxue zazhi",
      "year": "2019",
      "doi": "10.3760/cma.j.issn.0254-6450.2019.01.021",
      "authors": "Yang J C et al.",
      "keywords": "Common study designs; Intervention study; Risk of bias; Systematic review; Tool for assessment",
      "mesh_terms": "Bias; Evidence-Based Medicine; Health Services Research; Systematic Reviews as Topic",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30669741/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Clinical Trial",
      "key_findings": "It is expected that future research will give corresponding recommendations. \u672c\u6587\u5bf9\u7f8e\u56fd\u536b\u751f\u4fdd\u5065\u7814\u7a76\u548c\u8d28\u91cf\u673a\u6784\uff08Agency for Healthcare Research and Quality\uff0cAHRQ\uff09\u63a8\u8350\u7684\u5e72\u9884\u6027\u7814\u7a76\u504f\u501a\u98ce\u9669\u8bc4\u4ef7\u5de5\u5177\u7684\u4e3b\u8981\u5185\u5bb9\u8fdb\u884c\u8be6\u7ec6\u89e3\u8bfb\uff0c\u5e76\u5c55\u793a\u5982\u4f55\u4f7f\u7528Revman\u8f6f\u4ef6\u5236\u4f5c\u504f\u501a\u98ce\u9669\u8bc4\u4ef7\u56fe\u3002AHRQ\u504f\u501a\u98ce\u9669\u8bc4\u4ef7\u5de5\u5177\u662f\u4e00\u79cd\u7efc\u5408\u8bc4\u4ef7\u5de5\u5177\uff0c\u53ef\u7528\u6765\u8bc4\u4ef7\u5e38\u89c1\u7684\u7814\u7a76\u8bbe\u8ba1\u7c7b\u578b\uff08\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u7814\u7a76\u3001\u6709\u5bf9\u7167\u7684\u4e34\u5e8a\u8bd5\u9a8c\u7814\u7a76\u3001\u961f\u5217\u7814\u7a76\u3001\u75c5\u4f8b\u5bf9\u7167\u7814\u7a76\u3001\u75c5\u4f8b\u7cfb\u5217\u7814\u7a76\u3001\u6a2a\u65ad\u9762\u7814\u7a76\uff09\u7684\u504f\u501a\u98ce\u9669\u3002\u8be5\u5de5\u5177\u4ece\u9009\u62e9\u504f\u501a\u3001\u5b9e\u65bd\u504f\u501a\u3001\u968f\u8bbf\u504f\u501a\u3001\u6d4b\u91cf\u504f\u501a\u3001\u62a5\u544a\u504f\u501a5\u4e2a\u9886\u57df\u6765\u8bc4\u4ef7\u7814\u7a76\u7684\u504f\u501a\u98ce\u9669\uff0c\u6bcf\u4e2a\u9886\u57df\u542b\u6709\u4e0d\u540c\u7684\u6761\u76ee\uff0c\u6bcf\u4e2a\u6761\u76ee\u9002\u7528\u4e8e\u8bc4\u4ef71\u79cd\u6216\u51e0\u79cd\u7814\u7a76\u8bbe\u8ba1\u7c7b\u578b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\u5e94\u6839\u636e\u4e0d\u540c\u7684\u7814\u7a76\u8bbe\u8ba1\u7c7b\u578b\u9009\u62e9\u76f8\u5e94\u7684\u6761\u76ee\u8fdb\u884c\u8bc4\u4ef7\u800c\u4e0d\u662f\u76f4\u63a5\u4f7f\u7528\u6240\u6709\u6761\u76ee\u8fdb\u884c\u504f\u501a\u98ce\u9669\u8bc4\u4ef7\u3002\u5f531\u4e2a\u5e72\u9884\u6027\u7814\u7a76\u7684\u7cfb\u7edf\u7efc\u8ff0\u7eb3\u5165\u4e86\u591a\u79cd\u7814\u7a76\u8bbe\u8ba1\u7c7b\u578b\u65f6\uff0c\u53ea\u9700\u8981\u7528AHRQ\u5de5\u5177\u5c31\u53ef\u4ee5\u8bc4\u4ef7\u7eb3\u5165\u7814\u7a76\u5b58\u5728\u7684\u5e38\u89c1\u504f\u501a\u98ce\u9669\uff0c\u7701\u53bb\u4e86\u4f7f\u7528\u4e0d\u540c\u504f\u501a\u98ce\u9669\u5de5\u5177\u8fdb\u884c\u504f\u501a\u98ce\u9669\u8bc4\u4ef7\u7684\u7e41\u7410\u8fc7\u7a0b\u3002\u8be5\u5de5\u5177\u6761\u76ee\u76f8\u5bf9\u7b80\u5355\u6613...",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "30841773",
      "title": "Bias-corrected estimates of reduction of post-surgery length of stay and corresponding cost savings through the widespread national implementation of fast-tracking after liver transplantation: a quasi-experimental study.",
      "abstract": "Background: Fast-tracking is an approach adopted by Mayo Clinic in Florida's (MCF) liver transplant (LT) program, which consists of early tracheal extubation and transfer of patients to surgical ward, eliminating a stay in the intensive care unit in select patients. Since adopting this approach in 2002, MCF has successfully fast-tracked 54.3% of patients undergoing LT. Objectives: This study evaluated the reduction in post-operative length of stay (LOS) that resulted from the fast-tracking protocol and assessed the potential cost saving in the case of nationwide implementation. Methods: A propensity score for fast-tracking was generated based on MCF liver transplant databases during 2011-2013. Various propensity score matching algorithms were used to form control groups from the United Network of Organ Sharing Standard Analysis and Research (STAR) file that had comparable demographic characteristics and health status to the treatment group identified in MCF. Multiple regression and matching estimators were employed for evaluation of the post-surgery LOS. The algorithm generated from the analysis was also applied to the STAR data to determine the proportion of patients in the US who could potentially be candidates for fast-tracking, and the potential savings. Results: The effect of the fast-tracking on the post-transplant LOS was estimated at approximately from 2.5 (p-value\u2009=\u20090.001) to 3.2 (p-value\u2009<\u20090.001) days based on various matching algorithms. The cost saving from a nationwide implementation of fast-tracking of liver transplant patients was estimated to be at least $78 million during the 2-year period. Conclusion: The fast-track program was found to be effective in reducing post-transplant LOS, although the reduction appeared to be less than previously reported. Nationwide implementation of fast-tracking could result in substantial cost savings without compromising the patient outcome.",
      "journal": "Journal of medical economics",
      "year": "2019",
      "doi": "10.1080/13696998.2019.1592179",
      "authors": "Loh Chung-Ping A et al.",
      "keywords": "C40; C90; Fast-tracking; I11; I19; length of stay; liver transplant; matching; propensity score; quasi-experimental study",
      "mesh_terms": "Academic Medical Centers; Age Factors; Cohort Studies; Cost Savings; Databases, Factual; Early Ambulation; Female; Florida; Humans; Intensive Care Units; Length of Stay; Liver Transplantation; Logistic Models; Male; Middle Aged; Multivariate Analysis; Postoperative Care; Retrospective Studies; Risk Factors; Selection Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30841773/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Generative AI",
      "health_domain": "ICU/Critical Care; Surgery",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "Conclusion: The fast-track program was found to be effective in reducing post-transplant LOS, although the reduction appeared to be less than previously reported. Nationwide implementation of fast-tracking could result in substantial cost savings without compromising the patient outcome.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "30892656",
      "title": "Fair compute loads enabled by blockchain: sharing models by alternating client and server roles.",
      "abstract": "OBJECTIVE: Decentralized privacy-preserving predictive modeling enables multiple institutions to learn a more generalizable model on healthcare or genomic data by sharing the partially trained models instead of patient-level data, while avoiding risks such as single point of control. State-of-the-art blockchain-based methods remove the \"server\" role but can be less accurate than models that rely on a server. Therefore, we aim at developing a general model sharing framework to preserve predictive correctness, mitigate the risks of a centralized architecture, and compute the models in a fair way. MATERIALS AND METHODS: We propose a framework that includes both server and \"client\" roles to preserve correctness. We adopt a blockchain network to obtain the benefits of decentralization, by alternating the roles for each site to ensure computational fairness. Also, we developed GloreChain (Grid Binary LOgistic REgression on Permissioned BlockChain) as a concrete example, and compared it to a centralized algorithm on 3 healthcare or genomic datasets to evaluate predictive correctness, number of learning iterations and execution time. RESULTS: GloreChain performs exactly the same as the centralized method in terms of correctness and number of iterations. It inherits the advantages of blockchain, at the cost of increased time to reach a consensus model. DISCUSSION: Our framework is general or flexible and can also address intrinsic challenges of blockchain networks. Further investigations will focus on higher-dimensional datasets, additional use cases, privacy-preserving quality concerns, and ethical, legal, and social implications. CONCLUSIONS: Our framework provides a promising potential for institutions to learn a predictive model based on healthcare or genomic data in a privacy-preserving and decentralized way.",
      "journal": "Journal of the American Medical Informatics Association : JAMIA",
      "year": "2019",
      "doi": "10.1093/jamia/ocy180",
      "authors": "Kuo Tsung-Ting et al.",
      "keywords": "batch machine learning; blockchain distributed ledger technology; clinical information systems; decision support systems; privacy-preserving predictive modeling",
      "mesh_terms": "Algorithms; Blockchain; Computer Communication Networks; Confidentiality; Decision Support Systems, Clinical; Genomics; Health Information Systems; Humans; Information Dissemination; Information Systems; Machine Learning; Models, Theoretical",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30892656/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Logistic Regression; Clinical Prediction Model",
      "health_domain": "EHR/Health Informatics; Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Our framework provides a promising potential for institutions to learn a predictive model based on healthcare or genomic data in a privacy-preserving and decentralized way.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7787356"
    },
    {
      "pmid": "30924225",
      "title": "Investigating systematic bias in brain age estimation with application to post-traumatic stress disorders.",
      "abstract": "Brain age prediction using machine-learning techniques has recently attracted growing attention, as it has the potential to serve as a biomarker for characterizing the typical brain development and neuropsychiatric disorders. Yet one long-standing problem is that the predicted brain age is overestimated in younger subjects and underestimated in older. There is a plethora of claims as to the bias origins, both methodologically and in data itself. With a large neuroanatomical dataset (N\u2009=\u20092,026; 6-89 years of age) from multiple shared datasets, we show this bias is neither data-dependent nor specific to particular method including deep neural network. We present an alternative account that offers a statistical explanation for the bias and describe a simple, yet efficient, method using general linear model to adjust the bias. We demonstrate the effectiveness of bias adjustment with a large multi-modal neuroimaging data (N\u2009=\u2009804; 8-21 years of age) for both healthy controls and post-traumatic stress disorders patients obtained from the Philadelphia Neurodevelopmental Cohort.",
      "journal": "Human brain mapping",
      "year": "2019",
      "doi": "10.1002/hbm.24588",
      "authors": "Liang Hualou et al.",
      "keywords": "PTSD; bias; brain age prediction; machine-learning; regression to the mean",
      "mesh_terms": "Adolescent; Adult; Aged; Aged, 80 and over; Aging; Brain; Child; Humans; Machine Learning; Magnetic Resonance Imaging; Middle Aged; Neuroimaging; Stress Disorders, Post-Traumatic; Young Adult",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30924225/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Deep Learning; Neural Network",
      "health_domain": "Mental Health/Psychiatry; Emergency Medicine; ICU/Critical Care; Pediatrics; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "We present an alternative account that offers a statistical explanation for the bias and describe a simple, yet efficient, method using general linear model to adjust the bias. We demonstrate the effectiveness of bias adjustment with a large multi-modal neuroimaging data (N\u2009=\u2009804; 8-21 years of age) for both healthy controls and post-traumatic stress disorders patients obtained from the Philadelphia Neurodevelopmental Cohort.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC6865701"
    },
    {
      "pmid": "30935874",
      "title": "Bias in analytical chemistry: A review of selected procedures for incorporating uncorrected bias into the expanded uncertainty of analytical measurements and a graphical method for evaluating the concordance of reference and test procedures.",
      "abstract": "The Evaluation of measurement data - Guide to the Expression of Uncertainty in Measurement (GUM) provides the framework for evaluating measurement uncertainty. The preferred GUM approach for addressing bias assumes that all systematic errors are identified and corrected at an early stage in the measurement process. We review some procedures for treating uncorrected bias and its inclusion into an overall uncertainty statement. When bias and its uncertainty are recognised as metrological states independent of scatter in the test results, the uncertainty of the reference and uncertainty of the bias can be equated. The net standard uncertainty of a test result is the root-sum-square of the standard uncertainty of the bias and the standard uncertainty of measurements on the test. Since an incomplete and therefore potentially erroneous formula is often used for estimating bias standard uncertainty, we propose an alternative calculation. We next propose a graphical method using a simple algorithm that quantifies the discrepancy between the results of a test measurement and the corresponding reference value, in terms of the percentage overlap of two probability density functions. We propose that bias should be corrected wherever possible and we illustrate this approach using the graphical method. Even though this review is focused principally on analytical chemistry and medical laboratory applications, much of the discussion is applicable to all areas of metrology.",
      "journal": "Clinica chimica acta; international journal of clinical chemistry",
      "year": "2019",
      "doi": "10.1016/j.cca.2019.03.1633",
      "authors": "Frenkel Robert et al.",
      "keywords": "Bias; Bias correction; Bias uncertainty; Measurement uncertainty; Overlap; Probability density function",
      "mesh_terms": "Bias; Clinical Laboratory Techniques; Data Analysis; Humans; Reference Standards",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30935874/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Laboratory/Pathology",
      "key_findings": "We propose that bias should be corrected wherever possible and we illustrate this approach using the graphical method. Even though this review is focused principally on analytical chemistry and medical laboratory applications, much of the discussion is applicable to all areas of metrology.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "30947901",
      "title": "The Role of the ACR Data Science Institute in Advancing Health Equity in Radiology.",
      "abstract": "Commercially available artificial intelligence (AI) algorithms outside of health care have been shown to be susceptible to ethnic, gender, and social bias, which has important implications in the development of AI algorithms in health care and the radiologic sciences. To prevent the introduction bias in health care AI, the physician community should work with developers and regulators to develop pathways to ensure that algorithms marketed for widespread clinical practice are safe, effective, and free of unintended bias. The ACR Data Science Institute has developed structured AI use cases with data elements that allow the development of standardized data sets for AI testing and training across multiple institutions to promote the availability of diverse data for algorithm development. Additionally, the ACR Data Science Institute validation and monitoring services, ACR Certify-AI and ACR Assess-AI, incorporate standards to mitigate algorithm bias and promote health equity. In addition to promoting diversity, the ACR should promote and advocate for payment models for AI that afford access to AI tools for all of our patients regardless of socioeconomic status or the inherent resources of their health systems.",
      "journal": "Journal of the American College of Radiology : JACR",
      "year": "2019",
      "doi": "10.1016/j.jacr.2018.12.038",
      "authors": "Allen Bibb et al.",
      "keywords": "Artificial intelligence; algorithm bias; algorithm training; algorithm validation; structured AI use cases",
      "mesh_terms": "Artificial Intelligence; Data Science; Female; Health Equity; Healthcare Disparities; Humans; Male; Outcome Assessment, Health Care; Program Development; Program Evaluation; Radiology; United States",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30947901/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Socioeconomic Status",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Additionally, the ACR Data Science Institute validation and monitoring services, ACR Certify-AI and ACR Assess-AI, incorporate standards to mitigate algorithm bias and promote health equity. In addition to promoting diversity, the ACR should promote and advocate for payment models for AI that afford access to AI tools for all of our patients regardless of socioeconomic status or the inherent resources of their health systems.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "31138828",
      "title": "Rapid discrimination of multiple myeloma patients by artificial neural networks coupled with mass spectrometry of peripheral blood plasma.",
      "abstract": "Multiple myeloma (MM) is a highly heterogeneous disease of malignant plasma cells. Diagnosis and monitoring of MM patients is based on bone marrow biopsies and detection of abnormal immunoglobulin in serum and/or urine. However, biopsies have a single-site bias; thus, new diagnostic tests and early detection strategies are needed. Matrix-Assisted Laser Desorption/Ionization Time-of Flight Mass Spectrometry (MALDI-TOF MS) is a powerful method that found its applications in clinical diagnostics. Artificial intelligence approaches, such as Artificial Neural Networks (ANNs), can handle non-linear data and provide prediction and classification of variables in multidimensional datasets. In this study, we used MALDI-TOF MS to acquire low mass profiles of peripheral blood plasma obtained from MM patients and healthy donors. Informative patterns in mass spectra served as inputs for ANN that specifically predicted MM samples with high sensitivity (100%), specificity (95%) and accuracy (98%). Thus, mass spectrometry coupled with ANN can provide a minimally invasive approach for MM diagnostics.",
      "journal": "Scientific reports",
      "year": "2019",
      "doi": "10.1038/s41598-019-44215-1",
      "authors": "Deulofeu Meritxell et al.",
      "keywords": "",
      "mesh_terms": "Aged; Aged, 80 and over; Artificial Intelligence; Bone Marrow; Case-Control Studies; Datasets as Topic; Female; Humans; Immunoglobulins; Male; Metabolic Networks and Pathways; Metabolome; Middle Aged; Multiple Myeloma; Neural Networks, Computer; Principal Component Analysis; Spectrometry, Mass, Matrix-Assisted Laser Desorption-Ionization",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31138828/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Neural Network",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Informative patterns in mass spectra served as inputs for ANN that specifically predicted MM samples with high sensitivity (100%), specificity (95%) and accuracy (98%). Thus, mass spectrometry coupled with ANN can provide a minimally invasive approach for MM diagnostics.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC6538619"
    },
    {
      "pmid": "31241574",
      "title": "Algorithmic Bias and Computer-Assisted Scoring of Patient Notes in the USMLE Step 2 Clinical Skills Exam.",
      "abstract": "",
      "journal": "Academic medicine : journal of the Association of American Medical Colleges",
      "year": "2019",
      "doi": "10.1097/ACM.0000000000002746",
      "authors": "Spadafore Maxwell et al.",
      "keywords": "",
      "mesh_terms": "Bias; Clinical Competence; Educational Measurement; Humans; Licensure, Medical; Natural Language Processing",
      "pub_types": "Letter; Comment",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31241574/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "31278181",
      "title": "Benefits, Pitfalls, and Potential Bias in Health Care AI.",
      "abstract": "As the health care industry adopts artificial intelligence, machine learning, and other modeling techniques, it is seeing benefits to both patient outcomes and cost reduction; however, it needs to be cognizant of and ensure proper management of the risks, including bias. Lessons learned from other industries may provide a framework for acknowledging and managing data, machine, and human biases that arise while implementing AI.",
      "journal": "North Carolina medical journal",
      "year": "2019",
      "doi": "10.18043/ncm.80.4.219",
      "authors": "Hague Douglas C",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Bias; Data Analysis; Delivery of Health Care; Humans; Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31278181/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "As the health care industry adopts artificial intelligence, machine learning, and other modeling techniques, it is seeing benefits to both patient outcomes and cost reduction; however, it needs to be cognizant of and ensure proper management of the risks, including bias. Lessons learned from other industries may provide a framework for acknowledging and managing data, machine, and human biases that arise while implementing AI.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "31409293",
      "title": "Guide for library design and bias correction for large-scale transcriptome studies using highly multiplexed RNAseq methods.",
      "abstract": "BACKGROUND: Standard RNAseq methods using bulk RNA and recent single-cell RNAseq methods use DNA barcodes to identify samples and cells, and the barcoded cDNAs are pooled into a library pool before high throughput sequencing. In cases of single-cell and low-input RNAseq methods, the library is further amplified by PCR after the pooling. Preparation of hundreds or more samples for a large study often requires multiple library pools. However, sometimes correlation between expression profiles among the libraries is low and batch effect biases make integration of data between library pools difficult. RESULTS: We investigated 166 technical replicates in 14 RNAseq libraries made using the STRT method. The patterns of the library biases differed by genes, and uneven library yields were associated with library biases. The former bias was corrected using the NBGLM-LBC algorithm, which we present in the current study. The latter bias could not be corrected directly, but could be solved by omitting libraries with particularly low yields. A simulation experiment suggested that the library bias correction using NBGLM-LBC requires a consistent sample layout. The NBGLM-LBC correction method was applied to an expression profile for a cohort study of childhood acute respiratory illness, and the library biases were resolved. CONCLUSIONS: The R source code for the library bias correction named NBGLM-LBC is available at https://shka.github.io/NBGLM-LBC and https://shka.bitbucket.io/NBGLM-LBC . This method is applicable to correct the library biases in various studies that use highly multiplexed sequencing-based profiling methods with a consistent sample layout with samples to be compared (e.g., \"cases\" and \"controls\") equally distributed in each library.",
      "journal": "BMC bioinformatics",
      "year": "2019",
      "doi": "10.1186/s12859-019-3017-9",
      "authors": "Katayama Shintaro et al.",
      "keywords": "Gene expression; Library bias correction; Next-generation sequencing; Transcriptome",
      "mesh_terms": "Cell Line; Cluster Analysis; Gene Library; High-Throughput Nucleotide Sequencing; Humans; Principal Component Analysis; RNA; Sequence Analysis, RNA; Transcriptome; User-Computer Interface",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31409293/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Pediatrics; Pulmonology; Genomics/Genetics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSIONS: The R source code for the library bias correction named NBGLM-LBC is available at https://shka.github.io/NBGLM-LBC and https://shka.bitbucket.io/NBGLM-LBC . This method is applicable to correct the library biases in various studies that use highly multiplexed sequencing-based profiling methods with a consistent sample layout with samples to be compared (e.g., \"cases\" and \"controls\") equally distributed in each library.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC6693229"
    },
    {
      "pmid": "31441044",
      "title": "Investigation of bias in an epilepsy machine learning algorithm trained on physician notes.",
      "abstract": "Racial disparities in the utilization of epilepsy surgery are well documented, but it is unknown whether a natural language processing (NLP) algorithm trained on physician notes would produce biased recommendations for epilepsy presurgical evaluations. To assess this, an NLP algorithm was trained to identify potential surgical candidates using 1097 notes from 175 epilepsy patients with a history of resective epilepsy surgery and 268 patients who achieved seizure freedom without surgery (total N\u00a0=\u00a0443 patients). The model was tested on 8340 notes from 3776 patients with epilepsy whose surgical candidacy status was unknown (2029 male, 1747 female, median age = 9\u00a0years; age range = 0-60\u00a0years). Multiple linear regression using demographic variables as covariates was used to test for correlations between patient race and surgical candidacy scores. After accounting for other demographic and socioeconomic variables, patient race, gender, and primary language did not influence surgical candidacy scores (P\u00a0>\u00a0.35 for all). Higher scores were given to patients >18\u00a0years old who traveled farther to receive care, and those who had a higher family income and public insurance (P\u00a0<\u00a0.001, .001, .001, and .01, respectively). Demographic effects on surgical candidacy scores appeared to reflect patterns in patient referrals.",
      "journal": "Epilepsia",
      "year": "2019",
      "doi": "10.1111/epi.16320",
      "authors": "Wissel Benjamin D et al.",
      "keywords": "clinical decision support; epilepsy surgery; machine learning; natural language processing",
      "mesh_terms": "Adolescent; Adult; Age Factors; Algorithms; Child; Child, Preschool; Electroencephalography; Epilepsy; Healthcare Disparities; Humans; Infant; Machine Learning; Middle Aged; Patient Selection; Prejudice; Referral and Consultation; Young Adult",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31441044/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "NLP/LLM; Regression",
      "health_domain": "Surgery; Pediatrics; Neurology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Socioeconomic Status; Language; Insurance Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Higher scores were given to patients >18\u00a0years old who traveled farther to receive care, and those who had a higher family income and public insurance (P\u00a0<\u00a0.001, .001, .001, and .01, respectively). Demographic effects on surgical candidacy scores appeared to reflect patterns in patient referrals.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC6731998"
    },
    {
      "pmid": "31649194",
      "title": "Dissecting racial bias in an algorithm used to manage the health of populations.",
      "abstract": "Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.",
      "journal": "Science (New York, N.Y.)",
      "year": "2019",
      "doi": "10.1126/science.aax2342",
      "authors": "Obermeyer Ziad et al.",
      "keywords": "",
      "mesh_terms": "Black or African American; Algorithms; Bias; Chronic Disease; Health Care Costs; Health Status Disparities; Humans; Medical Records; Racism; Risk Assessment; United States; White People",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31649194/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Clinical Prediction Model",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "31665002",
      "title": "Key challenges for delivering clinical impact with artificial intelligence.",
      "abstract": "BACKGROUND: Artificial intelligence (AI) research in healthcare is accelerating rapidly, with potential applications being demonstrated across various domains of medicine. However, there are currently limited examples of such techniques being successfully deployed into clinical practice. This article explores the main challenges and limitations of AI in healthcare, and considers the steps required to translate these potentially transformative technologies from research to clinical practice. MAIN BODY: Key challenges for the translation of AI systems in healthcare include those intrinsic to the science of machine learning, logistical difficulties in implementation, and consideration of the barriers to adoption as well as of the necessary sociocultural or pathway changes. Robust peer-reviewed clinical evaluation as part of randomised controlled trials should be viewed as the gold standard for evidence generation, but conducting these in practice may not always be appropriate or feasible. Performance metrics should aim to capture real clinical applicability and be understandable to intended users. Regulation that balances the pace of innovation with the potential for harm, alongside thoughtful post-market surveillance, is required to ensure that patients are not exposed to dangerous interventions nor deprived of access to beneficial innovations. Mechanisms to enable direct comparisons of AI systems must be developed, including the use of independent, local and representative test sets. Developers of AI algorithms must be vigilant to potential dangers, including dataset shift, accidental fitting of confounders, unintended discriminatory bias, the challenges of generalisation to new populations, and the unintended negative consequences of new algorithms on health outcomes. CONCLUSION: The safe and timely translation of AI research into clinically validated and appropriately regulated systems that can benefit everyone is challenging. Robust clinical evaluation, using metrics that are intuitive to clinicians and ideally go beyond measures of technical accuracy to include quality of care and patient outcomes, is essential. Further work is required (1) to identify themes of algorithmic bias and unfairness while developing mitigations to address these, (2) to reduce brittleness and improve generalisability, and (3) to develop methods for improved interpretability of machine learning predictions. If these goals can be achieved, the benefits for patients are likely to be transformational.",
      "journal": "BMC medicine",
      "year": "2019",
      "doi": "10.1186/s12916-019-1426-2",
      "authors": "Kelly Christopher J et al.",
      "keywords": "Algorithms; Artificial intelligence; Evaluation; Machine learning; Regulation; Translation",
      "mesh_terms": "Algorithms; Artificial Intelligence; Delivery of Health Care; Humans; Peer Review",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31665002/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Public Health",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "CONCLUSION: The safe and timely translation of AI research into clinically validated and appropriately regulated systems that can benefit everyone is challenging. Robust clinical evaluation, using metrics that are intuitive to clinicians and ideally go beyond measures of technical accuracy to include quality of care and patient outcomes, is essential. Further work is required (1) to identify themes of algorithmic bias and unfairness while developing mitigations to address these, (2) to reduce br...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC6821018"
    },
    {
      "pmid": "31720554",
      "title": "Translational Health Disparities Research in a Data-Rich World.",
      "abstract": "Background: Despite decades of research and interventions, significant health disparities persist. Seventeen years is the estimated time to translate scientific discoveries into public health action. This Narrative Review argues that the translation process could be accelerated if representative data were gathered and used in more innovative and efficient ways. Methods: The National Institute on Minority Health and Health Disparities led a multiyear visioning process to identify research opportunities designed to frame the next decade of research and actions to improve minority health and reduce health disparities. \"Big data\" was identified as a research opportunity and experts collaborated on a systematic vision of how to use big data both to improve the granularity of information for place-based study and to efficiently translate health disparities research into improved population health. This Narrative Review is the result of that collaboration. Results: Big data could enhance the process of translating scientific findings into reduced health disparities by contributing information at fine spatial and temporal scales suited to interventions. In addition, big data could fill pressing needs for health care system, genomic, and social determinant data to understand mechanisms. Finally, big data could lead to appropriately personalized health care for demographic groups. Rich new resources, including social media, electronic health records, sensor information from digital devices, and crowd-sourced and citizen-collected data, have the potential to complement more traditional data from health surveys, administrative data, and investigator-initiated registries or cohorts. This Narrative Review argues for a renewed focus on translational research cycles to accomplish this continual assessment. Conclusion: The promise of big data extends from etiology research to the evaluation of large-scale interventions and offers the opportunity to accelerate translation of health disparities studies. This data-rich world for health disparities research, however, will require continual assessment for efficacy, ethical rigor, and potential algorithmic or system bias.",
      "journal": "Health equity",
      "year": "2019",
      "doi": "10.1089/heq.2019.0042",
      "authors": "Breen Nancy et al.",
      "keywords": "AI; NIMHD Methods Pillar; algorithmic bias; big data; interventions; translation",
      "mesh_terms": "",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31720554/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Narrative Review",
      "ai_ml_method": "Not specified",
      "health_domain": "EHR/Health Informatics; Genomics/Genetics; Public Health",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Diverse/Representative Data",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Conclusion: The promise of big data extends from etiology research to the evaluation of large-scale interventions and offers the opportunity to accelerate translation of health disparities studies. This data-rich world for health disparities research, however, will require continual assessment for efficacy, ethical rigor, and potential algorithmic or system bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC6844128"
    },
    {
      "pmid": "31788229",
      "title": "Artificial intelligence and algorithmic bias: implications for health systems.",
      "abstract": "",
      "journal": "Journal of global health",
      "year": "2019",
      "doi": "10.7189/jogh.09.020318",
      "authors": "Panch Trishan et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artificial Intelligence; Bias; Delivery of Health Care; Humans",
      "pub_types": "Journal Article; Review",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31788229/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "No abstract available",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC6875681"
    },
    {
      "pmid": "31840093",
      "title": "Eliminating biasing signals in lung cancer images for prognosis predictions with deep learning.",
      "abstract": "Deep learning has shown remarkable results for image analysis and is expected to aid individual treatment decisions in health care. Treatment recommendations are predictions with an inherently causal interpretation. To use deep learning for these applications in the setting of observational data, deep learning methods must be made compatible with the required causal assumptions. We present a scenario with real-world medical images (CT-scans of lung cancer) and simulated outcome data. Through the data simulation scheme, the images contain two distinct factors of variation that are associated with survival, but represent a collider (tumor size) and a prognostic factor (tumor heterogeneity), respectively. When a deep network would use all the information available in the image to predict survival, it would condition on the collider and thereby introduce bias in the estimation of the treatment effect. We show that when this collider can be quantified, unbiased individual prognosis predictions are attainable with deep learning. This is achieved by (1) setting a dual task for the network to predict both the outcome and the collider and (2) enforcing a form of linear independence of the activation distributions of the last layer. Our method provides an example of combining deep learning and structural causal models to achieve unbiased individual prognosis predictions. Extensions of machine learning methods for applications to causal questions are required to attain the long-standing goal of personalized medicine supported by artificial intelligence.",
      "journal": "NPJ digital medicine",
      "year": "2019",
      "doi": "10.1038/s41746-019-0194-x",
      "authors": "van Amsterdam W A C et al.",
      "keywords": "Computed tomography; Computer science; Epidemiology; Prognosis",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/31840093/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Deep Learning",
      "health_domain": "Oncology; Pulmonology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Our method provides an example of combining deep learning and structural causal models to achieve unbiased individual prognosis predictions. Extensions of machine learning methods for applications to causal questions are required to attain the long-standing goal of personalized medicine supported by artificial intelligence.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC6904461"
    },
    {
      "pmid": "33313606",
      "title": "Frequent Causal Pattern Mining: A Computationally Efficient Framework For Estimating Bias-Corrected Effects.",
      "abstract": "Our aging population increasingly suffers from multiple chronic diseases simultaneously, necessitating the comprehensive treatment of these conditions. Finding the optimal set of drugs for a combinatorial set of diseases is a combinatorial pattern exploration problem. Association rule mining is a popular tool for such problems, but the requirement of health care for finding causal, rather than associative, patterns renders association rule mining unsuitable. To address this issue, we propose a novel framework based on the Rubin-Neyman causal model for extracting causal rules from observational data, correcting for a number of common biases. Specifically, given a set of interventions and a set of items that define subpopulations (e.g., diseases), we wish to find all subpopulations in which effective intervention combinations exist and in each such subpopulation, we wish to find all intervention combinations such that dropping any intervention from this combination will reduce the efficacy of the treatment. A key aspect of our framework is the concept of closed intervention sets which extend the concept of quantifying the effect of a single intervention to a set of concurrent interventions. Closed intervention sets also allow for a pruning strategy that is strictly more efficient than the traditional pruning strategy used by the Apriori algorithm. To implement our ideas, we introduce and compare five methods of estimating causal effect from observational data and rigorously evaluate them on synthetic data to mathematically prove (when possible) why they work. We also evaluated our causal rule mining framework on the Electronic Health Records (EHR) data of a large cohort of 152000 patients from Mayo Clinic and showed that the patterns we extracted are sufficiently rich to explain the controversial findings in the medical literature regarding the effect of a class of cholesterol drugs on Type-II Diabetes Mellitus (T2DM).",
      "journal": "Proceedings : ... IEEE International Conference on Big Data. IEEE International Conference on Big Data",
      "year": "2019",
      "doi": "10.1109/bigdata47090.2019.9005977",
      "authors": "Yadav Pranjul et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/33313606/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "EHR/Health Informatics; Endocrinology/Diabetes",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Data Augmentation; Subgroup Analysis",
      "clinical_setting": "Public Health/Population",
      "key_findings": "To implement our ideas, we introduce and compare five methods of estimating causal effect from observational data and rigorously evaluate them on synthetic data to mathematically prove (when possible) why they work. We also evaluated our causal rule mining framework on the Electronic Health Records (EHR) data of a large cohort of 152000 patients from Mayo Clinic and showed that the patterns we extracted are sufficiently rich to explain the controversial findings in the medical literature regardi...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC7730315"
    },
    {
      "pmid": "28525542",
      "title": "Semiparametric model and inference for spontaneous abortion data with a cured proportion and biased sampling.",
      "abstract": "Evaluating and understanding the risk and safety of using medications for autoimmune disease in a woman during her pregnancy will help both clinicians and pregnant women to make better treatment decisions. However, utilizing spontaneous abortion (SAB) data collected in observational studies of pregnancy to derive valid inference poses two major challenges. First, the data from the observational cohort are not random samples of the target population due to the sampling mechanism. Pregnant women with early SAB are more likely to be excluded from the cohort, and there may be substantial differences between the observed SAB time and those in the target population. Second, the observed data are heterogeneous and contain a \"cured\" proportion. In this article, we consider semiparametric models to simultaneously estimate the probability of being cured and the distribution of time to SAB for the uncured subgroup. To derive the maximum likelihood estimators, we appropriately adjust the sampling bias in the likelihood function and develop an expectation-maximization algorithm to overcome the computational challenge. We apply the empirical process theory to prove the consistency and asymptotic normality of the estimators. We examine the finite sample performance of the proposed estimators in simulation studies and illustrate the proposed method through an application to SAB data from pregnant women.",
      "journal": "Biostatistics (Oxford, England)",
      "year": "2018",
      "doi": "10.1093/biostatistics/kxx024",
      "authors": "Piao Jin et al.",
      "keywords": "Biased sampling; Cure rate model; EM algorithm; Left truncation",
      "mesh_terms": "Abortion, Spontaneous; Adult; Algorithms; Female; Humans; Likelihood Functions; Models, Statistical; Pregnancy; Selection Bias",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/28525542/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Obstetrics/Maternal Health",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We apply the empirical process theory to prove the consistency and asymptotic normality of the estimators. We examine the finite sample performance of the proposed estimators in simulation studies and illustrate the proposed method through an application to SAB data from pregnant women.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC5862342"
    },
    {
      "pmid": "29281263",
      "title": "Correction for the Hematocrit Bias in Dried Blood Spot Analysis Using a Nondestructive, Single-Wavelength Reflectance-Based Hematocrit Prediction Method.",
      "abstract": "The hematocrit (Hct) effect is one of the most important hurdles currently preventing more widespread implementation of quantitative dried blood spot (DBS) analysis in a routine context. Indeed, the Hct may affect both the accuracy of DBS methods as well as the interpretation of DBS-based results. We previously developed a method to determine the Hct of a DBS based on its hemoglobin content using noncontact diffuse reflectance spectroscopy. Despite the ease with which the analysis can be performed (i.e., mere scanning of the DBS) and the good results that were obtained, the method did require a complicated algorithm to derive the total hemoglobin content from the DBS's reflectance spectrum. As the total hemoglobin was calculated as the sum of oxyhemoglobin, methemoglobin, and hemichrome, the three main hemoglobin derivatives formed in DBS upon aging, the reflectance spectrum needed to be unmixed to determine the quantity of each of these derivatives. We now simplified the method by only using the reflectance at a single wavelength, located at a quasi-isosbestic point in the reflectance curve. At this wavelength, assuming 1-to-1 stoichiometry of the aging reaction, the reflectance is insensitive to the hemoglobin degradation and only scales with the total amount of hemoglobin and, hence, the Hct. This simplified method was successfully validated. At each quality control level as well as at the limits of quantitation (i.e., 0.20 and 0.67) bias, intra- and interday imprecision were within 10%. Method reproducibility was excellent based on incurred sample reanalysis and surpassed the reproducibility of the original method. Furthermore, the influence of the volume spotted, the measurement location within the spot, as well as storage time and temperature were evaluated, showing no relevant impact of these parameters. Application to 233 patient samples revealed a good correlation between the Hct determined on whole blood and the predicted Hct determined on venous DBS. The bias obtained with Bland and Altman analysis was -0.015 and the limits of agreement were -0.061 and 0.031, indicating that the simplified, noncontact Hct prediction method even outperforms the original method. In addition, using caffeine as a model compound, it was demonstrated that this simplified Hct prediction method can effectively be used to implement a Hct-dependent correction factor to DBS-based results to alleviate the Hct bias.",
      "journal": "Analytical chemistry",
      "year": "2018",
      "doi": "10.1021/acs.analchem.7b03784",
      "authors": "Capiau Sara et al.",
      "keywords": "",
      "mesh_terms": "Adult; Algorithms; Dried Blood Spot Testing; Hematocrit; Humans; Linear Models; Reproducibility of Results; Spectrophotometry",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29281263/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age; Intersectional",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The bias obtained with Bland and Altman analysis was -0.015 and the limits of agreement were -0.061 and 0.031, indicating that the simplified, noncontact Hct prediction method even outperforms the original method. In addition, using caffeine as a model compound, it was demonstrated that this simplified Hct prediction method can effectively be used to implement a Hct-dependent correction factor to DBS-based results to alleviate the Hct bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "29289761",
      "title": "Technology-assisted risk of bias assessment in systematic reviews: a prospective cross-sectional evaluation of the RobotReviewer machine learning tool.",
      "abstract": "OBJECTIVES: To evaluate the reliability of RobotReviewer's risk of bias judgments. STUDY DESIGN AND SETTING: In this prospective cross-sectional evaluation, we used RobotReviewer to assess risk of bias among 1,180 trials. We computed reliability with human reviewers using Cohen's kappa coefficient and calculated sensitivity and specificity. We investigated differences in reliability by risk of bias domain, topic, and outcome type using the chi-square test in meta-analysis. RESULTS: Reliability (95% CI) was moderate for random sequence generation (0.48 [0.43, 0.53]), allocation concealment (0.45 [0.40, 0.51]), and blinding of participants and personnel (0.42 [0.36, 0.47]); fair for overall risk of bias (0.34 [0.25, 0.44]); and slight for blinding of outcome assessors (0.10 [0.06, 0.14]), incomplete outcome data (0.14 [0.08, 0.19]), and selective reporting (0.02 [-0.02, 0.05]). Reliability for blinding of participants and personnel (P\u00a0<\u00a00.001), blinding of outcome assessors (P\u00a0=\u00a00.005), selective reporting (P\u00a0<\u00a00.001), and overall risk of bias (P\u00a0<\u00a00.001) differed by topic. Sensitivity and specificity (95% CI) ranged from 0.20 (0.18, 0.23) to 0.76 (0.72, 0.80) and from 0.61 (0.56, 0.65) to 0.95 (0.93, 0.96), respectively. CONCLUSION: Risk of bias appraisal is subjective. Compared with reliability between author groups, RobotReviewer's reliability with human reviewers was similar for most domains and better for allocation concealment, blinding of participants and personnel, and overall risk of bias.",
      "journal": "Journal of clinical epidemiology",
      "year": "2018",
      "doi": "10.1016/j.jclinepi.2017.12.015",
      "authors": "Gates Allison et al.",
      "keywords": "Automation; Bias; Clinical trials; Evaluation; Evidence-based medicine; Machine learning",
      "mesh_terms": "Bias; Cross-Sectional Studies; Humans; Machine Learning; Prospective Studies; Reproducibility of Results; Systematic Reviews as Topic; Technology",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29289761/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Systematic Review",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Risk of bias appraisal is subjective. Compared with reliability between author groups, RobotReviewer's reliability with human reviewers was similar for most domains and better for allocation concealment, blinding of participants and personnel, and overall risk of bias.",
      "ft_include": false,
      "ft_reason": "Not health-related in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "29677939",
      "title": "Estimating a Bias in ICD Encodings for Billing Purposes.",
      "abstract": "ICD encoded diagnoses are a popular criterion for eligibility algorithms for study cohort recruitment. However, \"official\" ICD encoded diagnoses used for billing purposes are afflicted with a bias originating from legal issues. This work presents an approach to estimate the degree of the encoding bias for the complete ICD catalogue at a German university hospital. The free text diagnoses sections of discharge letters are automatically classified using a supervised machine learning algorithm. The automatic classifications are compared with the official, manually classified codes. For selected ICD codes the approach works sufficiently well.",
      "journal": "Studies in health technology and informatics",
      "year": "2018",
      "doi": "",
      "authors": "Fette Georg et al.",
      "keywords": "ICD; classification; machine learning; natural language processing",
      "mesh_terms": "Algorithms; Bias; Humans; International Classification of Diseases; Patient Discharge; Supervised Machine Learning",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29677939/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Commentary/Editorial",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Not specified",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "The automatic classifications are compared with the official, manually classified codes. For selected ICD codes the approach works sufficiently well.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "30195425",
      "title": "Attentional bias in MDD: ERP components analysis and classification using a dot-probe task.",
      "abstract": "BACKGROUND AND OBJECTIVE: Strands of evidence have supported existence of negative attentional bias in patients with depression. This study aimed to assess the behavioral and electrophysiological signatures of attentional bias in major depressive disorder (MDD) and explore whether ERP components contain valuable information for discriminating between MDD patients and healthy controls (HCs). METHODS: Electroencephalography data were collected from 17 patients with MDD and 17 HCs in a dot-probe task, with emotional-neutral pairs as experimental materials. Fourteen features related to ERP waveform shape were generated. Then, Correlated Feature Selection (CFS), ReliefF and GainRatio (GR) were applied for feature selection. For discriminating between MDDs and HCs, k-nearest neighbor (KNN), C4.5, Sequential Minimal Optimization (SMO) and Logistic Regression (LR) were used. RESULTS: Behaviorally, MDD patients showed significantly shorter reaction time (RT) to valid than invalid sad trials, with significantly higher bias score for sad-neutral pairs. Analysis of split-half reliability in RT indices indicated a strong reliability in RT, while coefficients of RT bias scores neared zero. These behavioral effects were supported by ERP results. MDD patients had higher P300 amplitude with the probe replacing a sad face than a neutral face, indicating difficult attention disengagement from negative emotional faces. Meanwhile, data mining analysis based on ERP components suggested that CFS was the best feature selection algorithm. Especially for the P300 induced by valid sad trials, the classification accuracy of CFS combination with any classifier was above 85%, and the KNN (k\u202f=\u202f3) classifier achieved the highest accuracy (94%). CONCLUSIONS: MDD patients show difficulty in attention disengagement from negative stimuli, reflected by P300. The CFS over other methods leads to a good overall performance in most cases, especially when KNN classifier is used for P300 component classification, illustrating that ERP component may be applied as a tool for auxiliary diagnosis of depression.",
      "journal": "Computer methods and programs in biomedicine",
      "year": "2018",
      "doi": "10.1016/j.cmpb.2018.07.003",
      "authors": "Li Xiaowei et al.",
      "keywords": "Attentional bias; Classification; Event-related potentials; Feature selection; Major depressive disorder",
      "mesh_terms": "Adult; Algorithms; Attentional Bias; Case-Control Studies; Major Depressive Disorder; Diagnosis, Computer-Assisted; Electroencephalography; Event-Related Potentials, P300; Evoked Potentials; Facial Expression; Female; Humans; Male; Photic Stimulation; Reaction Time; Young Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30195425/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Logistic Regression",
      "health_domain": "Mental Health/Psychiatry; ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSIONS: MDD patients show difficulty in attention disengagement from negative stimuli, reflected by P300. The CFS over other methods leads to a good overall performance in most cases, especially when KNN classifier is used for P300 component classification, illustrating that ERP component may be applied as a tool for auxiliary diagnosis of depression.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "30362919",
      "title": "Evaluating the Presence of Cognitive Biases in Health Care Decision Making: A Survey of U.S. Formulary Decision Makers.",
      "abstract": "BACKGROUND: Behavioral economics is a field of economics that draws on insights from psychology to understand and identify patterns of decision making. Cognitive biases are psychological tendencies to process information in predictable patterns that result in deviations from rational decision making. Previous research has not evaluated the influence of cognitive biases on decision making in a managed care setting. OBJECTIVE: To assess the presence of cognitive biases in formulary decision making. METHODS: An online survey was conducted with a panel of U.S. pharmacy and medical directors who worked at managed care organizations and served on pharmacy and therapeutics committees. Survey questions assessed 4 cognitive biases: relative versus absolute framing effect, risk aversion, zero-risk bias, and delay discounting. Simulated data were presented in various scenarios related to adverse event profiles, drug safety and efficacy, and drug pricing for new hypothetical oncology products. Survey questions prompted participants to select a preferred drug based on the information provided. Survey answers were analyzed to identify decision patterns that could be explained by the cognitive biases. Likelihood of bias was analyzed via chi-square tests for framing effect, risk aversion, and zero-risk bias. The delay discounting section used a published algorithm to characterize discounting patterns. RESULTS: A total of 35 pharmacy directors and 19 medical directors completed the survey. In the framing effect section, 80% of participants selected the suboptimal choice in the relative risk frame, compared with 38.9% in the absolute risk frame (P < 0.0001). When assessing risk aversion, 42.6% and 61.1% of participants displayed risk aversion in the cost- and efficacy-based scenarios, respectively, but these were not statistically significant (P = 0.27 and P = 0.10, respectively). In the zero-risk bias section, results from each scenario diverged. In the first zero-risk bias scenario, 90.7% of participants selected the drug with zero risk (P < 0.001), but in the second scenario, only 32.1% chose the zero-risk option (P < 0.01). In the section assessing delay discounting, 54% of survey participants favored a larger delayed rebate over a smaller immediate discount. A shallow delay discounting curve was produced, which indicated participants discounted delayed rewards to a minimal degree. CONCLUSIONS: Pharmacy and medical directors, like other decision makers, appear to be susceptible to some cognitive biases. Directors demonstrated a tendency to underestimate risks when they were presented in relative risk terms but made more accurate appraisals when information was presented in absolute risk terms. Delay discounting also may be applicable to directors when choosing immediate discounts over delayed rebates. However, directors neither displayed a statistically significant bias for risk aversion when assessing scenarios related to drug pricing or clinical efficacy nor were there significant conclusions for zero-risk biases. Further research with larger samples using real-world health care decisions is necessary to validate these findings. DISCLOSURES: This research was funded by Xcenda. Mezzio, Nguyen, and O'Day are employees of Xcenda. Kiselica was employed by Xcenda at the time the study was conducted. The authors have nothing to disclose. A portion of the preliminary data was presented as posters at the 2017 AMCP Managed Care & Specialty Pharmacy Annual Meeting; March 27-30, 2017; in Denver, CO, and the 2017 International Society for Pharmacoeconomics and Outcomes Research 22nd Annual International Meeting; May 20-24, 2017; in Boston, MA.",
      "journal": "Journal of managed care & specialty pharmacy",
      "year": "2018",
      "doi": "10.18553/jmcp.2018.24.11.1173",
      "authors": "Mezzio Dylan J et al.",
      "keywords": "",
      "mesh_terms": "Cognition; Decision Making; Economics, Pharmaceutical; Humans; Likelihood Functions; Managed Care Programs; Outcome Assessment, Health Care; Pharmacy; Physician Executives; Prejudice; Risk Assessment; Surveys and Questionnaires",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/30362919/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Generative AI",
      "health_domain": "Oncology; Drug Discovery/Pharmacology",
      "bias_axes": "Age",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Pharmacy and medical directors, like other decision makers, appear to be susceptible to some cognitive biases. Directors demonstrated a tendency to underestimate risks when they were presented in relative risk terms but made more accurate appraisals when information was presented in absolute risk terms. Delay discounting also may be applicable to directors when choosing immediate discounts over delayed rebates.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC10397589"
    },
    {
      "pmid": "25911333",
      "title": "A permutation test to analyse systematic bias and random measurement errors of medical devices via boosting location and scale models.",
      "abstract": "Measurement errors of medico-technical devices can be separated into systematic bias and random error. We propose a new method to address both simultaneously via generalized additive models for location, scale and shape (GAMLSS) in combination with permutation tests. More precisely, we extend a recently proposed boosting algorithm for GAMLSS to provide a test procedure to analyse potential device effects on the measurements. We carried out a large-scale simulation study to provide empirical evidence that our method is able to identify possible sources of systematic bias as well as random error under different conditions. Finally, we apply our approach to compare measurements of skin pigmentation from two different devices in an epidemiological study.",
      "journal": "Statistical methods in medical research",
      "year": "2017",
      "doi": "10.1177/0962280215581855",
      "authors": "Mayr Andreas et al.",
      "keywords": "Measurement errors; gradient boosting; permutation test; random error; regression; statistical models; systematic bias",
      "mesh_terms": "Algorithms; Bias; Colorimetry; Equipment Design; Equipment and Supplies; Female; Humans; Male; Models, Statistical; Regression Analysis; Research Design; Skin Pigmentation",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/25911333/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Not specified",
      "key_findings": "We carried out a large-scale simulation study to provide empirical evidence that our method is able to identify possible sources of systematic bias as well as random error under different conditions. Finally, we apply our approach to compare measurements of skin pigmentation from two different devices in an epidemiological study.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "27943018",
      "title": "Item bias detection in the Hospital Anxiety and Depression Scale using structural equation modeling: comparison with other item bias detection methods.",
      "abstract": "PURPOSE: Comparison of patient-reported outcomes may be invalidated by the occurrence of item bias, also known as differential item functioning. We show two ways of using structural equation modeling (SEM) to detect item bias: (1) multigroup SEM, which enables the detection of both uniform and nonuniform bias, and (2) multidimensional SEM, which enables the investigation of item bias with respect to several variables simultaneously. METHOD: Gender- and age-related bias in the items of the Hospital Anxiety and Depression Scale (HADS; Zigmond and Snaith in Acta Psychiatr Scand 67:361-370, 1983) from a sample of 1068 patients was investigated using the multigroup SEM approach and the multidimensional SEM approach. Results were compared to the results of the ordinal logistic regression, item response theory, and contingency tables methods reported by Cameron et al. (Qual Life Res 23:2883-2888, 2014). RESULTS: Both SEM approaches identified two items with gender-related bias and two items with age-related bias in the Anxiety subscale, and four items with age-related bias in the Depression subscale. Results from the SEM approaches generally agreed with the results of Cameron et al., although the SEM approaches identified more items as biased. CONCLUSION: SEM provides a flexible tool for the investigation of item bias in health-related questionnaires. Multidimensional SEM has practical and statistical advantages over multigroup SEM, and over other item bias detection methods, as it enables item bias detection with respect to multiple variables, of various measurement levels, and with more statistical power, ultimately providing more valid comparisons of patients' well-being in both research and clinical practice.",
      "journal": "Quality of life research : an international journal of quality of life aspects of treatment, care and rehabilitation",
      "year": "2017",
      "doi": "10.1007/s11136-016-1469-1",
      "authors": "Verdam Mathilde G E et al.",
      "keywords": "Depression Scale; Differential item functioning; Hospital Anxiety; Item bias; Structural equation modeling",
      "mesh_terms": "Adult; Age Factors; Aged; Anxiety; Bias; Depression; Female; Humans; Logistic Models; Male; Middle Aged; Patient Reported Outcome Measures; Quality of Life; Sex Factors; Surveys and Questionnaires",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/27943018/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Logistic Regression",
      "health_domain": "Mental Health/Psychiatry",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "CONCLUSION: SEM provides a flexible tool for the investigation of item bias in health-related questionnaires. Multidimensional SEM has practical and statistical advantages over multigroup SEM, and over other item bias detection methods, as it enables item bias detection with respect to multiple variables, of various measurement levels, and with more statistical power, ultimately providing more valid comparisons of patients' well-being in both research and clinical practice.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC5420371"
    },
    {
      "pmid": "29104450",
      "title": "Optimizing Variance-Bias Trade-off in the TWANG Package for Estimation of Propensity Scores.",
      "abstract": "While propensity score weighting has been shown to reduce bias in treatment effect estimation when selection bias is present, it has also been shown that such weighting can perform poorly if the estimated propensity score weights are highly variable. Various approaches have been proposed which can reduce the variability of the weights and the risk of poor performance, particularly those based on machine learning methods. In this study, we closely examine approaches to fine-tune one machine learning technique (generalized boosted models [GBM]) to select propensity scores that seek to optimize the variance-bias trade-off that is inherent in most propensity score analyses. Specifically, we propose and evaluate three approaches for selecting the optimal number of trees for the GBM in the twang package in R. Normally, the twang package in R iteratively selects the optimal number of trees as that which maximizes balance between the treatment groups being considered. Because the selected number of trees may lead to highly variable propensity score weights, we examine alternative ways to tune the number of trees used in the estimation of propensity score weights such that we sacrifice some balance on the pre-treatment covariates in exchange for less variable weights. We use simulation studies to illustrate these methods and to describe the potential advantages and disadvantages of each method. We apply these methods to two case studies: one examining the effect of dog ownership on the owner's general health using data from a large, population-based survey in California, and a second investigating the relationship between abstinence and a long-term economic outcome among a sample of high-risk youth.",
      "journal": "Health services & outcomes research methodology",
      "year": "2017",
      "doi": "10.1007/s10742-016-0168-2",
      "authors": "Parast Layla et al.",
      "keywords": "causal inference; machine learning; propensity score",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/29104450/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Survey/Qualitative",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Transfer Learning",
      "clinical_setting": "ICU; Public Health/Population",
      "key_findings": "We use simulation studies to illustrate these methods and to describe the potential advantages and disadvantages of each method. We apply these methods to two case studies: one examining the effect of dog ownership on the owner's general health using data from a large, population-based survey in California, and a second investigating the relationship between abstinence and a long-term economic outcome among a sample of high-risk youth.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC5667923"
    },
    {
      "pmid": "26256455",
      "title": "A joint latent class analysis for adjusting survival bias with application to a trauma transfusion study.",
      "abstract": "There is no clear classification rule to rapidly identify trauma patients who are severely hemorrhaging and may need substantial blood transfusions. Massive transfusion (MT), defined as the transfusion of at least 10 units of red blood cells within 24\u00a0h of hospital admission, has served as a conventional surrogate that has been used to develop early predictive algorithms and establish criteria for ordering an MT protocol from the blood bank. However, the conventional MT rule is a poor proxy, because it is likely to misclassify many severely hemorrhaging trauma patients as they could die before receiving the 10th red blood cells transfusion. In this article, we propose to use a latent class model to obtain a more accurate and complete metric in the presence of early death. Our new approach incorporates baseline patient information from the time of hospital admission, by combining respective models for survival time and usage of blood products transfused within the framework of latent class analysis. To account for statistical challenges, caused by induced dependent censoring inherent in 24-h sums of transfusions, we propose to estimate an improved standard via a pseudo-likelihood function using an expectation-maximization algorithm with the inverse weighting principle. We evaluated the performance of our new standard in simulation studies and compared with the conventional MT definition using actual patient data from the Prospective Observational Multicenter Major Trauma Transfusion study. Copyright \u00a9 2015 John Wiley & Sons, Ltd.",
      "journal": "Statistics in medicine",
      "year": "2016",
      "doi": "10.1002/sim.6615",
      "authors": "Ning Jing et al.",
      "keywords": "EM algorithm; induced dependent censoring; inverse weighting principle; latent class model; massive transfusion",
      "mesh_terms": "Algorithms; Bias; Biostatistics; Blood Transfusion; Computer Simulation; Hemorrhage; Humans; Kaplan-Meier Estimate; Likelihood Functions; Logistic Models; Survival Analysis; Wounds and Injuries",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/26256455/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Emergency Medicine",
      "bias_axes": "Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "We evaluated the performance of our new standard in simulation studies and compared with the conventional MT definition using actual patient data from the Prospective Observational Multicenter Major Trauma Transfusion study. Copyright \u00a9 2015 John Wiley & Sons, Ltd.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC4715697"
    },
    {
      "pmid": "26639183",
      "title": "Modeling X Chromosome Data Using Random Forests: Conquering Sex Bias.",
      "abstract": "Machine learning methods, including Random Forests (RF), are increasingly used for genetic data analysis. However, the standard RF algorithm does not correctly model the effects of X chromosome single nucleotide polymorphisms (SNPs), leading to biased estimates of variable importance. We propose extensions of RF to correctly model X SNPs, including a stratified approach and an approach based on the process of X chromosome inactivation. We applied the new and standard RF approaches to case-control alcohol dependence data from the Study of Addiction: Genes and Environment (SAGE), and compared the performance of the alternative approaches via a simulation study. Standard RF applied to a case-control study of alcohol dependence yielded inflated variable importance estimates for X SNPs, even when sex was included as a variable, but the results of the new RF methods were consistent with univariate regression-based approaches that correctly model X chromosome data. Simulations showed that the new RF methods eliminate the bias in standard RF variable importance for X SNPs when sex is associated with the trait, and are able to detect causal autosomal and X SNPs. Even in the absence of sex effects, the new extensions perform similarly to standard RF. Thus, we provide a powerful multimarker approach for genetic analysis that accommodates X chromosome data in an unbiased way. This method is implemented in the freely available R package \"snpRF\" (http://www.cran.r-project.org/web/packages/snpRF/).",
      "journal": "Genetic epidemiology",
      "year": "2016",
      "doi": "10.1002/gepi.21946",
      "authors": "Winham Stacey J et al.",
      "keywords": "Random Forest; X chromosome; bias; sex differences; variable importance",
      "mesh_terms": "Alcoholism; Algorithms; Bias; Case-Control Studies; Chromosomes, Human, X; Computer Simulation; Data Interpretation, Statistical; Decision Trees; Genetic Markers; Genetic Predisposition to Disease; Humans; Models, Genetic; Phenotype; Polymorphism, Single Nucleotide; Sex Factors",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/26639183/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Random Forest",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Thus, we provide a powerful multimarker approach for genetic analysis that accommodates X chromosome data in an unbiased way. This method is implemented in the freely available R package \"snpRF\" (http://www.cran.r-project.org/web/packages/snpRF/).",
      "ft_include": false,
      "ft_reason": "Not health-related in full text",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC4724236"
    },
    {
      "pmid": "26865273",
      "title": "Analysis of a Rapid Evolutionary Radiation Using Ultraconserved Elements: Evidence for a Bias in Some Multispecies Coalescent Methods.",
      "abstract": "Rapid evolutionary radiations are expected to require large amounts of sequence data to resolve. To resolve these types of relationships many systematists believe that it will be necessary to collect data by next-generation sequencing (NGS) and use multispecies coalescent (\"species tree\") methods. Ultraconserved element (UCE) sequence capture is becoming a popular method to leverage the high throughput of NGS to address problems in vertebrate phylogenetics. Here we examine the performance of UCE data for gallopheasants (true pheasants and allies), a clade that underwent a rapid radiation 10-15 Ma. Relationships among gallopheasant genera have been difficult to establish. We used this rapid radiation to assess the performance of species tree methods, using \u223c600 kilobases of DNA sequence data from \u223c1500 UCEs. We also integrated information from traditional markers (nuclear intron data from 15 loci and three mitochondrial gene regions). Species tree methods exhibited troubling behavior. Two methods [Maximum Pseudolikelihood for Estimating Species Trees (MP-EST) and Accurate Species TRee ALgorithm (ASTRAL)] appeared to perform optimally when the set of input gene trees was limited to the most variable UCEs, though ASTRAL appeared to be more robust than MP-EST to input trees generated using less variable UCEs. In contrast, the rooted triplet consensus method implemented in Triplec performed better when the largest set of input gene trees was used. We also found that all three species tree methods exhibited a surprising degree of dependence on the program used to estimate input gene trees, suggesting that the details of likelihood calculations (e.g., numerical optimization) are important for loci with limited phylogenetic information. As an alternative to summary species tree methods we explored the performance of SuperMatrix Rooted Triple - Maximum Likelihood (SMRT-ML), a concatenation method that is consistent even when gene trees exhibit topological differences due to the multispecies coalescent. We found that SMRT-ML performed well for UCE data. Our results suggest that UCE data have excellent prospects for the resolution of difficult evolutionary radiations, though specific attention may need to be given to the details of the methods used to estimate species trees.",
      "journal": "Systematic biology",
      "year": "2016",
      "doi": "10.1093/sysbio/syw014",
      "authors": "Meiklejohn Kelly A et al.",
      "keywords": "Galliformes, gene tree estimation error; Phasianidae; Triplec; polytomy; supermatrix rooted triplets; total evidence",
      "mesh_terms": "Biological Evolution; Classification; High-Throughput Nucleotide Sequencing; Models, Biological; Phylogeny; Probability",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/26865273/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "ICU",
      "key_findings": "We found that SMRT-ML performed well for UCE data. Our results suggest that UCE data have excellent prospects for the resolution of difficult evolutionary radiations, though specific attention may need to be given to the details of the methods used to estimate species trees.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "27411847",
      "title": "Estimating causal contrasts involving intermediate variables in the presence of selection bias.",
      "abstract": "An important goal across the biomedical and social sciences is the quantification of the role of intermediate factors in explaining how an exposure exerts an effect on an outcome. Selection bias has the potential to severely undermine the validity of inferences on direct and indirect causal effects in observational as well as in randomized studies. The phenomenon of selection may arise through several mechanisms, and we here focus on instances of missing data. We study the sign and magnitude of selection bias in the estimates of direct and indirect effects when data on any of the factors involved in the analysis is either missing at random or not missing at random. Under some simplifying assumptions, the bias formulae can lead to nonparametric sensitivity analyses. These sensitivity analyses can be applied to causal effects on the risk difference and risk-ratio scales irrespectively of the estimation approach employed. To incorporate parametric assumptions, we also develop a sensitivity analysis for selection bias in mediation analysis in the spirit of the expectation-maximization algorithm. The approaches are applied to data from a health disparities study investigating the role of stage at diagnosis on racial disparities in colorectal cancer survival. Copyright \u00a9 2016 John Wiley & Sons, Ltd.",
      "journal": "Statistics in medicine",
      "year": "2016",
      "doi": "10.1002/sim.7025",
      "authors": "Valeri Linda et al.",
      "keywords": "EM algorithm; controlled direct effects; mediation analysis; missing at random; natural direct and indirect effects; not missing at random; selection bias; sensitivity analyses",
      "mesh_terms": "Bias; Humans; Randomized Controlled Trials as Topic; Risk; Selection Bias",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/27411847/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The approaches are applied to data from a health disparities study investigating the role of stage at diagnosis on racial disparities in colorectal cancer survival. Copyright \u00a9 2016 John Wiley & Sons, Ltd.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "25609791",
      "title": "Bias in microRNA functional enrichment analysis.",
      "abstract": "MOTIVATION: Many studies have investigated the differential expression of microRNAs (miRNAs) in disease states and between different treatments, tissues and developmental stages. Given a list of perturbed miRNAs, it is common to predict the shared pathways on which they act. The standard test for functional enrichment typically yields dozens of significantly enriched functional categories, many of which appear frequently in the analysis of apparently unrelated diseases and conditions. RESULTS: We show that the most commonly used functional enrichment test is inappropriate for the analysis of sets of genes targeted by miRNAs. The hypergeometric distribution used by the standard method consistently results in significant P-values for functional enrichment for targets of randomly selected miRNAs, reflecting an underlying bias in the predicted gene targets of miRNAs as a whole. We developed an algorithm to measure enrichment using an empirical sampling approach, and applied this in a reanalysis of the gene ontology classes of targets of miRNA lists from 44 published studies. The vast majority of the miRNA target sets were not significantly enriched in any functional category after correction for bias. We therefore argue against continued use of the standard functional enrichment method for miRNA targets.",
      "journal": "Bioinformatics (Oxford, England)",
      "year": "2015",
      "doi": "10.1093/bioinformatics/btv023",
      "authors": "Bleazard Thomas et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Computational Biology; Gene Expression Regulation; Humans; MicroRNAs; Molecular Sequence Annotation; Sequence Analysis, RNA",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/25609791/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: We show that the most commonly used functional enrichment test is inappropriate for the analysis of sets of genes targeted by miRNAs. The hypergeometric distribution used by the standard method consistently results in significant P-values for functional enrichment for targets of randomly selected miRNAs, reflecting an underlying bias in the predicted gene targets of miRNAs as a whole. We developed an algorithm to measure enrichment using an empirical sampling approach, and applied this ...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC4426843"
    },
    {
      "pmid": "26232237",
      "title": "Diagnostic biases in translational bioinformatics.",
      "abstract": "BACKGROUND: With the surge of translational medicine and computational omics research, complex disease diagnosis is more and more relying on massive omics data-driven molecular signature detection. However, how to detect and prevent possible diagnostic biases in translational bioinformatics remains an unsolved problem despite its importance in the coming era of personalized medicine. METHODS: In this study, we comprehensively investigate the diagnostic bias problem by analyzing benchmark gene array, protein array, RNA-Seq and miRNA-Seq data under the framework of support vector machines for different model selection methods. We further categorize the diagnostic biases into different types by conducting rigorous kernel matrix analysis and provide effective machine learning methods to conquer the diagnostic biases. RESULTS: In this study, we comprehensively investigate the diagnostic bias problem by analyzing benchmark gene array, protein array, RNA-Seq and miRNA-Seq data under the framework of support vector machines. We have found that the diagnostic biases happen for data with different distributions and SVM with different kernels. Moreover, we identify total three types of diagnostic biases: overfitting bias, label skewness bias, and underfitting bias in SVM diagnostics, and present corresponding reasons through rigorous analysis. Compared with the overfitting and underfitting biases, the label skewness bias is more challenging to detect and conquer because it can be easily confused as a normal diagnostic case from its deceptive accuracy. To tackle this problem, we propose a derivative component analysis based support vector machines to conquer the label skewness bias by achieving the rivaling clinical diagnostic results. CONCLUSIONS: Our studies demonstrate that the diagnostic biases are mainly caused by the three major factors, i.e. kernel selection, signal amplification mechanism in high-throughput profiling, and training data label distribution. Moreover, the proposed DCA-SVM diagnosis provides a generic solution for the label skewness bias overcome due to the powerful feature extraction capability from derivative component analysis. Our work identifies and solves an important but less addressed problem in translational research. It also has a positive impact on machine learning for adding new results to kernel-based learning for omics data.",
      "journal": "BMC medical genomics",
      "year": "2015",
      "doi": "10.1186/s12920-015-0116-y",
      "authors": "Han Henry",
      "keywords": "",
      "mesh_terms": "Algorithms; Bias; Brain Neoplasms; Computational Biology; Diagnosis; Glioma; Humans; MicroRNAs; Monte Carlo Method; Neoplasm Grading; Oligonucleotide Array Sequence Analysis; Phenotype; Protein Array Analysis; Sequence Analysis, RNA; Support Vector Machine; Translational Research, Biomedical",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/26232237/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Support Vector Machine",
      "health_domain": "Oncology; Neurology",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Our studies demonstrate that the diagnostic biases are mainly caused by the three major factors, i.e. kernel selection, signal amplification mechanism in high-throughput profiling, and training data label distribution. Moreover, the proposed DCA-SVM diagnosis provides a generic solution for the label skewness bias overcome due to the powerful feature extraction capability from derivative component analysis.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC4522082"
    },
    {
      "pmid": "27092241",
      "title": "ve-SEQ: Robust, unbiased enrichment for streamlined detection and whole-genome sequencing of HCV and other highly diverse pathogens.",
      "abstract": "The routine availability of high-depth virus sequence data would allow the sensitive detection of resistance-associated variants that can jeopardize HIV or hepatitis C virus (HCV) treatment. We introduce ve-SEQ, a high-throughput method for sequence-specific enrichment and characterization of whole-virus genomes at up to 20% divergence from a reference sequence and 1,000-fold greater sensitivity than direct sequencing. The extreme genetic diversity of HCV led us to implement an algorithm for the efficient design of panels of oligonucleotide probes to capture any sequence among a defined set of targets without detectable bias. ve-SEQ enables efficient detection and sequencing of any HCV genome, including mixtures and intra-host variants, in a single experiment, with greater tolerance of sequence diversity than standard amplification methods and greater sensitivity than metagenomic sequencing, features that are directly applicable to other pathogens or arbitrary groups of target organisms, allowing the combination of sensitive detection with sequencing in many settings.",
      "journal": "F1000Research",
      "year": "2015",
      "doi": "10.12688/f1000research.7111.1",
      "authors": "Bonsall David et al.",
      "keywords": "Anti-viral resistance; Hepatitis C virus; Sequence capture and enrichment; Virus genome sequencing",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/27092241/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Generative AI",
      "health_domain": "Genomics/Genetics; Infectious Disease",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The extreme genetic diversity of HCV led us to implement an algorithm for the efficient design of panels of oligonucleotide probes to capture any sequence among a defined set of targets without detectable bias. ve-SEQ enables efficient detection and sequencing of any HCV genome, including mixtures and intra-host variants, in a single experiment, with greater tolerance of sequence diversity than standard amplification methods and greater sensitivity than metagenomic sequencing, features that are ...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC4821293"
    },
    {
      "pmid": "23990287",
      "title": "Evaluation of point-of-care analyzers' ability to reduce bias in conductivity-based hematocrit measurement during cardiopulmonary bypass.",
      "abstract": "Most point-of-care testing analyzers use the conductivity method to measure hematocrit (hct). During open-heart surgery, blood-conductivity is influenced by shifts in electrolyte and colloid concentrations caused by infusion media used, and this may lead to considerable bias in the hct measurement. We evaluated to what extent different analyzers correcting for 0, 1, 2, or 3 factors, respectively, compensated for this electrolyte/colloid interference: (1) the conductivity method with no correction (IRMA), (2) with a [Na(+)]-correction (GEM Premier 3000), (3) with a [Na(+)]/[K(+)]-correction (i-STAT), and (4) with a [Na(+)]/[K(+)]-correction in combination with an algorithm that estimates the protein dilution [i-STAT in cardiopulmonary bypass (CPB)-mode]. Bias in hct was measured during three consecutive stages of a CPB procedure: (I) before CPB, (II) start of CPB and (III) after cardioplegia. In order of high to low electrolyte/colloid interference: the analyzer with no correction, [Na(+)]-correction, [Na(+)/]/[K(+)]-correction, and [Na(+)/]/[K(+)]/estimated protein-correction showed a change of bias from stage I to stage III of -3.9 \u00b1 0.5, -3.4 \u00b1 0.4, -2.1 \u00b1 0.5, -0.3 \u00b1 0.5%. We conclude that correcting for more parameters (Na(+), K(+), estimated protein) gives less bias, but residual bias remains even after [Na(+)/]/[K(+)]/estimated protein-correction. This suggests that a satisfactory algorithm should also correct for other colloidal factors than protein.",
      "journal": "Journal of clinical monitoring and computing",
      "year": "2014",
      "doi": "10.1007/s10877-013-9504-z",
      "authors": "Teerenstra Steven et al.",
      "keywords": "",
      "mesh_terms": "Artifacts; Cardiopulmonary Bypass; Conductometry; Equipment Design; Equipment Failure Analysis; Hematocrit; Humans; Monitoring, Intraoperative; Point-of-Care Systems; Reproducibility of Results; Sensitivity and Specificity",
      "pub_types": "Comparative Study; Evaluation Study; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/23990287/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; Surgery; Pulmonology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We conclude that correcting for more parameters (Na(+), K(+), estimated protein) gives less bias, but residual bias remains even after [Na(+)/]/[K(+)]/estimated protein-correction. This suggests that a satisfactory algorithm should also correct for other colloidal factors than protein.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "24361666",
      "title": "Cortical surface-based analysis reduces bias and variance in kinetic modeling of brain PET data.",
      "abstract": "Exploratory (i.e., voxelwise) spatial methods are commonly used in neuroimaging to identify areas that show an effect when a region-of-interest (ROI) analysis cannot be performed because no strong a priori anatomical hypothesis exists. However, noise at a single voxel is much higher than noise in a ROI making noise management critical to successful exploratory analysis. This work explores how preprocessing choices affect the bias and variability of voxelwise kinetic modeling analysis of brain positron emission tomography (PET) data. These choices include the use of volume- or cortical surface-based smoothing, level of smoothing, use of voxelwise partial volume correction (PVC), and PVC masking threshold. PVC was implemented using the Muller-Gartner method with the masking out of voxels with low gray matter (GM) partial volume fraction. Dynamic PET scans of an antagonist serotonin-4 receptor radioligand ([(11)C]SB207145) were collected on sixteen healthy subjects using a Siemens HRRT PET scanner. Kinetic modeling was used to compute maps of non-displaceable binding potential (BPND) after preprocessing. The results showed a complicated interaction between smoothing, PVC, and masking on BPND estimates. Volume-based smoothing resulted in large bias and intersubject variance because it smears signal across tissue types. In some cases, PVC with volume smoothing paradoxically caused the estimated BPND to be less than when no PVC was used at all. When applied in the absence of PVC, cortical surface-based smoothing resulted in dramatically less bias and the least variance of the methods tested for smoothing levels 5mm and higher. When used in combination with PVC, surface-based smoothing minimized the bias without significantly increasing the variance. Surface-based smoothing resulted in 2-4 times less intersubject variance than when volume smoothing was used. This translates into more than 4 times fewer subjects needed in a group analysis to achieve similarly powered statistical tests. Surface-based smoothing has less bias and variance because it respects cortical geometry by smoothing the PET data only along the cortical ribbon and so does not contaminate the GM signal with that of white matter and cerebrospinal fluid. The use of surface-based analysis in PET should result in substantial improvements in the reliability and detectability of effects in exploratory PET analysis, with or without PVC.",
      "journal": "NeuroImage",
      "year": "2014",
      "doi": "10.1016/j.neuroimage.2013.12.021",
      "authors": "Greve Douglas N et al.",
      "keywords": "",
      "mesh_terms": "Adult; Algorithms; Artifacts; Cerebral Cortex; Computer Simulation; Female; Humans; Image Enhancement; Kinetics; Male; Metabolic Clearance Rate; Models, Biological; Piperidines; Positron-Emission Tomography; Radiopharmaceuticals; Receptors, Serotonin, 5-HT4; Reproducibility of Results; Sensitivity and Specificity; Tissue Distribution; Young Adult",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/24361666/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Generative AI",
      "health_domain": "Neurology; Drug Discovery/Pharmacology",
      "bias_axes": "Race/Ethnicity; Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "Not specified",
      "key_findings": "Surface-based smoothing has less bias and variance because it respects cortical geometry by smoothing the PET data only along the cortical ribbon and so does not contaminate the GM signal with that of white matter and cerebrospinal fluid. The use of surface-based analysis in PET should result in substantial improvements in the reliability and detectability of effects in exploratory PET analysis, with or without PVC.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC4008670"
    },
    {
      "pmid": "24982437",
      "title": "The impact of image reconstruction bias on PET/CT 90Y dosimetry after radioembolization.",
      "abstract": "UNLABELLED: PET/CT imaging after radioembolization is a viable method for determining the posttreatment (90)Y distribution in the liver. Low true-to-random coincidence ratios in (90)Y PET studies limit the quantitative accuracy of these studies when reconstruction algorithms optimized for traditional PET imaging are used. This study examined these quantitative limitations and assessed the feasibility of generating radiation dosimetry maps in liver regions with high and low (90)Y concentrations. METHODS: (90)Y PET images were collected on a PET/CT scanner and iteratively reconstructed with the vendor-supplied reconstruction algorithm. PET studies on a Jaszczak cylindric phantom were performed to determine quantitative accuracy and minimum detectable concentration (MDC). (90)Y and (18)F point-source studies were used to investigate the possible increase in detected random coincidence events due to bremsstrahlung photons. Retrospective quantitative analyses were performed on (90)Y PET/CT images obtained after 65 right or left hepatic artery radioembolizations in 59 patients. Quantitative image errors were determined by comparing the measured image activity with the assayed (90)Y activity. PET images were converted to dose maps through convolution with voxel S values generated using MCNPX, a Monte Carlo N-particle transport code system for multiparticle and high-energy applications. Tumor and parenchyma doses and potential bias based on measurements found below the MDC were recorded. RESULTS: Random coincidences were found to increase in (90)Y acquisitions, compared with (18)F acquisitions, at similar positron emission rates because of bremsstrahlung photons. Positive bias was observed in all images. Quantitative accuracy was achieved for phantom inserts above the MDC of 1 MBq/mL. The mean dose to viable tumors was 183.6 \u00b1 156.5 Gy, with an average potential bias of 3.3 \u00b1 6.4 Gy. The mean dose to the parenchyma was 97.1 \u00b1 22.1 Gy, with an average potential bias of 8.9 \u00b1 4.9 Gy. CONCLUSION: The low signal-to-noise ratio caused by low positron emission rates and high bremsstrahlung photon production resulted in a positive bias on (90)Y PET images reconstructed with conventional iterative algorithms. However, quantitative accuracy was good at high activity concentrations, such as those found in tumor volumes, allowing for adequate tumor (90)Y PET/CT dosimetry after radioembolization.",
      "journal": "Journal of nuclear medicine : official publication, Society of Nuclear Medicine",
      "year": "2014",
      "doi": "10.2967/jnumed.113.133629",
      "authors": "Tapp Katie N et al.",
      "keywords": "90Y dosimetry; quantitative PET/CT; reconstruction bias",
      "mesh_terms": "Embolization, Therapeutic; Humans; Image Processing, Computer-Assisted; Phantoms, Imaging; Positron-Emission Tomography; Radiometry; Tomography, X-Ray Computed; Yttrium Radioisotopes",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/24982437/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; Oncology; Pulmonology",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: The low signal-to-noise ratio caused by low positron emission rates and high bremsstrahlung photon production resulted in a positive bias on (90)Y PET images reconstructed with conventional iterative algorithms. However, quantitative accuracy was good at high activity concentrations, such as those found in tumor volumes, allowing for adequate tumor (90)Y PET/CT dosimetry after radioembolization.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "25086004",
      "title": "Bias correction for selecting the minimal-error classifier from many machine learning models.",
      "abstract": "MOTIVATION: Supervised machine learning is commonly applied in genomic research to construct a classifier from the training data that is generalizable to predict independent testing data. When test datasets are not available, cross-validation is commonly used to estimate the error rate. Many machine learning methods are available, and it is well known that no universally best method exists in general. It has been a common practice to apply many machine learning methods and report the method that produces the smallest cross-validation error rate. Theoretically, such a procedure produces a selection bias. Consequently, many clinical studies with moderate sample sizes (e.g. n = 30-60) risk reporting a falsely small cross-validation error rate that could not be validated later in independent cohorts. RESULTS: In this article, we illustrated the probabilistic framework of the problem and explored the statistical and asymptotic properties. We proposed a new bias correction method based on learning curve fitting by inverse power law (IPL) and compared it with three existing methods: nested cross-validation, weighted mean correction and Tibshirani-Tibshirani procedure. All methods were compared in simulation datasets, five moderate size real datasets and two large breast cancer datasets. The result showed that IPL outperforms the other methods in bias correction with smaller variance, and it has an additional advantage to extrapolate error estimates for larger sample sizes, a practical feature to recommend whether more samples should be recruited to improve the classifier and accuracy. An R package 'MLbias' and all source files are publicly available. AVAILABILITY AND IMPLEMENTATION: tsenglab.biostat.pitt.edu/software.htm. CONTACT: ctseng@pitt.edu SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
      "journal": "Bioinformatics (Oxford, England)",
      "year": "2014",
      "doi": "10.1093/bioinformatics/btu520",
      "authors": "Ding Ying et al.",
      "keywords": "",
      "mesh_terms": "Artificial Intelligence; Breast Neoplasms; Data Interpretation, Statistical; Female; Gene Expression Profiling; Genomics; Humans; Models, Theoretical; Sample Size",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/25086004/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "RESULTS: In this article, we illustrated the probabilistic framework of the problem and explored the statistical and asymptotic properties. We proposed a new bias correction method based on learning curve fitting by inverse power law (IPL) and compared it with three existing methods: nested cross-validation, weighted mean correction and Tibshirani-Tibshirani procedure. All methods were compared in simulation datasets, five moderate size real datasets and two large breast cancer datasets.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC4221122"
    },
    {
      "pmid": "25164868",
      "title": "Complementary frame reconstruction: a low-biased dynamic PET technique for low count density data in projection space.",
      "abstract": "A new data handling method is presented for improving the image noise distribution and reducing bias when reconstructing very short frames from low count dynamic PET acquisition. The new method termed 'Complementary Frame Reconstruction' (CFR) involves the indirect formation of a count-limited emission image in a short frame through subtraction of two frames with longer acquisition time, where the short time frame data is excluded from the second long frame data before the reconstruction. This approach can be regarded as an alternative to the AML algorithm recently proposed by Nuyts et al, as a method to reduce the bias for the maximum likelihood expectation maximization (MLEM) reconstruction of count limited data. CFR uses long scan emission data to stabilize the reconstruction and avoids modification of algorithms such as MLEM. The subtraction between two long frame images, naturally allows negative voxel values and significantly reduces bias introduced in the final image. Simulations based on phantom and clinical data were used to evaluate the accuracy of the reconstructed images to represent the true activity distribution. Applicability to determine the arterial input function in human and small animal studies is also explored. In situations with limited count rate, e.g. pediatric applications, gated abdominal, cardiac studies, etc., or when using limited doses of short-lived isotopes such as 15O-water, the proposed method will likely be preferred over independent frame reconstruction to address bias and noise issues.",
      "journal": "Physics in medicine and biology",
      "year": "2014",
      "doi": "10.1088/0031-9155/59/18/5441",
      "authors": "Hong Inki et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Animals; Humans; Phantoms, Imaging; Positron-Emission Tomography",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/25164868/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology; Pediatrics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "In situations with limited count rate, e.g. pediatric applications, gated abdominal, cardiac studies, etc., or when using limited doses of short-lived isotopes such as 15O-water, the proposed method will likely be preferred over independent frame reconstruction to address bias and noise issues.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "25316533",
      "title": "Posttreatment attrition and its predictors, attrition bias, and treatment efficacy of the anxiety online programs.",
      "abstract": "BACKGROUND: Although relatively new, the field of e-mental health is becoming more popular with more attention given to researching its various aspects. However, there are many areas that still need further research, especially identifying attrition predictors at various phases of assessment and treatment delivery. OBJECTIVE: The present study identified the predictors of posttreatment assessment completers based on 24 pre- and posttreatment demographic and personal variables and 1 treatment variable, their impact on attrition bias, and the efficacy of the 5 fully automated self-help anxiety treatment programs for generalized anxiety disorder (GAD), social anxiety disorder (SAD), panic disorder with or without agoraphobia (PD/A), obsessive-compulsive disorder (OCD), and posttraumatic stress disorder (PTSD). METHODS: A complex algorithm was used to diagnose participants' mental disorders based on the criteria of the Diagnostic and Statistical Manual of Mental Disorders (Fourth Edition, Text Revision; DSM-IV-TR). Those who received a primary or secondary diagnosis of 1 of 5 anxiety disorders were offered an online 12-week disorder-specific treatment program. A total of 3199 individuals did not formally drop out of the 12-week treatment cycle, whereas 142 individuals formally dropped out. However, only 347 participants who completed their treatment cycle also completed the posttreatment assessment measures. Based on these measures, predictors of attrition were identified and attrition bias was examined. The efficacy of the 5 treatment programs was assessed based on anxiety-specific severity scores and 5 additional treatment outcome measures. RESULTS: On average, completers of posttreatment assessment measures were more likely to be seeking self-help online programs; have heard about the program from traditional media or from family and friends; were receiving mental health assistance; were more likely to learn best by reading, hearing and doing; had a lower pretreatment Kessler-6 total score; and were older in age. Predicted probabilities resulting from these attrition variables displayed no significant attrition bias using Heckman's method and thus allowing for the use of completer analysis. Six treatment outcome measures (Kessler-6 total score, number of diagnosed disorders, self-confidence in managing mental health issues, quality of life, and the corresponding pre- and posttreatment severity for each program-specific anxiety disorder and for major depressive episode) were used to assess the efficacy of the 5 anxiety treatment programs. Repeated measures MANOVA revealed a significant multivariate time effect for all treatment outcome measures for each treatment program. Follow-up repeated measures ANOVAs revealed significant improvements on all 6 treatment outcome measures for GAD and PTSD, 5 treatment outcome measures were significant for SAD and PD/A, and 4 treatment outcome measures were significant for OCD. CONCLUSIONS: Results identified predictors of posttreatment assessment completers and provided further support for the efficacy of self-help online treatment programs for the 5 anxiety disorders. TRIAL REGISTRATION: Australian and New Zealand Clinical Trials Registry ACTRN121611000704998; http://www.anzctr.org.au/trial_view.aspx?ID=336143 (Archived by WebCite at http://www.webcitation.org/618r3wvOG).",
      "journal": "Journal of medical Internet research",
      "year": "2014",
      "doi": "10.2196/jmir.3513",
      "authors": "Al-Asadi Ali M et al.",
      "keywords": "Internet interventions; Web treatment; cognitive behavioral therapy; e-mental health; fully automated; generalized anxiety disorder; obsessive compulsive disorder; online therapy; posttreatment attrition; posttreatment predictors; self-help; treatment efficacy",
      "mesh_terms": "Adolescent; Adult; Aged; Aged, 80 and over; Anxiety; Bias; Female; Humans; Internet; Male; Middle Aged; Models, Psychological; Outcome Assessment, Health Care; Quality of Life; Randomized Controlled Trials as Topic; Treatment Outcome; Young Adult",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/25316533/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry; Emergency Medicine; Infectious Disease",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Explainability/Interpretability",
      "clinical_setting": "Clinical Trial",
      "key_findings": "CONCLUSIONS: Results identified predictors of posttreatment assessment completers and provided further support for the efficacy of self-help online treatment programs for the 5 anxiety disorders. TRIAL REGISTRATION: Australian and New Zealand Clinical Trials Registry ACTRN121611000704998; http://www.anzctr.org.au/trial_view.aspx?ID=336143 (Archived by WebCite at http://www.webcitation.org/618r3wvOG).",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC4211028"
    },
    {
      "pmid": "25599724",
      "title": "Female pseudohermaphroditism: strategy and bias in a fast diagnosis. How tricky could be a diagnosis with a wrong anamnesis.",
      "abstract": "AIM: Congenital genitalia anomalies are a spectrum of malformation, difficult to classify because similar or identical phenotypes could have several different aetiology; therefore it's essential to assess an efficient diagnostic algorithm for a quick diagnosis and to develop an efficient therapeutic strategy. The aim of this study is to underline the importance of imaging in case of ambiguous genitalia due to its high sensitivity and specificity in detecting internal organs and urogenital anatomy. MATERIAL OF STUDY: We report a case of a young girl affected by a complex genitor-urinary malformation with an initial wrong anamnesis that led to a tricky diagnosis. RESULTS: Imaging techniques - especially Magnetic Resonance Imaging (MRI) - together with karyotype, hormones and physical investigations, offered complete and reliable informations for the best surgical treatment of our patient. CONCLUSION: Karyotype, hormones investigation, and radiological examinations are the main criteria considered in the diagnostic iter. Ultrasonography (US) is the primary modality for the detection of the presence or absence of gonads and m\u00fcllerian derivatives, whereas Cystourethrography can define urethral and vaginal tract or the presence of fistulas. In our experience MRI, due to its multiplanar capability and superior soft tissue characterization, proved to be useful to provide detailed anatomic information.",
      "journal": "Annali italiani di chirurgia",
      "year": "2014",
      "doi": "",
      "authors": "Onesti Maria Giuseppina et al.",
      "keywords": "",
      "mesh_terms": "46, XX Disorders of Sex Development; Adolescent; Bias; Female; Humans; Karyotype; Magnetic Resonance Imaging; Medical History Taking; Predictive Value of Tests; Sensitivity and Specificity; Tomography, X-Ray Computed; Ultrasonography; Vagina",
      "pub_types": "Case Reports; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/25599724/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Generative AI",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care; Surgery",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSION: Karyotype, hormones investigation, and radiological examinations are the main criteria considered in the diagnostic iter. Ultrasonography (US) is the primary modality for the detection of the presence or absence of gonads and m\u00fcllerian derivatives, whereas Cystourethrography can define urethral and vaginal tract or the presence of fistulas. In our experience MRI, due to its multiplanar capability and superior soft tissue characterization, proved to be useful to provide detailed anato...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "23997761",
      "title": "Localized FCM Clustering with Spatial Information for Medical Image Segmentation and Bias Field Estimation.",
      "abstract": "This paper presents a novel fuzzy energy minimization method for simultaneous segmentation and bias field estimation of medical images. We first define an objective function based on a localized fuzzy c-means (FCM) clustering for the image intensities in a neighborhood around each point. Then, this objective function is integrated with respect to the neighborhood center over the entire image domain to formulate a global fuzzy energy, which depends on membership functions, a bias field that accounts for the intensity inhomogeneity, and the constants that approximate the true intensities of the corresponding tissues. Therefore, segmentation and bias field estimation are simultaneously achieved by minimizing the global fuzzy energy. Besides, to reduce the impact of noise, the proposed algorithm incorporates spatial information into the membership function using the spatial function which is the summation of the membership functions in the neighborhood of each pixel under consideration. Experimental results on synthetic and real images are given to demonstrate the desirable performance of the proposed algorithm.",
      "journal": "International journal of biomedical imaging",
      "year": "2013",
      "doi": "10.1155/2013/930301",
      "authors": "Cui Wenchao et al.",
      "keywords": "",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/23997761/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Computer Vision/Imaging AI; Clustering",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Besides, to reduce the impact of noise, the proposed algorithm incorporates spatial information into the membership function using the spatial function which is the summation of the membership functions in the neighborhood of each pixel under consideration. Experimental results on synthetic and real images are given to demonstrate the desirable performance of the proposed algorithm.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC3749607"
    },
    {
      "pmid": "24320536",
      "title": "Breast density quantification using magnetic resonance imaging (MRI) with bias field correction: a postmortem study.",
      "abstract": "PURPOSE: Quantification of breast density based on three-dimensional breast MRI may provide useful information for the early detection of breast cancer. However, the field inhomogeneity can severely challenge the computerized image segmentation process. In this work, the effect of the bias field in breast density quantification has been investigated with a postmortem study. METHODS: T1-weighted images of 20 pairs of postmortem breasts were acquired on a 1.5 T breast MRI scanner. Two computer-assisted algorithms were used to quantify the volumetric breast density. First, standard fuzzy c-means (FCM) clustering was used on raw images with the bias field present. Then, the coherent local intensity clustering (CLIC) method estimated and corrected the bias field during the iterative tissue segmentation process. Finally, FCM clustering was performed on the bias-field-corrected images produced by CLIC method. The left-right correlation for breasts in the same pair was studied for both segmentation algorithms to evaluate the precision of the tissue classification. Finally, the breast densities measured with the three methods were compared to the gold standard tissue compositions obtained from chemical analysis. The linear correlation coefficient, Pearson's r, was used to evaluate the two image segmentation algorithms and the effect of bias field. RESULTS: The CLIC method successfully corrected the intensity inhomogeneity induced by the bias field. In left-right comparisons, the CLIC method significantly improved the slope and the correlation coefficient of the linear fitting for the glandular volume estimation. The left-right breast density correlation was also increased from 0.93 to 0.98. When compared with the percent fibroglandular volume (%FGV) from chemical analysis, results after bias field correction from both the CLIC the FCM algorithms showed improved linear correlation. As a result, the Pearson's r increased from 0.86 to 0.92 with the bias field correction. CONCLUSIONS: The investigated CLIC method significantly increased the precision and accuracy of breast density quantification using breast MRI images by effectively correcting the bias field. It is expected that a fully automated computerized algorithm for breast density quantification may have great potential in clinical MRI applications.",
      "journal": "Medical physics",
      "year": "2013",
      "doi": "10.1118/1.4831967",
      "authors": "Ding Huanjun et al.",
      "keywords": "",
      "mesh_terms": "Autopsy; Breast; Female; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Organ Size",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural",
      "url": "https://pubmed.ncbi.nlm.nih.gov/24320536/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Computer Vision/Imaging AI; Clustering",
      "health_domain": "Radiology/Medical Imaging; Oncology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: The investigated CLIC method significantly increased the precision and accuracy of breast density quantification using breast MRI images by effectively correcting the bias field. It is expected that a fully automated computerized algorithm for breast density quantification may have great potential in clinical MRI applications.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC3862600"
    },
    {
      "pmid": "22614791",
      "title": "Limited sampling strategies to estimate the area under the concentration-time curve. Biases and a proposed more accurate method.",
      "abstract": "BACKGROUND: Over 100 limited sampling strategies (LSSs) have been proposed to reduce the number of blood samples necessary to estimate the area under the concentration-time curve (AUC). The conditions under which these strategies succeed or fail remain to be clarified. OBJECTIVES: We investigated the accuracy of existing LSSs both theoretically and numerically by Monte Carlo simulation. We also proposed two new methods for more accurate AUC estimations. METHODS: We evaluated the following existing methods theoretically: i) nonlinear curve fitting algorithm (NLF), ii) the trapezium rule with exponential curve approximation (TZE), and iii) multiple linear regression (MLR). Taking busulfan (BU) as a test drug, we generated a set of theoretical concentration-time curves based on the identified distribution of pharmacokinetic parameters of BU and re-evaluated the existing LSSs using these virtual validation profiles. Based on the evaluation results, we improved the TZE so that unrealistic parameter values were not used. We also proposed a new estimation method in which the most likely curve was selected from a set of pre-generated theoretical concentration-time curves. RESULTS: Our evaluation, based on clinical profiles and a virtual validation set, revealed: i) NLF sometimes overestimated the absorption rate constant Ka, ii) TZE overestimated AUC over 280% when Ka is small, and iii) MLR underestimated AUC over 30% when the elimination rate constant Ke is small. These results were consistent with our mathematical evaluations for these methods. In contrast, our two new methods had little bias and good precision. CONCLUSIONS: Our investigation revealed that existing LSSs induce different but specific biases in the estimation of AUC. Our two new LSSs, a modified TZE and one using model concentration-time curves, provided accurate and precise estimations of AUC.",
      "journal": "Methods of information in medicine",
      "year": "2012",
      "doi": "10.3414/ME11-01-0071",
      "authors": "Tsuruta H et al.",
      "keywords": "",
      "mesh_terms": "Antineoplastic Agents, Alkylating; Area Under Curve; Busulfan; Models, Statistical; Monte Carlo Method; Selection Bias",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/22614791/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Regression",
      "health_domain": "General Healthcare",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Data Collection; Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Our investigation revealed that existing LSSs induce different but specific biases in the estimation of AUC. Our two new LSSs, a modified TZE and one using model concentration-time curves, provided accurate and precise estimations of AUC.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "22719749",
      "title": "Reducing bias of allele frequency estimates by modeling SNP genotype data with informative missingness.",
      "abstract": "The presence of missing single-nucleotide polymorphism (SNP) genotypes is common in genetic studies. For studies with low-density SNPs, the most commonly used approach to dealing with genotype missingness is to simply remove the observations with missing genotypes from the analyses. This na\u00efve method is straightforward but is valid only when the missingness is random. However, a given assay often has a different capability in genotyping heterozygotes and homozygotes, causing the phenomenon of \"differential dropout\" in the sense that the missing rates of heterozygotes and homozygotes are different. In practice, differential dropout among genotypes exists in even carefully designed studies, such as the data from the HapMap project and the Wellcome Trust Case Control Consortium. Under the assumption of Hardy-Weinberg equilibrium and no genotyping error, we here propose a statistical method to model the differential dropout among different genotypes. Compared with the na\u00efve method, our method provides more accurate allele frequency estimates when the differential dropout is present. To demonstrate its practical use, we further apply our method to the HapMap data and a scleroderma data set.",
      "journal": "Frontiers in genetics",
      "year": "2012",
      "doi": "10.3389/fgene.2012.00107",
      "authors": "Lin Wan-Yu et al.",
      "keywords": "EM algorithm; allele frequency; genotype; informative missingness; missing at random; single-nucleotide polymorphism",
      "mesh_terms": "",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/22719749/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "Compared with the na\u00efve method, our method provides more accurate allele frequency estimates when the differential dropout is present. To demonstrate its practical use, we further apply our method to the HapMap data and a scleroderma data set.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC3376470"
    },
    {
      "pmid": "21204122",
      "title": "Using latent variable modeling and multiple imputation to calibrate rater bias in diagnosis assessment.",
      "abstract": "We present an approach that uses latent variable modeling and multiple imputation to correct rater bias when one group of raters tends to be more lenient in assigning a diagnosis than another. Our method assumes that there exists an unobserved moderate category of patient who is assigned a positive diagnosis by one type of rater and a negative diagnosis by the other type. We present a Bayesian random effects censored ordinal probit model that allows us to calibrate the diagnoses across rater types by identifying and multiply imputing 'case' or 'non-case' status for patients in the moderate category. A Markov chain Monte Carlo algorithm is presented to estimate the posterior distribution of the model parameters and generate multiple imputations. Our method enables the calibrated diagnosis variable to be used in subsequent analyses while also preserving uncertainty in true diagnosis. We apply our model to diagnoses of posttraumatic stress disorder (PTSD) from a depression study where nurse practitioners were twice as likely as clinical psychologists to diagnose PTSD despite the fact that participants were randomly assigned to either a nurse or a psychologist. Our model appears to balance PTSD rates across raters, provides a good fit to the data, and preserves between-rater variability. After calibrating the diagnoses of PTSD across rater types, we perform an analysis looking at the effects of comorbid PTSD on changes in depression scores over time. Results are compared with an analysis that uses the original diagnoses and show that calibrating the PTSD diagnoses can yield different inferences.",
      "journal": "Statistics in medicine",
      "year": "2011",
      "doi": "10.1002/sim.4109",
      "authors": "Siddique Juned et al.",
      "keywords": "",
      "mesh_terms": "Adult; Algorithms; Anxiety; Depression; Diagnosis, Differential; Female; Humans; Markov Chains; Models, Statistical; Nurse Practitioners; Observer Variation; Stress Disorders, Post-Traumatic",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/21204122/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Mental Health/Psychiatry; Emergency Medicine",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "After calibrating the diagnoses of PTSD across rater types, we perform an analysis looking at the effects of comorbid PTSD on changes in depression scores over time. Results are compared with an analysis that uses the original diagnoses and show that calibrating the PTSD diagnoses can yield different inferences.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC3058328"
    },
    {
      "pmid": "21273023",
      "title": "Bias field reduction by localized Lloyd-Max quantization.",
      "abstract": "Bias field reduction is a common problem in medical imaging. A bias field usually manifests itself as a smooth intensity variation across the image. The resulting image inhomogeneity is a severe problem for posterior image processing and analysis techniques such as registration or segmentation. In this article, we present a novel debiasing technique based on localized Lloyd-Max quantization (LMQ). The local bias is modeled as a multiplicative field and is assumed to be slowly varying. The method is based on the assumption that the global, undegraded histogram is characterized by a limited number of gray values. The goal is then to find the discrete intensity values such that spreading those values according to the local bias field reproduces the global histogram as good as possible. We show that our method is capable of efficiently reducing (even strong) bias fields in 3D volumes.",
      "journal": "Magnetic resonance imaging",
      "year": "2011",
      "doi": "10.1016/j.mri.2010.10.015",
      "authors": "Mai Zhenhua et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Computer Simulation; Humans; Image Enhancement; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Models, Statistical; Normal Distribution; Pattern Recognition, Automated; Phantoms, Imaging; Reproducibility of Results",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/21273023/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "Radiology/Medical Imaging",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The goal is then to find the discrete intensity values such that spreading those values according to the local bias field reproduces the global histogram as good as possible. We show that our method is capable of efficiently reducing (even strong) bias fields in 3D volumes.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "21385160",
      "title": "Buckley-James-type estimator with right-censored and length-biased data.",
      "abstract": "We present a natural generalization of the Buckley-James-type estimator for traditional survival data to right-censored length-biased data under the accelerated failure time (AFT) model. Length-biased data are often encountered in prevalent cohort studies and cancer screening trials. Informative right censoring induced by length-biased sampling creates additional challenges in modeling the effects of risk factors on the unbiased failure times for the target population. In this article, we evaluate covariate effects on the failure times of the target population under the AFT model given the observed length-biased data. We construct a Buckley-James-type estimating equation, develop an iterative computing algorithm, and establish the asymptotic properties of the estimators. We assess the finite-sample properties of the proposed estimators against the estimators obtained from the existing methods. Data from a prevalent cohort study of patients with dementia are used to illustrate the proposed methodology.",
      "journal": "Biometrics",
      "year": "2011",
      "doi": "10.1111/j.1541-0420.2011.01568.x",
      "authors": "Ning Jing et al.",
      "keywords": "",
      "mesh_terms": "Bias; Canada; Data Interpretation, Statistical; Dementia; Humans; Proportional Hazards Models; Survival Analysis; Survival Rate",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/21385160/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; Neurology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "We assess the finite-sample properties of the proposed estimators against the estimators obtained from the existing methods. Data from a prevalent cohort study of patients with dementia are used to illustrate the proposed methodology.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC3137763"
    },
    {
      "pmid": "21097200",
      "title": "Non-iterative relative bias correction for 3D reconstruction of in utero fetal brain MR imaging.",
      "abstract": "The slice intersection motion correction (SIMC) method is a powerful tool to compensate for motion that occurs during in utero acquisition of the multislice magnetic resonance (MR) images of the human fetal brain. The SIMC method makes use of the slice intersection intensity profiles of orthogonally planned slice pairs to simultaneously correct for the relative motion occurring between all the acquired slices. This approach is based on the assumption that the bias field is consistent between slices. However, for some clinical studies where there is a strong bias field combined with significant fetal motion relative to the coils, this assumption is broken and the resulting motion estimate and the reconstruction to a 3D volume can both contain errors. In this work, we propose a method to correct for the relative differences in bias field between all slice pairs. For this, we define the energy function as the mean square difference of the intersection profiles, that is then minimized with respect to the bias field parameters of the slices. A non iterative method which considers the relative bias between each slice simultaneously is used to efficiently remove inconsistencies. The method, when tested on synthetic simulations and actual clinical imaging studies where bias was an issue, brought a significant improvement to the final reconstructed image.",
      "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference",
      "year": "2010",
      "doi": "10.1109/IEMBS.2010.5627876",
      "authors": "Kim Kio et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artifacts; Artificial Intelligence; Brain; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Pattern Recognition, Automated; Prenatal Diagnosis; Reproducibility of Results; Sensitivity and Specificity",
      "pub_types": "Evaluation Study; Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/21097200/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Not specified",
      "health_domain": "Obstetrics/Maternal Health; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "A non iterative method which considers the relative bias between each slice simultaneously is used to efficiently remove inconsistencies. The method, when tested on synthetic simulations and actual clinical imaging studies where bias was an issue, brought a significant improvement to the final reconstructed image.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "18177784",
      "title": "New algorithm for treatment allocation reduced selection bias and loss of power in small trials.",
      "abstract": "OBJECTIVE: In clinical trials, patients become available for treatment sequentially. Especially in trials with a small number of patients, loss of power may become an important issue, if treatments are not allocated equally or if prognostic factors differ between the treatment groups. We present a new algorithm for sequential allocation of two treatments in small clinical trials, which is concerned with the reduction of both selection bias and imbalance. STUDY DESIGN AND SETTING: With the algorithm, an element of chance is added to the treatment as allocated by minimization. The amount of chance depends on the actual amount of imbalance of treatment allocations of the patients already enrolled. The sensitivity to imbalance may be tuned. We performed trial simulations with different numbers of patients and prognostic factors, in which we quantified loss of power and selection bias. RESULTS: With our method, selection bias is smaller than with minimization, and loss of power is lower than with pure randomization or treatment allocation according to a biased coin principle. CONCLUSION: Our method combines the conflicting aims of reduction of bias by predictability and reduction of loss of power, as a result of imbalance. The method may be of use in small trials.",
      "journal": "Journal of clinical epidemiology",
      "year": "2008",
      "doi": "10.1016/j.jclinepi.2007.04.002",
      "authors": "Hofmeijer J et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Humans; Patient Selection; Prognosis; Random Allocation; Randomized Controlled Trials as Topic; Research Design; Selection Bias",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/18177784/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Clinical Trial",
      "key_findings": "CONCLUSION: Our method combines the conflicting aims of reduction of bias by predictability and reduction of loss of power, as a result of imbalance. The method may be of use in small trials.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "18407896",
      "title": "Validation of image segmentation by estimating rater bias and variance.",
      "abstract": "The accuracy and precision of segmentations of medical images has been difficult to quantify in the absence of a 'ground truth' or reference standard segmentation for clinical data. Although physical or digital phantoms can help by providing a reference standard, they do not allow the reproduction of the full range of imaging and anatomical characteristics observed in clinical data. An alternative assessment approach is to compare with segmentations generated by domain experts. Segmentations may be generated by raters who are trained experts or by automated image analysis algorithms. Typically, these segmentations differ due to intra-rater and inter-rater variability. The most appropriate way to compare such segmentations has been unclear. We present here a new algorithm to enable the estimation of performance characteristics, and a true labelling, from observations of segmentations of imaging data where segmentation labels may be ordered or continuous measures. This approach may be used with, among others, surface, distance transform or level-set representations of segmentations, and can be used to assess whether or not a rater consistently overestimates or underestimates the position of a boundary.",
      "journal": "Philosophical transactions. Series A, Mathematical, physical, and engineering sciences",
      "year": "2008",
      "doi": "10.1098/rsta.2008.0040",
      "authors": "Warfield Simon K et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Analysis of Variance; Bayes Theorem; Bias; Biometry; Brain Neoplasms; Expert Testimony; Humans; Image Processing, Computer-Assisted; Likelihood Functions; Magnetic Resonance Imaging; Models, Statistical; Phantoms, Imaging",
      "pub_types": "Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/18407896/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "Oncology; ICU/Critical Care; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "We present here a new algorithm to enable the estimation of performance characteristics, and a true labelling, from observations of segmentations of imaging data where segmentation labels may be ordered or continuous measures. This approach may be used with, among others, surface, distance transform or level-set representations of segmentations, and can be used to assess whether or not a rater consistently overestimates or underestimates the position of a boundary.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC3227147"
    },
    {
      "pmid": "18602242",
      "title": "Communicating side effect risks in a tamoxifen prophylaxis decision aid: the debiasing influence of pictographs.",
      "abstract": "OBJECTIVE: To experimentally test whether using pictographs (image matrices), incremental risk formats, and varied risk denominators would influence perceptions and comprehension of side effect risks in an online decision aid about prophylactic use of tamoxifen to prevent primary breast cancers. METHODS: We recruited 631 women with elevated breast cancer risk from two healthcare organizations. Participants saw tailored estimates of the risks of 5 side effects: endometrial cancer, blood clotting, cataracts, hormonal symptoms, and sexual problems. Presentation format was randomly varied in a three factor design: (A) risk information was displayed either in pictographs or numeric text; (B) presentations either reported total risks with and without tamoxifen or highlighted the incremental risk most relevant for decision making; and (C) risk estimates used 100 or 1000 person denominators. Primary outcome measures included risk perceptions and gist knowledge. RESULTS: Incremental risk formats consistently lowered perceived risk of side effects but resulted in low knowledge when displayed by numeric text only. Adding pictographs, however, produced significantly higher comprehension levels. CONCLUSIONS: Pictographs make risk statistics easier to interpret, reducing biases associated with incremental risk presentations. PRACTICE IMPLICATIONS: Including graphs in risk communications is essential to support an informed treatment decision-making process.",
      "journal": "Patient education and counseling",
      "year": "2008",
      "doi": "10.1016/j.pec.2008.05.010",
      "authors": "Zikmund-Fisher Brian J et al.",
      "keywords": "",
      "mesh_terms": "Adult; Aged; Audiovisual Aids; Breast Neoplasms; Decision Support Techniques; Female; Health Knowledge, Attitudes, Practice; Humans; Internet; Michigan; Middle Aged; Multivariate Analysis; Patient Education as Topic; Risk Assessment; Tamoxifen; Washington",
      "pub_types": "Journal Article; Multicenter Study; Randomized Controlled Trial; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/18602242/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Generative AI",
      "health_domain": "Oncology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: Pictographs make risk statistics easier to interpret, reducing biases associated with incremental risk presentations. PRACTICE IMPLICATIONS: Including graphs in risk communications is essential to support an informed treatment decision-making process.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC2649664"
    },
    {
      "pmid": "17328979",
      "title": "Economic evaluation of services for a National Health scheme: the case for a fairness-based framework.",
      "abstract": "In this paper we argue that the usual framework for evaluating health services may need modification in the context of a National Health Scheme (NHS). Some costs and benefits may need to be ignored or discounted, others included at face value, and some transfer payments included in the decision algorithm. In contrast with the standard framework, we argue that economic evaluation in the context of an NHS should focus on 'social transfers' between taxpayers and beneficiaries, and that the nature and scope of these transfers is determined by the level of social generosity. Some of the implications of a modified framework are illustrated with a re-examination of (i) costs and transfer payments, (ii) unrelated future costs, (iii) moral hazard, and (iv) the rule that marginal costs should equal marginal benefits. We argue that an explicitly 'fairness-based' framework is needed for the evaluation of services in an NHS. In contrast, the usual welfare economic theoretic framework facilitates the sidelining of issues of fairness.",
      "journal": "Journal of health economics",
      "year": "2007",
      "doi": "10.1016/j.jhealeco.2006.11.004",
      "authors": "Richardson Jeff et al.",
      "keywords": "",
      "mesh_terms": "Evaluation Studies as Topic; Health Services Accessibility; Humans; National Health Programs; Social Justice; Victoria",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/17328979/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Framework/Toolkit",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Insurance Status",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We argue that an explicitly 'fairness-based' framework is needed for the evaluation of services in an NHS. In contrast, the usual welfare economic theoretic framework facilitates the sidelining of issues of fairness.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "17511519",
      "title": "A method to address differential bias in genotyping in large-scale association studies.",
      "abstract": "In a previous paper we have shown that, when DNA samples for cases and controls are prepared in different laboratories prior to high-throughput genotyping, scoring inaccuracies can lead to differential misclassification and, consequently, to increased false-positive rates. Different DNA sourcing is often unavoidable in large-scale disease association studies of multiple case and control sets. Here, we describe methodological improvements to minimise such biases. These fall into two categories: improvements to the basic clustering methods for identifying genotypes from fluorescence intensities, and use of \"fuzzy\" calls in association tests in order to make appropriate allowance for call uncertainty. We find that the main improvement is a modification of the calling algorithm that links the clustering of cases and controls while allowing for different DNA sourcing. We also find that, in the presence of different DNA sourcing, biases associated with missing data can increase the false-positive rate. Therefore, we propose the use of \"fuzzy\" calls to deal with uncertain genotypes that would otherwise be labeled as missing.",
      "journal": "PLoS genetics",
      "year": "2007",
      "doi": "10.1371/journal.pgen.0030074",
      "authors": "Plagnol Vincent et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Bias; Case-Control Studies; Computer Simulation; Databases, Genetic; Epidemiologic Methods; Female; Gene Frequency; Genetic Predisposition to Disease; Genotype; Humans; Male; Polymorphism, Single Nucleotide; United Kingdom",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/17511519/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Methodology",
      "ai_ml_method": "Clustering",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "We also find that, in the presence of different DNA sourcing, biases associated with missing data can increase the false-positive rate. Therefore, we propose the use of \"fuzzy\" calls to deal with uncertain genotypes that would otherwise be labeled as missing.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC1868951"
    },
    {
      "pmid": "15986433",
      "title": "Quantitative evaluation of automated skull-stripping methods applied to contemporary and legacy images: effects of diagnosis, bias correction, and slice location.",
      "abstract": "Performance of automated methods to isolate brain from nonbrain tissues in magnetic resonance (MR) structural images may be influenced by MR signal inhomogeneities, type of MR image set, regional anatomy, and age and diagnosis of subjects studied. The present study compared the performance of four methods: Brain Extraction Tool (BET; Smith [2002]: Hum Brain Mapp 17:143-155); 3dIntracranial (Ward [1999] Milwaukee: Biophysics Research Institute, Medical College of Wisconsin; in AFNI); a Hybrid Watershed algorithm (HWA, Segonne et al. [2004] Neuroimage 22:1060-1075; in FreeSurfer); and Brain Surface Extractor (BSE, Sandor and Leahy [1997] IEEE Trans Med Imag 16:41-54; Shattuck et al. [2001] Neuroimage 13:856-876) to manually stripped images. The methods were applied to uncorrected and bias-corrected datasets; Legacy and Contemporary T1-weighted image sets; and four diagnostic groups (depressed, Alzheimer's, young and elderly control). To provide a criterion for outcome assessment, two experts manually stripped six sagittal sections for each dataset in locations where brain and nonbrain tissue are difficult to distinguish. Methods were compared on Jaccard similarity coefficients, Hausdorff distances, and an Expectation-Maximization algorithm. Methods tended to perform better on contemporary datasets; bias correction did not significantly improve method performance. Mesial sections were most difficult for all methods. Although AD image sets were most difficult to strip, HWA and BSE were more robust across diagnostic groups compared with 3dIntracranial and BET. With respect to specificity, BSE tended to perform best across all groups, whereas HWA was more sensitive than other methods. The results of this study may direct users towards a method appropriate to their T1-weighted datasets and improve the efficiency of processing for large, multisite neuroimaging studies. Performance of automated methods to isolate brain from nonbrain tissues in magnetic resonance (MR) structural images may be influenced by MR signal inhomogeneities, type of MR image set, regional anatomy, and age and diagnosis of subjects studied. The present study compared the performance of four methods: Brain Extraction Tool (BET; Smith [2002]: Hum Brain Mapp 17:143\u2013155); 3dIntracranial (Ward [1999] Milwaukee: Biophysics Research Institute, Medical College of Wisconsin; in AFNI); a Hybrid Watershed algorithm (HWA, Segonne et al. [2004] Neuroimage 22:1060\u20131075; in FreeSurfer); and Brain Surface Extractor (BSE, Sandor and Leahy [1997] IEEE Trans Med Imag 16:41\u201354; Shattuck et al. [2001] Neuroimage 13:856\u2013876) to manually stripped images. The methods were applied to uncorrected and bias\u2010corrected datasets; Legacy and Contemporary T1\u2010weighted image sets; and four diagnostic groups (depressed, Alzheimer's, young and elderly control). To provide a criterion for outcome assessment, two experts manually stripped six sagittal sections for each dataset in locations where brain and nonbrain tissue are difficult to distinguish. Methods were compared on Jaccard similarity coefficients, Hausdorff distances, and an Expectation\u2010Maximization algorithm. Methods tended to perform better on contemporary datasets; bias correction did not significantly improve method performance. Mesial sections were most difficult for all methods. Although AD image sets were most difficult to strip, HWA and BSE were more robust across diagnostic groups compared with 3dIntracranial and BET. With respect to specificity, BSE tended to perform best across all groups, whereas HWA was more sensitive than other methods. The results of this study may direct users towards a method appropriate to their T1\u2010weighted datasets and improve the efficiency of processing for large, multisite neuroimaging studies. Hum. Brain Mapping, 2005. \u00a9 2005 Wiley\u2010Liss, Inc.",
      "journal": "Human brain mapping",
      "year": "2006",
      "doi": "10.1002/hbm.20161",
      "authors": "Fennema-Notestine Christine et al.",
      "keywords": "",
      "mesh_terms": "Adult; Age Factors; Aged; Algorithms; Brain; Brain Diseases; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Middle Aged; Radiography; Sensitivity and Specificity; Software",
      "pub_types": "Comparative Study; Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/15986433/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care; Neurology",
      "bias_axes": "Gender/Sex; Age; Geographic",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "Brain Mapping, 2005. \u00a9 2005 Wiley\u2010Liss, Inc.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 1 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC2408865"
    },
    {
      "pmid": "16386820",
      "title": "Revisiting evidence on lung cancer and passive smoking: adjustment for publication bias by means of \"trim and fill\" algorithm.",
      "abstract": "Meta-analyses are subject to bias because smaller or non-significant studies are less likely to be published, and most meta-analyses do not consider the effect of publication bias on their results. To assess the true risk, we revisited a famous meta-analysis including 37 studies on lung cancer and passive smoking, and adjusted for publication bias by means of the \"trim and fill\" algorithm. The adjusted relative risk of lung cancer in non-smokers who lived with a smoker from the 44 studies including the 7 filled ones was 1.19 (95% confidence interval 1.08-1.31, p = 0.0004), and the estimate of excess risk fell from 24 to 19%.",
      "journal": "Lung cancer (Amsterdam, Netherlands)",
      "year": "2006",
      "doi": "10.1016/j.lungcan.2005.11.004",
      "authors": "Takagi Hisato et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Humans; Lung Neoplasms; Risk; Tobacco Smoke Pollution",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/16386820/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Meta-Analysis",
      "ai_ml_method": "Not specified",
      "health_domain": "Oncology; Pulmonology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "To assess the true risk, we revisited a famous meta-analysis including 37 studies on lung cancer and passive smoking, and adjusted for publication bias by means of the \"trim and fill\" algorithm. The adjusted relative risk of lung cancer in non-smokers who lived with a smoker from the 44 studies including the 7 filled ones was 1.19 (95% confidence interval 1.08-1.31, p = 0.0004), and the estimate of excess risk fell from 24 to 19%.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "16943441",
      "title": "Configurational-bias sampling technique for predicting side-chain conformations in proteins.",
      "abstract": "Prediction of side-chain conformations is an important component of several biological modeling applications. In this work, we have developed and tested an advanced Monte Carlo sampling strategy for predicting side-chain conformations. Our method is based on a cooperative rearrangement of atoms that belong to a group of neighboring side-chains. This rearrangement is accomplished by deleting groups of atoms from the side-chains in a particular region, and regrowing them with the generation of trial positions that depends on both a rotamer library and a molecular mechanics potential function. This method allows us to incorporate flexibility about the rotamers in the library and explore phase space in a continuous fashion about the primary rotamers. We have tested our algorithm on a set of 76 proteins using the all-atom AMBER99 force field and electrostatics that are governed by a distance-dependent dielectric function. When the tolerance for correct prediction of the dihedral angles is a <20 degrees deviation from the native state, our prediction accuracies for chi1 are 83.3% and for chi1 and chi2 are 65.4%. The accuracies of our predictions are comparable to the best results in the literature that often used Hamiltonians that have been specifically optimized for side-chain packing. We believe that the continuous exploration of phase space enables our method to overcome limitations inherent with using discrete rotamers as trials.",
      "journal": "Protein science : a publication of the Protein Society",
      "year": "2006",
      "doi": "10.1110/ps.062165906",
      "authors": "Jain Tushar et al.",
      "keywords": "",
      "mesh_terms": "Bias; Computer Simulation; Databases, Factual; Models, Molecular; Monte Carlo Method; Predictive Value of Tests; Protein Conformation; Reproducibility of Results",
      "pub_types": "Evaluation Study; Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/16943441/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care; Surgery",
      "bias_axes": "Gender/Sex; Geographic",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Mitigation",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "The accuracies of our predictions are comparable to the best results in the literature that often used Hamiltonians that have been specifically optimized for side-chain packing. We believe that the continuous exploration of phase space enables our method to overcome limitations inherent with using discrete rotamers as trials.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content (only 0 indicators)",
      "ft_source": "Full text",
      "has_fulltext": true,
      "pmcid": "PMC2242598"
    },
    {
      "pmid": "17096661",
      "title": "Improving SVT discrimination in single-chamber ICDs: a new electrogram morphology-based algorithm.",
      "abstract": "INTRODUCTION: Wide-spread adoption of ICD therapy has focused efforts on improving the quality of life for patients by reducing \"inappropriate\" shock therapies. To this end, distinguishing supraventricular tachycardia from ventricular tachycardia remains a major challenge for ICDs. More sophisticated discrimination algorithms based on ventricular electrogram morphology have been made practicable by the increased computational ability of modern ICDs. METHODS AND RESULTS: We report results from a large prospective study (1,122 pts) of a new ventricular electrogram morphology tachycardia discrimination algorithm (Wavelet Dynamic Discrimination, Medtronic, Minneapolis, MN, USA) operating at minimal algorithm setting (RV coil-can electrogram, match threshold of 70%). This is a nonrandomized cohort study of ICD patients using the morphology discrimination of the Wavelet algorithm to distinguish SVT and VT/VF. The Wavelet criterion was required ON in all patients and all other supraventricular tachycardia discriminators were required to be OFF. Spontaneous episodes (N = 2,235) eligible for ICD therapy were adjudicated for detection algorithm performance. The generalized estimating equations method was used to remove bias introduced when an individual patient contributes multiple episodes. Inappropriate therapies for supraventricular tachycardia were reduced by 78% (90% CI: 72.8-82.9%) for episodes within the range of rates where Wavelet was programmed to discriminate. Sensitivity for sustained ventricular tachycardia was 98.6% (90% CI: 97-99.3%) without the use of high-rate time out. CONCLUSIONS: Results from this prospective study of the Wavelet electrogram morphology discrimination algorithm operating as the sole discriminator in the ON mode demonstrate that inappropriate therapy for supraventricular tachycardia in a single-chamber ICD can be dramatically reduced compared to rate detection alone.",
      "journal": "Journal of cardiovascular electrophysiology",
      "year": "2006",
      "doi": "10.1111/j.1540-8167.2006.00643.x",
      "authors": "Klein George J et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Cohort Studies; Defibrillators, Implantable; Diagnosis, Computer-Assisted; Diagnosis, Differential; Discriminant Analysis; Electric Countershock; Electrocardiography; Female; Humans; Male; Middle Aged; Reproducibility of Results; Sensitivity and Specificity; Tachycardia, Supraventricular; Tachycardia, Ventricular; Therapy, Computer-Assisted",
      "pub_types": "Controlled Clinical Trial; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/17096661/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "ICU/Critical Care",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "ICU",
      "key_findings": "CONCLUSIONS: Results from this prospective study of the Wavelet electrogram morphology discrimination algorithm operating as the sole discriminator in the ON mode demonstrate that inappropriate therapy for supraventricular tachycardia in a single-chamber ICD can be dramatically reduced compared to rate detection alone.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "17354851",
      "title": "Validation of image segmentation by estimating rater bias and variance.",
      "abstract": "The accuracy and precision of segmentations of medical images has been difficult to quantify in the absence of a \"ground truth\" or reference standard segmentation for clinical data. Although physical or digital phantoms can help by providing a reference standard, they do not allow the reproduction of the full range of imaging and anatomical characteristics observed in clinical data. An alternative assessment approach is to compare to segmentations generated by domain experts. Segmentations may be generated by raters who are trained experts or by automated image analysis algorithms. Typically these segmentations differ due to intra-rater and inter-rater variability. The most appropriate way to compare such segmentations has been unclear. We present here a new algorithm to enable the estimation of performance characteristics, and a true labeling, from observations of segmentations of imaging data where segmentation labels may be ordered or continuous measures. This approach may be used with, amongst others, surface, distance transform or level set representations of segmentations, and can be used to assess whether or not a rater consistently over-estimates or under-estimates the position of a boundary.",
      "journal": "Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "year": "2006",
      "doi": "10.1007/11866763_103",
      "authors": "Warfield Simon K et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artificial Intelligence; Brain Neoplasms; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Observer Variation; Pattern Recognition, Automated; Professional Competence; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Task Performance and Analysis",
      "pub_types": "Evaluation Study; Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.; Validation Study",
      "url": "https://pubmed.ncbi.nlm.nih.gov/17354851/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Computer Vision/Imaging AI",
      "health_domain": "Oncology; ICU/Critical Care; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Model Evaluation; Deployment",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "We present here a new algorithm to enable the estimation of performance characteristics, and a true labeling, from observations of segmentations of imaging data where segmentation labels may be ordered or continuous measures. This approach may be used with, amongst others, surface, distance transform or level set representations of segmentations, and can be used to assess whether or not a rater consistently over-estimates or under-estimates the position of a boundary.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "17354730",
      "title": "Many heads are better than one: jointly removing bias from multiple MRIs using nonparametric maximum likelihood.",
      "abstract": "The correction of multiplicative bias in magnetic resonance images is an important problem in medical image processing, especially as a preprocessing step for quantitative measurements and other numerical procedures. Most previous approaches have used a maximum likelihood method to increase the probability of the pixels in a single image by adaptively estimating a correction to the unknown image bias field. The pixel probabilities are defined either in terms of a pre-existing tissue model, or nonparametrically in terms of the image's own pixel values. In both cases, the specific location of a pixel in the image does not influence the probability calculation. Our approach, similar to methods of joint registration, simultaneously eliminates the bias from a set of images of the same anatomy, but from different patients. We use the statistics from the same location across different patients' images, rather than within an image, to eliminate bias fields from all of the images simultaneously. Evaluating the likelihood of a particular voxel in one patient's scan with respect to voxels in the same location in a set of other patients' scans disambiguates effects that might be due to either bias fields or anatomy. We present a variety of \"two-dimensional\" experimental results (working with one image from each patient) showing how our method overcomes serious problems experienced by other methods. We also present preliminary results on full three-dimensional volume correction across patients.",
      "journal": "Information processing in medical imaging : proceedings of the ... conference",
      "year": "2005",
      "doi": "10.1007/11505730_51",
      "authors": "Learned-Miller Erik G et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Artificial Intelligence; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Likelihood Functions; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",
      "pub_types": "Evaluation Study; Journal Article; Research Support, U.S. Gov't, Non-P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/17354730/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Radiology/Medical Imaging; ICU/Critical Care; Neurology",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Data Preprocessing",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "ICU",
      "key_findings": "We present a variety of \"two-dimensional\" experimental results (working with one image from each patient) showing how our method overcomes serious problems experienced by other methods. We also present preliminary results on full three-dimensional volume correction across patients.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "12848923",
      "title": "[An effective method to reduce bias between two compared groups: propensity score].",
      "abstract": "OBJECTIVE: Through introduction of principal theory and algorithm of propensity score to design SAS macro programs for binary data. METHODS: Propensity score method was used to compare the differences of character variables between two groups, and the association of DNR (Do Not Resuscitate) with the mortality of congestive heart failure was evaluated with different methods. RESULTS: Significant differences among the character variables between two groups were effectively balanced with stratification or matching method. The odds ratios of DNR with the in-hospital mortality rate of congestive heart failure were estimated identical with different algorithms and to find that the association of DNR to in-hospital mortality was highly significant. CONCLUSION: Propensity score was a good algorithm that could be used to analyze any kind of observational data for matching the effects among the character variables.",
      "journal": "Zhonghua liu xing bing xue za zhi = Zhonghua liuxingbingxue zazhi",
      "year": "2003",
      "doi": "",
      "authors": "Zhao Shou-jun et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Bias; Heart Failure; Humans; Models, Statistical",
      "pub_types": "Comparative Study; Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/12848923/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient",
      "key_findings": "CONCLUSION: Propensity score was a good algorithm that could be used to analyze any kind of observational data for matching the effects among the character variables.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "11779685",
      "title": "The problem of bias in training data in regression problems in medical decision support.",
      "abstract": "This paper describes a bias problem encountered in a machine learning approach to outcome prediction in anticoagulant drug therapy. The outcome to be predicted is a measure of the clotting time for the patient; this measure is continuous and so the prediction task is a regression problem. Artificial neural networks (ANNs) are a powerful mechanism for learning to predict such outcomes from training data. However, experiments have shown that an ANN is biased towards values more commonly occurring in the training data and is thus, less likely to be correct in predicting extreme values. This issue of bias in training data in regression problems is similar to the associated problem with minority classes in classification. However, this bias issue in classification is well documented and is an on-going area of research. In this paper, we consider stratified sampling and boosting as solutions to this bias problem and evaluate them on this outcome prediction problem and on two other datasets. Both approaches produce some improvements with boosting showing the most promise.",
      "journal": "Artificial intelligence in medicine",
      "year": "2002",
      "doi": "10.1016/s0933-3657(01)00092-6",
      "authors": "Mac Namee B et al.",
      "keywords": "",
      "mesh_terms": "Bias; Computer Simulation; Decision Making, Computer-Assisted; Decision Support Techniques; Humans; Regression Analysis",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/11779685/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Neural Network",
      "health_domain": "General Healthcare",
      "bias_axes": "Race/Ethnicity; Gender/Sex",
      "lifecycle_stage": "Data Collection",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "In this paper, we consider stratified sampling and boosting as solutions to this bias problem and evaluate them on this outcome prediction problem and on two other datasets. Both approaches produce some improvements with boosting showing the most promise.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "11700251",
      "title": "Use of missing-data methods to correct bias and improve precision in case-control studies in which cases are subtyped but subtype information is incomplete.",
      "abstract": "Histologic and genetic markers can sometimes make it possible to refine a disease into subtypes. In a case-control study, an attempt to subcategorize a disease in this way can be important to elucidating its etiology if the subtypes tend to result from distinct causal pathways. Using subtyped case outcomes, one can carry out either a case-case analysis to investigate etiologic heterogeneity or do polytomous logistic regression to estimate odds ratios specific to subtypes. Unfortunately, especially when such an analysis is undertaken after the study has been completed, it may be compromised by the unavailability of tissue specimens, resulting in missing subtype data for many enrolled cases. The authors propose that one can more fully use the available data, including that provided by cases with missing subtype, by using the expectation-maximization algorithm to estimate risk parameters. For illustration, they apply the method to a study of non-Hodgkin's lymphoma in the midwestern United States. The simulations then demonstrate that, under assumptions likely to hold in many settings, the approach eliminates bias that would arise if unclassified cases were ignored and also improves the precision of estimation. Under the same assumptions, empirical confidence interval coverage is consistent with the nominal 95%.",
      "journal": "American journal of epidemiology",
      "year": "2001",
      "doi": "10.1093/aje/154.10.954",
      "authors": "Schroeder J C et al.",
      "keywords": "",
      "mesh_terms": "Algorithms; Analysis of Variance; Bias; Case-Control Studies; Computer Simulation; Humans; Likelihood Functions; Lymphoma, Non-Hodgkin; Midwestern United States; Models, Statistical; Odds Ratio; Outcome Assessment, Health Care",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/11700251/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Logistic Regression",
      "health_domain": "Genomics/Genetics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "The simulations then demonstrate that, under assumptions likely to hold in many settings, the approach eliminates bias that would arise if unclassified cases were ignored and also improves the precision of estimation. Under the same assumptions, empirical confidence interval coverage is consistent with the nominal 95%.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "11008073",
      "title": "Lung allocation in the United States, 1995-1997: an analysis of equity and utility.",
      "abstract": "BACKGROUND: Waiting time for organ transplantation varies widely between programs of different sizes and by geographic regions. The purpose of this study was to determine if the current lung-allocation policy is equitable for candidates waiting at various-sized centers, and to model how national allocation based solely on waiting time might affect patients and programs. METHODS: UNOS provided data on candidate registrations; transplants and outcomes; waiting times; and deaths while waiting for all U.S. lung-transplant programs during 1995-1997. Transplant centers were categorized based on average yearly volume: small (< or = 10 pounds sterling transplants/year; n = 46), medium (11-30 transplants/year; n = 29), or large (>30 transplants/year; n = 6). This data was used to model national organ allocation based solely on accumulated waiting time for candidates listed at the end of 1997. RESULTS: Median waiting time for patients transplanted was longest at large programs (724-848 days) compared to small and medium centers (371-552 days and 337-553 days, respectively) and increased at programs of all sizes during the study period. Wait-time-adjusted risk of death correlated inversely with program size (365 vs 261 vs 148 deaths per 1,000 patient-years-at-risk at small, medium, and large centers, respectively). Mortality as a percentage of new candidate registrations was similar for all program categories, ranging from 21 to 25%. Survival rates following transplantation were equivalent at medium-sized centers vs large centers (p = 0.50), but statistically lower when small centers were compared to either large- or medium-size centers (p < or = 0.05). Using waiting time as the primary criterion lung allocation would acutely shift 10 to 20% of lung-transplant activity from medium to large programs. CONCLUSIONS: 1) Waiting list mortality rates are not higher at large lung-transplant programs with long average waiting times. 2) A lung-allocation algorithm based primarily on waiting-list seniority would probably disadvantage candidates at medium-size centers without improving overall lung-transplant outcomes. 3) If fairness is measured by equal distribution of opportunity and risk, we conclude that the current allocation system is relatively equitable for patients currently entering the lung-transplant system.",
      "journal": "The Journal of heart and lung transplantation : the official publication of the International Society for Heart Transplantation",
      "year": "2000",
      "doi": "10.1016/s1053-2498(00)00151-0",
      "authors": "Pierson R N et al.",
      "keywords": "Empirical Approach; Health Care and Public Health",
      "mesh_terms": "Actuarial Analysis; Health Care Rationing; Humans; Lung Transplantation; Retrospective Studies; Tissue and Organ Procurement; United States; Waiting Lists",
      "pub_types": "Journal Article; Multicenter Study; Research Support, U.S. Gov't, Non-P.H.S.; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/11008073/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Guideline/Policy",
      "ai_ml_method": "Generative AI",
      "health_domain": "Pulmonology",
      "bias_axes": "Age; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSIONS: 1) Waiting list mortality rates are not higher at large lung-transplant programs with long average waiting times. 2) A lung-allocation algorithm based primarily on waiting-list seniority would probably disadvantage candidates at medium-size centers without improving overall lung-transplant outcomes. 3) If fairness is measured by equal distribution of opportunity and risk, we conclude that the current allocation system is relatively equitable for patients currently entering the lung-...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "11128891",
      "title": "Clinician judgments of functional outcomes: how bias and perceived accuracy affect rating.",
      "abstract": "OBJECTIVE: To evaluate the accuracy of clinician judgments of patient function, the susceptibility of judges to bias, and the relation between a judge's degree of belief in his/her accuracy of classification to observed accuracy when using the FIM instrument. PARTICIPANTS: Fifty rehabilitation professionals. SETTING: 3 urban medical centers. DESIGN: Four randomized experiments among subjects to examine the effect of potentially biasing information on FIM ratings of patient vignettes. Participants answered 60 true/false questions regarding patient function and FIM score and indicated confidence in the accuracy of their answers. INTERVENTIONS: Manipulation of patient information. MAIN OUTCOME MEASURES: The standard FIM 7-point scale, observed proportion of correct responses to the 60 true/false questions, and a 6-category confidence scale for each of the 60 questions were used as dependent measures. RESULTS: FIM ratings assigned to others biased participants' FIM ratings of patient vignettes. Functional ability was overestimated when ratings in other domains were high and underestimated when they were low. Participants were overconfident in their ability to answer FIM questions accurately across all professional disciplines. CONCLUSION: Bias and poor judgment of level accuracy play a significant role in clinician ratings of patient functioning. Blind ratings and training in debiasing are potential solutions to the problem.",
      "journal": "Archives of physical medicine and rehabilitation",
      "year": "2000",
      "doi": "10.1053/apmr.2000.16345",
      "authors": "Wolfson A M et al.",
      "keywords": "",
      "mesh_terms": "Activities of Daily Living; Adult; Bias; Disability Evaluation; Female; Humans; Judgment; Male; Observer Variation; Rehabilitation; Statistics, Nonparametric; Washington",
      "pub_types": "Clinical Trial; Journal Article; Randomized Controlled Trial",
      "url": "https://pubmed.ncbi.nlm.nih.gov/11128891/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "General Healthcare",
      "bias_axes": "Gender/Sex; Geographic",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Both",
      "approach_method": "Not specified",
      "clinical_setting": "Not specified",
      "key_findings": "CONCLUSION: Bias and poor judgment of level accuracy play a significant role in clinician ratings of patient functioning. Blind ratings and training in debiasing are potential solutions to the problem.",
      "ft_include": false,
      "ft_reason": "No AI/ML component in full text",
      "ft_source": "No full text",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "7572581",
      "title": "Comparison of the sensitivity and specificity of exercise electrocardiography in biased and unbiased populations of men and women.",
      "abstract": "To assess for sex-related differences in posttest referral bias, we compared the accuracy of exercise electrocardiography in biased (coronary angiography only) and unbiased (all unselected) populations with possible coronary disease. A retrospective analysis of clinical and exercise test data from 4467 patients (788 who underwent angiography) was performed (2824 men and 1643 women). The accuracy of a positive exercise test result was assessed in the entire unbiased group with a method that used disease probability (derived with a logistic algorithm) rather than angiography results. We found that the sensitivity and specificity were significantly greater in men than in women with use of the biased or unbiased groups. When the results for the unbiased and biased groups were compared, the sensitivities for the unbiased group were significantly lower and the specificities were significantly higher than those of the biased group. These differences reflect the effects of posttest referral bias. The amounts that sensitivity decreased and specificity increased, however, was not different for men and women. Therefore, we conclude that the accuracy of exercise electrocardiography is lower in women than men irrespective of whether a biased or an unbiased group is used. However, these differences cannot be explained on the basis of sex-related differences in posttest referral bias.",
      "journal": "American heart journal",
      "year": "1995",
      "doi": "10.1016/0002-8703(95)90072-1",
      "authors": "Morise A P et al.",
      "keywords": "",
      "mesh_terms": "Adult; Electrocardiography; Exercise Test; Female; Humans; Male; Middle Aged; Predictive Value of Tests; Selection Bias; Sensitivity and Specificity; Sex Characteristics",
      "pub_types": "Comparative Study; Journal Article; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, P.H.S.",
      "url": "https://pubmed.ncbi.nlm.nih.gov/7572581/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology",
      "bias_axes": "Gender/Sex",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "Therefore, we conclude that the accuracy of exercise electrocardiography is lower in women than men irrespective of whether a biased or an unbiased group is used. However, these differences cannot be explained on the basis of sex-related differences in posttest referral bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "2033767",
      "title": "Biased estimates of expected acute myocardial infarction mortality using MedisGroups admission severity groups.",
      "abstract": "This study examines whether the MedisGroups admission severity groups give unbiased estimates of 30-day mortality in 3037 Medicare-aged patients who were hospitalized in 1985 through 1986 with acute myocardial infarction. The average observed death rate for all acute myocardial infarction patients in the study who were in a given admission severity group was used to estimate the expected death probability for each case in a given group. (This is the same method used by the Pennsylvania Health Care Cost Containment Council for risk adjusting hospital mortality by diagnosis related groups in that state.) When compared with observed deaths, estimates of expected mortality were significantly biased for many patient attributes (eg, age, location of acute myocardial infarction, history of congestive heart failure, serum potassium level, serum urea nitrogen level, pulse rate, and blood pressure). These results are consistent with a conclusion that the MedisGroups scoring algorithm omits some important risk variables, inappropriately includes some other variables reflecting postadmission status, and gives the wrong weights to some appropriate risk variables. To the extent that these findings are also applicable to current MedisGroups scoring algorithms and to other conditions and procedures, MedisGroups admission severity groups cannot fairly adjust for interhospital case mix differences in outcome studies.",
      "journal": "JAMA",
      "year": "1991",
      "doi": "",
      "authors": "Blumberg M S",
      "keywords": "",
      "mesh_terms": "Aged; Aged, 80 and over; Algorithms; Bias; Hospitals; Humans; Length of Stay; Myocardial Infarction; Outcome and Process Assessment, Health Care; Probability; Professional Review Organizations; Random Allocation; Severity of Illness Index; United States",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/2033767/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Cardiology",
      "bias_axes": "Gender/Sex; Age; Insurance Status",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Hospital/Inpatient; Clinical Trial",
      "key_findings": "These results are consistent with a conclusion that the MedisGroups scoring algorithm omits some important risk variables, inappropriately includes some other variables reflecting postadmission status, and gives the wrong weights to some appropriate risk variables. To the extent that these findings are also applicable to current MedisGroups scoring algorithms and to other conditions and procedures, MedisGroups admission severity groups cannot fairly adjust for interhospital case mix differences ...",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "2220312",
      "title": "Thermal discrimination thresholds: a comparison of different methods.",
      "abstract": "Thermal testing was carried out on 55 healthy subjects in order to establish normal results and reproducibility of warm and cold thresholds. Diurnal variations of thresholds were investigated in a further 30 normal subjects. Then the sensitivity of different testing procedures was investigated in 33 patients with diabetes mellitus, but without severe polyneuropathy. Forced choice testing takes 6 times longer than the method of limits, and the results are not considerably different. It is thought that the forced choice algorithm does not provide a method for clinical routine. Another new approach, the double random staircase method, may help to exclude bias without taking too much time.",
      "journal": "Acta neurologica Scandinavica",
      "year": "1990",
      "doi": "10.1111/j.1600-0404.1990.tb01015.x",
      "authors": "Claus D et al.",
      "keywords": "",
      "mesh_terms": "Adolescent; Adult; Aged; Circadian Rhythm; Diabetes Mellitus, Type 1; Diabetes Mellitus, Type 2; Diabetic Neuropathies; Female; Humans; Male; Middle Aged; Sensory Thresholds; Skin; Thermoreceptors; Thermosensing",
      "pub_types": "Journal Article; Research Support, Non-U.S. Gov't",
      "url": "https://pubmed.ncbi.nlm.nih.gov/2220312/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Endocrinology/Diabetes",
      "bias_axes": "Not specified",
      "lifecycle_stage": "Model Evaluation",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Threshold Adjustment",
      "clinical_setting": "Not specified",
      "key_findings": "It is thought that the forced choice algorithm does not provide a method for clinical routine. Another new approach, the double random staircase method, may help to exclude bias without taking too much time.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    },
    {
      "pmid": "3853482",
      "title": "A computer algorithm for the assessment of age reporting bias in censal population estimates using Myers' 'blended' method.",
      "abstract": "A population's age structure is widely used in the computation of many vital statistics. The importance of highly accurate vital statistics cannot be overemphasized--such statistics are used extensively by governments to determine the proper allocation of health resources and services, and by demographers, sociologists and epidemiologists to study secular trends. A computer program has been developed for use on an Apple II+ microcomputer for the analysis of population age profiles and determination of age reporting bias.",
      "journal": "Computer methods and programs in biomedicine",
      "year": "1985",
      "doi": "10.1016/0169-2607(85)90069-0",
      "authors": "Ayiomamitis A",
      "keywords": "",
      "mesh_terms": "Adolescent; Adult; Age Factors; Aged; Child; Computers; Demography; Female; Humans; Life Expectancy; Male; Microcomputers; Middle Aged; Software; Vital Statistics",
      "pub_types": "Journal Article",
      "url": "https://pubmed.ncbi.nlm.nih.gov/3853482/",
      "include": true,
      "screen_reason": "Included",
      "study_type": "Empirical Study",
      "ai_ml_method": "Not specified",
      "health_domain": "Pediatrics",
      "bias_axes": "Gender/Sex; Age",
      "lifecycle_stage": "Not specified",
      "assessment_or_mitigation": "Assessment",
      "approach_method": "Not specified",
      "clinical_setting": "Public Health/Population",
      "key_findings": "The importance of highly accurate vital statistics cannot be overemphasized--such statistics are used extensively by governments to determine the proper allocation of health resources and services, and by demographers, sociologists and epidemiologists to study secular trends. A computer program has been developed for use on an Apple II+ microcomputer for the analysis of population age profiles and determination of age reporting bias.",
      "ft_include": false,
      "ft_reason": "Excluded: insufficient approach content in abstract (0 indicators)",
      "ft_source": "Abstract only",
      "has_fulltext": false,
      "pmcid": ""
    }
  ],
  "query_stats": [
    {
      "id": "Q1",
      "label": "Core: algorithmic bias/fairness terms + health",
      "query": "(\"algorithmic bias\" OR \"algorithmic fairness\" OR \"AI bias\" OR \"AI fairness\" OR \"machine learning bias\" OR \"machine learning fairness\" OR \"bias mitigation\" OR \"debiasing\" OR \"fairness-aware\" OR \"bias detection\" OR \"bias audit\" OR \"disparate impact\" OR \"demographic parity\" OR \"equalized odds\") AND (\"health\" OR \"healthcare\" OR \"clinical\" OR \"medical\" OR \"biomedical\")",
      "total_in_pubmed": 1627,
      "retrieved": 1627,
      "new_unique": 1627,
      "cumulative": 1627
    },
    {
      "id": "Q2",
      "label": "Bias assessment/mitigation approaches in health AI",
      "query": "(\"bias\" OR \"fairness\") AND (\"assess\" OR \"mitigat\" OR \"detect\" OR \"evaluat\" OR \"framework\" OR \"approach\" OR \"method\") AND (\"artificial intelligence\" OR \"machine learning\" OR \"deep learning\" OR \"algorithm\") AND (\"health\" OR \"healthcare\" OR \"clinical\" OR \"medical\")",
      "total_in_pubmed": 6129,
      "retrieved": 6129,
      "new_unique": 5689,
      "cumulative": 7316
    },
    {
      "id": "Q3",
      "label": "Specific bias axes in clinical AI",
      "query": "(\"racial bias\" OR \"gender bias\" OR \"age bias\" OR \"socioeconomic bias\" OR \"ethnic bias\") AND (\"artificial intelligence\" OR \"machine learning\" OR \"deep learning\" OR \"clinical prediction\" OR \"clinical algorithm\" OR \"clinical decision support\")",
      "total_in_pubmed": 190,
      "retrieved": 190,
      "new_unique": 104,
      "cumulative": 7420
    }
  ],
  "meta": {
    "title_abstract_included": 947,
    "pmc_available": 667,
    "fulltext_fetched": 666,
    "ft_included": 402,
    "ft_excluded": 545
  }
}